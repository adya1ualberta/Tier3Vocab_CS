course syllabus cmpt mathematical logic and computing catalogue description an introduction to elementary applied propositional and predicate logic fundamental proof techniques with an emphasis on induction the theory of sets relations and functions course concepts are related to computer science areas with an emphasis on relational databases course objectives learn to reason logically and to manipulate logical expressions including boolean expressions learn the importance of proof and the fundamental techniques to mathematically prove a result learn the basic organizations of discrete data sets tuples relations functions and graphs understand the use of concept of relations to manipulate large masses of data via a relational database student evaluation grading scheme five assignments involves sql midterm exam november final exam total criteria that must be met to pass see grading scheme final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text discrete mathematics with applications edition susanna epp lecture schedule topic book chapter lectures introduction and mathematical language logic of compound statements logic of quantified statements methods of proof mathematical induction and recursion proofs on graphs and trees relational databases and sql set theory functions relations review tutorials begin september policies late assignments normally each assignment is due at the start of lecture on the specified date due if this is missed then the assignment is due at the start of the next lecture with a penalty for being late if this second deadline is missed then permission of the instructor must be obtained to hand in the assignment so late and if a submission is accepted so late the penalty will be much higher for one or two assignments like the last assignment the deadline will be a hard deadline with no late submissions accepted missed assignments missed assignments are graded as receiving a mark of missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http www arts usask ca students transition tips php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar university of saskatchewan calendar academic courses policy academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behavior that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of complaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals section of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to register with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss course syllabus cmpt automata and formal languages catalogue description introduces the foundations of computer science the theory of computation is explored through automata and formal languages in particular finite automata grammars turing machines and applications to computer science such as dynamic modelling computer architecture lexical analysis and parsing are studied course objectives the course objectives are as follows to better understand the mathematical foundations of computer science to gain experience with mathematical proofs generally to understand the relationships between proofs and algorithms and proofs of correctness of algorithms to understand the notion of nondeterminism and how it relates to determinism and computers to understand the topics of regular languages deterministic finite automata nondeterministic finite au tomata regular expressions context free grammars pushdown automata turing machines undecidabil ity computational complexity and other grammars to gain an appreciation for applications of formal languages and automata student evaluation grading scheme assignments each midterm exam final exam total the assignments will be due during class assignment friday january assignment wednesday february assignment wednesday march assignment wednesday march assignment wednesday march the midterm will be held monday february 2016 at the location will be announced during class and on the class website final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information recommended text john e hopcroft rajeev motwani jeffrey d ullman introduction to automata theory languages and computation edition addison wesley lecture notes will be provided online via the course website however lecture notes are not a substitute for attending class additional material and specific examples of concepts that are not present in the online lecture notes will be presented in class and only in class thus class attendance is essential to be successful in this course lecture schedule topic week date topic subtopics introduction to theoretical computer science brief history of theoretical computer science overview of course mathematical preliminaries review of sets sequences functions graphs strings languages proof by construction contradiction induction regular languages deterministic finite automaton definition examples of dfa and languages construction of a dfa from a computer program languages and automata versus inputs and algorithms applications dynamic modelling computer architec ture string matching nondeterministic finite automaton definition why silicon computers are not nondeterministic equivalence of languages accepted by nondeterminis tic finite automata to deterministic nfas and equivalence to nfas regular expressions definitions regular expression equivalence to finite automata applications pattern matching lexical analysis non regular languages and the pumping lemma context free languages grammars and generators versus acceptors history of grammars natural languages programming lan guages context free grammars applications to parsing markup xml pushdown automata equivalence of pushdown automata to context free gram mars pumping lemma turing machines deterministic turing machines definition examples turing machines as programs storage variants and equivalence random access machines and equivalence of turing ma chines to random access machines church turing thesis and the universe nondeterministic turing machines brief introduction to undecidable problems brief introduction to computational complexity grammars restrictions on type grammars context sensitive context free and regular grammars and their equivalence to au tomata models policies late assignments extensions on assignments will be granted only by the course instructor as a general rule individual requests for extensions on medical or compassionate grounds will only be considered if made prior to the due date of the assignment all extension requests will require suitable documentation missed examinations students who miss an exam should contact the instructor as soon as possible if it is known in advance that an exam will be missed the instructor should be contacted before the exam a student who is absent from a final examination due to medical compassionate or other valid reasons may apply to the college of arts and science undergraduate studentšs office for a deferred exam application must be made within three business days of the missed examination and be accompanied by supporting documents http artsandscience usask ca undergraduate advising strategies php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions past the final examination date for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the class which factors in the incomplete coursework as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the student has a passing percentile grade but the instructor has indicated in the course outline that failure to complete the required coursework will result in failure in the course a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised assigned final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed a student can pass a course on the basis of work completed in the course provided that any incom plete course work has not been deemed mandatory by the instructor in the course outline and or by col lege regulations for achieving a passing grade http policies usask ca policies academic affairs academic courses php for policies governing examinations and grading students are referred to the assessment of students section of the university policy academic courses class delivery examinations and assessment of student learning http policies usask ca policies academic affairs academic courses php academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals subsection of the university secretary website and avoid any behaviour that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca honesty studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of com plaints and appeals http www usask ca honesty studentnon pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca students academic honesty index php for more information on what academic integrity means for students see the student conduct appeals subsection of the university secretary website at http www usask ca pdf pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to regis ter with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports stu dents must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss final examination schedules information oakland campus spring term final exam schedule information the final examination period for daytime undergraduate students for the spring term begins on monday april and ends on saturday april undergraduate students taking courses in the school of arts and sciences the college of business administration and the schools of education engineering health and rehabilitation sciences information sciences and social work follow the examination schedule day classes with a course number below will meet for their last regular session the week of april there are two types of examination periods and do not depend on the hour or the day the particular class meets and are used primarily for large classes or multiple sections giving a common examination courses not identified in the departmental examination schedule should follow the hourly examination schedule which is determined by the day and time the class is scheduled during the term for example if your class meets on tuesday and thursday at a m which is not covered by a departmental examination your final examination for this class will be thursday april from p m p m with regard to if your class starts on the half hour you are to follow the hourly schedule preceding this time for example if your class meets on tuesday and thursday at a m follow the tuesday a m hourly examination schedule this schedule shows your final examination will be given on monday april from a m a m if your class meets for a period longer than one hour you are to follow the hourly schedule corresponding to the class starting time for example if your class meets on monday and wednesday from a m to a m follow the monday a m hourly examination schedule this schedule shows your final examination will be given on wednesday april from a m a m courses that meet only on wednesday thursday or friday are not specifically covered in the examination schedule final examinations for these classes are given at the monday or tuesday class period parallel to the normal class time as covered in the hourly examination schedule for example if a class meets thursdays only at p m the tuesday schedule at p m should be checked the following general rules apply to final examinations final examinations are to be given during the final examination period only in class examinations are held at the assigned times unless written approval to change an examination is obtained from the dean of the academic center in which the course is offered faculty and students with questions regarding the scheduling of examinations should contact the appropriate dean classes with a course number and above are not subject to the examination schedule and should have final examinations by the last day of the term note in most cases hourly final examinations are scheduled in the same room in which the class was held during the term if the room is available departmental examinations are usually scheduled in a different room approximately seven weeks into the term students may view their final exam schedule in peoplesoft menu self service enrollment view my exam schedule note college of general studies classes saturday college classes graduate classes and evening classes starting at p m or later are not subject to the final examination schedule and should continue to meet during this period final examinations should be held during the last scheduled class meeting hourly and departmental final examination schedule grid spring term cs fall exam for the evening section there are parts a to h on pages with a total score of points do all problems calculators are not allowed part a propositional and predicate logics points translate each english sentence a e into logic and each logic proposition f j into colloquial english let h x x is happy w x y x works for y a joe doesn t work for jack b someone works for tom c everyone works for somebody d if jill is happy then she works for herself e everyone who works for herself is happy f h ben h cindy g x h x h x h x i x h x w x greg j x w x bill h x part a continue a points construct the truth table for r p q p b point classify the above proposition circle one tautology contradiction contingency part b methods of proof points prove that the following argument is valid p r premise q premise p q premise r conclusion points prove that if n n is an even number then n is an even number part c sets points suppose the universal set is u x x is a lower case letter in the english alphabet let a i p r let b x x is a vowel x follows g in the alphabet let c a z represent each of the sets by listing their members a b b a c c c a d c points true false c c a b c true false circle one a b a b b a a true false circle one b c c true false circle one d suppose a then a part d functions points let a let b u v w x y let f ab where f x f y f u f v a determine f b f is a one to one function true false circle one c f is an onto function true false circle one points let g rr where g x let h rr where h x determine g h x points let f rr such that f x x x what is the range of f points suppose f ab is a bijection and g bc is an onto function then g f is an onto function true false circle one part e big o notation give the best big o estimate for each of these functions points points points points if f n g n and h n o g n then h n o f n true false circle one points show that is o you should show systematically how to obtain a pair of witnesses c and k for this relationship part f sequences and summation points write a formula using the summation symbol for the sum of the first n even positive integers points suppose an is an arithmetic sequence with n and the first five terms are compute the term compute the sum of the first terms part g recursive definition points let f be a function defined below compute f f f f n f n f n for n points let s be a set defined below describe the set s in simple english s s if x s and y s then x y s nothing else is in s points let an be a sequence where an and n give a recursive definition for an part h mathematical induction points to prove n p n using mathematical induction what two statements do we need to prove prove that the sum of the first n even positive integers is n n using mathematical induction give a predicate p n for the statement to be proved points p n means basis step points what is the statement that we need to prove in the basis step points prove this statement inductive step points what is the inductive hypothesis what do we assume in the inductive step points what is the statement that we need to prove using the inductive hypothesis points prove this statement summary of inference rules there are no exam questions on this page cs fall exam there are parts a to f on pages with a total score of points do all problems calculators are not allowed part a propositional and predicate logics points translate each english sentence a c into logic and each logic proposition d f into colloquial english let p x x has a cell phone c x y x has called y a jill has never called joe b everybody who has a cell phone has called somebody c no one has called anybody unless he she has a cell phone e x p x g x y c y x c x y h x p x c telemarketer x part a continue points consider the following sentence no one knows the email address of everybody in this class except for mr p who knows all email addresses define one or more appropriate predicates translate the sentence into a quantified proposition using the predicate indicate the domain of discourse of each quantifier a points construct the truth table for p q r p b point classify the above proposition circle one tautology contradiction contingency part b methods of proof points the proof in the table below proves that the following argument is valid complete the proof by filling in the inference rule and citation for each line see the list of inference rules on the last page r a p a premise x r x p x q x premise x q x conclusion points prove that if n n is an odd number then n is an odd number hint use one of the following proof strategies direct proof proof of contrapositive or proof by contradiction part c sets points suppose the universal set is u the set of integers between and inclusive let a let b x x is divisible by and x let c determine the followings a b b a c c c a d a points true false a b a a b b true false circle one b a b a b c true false circle one c a b a true false circle one d if a b and b c then it is always the case that a c true false circle one part d functions points let a u v w x y let b let f ab where f u f v f w f x and f y a let s be the set v x y determine f s b f is a one to one function true false circle one c f is an onto function true false circle one points let g rr where g x let h rr where h x determine g h x points let f rr such that f x x x what is the range of f points suppose f ab is a function and g bc is a bijection then g f is always a bijection true false circle one part e sequences and summation points write a formula for using the summation symbol do not compute it points compute the value of part e continue points suppose an is an arithmetic sequence with n and the first five terms are compute the term compute the sum of the first terms part f mathematical induction and recursive definition in this problem you will use mathematical induction to prove the following statement for any positive integer n it is the case that n suppose we want to state this sentence as n p n define an appropriate predicate p points p n is the statement basis step points what is the statement p points prove the basis step inductive step points what is the inductive hypothesis points what do you need to prove in the inductive step points complete the inductive step part f continue points let f be a function defined below compute f f f f n f n f n for n points let s be a set defined below list all elements of s that are smaller than s if x s then s and s nothing else is in s points let an be a sequence where an and n give a recursive definition for an summary of inference rules there are no exam questions on this page cs fall exam for the section there are parts a to f on pages with a total score of points do all problems calculators are not allowed part a propositional and predicate logics a points construct the truth table for p r q r b point classify the above proposition circle one tautology contradiction contingency part a continue points translate each english sentence a c into logic and each logic proposition d f into colloquial english let c x x has an email account m x y x has sent an email message to y a james has sent an email message to ken but ken never replies b any person who doesn t have an email account never gets an email message c there is a person who has emailed to everybody e x c x g x c x m spammer x h x y c x m y x part b methods of proof points prove that for any positive integer n if n is divisible by then n n is divisible by note that if n is divisible by then there exists an integer k such that n hint use one of the following strategies direct proof proof by contraposition or proof by contradiction part c sets points suppose the universal set is u the set of integers between and inclusive let a let b find the following sets a b b a b points suppose a a b and b find the followings a ba b p a points true false and short answers a b a a b a b true false circle one b a b true false circle one c suppose s a a a a a a a a then s d if a b and a c then it is always the case that a b c true false circle one e if a b then a b true false circle one part d functions points let a let b a b c d e let f ab where f c f b f a f c and f e a determine f b f is a one to one function true false circle one c f is an onto function true false circle one points let g rr where g x let h rr where h x determine g h x points let f rr such that f x x what is the range of f points suppose g ab and a b if g is not a function then g is not an onto function true false circle one part e sequences and summation points write a formula for using the summation symbol do not compute the value of the sum points compute the value of part e continue points compute the summation note after you replace all summations by appropriate formulas you may leave the numbers unevaluated for example you may stop when your answer looks like however your answer should not look like because it contains ellipsis part f mathematical induction and recursive definition points use induction to prove that for any positive integer n this sentence is in the form n p n p n is the statement basis step induction step induction hypothesis part f continue points let f be a function defined below compute f f f f n f n f n for n points let s be a set defined below circle all and only elements of s that are smaller than s if x s then x s and s nothing else is in s cs fall exam there are problems in parts a to c on pages with a total score of points do all problems calculators are not allowed part a counting points compute the value of each of these quantities a p b c points consider a cow that is walking along a road that is in the east west direction each minute the cow either walks yards to the east or yards to the west how many ways are there for the cow to walk to end up yards to the east from its starting position after walking for minutes points suppose we want to toss a coin until we get either a total of heads or a total of tails how many possible sequences of heads and tails are there show your work points suppose you are shopping for gifts to give to people for the holidays there are kinds of gifts that you are considering buying suppose it is fine to give the same kind of gift to more than one person how many ways are there to buy gifts for these people points suppose a department contains men and women how many ways are there to form a committee with members if there must be at least man and at least woman part b discrete probability points consider a biased coin whose heads comes up times as often as tails suppose we toss this coin twice and want to compute the probability that heads appears exactly once a determine the parameters of the experiment b compute the probability that heads appears exactly once c compute the expected number of heads points suppose we roll a die times compute the probability that the sum of the numbers is given that the first number is points suppose that we roll a die times what is the probability that comes up exactly twice points suppose that we roll a die until either it comes up or we have rolled it times what is the expected number of times we roll the die points consider a game where a number between and inclusive is randomly chosen the player gets points if the number is even the player gets points if the number is or smaller the player loses points if the number is divisible by for example if the number is then the player gets point what is the expected number of points if the player plays this game times part c relations points let a and b let r be a relation from a to b defined by r a b a b a list all the elements in r b display this relation graphically points let r be a relation on a set a define r to be a relation on a such that x r y if there exists z such that x r z and y r z a suppose r is the relation on the set a b c d given below show r using a digraph r r b for any relation r it is always the case that r is reflexive true false c for any relation r it is always the case that r is symmetric true false d for any relation r it is always the case that r is transitive true false points let r be a relation on defined by r circle the properties that r has and cross out the properties that r does not have reflexive symmetric antisymmetric transitive points compute r o r where r is a relation on and is given by r points suppose we want to compute the transitive closure of the relation r using warshall algorithm note that the algorithm compute a sequence of relations rn and returns rn as the output a in warshall algorithm how do we compute rk from rk in other words how do we determine if vi rk vj b suppose has been computed and is shown below compute from cs fall final exam name there are problems in parts a and b on pages with a total score of points do all problems calculators are not allowed part a counting points give a formula for p n r compute the value of p give a formula for c n r compute the value of p points suppose we want to flip a coin until either a we get more heads than tails b we get more tails than heads or c we have flipped the coin times how many possible sequences of heads and tails are there points there are kinds of donuts at a donut shop how many ways are there to get a box of donuts from this shop points suppose we want to generate a string of length from the english alphabet how many strings begin with xx consecutive x or ends with yyy consecutive y points how many people do we need to make sure that there are at least people that are born on the same month points suppose we want to generate a string of length from the english alphabet how many strings contain at most one x how many strings contain at least two x not necessarily consecutive part b discrete probability points there is a deck of cards which are numbered and two cards are chosen at random from the deck determine the size of the sample space of this experiment you don t need to give the sample space itself compute the probability of each outcome in this experiment let e be the event that both of the chosen cards are odd numbers suppose e is represented as a set determine the size of e compute p e compute the probability that one of the chosen cards is an even number points in an experiment we start by tossing a fair coin if the coin comes up tails we stop if the coin comes up heads we flip the coin one more time then we stop determine the sample space of this experiment by listing its members determine the probability distribution of this experiment give it some thought let e be the event that tails comes up at least once represent e as a set by listing its members compute p e points suppose we roll dice a red die and a blue die what is the probability that the blue die is given that the sum of the two dice is points suppose that we roll a die times what is the expected number of times that comes up your answer must be a number what is the probability that comes up exactly times give a formula that can be evaluated to a number however you don t need to simplify it points the outcome of rolling a die is considered low if either or comes up suppose we roll a die times what is the expected number of times that we have low outcomes suppose we roll a die until we get a low outcome what is the expected number of times we roll the die points consider a game where a number between and inclusive is randomly generated the player gets points if the number is or less the player gets points if the number is divisible by the player loses points if the number is divisible by for example if the number is then the player gets a net profit of point what is the expected number of points obtained if the player plays this game times points let x be a random variable that equals the number of heads minus the number of tails when biased coins are flipped suppose p h and p t for each coin what is the expected value of x cs fall final exam for section name there are problems in parts a and b on pages do all problems calculators are not allowed unless directed otherwise your answer may be a formula of numbers e g c p or a computation tree you don t have to evaluate the formula into a number however to get full credits make sure you explain a little where each component of the formula comes from or what quantity each node of the computation tree represents part a counting points give a formula for p n r compute the value of p you must evaluate the result into a number give a formula for c n r compute the value of c you must evaluate the result into a number points suppose we are generating random positive integers we are interested in the remainder when we divide each of these numbers by how many numbers do we need to make sure that there are at least numbers with the same remainder points how many ways are there to arrange the letters in the word milliliter in a row points suppose we have a bin with a large amount of red green and blue pebbles suppose we want to take one pebble at a time from the bin and put them in a row until either a we get a red pebble or b we get pebbles of the same color how many possible sequences of color pebbles are there show your work to get full credits points at an ice cream shop there are kinds of ice cream how many ways are there to get a bowl of scoops of ice cream with exactly scoop of chocolate and at least scoops of vanilla points how many ways are there to arrange the letters abcdefghxyz letters with the following restrictions a abc appear consecutively but don t have to be in that order b xyz appear consecutively in that order and c abc appear before xyz suppose we want to generate a string of length from the english alphabet a points how many strings have no y b points how many strings have no x or has at least one y or both part b discrete probability points there is a deck of cards which are numbered and two cards are chosen at random from the deck determine the size of the sample space of this experiment you don t need to give the sample space itself compute the probability of each outcome in this experiment let e be the event that numbers on the chosen cards differ by give e as a set compute p e compute the probability that one of the chosen cards is points in an experiment we start by tossing a fair coin if the coin comes up tails we stop if the coin comes up heads we flip the coin two more times then we stop determine the sample space of this experiment by listing its members determine the probability distribution of this experiment give it some thought let e be the event that heads comes up at least twice represent e as a set by listing its members compute p e points suppose we roll dice a red die and a blue die what is the probability that the blue die is given that the sum of the two dice is at least points in a bin there are red ball green balls and blue balls suppose that we randomly pick a ball from the bin times each time we put the ball back into the bin what is the expected number of times that a green ball comes up your answer must be a number what is the probability that a blue ball comes up exactly times give a formula that can be evaluated to a number you don t have to simplify it in a bin there are balls labeled suppose we randomly pick a ball from the bin until comes up note once a ball is picked we do not put the it back into the bin a points what is the probability that we pick ball and stop b points what is the probability that we pick balls and stop c points what is the expected number of balls we pick points consider a game where a number between and inclusive is randomly generated suppose the number is k the player gets points according to the following rules the player gets points if k the player gets points if k is divisible by the player gets k points for any k for example if k then the player gets points from rule and points from rule for a total of points what is the expected number of points obtained if the player plays this game points suppose we toss coins let x be a random variable that counts how many pairs of consecutive tosses have the same outcome for example suppose we toss coins and the outcome is hhttt then x because a the outcome of coins and are the same b the outcome of coins and are the same and c the outcome of coins and are the same compute e x your answer must be a number hint define indicator random variables discrete structures for computer science adam j lee sennott square lecture functions and sequences differences between and recall that a b if a is a subset of b whereas a a means that a is an element of a examples is is is is is is is be careful when computing power sets question what is p note the set has three elements so we need all combinations of those elements p this power set has elements today topics sequences and summations specifying and recognizing sequences summation notation closed forms of summations cardinality of infinite sets sequences are ordered lists of elements definition a sequence is a function from the set of integers to a set s we use the notation an to denote the image of the integer n an is called a term of the sequence examples a sequence with terms an infinite sequence note the second example can be described as the sequence an where an n what makes sequences so special question aren t sequences just sets answer the elements of a sequence are members of a set but a sequence is ordered a set is not question how are sequences different from ordered n tuples answer an ordered n tuple is ordered but always contains n elements sequences can be infinite some special sequences geometric progressions are sequences of the form arn where a and r are real numbers examples arithmetic progressions are sequences of the form a nd where a and d are real numbers examples sometimes we need to figure out the formula for a sequence given only a few terms questions to ask yourself are there runs of the same value are terms obtained by multiplying the previous value by a particular amount possible geometric sequence are terms obtained by adding a particular amount to the previous value possible arithmetic sequence are terms obtained by combining previous terms in a certain way are there cycles amongst terms what are the formulas for these sequences problem problem problem problem this is called the fibonacci sequence sometimes we want to find the sum of the terms in a sequence summation notation lets us compactly represent the sum of terms am am an upper limit index of summation lower limit example i the usual laws of arithmetic still apply constant factors can be pulled out of the summation a summation over a sum or difference can be split into a sum or difference of smaller summations example j j j j example sums example express the sum of the first terms of the sequence for n answer example what is the value of answer we can also compute the summation of the elements of some set example compute answer example let f x compute answer f f f f sometimes it is helpful to shift the index of a summation this is particularly useful when combining two or more summations for example let j k need to add to each j summations can be nested within one another often you ll see this when analyzing nested loops within a program i e cs example compute solution expand inner sum simplify if possible expand outer sum group work problem what are the formulas for the following sequences problem compute the following summations computing the sum of a geometric series by hand is time consuming would you really want to calculate by hand fortunately we have a closed form solution for computing the sum of a geometric series so proof of geometric series closed form there are other closed form summations that you should know we can use the notion of sequences to analyze the cardinality of infinite sets definition two sets a and b have the same cardinality if and only if there is a one to one correspondence from a to b definition a finite set or a set that has the same cardinality as the natural numbers is called countable a set that is not countable is called uncountable implication any sequence an ranging over the natural numbers is countable show that the set of even positive integers is countable proof graphical we have the following to correspondence between the natural numbers and the even positive integers so the even positive integers are countable proof we can define the even positive integers as the sequence for all k n so it has the same cardinality as n and is thus countable is the set of all rational numbers countable perhaps surprisingly yes this yields the sequence so the set of rational numbers is countable is the set of real numbers countable no it is not we can prove this using a proof method called diagonalization invented by georg cantor proof assume that the set of real numbers is countable then the subset of real numbers between and is also countable by definition this implies that the real numbers can be listed in some order say let the decimal representation these numbers be where dij i j proof continued now form a new decimal number r where di if dii and di otherwise example r note that the ith decimal place of r differs from the ith decimal place of each ri by construction thus r is not included in the list of all real numbers between and this is a contradiction of the assumption that all real numbers between and could be listed thus not all real numbers can be listed and r is uncountable final thoughts sets are the basis of functions which are used throughout computer science and mathematics sequences allow us to represent potentially infinite ordered lists of elements summation notation is a compact representation for adding together the elements of a sequence we can use sequences to help us compare the cardinality of infinite sets next time integers and division section discrete structures for computer science adam j lee sennott square lecture integers and modular arithmetic today topics integers and division the division algorithm modular arithmetic applications of modular arithmetic what is number theory number theory is the branch of mathematics that explores the integers and their properties number theory has many applications within computer science including organizing data encrypting sensitive data developing error correcting codes generating random numbers we will only scratch the surface the notion of divisibility is one of the most basic properties of the integers definition if a and b are integers and a we say that a divides b if there is an integer c such that b ac we write a b to say that a divides b and a b to say that a does not divide b mathematically a b c z b ac note if a b then a is called a factor of b b is called a multiple of a we ve been using the notion of divisibility all along e x x k z division examples examples does does does question let n and d be two positive integers how many positive integers not exceeding n are divisible by d answer we want to count the number of integers of the form dk that are less than n that is we want to know the number of integers k with dk n or k n d therefore there are n d positive integers not exceeding n that are divisible by d important properties of divisibility property if a b and a c then a b c proof if a b and a c then there exist integers j and k such that b aj and c ak hence b c aj ak a j k thus a b c property if a b then a bc for all integers c proof if a b then this is some integer j such that b aj multiplying both sides by c gives us bc ajc so by definition a bc one more property property if a b and b c then a c proof if a b and b c then there exist integers j and k such that b aj and c bk by substitution we have that c ajk so a c division algorithm theorem let a be an integer and let d be a positive integer there are unique integers q and r with r d such that a dq r for historical reasons the above theorem is called the division algorithm even though it isn t an algorithm terminology given a dq r d is called the divisor q is called the quotient r is called the remainder q a div d r a mod d examples question what are the quotient and remainder when is divided by answer we have that so the quotient is div and the remainder is mod question what are the quotient and remainder when is divided by answer since we have that the quotient is and the remainder is recall that since the remainder must be positive is not a valid use of the division theorem many programming languages use the div and mod operations for example in java c and c corresponds to div when used on integer arguments corresponds to mod public static void main string args prints out int x int y float z system out println y x system out println y x system out println y z prints out not prints out this can be a source of many errors so be careful in your future classes group work problem does problem what are the quotient and remainder when is divided by is divided by is divided by is divided by sometimes we care only about the remainder of an integer after it is divided by some other integer example what time will it be hours from now answer if it is now it will be mod mod am in hours since remainders can be so important they have their own special notation definition if a and b are integers and m is a positive integer we say that a is congruent to b modulo m if m a b we write this as a b mod m note a b mod m iff a mod m b mod m examples is congruent to modulo is congruent to modulo properties of congruencies theorem let m be a positive integer the integers a and b are congruent modulo m iff there is an integer k such that a b km theorem let m be a positive integer if a b mod and c d mod m then a c b d mod m ac bd mod m congruencies have many applications within computer science today we ll look at three hash functions the generation of pseudorandom numbers cryptography hash functions allow us to quickly and efficiently locate data problem given a large collection of records how can we find the one we want quickly solution apply a hash function that determines the storage location of the record based on the record id a common hash function is h k k mod n where n is the number of available storage locations memory mod mod mod id id id hash functions are not one to one so we must expect occasional collisions solution use next available location memory mod mod id id memory solution pointer tables mod id id mod many areas of computer science rely on the ability to generate pseudorandom numbers coding algorithms hardware software and network simulation security network protocols congruencies can be used to generate pseudorandom sequences step choose a modulus m a multiplier a an increment c a seed step apply the following xn axn c mod m example m a c mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod the field of cryptography makes heavy use of number theory and congruencies cryptography is the study of secret messages uses of cryptography protecting medical records storing and transmitting military secrets secure web browsing congruencies are used in cryptosystems from antiquity as well as in modern day algorithms since modern algorithms require quite a bit of sophistication to discuss we ll examine an ancient cryptosystem the caesar cipher is based on congruencies to encode a message using the caesar cipher choose a shift index convert each letter a z into a number compute f p p mod example let encode attack attack f f f f encrypted message jddjlu decryption involves using the inverse function that is f p p mod example assume that decrypt the message vhwvhdw vhwvhdw f f f f decrypted result retreat group work problem is congruent to mod is congruent to mod is congruent to mod problem the message rovvy hycvn was encrypted with the caesar cipher using decrypt it final thoughts number theory is the study of integers and their properties divisibility modular arithmetic and congruency are used throughout computer science next time prime numbers gcds integer representation section and discrete structures for computer science adam j lee sennott square lecture primes gcds and representations today topics primes greatest common divisors prime representations important theorems about primality greatest common divisors least common multiples euclid algorithm once and for all what are prime numbers definition a prime number is a positive integer p that is divisible by only and itself if a number is not prime it is called a composite number mathematically p is prime x z x x p x p examples are the following numbers prime or composite any positive integer can be represented as a unique product of prime numbers theorem the fundamental theorem of arithmetic every positive integer greater than can be written uniquely as a prime or the product of two or more primes where the prime factors are written in order of nondecreasing size examples note proving the fundamental theorem of arithmetic requires some mathematical tools that we have not yet learned this leads to a related theorem theorem if n is a composite integer then n has a prime divisor less than n proof if n is composite then it has a positive integer factor a with a n by definition this means that n ab where b is an integer greater than assume a n and b n then ab n n n which is a contradiction so either a n or b n thus n has a divisor less than n by the fundamental theorem of arithmetic this divisor is either prime or is a product of primes in either case n has a prime divisor less than n applying contraposition leads to a naive primality test corollary if n is a positive integer that does not have a prime divisor less than n then n prime example is prime the primes less than are and since is not divisible by or it must be prime example is prime the primes less than are and so must be composite this approach can be generalized the sieve of eratosthenes is a brute force algorithm for finding all prime numbers less than some value n step list the numbers less than n step if the next available number is less than n cross out all of its multiples step repeat until the next available number is n step all remaining numbers are prime how many primes are there theorem there are infinitely many prime numbers proof by contradiction assume that there are only a finite number of primes pn let q pn by the fundamental theorem of arithmetic q can be written as the product of two or more primes note that no pj divides q for if pj q then pj also divides q pn therefore there must be some prime number not in our list this prime number is either q if q is prime or a prime factor of q if q is composite this is a contradiction since we assumed that all primes were listed therefore there are infinitely many primes this is a non constructive existence proof group work problem what is the prime factorization of problem is prime is prime problem is the set of all prime numbers countable or uncountable if it is countable show a to correspondence between the prime numbers and the natural numbers greatest common divisors definition let a and b be integers not both zero the largest integer d such that d a and d b is called the greatest common divisor of a and b denoted by gcd a b note we can naively find gcds by comparing the common divisors of two numbers example what is the gcd of and factors of factors of gcd sometimes the gcd of two numbers is example what is gcd factors of factors of gcd definition if gcd a b we say that a and b are relatively prime or coprime we say that an are pairwise relatively prime if gcd ai aj i j example are and pairwise coprime factors of factors of factors of we can leverage the fundamental theorem of arithmetic to develop a better algorithm let and then greatest multiple of in both a and b greatest multiple of in both a and b example compute gcd so gcd better still is euclid algorithm observation if a bq r then gcd a b gcd b r proved in section of the book so let a and b then rn rn rn rn rn rn rnqn gcd a b rn examples of euclid algorithm example compute gcd 248 gcd example compute gcd 3828 1596 324 gcd least common multiples definition the least common multiple of the integers a and b is the smallest positive integer that is divisible by both a and b the least common multiple of a and b is denoted lcm a b example what is lcm multiples of multiples of so lcm note lcm a b is guaranteed to exist since a common multiple exists i e ab we can leverage the fundamental theorem of arithmetic to develop a better algorithm let and then greatest multiple of in either a or b greatest multiple of in either a or b example compute lcm so lcm lcms are closely tied to gcds note ab lcm a b gcd a b example a b lcm 3000 gcd lcm gcd group work problem use euclid algorithm to compute gcd problem compute gcd and lcm verify that gcd lcm final thoughts prime numbers play an important role in number theory there are an infinite number of prime numbers any number can be represented as a product of prime numbers this has implications when computing gcds and lcms next time proof by induction discrete structures for computer science adam j lee sennott square lecture proof by induction we ve learned a lot of proof methods basic proof methods direct proof contradiction contraposition cases proof of quantified statements existential statements i e x p x finding a single example suffices universal statements i e x p x can be harder to prove bottom line we need new tools mathematical induction lets us prove universally quantified statements intuition if p is true goal prove x n p x procedure prove p then p is true if p is true then p is true show that p k p k for any arbitrary k conclude that p x is true x n p p k p k x n p x analogy climbing a ladder proving p you can get on the first rung of the ladder proving p k p k if you are on the kth step you can get to the k st step x p x you can get to any step on the ladder analogy playing with dominoes proving p the first domino falls proving p k p k if the kth domino falls then the k st domino will fall x p x all dominoes will fall all of your proofs should have the same overall structure prove that induction cannot give us a formula to prove but can allow us to verify conjectures mathematical induction is not a tool for discovering new theorems but rather a powerful way to prove them example make a conjecture about the first n odd positive numbers then prove it the sequence appears to be the sequence conjecture the sum of the first n odd positive integers is prove that the sum of the first n positive odd integers is prove that the sum for all nonnegative integers n why does mathematical induction work this follows from the well ordering axiom i e every set of positive integers has a least element we can prove that mathematical induction is valid using a proof by contradiction assume that p holds and p k p k but x p x this means that the set s x p x is nonempty by well ordering s has a least element m with p m since m is the least element of s p m is true by p k p k p m p m since we have p m p m this is a contradiction result mathematical induction is a valid proof method group work a problem prove that hint be sure to define p x prove the base case make an inductive hypothesis carry out the inductive step draw the final conclusion induction can also be used to prove properties other than summations p inequalities φ divisibility and results from number theory set theory algorithms and data structures prove that n for every positive integer n prelude the expression n is called the factorial of n definition n n n examples note how quickly the factorial of n grows prove that n for every positive integer n prove that n is divisible by whenever n is a positive integer prove that if s is a finite set with n elements then s has subsets final thoughts mathematical induction lets us prove universally quantified statements using this inference rule p p k p k x n p x induction is useful for proving summations inequalities claims about countable sets theorems from number theory next time strong induction and recursive definitions sections discrete structures for computer science adam j lee sennott square lecture strong induction recall that mathematical induction let us prove universally quantified statements intuition if p is true goal prove x n p x procedure prove p then p is true if p is true then p is true show that p k p k for any arbitrary k conclude that p x is true x n p p k p k x n p x strong mathematical induction is another flavor of induction goal prove x n p x procedure prove p show that p p p k p k for any arbitrary k conclude that p x is true x n p p p p k p k x n p x so what the big deal recall in mathematical induction our inductive hypothesis allows us to assume that p k is true and use this knowledge to prove p k however in strong induction we can assume that p p p k is true before trying to prove p k for certain types of proofs this is much easier than trying to prove p k from p k alone for example show that if n is an integer greater than then n can be written as the product of primes is strong induction somehow more powerful than mathematical induction the ability to assume p p p k true before proving p k seems more powerful than just assuming p k is true perhaps surprisingly mathematical induction strong induction and well ordering are all equivalent that is a proof using one of these methods can always be written using the other two methods this may not be easy though prove that every amount of postage of cents or more can be formed using just cent and cent stamps prove that every amount of postage of cents or more can be formed using just cent and cent stamps so when should we use strong induction if it is straightforward to prove p k from p k alone use mathematical induction p p p k p k p k if it would be easier to prove p k using one or more p j for j k use strong induction p p p k p k p k group work problem use strong induction to prove that any whole dollar amount greater than or equal to can be formed using only and bills final thoughts strong induction lets us prove universally quantified statements using this inference rule p p p p k p k x n p x although sometimes more convenient than mathematical induction strong induction is no more powerful next week midterm review and midterm discrete structures for computer science adam j lee sennott square lecture recursion and structural induction there are many uses of induction in computer science proof by induction is often used to reason about algorithm properties correctness etc properties of data structures membership in certain sets determining whether certain expressions are well formed to begin looking at how we can use induction to prove the above types of statements we first need to learn about recursion sometimes it is difficult or messy to define some object explicitly recursive objects are defined in terms of themselves we often see the recursive versions of the following types of objects functions sequences sets data structures let look at some examples recursive functions are useful when defining a recursive function whose domain is the set of natural numbers we have two steps basis step define the behavior of f recursive step compute f n using f f n doesn t this look a little bit like strong induction example let f f n n f f f f some functions can be defined more precisely using recursion example define the factorial function f n recursively basis step f recursive step f n n f n note f f f f f the recursive definition avoids using the shorthand compare the above definition our old definition f n n n it should be no surprise that we can also define recursive sequences example the fibonacci numbers fn are defined as follows fn fn fn this is like strong induction since we need more than fn to compute fn calculate and f2 f3 this gives us the sequence fn recursion is used heavily in the study of strings let be defined as an alphabet binary strings lower case letters a b c z we can define the set containing all strings over the alphabet as follows basis step λ λ is the empty string containing no characters recursive step if w and x then wx example if then λ this recursive definition allows us to easily define important string operations definition the concatenation of two strings can be defined as follows basis step if w then w λ recursive step if and x then x example concatenate the strings hello and world hello world hello worl d hello wor ld hello wo rld hello w orld helloworld this recursive definition allows us to easily define important string operations definition the length l w of a string can be defined as follows basis step l λ recursive step l wx l w if w and x example l l l l l λ we can define sets of well formed formulae recursively this is often used to specify the operations permissible in a given formal language e g a programming language example defining propositional logic basis step t f and are well formed propositional logic statements where is a propositional variable recursive step if e and f are well formed statements so are e e f e f e f e f example question is p q r q t well formed basis tells us that p q r t are well formed application p q r are well formed application r q is well formed application r q t application p q r q t is well formed group work problem construct a recursive definition of the sequence an where the nth term is a natural number computed by adding the n st term to twice the n rd term assume that the first three terms of this sequence are problem construct a recursive definition of the function r that reverses a string e g r cat tac like other forms of induction structural induction requires that we consider two cases basis step show that the result holds for the objects specified in the basis case of the recursive definition recursive step show that if the result holds for the objects used to construct new elements using the recursive step of the definition then it holds for the new object as well to see how this works let revisit string length recall from earlier definition the length l w of a string can be defined as follows basis step l λ recursive step l wx l w if w and x example l l l l l λ prove that l xy l x l y for x y many common data structures used in computer science have recursive definitions example rooted trees base step a single node is a rooted tree recursive step if tn are disjoint rooted trees with roots rn then introducing a new root r connected to rn forms a new rooted tree example rooted trees base case one application two applications many common data structures used in computer science have recursive definitions example extended binary trees base step the empty set is an extended binary tree recursive step if and are disjoint extended binary trees with roots and then introducing a new root r connected to and forms a new extended binary tree example extended binary trees base case step step step many common data structures used in computer science have recursive definitions example full binary trees base step a single root node r is a full binary tree recursive step if and are disjoint full binary trees with roots and then introducing a new root r connected to and forms a new full binary tree example full binary trees base case step step trees are used to parse expressions trees are used to enable fast searches consider the set s 22 77 question is s question is s yes no as with other recursively defined objects we can define many properties of trees recursively definition given a tree t we can define the height of t recursively as follows basis step if t consists only of the root node r then h t recursive step if t consists of a root r that connects to subtrees tn then h t max h h tn example what is the height of this tree t h t max l r h l h h max h h h r h h h h if t is a full binary tree then the number of nodes in t denoted n t is less than or equal to t r if t is a full binary tree then the number of nodes in t denoted num t is less than or equal to t r inductive step cont we have that n t n n by recursive formula of n t by i h max sum of terms twice larger term h h max x y t by recursive def n of h t t conclusion since we have proved the base case and the inductive case the claim holds by structural induction group work problem use structural induction to prove that checking whether some number is contained in a binary search tree t involves at most t comparison operations final thoughts structural induction can be used to prove properties of recursive functions sequences sets data structures next time we start learning about counting and combinatorics section recursively defined sets are also used frequently in computer science simple example consider the following set s basis step s recursive step if x s and y s then x y s claim the set s thus contains every multiple of intuition s s since and are in s s since and are in s we ll show how we can prove this claim during the next lecture recall from last lecture simple example consider the following set s basis step s recursive step if x s and y s then x y s claim the set s thus contains exactly the set of all positive multiples of let the set a contain all positive multiples of to prove the above claim we need to show a s and s a we can do this using structural induction the set s contains exactly the set of all positive integer multiples of first we ll prove that a s the set s contains exactly the set of all positive integer multiples of now we ll prove that s a since s a and a s the claim holds discrete structures for computer science adam j lee sennott square lecture counting basics today topics n introduction to combinatorics n product rule n sum rule what is combinatorics combinatorics is the study of arrangements of discrete objects just think of this as a fancy word for counting many applications throughout computer science l algorithm complexity analysis l resource allocation scheduling l security analysis l today we will learn the basics of counting more advanced topics will be covered in later lectures a motivating example to access most computer systems you need to login with a user name and a password suppose that for a certain system l passwords must contain either or characters l each character must be an uppercase letter or a digit l every password must contain at least one digit solving these types of problems requires that we learn how to count complex objects fortunately we can solve many types of combinatorial problems using two simple rules the product rule the sum rule product rule applies when a counting problem can be broken into multiple tasks the product rule suppose a procedure can be broken into a sequence tk of tasks further let there be nk ways to complete each task then there are nk ways to complete the procedure to apply the product rule do the following identify each task tk for each task ti determine the ni the number of possible ways to complete ti compute nk let look at a few examples an easy example assigning offices example it is alice and bob first day of work at acme inc if there are unused offices at acme how many ways can alice and bob be assigned an office step determine tasks give alice an office give bob an office step count possible completions can give any one of offices to alice can give any one of the remaining offices to bob step compute the product l alice and bob can be assigned offices in ways auditorium seating example the chairs in an auditorium are to be labeled using an upper case letter and a positive number not exceeding e g what is the maximum number of seats that can be placed in the auditorium solution l task count the letters that can be used l task count the numbers that can be used l so the auditorium can hold chairs counting bit strings example how many bit strings of length are there solution l so there are bit strings of length counting to functions example how many to functions are there mapping a set a containing m elements to another set b containing n elements assuming that m n solution l task map first element of a to b l task map second element of a to b l task map third element of a to b l l task m map last element of a to b l so there are a total of to functions from a to b license plates example suppose that in some state license plates consist of three letters followed by three decimal digits how many valid license plates are there a b c choices for each choices for each solution there are possible valid license plates group work problem how many subsets does a finite set s have assuming that s n hint think about a bitmap representation of the set problem in north america phone numbers are of the form nxx nxx xxxx where n and x how many valid telephone numbers are there the sum rule applies when a single task can be completed using several different approaches the sum rule suppose that a single task can be completed in either one of ways one of ways or one of nk ways then the task can be completed in nk different ways note we can break the set of all possible solutions to the problem into disjoint subsets e g if we have k classes of solutions then s sk l s sk l sk since sk are disjoint l nk university committees example suppose that either a cs professor or a cs graduate student can be nominated to serve on a particular university committee if there are cs professors and cs graduate students how many ways can this seat on the committee be chosen solution l let p be the set of professors g be the set of graduate students s be the solution set with s p g l then there are s p g p g ways to fill the empty seat on the committee travel choices example jane wants to travel from pittsburgh to new york city if she flies she can leave at any one of departure times if she takes the bus she can leave at any one of departure times if she takes the train she can leave at any one of departure times how many different departure times can jane choose from solution l s f b t so l s f b t l f b t l l departure times the product and sum rules are kind of boring most interesting counting problems cannot be solved using the product rule or the sum rule alone but many interesting problems can be solved by combining these two approaches passwords revisited to access most computer systems you need to login with a user name and a password use the product rule to count passwords of each possible length choices sum rule suppose that for a certain system l passwords must contain either or characters l each character must be an uppercase letter or a digit l every password must contain at least one digit first we ll apply the sum rule let l set of passwords of length l set of passwords of length l set of passwords of length l s note s since each element of and is made up of independent choices of letters and numbers we can apply the product rule to determine and recall a password must contain at least one number observation to figure out the number of character passwords containing at least one number it is easier for us to count all character passwords and then subtract away those passwords not containing a number note there are l character passwords l character passwords not containing a digit so wrapping it all up we can compute l l 353 l 576 by leveraging our earlier observation that s we can conclude that there are valid passwords for our target system ip addresses an ip address is a bit string that is used to identify a computer that is connected to the internet there are three categories of ip addresses that can be assigned to computers class a addresses consist of the prefix followed by a bit network id and a bit host id class b addresses consist of the prefix followed by a bit network id and a bit host id class c addresses consist of the prefix followed by a bit network id and an bit host id so how many valid ip addresses are there note ip addresses are subject to restrictions l cannot be used as the network id of a class a ip l host ids consisting of only or only cannot be used to count ip addresses we will use the sum rule and the product rule so s sa sb sc so s sa sb sc compute sa l network ids since can t be used l host ids for each network id l total of class a ip addresses so how many valid ip addresses are there cont compute sb l network ids l host ids for each network id l total of class b ip addresses compute sc l network ids l host ids for each network id l total of class c ip addresses since s sa sb sc there are ip addresses that can be assigned to computers connected to the internet group work problem a committee is formed by choosing one representative from each of the us states this representative is either the governor of that state or one of the two senators from that state how many possible ways are there to form this committee problem how many license plates can be made using either two letters followed by four digits or two digits followed by four letters final thoughts n combinatorics is just a fancy word for counting n there are many any uses of combinatorics throughout computer science n we can solve a variety of interesting problems using simple rules like the product rule and the sum rule n next time l inclusion exclusion principle section l the pigeonhole principle section discrete structures for computer science adam j lee sennott square lecture counting basics pigeonhole principle today topics inclusion exclusion principle the pigeonhole principle sometimes when counting a set we count the same item more than once for instance if something can be done ways or ways but some of the ways are the same as some of the ways in this case is an overcount of the ways to complete the task what we really want to do is count the ways to complete the task and then subtract out the common solutions this is called the inclusion exclusion principle we can formulate this concept using set theory suppose that a task t can be completed using a solution drawn from one of two classes and as in the sum rule we can define the solution set for the task t as s then s do you remember this from earlier this semester counting bit strings example how many bit strings of length start with a or end with solution bit strings start with a bit strings end with bit strings start with a and end with so we have ways to construct an bit string that starts with a or ends with job applications example a company receives applications suppose of these people majored in cs majored in business and were cs business double majors how many applicants majored in neither cs nor business solution let c be the set of cs majors b be the set of business majors c b c b c b so of the applications applications neither majored in cs nor business the pigeonhole principle is an incredibly simple concept that is extremely useful the pigeonhole principle if k is a positive integer and k objects are placed in k boxes then at least one box contains at least two objects example k the pigeonhole principle is also easy to prove the pigeonhole principle if k is a positive integer and k objects are placed in k boxes then at least one box contains at least two objects proof assume that each of the k boxes contains at most item this means that there are at most k items which is a contradiction of our assumption that we have k items so at least one box must contain more than one item examples example among any group of people there are at least two with the same birthday since there are only possible birthdays example among any english words at least two will start with the same letter the pigeonhole principle can be used to prove a number of interesting results claim every integer n has a multiple whose decimal representation contains only and proof let n be a positive integer this number has n ones consider the set of n integers s 111 note that when any integer x is divided by n there are n possible remainders through n since s contains n elements at least two elements of s have the same remainder when divided by n say x and y with x y since x y mod n n x y and thus na x y finally we note that x y contains only and group work problem how many strings of three decimal digits begin with a or end with a problem how many bit strings of length begin with three or end with two problem if a student can get either an a b c d or f on a test how many students are needed to ensure that at least two get the same grade there is a more general form of the pigeonhole principle that is even more useful the generalized pigeonhole principle if n objects are placed into k boxes then there is at least one box containing at least n k items proof assume that no box contains more than n k objects note that n k n k so k n k k n k n this contradicts our assumption that we had n objects example what is the minimum number of students needed to guarantee that at least six students receive the same grade if possible grades are a b c d and f solution need the smallest integer n such that n with students it would be possible though unlikely to have students get each possible grade by adding a student we guarantee that at least students get one possible grade so the smallest such n is from the casino how many cards must be drawn from a standard card deck to guarantee that three cards of the same suit are drawn solution let make piles one for each suit we want to have n we can do this using cards note we don t need cards to end up with three from the same suit if we did we could never get a flush in poker we can t always use the pigeonhole principle directly how many card would we need to draw to ensure that we picked at least three hearts in the worst case we would need to draw every club spade and diamond before getting three hearts so to guarantee three hearts we need to draw cards ma bell what is the least number of area codes needed to guarantee that the million phones in some state can be assigned distinct digit phone numbers of the form nxx nxx xxxx solution the product rule tells us that there are million phone numbers of the form nxx xxxx think of phones as objects and phone numbers as boxes by the generalized pigeonhole principle we know that some box contains at least objects this means that we need area codes to ensure that each phone gets a unique digit number this has been easy so far right unfortunately life isn t always easy sometimes we need to be clever when we are defining our boxes or assigning objects to them for example sports during a month with days a baseball team plays at least one game per day but no more than games total show that there must be some period of consecutive days in which exactly games are played solution let aj be the number of games played on or before the jth day of the month note that the sequence aj is strictly increasing note also that aj is also an increasing sequence now consider there are terms in this sequence all by the pigeonhole principle at least two terms are equal note each aj for j is distinct as is each aj this means there exists some ai that is equal to some aj so games were played from day j to day i number theory show that among any n positive integers not exceeding there must be an integer that divides one of the other integers proof call our n positive integers an write each ai as where qi is an odd positive integer and ki is non negative i e ki might be zero note that there are n odd positive integers less than by the pigeonhole principle at least two of qn must be equal this means we have some ai and some aj if ki kj then ai aj if ki kj then aj ai group work problem what is the minimum number of students each of whom comes from one of the states who must be enrolled in a university to guarantee that there are at least who come from the same state problem a drawer contains a dozen brown socks and a dozen black socks all unmatched how many socks must be drawn to find a matching pair how many socks must be drawn to find a pair of black socks final thoughts the inclusion exclusion principle is useful when we need to avoid overcounting the pigeonhole principle and its generalized form are useful for solving many types of counting problems next time permutations and combinations section discrete structures for computer science adam j lee sennott square lecture propositional logic january announcements homework will be assigned wednesday today topic propositional logic what is a proposition logical connectives and truth tables translating between english and propositional logic logic is the basis of all mathematical and analytical reasoning given a collection of known truths logic allows us to deduce new truths example base facts if it is raining i will not go outside if i am inside stephanie will come over stephanie and i always play scrabble if we are together during the weekend today is a rainy saturday conclusion stephanie and i will play scrabble today logic allows us to advance mathematics through an iterative process of conjecture and proof propositional logic is a very simple logic definition a proposition is a precise statement that is either true or false but not both examples true all dogs have legs false false washington d c is the capital of the usa true not all statements are propositions marcia is pretty pretty is a subjective term true if x false otherwise springfield is the capital true in illinois false in massachusetts we can use logical connectives to build complex propositions we will discuss the following logical connectives not conjunction and disjunction or exclusive or implication biconditional negation the negation of a proposition is true iff the proposition is false what we know what we want one row for each possible value of what we know the truth table for negation negation examples negate the following propositions today is monday what is the truth value of the following propositions is a prime number pittsburgh is in pennsylvania conjunction the conjunction of two propositions is true iff both propositions are true the truth table for conjunction rows since we know both p and q disjunction the disjunction of two propositions is true if at least one proposition is true the truth table for disjunction conjunction and disjunction examples let p this symbol means is defined as or is equivalent to q a lion weighs less than a mouse r pittsburgh is located in pennsylvania what are the truth values of these expressions p q p p q q r exclusive or xor the exclusive or of two propositions is true if exactly one proposition is true the truth table for exclusive or note exclusive or is typically used to natural language to identify choices for example you may have a soup or salad with your entree implication the implication p q is false if p is true and q is false and true otherwise terminology p is called the hypothesis q is called the conclusion the truth table for implication implication cont the implication p q can be read in a number of equivalent ways if p then q p only if q p is sufficient for q q whenever p implication examples let p jane gets a on her final q jane gets an a what are the truth values of these implications p q q p other conditional statements given an implication p q q p is its converse q p is its contrapositive p q is its inverse why might this be useful note an implication and its contrapositive always have the same truth value biconditional the biconditional p q is true if and only if p and q assume the same truth value the truth table for the biconditional note the biconditional statement p q is often read as p if and only if q or p is a necessary and sufficient condition for q truth tables can also be made for more complex expressions example what is the truth table for p q r subexpressions of what we want to know what we want to know like mathematical operators logical operators are assigned precedence levels negation q r means q r not q r conjunction disjunction q r means q r not q r implication q r means q r not q r biconditional in general we will try to use parenthesis to disambiguate these types of expressions group exercises problem show that an implication p q and its contrapositive q p always have the same value hint construct two truth tables problem construct the truth table for the compound proposition p q r english sentences can often be translated into propositional sentences but why would we do that reasoning about law philosophy and epistemology verifying complex system specifications example example you can see an r rated movie only if you are over or you are accompanied by your legal guardian let find logical connectives translate fragments create logical expression example example you can have free coffee if you are senior citizen and it is a tuesday let example example if you are under and are not accompanied by your legal guardian then you cannot see the r rated movie let note the above translation is the contrapositive of the translation from example logic also helps us understand bitwise operations computers represent data as sequences of bits e g bitwise logical operations are often used to manipulate these data if we treat as true and as false our logic truth tables tell us how to carry out bitwise logical operations bitwise logic examples group exercises problem translate the following sentences if it is raining then i will either play video games or watch a movie you get a free salad only if you order off of the extended menu and it is a wednesday problem solve the following bitwise problems final thoughts propositional logic is a simple logic that allows us to reason about a variety of concepts in recitation more examples and practice problems be sure to attend next lecture logic puzzles and logical equivalences please read sections and discrete structures for computer science adam j lee sennott square lecture permutations and combinations today topics permutations combinations binomial coefficients a permutation is an ordered arrangement of a set of objects s σ s note a permutation of some set is essentially just a shuffle of that set sometimes we re interested in counting the number of ways that a given set can be arranged example suppose that a photographer wants to take a picture of three dogs how many ways can the dogs be arranged there are six possible arrangements of three dogs counting permutations in general we can use the product rule to count the number of permutations of a given set given a set of n items we have n ways to pick the item in the permuted set n ways to pick the item in the permuted set n ways to pick the item in the permuted set way to choose the last item in the permuted set so for a set of size n we have n n n n ways to permute that set and the winner is example six friends run in a foot race how many possible outcomes of the race are there assuming that there are no ties solution ways to choose place ways to choose place ways to chose place ways to choose place ways to choose place way to choose last place so there are possible outcomes functions and permutations let s be some set if f s s is a bijection then f describes a permutation of the set s example let s f f f s f s more often than not we re only interested in arranging a subset of a given set definition an r permutation is a permutation of some r elements of a set example let s alice bob carol dave then dave bob is a permutation of s carol alice bob is a permutation of s bob dave is a permutation of s rather than specifying a particular r permutation of a set we re usually more interested in counting the number of r permutations of a set counting r permutations definition we denote the number of r permutations of a set of size n by p n r by the product rule p n r n n n r n n r example in a foot race between six people how many ways can the gold silver and bronze medals be assigned assuming that there are no ties solution p so ways to assign medals the traveling salesperson example a salesperson must visit different cities the first and last cities of her route are specified by her boss but she can choose the order of the other visits how many possible trips can she take solution since first and last cities are fixed we must count the number of ways to permute the remaining cities so there are possible trips that the salesperson can take counting strings example how many permutations of abcdefg contain the substring abc solution observation treat abc as one character now how many ways are there to permute abc d e f g abc d e f g permutations contain the substring abc how do we count unordered selections of objects example how many ways can we choose two objects from the set s conclusion there are three combinations of a set of size three counting r combinations definition let c n r denote the number of r combinations of a set of size n then proof the r permutations of the set can be formed by finding all of the r combinations of the set and then permuting each r combination in each possible way there are p r r ways to permute each possible r combination so p n r c n r p r r this means that c n r p n r p r r n n r r n r n r alternate notation note sometimes c n r is read n choose r this is intuitive as c n r specifies the number of ways to choose r objects from a set of size n note c n r is often written as in this class i will use the notation c n r exclusively be careful using the formula for c n r using the formula for c n r directly can result in doing lots of multiplication instead note that much of the denominator cancels out terms in the numerator for example choosing participants example say that a class consists of students how many ways can people be chosen from this class to participate in a survey solution want to compute is choose c 28 ways to choose participants poker hands example given a standard card deck how many card poker hands can be drawn solution c different hands an interesting observation note given a set of size n choosing r elements is the same as excluding n r elements this means that c n r c n n r proof permutations and combinations can be used in conjunction with the product and sum rules example suppose the cs department has faculty members and the math department has faculty members how many ways can a committee consisting of cs faculty members and math faculty members be chosen solution c ways to choose cs profs for each of these there are c ways to choose math profs so we need to compute c c c c 720 ways group work problem consider a boat race with entrants how many ways can the and place trophies be awarded problem in the above race how many ways can the set of entrants not receiving a trophy be selected problem consider a standard card deck if two jokers are added to this deck how many card hands can be dealt containing both jokers combinations can be helpful when examining binomial expressions definition a binomial is an expression involving the sum of two terms for example x y is a binomial as is j r combinations are also called binomial coefficients since they occur as coefficients in the expansions of binomial expressions example x y c c c c the binomial theorem the binomial theorem let x and y be variables and let n be a non negative integer then example compute x y c c c x y 2xy an easy example question what is the coefficient of in the expansion of x y solution by the binomial theorem we know that this term can be written as c so the coefficient of is c a slightly more complicated example question what is the coefficient of in the expansion of solution note that y the binomial theorem tells us that thus the coefficient of occurs when j c c 13 25 13 pascal identity pascal identity c n k c n k c n k proof let t be a set of n elements let a t and s t a by definition there are c n k subsets of t containing k elements a subset of t containing k elements either contains k elements of s or the element a along with k elements of s there are c n k subsets of s containing k elements so there are c n k subsets of t not containing a further there are c n k subsets of s containing k elements so there are c n k subsets of t containing a thus c n k c n k c n k we can use pascal identity to define c n r recursively basis step for all n we have that c n c n n recursive step c n k c n k c n k example compute c c c c c c using this definition we can compute c n r without using multiplication at all group work problem what is the coefficient of the term in x y problem use the recursive definition of c n r to calculate c final thoughts permutations count the ways that we can shuffle a set r permutations count the number of ways that we can shuffle r items in a set r combinations are useful when we want to count unordered subsets of a given set next time generalized permutations and combinations discrete structures for computer science adam j lee sennott square lecture generalized permutations and combinations counting problems can often be harder than those from the last few lectures for example combinations with repetition repeated choice permuting indistinguishable items permutations with repetition recall r permutations are ordered collections of r elements drawn from some set if an r permutation is drawn from a set of size n without replacement then there are p n r n n r possible r permutations if we select the elements of a permutation with replacement then we can use the product rule to count the number of possible r permutations how many strings of length r can be created using the english letters let our set s a b c z with s to count the number of r length strings note that ways to choose letter ways to choose letter not ways to choose letter not ways to choose rth letter not r so there are possible ways to choose an r length string from the set s with replacement in general there are nr possible ways to permute a set of size n if repetition of elements is allowed many times we want to examine combinations of objects in which repeated choices are allowed example how many ways can four pieces of fruit be chosen from a bowl containing at least four apples four oranges and four pears assume that only the type of fruit chosen matters not the individual piece solution explicit enumeration this is tedious apples apples orange oranges pear apples oranges apples orange pear oranges apples pear pears apple apples pears oranges apple pear pears oranges apple pears orange oranges pears pears apple orange so there are possible combinations of a set containing items if repetition is allowed let find a nice closed form expression for counting r combinations with repetition example consider a cash box containing bills bill bills bill bills bills and bills how many ways are there to choose bills if order does not matter and bills within a single denomination are indistinguishable from one another assume that there are at least bills of each denomination observations denominations of bills the order that bills are drawn does not matter at least bills of each denomination implication we are counting combinations with repetition from a set of items an interesting insight note the cash box has compartments these compartments are separated by dividers choosing bills is the same as arranging placeholders and dividers examples this leads us to a nice formula observation arranging stars and bars is the same as choosing places for the stars out of total places this can be done in c ways general theorem there are c n r r r combinations from a set with n elements when repetition of elements is allowed buying cookies example how many ways can we choose six cookies at a cookie shop that makes types of cookie assume that only the type of cookies chosen matters not the order in which they are chosen or the individual cookies within a given type solution need six stars since we are choosing six cookies need bars to separate the cookies by type so c ways to choose places to put stars solution since we choose six cookies r four possible cookie types means n so c c ways to choose cookies solving equations example how many solutions does the equation have if and are non negative integers observation solving this problem is the same as choosing objects from a set of objects such that objects of type one are chosen objects of type two are chosen and objects of type three are chosen solution n r so there are c c ways to solve this equation formula summary how do we deal with indistinguishable items example how many strings can be formed by permuting the letters of the word mom observation we can t simply count permutations of the letters in mom why not counting permutations leads to an overcount rewrite mom as possible permutations are these are really the same rather than permuting all letters as a group arrange identical letters separately note the string mom contains two ms and one o we can count the distinct strings formed by permuting mom as follows set up slots for letters count the ways that the ms can be assigned to the these slots count the ways that the o can be assigned to the remaining slots use the product rule this tactic can be stated more generally theorem the number of different permutations of n objects where there are indistinguishable objects of type indistinguishable objects of type and nk indistinguishable objects of type k is ways to place objects of type ways to place there is always only one way to place objects of type k objects of type how many strings can be formed by permuting the letters in success note success contains s u c e ways to assign each letter group s c u c c c e c so we can form c c c c distinct strings using letters from the word success group work problem how many ways can we choose donuts from a donut shop that sells three types of donut problem how many distinct strings can be formed by permuting the letters of the word radar many counting problems can be solved by placing items in boxes we can consider two types of objects distinguishable objects e g billy chrissy and dan indistinguishable objects e g three students we can also consider two types of boxes distinguishable boxes e g room and room indistinguishable boxes e g two homerooms counting assignments of distinguishable items to distinguishable boxes example how many ways are there to deal card poker hands from a card deck to each of four players solution player c ways to deal player c ways to deal player c ways to deal player c ways to deal theorem the number of ways that n distinguishable items can be placed into k distinguishable boxes so that ni objects are placed into box i i k is we can prove this using the product rule how can we place n indistinguishable items into k distinguishable boxes this turns out to be the same as counting the n combinations for a set with k elements when repetition is allowed recall we solved the above problem by arranging placeholders and dividers to place n indistinguishable items into k distinguishable bins treat our indistinguishable items as use to divide our distinguishable bins count the ways to arrange n placeholders and k dividers result there are c n k n ways to place n indistinguishable objects into k distinguishable boxes let see how this works example how many ways are there to place indistinguishable balls into distinguishable bins observation treat balls as use dividers to separate bins pick positions out of a total to place balls all remaining solution we have c c ways to arrange indistinguishable balls into distinguishable bins sadly counting the ways to place distinguishable items into indistinguishable boxes isn t so easy example how many ways can anna billy caitlin and danny be placed into three indistinguishable homerooms solution let call our students a b c and d goal partition a b c and d into at most disjoint subsets one way to put everyone in the same homeroom a b c d seven ways to put everyone in two homerooms a b c d a b d c a c d b b c d a a b c d a c b d a d b c six ways to put everyone into three homerooms a b c d a c b d a d b c b c a d b d a c c d a b total ways to assign anna billy caitlin and danny to three indistinguishable homerooms is there some simple closed form that we can use to solve this type of problem no but there is a complicated one s n j is a stirling number of the second kind that tells us the number of ways that a set of n items can be partitioned into j non empty subsets s n j is defined as follows result the number of ways to distribute n distinguishable objects into k indistinguishable boxes is what about distributing indistinguishable objects into indistinguishable boxes example how many ways can six copies of the same book be packed in at most four boxes if each box can hold up to six books solution total there are ways to pack identical books into at most indistinguishable boxes that was ugly unfortunately no here why placing n indistinguishable objects into k indistinguishable boxes is the same as writing n as the sum of at most k positive integers arranged in non increasing order i e n aj where aj and j k we say that aj is a partition of n into j integers there is no simple closed formula for counting the partitions of an integer thus there is no solution for placing n indistinguishable items into k indistinguishable boxes group work problem joe karen and liz need to get their cars fixed if bob body shop has two branches in town how many ways can cars be partitioned between these indistinguishable branches problem animal control picks up stray dogs how many ways can animals be turned over to the humane society the spca and the springfield animal shelter provided that each organization can accept at least dogs final thoughts many counting problems require us to generalize the simple permutation and combination formulas from last time other problems can be cast as counting the ways to arrange in distinguishable objects into in distinguishable boxes discrete structures for computer science adam j lee sennott square lecture discrete probability the study of probability is concerned with the likelihood of events occurring like combinatorics the origins of probability theory can be traced back to the study of gambling games still a popular branch of mathematics with many applications risk assessment simulation genetics algorithm design gambling many situations can be analyzed using a simplified model of probability assumptions finite number of possible outcomes each outcome is equally likely card games dice flipping coins roulette lotteries terminology definitions an experiment is a procedure that yields one of a given set of possible outcomes the sample space of an experiment is the set of possible outcomes an event is a subset of the sample space given a finite sample space s of equally likely outcomes the probability of an event e is p e e s example experiment roll a single sided die one time sample space one possible event roll an even number the probability of rolling an even number is solving these simplified finite probability problems is easy i told you that combinatorics and probability were related step identify and count the sample space step divide step count the size of the desired event space when two dice are rolled what is the probability that the sum of the two numbers is seven step identify and count sample space sample space s is all possible pairs of numbers product rule tells us that s step count event space e step divide probability of rolling two dice that sum to is p e p e e s balls and bins example a bin contains green balls and red balls what is the probability that a ball chosen from the bin is green solution possible outcomes balls green balls so e so p e that a green ball is chosen hit the lotto example suppose a lottery gives a large prize to a person who picks digits between in the correct order and a smaller prize if only three digits are matched what is the probability of winning the large prize the small prize solution grand prize s possible lottery outcomes s e all digits correct e so p e smaller prize s possible lottery outcomes s e one digit incorrect we can count e using the sum rule ways to get digit wrong or ways to get digit wrong or ways to get digit wrong or ways to get digit wrong so e p e mega lotteries example consider a lottery that awards a prize if a person can correctly choose a set of numbers from the set of the first positive numbers what is the probability of winning this lottery solution s all sets of six numbers between and note that order does not matter in this lottery thus s c only one way to do this correctly so e so p e lesson you stand a better chance at being struck by lightning than winning this lottery four of a kind example what is the probability of getting four of a kind in a card poker hand solution s set of all possible poker hands recall s c e all poker hands with cards of the same type to draw a four of a kind hand c ways to choose the type of card king ace c way to choose all cards of that type c ways to choose the card in the hand so e c c c p e a full house example how many ways are there to draw a full house during a game of poker reminder a full house is three cards of one kind and two cards of another kind solution s c 960 e all hands containing a full house to draw a full house choose two types of cards order matters choose three cards of the first type choose two cards of the second type so e p e sampling with or without replacement makes a difference example consider a bin containing balls labeled with the numbers how likely is the sequence to be drawn in order if a selected ball is not returned to the bin what if selected balls are immediately returned to the bin solution note since order is important we need to consider permutations if balls are not returned to the bin we have p ways to select balls if balls are returned we have ways to select balls since there is only one way to select the sequence 48 in order we have that p e 200 if balls are not replaced p e if balls are replaced yes calculating probabilities is easy anyone can divide two numbers key point be careful when you define the sets s and e count the cardinality of s and e group work problem consider a box with green balls and pink ball what is the probability of drawing a pink ball what is the probability of drawing two green balls in two successive picks problem in poker a straight flush is a hand in which all cards are from the same suit and occur in order for example a hand containing the and of hearts would be a straight flush while the hand containing the and of hearts would not be what is the probability of drawing a straight flush in poker problem a flush is a hand in which all five cards are of the same suit but do not form an ordered sequence what is the probability of drawing a flush in poker what about events that are derived from other events recall an event e is a subset of the sample space s s definition p e p e e proof note that e s e since s is universe of all possible outcomes so e s e thus p e e s by definition s e s by substitution e e s simplification p e by definition sometimes counting e is hard example a bit sequence is randomly generated what is the probability that at least bit is solution s all bit strings s e all bit strings with at least zero e all bit strings with no zeros p e p e so the probability of a randomly generated bit string containing at least one is we can also calculate the probability of the union of two events definition if and are two events in the sample space s then p p p p why does this look familiar proof recall p s s s s s p p p divisibility example what is the probability that a positive integer not exceeding is divisible by either or solution let be the event that an integer is divisible by let be the event that an integer is divisible by is the event that an integer is divisible by or is the event that an integer is divisible by and p not all events are equally likely to occur sporting events investments games of strategy nature we can model these types of real life situations by relaxing our model of probability as before let s be our sample space unlike before we will allow s to be either finite or countable we will require that the following conditions hold p for each s no event can have a negative likelihood of occurrence or more than a chance of occurence in any given experiment some event will occur the function p s is called a probability distribution simple example fair and unfair coins example what probabilities should be assigned to outcomes heads h and tails t if a fair coin is flipped what if the coin is biased so that heads is twice as likely to occur as tails case fair coins each outcome is equally likely so p h p t check case biased coins note p h t p h p t t p t t p t p h are the following probability distributions valid why or why not s where p p p p s a b c p a p b p c s where p p p p s a b c p a p b p c more definitions definition suppose that s is a set with n elements the uniform distribution assigns the probability n to each element of s the distribution of fair coin flips is a uniform distribution definition the probability of an event e s is the sum of the probabilities of the outcomes in e that is can we reconcile this definition of probability with that of laplace consider the uniform distribution over a finite sample space s s n in this case p n for each s check definitions n under laplace p s n this is the same probability assigned to e by the formula on the last slide for an event e such that e e p e e s e n e n loaded dice example suppose that a die is biased so that appears twice as often as each other number but that the other five outcomes are equally likely what is the probability that an odd number appears when we roll this die solution p p p p p p note that p p p p p and p so p p p p p thus p p p p p and p now we want to find p e where e p e p p p group work consider a die in which i and are rolled with the same frequency ii is rolled times more often than iii is rolled times more often than iv and is rolled times more often than problem what is the probability distribution for this die problem what is the probability of rolling a or a problem what is the probability of rolling an even number an odd number final thoughts probability allows us to analyze the likelihood of events occurring today we learned how to analyze events that are equally likely as well as those that have non equal probabilities of occurrence next time more probability theory section discrete structures for computer science adam j lee sennott square lecture probability theory not all events are equally likely to occur sporting events investments games of strategy nature we can model these types of real life situations by relaxing our model of probability as before let s be our sample space unlike before we will allow s to be either finite or countable we will require that the following conditions hold p for each s no event can have a negative likelihood of occurrence or more than a chance of occurence in any given experiment some event will occur the function p s is called a probability distribution our formulas for the probability of combinations of events still work property p e p e s recall that s e e for any event e further so p s p e p e thus p e p e property p p p p recall that s let x be some outcome in if x is in one of or then p x is counted once on the rhs of the equation if x is in both or then p x is counted times on the rhs of the equation a formula for the probability of pairwise disjoint events theorem if en is a sequence of pairwise disjoint events in a sample space s then we have recall en are pairwise disjoint iff ei ej for i j n s we can prove this theorem using mathematical induction how can we incorporate prior knowledge sometimes we want to know the probability of some event given that another event has occurred example a fair coin is flipped three times the first flip turns up tails given this information what is the probability that an odd number of tails appear in the three flips solution let f the first flip of three comes up tails let e tails comes up an odd number of times in three flips since f has happened s is reduced to thh tht tth ttt by laplace definition of probability we know p e e s thh ttt thh tht tth ttt conditional probability definition let e and f be events with p f the conditional probability of e given f denoted p e f is defined as intuition think of the event f as reducing the sample space that can be considered the numerator looks at the likelihood of the outcomes in e that overlap those in f the denominator accounts for the reduction in sample size indicated by our prior knowledge that f has occurred bit strings example suppose that a bit string of length is generated at random so that each of the possible bit strings is equally likely to occur what is the probability that it contains at least two consecutive given that the first bit in the string is a solution let e a bit string has at least two consecutive zeros let f the first bit of a bit string is a zero want to calculate p e f p e f p f e f so p e f since each bit string is equally likely to occur p f so p e f kids example what is the conditional probability that a family with two kids has two boys given that they have at least one boy assume that each of the possibilities bb bg gb gg is equally likely to occur boy is older girl is older solution let e a family with kids has boys e bb let f a family with kids has at least boy f bb bg gb e f bb so p e f p e f p f does prior knowledge always help us example suppose a fair coin is flipped twice does knowing that the coin comes up tails on the first flip help you predict whether the coin will be tails on the second flip solution s hh ht th tt f coin was tails on the first flip th tt e coin is tails on the second flip tt ht p e p e f p e f p f knowing the first flip does not help you guess the second flip independent events definition we say that events e and f are independent if and only if p e f p e p f recall in our last example s hh ht th tt f th tt e ht tt e f tt this checks out so p e f e f s p e p f example bit strings example suppose that e is the event that a randomly generated bit string of length four begins with a and f is the event that this bit string contains an even number of are e and f independent if all bit strings are equally likely to occur solution by the product rule s e f 1010 so p e p f p e p f e f 1010 p e f since p e f p e p f e and f are independent events example distribution of kids example assume that each of the four ways that a family can have two children are equally likely are the events e that a family with two children has two boys and f that a family with two children has at least one boy independent solution e bb f bb bg gb p e p f p e p f e f bb p e f since e and f are not independent if probabilities are independent we can use the product rule to determine the probabilities of combinations of events example what is the probability of flipping heads times in a row using a fair coin answer p h so p hhhh example what is the probability of rolling the same number times in a row using an unbiased sided die answer first roll agrees with itself with probability roll agrees with first with probability roll agrees with first two with probability so probability of rolling the same number times is group work problem what is the conditional probability that a randomly generated bit string contains two consecutive give that the first bit of the string is a problem what is the conditional probability that exactly heads appear when a fair coin is flipped four times given that the first flip came up heads some experiments only have two outcomes p x coin flips heads or tails bit strings or predicates t or f these types of experiments are called bernoulli trials two outcomes success probability p failure probability q p many problems can be solved by examining the probability of k successes in an experiment consisting of mutually independent bernoulli trials example coin flips example a coin is biased so that the probability of heads is what is the probability that exactly four heads come up when the coin is flipped seven times assuming that each flip is independent solution possible outcomes for seven flips there are c ways that heads can be flipped four times since each flip is independent the probability of each of these outcomes is so the probability of exactly heads occurring in flips of this biased coin is c choose outcomes to make heads probability of each tails combined using product rule probability of each heads combined using product rule this general reasoning provides us with a nice formula theorem the probability of exactly k successes in n independent bernoulli trials with probability of success p and probability of failure q p is c n k pkqn k proof the outcome of n bernoulli trials is an n tuple tn each ti is either s for success or f for failure c n k ways to choose k tis to label s since each trial is independent the probability of each outcome with k sucesses and n k failures is pkqn k so the probability of exactly k successes is c n k pkqn k notation we denote the probability of k successes in n independent bernoulli trials with probability of success p as b k n p bits again example suppose that the probability that a bit is generated is that the probability that a bit is generated is and that bits are generated independently what is the probability that exactly eight bits are generated when ten random bits are generated solution number of trials number of successes probability of success probability of failure want to compute b k c many probability questions are concerned with some numerical value associated with an experiment number of boys in a family number of bits generated beats per minute of a heart longevity of a chicken number of heads flips what is a random variable definition a random variable is a function from the sample space of an experiment to the set of real numbers r that is a random variable assigns a real number to each possible outcome note x is not a variable and is not random x is a function example suppose that a coin is flipped three times let x be the random variable that equals the numbers of heads that appear when is the outcome then x takes the following values x hhh x hht x hth x thh x tth x tht x htt x ttt random variables and distributions definition the distribution of a random variable x on a sample space s is the set of pairs r p x r for all r x s where p x r is the probability that x takes the value r note a distribution is usually described by specifying p x r for each r x s example assume that our coin flips from the previous slide were all equally likely to occur we then get the following distribution for the random variable x p x p x p x p x example rolling dice let x be the sum of the numbers that appear when a pair of fair dice is rolled what are the values of this random variable for the possible outcomes i j where i and j are the numbers that appear on the first and second die respectively answer x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x sometimes probabilistic reasoning can lead us to some interesting and unexpected conclusions question how many people need to be in the same room so that the probability of two people sharing the same birthday is greater than assumptions there are possible birthdays all birthdays are equally likely to occur birthdays are independent solution tactic find the probability pn that the n people in a room all have different birthdays then compute pn which is the probability that at least two people share the same birthday let figure this out let assess probabilities as people enter the room person clearly doesn t have the same birthday as anyone else in the room has a different birthday than with probability has a different birthday than and with probability in general pj has a different birthday than pj with probability j j recall that pn is the probability that n people in the room all have different birthdays using our above observations this means but we re interested in pn to check the minimum number of people need in the room to ensure that pn we ll use trial and error if n then pn if n then pn so you need only people in a room to have a better than chance that two people share the same birthday group work problem what is the probability that exactly heads occur when a fair coin is flipped times problem consider a game between alice and bob over time alice has been shown to win this game of the time if alice and bob play games in a row what is the probability that alice wins every game final thoughts today we covered conditional probability independence bernoulli trials random variables probabilistic analysis next time bayes theorem section discrete structures for computer science adam j lee sennott square lecture bayes theorem conditional probability definition let e and f be events with p f the conditional probability of e given f denoted p e f is defined as intuition think of the event f as reducing the sample space that can be considered the numerator looks at the likelihood of the outcomes in e that overlap those in f the denominator accounts for the reduction in sample size indicated by our prior knowledge that f has occurred bayes theorem bayes theorem allows us to relate the conditional and marginal probabilities of two random events in english bayes theorem will help us assess the probability that an event occurred given only partial evidence doesn t our formula for conditional probability do this already we can t always use this formula directly a motivating example suppose that a certain opium test correctly identifies a person who uses opiates as testing positive of the time and will correctly identify a non user as testing negative of the time if a company suspects that of its employees are opium users what is the probability that an employee that tests positive for this drug is actually a user question can we use our simple conditional probability formula x is a user x tested positive the foot view in situations like those on the last slide bayes theorem can help essentially bayes theorem will allow us to calculate p e f assuming that we know or can derive p e p f e p f e probability that x is a user test success rate probability that x is an opium test false positive rate user given a positive test returning to our earlier example let e person x is an opium user let f person x tested positive for opium it looks like bayes theorem could help in this case new notation today we will use the notation ec to denote the complementary event of e that is e ec a simple example we have two boxes the first contains two green balls and seven red balls the second contains four green balls and three red balls bob selects a ball by first choosing a box at random he then selects one of the balls from that box at random if bob has selected a red ball what is the probability that he took it from the first box picking the problem apart first let define a few events relevant to this problem let e bob has chosen a red ball by definition ec bob has chosen a green ball let f bob chose his ball from this first box therefore fc bob chose his ball from the second box we want to find the probability that bob chose from the first box given that he picked a red ball that is we want p f e goal given that p f e p f e p e use what we know to derive p f e and p e what do we know we have two boxes the first contains two green balls and seven red balls the second contains four green balls and three red balls bob selects a ball by first choosing a box at random he then selects one of the balls from that box at random if bob has selected a red ball what is the probability that he took it from the first box statement bob selects a ball by first choosing a box at random bob is equally likely to choose the first box or the second box p f p fc statement the first contains two green balls and seven red balls the first box has nine balls seven of which are red p e f statement the second contains four green balls and three red balls the second box contains seven balls three of which are red p e fc now for a little algebra the end goal compute p f e p f e p e note that p e f p e f p f if we multiply by p f we get p e f p e f p f further we know that p e f and p f so p e f similarly p e fc p e fc p fc observation e e f e fc this means that p e p e f p e fc 126 denouement the end goal compute p f e p f e p e so p f e 63 how did we get here extract what we could from the problem definition itself rearrange terms to derive p f e and p e use our trusty definition of conditional probability to do the rest the reasoning that we used in the last problem essentially derives bayes theorem for us bayes theorem suppose that e and f are events from some sample space s such that p e and p f then proof the definition of conditional probability says that p f e p f e p e p e f p e f p f this means that p e f p f e p e p e f p e f p f so p f e p e p e f p f therefore p f e p e f p f p e proof continued note to finish we must prove p e p e f p f p e fc p fc observe that e e s e f fc e f e fc note also that e f and e fc are disjoint i e no x can be in both f and fc this means that p e p e f p e fc we already have shown that p e f p e f p f further since p e fc p e fc p fc we have that p e fc p e fc p fc so p e p e f p e fc p e f p f p e fc p fc putting everything together we get and why is this useful in a nutshell bayes thereom is useful if you want to find p f e but you don t know p e f or p e here a general solution tactic step identify the independent events that are being investigated for example f bob chooses the first box fc bob chooses the second box e bob chooses a red ball ec bob chooses a green ball step record the probabilities identified in the problem statement for example p f p fc p e f p e fc step plug into bayes formula and solve example pants and skirts suppose there is a co ed school having boys and girls as students the girl students wear trousers or skirts in equal numbers the boys all wear trousers an observer sees a random student from a distance all they can see is that this student is wearing trousers what is the probability this student is a girl step set up events e x is wearing pants ec x is wearing a skirt f x is a girl fc x is a boy step extract probabilities from problem definition p f p fc p e f p ec f p e fc pants and skirts continued step plug in to bayes theorem p f e conclusion there is a chance that the person seen was a girl given that they were wearing pants drug screening revisited suppose that a certain opium test correctly identifies a person who uses opiates as testing positive of the time and will correctly identify a non user as testing negative of the time if a company suspects that of its employees are opium users what is the probability that an employee that tests positive for this drug is actually a user step set up events f x is an opium user fc x is not an opium user e x tests positive for opiates ec x tests negative for opiates step extract probabilities from problem definition p f p fc p e f p e fc drug screening continued step plug in to bayes theorem p f e conclusion if an employee tests positive for opiate use there is only a chance that they are actually an opium user group work suppose that person in has a particular rare disease a diagnostic test is correct of the time when given to someone with the disease and is correct of the time when given to someone without the disease problem calculate the probability that someone who tests positive for the disease actually has it problem calculate the probability that someone who tests negative for the disease does not have the disease application spam filtering definition spam is unsolicited bulk email i didn t ask for it i probably don t want it sent to lots of people in recent years spam has become increasingly problematic for example in spam accounted for of all email messages sent to combat this problem people have developed spam filters based on bayes theorem how does a bayesian spam filter work essentially these filters determine the probability that a message is spam given that it contains certain keywords message contains questionable keyword in the above equation p e f probability that our keyword occurs in spam messages p e fc probability that our keyword occurs in legitimate messages p f probability that an arbitrary message is spam p fc probability that an arbitrary message is legitimate question how do we derive these parameters we can learn these parameters by examining historical email traces imagine that we have a corpus of email messages we can ask a few intelligent questions to learn the parameters of our bayesian filter how many of these messages do we consider spam in the spam messages how often does our keyword appear in the good messages how often does our keyword appear aside this is what happens every time you click the mark as spam button in your email client given this information we can apply bayes theorem filtering spam using a single keyword suppose that the keyword rolex occurs in of known spam messages and in of known good messages estimate the probability that an incoming message containing the word rolex is spam assuming that it is equally likely that an incoming message is spam or not spam if our threshold for classifying a message as spam is will we reject this message step define events f message is spam fc message is good e message contains the keyword rolex ec message does not contain the keyword rolex step gather probabilities from the problem statement p f p fc p e f p e fc spam rolexes continued step plug in to bayes theorem p f e 005 conclusion since the probability that our message is spam given that it contains the string rolex is approximately we will discard the message problems with this simple filter how would you choose a single keyword phrase to use all natural nigeria click here users get upset if false positives occur i e if legitimate messages are incorrectly classified as spam when was the last time you checked your spam folder how can we fix this choose keywords t p spam keyword is very high or very low filter based on multiple keywords specifically we want to develop a bayesian filter that tells us p f first some assumptions events and are independent the events s and s are independent p f p fc now let derive formula for this p f by bayes theorem assumption assumptions and spam filtering on two keywords suppose that we train a bayesian spam filter on a set of spam messages and messages that are not spam the word stock appears in spam messages and good messages and the word undervalued appears in spam messages and good messages estimate the probability that a message containing the words stock and undervalued is spam will we reject this message if our spam threshold is set at step set up events f message is spam fc message is good message contains the word stock message contains the word undervalued step identify probabilities p f p fc p f p fc two keywords continued p f n p f p f p f p f p f c p f c step plug in to bayes theorem p f 06 02 conclusion since the probability that our message is spam given that it contains the strings stock and undervalued is 9302 we will reject this message final thoughts conditional probability is very useful bayes theorem helps us assess conditional probabilities has a range of important applications next time expected values and variance section discrete structures for computer science adam j lee sennott square lecture expected value many probability questions are concerned with some numerical value associated with an experiment number of boys in a family number of bits generated beats per minute of a heart longevity of a chicken number of heads flips what is a random variable definition a random variable is a function from the sample space of an experiment to the set of real numbers r that is a random variable assigns a real number to each possible outcome note x is not a variable and is not random x is a function example suppose that a coin is flipped three times let x be the random variable that equals the numbers of heads that appear when is the outcome then x takes the following values l x hhh l x hht x hth x thh l x tth x tht x htt l x ttt random variables and distributions definition the distribution of a random variable x on a sample space s is the set of pairs r p x r for all r x s where p x r is the probability that x takes the value r note a distribution is usually described by specifying p x r for each r x s example assume that our coin flips from the previous slide were all equally likely to occur we then get the following distribution for the random variable x l p x l p x l p x l p x many times we want to study the expected value of a random variable definition the expected value or expectation of a random variable x on the sample space s is equal to e x p x s for every outcome use the probability of that outcome occuring to weight the value of the random variable for that outcome note the expected value of a random variable defined on an infinite sample space is defined iff the infinte series in the definition is absolutely convergent a roll of the dice example let x be the number that comes up when a die is rolled what is the expected value of x solution l possible outcomes l each outcomes occurs with the probability l e x l l a flip of the coin example a fair coin is flipped three times let s be the sample space of the eight possible outcomes and x be the random variable that assigns to an outcome the number of heads in that outcome what is the expected value of x solution l since coin flips are independent each outcome is equally likely l e x x hhh x hht x hth x thh x tth x tht x htt x ttt l l l if s is large the definition of expected value can be difficult to use directly definition if x is a random variable and p x r is the probability that x r i e p x r s x r p then e x rex s p x r r each value of x is weighted by its probability of occurrence proof l suppose that x is a random variable ranging over s l note that p x r is the probability that x takes the value r l this means that p x r is the sum of the probabilities of the outcomes s such that x r l it thus follows that e x rex s p x r r rolling two dice example let x be the sum of the numbers that appear when a pair of fair dice is rolled what is the expected value of x recall from last week so we have that l e x 9 l we can apply this formula to reason about bernoulli trials theorem the expected number of successes when n independent bernoulli trials are performed in which p is the probability of success is np proof l let x be a random variable equal to the number of successes in n trials l we know from last week that p x k c n k pkqn k so n l e x kp x k k definition of e x n l kc n k pkqn k k probability of k successes in n trials n l nc n k pkqn k k lemma kc n k nc n k proof continued n l np c n k pk k k n l np c n j p q j factor out np from each term shift index j k l np p q n l np binomial theorem p q note as long as we can prove that kc n k nc n k the theorem has been proved since we have shown that np is the expected number of successes in n independent bernoulli trials proof of lemma lemma kc n k nc n k proof l kc n k k n k n k definition of c n k l n k n k cancel out k term l n n k n k factor out n from numerator l n n k n k n k n k l nc n k expected values are linear theorem if xn are random variables on s and if a and b are real numbers then e xn e e e xn e ax b ae x b proof dice revisited example what is the expected value of the sum of the numbers that appear when two fair dice are rolled solution l let and be random variables indicating the value on the first and second die respectively l want to calculate e l by the previous theorem we have that e e e l from earlier in lecture we know that e e l so e note this agrees with the more complicated calculation that we made ealier in lecture linearity is useful in general observation we can formulate many hard problems in terms of the sum of much easier problems the forgetful coat check clerk example a careless coat check clerk takes the coats of n people at a restaurant but forgets to attach the claim number to each coat when customers return for their coats the clerk simply returns a coat at random what is the expected number of coats returned correctly solution l let x be the random variable that equals the number of people who get back the correct coat l let xi be the random variable with xi if person i gets the correct coat back and xi otherwise l it follows that x xn l since any coat can be returned to any person the probability that person i gets back the right coat is n l so e xi p xi p xi n n l by the linearity of expectations we have that l e x e e e xn l n n n l expected number of inversions in a permutation definition the ordered pair j k is called an inversion in a permutation if j k but k precedes j in the permutation for example the permutation contains two inversions namely and example what is the expected number of inversions in a permutation of the first n positive integers solution l let x be a rv equal to the number of inversions in a permutation l let ij k be a rv t ij k if j k is an inversion and ij k otherwise l it follows that e x j k n e ij k l note in any permutatation it is equally likely for j to precede k as it is for k to precede j l as a result we have that e ij k for all j k l since there c n pairs j k with j k n we have that l e x j k n e ij k l c n l n n independent random variables definition random variables x and y on a sample space s are independent if p x and y p x p y for all real numbers and example let and be two random variables that take the values of two dice rolled are these two rvs independent solution l s l let i j s l note that there are equally likely possible outcomes for two dice l so p i and j l note also that p i p j l so p i p j l as a result we can conclude that and are independent not all random variables are independent example let and be random variables defined as before show that and x are not independent solution l note that p and x why means that is impossible l note also that p l further p x l this means that p p x p and x l by definition and x are not independent one last result theorem if x and y are independent random variables on a sample space s then e xy e x e y note this theorem only works if x and y are independent for example let x and y be two random variables that count the number of heads and tails respectively when a coin is flipped twice clearly x and y are not independent we know that p x p y p x p y and p x p y so e x e y now note that xy if either two heads or two tails are flipped and xy when one head and one tail come up this gives us that e xy however e x e y so e xy e x e y group work problem consider a die in which the number is two times as likely to be rolled as any other number what is the expected value of this die problem alice and bob regularly play chess together historically alice wins of the time if alice and bob play games of chess how many games can alice be expected to win problem a test contains t f questions each worth two points and multiple choice questions each worth four points the probability that alice answers and t f question correctly is 9 the probability that alice answers a multiple choice question correctly is what is her expected score on the final sometimes we need more information than the expected value can give us the expected value of a random variable doesn t tell us the whole story p x r p x r x x p x r x the variance of a random variable gives us information about how wide it is spread definition the variance of a random variable x on a sample space s is defined as v x x e x s squared difference from expected value weighted by probability of occurrence definition the standard deviation of a random variable x on a sample space s is defined as v s variance the short form theorem if x is a random variable on a sample space s then v x e e x proof l v x s x e x l s x x s x p e x s p l e x e x e x l e e x variance of a bernoulli distribution example what is the variance of random variable x with x t if a bernoulli trial is a success and x t otherwise assume that the probability of success is p solution l note that x takes only the values and l hence x t t l v x e e x l p l p p l pq this tells us that the variance of any bernoulli distribution is pq rolling dice example two dice are rolled what is the variance of the random variable x j k where j is the number appearing on the first die and k is the number appearing on the second die solution l v x e e x l note that p x k for k and is otherwise l e x 6 l e 22 102 l so v x group work problem find the variance of a random variable x where x is the number that comes up when a single die is rolled the variance of independent random variables theorem let x and y be to independent random variables on a sample space s then v x y v x v y more generally if xn are pairwise independent random variables on a sample space s then v xn v v v xn proof variable case l v x y e x y e x y l e e x e y since x and y are independent l e xy e e x x e y e y l e e x e e y x e y x e y l e e x e e y l v x v y one final roll of the dice example the x be a random variable whose value is the sum that appears when two dice are rolled what is v x solution l let and be random variables taking the value that appears on the first and second die respectively l so x l note that and are independent l v x v l l l group work problem what is the variance of the number of successes when n independent bernoulli trials are performed where p is the probability of success for each trial hint let xi tn be a random variable such that xi if ti was a success and xi otherwise final thoughts n analyzing the expected value of a random variable allows us to answer a range of interesting questions n the variance of a random variable tells us about the spread of values that the random variable can take discrete structures for computer science adam j lee sennott square lecture relations and representations binary relations establish a relationship between elements of two sets definition let a and b be two sets a binary relation from a to b is a subset of a b in other words a binary relation r is a set of ordered pairs ai bi where ai a and bi b notation we say that a r b if a b r a r b if a b r example course enrollments let say that alice and bob are taking cs alice is also taking math furthermore charlie is taking art and business define a relation r that represents the relationship between people and classes solution let the set p denote people so p alice bob charlie let the set c denote classes so c cs math art business by definition r p c from the above statement we know that alice cs r bob cs r alice math r charlie art r charlie business r so r alice cs bob cs alice math charlie art charlie business a relation can also be represented as a graph let say that alice and bob are taking cs alice is also taking math furthermore charlie is taking art and business define a relation r that represents the relationship between people and classes alice alice cs r art bob business charlie cs math elements of p i e people elements of c i e classes a relation can also be represented as a table let say that alice and bob are taking cs alice is also taking math furthermore charlie is taking art and business define a relation r that represents the relationship between people and classes name of the relation bob cs r elements of c i e courses elements of p i e people wait doesn t this mean that relations are the same as functions not quite recall the following definition from lecture definition let a and b be nonempty sets a function f is an assignment of exactly one element of set b to each element of set a this would mean that e g a person only be enrolled in one course reconciling this with our definition of a relation we see that every function is also a relation not every relation is a function let see some quick examples short and sweet consider f s g clearly a function can also be represented as the relation r anna c brian a christine a anna brian christine f s g a b c d f consider the set r a a clearly a relation cannot be represented as a function r a we can also define binary relations on a single set definition a relation on the set a is a relation from a to a that is a relation on the set a is a subset of a a example let a be the set which ordered pairs are in the relation r a b a divides b solution divides everything divides itself and divides itself divides itself so r representing the last example as a graph example let a be the set which ordered pairs are in the relation r a b a divides b tell me what you know question which of the following relations contain each of the pairs and a b a b a b a b a b a b or a b a b a b a b a b a b a b answer these are all relations on an infinite set properties of relations definition a relation r on a set a is reflexive if a a r for every a a note our divides relation on the set a is reflexive every a a divides itself properties of relations definition a relation r on a set a is symmetric if b a r whenever a b r for every a b a if r is a relation in which a b r and b a r implies that a b we say that r is antisymmetric mathematically symmetric a b a b r b a r antisymmetric a b a b r b a r a b examples symmetric r antisymmetric r symmetric and antisymmetric relations r r symmetric relation diagonal axis of symmetry not all elements on the axis of symmetry need to be included in the relation antisymmetric relation no axis of symmetry only symmetry occurs on diagonal not all elements on the diagonal need to be included in the relation properties of relations definition a relation r on a set a is transitive if whenver a b r and b c r then a c r for every a b c a note our divides relation on the set a is transitive divides divides this isn t terribly interesting but it is transitive nonetheless more common transitive relations include equality and comparison operators like and examples redux question which of the following relations are reflexive symmetric antisymmetric and or transitive a b a b a b a b a b a b or a b a b a b a b a b r6 a b a b answer relations can be combined using set operations example let r be the relation that pairs students with courses that they have taken let s be the relation that pairs students with courses that they need to graduate what do the relations r s r s and s r represent solution r s all pairs a b where student a has taken course b or student a needs to take course b to graduate r s all pairs a b where student a has taken course b and student a needs course b to graduate s r all pairs a b where student a needs to take course b to graduate but student a has not yet taken course b relations can be combined using functional composition definition let r be a relation from the set a to the set b and s be a relation from the set b to the set c the composite of r and s is the relation of ordered pairs a c where a a and c c for which there exists an element b b such that a b r and b c s we denote the composite of r and s by r º s example what is the composite relation of r and s r r s s so r º s group work problem list the ordered pairs of the relation r from a to b where a b r iff a b 4 problem draw the graph and table representations of the above relation problem is the above relation reflexive symmetric antisymmetric and or transitive final thoughts relations allow us to represent and reason about the relationships between sets relations are more general than functions relations are use all over mathematical operators bindings between sets of objects etc next time n ary relations discrete structures for computer science adam j lee sennott square lecture n ary relations today quick binary relation recap n ary relations l definitions l cs application relational dbms binary relations establish a relationship between elements of two sets definition let a and b be two sets a binary relation from a to b is a subset of a b in other words a binary relation r is a set of ordered pairs ai bi where ai a and bi b notation we say that l a r b if a b r l a r b if a b r example course enrollments let say that alice and bob are taking cs alice is also taking math furthermore charlie is taking art and business define a relation r that represents the relationship between people and classes solution l let the set p denote people so p alice bob charlie l let the set c denote classes so c cs math art business l by definition r p c l from the above statement we know that alice cs r bob cs r alice math r charlie art r charlie business r l so r alice cs bob cs alice math charlie art charlie business a relation can also be represented as a graph let say that alice and bob are taking cs alice is also taking math furthermore charlie is taking art and business define a relation r that represents the relationship between people and classes alice alice cs r art bob business charlie cs math elements of p i e people elements of c i e classes a relation can also be represented as a table let say that alice and bob are taking cs alice is also taking math furthermore charlie is taking art and business define a relation r that represents the relationship between people and classes name of the relation bob cs r elements of c i e courses elements of p i e people we can also relate elements of more than two sets definition let an be sets an n ary relation on these sets is a subset of an the sets an are called the domains of the relation and n is its degree example let r be the relation on z z z consisting of triples a b m where a b mod m l what is the degree of this relation l what are the domains of this relation l are the following tuples in this relation n ary relations are the basis of relational database management systems data is stored in relations a k a tables columns of a table represent the attributes of a relation rows or records contain the actual data defining the relation operations on an rdbms are formally defined in terms of a relational algebra relational algebra gives a formal semantics to the operations performed on a database by rigorously defining these operations in terms of manipulations on sets of tuples i e records operators in relational algebra include l selection l projection l rename l join equijoin left outer join right outer join l aggregation the selection operator allows us to filter the rows in a table definition let r be an n ary relation and let c be a condition that elements in r must satisfy the selection sc maps the n ary relation r to the n ary relation of all n tuples from r that satisfy the condition c example consider the students relation from earlier in lecture let the condition be major cs and let be gpa what is the result of students answer l alice cs l charlie cs the projection operator allows us to consider only a subset of the columns of a table definition the projection im maps the n tuple an to the m tuple aim where m n example what is the result of applying the projection to the students table the equijoin operator allows us to create a new table based on data from two or more related tables definition let r be a relation of degree m and s be a relation of degree n the equijoin ik jk where k m and k n creates a new relation of degree m n k containing the subset of s r in which sik rjk and duplicate columns are removed via projection example what is the result of the equijoin on the students and enrollment tables what is the result of the equijoin on the students and enrollment tables sql queries correspond to statements in relational algebra select name id from students where major cs and gpa select is actually a projection in this case p the where clause lets us filter i e smajor cs gpa sql an equijoin example select name id major gpa course from students enrollment where id why are n ary relations and relational algebra interesting reason formal representation of db state reason efficient way to process sql queries l parse tokenize sql query l compile into a tree of relational operators l optimize tree for efficient execution l execute plan and return results example assume table t has tuples with two attributes a and b ten of these tuples have a while the other have a query select b from t where a ø parse sa t ø parse sa t group work problem what is students problem what relational operators would you use to generate a table containing only the names of math and cs majors with a gpa problem write an sql statement corresponding to the solution to problem logic programming allows us to manipulate data logically prolog l programming in logic l developed in the for ai purposes datalog l logical formalization of databases l developed in the for our purposes we can consider prolog and datalog to be the same though in reality they have very important differences two main constructs l facts instructor lee student smith l rules lower case constant upper case variable teaches p s instructor p c student s c lecture rules and facts define predicates facts define predicates by explicitly listing elements that satisfy those predicates l prof lee is the instructor for instructor lee rules define predicates by combining previously specified predicates l professors teach the students enrolled in the courses for which they are the instructor teaches p s instructor p c student s c prolog is an environment that lets us issue queries to determine which predicates are true today rules and facts define relations facts define relations by explicitly listing tuples l prof lee is the instructor for instructor lee l relation instructor name course rules define relations by combining other relations l professors teach the students enrolled in the courses for which they are the instructor teaches p s instructor p c student s c joins the instructor and student tables by course then project the name column from each let revisit our earlier example students alice cs students bob math 23 students charlie cs students denise art 4 enrollment enrollment enrollment enrollment sql prolog a projection example example what is the result of applying the projection to the students table sql select name major from students prolog proj n m students n i m g sql prolog a selection example example consider the students relation from earlier in lecture let the condition be major cs and let be gpa what is the result of students answer l alice cs 45 l charlie cs sql select from students where major cs and gpa prolog sel n i m g students n i m g m cs g sql prolog an equijoin example sql select name id major gpa course from students enrollment where id prolog join n i m g c students n i m g enrollment s c i s final thoughts relations allow us to represent and reason about the relationships between sets in a more general way than functions did without n ary relations dbms systems would not exist do you find this stuff interesting l cs introduction to database systems l cs introduction to artificial intelligence next time wrap up discrete structures for computer science adam j lee sennott square lecture logic puzzles and propositional equivalence january homework is posted check the homework section of the web page the rules staple a copy of the cover sheet to your assignment only talk to other students about clarifying what questions are asking developing general solution techniques identify your collaborators do your own work due before class next wednesday general comments slides on the web are incomplete provided for those who want to follow along note that simply filling in the blanks isn t good enough please ask questions if i m talking too fast let me know if you get confused ask for clarification some of your classmates probably have the same question i can t read minds today topics logic puzzles propositional equivalences a technical support conundrum alice and bob are technical support agents if an agent is having a bad day he or she will always lie to you if an agent is having a good day he or she will always tell you the truth alice tells you that bob is having a bad day bob tells you that he and alice are both having the same type of day can you trust the advice you receive from alice during your call solving logic puzzles is easy step identify rules and constraints step assign propositions to key concepts step make assumptions and reason logically technical support revisited alice and bob are technical support agents if an agent is having a bad day he or she will always lie to you if an agent is having a good day he or she will always tell you the truth alice tells you that bob is having a bad day bob tells you that he and alice are both having the same type of day can you trust the advice you receive from alice during your call step identify the rules of the puzzle step assign propositions to the key concepts in the puzzle step make assumptions and reason logically another example consider a group of friends frank anna and chris if frank is not the oldest then anna is if anna is not the youngest then chris is the oldest determine the relative ages of frank anna and chris propositions rules step make assumptions and reason logically sometimes no solution is a solution alice and bob are technical support agents alice says i am having a good day bob says i am having a good day can you trust either alice or bob step identify rules step assign propositions step make assumptions and reason logically group work problem alice and bob are technical support agents working to fix your computer alice tells you that bob is having a bad day today and that you should expect a long wait before your computer is fixed bob tells you not to worry alice is just having a bad day your computer will be ready in no time question can you draw any conclusions about when your computer will be fixed if so what can you learn propositional equivalences preliminaries definition a tautology is a compound proposition that is always true regardless of the truth values of the propositions occurring within it definition a contradiction is a compound proposition that is always false regardless of the truth values of the propositions occurring within it definition a contingency is a compound proposition whose truth value is dependent on the propositions occurring within it examples are the following compound propositions tautologies contradictions or contingencies p p p p p q tautology contradiction contingency what are logical equivalences and why are they useful definition compound propositions p and q are logically equivalent if p q is a tautology the notation p q means that p and q are logically equivalent logical equivalences are extremely useful aid in the construction of proofs allow us to simplify compound propositions example p q p q how do we prove this type of statement it is easy to prove propositional equivalences we can prove simple logical equivalences using our good friend the truth table prove p q p q demorgan laws allow us to distribute negation over compound propositions two laws p q p q p q p q prove p q p q if p or q isn t true then neither p nor q is true if p and q isn t true then at least one of p or q is false using demorgan laws use demorgan laws to negate the following expressions bob is wearing blue pants and a sweatshirt b b b bob is not wearing blue pants or is not wearing a sweatshirt i will drive or i will walk d w d w d w i will not drive and i will not walk group work problem prove that p q and p q are logically equivalent i e p q p q this is the second demorgan law problem use demorgan laws to negate the following propositions today i will go running or ride my bike tom likes both pizza and beer sometimes using truth tables to prove logical equivalencies can become cumbersome recall that for an equivalence with n propositions we need to build a truth table with rows fine for tables with n 3 or consider n we would need 824 rows in the truth table another option direct manipulation of compound propositions using known logical equivalencies there are many useful logical equivalences more useful logical equivalences more equivalencies in the book prove p q p r p q r prove that p q p q is a tautology final thoughts logic can help us solve real world problems and play challenging games logical equivalences help us simplify complex propositions and construct proofs more on proofs later in the course next time predicate logic and quantification please read section 3 discrete structures for computer science adam j lee sennott square lecture predicates and quantifiers january announcements be sure that you are attending the correct recitation homework is due homework will be posted on monday topics predicates and quantifiers today topics predicates quantifiers logical equivalences in predicate logic translations using quantifiers propositional logic is simple therefore limited propositional logic cannot represent some classes of natural language statements given kody is one of my dogs given all of my dogs like peanut butter propositional logic gives us no way to draw the obvious conclusion that kody likes peanut butter propositional logic also limits the mathematical truths that we can express and reason about consider the following has no divisors other than and itself has no divisors other than and itself has no divisors other than and itself has no divisors other than and itself 11 has no divisors other than and itself this is an inefficient way to reason about the properties of prime numbers general problem propositional logic has no way of reasoning about instances of general statements historical context the previous examples are called syllogisms aristotle used syllogisms in his prior analytics to deductively infer new facts from existing knowledge major premise all men are mortal socrates is a man minor premise socrates is mortal conclusion predicate logic allows us to reason about the properties of individual objects and classes of objects predicate logic allows us to use propositional functions during our logical reasoning p x variable predicate note a propositional function p x has no truth value unless it is evaluated for a given x or set of xs examples assume p x what are the truth values of the following expressions p p p we can express the prime number property using predicate logic predicates can also be defined on more than one variable let p x y x y what are the truth values of the following expressions p p p let s x y z x y z what are the truth values of the following expressions s s 23 s 18 predicates play a central role in program control flow and debugging if then statements if x then y loops while y do end while debugging in c c assert strlen passwd this is a predicate quantifiers allow us to make general statements that turn propositional functions into propositions in english we use quantifiers on a regular basis all students can ride the bus for free many people like chocolate i enjoy some types of tea at least one person will sleep through their final exam quantifiers require us to define a universe of discourse also called a domain in order for the quantification to make sense many like chocolate doesn t make sense what are the universes of discourse for the above statements universal quantification allows us to make statements about the entire universe of discourse examples all of my dogs like peanut butter every even integer is a multiple of two for each integer x x given a propositional function p x we express the universal quantification of p x as x p x what is the truth value of x p x examples all rational numbers are greater than if an natural number is prime it has no divisors other than and itself existential quantifiers allow us to make statements about some objects examples some elephants are scared of mice there exist integers a b and c such that the equality is true there is at least one person who did better than john on the midterm given a propositional function p x we express the existential quantification of p x as x p x what is the truth value of x p x examples the inequality x x holds for at least one integer for some integers the equality is true we can restrict the domain of quantification the square of every natural number less than is no more than domain natural numbers statement x truth value true this is equivalent to writing x x some integers between and are prime domain integers propositional function p x x is prime statement x p x truth value true this is equivalent to writing x x p x precedence of quantifiers the universal and existential quantifiers have the highest precedence of all logical operators for example x p x q x actually means x p x q x x p x q x actually means x p x q x for the most part we will use parentheses to disambiguate these types of statements but you are still responsible for understanding precedence group work problem assume m x x is a monday and d x y y what are the truth values of the following statements m january d problem let p x x is prime where the domain of x is the integers let t x y x where the domain of x and y is all natural numbers what are the truth values of the following statements x p x x y t x y we can extend the notion of logical equivalence to expressions containing predicates or quantifiers definition two statements involving predicates and quantifiers are logically equivalent iff they take on the same truth value regardless of which predicates are substituted into these statements and which domains of discourse are used prove x p x q x x p x x q x we also have demorgan laws for quantifiers negation over universal quantifier x p x x p x negation over existential quantifer x p x x p x these are very useful logical equivalences so let prove one of them prove x p x x p x translations from english to translate english sentences into logical expressions rewrite the sentence to make it easier to translate determine the appropriate quantifiers to use look for words that indicate logical operators formalize sentence fragments put it all together example at least one person in this classroom is named adam and has lived in pittsburgh for years existential quantifier rewrite there exists at least one person who is in this classroom is named adam and has lived in pittsburgh for years conjunction formalize c x x is in this classroom n x x is named adam p x x has lived in pittsburgh for years final expression x c x n x p x example if a student is taking then they have taken high school algebra universal quantifier rewrite for all students if a student is in cs then they have taken high school algebra formalize c x x is taking implication h x x has taken high school algebra final expression x c x h x negate the previous example x c x h x translate back into english there is a student taking that has not taken high school algebra example jane enjoys drinking some types of tea rewrite there exist some types of tea that jane enjoys drinking formalize t x x is a type of tea d x jane enjoys drinking x final expression x t x d x negate the previous example x t x d x group work problem 3 translate the following sentences into logical expressions some cows have black spots at least one student likes to watch football or ice hockey any adult citizen of the us can register to vote if he or she is not a convicted felon problem negate the translated expressions from problem 3 translate these back into english final thoughts the simplicity of propositional logic makes it unsuitable for solving certain types of problems predicate logic makes use of propositional functions to describe properties of objects the universal quantifier to assert properties of all objects within a given domain the existential quantifier to assert properties of some objects within a given domain predicate logic can be used to reason about relationships between objects and classes of objects next lecture applications of predicate logic and nested quantifiers please read section 4 discrete structures for computer science adam j lee sennott square lecture logic programming and nested quantifiers january announcements n homework is out today l topics predicates and quantifiers l start early ask questions today topics n applications of predicate logic n nested quantifiers logic programming enables automated reasoning prolog l programming in logic l developed in the for ai purposes datalog l logical formalization of databases l developed in the for our purposes we can consider prolog and datalog to be the same though in reality they have very important differences two main constructs l facts instructor lee student smith l rules lower case constant upper case variable teaches p s instructor p c student s c rules and facts define predicates facts define predicates by explicitly listing elements that satisfy those predicates l prof lee is the instructor for instructor lee rules define predicates by combining previously specified predicates l professors teach the students enrolled in the courses for which they are the instructor teaches p s instructor p c student s c prolog is an environment that lets us issue queries to determine which predicates are true a security example grant u projector located u role u presenter located u r owns u d d r role bob presenter role carol presenter owns alice owns bob owns carol can bob run the projector l query grant bob projector l solution true who is in room l query location x l solution alice bob carol knowledge base write and evaluate the following queries grant u projector located u role u presenter located u r owns u d d r role bob presenter role carol presenter owns alice owns bob owns carol dev_loc n can alice use the projector l grant alice projector l false n can carol use the projector l grant carol projector l true n which devices does alice own l owns alice x l logic programming is a useful tool artificial intelligence databases route planning security just for grins if you are interested in playing around with logic programming download swi prolog l url this free package is a runtime environment in which you can write logic programs and evaluate queries dave charlie alice bob elise becky frank sarah tommy logic programming demo nested quantifiers many times we need the ability to nest one quantifier within the scope of another quantifier example all integers have an additive inverse that is for any integer x we can choose an integer y such that the sum of x and y is zero there is no way to express this statement using only a single quantifier deciphering nested quantifiers isn t as scary as it looks you just read from left to right x y z x y z for all x there exists a y such that for all z x y z a few more examples x y x y y x this is the commutative law for addition l for all integers x and for all integers y x y y x this is the associative law for addition x y z x y z x y z l for all integers x for all integers y and for all integers z x y z x y z x y x y l there exists an x such that for all y x y since we always read from left to right the order of quantifiers matters clearly true just set y consider x y x y transpose y x x y x not true remember as long as you read from left to right you won t have any problems many mathematical statements can be translated into logical statements with nested quantifiers translating mathematical expressions is often easier than translating english statements steps rewrite statement to make quantification and logical operators more explicit determine the order of in which quantifiers should appear generate logical expression let try a translation universal quantifier statement every real number except zero has a multiplicative inverse x y singular suggestive of an existential quantifier x rewrite for every real number x if x then there exists a real number y such that x y y x y x more examples statement the product of any two negative integers is always positive statement for any real number a it is possible to choose real numbers b and c such that translating quantified statements to english is as easy as reading a sentence let l c x x is enrolled in l m x x has an player l f x y x and y are friends l domain of x and y is all students statement x c x m x y f x y m y for every student x if x is enrolled in then x has an player or there exists another student y such that x and y are friends and y has an player translate the following expressions into english let l o x y x is older than y l f x y x and y are friends l the domain for variables x and y is all students statement x y o x y statement x y f x y z y z f x z group work problem translate the following mathematical statement into predicate logic every even number is a multiple of assume that the predicate e x means x is even l hint what does x is a multiple of mean algebraically problem translate the following expressions into english assume that c x means x has a car f x y means x and y are friends and s x means x is a student l x s x c x y f x y c y l x y z c x f x y c y f x y f y z c z translating from english to a logical expression with nested quantifiers is a little bit more work steps if necessary rewrite the sentence to make quantifiers and logical operations more explicit create propositional functions to express the concepts in the sentence state the domains of the variables in each propositional function determine the order of quantifiers generate logical expression let try an example universal quantifier statement every student has asked at least one professor a question existential quantifier rewrite for every person x if x is a student then there exists a professor whom x has asked a question let l s x x is a student l p x x is a professor l q x y x has asked y a question domains for x and y are all people translation x s x y p y q x y translate the following from english statement there is a man who has tasted every type of beer let translation domain all people domain all drinks domains x all people y all drinks negating expression with nested quantifiers is actually pretty easy you just repeatedly apply demorgan laws x m x y b y t x y a b a b in english for all people x if x is a man then there exists some type beer that x has not tasted alternatively no man has tasted every type of beer negate x s x y p y q x y x s x y p y q x y in english there exists a student x such that for all people y if y is a professor then x has not asked y a question alternatively there exists a student that has never asked any professor a question group work problem translate the following english sentences into predicate logic there is a woman has tried every flavor of ben and jerry ice cream every student has at least one friend that is dating penguins fan if a person is a parent and a man then they are the father of some child problem negate the results from problem and translate the negated expressions back into english final thoughts n logic programming is an interesting application of predicate logic that is used throughout computer science n quantifiers can be nested l nested quantifiers are read left to right l order is important l translation and negation work the same as they did before n next lecture l sets l please read sections discrete structures for computer science adam j lee sennott square lecture january announcements homework is due tuesday topics predicates and quantifiers recitation update please bring your book for reference a few stumbling blocks whether the negation sign is on the inside or the outside of a quantified statement makes a big difference example let t x x is tall consider the following x t x it is not the case that all people are tall x t x for all people x it is not the case that x is tall note x t x x t x x t x recall when we push negation into a quantifier demorgan law says that we need to switch the quantifier a few stumbling blocks let c x x is enrolled in s x x is smart question the following two statements look the same what the difference x c x s x x c x s x there is a smart student in there exists a student x such that if x is in then x is smart subtle note the second statement is true if there exists even one smart person on earth because f t today topics rules of inference what have we learned where are we going propositional logic representation predicate logic refined representation quantifiers generalization inference and proof deriving new knowledge writing valid proofs is a subtle art this is called research step discover and formalize the property that you wish to prove step formalize the ground truths axioms that you will use to prove this property subtle but not terribly difficult step show that the property in question follows from the truth of your axioms this is the hard part what is science without jargon a conjecture is a statement that is thought to be true a proof is a valid argument that establishes the truth of a given statement i e a conjecture after a proof has been found for a given conjecture it becomes a theorem a tale of two proof techniques in a formal proof each step of the proof clearly follows from the postulates and axioms assumed in the conjecture statements that are assumed to be true in an informal proof one step in the proof may consist of multiple derivations portions of the proof may be skipped or assumed correct and axioms may not be explicitly stated how can we formalize an argument consider the following argument if you have an account you can access the network you have an account therefore you can access the network this argument seems valid but how can we demonstrate this formally let analyze the form of our argument p q if you have an account then you can access the network you have an account therefore you can access the network this is called a rule of inference rules of inference are logically valid ways to draw conclusions when constructing a formal proof the previous rule is called modus ponens rule of inference p q p q informally given an implication p q if we know that p is true then q is also true but why can we trust modus ponens tautology p q p q truth table any time that p q and p are both true q is also true there are lots of other rules of inference that we can use addition tautology p p q rule of inference example it is raining now therefore it is raining now or it is snowing now simplification tautology p q p rule of inference example it is cold outside and it is snowing therefore it is cold outside there are lots of other rules of inference that we can use modus tollens tautology q p q p rule of inference example if i am hungry then i will eat i am not eating therefore i am not hungry hypothetical syllogism tautology p q q r p r rule of inference example if i eat a big meal then i feel full if i feel full then i am happy therefore if i eat a big meal then i am happy there are lots of other rules of inference that we can use disjunctive syllogism tautology p p q q rule of inference example either the heat is broken or i have a fever the heat is not broken therefore i have a fever conjunction tautology p q p q rule of inference example jack is tall jack is skinny therefore jack is tall and skinny there are lots of other rules of inference that we can use resolution tautology p q p r q r rule of inference example if it is not raining i will ride my bike if it is raining i will lift weights therefore i will ride my bike or lift weights special cases if r q we get if r f we get we can use rules of inference to build valid arguments if it is raining i will stay inside if am inside stephanie will come over if stephanie comes over and it is a saturday then we will play scrabble today is saturday it is raining let r it is raining i i am inside stephanie will come over c we will play scrabble a it is saturday we can use rules of inference to build valid arguments let r it is raining i i am inside stephanie will come over c we will play scrabble a it is saturday step 4 6 hypotheses we also have rules of inference for statements with quantifiers universal instantiation intuition if we know that p x is true for all x then p c is true for a particular c rule of inference universal generalization intuition if we can show that p c is true for an arbitrary c then we can conclude that p x is true for any x rule of inference note that arbitrary does not mean randomly chosen it means that we cannot make any assumptions about c other than the fact that it comes from the appropriate domain we also have rules of inference for statements with quantifiers existential instantiation intuition if we know that x p x is true then we know that p c is true for some c rule of inference again we cannot make assumptions about c other than the fact that it exists and is from the appropriate domain existential generalization intuition if we can show that p c is true for a particular c then we can conclude that p x is true rule of inference hungry dogs redux given kody is one of my dogs m kody given all of my dogs like peanut butter m x p x 4 reasoning about our class show that the premises a student in this class has not read the book and everyone in this class turned in imply the conclusion someone who turned in has not read the book let premises let reasoning about our class c x x is in this class b x x has read the book t x x turned in steps 4 6 premises x c x b x x c x t x group work problem which rules of inference were used to make the following arguments kangaroos live in australia and are marsupials therefore kangaroos are marsupials linda is an excellent swimmer if linda is an excellent swimmer then she can work as a lifeguard therefore linda can work as a lifeguard problem show that the premises everyone in this discrete math class has taken a course in computer science and melissa is a student in this discrete math class lead to the conclusion melissa has taken a course in computer science final thoughts until today we had look at representing different types of logical statements rules of inference allow us to derive new results by reasoning about known truths next time proof techniques please read section and discrete structures for computer science adam j lee sennott square lecture informal proofs mathematical theorems are often stated in the form of an implication example if x y where x and y are positive real numbers then x y x y x y x y p x y q x y we will discuss three applicable proof methods direct proof proof by contraposition proof by contradiction direct proof in a direct proof we prove p q by showing that if p is true then q must necessarily be true example prove that if n is an odd integer then is an odd integer proof direct proofs are not always the easiest way to prove a given conjecture in this case we can try proof by contraposition how does this work recall that p q q p therefore a proof of q p is also a proof of p q proof by contraposition is an indirect proof technique since we don t prove p q directly let take a look at an example prove if n is an integer and is odd then n is odd first attempt a direct proof assume that is odd thus for some k can solve to find that n where do we go from here now try proof by contraposition proof by contradiction given a conditional p q the only way to reject this claim is to prove that p q is true in a proof by contradiction we assume that p q is true proceed with the proof if this assumption leads us to a contradiction we can conclude that p q is true let revisit an earlier example prove if n is an integer and is odd then n is odd proof we can also use proof by contradiction in cases where were the theorem to be proved is not of the form p q prove at least of any days fall on the same day of the week proof let p at least of any days fall on the same day of the week assume p is true that is at most of any days fall on the same day of the week since there are days in a week at at most 9 days can be chosen this is a contradiction of the fact that we chose days therefore we can conclude that at least of any days fall on the same day of the week this proof is an example of the pigeonhole principle which we will study during our combinatorics unit group work problem prove the following claims use a direct proof to show that the square of an even number is an even number show that if m n and n p are even integers then the sum m p is also an even integer use proof by contraposition to show that if n is an integer and is odd then n is even sadly not all theorems are of the form p q sometimes we need to prove a theorem of the form pn q note pn q distributive law so we might need to examine multiple cases prove that where n is a positive integer with n proof n n n n since we have verified each case we have shown that where n is a positive integer with n with only cases to consider exhaustive proof was a good choice sometimes exhaustive proof isn t an option but we still need to examine multiple possibilities example prove the triangle inequality that is if x and y are real numbers then x y x y clearly we can t use exhaustive proof here since there are infinitely many real numbers to consider we also can t use a simple direct proof either since our proof depends on the signs of x and y example prove that if x and y are real numbers then x y x y making mistakes when using proof by cases is all too easy mistake proof by a few cases is not equivalent to proof by cases this is a there exists proof not a for all proof example prove that all odd numbers are prime proof case i the number is both odd and prime case ii the number is both odd and prime case iii the number is both odd and prime case iv the number is both odd and prime thus we have shown that odd numbers are prime making mistakes when using proof by cases is all too easy mistake leaving out critical cases example prove that for all integers x proof case i assume that x since the product of two negative numbers is always positive case ii assume that x since the product of two positive numbers is always positive since we have proven the claim for all cases we can conclude that for all integers x what about the case in which x sometimes we need to prove the existence of a given element there are two ways to do this the constructive approach s the non constructive approach a constructive existence proof prove show that there is a positive integer that can be written as the sum of cubes of positive integers in two different ways proof obviously the claim has been proven because we have shown that a specific instance of the claim is valid constructive existence proofs are really just instances of existential generalization a non constructive existence proof prove show that there exist two irrational numbers x and y such that xy is rational proof note we don t know whether is rational or irrational however in either case we can use it to construct a rational number sometimes existence is not enough and we need to prove uniqueness this process has two steps example prove that if a and b are real numbers then there exists a unique real number r such that ar b existence proof note that r b a is a solution to this equality since a b a b b b assume that as b r then as b so b a r which is a contradiction uniqueness the scientific process is not always straightforward conjecture gather evidence prove lemmas prove theorem proof strategies can help preserve your sanity proof strategies help us organize our problem solving approach effectively use all of the tools at our disposal develop a coherent plan of attack types of proof strategy today we ll discuss four types of strategy forward reasoning backward reasoning searching for counterexamples adapting existing proofs sometimes forward reasoning doesn t work in these cases it is often helpful to reason backwards starting with the goal that we want to prove example prove that given two distinct positive real numbers x and y the arithmetic mean of x and y is always greater than the geometric mean of x and y x y xy prove that x y xy for all distinct pairs of positive real numbers x and y proof since x y whenever x y the final inequality is true since all of these inequalities are the same it follows that x y xy other times searching for a counterexample is helpful proof by counterexample is helpful if proof attempts repeatedly fail the conjecture to be proven looks funny example prove that every positive integer is the sum of two squares this seems strange to me since other factorizations e g prime factorizations can be complex counterexample is not the sum of two squares so the claim is false these four proof strategies are just a start when trying to prove a new conjecture a good meta strategy is to if possible try to reuse an existing proof if the conjecture looks fishy check for a counterexample attempt a real proof either apply the forward reasoning strategy or apply the backward reasoning strategy unfortunately not every proof can be solved using this nice little meta strategy in fact there are many many proof strategies out there and none of them can be guaranteed to find a proof group work problem prove that there exists a positive integer that is equal to the sum of all positive integers not exceeding it is your proof constructive or non constructive problem prove that there is no positive integer n such that problem use proof by cases to show that min a min b c min min a b c whenever a b and c are real numbers final thoughts proving theorems is not always straightforward having several proof strategies at your disposal will make a huge difference in your success rate we are done with our intro to logic and proofs next lecture intro to set theory please read sections and discrete structures for computer science adam j lee sennott square lecture sets february announcements hw is out it due next monday today topics introduction to set theory what is a set set notation basic set operations what is a set definition a set is an unordered collection of objects sets can contain items examples a b cow pig turkey of mixed types c motorcycle socrates e sets can contain other sets informally sets are really just a precise way of grouping a bunch of stuff a set is made up of elements definition the objects making up a set are called elements of that set examples is an element of bob is an element of alice bob charlie daniel we can express the above examples in a more precise manner as follows bob alice bob charlie daniel question is there are many different ways to describe a set explicit enumeration a using ellipses if the general pattern is obvious e set builder notation aka set comprehensions m y y for some integer k the set m contains all elements y such that y for some integer k there are a number of sets that are so important to mathematics that they get their own symbol n z z q p q p q z q r note this notation differs from book to book some authors write these sets as some authors do not include zero in the natural numbers be careful when reading other books or researching on the web as things may be slightly different you ve actually been using sets implicitly all along mathematics domains of propositional functions programming language data types set equality definition two sets are equal if and only if they contain exactly the same elements mathematically a b iff x x a x b example are the following sets equal 4 and 4 4 and 4 a b c d e and a a c b e d a e i o and a e i o u we can use venn diagrams to graphically represent sets u is the universe of all elements u a v i e u o the set v of all vowels is contained with in the universe of all letters sometimes we add points for the elements of a set sets can be contained within one another definition some set a is a subset of another set b iff every element of a is contained in the set b we denote this fact as a b and call b a superset of a graphically mathematically definition we say that a is a proper subset of b iff a b but a b we denote this by a b more precisely a b iff x x a x b y y b y a properties of subsets property for all sets s we have that s proof the set contains no elements so trivially every element of the set is contained in any other set s property for any set s s s property if then and group work problem come up with two ways to represent each of the following sets the even integers negative numbers between and inclusive the positive integers problem are the sets a b c and c c a b a b equal why or why not problem draw a venn diagram representing the sets and 4 we can create a new set by combining two or more existing sets definition the union of two sets a and b contains every element that is either in a or in b we denote the union of the sets a and b as a b graphically mathematically example 6 6 we can take the union of any number of sets example a b c graphically in general we can express the union sn using the following notation n si i this is just like summation notation sometimes we re interested in the elements that are in more than one set definition the intersection of two sets a and b contains every element that is in a and also in b we denote the intersection of the sets a and b as a b graphically mathematically examples 6 6 we say that two sets a and b are disjoint if a b we can take the intersection of any number of sets example a b c graphically as with the union operation we can express the intersection sn as n si i set differences definition the difference of two sets a and b denoted by a b contains every element that is in a but not in b graphically mathematically example 2 4 4 6 2 be careful some authors use the notation a b to denote the set difference a b if we have specified a universe u we can determine the complement of a set definition the complement of a set a denoted by a contains every element that is in u but not in a graphically mathematically examples assume that u 2 2 4 2 4 6 8 cardinality is the measure of a set size definition let s be a set if there are exactly n elements in s where n is a nonnegative integer then s is a finite set whose cardinality is n the cardinality of s is denoted by s example if s a e i o u then s useful facts if a and b are finite sets then a b a b a b a b a a b aside we ll talk about the cardinality of infinite sets later in the course power set definition given a set s it power set is the set containing all subsets of s we denote the power set of s as p s examples p p 2 2 2 3 2 3 2 3 note the set is always in the power set of any set s the set s is always in its own power set p s 2 s some authors use the notation to represent the power set of s how do we represent ordered collections definition the ordered n tuple an is the ordered collection that has as its first element as its second element and an as its nth element note an bn iff ai bi for i n special case ordered pairs of the form x z y z are the basis of the cartesian plane a b c d iff a c and b d a b b a iff a b how can we construct and describe ordered n tuples we use the cartesian product operator to construct ordered n tuples definition if a and b are sets the cartesian product of a and b which is denoted a b is the set of all ordered pairs a b such that a a and b b mathematically examples let a 1 2 and b y z what is a b b a are a b and b a equivalent cartesian products can be made from more than two sets example let s x x is enrolled in cs g x x r x y freshman sophomore junior senior the set s y g consists of all possible student year grade combinations note my grades database is a subset of s y g that defines a relation between students in the class their year at pitt and their grade we will study the properties of relations towards the end of this course sets and cartesian products can be used to represent trees and graphs dave charlie let alice bob elise becky frank node edge n all names f n n sarah tommy a social network can be represented as a graph v e n f in which the set v denotes the people in the network and the set e denotes the set of friendship links in the above network v alice bob tommy n e alice bob alice dave sarah tommy n n set notation allows us to make quantified statements more precise we can use set notation to make the domain of a quantified statement explicit example x r the square of any real number is a least zero example n z j k z 2 1 n 1 if n is an integer and 2 is odd then n is odd note this notation is far less ambiguous than simply stating the domains of propositional functions in the remainder of the course we will use this notation whenever possible truth sets describe when a predicate is true definition given a predicate p and its corresponding domain d the truth set of p enumerates all elements in d that make the predicate p true examples what are the truth sets of the following predicates given that their domain is the set z p x x 1 q x r x note x p x is true iff the truth set of p is the entire domain d x p x is true iff the truth set of p is non empty how do computers represent and manipulate finite sets observation representing sets as unordered collections of elements e g arrays of java object data types is very inefficient as a result sets are usually represented using either hash maps or bitmaps you ll learn about these in a data structures class so today we ll focus on bitmap representations this is probably best explained through an example playing with the set s x x n x to represent a set as a bitmap we must first agree on an ordering for the set in the case of s let use the natural ordering of the numbers now any subset of s can be represented using s bits for example 1 3 1 1 1 4 0100 what subsets of s do the following bitmaps represent set operations can be carried out very efficiently as bitwise operations example 1 3 2 3 8 0011 example 1 3 2 3 8 0011 10 note these operations are much faster than searching through unordered lists set operations can be carried out very efficiently as bitwise operations example 1 3 0001 1010 0 2 4 6 8 since the set difference a b can be written as a a b we can calculate it as a a b although set difference is more complicated than the basic operations it is still much faster to calculate set differences using a bitmap approach as opposed to an unordered search group work problem 4 let a 1 2 3 4 b 3 9 and c 8 9 10 calculate the following a b a b c b c a b c problem 5 come up with a bitmap representation of the sets a a c d f and b a b c use this to calculate the following a b a b final thoughts sets are one of the most basic data structures used in computer science today we looked at how to define sets basic set operations how computers represent sets next time set identities section 2 2 functions section 2 3 discrete structures for computer science adam j lee sennott square lecture set identities and functions today topics set identities methods of proof relationships to logical equivalences functions important definitions relationships to sets relations special of functions set identities help us manipulate complex expressions recall from last lecture that set operations bear a striking resemblance to logical operations disjunction and set union conjunction and set intersection negation and complement just as logical equivalences helped us manipulate logical expressions set identities help us simplify and understand complex set definitions some important set identities we don t have commutative or associative laws for set difference some important set identities there are many ways to prove set identities today we ll discuss four common methods membership tables logical argument using set builder notation applying other known set identities membership tables allow us to write proofs like we did using truth tables the membership table for an expression has columns for sub expressions and rows to indicate the ways in which an arbitrary element may or may not be included example a membership table for set intersection an element is in a b iff it is in both a and b prove that a b c a b a c since the appropriate columns of the membership table are the same we can conclude that a b c a b a c sometimes it s easier to make a logical argument about a set identity recall a b iff a b and b a as a result we can prove a set identity by arguing that each side of the equality is a subset of the other example prove that a b a b first prove that a b a b then prove that a b a b let s see how this is done prove that a b a b we can use set builder notation and logical definition to make very precise proofs example prove that a b a b proof a b x x a b def n of complement x x a b def n of x x a x b def n of x x a x b demorgan s law x x a x b def n of x x a x b def n of complement x x a b def n of a b set builder notation we can also construct proofs by repeatedly applying known set identities example prove that a b c c b a proof a b c a b c demorgan s law a b c demorgan s law b c a commutative law c b a commutative law note how similar this process is to that of proving logical equivalences using known logical equivalences group work problem prove demorgan s law for complement over intersection using a membership table problem prove the complementation law using set builder notation sets give us a way to formalize the concept of a function definition let a and b be nonempty sets a function f is an assignment of exactly one element of set b to each element of set a note we write f a b to denote that f is a function from a to b note we say that f a b if the element a a is mapped to the unique element b b by the function f f a b f a b a b functions can be defined in a number of ways explicitly f z z f x using a programming language int min int x int y x y return x return y using a relation let s anna brian christine let g a b c d f f s g anna a brian b christine c d e more terminology the domain of a function is the set that the function maps from while the codomain is the set that is mapped to if f a b b is called the image of a and a is called the preimage of b the range of a function f a b is the set of all images of elements of a domain s anna brian christine codomain g a b c d f f s g anna brian christine a b c d range a c e a one to one function never assigns the same image to two different elements definition a function f a b is one to one or injective iff x y a f x f y x y are the following functions injections f r r f x x f z z f x f r r f x x f s g f s g anna a brian b christine c d e an onto function uses every element of its codomain definition we call a function f a b onto or surjective iff for every element b b there is some element a a such that f a b think about an onto function as covering the entirety of its codomain the following function is a surjection f a b a b c d are the following functions one to one onto both or neither f a b a b c f a b a b c d d neither one to one and onto aside functions that are both one to one and onto are called bijections f a b a b c f a b a b d one to one c d onto bijections have inverses definition if f a b is a bijection the inverse of f is the function f b a that assigns to each b b the unique value a a such that f a b that is f b a iff f a b graphically f a b f a b a b f b a note only a bijection can have an inverse why functions can be composed with one another definition given two functions g a b and f b c the composition of f and g denoted f g is defined as f g x f g x g a b g a b a g a f c f g a c c f º g note for f º g to exist the domain of f must be a subset of the codomain of g can the following functions be composed if so what is their composition let f a a such that f a b f b c f c a g b a such that g b g a f º g x g º f x let f z z f x g z z g x f º g x g º f x note there is never a guarantee that f º g x g º f x important functions definition the floor function maps a real number x to the largest integer y that is not greater than x the floor of x is denoted x definition the ceiling function maps a real number x to the smallest integer y that is not less than x the ceiling of x is denoted x examples 1 1 2 7 0 7 24 we actually use floor and ceiling quite a bit in computer science example a byte which holds bits is typically the smallest amount of memory that can be allocated on most systems how many bytes are needed to store bits of data answer we need 8 375 bytes example how many byte packets can be transmitted over a kbps modem in one minute answer a 4 kbps modem can transmit bits per minute therefore we can transmit 000 8 packets group work problem find the domain and range of each of the following functions the function that determines the number of zeros in some bit string the function that maps an english word to its two rightmost letters the function that assigns to an integer the sum of its individual digits problem 4 compute the following a 5 b c 5 5 1 22 final thoughts set identities are useful tools we can prove set identities in a number of equivalent ways sets are the basis of functions which are used throughout computer science and mathematics next time summations section 2 4 the rules of logic specify the meaning of mathematical statements for instance these rules help us understand and reason with statements such as there exists an integer that is not the sum of two squares and for every positive integer n the sum of the positive integers not exceeding n is n n logic is the basis of all mathematical reasoning and of all automated reasoning it has practical applications to the design of computing machines to the specification of systems to artificial intelligence to computer programming to programming languages and to other areas of computer science as well as to many other fields of study to understand mathematics we must understand what makes up a correct mathematical argument that is a proof once we prove a mathematical statement is true we call it a theorem a collection of theorems on a topic organize what we know about this topic to learn a mathematical topic a person needs to actively construct mathematical arguments on this topic and not just read exposition moreover knowing the proof of a theorem often makes it possible to modify the result to fit new situations everyone knows that proofs are important throughout mathematics but many people find it surprising how important proofs are in computer science in fact proofs are used to verify that computer programs produce the correct output for all possible input values to show that algorithms always produce the correct result to establish the security of a system and to create artificial intelligence furthermore automated reasoning systems have been created to allow computers to construct their own proofs in this chapter we will explain what makes up a correct mathematical argument and intro duce tools to construct these arguments we will develop an arsenal of different proof methods that will enable us to prove many different types of results after introducing many different methods of proof we will introduce several strategies for constructing proofs we will intro duce the notion of a conjecture and explain the process of developing mathematics by studying conjectures propositional logic introduction the rules of logic give precise meaning to mathematical statements these rules are used to distinguish between valid and invalid mathematical arguments because a major goal of this book is to teach the reader how to understand and how to construct correct mathematical arguments we begin our study of discrete mathematics with an introduction to logic besides the importance of logic in understanding mathematical reasoning logic has numer ous applications to computer science these rules are used in the design of computer circuits the construction of computer programs the verification of the correctness of programs and in many other ways furthermore software systems have been developed for constructing some but not all types of proofs automatically we will discuss these applications of logic in this and later chapters propositions our discussion begins with an introduction to the basic building blocks of logic propositions a proposition is a declarative sentence that is a sentence that declares a fact that is either true or false but not both example all the following declarative sentences are propositions washington d c is the capital of the united states of america toronto is the capital of canada propositions and are true whereas and are false some sentences that are not propositions are given in example example consider the following sentences what time is it read this carefully x x y z sentences and are not propositions because they are not declarative sentences sentences and are not propositions because they are neither true nor false note that each of sentences and can be turned into a proposition if we assign values to the variables we will also discuss other ways to turn sentences such as these into propositions in section we use letters to denote propositional variables or statement variables that is vari ables that represent propositions just as letters are used to denote numerical variables the aristotle b c e b c e aristotle was born in stagirus stagira in northern greece his father was the personal physician of the king of macedonia because his father died when aristotle was young aristotle could not follow the custom of following his father profession aristotle became an orphan at a young age when his mother also died his guardian who raised him taught him poetry rhetoric and greek at the age of his guardian sent him to athens to further his education aristotle joined plato academy where for years he attended plato lectures later presenting his own lectures on rhetoric when plato died in b c e aristotle was not chosen to succeed him because his views differed too much from those of plato instead aristotle joined the court of king hermeas where he remained for three years and married the niece of the king when the persians defeated hermeas aristotle moved to mytilene and at the invitation of king philip of macedonia he tutored alexander philip son who later became alexander the great aristotle tutored alexander for five years and after the death of king philip he returned to athens and set up his own school called the lyceum aristotle followers were called the peripatetics which means to walk about because aristotle often walked around as he discussed philosophical questions aristotle taught at the lyceum for years where he lectured to his advanced students in the morning and gave popular lectures to a broad audience in the evening when alexander the great died in b c e a backlash against anything related to alexander led to trumped up charges of impiety against aristotle aristotle fled to chalcis to avoid prosecution he only lived one year in chalcis dying of a stomach ailment in b c e aristotle wrote three types of works those written for a popular audience compilations of scientific facts and systematic treatises the systematic treatises included works on logic philosophy psychology physics and natural history aristotle writings were preserved by a student and were hidden in a vault where a wealthy book collector discovered them about years later they were taken to rome where they were studied by scholars and issued in new editions preserving them for posterity conventional letters used for propositional variables are p q r the truth value of a proposition is true denoted by t if it is a true proposition and the truth value of a proposition is false denoted by f if it is a false proposition the area of logic that deals with propositions is called the propositional calculus or propo sitional logic it was first developed systematically by the greek philosopher aristotle more than years ago we now turn our attention to methods for producing new propositions from those that we already have these methods were discussed by the english mathematician george boole in in his book the laws of thought many mathematical statements are constructed by combining one or more propositions new propositions called compound propositions are formed from existing propositions using logical operators definition example find the negation of the proposition michael pc runs linux and express this in simple english solution the negation is it is not the case that michael pc runs linux this negation can be more simply expressed as michael pc does not run linux example find the negation of the proposition vandana smartphone has at least of memory and express this in simple english solution the negation is it is not the case that vandana smartphone has at least of memory this negation can also be expressed as vandana smartphone does not have at least of memory or even more simply as vandana smartphone has less than of memory table the truth table for the negation of a proposition p p t f f t definition table displays the truth table for the negation of a proposition p this table has a row for each of the two possible truth values of a proposition p each row shows the truth value of p corresponding to the truth value of p for this row the negation of a proposition can also be considered the result of the operation of the negation operator on a proposition the negation operator constructs a new proposition from a single existing proposition we will now introduce the logical operators that are used to form new propositions from two or more existing propositions these logical operators are also called connectives table displays the truth table of p q this table has a row for each of the four possible combinations of truth values of p and q the four rows correspond to the pairs of truth values tt tf ft and ff where the first truth value in the pair is the truth value of p and the second truth value is the truth value of q note that in logic the word but sometimes is used instead of and in a conjunction for example the statement the sun is shining but it is raining is another way of saying the sun is shining and it is raining in natural language there is a subtle difference in meaning between and and but we will not be concerned with this nuance here example find the conjunction of the propositions p and q where p is the proposition rebecca pc has more than gb free hard disk space and q is the proposition the processor in rebecca pc runs faster than ghz solution the conjunction of these propositions p q is the proposition rebecca pc has more than gb free hard disk space and the processor in rebecca pc runs faster than ghz this conjunction can be expressed more simply as rebecca pc has more than gb free hard disk space and its processor runs faster than ghz for this conjunction to be true both conditions given must be true it is false when one or both of these conditions are false definition table displays the truth table for p q the use of the connective or in a disjunction corresponds to one of the two ways the word or is used in english namely as an inclusive or a disjunction is true when at least one of the two propositions is true for instance the inclusive or is being used in the statement students who have taken calculus or computer science can take this class here we mean that students who have taken both calculus and computer science can take the class as well as the students who have taken only one of the two subjects on the other hand we are using the exclusive or when we say students who have taken calculus or computer science but not both can enroll in this class here we mean that students who have taken both calculus and a computer science course cannot take the class only those who have taken exactly one of the two courses can take the class similarly when a menu at a restaurant states soup or salad comes with an entrée the restaurant almost always means that customers can have either soup or salad but not both hence this is an exclusive rather than an inclusive or example what is the disjunction of the propositions p and q where p and q are the same propositions as in example solution the disjunction of p and q p q is the proposition rebecca pc has at least gb free hard disk space or the processor in rebecca pc runs faster than ghz this proposition is true when rebecca pc has at least gb free hard disk space when the pc processor runs faster than ghz and when both conditions are true it is false when both of these conditions are false that is when rebecca pc has less than gb free hard disk space and the processor in her pc runs at ghz or slower as was previously remarked the use of the connective or in a disjunction corresponds to one of the two ways the word or is used in english namely in an inclusive way thus a disjunction is true when at least one of the two propositions in it is true sometimes we use or in an exclusive sense when the exclusive or is used to connect the propositions p and q the proposition p or q but not both is obtained this proposition is true when p is true and q is false and when p is false and q is true it is false when both p and q are false and when both are true george boole george boole the son of a cobbler was born in lincoln england in november because of his family difficult financial situation boole struggled to educate himself while supporting his family nevertheless he became one of the most important mathematicians of the although he considered a career as a clergyman he decided instead to go into teaching and soon afterward opened a school of his own in his preparation for teaching mathematics boole unsatisfied with textbooks of his day decided to read the works of the great mathematicians while reading papers of the great french mathematician lagrange boole made discoveries in the calculus of variations the branch of analysis dealing with finding curves and surfaces by optimizing certain parameters in boole published the mathematical analysis of logic the first of his contributions to symbolic logic in he was appointed professor of mathematics at queen college in cork ireland in he published the laws of thought his most famous work in this book boole introduced what is now called boolean algebra in his honor boole wrote textbooks on differential equations and on difference equations that were used in great britain until the end of the nineteenth century boole married in his wife was the niece of the professor of greek at queen college in boole died from pneumonia which he contracted as a result of keeping a lecture engagement even though he was soaking wet from a rainstorm definition the truth table for the exclusive or of two propositions is displayed in table conditional statements we will discuss several other important ways in which propositions can be combined definition the statement p q is called a conditional statement because p q asserts that q is true on the condition that p holds a conditional statement is also called an implication the truth table for the conditional statement p q is shown in table note that the statement p q is true when both p and q are true and when p is false no matter what truth value q has because conditional statements play such an essential role in mathematical reasoning a variety of terminology is used to express p q you will encounter most if not all of the following ways to express this conditional statement if p then q p implies q if p q p only if q p is sufficient for q a sufficient condition for q is p q if p q whenever p q when p q is necessary for p a necessary condition for p is q q follows from p q unless p a useful way to understand the truth value of a conditional statement is to think of an obligation or a contract for example the pledge many politicians make when running for office is if i am elected then i will lower taxes you might have trouble understanding how unless is used in conditional statements unless you read this paragraph carefully if the politician is elected voters would expect this politician to lower taxes furthermore if the politician is not elected then voters will not have any expectation that this person will lower taxes although the person may have sufficient influence to cause those in power to lower taxes it is only when the politician is elected but does not lower taxes that voters can say that the politician has broken the campaign pledge this last scenario corresponds to the case when p is true but q is false in p q similarly consider a statement that a professor might make if you get on the final then you will get an a if you manage to get a on the final then you would expect to receive an a if you do not get you may or may not receive an a depending on other factors however if you do get but the professor does not give you an a you will feel cheated of the various ways to express the conditional statement p q the two that seem to cause the most confusion are p only if q and q unless p consequently we will provide some guidance for clearing up this confusion to remember that p only if q expresses the same thing as if p then q note that p only if q says that p cannot be true when q is not true that is the statement is false if p is true but q is false when p is false q may be either true or false because the statement says nothing about the truth value of q be careful not to use q only if p to express p q because this is incorrect to see this note that the true values of q only if p and p q are different when p and q have different truth values to remember that q unless p expresses the same conditional statement as if p then q note that q unless p means that if p is false then q must be true that is the statement q unless p is false when p is true but q is false but it is true otherwise consequently q unless p and p q always have the same truth value we illustrate the translation between conditional statements and english statements in ex ample example let p be the statement maria learns discrete mathematics and q the statement maria will find a good job express the statement p q as a statement in english solution from the definition of conditional statements we see that when p is the statement maria learns discrete mathematics and q is the statement maria will find a good job p q represents the statement if maria learns discrete mathematics then she will find a good job there are many other ways to express this conditional statement in english among the most natural of these are maria will find a good job when she learns discrete mathematics for maria to get a good job it is sufficient for her to learn discrete mathematics and maria will find a good job unless she does not learn discrete mathematics note that the way we have defined conditional statements is more general than the meaning attached to such statements in the english language for instance the conditional statement in example and the statement if it is sunny then we will go to the beach are statements used in normal language where there is a relationship between the hypothesis and the conclusion further the first of these statements is true unless maria learns discrete mathematics but she does not get a good job and the second is true unless it is indeed sunny but we do not go to the beach on the other hand the statement if juan has a smartphone then is true from the definition of a conditional statement because its conclusion is true the truth value of the hypothesis does not matter then the conditional statement if juan has a smartphone then is true if juan does not have a smartphone even though is false we would not use these last two conditional statements in natural language except perhaps in sarcasm because there is no relationship between the hypothesis and the conclusion in either statement in math ematical reasoning we consider conditional statements of a more general sort than we use in english the mathematical concept of a conditional statement is independent of a cause and effect relationship between hypothesis and conclusion our definition of a conditional statement specifies its truth values it is not based on english usage propositional language is an artificial language we only parallel english usage to make it easy to use and remember the if then construction used in many programming languages is different from that used in logic most programming languages contain statements such as if p then s where p is a proposition and s is a program segment one or more statements to be executed when execution of a program encounters such a statement s is executed if p is true but s is not executed if p is false as illustrated in example example what is the value of the variable x after the statement if then x x if x before this statement is encountered the symbol stands for assignment the statement x x means the assignment of the value of x to x solution because is true the assignment statement x x is executed hence x has the value after this statement is encountered remember that the contrapositive but neither the converse or inverse of a conditional statement is equivalent to it converse contrapositive and inverse we can form some new conditional statements starting with a conditional statement p q in particular there are three related conditional statements that occur so often that they have special names the proposition q p is called the converse of p q the contrapositive of p q is the proposition q p the proposition p q is called the inverse of p q we will see that of these three conditional statements formed from p q only the contrapositive always has the same truth value as p q we first show that the contrapositive q p of a conditional statement p q always has the same truth value as p q to see this note that the contrapositive is false only when p is false and q is true that is only when p is true and q is false we now show that neither the converse q p nor the inverse p q has the same truth value as p q for all possible truth values of p and q note that when p is true and q is false the original conditional statement is false but the converse and the inverse are both true when two compound propositions always have the same truth value we call them equiv alent so that a conditional statement and its contrapositive are equivalent the converse and the inverse of a conditional statement are also equivalent as the reader can verify but neither is equivalent to the original conditional statement we will study equivalent propositions in sec tion take note that one of the most common logical errors is to assume that the converse or the inverse of a conditional statement is equivalent to this conditional statement we illustrate the use of conditional statements in example example what are the contrapositive the converse and the inverse of the conditional statement the home team wins whenever it is raining solution because q whenever p is one of the ways to express the conditional statement p q the original statement can be rewritten as if it is raining then the home team wins consequently the contrapositive of this conditional statement is if the home team does not win then it is not raining the converse is if the home team wins then it is raining the inverse is if it is not raining then the home team does not win only the contrapositive is equivalent to the original statement biconditionals we now introduce another way to combine propositions that expresses that two propositions have the same truth value definition the truth table for p q is shown in table note that the statement p q is true when both the conditional statements p q and q p are true and is false otherwise that is why we use the words if and only if to express this logical connective and why it is symbolically written by combining the symbols and there are some other common ways to express p q p is necessary and sufficient for q if p then q and conversely p iff q the last way of expressing the biconditional statement p q uses the abbreviation iff for if and only if note that p q has exactly the same truth value as p q q p table the truth table for the biconditional p q p q p q t t t t f f f t f f f t example let p be the statement you can take the flight and let q be the statement you buy a ticket then p q is the statement you can take the flight if and only if you buy a ticket this statement is true if p and q are either both true or both false that is if you buy a ticket and can take the flight or if you do not buy a ticket and you cannot take the flight it is false when p and q have opposite truth values that is when you do not buy a ticket but you can take the flight such as when you get a free trip and when you buy a ticket but you cannot take the flight such as when the airline bumps you implicit use of biconditionals you should be aware that biconditionals are not always explicit in natural language in particular the if and only if construction used in biconditionals is rarely used in common language instead biconditionals are often expressed using an if then or an only if construction the other part of the if and only if is implicit that is the converse is implied but not stated for example consider the statement in english if you finish your meal then you can have dessert what is really meant is you can have dessert if and only if you finish your meal this last statement is logically equivalent to the two statements if you finish your meal then you can have dessert and you can have dessert only if you finish your meal because of this imprecision in natural language we need to make an assumption whether a conditional statement in natural language implicitly includes its converse because precision is essential in mathematics and in logic we will always distinguish between the conditional statement p q and the biconditional statement p q truth tables of compound propositions we have now introduced four important logical connectives conjunctions disjunctions con ditional statements and biconditional statements as well as negations we can use these con nectives to build up complicated compound propositions involving any number of propositional variables we can use truth tables to determine the truth values of these compound propositions as example illustrates we use a separate column to find the truth value of each compound expression that occurs in the compound proposition as it is built up the truth values of the compound proposition for each combination of truth values of the propositional variables in it is found in the final column of the table example construct the truth table of the compound proposition p q p q solution because this truth table involves two propositional variables p and q there are four rows in this truth table one for each of the pairs of truth values tt tf ft and ff the first two columns are used for the truth values of p and q respectively in the third column we find the truth value of q needed to find the truth value of p q found in the fourth column the fifth column gives the truth value of p q finally the truth value of p q p q is found in the last column the resulting truth table is shown in table table the truth table of p q p q p q q p q p q p q p q t t f t t t t f t t f f f t f f f t f f t t f f precedence of logical operators we can construct compound propositions using the negation operator and the logical operators defined so far we will generally use parentheses to specify the order in which logical operators in a compound proposition are to be applied for instance p q r is the conjunction of p q and r however to reduce the number of parentheses we specify that the negation operator is applied before all other logical operators this means that p q is the conjunction of p and q namely p q not the negation of the conjunction of p and q namely p q another general rule of precedence is that the conjunction operator takes precedence over the disjunction operator so that p q r means p q r rather than p q r because this rule may be difficult to remember we will continue to use parentheses so that the order of the disjunction and conjunction operators is clear finally it is an accepted rule that the conditional and biconditional operators and have lower precedence than the conjunction and disjunction operators and consequently p q r is the same as p q r we will use parentheses when the order of the con ditional operator and biconditional operator is at issue although the conditional operator has precedence over the biconditional operator table displays the precedence levels of the logical operators and logic and bit operations computers represent information using bits a bit is a symbol with two possible values namely zero and one this meaning of the word bit comes from binary digit because zeros and ones are the digits used in binary representations of numbers the well known statistician john tukey introduced this terminology in a bit can be used to represent a truth value because there are two truth values namely true and false as is customarily done we will use a bit to represent true and a bit to represent false that is represents t true represents f false a variable is called a boolean variable if its value is either true or false consequently a boolean variable can be represented using a bit computer bit operations correspond to the logical connectives by replacing true by a one and false by a zero in the truth tables for the operators and the tables shown in table for the corresponding bit operations are obtained we will also use the notation or and and xor for the operators and as is done in various programming languages john wilder tukey tukey born in new bedford massachusetts was an only child his parents both teachers decided home schooling would best develop his potential his formal education began at brown university where he studied mathematics and chemistry he received a master degree in chemistry from brown and continued his studies at princeton university changing his field of study from chemistry to mathematics he received his ph d from princeton in for work in topology when he was appointed an instructor in mathematics at princeton with the start of world war ii he joined the fire control research office where he began working in statistics tukey found statistical research to his liking and impressed several leading statisticians with his skills in at the conclusion of the war tukey returned to the mathematics department at princeton as a professor of statistics and he also took a position at at t bell laboratories tukey founded the statistics department at princeton in and was its first chairman tukey made significant contributions to many areas of statistics including the analysis of variance the estimation of spectra of time series inferences about the values of a set of parameters from a single experiment and the philosophy of statistics however he is best known for his invention with j w cooley of the fast fourier transform in addition to his contributions to statistics tukey was noted as a skilled wordsmith he is credited with coining the terms bit and software tukey contributed his insight and expertise by serving on the president science advisory committee he chaired several important committees dealing with the environment education and chemicals and health he also served on committees working on nuclear disarmament tukey received many awards including the national medal of science historical note there were several other suggested words for a binary digit including binit and bigit that never were widely accepted the adoption of the word bit may be due to its meaning as a common english word for an account of tukey coining of the word bit see the april issue of annals of the history of computing table table for the bit operators or and and xor x y x y x y x y information is often represented using bit strings which are lists of zeros and ones when this is done operations on the bit strings can be used to manipulate this information definition example is a bit string of length nine we can extend bit operations to bit strings we define the bitwise or bitwise and and bitwise xor of two strings of the same length to be the strings that have as their bits the or and and xor of the corresponding bits in the two strings respectively we use the symbols and to represent the bitwise or bitwise and and bitwise xor operations respectively we illustrate bitwise operations on bit strings with example example find the bitwise or bitwise and and bitwise xor of the bit strings and here and throughout this book bit strings will be split into blocks of four bits to make them easier to read solution the bitwise or bitwise and and bitwise xor of these strings are obtained by taking the or and and xor of the corresponding bits respectively this gives us bitwise or bitwise and bitwise xor exercises which of these sentences are propositions what are the truth values of those that are propositions a boston is the capital of massachusetts b miami is the capital of florida c d e x f answer this question which of these are propositions what are the truth values of those that are propositions a do not pass go b what time is it c there are no black flies in maine d x e the moon is made of green cheese f what is the negation of each of these propositions a mei has an player b there is no pollution in new jersey c d the summer in maine is hot and sunny what is the negation of each of these propositions a jennifer and teja are friends b there are items in a baker dozen c abby sent more than text messages every day d is a perfect square what is the negation of each of these propositions a steve has more than gb free disk space on his laptop b zach blocks e mails and texts from jennifer c d diane rode her bicycle miles on sunday suppose that smartphone a has mb ram and gb rom and the resolution of its camera is mp smart phone b has mb ram and gb rom and the resolution of its camera is mp and smartphone c has mb ram and gb rom and the resolution of its camera is mp determine the truth value of each of these propositions a smartphone b has the most ram of these three smart phones b smartphone c has more rom or a higher resolution camera than smartphone b c smartphone b has more ram more rom and a higher resolution camera than smartphone a d if smartphone b has more ram and more rom than smartphone c then it also has a higher resolution camera e smartphone a has more ram than smartphone b if and only if smartphone b has more ram than smart phone a suppose that during the most recent fiscal year the an nual revenue of acme computer was billion dollars and its net profit was billion dollars the annual revenue of nadir software was billion dollars and its net profit was billion dollars and the annual revenue of quixote media was billion dollars and its net profit was billion dollars determine the truth value of each of these propositions for the most recent fiscal year a quixote media had the largest annual revenue b nadir software had the lowest net profit and acme computer had the largest annual revenue c acme computer had the largest net profit or quixote media had the largest net profit d if quixote media had the smallest net profit then acme computer had the largest annual revenue e nadir software had the smallest net profit if and only if acme computer had the largest annual revenue let p and q be the propositions p i bought a lottery ticket this week q i won the million dollar jackpot express each of these propositions as an english sen tence a p b p q c p q d p q e p q f p q g p q h p p q let p and q be the propositions swimming at the new jersey shore is allowed and sharks have been spotted near the shore respectively express each of these com pound propositions as an english sentence a q b p q c p q d p q e q p f p q g p q h p p q let p and q be the propositions the election is decided and the votes have been counted respectively express each of these compound propositions as an english sen tence a p b p q c p q d q p e q p f p q g p q h q p q let p and q be the propositions p it is below freezing q it is snowing write these propositions using p and q and logical con nectives including negations a it is below freezing and snowing b it is below freezing but not snowing c it is not below freezing and it is not snowing d it is either snowing or below freezing or both e if it is below freezing it is also snowing f either it is below freezing or it is snowing but it is not snowing if it is below freezing g that it is below freezing is necessary and sufficient for it to be snowing let p q and r be the propositions p you have the flu q you miss the final examination r you pass the course express each of these propositions as an english sen tence a p q b q r c q r d p q r e p r q r f p q q r let p and q be the propositions p you drive over miles per hour q you get a speeding ticket write these propositions using p and q and logical con nectives including negations a you do not drive over miles per hour b you drive over miles per hour but you do not get a speeding ticket c you will get a speeding ticket if you drive over miles per hour d if you do not drive over miles per hour then you will not get a speeding ticket e driving over miles per hour is sufficient for getting a speeding ticket f you get a speeding ticket but you do not drive over miles per hour g whenever you get a speeding ticket you are driving over miles per hour let p q and r be the propositions p you get an a on the final exam q you do every exercise in this book r you get an a in this class write these propositions using p q and r and logical connectives including negations a you get an a in this class but you do not do every exercise in this book b you get an a on the final you do every exercise in this book and you get an a in this class c to get an a in this class it is necessary for you to get an a on the final d you get an a on the final but you don t do every ex ercise in this book nevertheless you get an a in this class e getting an a on the final and doing every exercise in this book is sufficient for getting an a in this class f you will get an a in this class if and only if you either do every exercise in this book or you get an a on the final let p q and r be the propositions p grizzly bears have been seen in the area q hiking is safe on the trail r berries are ripe along the trail write these propositions using p q and r and logical connectives including negations a berries are ripe along the trail but grizzly bears have not been seen in the area b grizzly bears have not been seen in the area and hik ing on the trail is safe but berries are ripe along the trail c if berries are ripe along the trail hiking is safe if and only if grizzly bears have not been seen in the area d it is not safe to hike on the trail but grizzly bears have not been seen in the area and the berries along the trail are ripe e for hiking on the trail to be safe it is necessary but not sufficient that berries not be ripe along the trail and for grizzly bears not to have been seen in the area f hiking is not safe on the trail whenever grizzly bears have been seen in the area and berries are ripe along the trail determine whether these biconditionals are true or false a if and only if b if and only if c if and only if monkeys can fly d if and only if determine whether each of these conditional statements is true or false a if then b if then c if then d if monkeys can fly then determine whether each of these conditional statements is true or false a if then unicorns exist b if then dogs can fly c if then dogs can fly d if then for each of these sentences determine whether an in clusive or or an exclusive or is intended explain your answer a coffee or tea comes with dinner b a password must have at least three digits or be at least eight characters long c the prerequisite for the course is a course in number theory or a course in cryptography d you can pay using u s dollars or euros for each of these sentences determine whether an in clusive or or an exclusive or is intended explain your answer a experience with c or java is required b lunch includes soup or salad c to enter the country you need a passport or a voter registration card d publish or perish for each of these sentences state what the sentence means if the logical connective or is an inclusive or that is a dis junction versus an exclusive or which of these meanings of or do you think is intended a to take discrete mathematics you must have taken calculus or a course in computer science b when you buy a new car from acme motor company you get back in cash or a car loan c dinner for two includes two items from column a or three items from column b d school is closed if more than feet of snow falls or if the wind chill is below write each of these statements in the form if p then q in english hint refer to the list of common ways to ex press conditional statements provided in this section a it is necessary to wash the boss car to get promoted b winds from the south imply a spring thaw c a sufficient condition for the warranty to be good is that you bought the computer less than a year ago d willy gets caught whenever he cheats e you can access the website only if you pay a subscrip tion fee f getting elected follows from knowing the right peo ple g carol gets seasick whenever she is on a boat write each of these statements in the form if p then q in english hint refer to the list of common ways to express conditional statements a it snows whenever the wind blows from the northeast b the apple trees will bloom if it stays warm for a week c that the pistons win the championship implies that they beat the lakers d it is necessary to walk miles to get to the top of long peak e to get tenure as a professor it is sufficient to be world famous f if you drive more than miles you will need to buy gasoline g your guarantee is good only if you bought your cd player less than days ago h jan will go swimming unless the water is too cold write each of these statements in the form if p then q in english hint refer to the list of common ways to ex press conditional statements provided in this section a i will remember to send you the address only if you send me an e mail message b to be a citizen of this country it is sufficient that you were born in the united states c if you keep your textbook it will be a useful reference in your future courses d the red wings will win the stanley cup if their goalie plays well e that you get the job implies that you had the best credentials f c q p r t u d p r t q t how many rows appear in a truth table for each of these compound propositions a q p p q b p t p c p r t u v d p r q t r t construct a truth table for each of these compound propo sitions a p p b p p c p q q d p q p q the beach erodes whenever there is a storm g it is necessary to have a valid password to log on to the server h you will reach the summit unless you begin your climb too late write each of these propositions in the form p if and only if q in english a if it is hot outside you buy an ice cream cone and if you buy an ice cream cone it is hot outside b for you to win the contest it is necessary and sufficient that you have the only winning ticket c you get promoted only if you have connections and you have connections only if you get promoted d if you watch television your mind will decay and con versely e the trains run late on exactly those days when i take it write each of these propositions in the form p if and only if q in english a for you to get an a in this course it is necessary and sufficient that you learn how to solve discrete mathe matics problems b if you read the newspaper every day you will be in formed and conversely c it rains if it is a weekend day and it is a weekend day if it rains d you can see the wizard only if the wizard is not in and the wizard is not in only if you can see him state the converse contrapositive and inverse of each of these conditional statements a if it snows today i will ski tomorrow b i come to class whenever there is going to be a quiz c a positive integer is a prime only if it has no divisors other than and itself state the converse contrapositive and inverse of each of these conditional statements a if it snows tonight then i will stay at home b i go to the beach whenever it is a sunny summer day c when i stay up late it is necessary that i sleep until noon how many rows appear in a truth table for each of these compound propositions a p p b p r q e p q q p f p q q p construct a truth table for each of these compound propo sitions a p p b p p c p p q d p q p q e q p p q f p q p q construct a truth table for each of these compound propo sitions a p q p q b p q p q c p q p q d p q p q e p q p r f p q p q construct a truth table for each of these compound propo sitions a p p b p p c p q d p q e p q p q f p q p q construct a truth table for each of these compound propo sitions a p q b p q c p q p q d p q p q e p q p q f p q p q construct a truth table for each of these compound propo sitions a p q r b p q r c p q r d p q r e p q r f p q r construct a truth table for each of these compound propo sitions a p q r b p q r c p q p r d p q p r e p q q r f p q q r construct a truth table for p q r construct a truth table for p q r explain without using a truth table why p q q r r p is true when p q and r have the same truth value and it is false otherwise explain without using a truth table why p q r p q r is true when at least one of p q and r is true and at least one is false but is false when all three variables have the same truth value what is the value of x after each of these statements is encountered in a computer program if x before the statement is reached a if x then x x b if x or then x x c if and then x x d if x xor x then x x e if x then x x find the bitwise or bitwise and and bitwise xor of each of these pairs of bit strings a b c d evaluate each of these expressions a b c d fuzzy logic is used in artificial intelligence in fuzzy logic a proposition has a truth value that is a number between and inclusive a proposition with a truth value of is false and one with a truth value of is true truth values that are between and indicate varying degrees of truth for instance the truth value can be assigned to the statement fred is happy because fred is happy most of the time and the truth value can be assigned to the statement john is happy because john is happy slightly less than half the time use these truth values to solve exercises the truth value of the negation of a proposition in fuzzy logic is minus the truth value of the proposition what are the truth values of the statements fred is not happy and john is not happy the truth value of the conjunction of two propositions in fuzzy logic is the minimum of the truth values of the two propositions what are the truth values of the statements fred and john are happy and neither fred nor john is happy the truth value of the disjunction of two propositions in fuzzy logic is the maximum of the truth values of the two propositions what are the truth values of the statements fred is happy or john is happy and fred is not happy or john is not happy is the assertion this statement is false a proposition the nth statement in a list of statements is exactly n of the statements in this list are false a what conclusions can you draw from these state ments b answer part a if the nth statement is at least n of the statements in this list are false c answer part b assuming that the list contains statements an ancient sicilian legend says that the barber in a remote town who can be reached only by traveling a dangerous mountain road shaves those people and only those peo ple who do not shave themselves can there be such a barber applications of propositional logic introduction logic has many important applications to mathematics computer science and numerous other disciplines statements in mathematics and the sciences and in natural language often are im precise or ambiguous to make such statements precise they can be translated into the language of logic for example logic is used in the specification of software and hardware because these specifications need to be precise before development begins furthermore propositional logic and its rules can be used to design computer circuits to construct computer programs to verify the correctness of programs and to build expert systems logic can be used to analyze and solve many familiar puzzles software systems based on the rules of logic have been developed for constructing some but not all types of proofs automatically we will discuss some of these applications of propositional logic in this section and in later chapters translating english sentences there are many reasons to translate english sentences into expressions involving propositional variables and logical connectives in particular english and every other human language is often ambiguous translating sentences into compound statements and other types of logical expressions which we will introduce later in this chapter removes the ambiguity note that this may involve making a set of reasonable assumptions based on the intended meaning of the sentence moreover once we have translated sentences from english into logical expressions we can analyze these logical expressions to determine their truth values we can manipulate them and we can use rules of inference which are discussed in section to reason about them to illustrate the process of translating an english sentence into a logical expression consider examples and example how can this english sentence be translated into a logical expression you can access the internet from campus only if you are a computer science major or you are not a freshman solution there are many ways to translate this sentence into a logical expression although it is possible to represent the sentence by a single propositional variable such as p this would not be useful when analyzing its meaning or reasoning with it instead we will use propositional vari ables to represent each sentence part and determine the appropriate logical connectives between them in particular we let a c and f represent you can access the internet from campus you are a computer science major and you are a freshman respectively noting that only if is one way a conditional statement can be expressed this sentence can be represented as a c f example how can this english sentence be translated into a logical expression you cannot ride the roller coaster if you are under feet tall unless you are older than years old solution let q r and represent you can ride the roller coaster you are under feet tall and you are older than years old respectively then the sentence can be translated to r q of course there are other ways to represent the original sentence as a logical expression but the one we have used should meet our needs system specifications translating sentences in natural language such as english into logical expressions is an essential part of specifying both hardware and software systems system and software engineers take requirements in natural language and produce precise and unambiguous specifications that can be used as the basis for system development example shows how compound propositions can be used in this process example express the specification the automated reply cannot be sent when the file system is full using logical connectives solution one way to translate this is to let p denote the automated reply can be sent and q denote the file system is full then p represents it is not the case that the automated reply can be sent which can also be expressed as the automated reply cannot be sent consequently our specification can be represented by the conditional statement q p system specifications should be consistent that is they should not contain conflicting requirements that could be used to derive a contradiction when specifications are not consistent there would be no way to develop a system that satisfies all specifications example determine whether these system specifications are consistent the diagnostic message is stored in the buffer or it is retransmitted the diagnostic message is not stored in the buffer if the diagnostic message is stored in the buffer then it is retransmitted solution to determine whether these specifications are consistent we first express them using logical expressions let p denote the diagnostic message is stored in the buffer and let q denote the diagnostic message is retransmitted the specifications can then be written as p q p and p q an assignment of truth values that makes all three specifications true must have p false to make p true because we want p q to be true but p must be false q must be true because p q is true when p is false and q is true we conclude that these specifications are consistent because they are all true when p is false and q is true we could come to the same conclusion by use of a truth table to examine the four possible assignments of truth values to p and q example do the system specifications in example remain consistent if the specification the diagnostic message is not retransmitted is added solution by the reasoning in example the three specifications from that example are true only in the case when p is false and q is true however this new specification is q which is false when q is true consequently these four specifications are inconsistent boolean searches logical connectives are used extensively in searches of large collections of information such as indexes of web pages because these searches employ techniques from propositional logic they are called boolean searches in boolean searches the connective and is used to match records that contain both of two search terms the connective or is used to match one or both of two search terms and the connective not sometimes written as and not is used to exclude a particular search term careful planning of how logical connectives are used is often required when boolean searches are used to locate information of potential interest example illustrates how boolean searches are carried out example web page searching most web search engines support boolean searching techniques which usually can help find web pages about particular subjects for instance using boolean searching to find web pages about universities in new mexico we can look for pages matching new and mexico and universities the results of this search will include those pages that contain the three words new mexico and universities this will include all of the pages of interest together with others such as a page about new universities in mexico note that in google and many other search engines the word and is not needed although it is understood because all search terms are included by default these search engines also support the use of quotation marks to search for specific phrases so it may be more effective to search for pages matching new mexico and universities next to find pages that deal with universities in new mexico or arizona we can search for pages matching new and mexico or arizona and universities note here the and operator takes precedence over the or operator also in google the terms used for this search would be new mexico or arizona the results of this search will include all pages that contain the word universities and either both the words new and mexico or the word arizona again pages besides those of interest will be listed finally to find web pages that deal with universities in mexico and not new mexico we might first look for pages matching mexico and universities but because the results of this search will include pages about universities in new mexico as well as universities in mexico it might be better to search for pages matching mexico and universities not new the results of this search include pages that contain both the words mexico and universities but do not contain the word new in google and many other search engines the word not is replaced by the symbol in google the terms used for this last search would be mexico universities new logic puzzles puzzles that can be solved using logical reasoning are known as logic puzzles solving logic puzzles is an excellent way to practice working with the rules of logic also computer programs designed to carry out logical reasoning often use well known logic puzzles to illustrate their capabilities many people enjoy solving logic puzzles published in periodicals books and on the web as a recreational activity we will discuss two logic puzzles here we begin with a puzzle originally posed by raymond smullyan a master of logic puzzles who has published more than a dozen books containing challenging puzzles that involve logical reasoning in section we will also discuss the extremely popular logic puzzle sudoku example in smullyan posed many puzzles about an island that has two kinds of inhabitants knights who always tell the truth and their opposites knaves who always lie you encounter two people a and b what are a and b if a says b is a knight and b says the two of us are opposite types solution let p and q be the statements that a is a knight and b is a knight respectively so that p and q are the statements that a is a knave and b is a knave respectively we first consider the possibility that a is a knight this is the statement that p is true if a is a knight then he is telling the truth when he says that b is a knight so that q is true and a and b are the same type however if b is a knight then b statement that a and b are of opposite types the statement p q p q would have to be true which it is not because a and b are both knights consequently we can conclude that a is not a knight that is that p is false if a is a knave then because everything a knave says is false a statement that b is a knight that is that q is true is a lie this means that q is false and b is also a knave furthermore if b is a knave then b statement that a and b are opposite types is a lie which is consistent with both a and b being knaves we can conclude that both a and b are knaves we pose more of smullyan puzzles about knights and knaves in exercises in exercises we introduce related puzzles where we have three types of people knights and knaves as in this puzzle together with spies who can lie next we pose a puzzle known as the muddy children puzzle for the case of two children example a father tells his two children a boy and a girl to play in their backyard without getting dirty however while playing both children get mud on their foreheads when the children stop playing the father says at least one of you has a muddy forehead and then asks the children to answer yes or no to the question do you know whether you have a muddy forehead the father asks this question twice what will the children answer each time this question is asked assuming that a child can see whether his or her sibling has a muddy forehead but cannot see his or her own forehead assume that both children are honest and that the children answer each question simultaneously solution let be the statement that the son has a muddy forehead and let d be the statement that the daughter has a muddy forehead when the father says that at least one of the two children has a muddy forehead he is stating that the disjunction d is true both children will answer no the first time the question is asked because each sees mud on the other child forehead that is the son knows that d is true but does not know whether is true and the daughter knows that is true but does not know whether d is true after the son has answered no to the first question the daughter can determine that d must be true this follows because when the first question is asked the son knows that d is true but cannot determine whether is true using this information the daughter can conclude that d must be true for if d were false the son could have reasoned that because d is true then must be true and he would have answered yes to the first question the son can reason in a similar way to determine that must be true it follows that both children answer yes the second time the question is asked in chapter we design some useful circuits logic circuits propositional logic can be applied to the design of computer hardware this was first observed in by claude shannon in his mit master thesis in chapter we will study this topic in depth see that chapter for a biography of shannon we give a brief introduction to this application here a logic circuit or digital circuit receives input signals pn each a bit either off or on and produces output signals sn each a bit in this section we will restrict our attention to logic circuits with a single output signal in general digital circuits may have multiple outputs raymond smullyan born raymond smullyan dropped out of high school he wanted to study what he was really interested in and not standard high school material after jumping from one university to the next he earned an undergraduate degree in mathematics at the university of chicago in he paid his college expenses by performing magic tricks at parties and clubs he obtained a ph d in logic in at princeton studying under alonzo church after graduating from princeton he taught mathematics and logic at dartmouth college princeton university yeshiva university and the city university of new york he joined the philosophy department at indiana university in where he is now an emeritus professor smullyan has written many books on recreational logic and mathematics including satan cantor and infinity what is the name of this book the lady or the tiger alice in puzzleland to mock a mockingbird forever undecided and the riddle of scheherazade amazing logic puzzles ancient and modern because his logic puzzles are challenging entertaining and thought provoking he is considered to be a modern day lewis carroll smullyan has also written several books about the application of deductive logic to chess three collections of philosophical essays and aphorisms and several advanced books on mathematical logic and set theory he is particularly interested in self reference and has worked on extending some of gödel results that show that it is impossible to write a computer program that can solve all mathematical problems he is also particularly interested in explaining ideas from mathematical logic to the public smullyan is a talented musician and often plays piano with his wife who is a concert level pianist making telescopes is one of his hobbies he is also interested in optics and stereo photography he states i ve never had a conflict between teaching and research as some people do because when i m teaching i m doing research smullyan is the subject of a documentary short film entitled this film needs no title p p p q p q p p q q inverter or gate and gate figure basic logic gates p q p q r r r figure a combinatorial circuit complicated digital circuits can be constructed from three basic circuits called gates shown in figure the inverter or not gate takes an input bit p and produces as output p the or gate takes two input signals p and q each a bit and produces as output the signal p q finally the and gate takes two input signals p and q each a bit and produces as output the signal p q we use combinations of these three basic gates to build more complicated circuits such as that shown in figure given a circuit built from the basic logic gates and the inputs to the circuit we determine the output by tracing through the circuit as example shows example determine the output for the combinatorial circuit in figure solution in figure we display the output of each logic gate in the circuit we see that the and gate takes input of p and q the output of the inverter with input q and produces p q next we note that the or gate takes input p q and r the output of the inverter with input r and produces the final output p q r suppose that we have a formula for the output of a digital circuit in terms of negations disjunctions and conjunctions then we can systematically build a digital circuit with the desired output as illustrated in example example build a digital circuit that produces the output p r p q r when given input bits p q and r solution to construct the desired circuit we build separate circuits for p r and for p q r and combine them using an and gate to construct a circuit for p r we use an inverter to produce r from the input r then we use an or gate to combine p and r to build a circuit for p q r we first use an inverter to obtain r then we use an or gate with inputs q and r to obtain q r finally we use another inverter and an or gate to get p q r from the inputs p and q r to complete the construction we employ a final and gate with inputs p r and p q r the resulting circuit is displayed in figure we will study logic circuits in great detail in chapter in the context of boolean algebra and with different notation p r p q r figure the circuit for p r p q r p r p q r exercises in exercises translate the given statement into proposi tional logic using the propositions provided you cannot edit a protected wikipedia entry unless you are an administrator express your answer in terms of e you can edit a protected wikipedia entry and a you are an administrator you can see the movie only if you are over years old or you have the permission of a parent express your an swer in terms of m you can see the movie e you are over years old and p you have the permission of a parent you can graduate only if you have completed the require ments of your major and you do not owe money to the university and you do not have an overdue library book express your answer in terms of g you can graduate m you owe money to the university r you have com pleted the requirements of your major and b you have an overdue library book to use the wireless network in the airport you must pay the daily fee unless you are a subscriber to the service express your answer in terms of w you can use the wire less network in the airport d you pay the daily fee and you are a subscriber to the service you are eligible to be president of the u s a only if you are at least years old were born in the u s a or at the time of your birth both of your parents were citizens and you have lived at least years in the country express your answer in terms of e you are eligible to be pres ident of the u s a a you are at least years old b you were born in the u s a p at the time of your birth both of your parents where citizens and r you have lived at least years in the u s a you can upgrade your operating system only if you have a bit processor running at ghz or faster at least gb ram and gb free hard disk space or a bit processor running at ghz or faster at least gb ram and at least gb free hard disk space express you answer in terms of u you can upgrade your oper ating system you have a bit processor you have a bit processor your processor runs at ghz or faster your processor runs at ghz or faster your processor has at least gb ram your processor has at least gb ram you have at least gb free hard disk space and you have at least gb free hard disk space express these system specifications using the proposi tions p the message is scanned for viruses and q the message was sent from an unknown system together with logical connectives including negations a the message is scanned for viruses whenever the message was sent from an unknown system b the message was sent from an unknown system but it was not scanned for viruses c it is necessary to scan the message for viruses when ever it was sent from an unknown system d when a message is not sent from an unknown system it is not scanned for viruses express these system specifications using the proposi tions p the user enters a valid password q access is granted and r the user has paid the subscription fee and logical connectives including negations a the user has paid the subscription fee but does not enter a valid password b access is granted whenever the user has paid the subscription fee and enters a valid password c access is denied if the user has not paid the subscrip tion fee d if the user has not entered a valid password but has paid the subscription fee then access is granted are these system specifications consistent the system is in multiuser state if and only if it is operating normally if the system is operating normally the kernel is func tioning the kernel is not functioning or the system is in interrupt mode if the system is not in multiuser state then it is in interrupt mode the system is not in interrupt mode are these system specifications consistent whenever the system software is being upgraded users cannot ac cess the file system if users can access the file system then they can save new files if users cannot save new files then the system software is not being upgraded are these system specifications consistent the router can send packets to the edge system only if it supports the new address space for the router to support the new ad dress space it is necessary that the latest software release be installed the router can send packets to the edge sys tem if the latest software release is installed the router does not support the new address space are these system specifications consistent if the file system is not locked then new messages will be queued if the file system is not locked then the system is func tioning normally and conversely if new messages are not queued then they will be sent to the message buffer if the file system is not locked then new messages will be sent to the message buffer new messages will not be sent to the message buffer what boolean search would you use to look for web pages about beaches in new jersey what if you wanted to find web pages about beaches on the isle of jersey in the english channel what boolean search would you use to look for web pages about hiking in west virginia what if you wanted to find web pages about hiking in virginia but not in west virginia each inhabitant of a remote village always tells the truth or always lies a villager will give only a yes or a no response to a question a tourist asks suppose you are a tourist visiting this area and come to a fork in the road one branch leads to the ruins you want to visit the other branch leads deep into the jungle a villager is standing at the fork in the road what one question can you ask the villager to determine which branch to take an explorer is captured by a group of cannibals there are two types of cannibals those who always tell the truth and those who always lie the cannibals will barbecue the explorer unless he can determine whether a particu lar cannibal always lies or always tells the truth he is allowed to ask the cannibal exactly one question a explain why the question are you a liar does not work b find a question that the explorer can use to determine whether the cannibal always lies or always tells the truth when three professors are seated in a restaurant the host ess asks them does everyone want coffee the first professor says i do not know the second professor then says i do not know finally the third professor says no not everyone wants coffee the hostess comes back and gives coffee to the professors who want it how did she figure out who wanted coffee when planning a party you want to know whom to in vite among the people you would like to invite are three touchy friends you know that if jasmine attends she will become unhappy if samir is there samir will attend only if kanti will be there and kanti will not attend unless jas mine also does which combinations of these three friends can you invite so as not to make someone unhappy exercises relate to inhabitants of the island of knights and knaves created by smullyan where knights always tell the truth and knaves always lie you encounter two people a and b determine if possible what a and b are if they address you in the ways described if you cannot determine what these two people are can you draw any conclusions a says at least one of us is a knave and b says nothing a says the two of us are both knights and b says a is a knave a says i am a knave or b is a knight and b says nothing both a and b say i am a knight a says we are both knaves and b says nothing exercises relate to inhabitants of an island on which there are three kinds of people knights who always tell the truth knaves who always lie and spies called normals by smullyan who can either lie or tell the truth you encounter three people a b and c you know one of these people is a knight one is a knave and one is a spy each of the three people knows the type of person each of other two is for each of these situations if possible determine whether there is a unique solution and determine who the knave knight and spy are when there is no unique solution list all possible solutions or state that there are no solutions a says c is the knave b says a is the knight and c says i am the spy a says i am the knight b says i am the knave and c says b is the knight a says i am the knave b says i am the knave and c says i am the knave a says i am the knight b says a is telling the truth and c says i am the spy a says i am the knight b says a is not the knave and c says b is not the knave a says i am the knight b says i am the knight and c says i am the knight a says i am not the spy b says i am not the spy and c says a is the spy a says i am not the spy b says i am not the spy and c says i am not the spy exercises are puzzles that can be solved by translating statements into logical expressions and reasoning from these expressions using truth tables the police have three suspects for the murder of mr cooper mr smith mr jones and mr williams smith jones and williams each declare that they did not kill cooper smith also states that cooper was a friend of jones and that williams disliked him jones also states that he did not know cooper and that he was out of town the day cooper was killed williams also states that he saw both smith and jones with cooper the day of the killing and that either smith or jones must have killed him can you determine who the murderer was if a one of the three men is guilty the two innocent men are telling the truth but the statements of the guilty man may or may not be true b innocent men do not lie steve would like to determine the relative salaries of three coworkers using two facts first he knows that if fred is not the highest paid of the three then janice is sec ond he knows that if janice is not the lowest paid then maggie is paid the most is it possible to determine the relative salaries of fred maggie and janice from what steve knows if so who is paid the most and who the least explain your reasoning five friends have access to a chat room is it possible to determine who is chatting if the following information is known either kevin or heather or both are chatting either randy or vijay but not both are chatting if abby is chatting so is randy vijay and kevin are either both chatting or neither is if heather is chatting then so are abby and kevin explain your reasoning a detective has interviewed four witnesses to a crime from the stories of the witnesses the detective has con cluded that if the butler is telling the truth then so is the cook the cook and the gardener cannot both be telling the truth the gardener and the handyman are not both lying and if the handyman is telling the truth then the cook is lying for each of the four witnesses can the detective de termine whether that person is telling the truth or lying explain your reasoning four friends have been identified as suspects for an unau thorized access into a computer system they have made statements to the investigating authorities alice said carlos did it john said i did not do it carlos said diana did it diana said carlos lied when he said that i did it a if the authorities also know that exactly one of the four suspects is telling the truth who did it explain your reasoning b if the authorities also know that exactly one is lying who did it explain your reasoning suppose there are signs on the doors to two rooms the sign on the first door reads in this room there is a lady and in the other one there is a tiger and the sign on the second door reads in one of these rooms there is a lady and in one of them there is a tiger suppose that you know that one of these signs is true and the other is false behind which door is the lady solve this famous logic puzzle attributed to albert ein stein and known as the zebra puzzle five men with different nationalities and with different jobs live in con secutive houses on a street these houses are painted dif ferent colors the men have different pets and have dif ferent favorite drinks determine who owns a zebra and whose favorite drink is mineral water which is one of the favorite drinks given these clues the englishman lives in the red house the spaniard owns a dog the japanese man is a painter the italian drinks tea the norwegian lives in the first house on the left the green house is immediately to the right of the white one the photogra pher breeds snails the diplomat lives in the yellow house milk is drunk in the middle house the owner of the green house drinks coffee the norwegian house is next to the blue one the violinist drinks orange juice the fox is in a house next to that of the physician the horse is in a house next to that of the diplomat hint make a table where the rows represent the men and columns represent the color of their houses their jobs their pets and their favorite drinks and use logical reasoning to determine the correct entries in the table freedonia has fifty senators each senator is either honest or corrupt suppose you know that at least one of the free donian senators is honest and that given any two free donian senators at least one is corrupt based on these facts can you determine how many freedonian senators are honest and how many are corrupt if so what is the answer find the output of each of these combinatorial circuits a p q b p p q find the output of each of these combinatorial circuits a p q r b p q p r construct a combinatorial circuit using inverters or gates and and gates that produces the output p r q r from input bits p q and r construct a combinatorial circuit using inverters or gates and and gates that produces the output p r q p q r from input bits p propositional equivalences introduction an important type of step used in a mathematical argument is the replacement of a statement with another statement with the same truth value because of this methods that produce propo sitions with the same truth value as a given compound proposition are used extensively in the construction of mathematical arguments note that we will use the term compound proposi tion to refer to an expression formed from propositional variables using logical operators such as p q we begin our discussion with a classification of compound propositions according to their possible truth values definition tautologies and contradictions are often important in mathematical reasoning example illus trates these types of compound propositions example we can construct examples of tautologies and contradictions using just one propositional vari able consider the truth tables of p p and p p shown in table because p p is always true it is a tautology because p p is always false it is a contradiction logical equivalences compound propositions that have the same truth values in all possible cases are called logically equivalent we can also define this notion as follows definition remark the symbol is not a logical connective and p q is not a compound proposition but rather is the statement that p q is a tautology the symbol is sometimes used instead of to denote logical equivalence one way to determine whether two compound propositions are equivalent is to use a truth table in particular the compound propositions p and q are equivalent if and only if the columns table examples of a tautology and a contradiction p p p p p p t f f t t t f f giving their truth values agree example illustrates this method to establish an extremely important and useful logical equivalence namely that of p q with p q this logical equivalence is one of the two de morgan laws shown in table named after the english mathematician augustus de morgan of the mid nineteenth century example show that p q and p q are logically equivalent solution the truth tables for these compound propositions are displayed in table because the truth values of the compound propositions p q and p q agree for all possible combinations of the truth values of p and q it follows that p q p q is a tautology and that these compound propositions are logically equivalent table truth tables for p q and p q p q p q p q p q p q t t t f f f f t f t f f t f f t t f t f f f f f t t t t example show that p q and p q are logically equivalent solution we construct the truth table for these compound propositions in table because the truth values of p q and p q agree they are logically equivalent table p q truth tables for p q and p q p p q p q t t f t t t f f f f f t t t t f f t t t we will now establish a logical equivalence of two compound propositions involving three different propositional variables p q and r to use a truth table to establish such a logical equivalence we need eight rows one for each possible combination of truth values of these three variables we symbolically represent these combinations by listing the truth values of p q and r respectively these eight combinations of truth values are ttt ttf tft tff ftt ftf fft and fff we use this order when we display the rows of the truth table note that we need to double the number of rows in the truth tables we use to show that compound propositions are equivalent for each additional propositional variable so that rows are needed to establish the logical equivalence of two compound propositions involving four propositional variables and so on in general rows are required if a compound proposition involves n propositional variables table a demonstration that equivalent p q r and p q p r are logically p q r q r p q r p q p r p q p r t t t t t t t t t t f f t t t t t f t f t t t t t f f f t t t t f t t t t t t t f t f f f t f f f f t f f f t f f f f f f f f f example show that p q r and p q p r are logically equivalent this is the distributive law of disjunction over conjunction the identities in table are a special case of boolean algebra identities found in table of section see table in section for analogous set identities solution we construct the truth table for these compound propositions in table because the truth values of p q r and p q p r agree these compound propositions are logically equivalent table contains some important equivalences in these equivalences t denotes the com pound proposition that is always true and f denotes the compound proposition that is always false we also display some useful equivalences for compound propositions involving condi tional statements and biconditional statements in tables and respectively the reader is asked to verify the equivalences in tables in the exercises the associative law for disjunction shows that the expression p q r is well defined in the sense that it does not matter whether we first take the disjunction of p with q and then the disjunction of p q with r or if we first take the disjunction of q and r and then take the disjunction of p with q r similarly the expression p q r is well defined by extending this reasoning it follows that pn and pn are well defined whenever pn are propositions furthermore note that de morgan laws extend to pn pn and pn pn we will sometimes use the notation vn pj for pn and n pj for written concisely as vn pj n pj and n pj vn pj methods for proving these identities will be given in section when using de morgan laws remember to change the logical connective after you negate using de morgan laws the two logical equivalences known as de morgan laws are particularly important they tell us how to negate conjunctions and how to negate disjunctions in particular the equivalence p q p q tells us that the negation of a disjunction is formed by taking the con junction of the negations of the component propositions similarly the equivalence p q p q tells us that the negation of a conjunction is formed by taking the disjunction of the negations of the component propositions example illustrates the use of de morgan laws example use de morgan laws to express the negations of miguel has a cellphone and he has a laptop computer and heather will go to the concert or steve will go to the concert solution let p be miguel has a cellphone and q be miguel has a laptop computer then miguel has a cellphone and he has a laptop computer can be represented by p q by the first of de morgan laws p q is equivalent to p q consequently we can express the negation of our original statement as miguel does not have a cellphone or he does not have a laptop computer let r be heather will go to the concert and be steve will go to the concert then heather will go to the concert or steve will go to the concert can be represented by r by the second of de morgan laws r is equivalent to r consequently we can express the negation of our original statement as heather will not go to the concert and steve will not go to the concert constructing new logical equivalences the logical equivalences in table as well as any others that have been established such as those shown in tables and can be used to construct additional logical equivalences the reason for this is that a proposition in a compound proposition can be replaced by a compound proposition that is logically equivalent to it without changing the truth value of the original compound proposition this technique is illustrated in examples where we also use the fact that if p and q are logically equivalent and q and r are logically equivalent then p and r are logically equivalent see exercise example show that p q and p q are logically equivalent solution we could use a truth table to show that these compound propositions are equivalent similar to what we did in example indeed it would not be hard to do so however we want to illustrate how to use logical identities that we already know to establish new logical identities something that is of practical importance for establishing equivalences of compound propositions with a large number of variables so we will establish this equivalence by developing a series of augustus de morgan augustus de morgan was born in india where his father was a colonel in the indian army de morgan family moved to england when he was months old he attended private schools where in his early teens he developed a strong interest in mathematics de morgan studied at trinity college cambridge graduating in although he considered medicine or law he decided on mathematics for his career he won a position at university college london in but resigned after the college dismissed a fellow professor without giving reasons however he resumed this position in when his successor died remaining until de morgan was a noted teacher who stressed principles over techniques his students included many famous mathematicians including augusta ada countess of lovelace who was charles babbage collaborator in his work on computing machines see page for biographical notes on augusta ada de morgan cautioned the countess against studying too much mathematics because it might interfere with her childbearing abilities de morgan was an extremely prolific writer publishing more than articles in more than periodicals de morgan also wrote textbooks on many subjects including logic probability calculus and algebra in he presented what was perhaps the first clear explanation of an important proof technique known as mathematical induction discussed in section of this text a term he coined in the de morgan made fundamental contributions to the development of symbolic logic he invented notations that helped him prove propositional equivalences such as the laws that are named after him in de morgan presented what is considered to be the first precise definition of a limit and developed new tests for convergence of infinite series de morgan was also interested in the history of mathematics and wrote biographies of newton and halley in de morgan married sophia frend who wrote his biography in de morgan research writing and teaching left little time for his family or social life nevertheless he was noted for his kindness humor and wide range of knowledge logical equivalences using one of the equivalences in table at a time starting with p q and ending with p q we have the following equivalences p q p q by example p q by the second de morgan law p q by the double negation law example show that p p q and p q are logically equivalent by developing a series of logical equivalences solution we will use one of the equivalences in table at a time starting with p p q and ending with p q note we could also easily establish this equivalence using a truth table we have the following equivalences p p q p p q by the second de morgan law p p q by the first de morgan law p p q by the double negation law p p p q by the second distributive law f p q because p p f p q f by the commutative law for disjunction p q by the identity law for f consequently p p q and p q are logically equivalent example show that p q p q is a tautology solution to show that this statement is a tautology we will use logical equivalences to demon strate that it is logically equivalent to t note this could also be done using a truth table p q p q p q p q by example p q p q by the first de morgan law p p q q by the associative and commutative laws for disjunction t t by example and the commutative law for disjunction t by the domination law propositional satisfiability a compound proposition is satisfiable if there is an assignment of truth values to its variables that makes it true when no such assignments exists that is when the compound proposition is false for all assignments of truth values to its variables the compound proposition is unsatisfiable note that a compound proposition is unsatisfiable if and only if its negation is true for all assignments of truth values to the variables that is if and only if its negation is a tautology when we find a particular assignment of truth values that makes a compound proposition true we have shown that it is satisfiable such an assignment is called a solution of this particular satisfiability problem however to show that a compound proposition is unsatisfiable we need to show that every assignment of truth values to its variables makes it false although we can always use a truth table to determine whether a compound proposition is satisfiable it is often more efficient not to as example demonstrates example determine whether each of the compound propositions p q q r r p p q r p q r and p q q r r p p q r p q r is satisfiable solution instead of using truth table to solve this problem we will reason about truth values note that p q q r r p is true when the three variable p q and r have the same truth value see exercise of section hence it is satisfiable as there is at least one assignment of truth values for p q and r that makes it true similarly note that p q r p q r is true when at least one of p q and r is true and at least one is false see exercise of section hence p q r p q r is satisfiable as there is at least one assignment of truth values for p q and r that makes it true finally note that for p q q r r p p q r p q r to be true p q q r r p and p q r p q r must both be true for the first to be true the three variables must have the same truth values and for the second to be true at least one of three variables must be true and at least one must be false however these conditions are contradictory from these observations we conclude that no assignment of truth values to p q and r makes p q q r r p p q r p q r true hence it is unsatisfiable augusta ada countess of lovelace augusta ada was the only child from the marriage of the famous poet lord byron and lady byron annabella millbanke who separated when ada was month old because of lord byron scandalous affair with his half sister the lord byron had quite a reputation being described by one of his lovers as mad bad and dangerous to know lady byron was noted for her intellect and had a passion for mathematics she was called by lord byron the princess of parallelograms augusta was raised by her mother who encouraged her intellectual talents especially in music and mathematics to counter what lady byron considered dangerous poetic tendencies at this time women were not allowed to attend universities and could not join learned societies nevertheless augusta pursued her mathematical studies independently and with mathematicians including william frend she was also encouraged by another female mathematician mary somerville and in at a dinner party hosted by mary somerville she learned about charles babbage ideas for a calculating machine called the analytic engine in augusta ada married lord king later elevated to earl of lovelace together they had three children augusta ada continued her mathematical studies after her marriage charles babbage had continued work on his analytic engine and lectured on this in europe in babbage asked augusta ada to translate an article in french describing babbage invention when babbage saw her translation he suggested she add her own notes and the resulting work was three times the length of the original the most complete accounts of the analytic engine are found in augusta ada notes in her notes she compared the working of the analytic engine to that of the jacquard loom with babbage punch cards analogous to the cards used to create patterns on the loom furthermore she recognized the promise of the machine as a general purpose computer much better than babbage did she stated that the engine is the material expression of any indefinite function of any degree of generality and complexity her notes on the analytic engine anticipate many future developments including computer generated music augusta ada published her writings under her initials a a l concealing her identity as a woman as did many women at a time when women were not considered to be the intellectual equals of men after she and babbage worked toward the development of a system to predict horse races unfortunately their system did not work well leaving augusta ada heavily in debt at the time of her death at an unfortunately young age from uterine cancer in augusta ada notes on the analytic engine were republished more than years after they were written and after they had been long forgotten in his work in the on the capacity of computers to think and his famous turing test alan turing responded to augusta ada statement that the analytic engine has no pretensions whatever to originate anything it can do whatever we know how to order it to perform this dialogue between turing and augusta ada is still the subject of controversy because of her fundamental contributions to computing the programming language ada is named in honor of the countess of lovelace figure sudoku puzzle applications of satisfiability many problems in diverse areas such as robotics software testing computer aided design machine vision integrated circuit design computer networking and genetics can be modeled in terms of propositional satisfiability although most of these applications are beyond the scope of this book we will study one application here in particular we will show how to use propositional satisfiability to model sudoku puzzles sudoku a sudoku puzzle is represented by a grid made up of nine subgrids known as blocks as shown in figure for each puzzle some of the cells called givens are assigned one of the numbers and the other cells are blank the puzzle is solved by assigning a number to each blank cell so that every row every column and every one of the nine blocks contains each of the nine possible numbers note that instead of using a grid sudoku puzzles can be based on grids for any positive integer n with the grid made up of n n subgrids the popularity of sudoku dates back to the when it was introduced in japan it took years for sudoku to spread to rest of the world but by sudoku puzzles were a worldwide craze the name sudoku is short for the japanese suuji wa dokushin ni kagiru which means the digits must remain single the modern game of sudoku was apparently designed in the late by an american puzzle designer the basic ideas of sudoku date back even further puzzles printed in french newspapers in the were quite similar but not identical to modern sudoku sudoku puzzles designed for entertainment have two additional important properties first they have exactly one solution second they can be solved using reasoning alone that is without resorting to searching all possible assignments of numbers to the cells as a sudoku puzzle is solved entries in blank cells are successively determined by already known values for instance in the grid in figure the number must appear in exactly one cell in the second row how can we determine which of the seven blank cells it must appear first we observe that cannot appear in one of the first three cells or in one of the last three cells of this row because it already appears in another cell in the block each of these cells is in we can also see that cannot appear in the fifth cell in this row as it already appears in the fifth column in the fourth row this means that must appear in the sixth cell of the second row many strategies based on logic and mathematics have been devised for solving sudoku puzzles see for example here we discuss one of the ways that have been developed for solving sudoku puzzles with the aid of a computer which depends on modeling the puzzle as a propositional satisfiability problem using the model we describe particular sudoku puzzles can be solved using software developed to solve satisfiability problems currently sudoku puzzles can be solved in less than milliseconds this way it should be noted that there are many other approaches for solving sudoku puzzles via computers using other techniques to encode a sudoku puzzle let p i j n denote the proposition that is true when the number n is in the cell in the ith row and j th column there are such propositions as i j and n all range from to for example for the puzzle in figure the number is given as the value in the fifth row and first column hence we see that p is true but p j is false for j given a particular sudoku puzzle we begin by encoding each of the given values then we construct compound propositions that assert that every row contains every number every column contains every number every block contains every number and each cell contains no more than one number it follows as the reader should verify that the sudoku puzzle is solved by finding an assignment of truth values to the propositions p i j n with i j and n each ranging from to that makes the conjunction of all these compound propositions true after listing these assertions we will explain how to construct the assertion that every row contains every integer from to we will leave the construction of the other assertions that every column contains every number and each of the nine blocks contains every number to the exercises for each cell with a given value we assert p i j n when the cell in row i and column j has the given value n we assert that every row contains every number p i j n i n j we assert that every column contains every number it is tricky setting up the two inner indices so that all nine cells in each square block are examined p i j n j n i we assert that each of the nine blocks contains every number p i j n r n i j to assert that no cell contains more than one number we take the conjunction over all values of n nt i and j where each variable ranges from to and n nt of p i j n p i j nt we now explain how to construct the assertion that every row contains every number first to assert that row i contains the number n we form p i j n to assert that possible values of n giving us j p i j n finally to assert that every row contains every number we take the conjunction of j p i j n over all nine rows this gives i n j p i j n exercises and ask for explanations of the assertions that every column contains every number and that each of the nine blocks contains every number given a particular sudoku puzzle to solve this puzzle we can find a solution to the satisfia bility problems that asks for a set of truth values for the variables p i j n that makes the conjunction of all the listed assertions true exercises solving satisfiability problems a truth table can be used to determine whether a compound proposition is satisfiable or equiv alently whether its negation is a tautology see exercise this can be done by hand for a compound proposition with a small number of variables but when the number of variables grows this becomes impractical for instance there are rows in the truth ta ble for a compound proposition with variables clearly you need a computer to help you determine in this way whether a compound proposition in variables is satisfiable when many applications are modeled questions concerning the satisfiability of compound propositions with hundreds thousands or millions of variables arise note for example that when there are variables checking every one of the a number with more than decimal digits possible combinations of truth values of the variables in a compound proposition cannot be done by a computer in even trillions of years no procedure is known that a com puter can follow to determine in a reasonable amount of time whether an arbitrary compound proposition in such a large number of variables is satisfiable however progress has been made developing methods for solving the satisfiability problem for the particular types of compound propositions that arise in practical applications such as for the solution of sudoku puzzles many computer programs have been developed for solving satisfiability problems which have practical use in our discussion of the subject of algorithms in chapter we will discuss this question further in particular we will explain the important role the propositional satisfiability problem plays in the study of the complexity of algorithms use truth tables to verify these equivalences a p t p b p f p c p f f d p t t e p p p f p p p show that p and p are logically equivalent use truth tables to verify the commutative laws a p q q p b p q q p use truth tables to verify the associative laws a p q r p q r b p q r p q r use a truth table to verify the distributive law p q r p q p r use a truth table to verify the first de morgan law p q p q use de morgan laws to find the negation of each of the following statements a jan is rich and happy b carlos will bicycle or run tomorrow henry maurice sheffer henry maurice sheffer born to jewish parents in the western ukraine emigrated to the united states in with his parents and six siblings he studied at the boston latin school before entering harvard where he completed his undergraduate degree in his master in and his ph d in philosophy in after holding a postdoctoral position at harvard henry traveled to europe on a fellowship upon returning to the united states he became an academic nomad spending one year each at the university of washington cornell the university of minnesota the university of missouri and city college in new york in he returned to harvard as a faculty member in the philosophy department he remained at harvard until his retirement in sheffer introduced what is now known as the sheffer stroke in it became well known only after its use in the edition of whitehead and russell principia mathematica in this same edition russell wrote that sheffer had invented a powerful method that could be used to simplify the principia because of this comment sheffer was something of a mystery man to logicians especially because sheffer who published little in his career never published the details of this method only describing it in mimeographed notes and in a brief published abstract sheffer was a dedicated teacher of mathematical logic he liked his classes to be small and did not like auditors when strangers appeared in his classroom sheffer would order them to leave even his colleagues or distinguished guests visiting harvard sheffer was barely five feet tall he was noted for his wit and vigor as well as for his nervousness and irritability although widely liked he was quite lonely he is noted for a quip he spoke at his retirement old professors never die they just become emeriti sheffer is also credited with coining the term boolean algebra the subject of chapter of this text sheffer was briefly married and lived most of his later life in small rooms at a hotel packed with his logic books and vast files of slips of paper he used to jot down his ideas unfortunately sheffer suffered from severe depression during the last two decades of his life c mei walks or takes the bus to class d ibrahim is smart and hard working use de morgan laws to find the negation of each of the following statements a kwame will take a job in industry or go to graduate school b show that p q q r p r is a tautol show that p q p r q r is a tautology show that p q r and p q r are not log ically equivalent show that p q r and p r q r are not yoshiko knows java and calculus c james is young and strong d rita will move to oregon or washington show that each of these conditional statements is a tau tology by using truth tables a p q p b p p q c p p q d p q p q e p q p f p q q show that each of these conditional statements is a tau tology by using truth tables a p p q q b p q q r p r c p p q q logically equivalent show that p q r and p r q are not logically equivalent the dual of a compound proposition that contains only the logical operators and is the compound proposition obtained by replacing each by each by each t by f and each f by t the dual of is denoted by find the dual of each of these compound propositions a p q b p q r t c p q q f find the dual of each of these compound propositions a p q r b p q r d p q p r q r r show that each conditional statement in exercise is a tautology without using truth tables show that each conditional statement in exercise is a tautology without using truth tables use truth tables to verify the absorption laws a p p q p b p p q p determine whether p p q q is a tautol determine whether q p q p is a tautol each of exercises asks you to show that two compound propositions are logically equivalent to do this either show that both sides are true or that both sides are false for exactly the same combinations of truth values of the propositional variables in these expressions whichever is easier show that p q and p q p q are logically equivalent show that p q and p q are logically equiva lent show that p q and q p are logically equivalent show that p q and p q are logically equivalent show that p q and p q are logically equivalent show that p q and p q are logically equiva lent show that p q p r and p q r are log ically equivalent show that p r q r and p q r are log ically equivalent show that p q p r and p q r are log ically equivalent show that p r q r and p q r are log ically equivalent show that p q r and q p r are logically equivalent show that p q and p q q p are logically equivalent show that p q and p q are logically equivalent c p f q t when does where is a compound proposition show that when is a compound proposition show that the logical equivalences in table except for the double negation law come in pairs where each pair contains compound propositions that are duals of each other why are the duals of two equivalent compound proposi tions also equivalent where these compound propositions contain only the operators and find a compound proposition involving the propositional variables p q and r that is true when p and q are true and r is false but is false otherwise hint use a con junction of each propositional variable or its negation find a compound proposition involving the propositional variables p q and r that is true when exactly two of p q and r are true and is false otherwise hint form a dis junction of conjunctions include a conjunction for each combination of values for which the compound proposi tion is true each conjunction should include each of the three propositional variables or its negations suppose that a truth table in n propositional variables is specified show that a compound proposition with this truth table can be formed by taking the disjunction of conjunctions of the variables or their negations with one conjunction included for each combination of values for which the compound proposition is true the resulting compound proposition is said to be in disjunctive nor mal form a collection of logical operators is called functionally com plete if every compound proposition is logically equivalent to a compound proposition involving only these logical opera tors show that and form a functionally complete col lection of logical operators hint use the fact that every compound proposition is logically equivalent to one in disjunctive normal form as shown in exercise show that and form a functionally complete col lection of logical operators hint first use a de mor gan law to show that p q is logically equivalent to p q show that and form a functionally complete collec tion of logical operators the following exercises involve the logical operators nand and nor the proposition p nand q is true when either p or q or both are false and it is false when both p and q are true the proposition p nor q is true when both p and q are false and it is false otherwise the propositions p nand q and p nor q are denoted by p q and p q respectively the operators and are called the sheffer stroke and the peirce arrow after h m sheffer and c s peirce respec tively construct a truth table for the logical operator nand show that p q is logically equivalent to p q construct a truth table for the logical operator nor show that p q is logically equivalent to p q in this exercise we will show that is a functionally complete collection of logical operators a show that p p is logically equivalent to p b show that p q p q is logically equivalent to p q c conclude from parts a and b and exercise that is a functionally complete collection of logical operators find a compound proposition logically equivalent to p q using only the logical operator show that is a functionally complete collection of log ical operators show that p q and q p are equivalent show that p q r and p q r are not equivalent so that the logical operator is not associative how many different truth tables of compound proposi tions are there that involve the propositional variables p and q show that if p q and r are compound propositions such that p and q are logically equivalent and q and r are log ically equivalent then p and r are logically equivalent the following sentence is taken from the specification of a telephone system if the directory database is opened then the monitor is put in a closed state if the system is not in its initial state this specification is hard to under stand because it involves two conditional statements find an equivalent easier to understand specification that in volves disjunctions and negations but not conditional statements how many of the disjunctions p q p q q r q r and q r can be made simultaneously true by an assignment of truth values to p q and r how many of the disjunctions p q p r p r p q q r q r p q p r and p r can be made simultaneously true by an assignment of truth values to p q r and show that the negation of an unsatisfiable compound proposition is a tautology and the negation of a compound proposition that is a tautology is unsatisfiable determine whether each of these compound propositions is satisfiable a p q p q p q b p q p q p q p q c p q p q determine whether each of these compound propositions is satisfiable a p q r p q p r p q p q b p q r p q p q p r p q r p r c p q r p q q r p r p q p q r p q p r show how the solution of a given sudoku puzzle can be found by solving a satisfiability problem construct a compound proposition that asserts that ev ery cell of a sudoku puzzle contains at least one number explain the steps in the construction of the compound proposition given in the text that asserts that every col umn of a sudoku puzzle contains every number explain the steps in the construction of the compound proposition given in the text that asserts that each of the nine blocks of a sudoku puzzle contains ev ery number predicates and quantifiers introduction propositional logic studied in sections cannot adequately express the meaning of all statements in mathematics and in natural language for example suppose that we know that every computer connected to the university network is functioning properly no rules of propositional logic allow us to conclude the truth of the statement is functioning properly where is one of the computers connected to the university network likewise we cannot use the rules of propositional logic to conclude from the statement is under attack by an intruder where is a computer on the university network to conclude the truth of there is a computer on the university network that is under attack by an intruder in this section we will introduce a more powerful type of logic called predicate logic we will see how predicate logic can be used to express the meaning of a wide range of statements in mathematics and computer science in ways that permit us to reason and explore relationships between objects to understand predicate logic we first need to introduce the concept of a predicate afterward we will introduce the notion of quantifiers which enable us to reason with statements that assert that a certain property holds for all objects of a certain type and with statements that assert the existence of an object with a particular property predicates statements involving variables such as x x y x y z and computer x is under attack by an intruder and computer x is functioning properly are often found in mathematical assertions in computer programs and in system specifications these statements are neither true nor false when the values of the variables are not specified in this section we will discuss the ways that propositions can be produced from such statements the statement x is greater than has two parts the first part the variable x is the subject of the statement the second part the predicate is greater than refers to a property that the subject of the statement can have we can denote the statement x is greater than by p x where p denotes the predicate is greater than and x is the variable the statement p x is also said to be the value of the propositional function p at x once a value has been assigned to the variable x the statement p x becomes a proposition and has a truth value consider examples and example let p x denote the statement x what are the truth values of p and p solution we obtain the statement p by setting x in the statement x hence p which is the statement is true however p which is the statement is false example let a x denote the statement computer x is under attack by an intruder suppose that of the computers on campus only and are currently under attack by intruders what are truth values of a a and a solution we obtain the statement a by setting x in the statement computer x is under attack by an intruder because is not on the list of computers currently under attack we conclude that a is false similarly because and are on the list of computers under attack we know that a and a are true we can also have statements that involve more than one variable for instance consider the statement x y we can denote this statement by q x y where x and y are variables and q is the predicate when values are assigned to the variables x and y the statement q x y has a truth value example let q x y denote the statement x y what are the truth values of the propositions q and q solution to obtain q set x and y in the statement q x y hence q is the statement which is false the statement q is the proposition which is true charles sanders peirce many consider charles peirce born in cambridge mas sachusetts to be the most original and versatile american intellect he made important contributions to an amazing number of disciplines including mathematics astronomy chemistry geodesy metrology engineer ing psychology philology the history of science and economics peirce was also an inventor a lifelong student of medicine a book reviewer a dramatist and an actor a short story writer a phenomenologist a logician and a metaphysician he is noted as the preeminent system building philosopher competent and productive in logic mathematics and a wide range of sciences he was encouraged by his father benjamin peirce a professor of mathematics and natural philosophy at harvard to pursue a career in science instead he decided to study logic and scientific methodology peirce attended harvard and received a harvard master of arts degree and an advanced degree in chemistry from the lawrence scientific school in peirce became an aide in the u s coast survey with the goal of better understanding scientific methodology his service for the survey exempted him from military service during the civil war while working for the survey peirce did astronomical and geodesic work he made fundamental contributions to the design of pendulums and to map projections applying new mathematical developments in the theory of elliptic functions he was the first person to use the wavelength of light as a unit of measurement peirce rose to the position of assistant for the survey a position he held until forced to resign in when he disagreed with the direction taken by the survey new administration while making his living from work in the physical sciences peirce developed a hierarchy of sciences with mathematics at the top rung in which the methods of one science could be adapted for use by those sciences under it in the hierarchy during this time he also founded the american philosophical theory of pragmatism the only academic position peirce ever held was lecturer in logic at johns hopkins university in baltimore his mathematical work during this time included contributions to logic set theory abstract algebra and the philosophy of mathematics his work is still relevant today with recent applications of this work on logic to artificial intelligence peirce believed that the study of mathematics could develop the mind powers of imagination abstraction and generalization his diverse activities after retiring from the survey included writing for periodicals contributing to scholarly dictionaries translating scientific papers guest lecturing and textbook writing unfortunately his income from these pursuits was insufficient to protect him and his second wife from abject poverty he was supported in his later years by a fund created by his many admirers and administered by the philosopher william james his lifelong friend although peirce wrote and published voluminously in a vast range of subjects he left more than pages of unpublished manuscripts because of the difficulty of studying his unpublished writings scholars have only recently started to understand some of his varied contributions a group of people is devoted to making his work available over the internet to bring a better appreciation of peirce accomplishments to the world example let a c n denote the statement computer c is connected to network n where c is a variable representing a computer and n is a variable representing a network suppose that the computer is connected to network but not to network what are the values of a and a solution because is not connected to the network we see that a is false however because is connected to the network we see that a is true similarly we can let r x y z denote the statement x y z when values are assigned to the variables x y and z this statement has a truth value example what are the truth values of the propositions r and r solution the proposition r is obtained by setting x y and z in the statement r x y z we see that r is the statement which is true also note that r which is the statement is false in general a statement involving the n variables xn can be denoted by p xn a statement of the form p xn is the value of the propositional function p at the n tuple xn and p is also called an n place predicate or a n ary predicate propositional functions occur in computer programs as example demonstrates example consider the statement if x then x x when this statement is encountered in a program the value of the variable x at that point in the execution of the program is inserted into p x which is x if p x is true for this value of x the assignment statement x x is executed so the value of x is increased by if p x is false for this value of x the assignment statement is not executed so the value of x is not changed preconditions and postconditions predicates are also used to establish the correctness of computer programs that is to show that computer programs always produce the desired output when given valid input note that unless the correctness of a computer program is established no amount of testing can show that it produces the desired output for all input values unless every input value is tested the statements that describe valid input are known as preconditions and the conditions that the output should satisfy when the program has run are known as postconditions as example illustrates we use predicates to describe both preconditions and postconditions we will study this process in greater detail in section example consider the following program designed to interchange the values of two variables x and y temp x x y y temp find predicates that we can use as the precondition and the postcondition to verify the correctness of this program then explain how to use them to verify that for all valid input the program does what is intended solution for the precondition we need to express that x and y have particular values before we run the program so for this precondition we can use the predicate p x y where p x y is the statement x a and y b where a and b are the values of x and y before we run the program because we want to verify that the program swaps the values of x and y for all input values for the postcondition we can use q x y where q x y is the statement x b and y a to verify that the program always does what it is supposed to do suppose that the precon dition p x y holds that is we suppose that the statement x a and y b is true this means that x a and y b the first step of the program temp x assigns the value of x to the variable temp so after this step we know that x a temp a and y b after the second step of the program x y we know that x b temp a and y b finally after the third step we know that x b temp a and y a consequently after this program is run the postcondition q x y holds that is the statement x b and y a is true quantifiers when the variables in a propositional function are assigned values the resulting statement becomes a proposition with a certain truth value however there is another important way called quantification to create a proposition from a propositional function quantification expresses the extent to which a predicate is true over a range of elements in english the words all some many none and few are used in quantifications we will focus on two types of quantification here universal quantification which tells us that a predicate is true for every element under consideration and existential quantification which tells us that there is one or more element under consideration for which the predicate is true the area of logic that deals with predicates and quantifiers is called the predicate calculus the universal quantifier many mathematical statements assert that a property is true for all values of a variable in a particular domain called the domain of discourse or the universe of discourse often just referred to as the domain such a statement is expressed using universal quantification the universal quantification of p x for a particular domain is the proposition that asserts that p x is true for all values of x in this domain note that the domain specifies the possible values of the variable x the meaning of the universal quantification of p x changes when we change the domain the domain must always be specified when a universal quantifier is used without it the universal quantification of a statement is not defined definition the meaning of the universal quantifier is summarized in the first row of table we illustrate the use of the universal quantifier in examples table quantifiers statement when true when false xp x xp x p x is true for every x there is an x for which p x is true there is an x for which p x is false p x is false for every x example let p x be the statement x x what is the truth value of the quantification xp x where the domain consists of all real numbers solution because p x is true for all real numbers x the quantification xp x is true remember that the truth value of xp x depends on the domain remark generally an implicit assumption is made that all domains of discourse for quantifiers are nonempty note that if the domain is empty then xp x is true for any propositional function p x because there are no elements x in the domain for which p x is false besides for all and for every universal quantification can be expressed in many other ways including all of for each given any for arbitrary for each and for any remark it is best to avoid using for any x because it is often ambiguous as to whether any means every or some in some cases any is unambiguous such as when it is used in negatives for example there is not any reason to avoid studying a statement xp x is false where p x is a propositional function if and only if p x is not always true when x is in the domain one way to show that p x is not always true when x is in the domain is to find a counterexample to the statement xp x note that a single counterexample is all we need to establish that xp x is false example illustrates how counterexamples are used example let q x be the statement x what is the truth value of the quantification xq x where the domain consists of all real numbers solution q x is not true for every real number x because for instance q is false that is x is a counterexample for the statement xq x thus xq x is false example suppose that p x is to show that the statement xp x is false where the uni verse of discourse consists of all integers we give a counterexample we see that x is a counterexample because when x so that is not greater than when x looking for counterexamples to universally quantified statements is an important activity in the study of mathematics as we will see in subsequent sections of this book when all the elements in the domain can be listed say xn it follows that the universal quantification xp x is the same as the conjunction p p p xn because this conjunction is true if and only if p p p xn are all true example what is the truth value of xp x where p x is the statement and the domain consists of the positive integers not exceeding solution the statement xp x is the same as the conjunction p p p p because the domain consists of the integers and because p which is the statement is false it follows that xp x is false example what does the statement xn x mean if n x is computer x is connected to the network and the domain consists of all computers on campus solution the statement xn x means that for every computer x on campus that computer x is connected to the network this statement can be expressed in english as every computer on campus is connected to the network as we have pointed out specifying the domain is mandatory when quantifiers are used the truth value of a quantified statement often depends on which elements are in this domain as example shows example what is the truth value of x x if the domain consists of all real numbers what is the truth value of this statement if the domain consists of all integers solution the universal quantification x x where the domain consists of all real num bers is false for example note that x if and only if x x x consequently x if and only if x or x it follows that x x is false if the domain consists of all real numbers because the inequality is false for all real numbers x with x however if the domain consists of the integers x x is true because there are no integers x with x the existential quantifier many mathematical statements assert that there is an element with a certain property such statements are expressed using existential quantification with existential quantification we form a proposition that is true if and only if p x is true for at least one value of x in the domain definition a domain must always be specified when a statement xp x is used furthermore the meaning of xp x changes when the domain changes without specifying the domain the statement xp x has no meaning besides the phrase there exists we can also express existential quantification in many other ways such as by using the words for some for at least one or there is the existential quantification xp x is read as there is an x such that p x there is at least one x such that p x or for some xp x the meaning of the existential quantifier is summarized in the second row of table we illustrate the use of the existential quantifier in examples example let p x denote the statement x what is the truth value of the quantification xp x where the domain consists of all real numbers solution because x is sometimes true for instance when x the existential quan tification of p x which is xp x is true observe that the statement xp x is false if and only if there is no element x in the domain for which p x is true that is xp x is false if and only if p x is false for every element of the domain we illustrate this observation in example example let q x denote the statement x x what is the truth value of the quantification xq x where the domain consists of all real numbers remember that the truth value of xp x depends on the domain solution because q x is false for every real number x the existential quantification of q x which is xq x is false remark generally an implicit assumption is made that all domains of discourse for quantifiers are nonempty if the domain is empty then xq x is false whenever q x is a propositional function because when the domain is empty there can be no element x in the domain for which q x is true when all elements in the domain can be listed say xn the existential quan tification xp x is the same as the disjunction p p p xn because this disjunction is true if and only if at least one of p p p xn is true example what is the truth value of xp x where p x is the statement and the universe of discourse consists of the positive integers not exceeding solution because the domain is the proposition xp x is the same as the disjunc tion p p p p because p which is the statement is true it follows that xp x is true it is sometimes helpful to think in terms of looping and searching when determining the truth value of a quantification suppose that there are n objects in the domain for the variable x to determine whether xp x is true we can loop through all n values of x to see whether p x is always true if we encounter a value x for which p x is false then we have shown that xp x is false otherwise xp x is true to see whether xp x is true we loop through the n values of x searching for a value for which p x is true if we find one then xp x is true if we never find such an x then we have determined that xp x is false note that this searching procedure does not apply if there are infinitely many values in the domain however it is still a useful way of thinking about the truth values of quantifications the uniqueness quantifier we have now introduced universal and existential quan tifiers these are the most important quantifiers in mathematics and computer science however there is no limitation on the number of different quantifiers we can define such as there are exactly two there are no more than three there are at least and so on of these other quantifiers the one that is most often seen is the uniqueness quantifier denoted by or the notation xp x or x states there exists a unique x such that p x is true other phrases for uniqueness quantification include there is exactly one and there is one and only one for instance x x where the domain is the set of real numbers states that there is a unique real number x such that x this is a true statement as x is the unique real number such that x observe that we can use quantifiers and propositional logic to express uniqueness see exercise in section so the uniqueness quantifier can be avoided generally it is best to stick with existential and universal quantifiers so that rules of inference for these quantifiers can be used quantifiers with restricted domains an abbreviated notation is often used to restrict the domain of a quantifier in this nota tion a condition a variable must satisfy is included after the quantifier this is illustrated in example we will also describe other forms of this notation involving set membership in section example what do the statements x y and z mean where the domain in each case consists of the real numbers solution the statement x states that for every real number x with x that is it states the square of a negative real number is positive this statement is the same as x x the statement y states that for every real number y with y we have that is it states the cube of every nonzero real number is nonzero note that this statement is equivalent to y y finally the statement z states that there exists a real number z with z such that that is it states there is a positive square root of this statement is equivalent to z z note that the restriction of a universal quantification is the same as the universal quantifi cation of a conditional statement for instance x is another way of expressing x x on the other hand the restriction of an existential quantification is the same as the existential quantification of a conjunction for instance z is another way of expressing z z precedence of quantifiers the quantifiers and have higher precedence than all logical operators from propositional calculus for example xp x q x is the disjunction of xp x and q x in other words it means xp x q x rather than x p x q x binding variables when a quantifier is used on the variable x we say that this occurrence of the variable is bound an occurrence of a variable that is not bound by a quantifier or set equal to a particular value is said to be free all the variables that occur in a propositional function must be bound or set equal to a particular value to turn it into a proposition this can be done using a combination of universal quantifiers existential quantifiers and value assignments the part of a logical expression to which a quantifier is applied is called the scope of this quantifier consequently a variable is free if it is outside the scope of all quantifiers in the formula that specify this variable example in the statement x x y the variable x is bound by the existential quantification x but the variable y is free because it is not bound by a quantifier and no value is assigned to this variable this illustrates that in the statement x x y x is bound but y is free in the statement x p x q x xr x all variables are bound the scope of the first quantifier x is the expression p x q x because x is applied only to p x q x and not to the rest of the statement similarly the scope of the second quantifier x is the expression r x that is the existential quantifier binds the variable x in p x q x and the universal quantifier x binds the variable x in r x observe that we could have written our statement using two different variables x and y as x p x q x yr y because the scopes of the two quantifiers do not overlap the reader should be aware that in common usage the same letter is often used to represent variables bound by different quantifiers with scopes that do not overlap logical equivalences involving quantifiers in section we introduced the notion of logical equivalences of compound propositions we can extend this notion to expressions involving predicates and quantifiers definition example illustrates how to show that two statements involving predicates and quantifiers are logically equivalent example show that x p x q x and xp x xq x are logically equivalent where the same domain is used throughout this logical equivalence shows that we can distribute a universal quantifier over a conjunction furthermore we can also distribute an existential quantifier over a disjunction however we cannot distribute a universal quantifier over a disjunction nor can we distribute an existential quantifier over a conjunction see exercises and solution to show that these statements are logically equivalent we must show that they always take the same truth value no matter what the predicates p and q are and no matter which domain of discourse is used suppose we have particular predicates p and q with a common domain we can show that x p x q x and xp x xq x are logically equivalent by doing two things first we show that if x p x q x is true then xp x xq x is true second we show that if xp x xq x is true then x p x q x is true so suppose that x p x q x is true this means that if a is in the domain then p a q a is true hence p a is true and q a is true because p a is true and q a is true for every element in the domain we can conclude that xp x and xq x are both true this means that xp x xq x is true next suppose that xp x xq x is true it follows that xp x is true and xq x is true hence if a is in the domain then p a is true and q a is true because p x and q x are both true for all elements in the domain there is no conflict using the same value of a here it follows that for all a p a q a is true it follows that x p x q x is true we can now conclude that x p x q x xp x xq x negating quantified expressions we will often want to consider the negation of a quantified expression for instance consider the negation of the statement every student in your class has taken a course in calculus this statement is a universal quantification namely xp x where p x is the statement x has taken a course in calculus and the domain consists of the students in your class the negation of this statement is it is not the case that every student in your class has taken a course in calculus this is equivalent to there is a student in your class who has not taken a course in calculus and this is simply the existential quantification of the negation of the original propositional function namely x p x this example illustrates the following logical equivalence xp x x p x to show that xp x and xp x are logically equivalent no matter what the propositional function p x is and what the domain is first note that xp x is true if and only if xp x is false next note that xp x is false if and only if there is an element x in the domain for which p x is false this holds if and only if there is an element x in the domain for which p x is true finally note that there is an element x in the domain for which p x is true if and only if x p x is true putting these steps together we can conclude that xp x is true if and only if x p x is true it follows that xp x and x p x are logically equivalent suppose we wish to negate an existential quantification for instance consider the propo sition there is a student in this class who has taken a course in calculus this is the existential quantification xq x where q x is the statement x has taken a course in calculus the negation of this statement is the proposition it is not the case that there is a student in this class who has taken a course in calculus this is equivalent to every student in this class has not taken calculus which is just the universal quantification of the negation of the original propositional function or phrased in the language of quantifiers x q x this example illustrates the equivalence xq x x q x to show that xq x and x q x are logically equivalent no matter what q x is and what the domain is first note that xq x is true if and only if xq x is false this is true if and table de morgan laws for quantifiers negation equivalent statement when is negation true when false xp x xp x x p x x p x for every x p x is false there is an x for which p x is false there is an x for which p x is true p x is true for every x only if no x exists in the domain for which q x is true next note that no x exists in the domain for which q x is true if and only if q x is false for every x in the domain finally note that q x is false for every x in the domain if and only if q x is true for all x in the domain which holds if and only if x q x is true putting these steps together we see that xq x is true if and only if x q x is true we conclude that xq x and x q x are logically equivalent the rules for negations for quantifiers are called de morgan laws for quantifiers these rules are summarized in table remark when the domain of a predicate p x consists of n elements where n is a positive integer greater than one the rules for negating quantified statements are exactly the same as de morgan laws discussed in section this is why these rules are called de morgan laws for quantifiers when the domain has n elements xn it follows that xp x is the same as p p p xn which is equivalent to p p p xn by de morgan laws and this is the same as x p x similarly xp x is the same as p p p xn which by de morgan laws is equivalent to p p p xn and this is the same as x p x we illustrate the negation of quantified statements in examples and example what are the negations of the statements there is an honest politician and all americans eat cheeseburgers solution let h x denote x is honest then the statement there is an honest politician is represented by xh x where the domain consists of all politicians the negation of this statement is xh x which is equivalent to x h x this negation can be expressed as every politician is dishonest note in english the statement all politicians are not honest is ambiguous in common usage this statement often means not all politicians are honest consequently we do not use this statement to express this negation let c x denote x eats cheeseburgers then the statement all americans eat cheese burgers is represented by xc x where the domain consists of all americans the negation of this statement is xc x which is equivalent to x c x this negation can be expressed in several different ways including some american does not eat cheeseburgers and there is an american who does not eat cheeseburgers example what are the negations of the statements x x and x solution the negation of x x is the statement x x which is equivalent to x x this can be rewritten as x x the negation of x is the statement x which is equivalent to x this can be rewritten as x the truth values of these statements depend on the domain we use de morgan laws for quantifiers in example example show that x p x q x and x p x q x are logically equivalent solution by de morgan law for universal quantifiers we know that x p x q x and x p x q x are logically equivalent by the fifth logical equivalence in table in section we know that p x q x and p x q x are logically equivalent for every x because we can substitute one logically equivalent expression for another in a logical equivalence it follows that x p x q x and x p x q x are logically equivalent translating from english into logical expressions translating sentences in english or other natural languages into logical expressions is a crucial task in mathematics logic programming artificial intelligence software engineering and many other disciplines we began studying this topic in section where we used propositions to express sentences in logical expressions in that discussion we purposely avoided sentences whose translations required predicates and quantifiers translating from english to logical ex pressions becomes even more complex when quantifiers are needed furthermore there can be many ways to translate a particular sentence as a consequence there is no cookbook approach that can be followed step by step we will use some examples to illustrate how to translate sentences from english into logical expressions the goal in this translation is to pro duce simple and useful logical expressions in this section we restrict ourselves to sentences that can be translated into logical expressions using a single quantifier in the next section we will look at more complicated sentences that require multiple quantifiers example express the statement every student in this class has studied calculus using predicates and quantifiers solution first we rewrite the statement so that we can clearly identify the appropriate quantifiers to use doing so we obtain for every student in this class that student has studied calculus next we introduce a variable x so that our statement becomes for every student x in this class x has studied calculus continuing we introduce c x which is the statement x has studied calculus consequently if the domain for x consists of the students in the class we can translate our statement as xc x however there are other correct approaches different domains of discourse and other predicates can be used the approach we select depends on the subsequent reasoning we want to carry out for example we may be interested in a wider group of people than only those in this class if we change the domain to consist of all people we will need to express our statement as for every person x if person x is a student in this class then x has studied calculus if s x represents the statement that person x is in this class we see that our statement can be expressed as x s x c x caution our statement cannot be expressed as x s x c x because this statement says that all people are students in this class and have studied calculus finally when we are interested in the background of people in subjects besides calculus we may prefer to use the two variable quantifier q x y for the statement student x has studied subject y then we would replace c x by q x calculus in both approaches to obtain xq x calculus or x s x q x calculus in example we displayed different approaches for expressing the same statement using predicates and quantifiers however we should always adopt the simplest approach that is adequate for use in subsequent reasoning example express the statements some student in this class has visited mexico and every student in this class has visited either canada or mexico using predicates and quantifiers solution the statement some student in this class has visited mexico means that there is a student in this class with the property that the student has visited mexico we can introduce a variable x so that our statement becomes there is a student x in this class having the property that x has visited mexico we introduce m x which is the statement x has visited mexico if the domain for x consists of the students in this class we can translate this first statement as xm x however if we are interested in people other than those in this class we look at the statement a little differently our statement can be expressed as there is a person x having the properties that x is a student in this class and x has visited mexico in this case the domain for the variable x consists of all people we introduce s x to represent x is a student in this class our solution becomes x s x m x because the statement is that there is a person x who is a student in this class and who has visited mexico caution our statement cannot be expressed as x s x m x which is true when there is someone not in the class because in that case for such a person x s x m x becomes either f t or f f both of which are true similarly the second statement can be expressed as for every x in this class x has the property that x has visited mexico or x has visited canada note that we are assuming the inclusive rather than the exclusive or here we let c x be x has visited canada following our earlier reasoning we see that if the domain for x consists of the students in this class this second statement can be expressed as x c x m x however if the domain for x consists of all people our statement can be expressed as for every person x if x is a student in this class then x has visited mexico or x has visited canada in this case the statement can be expressed as x s x c x m x instead of using m x and c x to represent that x has visited mexico and x has visited canada respectively we could use a two place predicate v x y to represent x has visited country y in this case v x mexico and v x canada would have the same meaning as m x and c x and could replace them in our answers if we are working with many statements that involve people visiting different countries we might prefer to use this two variable approach otherwise for simplicity we would stick with the one variable predicates m x and c x using quantifiers in system specifications in section we used propositions to represent system specifications however many system specifications involve predicates and quantifications this is illustrated in example example use predicates and quantifiers to express the system specifications every mail message larger than one megabyte will be compressed and if a user is active at least one network link will be available remember the rules of precedence for quantifiers and logical connectives solution let s m y be mail message m is larger than y megabytes where the variable x has the domain of all mail messages and the variable y is a positive real number and let c m denote mail message m will be compressed then the specification every mail message larger than one megabyte will be compressed can be represented as m s m c m let a u represent user u is active where the variable u has the domain of all users let s n x denote network link n is in state x where n has the domain of all network links and x has the domain of all possible states for a network link then the specifica tion if a user is active at least one network link will be available can be represented by ua u ns n available examples from lewis carroll lewis carroll really c l dodgson writing under a pseudonym the author of alice in wonder land is also the author of several works on symbolic logic his books contain many examples of reasoning using quantifiers examples and come from his book symbolic logic other examples from that book are given in the exercises at the end of this section these examples illustrate how quantifiers are used to express various types of statements example consider these statements the first two are called premises and the third is called the conclusion the entire set is called an argument all lions are fierce some lions do not drink coffee some fierce creatures do not drink coffee in section we will discuss the issue of determining whether the conclusion is a valid conse quence of the premises in this example it is let p x q x and r x be the statements x is a lion x is fierce and x drinks coffee respectively assuming that the domain consists of all creatures express the statements in the argument using quantifiers and p x q x and r x charles lutwidge dodgson we know charles dodgson as lewis carroll the pseudonym he used in his literary works dodgson the son of a clergyman was the third of children all of whom stuttered he was uncomfortable in the company of adults and is said to have spoken without stuttering only to young girls many of whom he entertained corresponded with and photographed sometimes in poses that today would be considered inappropriate although attracted to young girls he was extremely puritanical and religious his friendship with the three young daughters of dean liddell led to his writing alice in wonderland which brought him money and fame dodgson graduated from oxford in and obtained his master of arts degree in he was appointed lecturer in mathematics at christ church college oxford in he was ordained in the church of england in but never practiced his ministry his writings published under this real name include articles and books on geometry determinants and the mathematics of tournaments and elections he also used the pseudonym lewis carroll for his many works on recreational logic solution we can express these statements as x p x q x x p x r x x q x r x notice that the second statement cannot be written as x p x r x the reason is that p x r x is true whenever x is not a lion so that x p x r x is true as long as there is at least one creature that is not a lion even if every lion drinks coffee similarly the third statement cannot be written as x q x r x example consider these statements of which the first three are premises and the fourth is a valid conclu sion all hummingbirds are richly colored no large birds live on honey birds that do not live on honey are dull in color hummingbirds are small let p x q x r x and s x be the statements x is a hummingbird x is large x lives on honey and x is richly colored respectively assuming that the domain consists of all birds express the statements in the argument using quantifiers and p x q x r x and s x solution we can express the statements in the argument as x p x s x x q x r x x r x s x x p x q x note we have assumed that small is the same as not large and that dull in color is the same as not richly colored to show that the fourth statement is a valid conclusion of the first three we need to use rules of inference that will be discussed in section logic programming an important type of programming language is designed to reason using the rules of predicate logic prolog from programming in logic developed in the by computer scientists working in the area of artificial intelligence is an example of such a language prolog programs include a set of declarations consisting of two types of statements prolog facts and prolog rules prolog facts define predicates by specifying the elements that satisfy these predicates prolog rules are used to define new predicates using those already defined by prolog facts example illustrates these notions example consider a prolog program given facts telling it the instructor of each class and in which classes students are enrolled the program uses these facts to answer queries concerning the professors who teach particular students such a program could use the predicates instructor p c and enrolled c to represent that professor p is the instructor of course c and that student is enrolled in course c respectively for example the prolog facts in such a program might include instructor chan instructor patel instructor grossman enrolled kevin enrolled juana enrolled juana enrolled kiko enrolled kiko lowercase letters have been used for entries because prolog considers names beginning with an uppercase letter to be variables a new predicate teaches p representing that professor p teaches student can be defined using the prolog rule teaches p s instructor p c enrolled s c which means that teaches p is true if there exists a class c such that professor p is the instructor of class c and student is enrolled in class c note that a comma is used to represent a conjunction of predicates in prolog similarly a semicolon is used to represent a disjunction of predicates prolog answers queries using the facts and rules it is given for example using the facts and rules listed the query enrolled kevin produces the response yes because the fact enrolled kevin was provided as input the query enrolled x produces the response kevin kiko to produce this response prolog determines all possible values of x for which enrolled x has been included as a prolog fact similarly to find all the professors who are instructors in classes being taken by juana we use the query teaches x juana this query returns patel grossman exercises let p x denote the statement x what are these truth values a p b p c p let p x be the statement the word x contains the letter a what are these truth values a p orange b p lemon c p true d p false let q x y denote the statement x is the capital of y what are these truth values a q denver colorado b q detroit michigan c q massachusetts boston d q new york new york state the value of x after the statement if p x then x is executed where p x is the statement x if the value of x when this statement is reached is a x b x c x let p x be the statement x spends more than five hours every weekday in class where the domain for x consists of all students express each of these quantifications in english a xp x b xp x c x p x d x p x let n x be the statement x has visited north dakota where the domain consists of the students in your school express each of these quantifications in english a xn x b xn x c xn x d x n x e xn x f x n x translate these statements into english where c x is x is a comedian and f x is x is funny and the domain consists of all people a x c x f x b x c x f x c x c x f x d x c x f x translate these statements into english where r x is x is a rabbit and h x is x hops and the domain consists of all animals a x r x h x b x r x h x c x r x h x d x r x h x let p x be the statement x can speak russian and let q x be the statement x knows the computer language c express each of these sentences in terms of p x q x quantifiers and logical connectives the domain for quantifiers consists of all students at your school a there is a student at your school who can speak rus sian and who knows c b there is a student at your school who can speak rus sian but who doesn t know c c every student at your school either can speak russian or knows c d no student at your school can speak russian or knows c let c x be the statement x has a cat let d x be the statement x has a dog and let f x be the statement x has a ferret express each of these statements in terms of c x d x f x quantifiers and logical connectives let the domain consist of all students in your class a a student in your class has a cat a dog and a ferret b all students in your class have a cat a dog or a ferret c some student in your class has a cat and a ferret but not a dog d no student in your class has a cat a dog and a ferret e for each of the three animals cats dogs and ferrets there is a student in your class who has this animal as a pet let p x be the statement x if the domain con sists of the integers what are these truth values a p b p c p d p e xp x f xp x let q x be the statement x if the domain consists of all integers what are these truth values a q b q c q d xq x e xq x f x q x g x q x determine the truth value of each of these statements if the domain consists of all integers a n n n b n c n n n d n determine the truth value of each of these statements if the domain consists of all real numbers a x b x c x x d x x determine the truth value of each of these statements if the domain for all variables consists of all integers a n b n c n n d n determine the truth value of each of these statements if the domain of each variable consists of all real numbers a x b x c x d x x suppose that the domain of the propositional function p x consists of the integers and write out each of these propositions using disjunctions conjunc tions and negations a xp x b xp x c x p x d x p x e xp x f xp x suppose that the domain of the propositional function p x consists of the integers and write out each of these propositions using disjunctions con junctions and negations a xp x b xp x c x p x d x p x e xp x f xp x suppose that the domain of the propositional function p x consists of the integers and express these statements without using quantifiers instead using only negations disjunctions and conjunctions a xp x b xp x c xp x d xp x e x x p x x p x suppose that the domain of the propositional function p x consists of and express these statements without using quantifiers instead using only negations disjunctions and conjunctions a xp x b xp x c x x p x d x x p x e x p x x x p x for each of these statements find a domain for which the statement is true and a domain for which the statement is false a everyone is studying discrete mathematics b everyone is older than years c every two people have the same mother d no two different people have the same grandmother for each of these statements find a domain for which the statement is true and a domain for which the statement is false a everyone speaks hindi b there is someone older than years c every two people have the same first name d someone knows more than two other people translate in two ways each of these statements into logi cal expressions using predicates quantifiers and logical connectives first let the domain consist of the students in your class and second let it consist of all people a someone in your class can speak hindi b everyone in your class is friendly c there is a person in your class who was not born in california d a student in your class has been in a movie e no student in your class has taken a course in logic programming translate in two ways each of these statements into logi cal expressions using predicates quantifiers and logical connectives first let the domain consist of the students in your class and second let it consist of all people a everyone in your class has a cellular phone b somebody in your class has seen a foreign movie c there is a person in your class who cannot swim d all students in your class can solve quadratic equa tions e some student in your class does not want to be rich translate each of these statements into logical expres sions using predicates quantifiers and logical connec tives a no one is perfect b not everyone is perfect c all your friends are perfect d at least one of your friends is perfect e everyone is your friend and is perfect f not everybody is your friend or someone is not per fect translate each of these statements into logical expres sions in three different ways by varying the domain and by using predicates with one and with two variables a someone in your school has visited uzbekistan b everyone in your class has studied calculus and c c no one in your school owns both a bicycle and a mo torcycle d there is a person in your school who is not happy e everyone in your school was born in the twentieth century translate each of these statements into logical expres sions in three different ways by varying the domain and by using predicates with one and with two variables a a student in your school has lived in vietnam b there is a student in your school who cannot speak hindi c a student in your school knows java prolog and c d everyone in your class enjoys thai food e someone in your class does not play hockey translate each of these statements into logical expres sions using predicates quantifiers and logical connec tives a something is not in the correct place b all tools are in the correct place and are in excellent condition c everything is in the correct place and in excellent con dition d nothing is in the correct place and is in excellent con dition e one of your tools is not in the correct place but it is in excellent condition express each of these statements using logical operators predicates and quantifiers a some propositions are tautologies b the negation of a contradiction is a tautology c the disjunction of two contingencies can be a tautol ogy d the conjunction of two tautologies is a tautology suppose the domain of the propositional function p x y consists of pairs x and y where x is or and y is or write out these propositions using disjunctions and conjunctions a x p x b yp y c y p y d x p x suppose that the domain of q x y z consists of triples x y z where x or y or and z or write out these propositions using disjunctions and con junctions a yq y b xq x c z q z d x q x express each of these statements using quantifiers then form the negation of the statement so that no negation is to the left of a quantifier next express the negation in simple english do not simply use the phrase it is not the case that a all dogs have fleas b there is a horse that can add c every koala can climb d no monkey can speak french e there exists a pig that can swim and catch fish express each of these statements using quantifiers then form the negation of the statement so that no negation is to the left of a quantifier next express the negation in simple english do not simply use the phrase it is not the case that a some old dogs can learn new tricks b no rabbit knows calculus c every bird can fly d there is no dog that can talk e there is no one in this class who knows french and russian express the negation of these propositions using quanti fiers and then express the negation in english a some drivers do not obey the speed limit b all swedish movies are serious c no one can keep a secret d there is someone in this class who does not have a good attitude find a counterexample if possible to these universally quantified statements where the domain for all variables consists of all integers a x x b x x x c x x find a counterexample if possible to these universally quantified statements where the domain for all variables consists of all real numbers a x x b x c x x express each of these statements using predicates and quantifiers a a passenger on an airline qualifies as an elite flyer if the passenger flies more than miles in a year or takes more than flights during that year b a man qualifies for the marathon if his best previ ous time is less than hours and a woman qualifies for the marathon if her best previous time is less than hours c a student must take at least course hours or at least course hours and write a master thesis and re ceive a grade no lower than a b in all required courses to receive a master degree d there is a student who has taken more than credit hours in a semester and received all a exercises deal with the translation between system specification and logical expressions involving quantifiers translate these system specifications into english where the predicate s x y is x is in state y and where the domain for x and y consists of all systems and all possible states respectively a xs x open b x s x malfunctioning s x diagnostic c xs x open xs x diagnostic d x s x available e x s x working translate these specifications into english where f p is printer p is out of service b p is printer p is busy l j is print job j is lost and q j is print job j is queued a p f p b p j l j b pb p j q j c j q j l j pf p d pb p j q j j l j express each of these system specifications using predi cates quantifiers and logical connectives a when there is less than megabytes free on the hard disk a warning message is sent to all users b no directories in the file system can be opened and no files can be closed when system errors have been detected c the file system cannot be backed up if there is a user currently logged on d video on demand can be delivered when there are at least megabytes of memory available and the con nection speed is at least kilobits per second express each of these system specifications using predi cates quantifiers and logical connectives a at least one mail message among the nonempty set of messages can be saved if there is a disk with more than kilobytes of free space b whenever there is an active alert all queued messages are transmitted c the diagnostic monitor tracks the status of all systems except the main console d each participant on the conference call whom the host of the call did not put on a special list was billed express each of these system specifications using predi cates quantifiers and logical connectives a every user has access to an electronic mailbox b the system mailbox can be accessed by everyone in the group if the file system is locked c the firewall is in a diagnostic state only if the proxy server is in a diagnostic state d at least one router is functioning normally if the throughput is between kbps and kbps and the proxy server is not in diagnostic mode determine whether x p x q x and xp x xq x are logically equivalent justify your answer determine whether x p x q x and x p x xq x are logically equivalent justify your answer show that x p x q x and xp x xq x are logically equivalent exercises establish rules for null quantification that we can use when a quantified variable does not appear in part of a statement establish these logical equivalences where x does not occur as a free variable in a assume that the domain is nonempty a xp x a x p x a b xp x a x p x a establish these logical equivalences where x does not occur as a free variable in a assume that the domain is nonempty a xp x a x p x a b xp x a x p x a establish these logical equivalences where x does not occur as a free variable in a assume that the domain is nonempty a x a p x a xp x b x a p x a xp x establish these logical equivalences where x does not occur as a free variable in a assume that the domain is nonempty a x p x a xp x a b x p x a xp x a show that xp x xq x and x p x q x are not logically equivalent show that xp x xq x and x p x q x are not logically equivalent as mentioned in the text the notation xp x denotes there exists a unique x such that p x is true if the domain consists of all integers what are the truth values of these statements a x x b x c x x d x x x what are the truth values of these statements a xp x xp x b xp x xp x c x p x xp x write out xp x where the domain consists of the in tegers and in terms of negations conjunctions and disjunctions given the prolog facts in example what would prolog return given these queries a instructor chan b instructor patel c enrolled x d enrolled kiko y e teaches grossman y given the prolog facts in example what would prolog return when given these queries a enrolled kevin b enrolled kiko c instructor grossman x d instructor x e teaches x kevin suppose that prolog facts are used to define the predicates mother m y and father f x which represent that m is the mother of y and f is the father of x respectively give a prolog rule to define the predicate sibling x y which represents that x and y are siblings that is have the same mother and the same father suppose that prolog facts are used to define the predi cates mother m y and father f x which represent that m is the mother of y and f is the father of x respectively give a prolog rule to define the predicate grandfather x y which represents that x is the grand father of y hint you can write a disjunction in prolog either by using a semicolon to separate predicates or by putting these predicates on separate lines exercises are based on questions found in the book symbolic logic by lewis carroll let p x q x and r x be the statements x is a professor x is ignorant and x is vain respectively express each of these statements using quantifiers log ical connectives and p x q x and r x where the domain consists of all people a no professors are ignorant b all ignorant people are vain c no professors are vain d does c follow from a and b let p x q x and r x be the statements x is a clear explanation x is satisfactory and x is an excuse respectively suppose that the domain for x consists of all english text express each of these statements using quan tifiers logical connectives and p x q x and r x a all clear explanations are satisfactory b some excuses are unsatisfactory c some excuses are not clear explanations d does c follow from a and b let p x q x r x and s x be the statements x is a baby x is logical x is able to manage a crocodile and x is despised respectively suppose that the domain consists of all people express each of these statements using quantifiers logical connectives and p x q x r x and s x a babies are illogical b nobody is despised who can manage a crocodile c illogical persons are despised d babies cannot manage crocodiles e does d follow from a b and c if not is there a correct conclusion let p x q x r x and s x be the statements x is a duck x is one of my poultry x is an officer and x is willing to waltz respectively express each of these statements using quantifiers logical connectives and p x q x r x and s x a no ducks are willing to waltz b no officers ever decline to waltz c all my poultry are ducks d my poultry are not officers e does d follow from a b and c if not is there a correct conclusion nested quantifiers introduction in section we defined the existential and universal quantifiers and showed how they can be used to represent mathematical statements we also explained how they can be used to translate english sentences into logical expressions however in section we avoided nested quantifiers where one quantifier is within the scope of another such as x y x y note that everything within the scope of a quantifier can be thought of as a propositional function for example x y x y is the same thing as xq x where q x is yp x y where p x y is x y nested quantifiers commonly occur in mathematics and computer science although nested quantifiers can sometimes be difficult to understand the rules we have already studied in section can help us use them in this section we will gain experience working with nested quantifiers we will see how to use nested quantifiers to express mathematical statements such as the sum of two positive integers is always positive we will show how nested quantifiers can be used to translate english sentences such as everyone has exactly one best friend into logical statements moreover we will gain experience working with the negations of statements involving nested quantifiers understanding statements involving nested quantifiers to understand statements involving nested quantifiers we need to unravel what the quantifiers and predicates that appear mean this is illustrated in examples and example assume that the domain for the variables x and y consists of all real numbers the statement x y x y y x says that x y y x for all real numbers x and y this is the commutative law for addition of real numbers likewise the statement x y x y says that for every real number x there is a real number y such that x y this states that every real number has an additive inverse similarly the statement x y z x y z x y z is the associative law for addition of real numbers example translate into english the statement x y x y xy where the domain for both variables consists of all real numbers solution this statement says that for every real number x and for every real number y if x and y then xy that is this statement says that for real numbers x and y if x is positive and y is negative then xy is negative this can be stated more succinctly as the product of a positive real number and a negative real number is always a negative real number thinking of quantification as loops in working with quantifications of more than one variable it is sometimes helpful to think in terms of nested loops of course if there are infinitely many elements in the domain of some variable we cannot actually loop through all values nevertheless this way of thinking is helpful in understanding nested quantifiers for example to see whether x yp x y is true we loop through the values for x and for each x we loop through the values for y if we find that p x y is true for all values for x and y we have determined that x yp x y is true if we ever hit a value x for which we hit a value y for which p x y is false we have shown that x yp x y is false similarly to determine whether x yp x y is true we loop through the values for x for each x we loop through the values for y until we find a y for which p x y is true if for every x we hit such a y then x yp x y is true if for some x we never hit such a y then x yp x y is false to see whether x yp x y is true we loop through the values for x until we find an x for which p x y is always true when we loop through all values for y once we find such an x we know that x yp x y is true if we never hit such an x then we know that x yp x y is false finally to see whether x yp x y is true we loop through the values for x where for each x we loop through the values for y until we hit an x for which we hit a y for which p x y is true the statement x yp x y is false only if we never hit an x for which we hit a y such that p x y is true the order of quantifiers many mathematical statements involve multiple quantifications of propositional functions in volving more than one variable it is important to note that the order of the quantifiers is important unless all the quantifiers are universal quantifiers or all are existential quantifiers these remarks are illustrated by examples example let p x y be the statement x y y x what are the truth values of the quantifications x yp x y and y xp x y where the domain for all variables consists of all real numbers solution the quantification x yp x y denotes the proposition for all real numbers x for all real numbers y x y y x because p x y is true for all real numbers x and y it is the commutative law for addition which is an axiom for the real numbers see appendix the proposition x yp x y is true note that the statement y xp x y says for all real numbers y for all real numbers x x y y x this has the same meaning as the statement for all real numbers x for all real numbers y x y y x that is x yp x y and y xp x y have the same meaning and both are true this illustrates the principle that the order of nested universal quantifiers in a statement without other quantifiers can be changed without changing the meaning of the quantified statement example let q x y denote x y what are the truth values of the quantifications y xq x y and x yq x y where the domain for all variables consists of all real numbers solution the quantification y xq x y denotes the proposition there is a real number y such that for every real number x q x y no matter what value of y is chosen there is only one value of x for which x y because there is no real number y such that x y for all real numbers x the statement y xq x y is false the quantification x yq x y denotes the proposition for every real number x there is a real number y such that q x y given a real number x there is a real number y such that x y namely y x hence be careful with the order of existential and universal quantifiers the statement x yq x y is true example illustrates that the order in which quantifiers appear makes a difference the state ments y xp x y and x yp x y are not logically equivalent the statement y xp x y is true if and only if there is a y that makes p x y true for every x so for this statement to be true there must be a particular value of y for which p x y is true regardless of the choice of x on the other hand x yp x y is true if and only if for every value of x there is a value of y for which p x y is true so for this statement to be true no matter which x you choose there must be a value of y possibly depending on the x you choose for which p x y is true in other words in the second case y can depend on x whereas in the first case y is a constant independent of x from these observations it follows that if y xp x y is true then x yp x y must also be true however if x yp x y is true it is not necessary for y xp x y to be true see supplementary exercises and table summarizes the meanings of the different possible quantifications involving two variables quantifications of more than two variables are also common as example illustrates example let q x y z be the statement x y z what are the truth values of the statements x y zq x y z and z x yq x y z where the domain of all variables consists of all real numbers solution suppose that x and y are assigned values then there exists a real number z such that x y z consequently the quantification x y zq x y z which is the statement for all real numbers x and for all real numbers y there is a real number z such that x y z table quantifications of two variables statement when true when false x yp x y y xp x y p x y is true for every pair x y there is a pair x y for which p x y is false x yp x y for every x there is a y for which p x y is true there is an x such that p x y is false for every y x yp x y there is an x for which p x y is true for every y for every x there is a y for which p x y is false x yp x y y xp x y there is a pair x y for which p x y is true p x y is false for every pair x y is true the order of the quantification here is important because the quantification z x yq x y z which is the statement there is a real number z such that for all real numbers x and for all real numbers y it is true that x y z is false because there is no value of z that satisfies the equation x y z for all values of x and y translating mathematical statements into statements involving nested quantifiers mathematical statements expressed in english can be translated into logical expressions as examples show example translate the statement the sum of two positive integers is always positive into a logical expression solution to translate this statement into a logical expression we first rewrite it so that the implied quantifiers and a domain are shown for every two integers if these integers are both positive then the sum of these integers is positive next we introduce the variables x and y to obtain for all positive integers x and y x y is positive consequently we can express this statement as x y x y x y where the domain for both variables consists of all integers note that we could also translate this using the positive integers as the domain then the statement the sum of two positive integers is always positive becomes for every two positive integers the sum of these integers is positive we can express this as x y x y where the domain for both variables consists of all positive integers example translate the statement every real number except zero has a multiplicative inverse a mul tiplicative inverse of a real number x is a real number y such that xy solution we first rewrite this as for every real number x except zero x has a multiplicative inverse we can rewrite this as for every real number x if x then there exists a real number y such that xy this can be rewritten as x x y xy one example that you may be familiar with is the concept of limit which is important in calculus example requires calculus use quantifiers to express the definition of the limit of a real valued function f x of a real variable x at a point a in its domain solution recall that the definition of the statement lim f x l x a is for every real number e there exists a real number δ such that f x l e whenever x a δ this definition of a limit can be phrased in terms of quantifiers by e δ x x a δ f x l e where the domain for the variables δ and e consists of all positive real numbers and for x consists of all real numbers this definition can also be expressed as e δ x x a δ f x l e when the domain for the variables e and δ consists of all real numbers rather than just the positive real numbers here restricted quantifiers have been used recall that x p x means that for all x with x p x is true translating from nested quantifiers into english expressions with nested quantifiers expressing statements in english can be quite complicated the first step in translating such an expression is to write out what the quantifiers and predicates in the expression mean the next step is to express this meaning in a simpler sentence this process is illustrated in examples and example translate the statement x c x y c y f x y into english where c x is x has a computer f x y is x and y are friends and the domain for both x and y consists of all students in your school solution the statement says that for every student x in your school x has a computer or there is a student y such that y has a computer and x and y are friends in other words every student in your school has a computer or has a friend who has a computer example translate the statement x y z f x y f x z y z f y z into english where f a b means a and b are friends and the domain for x y and z consists of all students in your school solution we first examine the expression f x y f x z y z f y z this expression says that if students x and y are friends and students x and z are friends and furthermore if y and z are not the same student then y and z are not friends it follows that the original statement which is triply quantified says that there is a student x such that for all students y and all students z other than y if x and y are friends and x and z are friends then y and z are not friends in other words there is a student none of whose friends are also friends with each other translating english sentences into logical expressions in section we showed how quantifiers can be used to translate sentences into logical expres sions however we avoided sentences whose translation into logical expressions required the use of nested quantifiers we now address the translation of such sentences example express the statement if a person is female and is a parent then this person is someone mother as a logical expression involving predicates quantifiers with a domain consisting of all people and logical connectives solution the statement if a person is female and is a parent then this person is someone mother can be expressed as for every person x if person x is female and person x is a parent then there exists a person y such that person x is the mother of person y we introduce the propositional functions f x to represent x is female p x to represent x is a parent and m x y to represent x is the mother of y the original statement can be represented as x f x p x ym x y using the null quantification rule in part b of exercise in section we can move y to the left so that it appears just after x because y does not appear in f x p x we obtain the logically equivalent expression x y f x p x m x y example express the statement everyone has exactly one best friend as a logical expression involving predicates quantifiers with a domain consisting of all people and logical connectives solution the statement everyone has exactly one best friend can be expressed as for every person x person x has exactly one best friend introducing the universal quantifier we see that this statement is the same as x person x has exactly one best friend where the domain consists of all people to say that x has exactly one best friend means that there is a person y who is the best friend of x and furthermore that for every person z if person z is not person y then z is not the best friend of x when we introduce the predicate b x y to be the statement y is the best friend of x the statement that x has exactly one best friend can be represented as y b x y z z y b x z consequently our original statement can be expressed as x y b x y z z y b x z note that we can write this statement as x yb x y where is the uniqueness quantifier defined in section example use quantifiers to express the statement there is a woman who has taken a flight on every airline in the world solution let p w f be w has taken f and q f a be f is a flight on a we can express the statement as w a f p w f q f a where the domains of discourse for w f and a consist of all the women in the world all airplane flights and all airlines respectively the statement could also be expressed as w a f r w f a where r w f a is w has taken f on a although this is more compact it somewhat obscures the relationships among the variables consequently the first solution is usually preferable negating nested quantifiers statements involving nested quantifiers can be negated by successively applying the rules for negating statements involving a single quantifier this is illustrated in examples example express the negation of the statement x y xy so that no negation precedes a quantifier solution by successively applying de morgan laws for quantifiers in table of section we can move the negation in x y xy inside all the quantifiers we find that x y xy is equivalent to x y xy which is equivalent to x y xy because xy can be expressed more simply as xy we conclude that our negated statement can be expressed as x y xy example use quantifiers to express the statement that there does not exist a woman who has taken a flight on every airline in the world solution this statement is the negation of the statement there is a woman who has taken a flight on every airline in the world from example by example our statement can be expressed as w a f p w f q f a where p w f is w has taken f and q f a is f is a flight on a by successively applying de morgan laws for quantifiers in table of section to move the negation inside successive quantifiers and by applying de morgan law for negating a conjunction in the last step we find that our statement is equivalent to each of this sequence of statements w a f p w f q f a w a f p w f q f a w a f p w f q f a w a f p w f q f a this last statement states for every woman there is an airline such that for all flights this woman has not taken that flight or that flight is not on this airline example requires calculus use quantifiers and predicates to express the fact that limx a f x does not exist where f x is a real valued function of a real variable x and a belongs to the domain of f solution to say that limx a f x does not exist means that for all real numbers l limx a f x l by using example the statement limx a f x l can be expressed as e δ x x a δ f x l e successively applying the rules for negating quantified expressions we construct this sequence of equivalent statements e δ x x a δ f x l e e δ x x a δ f x l e e δ x x a δ f x l e e δ x x a δ f x l e e δ x x a δ f x l e in the last step we used the equivalence p q p q which follows from the fifth equivalence in table of section because the statement limx a f x does not exist means for all real numbers l limx a f x l this can be expressed as l e δ x x a δ f x l e this last statement says that for every real number l there is a real number e such that for every real number δ there exists a real number x such that x a δ and f x l e exercises translate these statements into english where the domain for each variable consists of all real numbers a x y x y b x y x y xy c x y z xy z translate these statements into english where the domain for each variable consists of all real numbers a x y xy y b x y x y x y c x y z x y z let q x y be the statement x has sent an e mail mes sage to y where the domain for both x and y consists of all students in your class express each of these quantifi cations in english a x yq x y b x yq x y c x yq x y d y xq x y e y xq x y f x yq x y let p x y be the statement student x has taken class y where the domain for x consists of all students in your class and for y consists of all computer science courses at your school express each of these quantifications in english a x yp x y b x yp x y c x yp x y d y xp x y e y xp x y f x yp x y let w x y mean that student x has visited website y where the domain for x consists of all students in your school and the domain for y consists of all websites ex press each of these statements by a simple english sen tence a w sarah smith www att com b xw x www imdb org c yw josé orez y d y w ashok puri y w cindy yoon y e y z y david belcher w david belcher z w y z f x y z x y w x z w y z let c x y mean that student x is enrolled in class y where the domain for x consists of all students in your school and the domain for y consists of all classes being given at your school express each of these statements by a simple english sentence a c randy goldberg cs b xc x math c yc carol sitea y d x c x math c x cs e x y z x y c x z c y z f x y z x y c x z c y z let t x y mean that student x likes cuisine y where the domain for x consists of all students at your school and the domain for y consists of all cuisines express each of these statements by a simple english sentence a t abdallah hussein japanese b xt x korean xt x mexican c y t monique arsenault y t jay johnson y d x z y x z t x y t z y e x z y t x y t z y x z y t x y t z y let q x y be the statement student x has been a con testant on quiz show y express each of these sentences in terms of q x y quantifiers and logical connectives where the domain for x consists of all students at your school and for y consists of all quiz shows on television a there is a student at your school who has been a con testant on a television quiz show b no student at your school has ever been a contestant on a television quiz show c there is a student at your school who has been a con testant on jeopardy and on wheel of fortune d every television quiz show has had a student from your school as a contestant e at least two students from your school have been con testants on jeopardy let l x y be the statement x loves y where the do main for both x and y consists of all people in the world use quantifiers to express each of these statements a everybody loves jerry b everybody loves somebody c there is somebody whom everybody loves d nobody loves everybody e there is somebody whom lydia does not love f there is somebody whom no one loves g there is exactly one person whom everybody loves h there are exactly two people whom lynn loves i everyone loves himself or herself j there is someone who loves no one besides himself or herself let f x y be the statement x can fool y where the domain consists of all people in the world use quantifiers to express each of these statements a everybody can fool fred b evelyn can fool everybody c everybody can fool somebody d there is no one who can fool everybody e everyone can be fooled by somebody f no one can fool both fred and jerry g nancy can fool exactly two people h there is exactly one person whom everybody can fool i no one can fool himself or herself j there is someone who can fool exactly one person besides himself or herself let s x be the predicate x is a student f x the pred icate x is a faculty member and a x y the predicate x has asked y a question where the domain consists of all people associated with your school use quantifiers to express each of these statements a lois has asked professor michaels a question b every student has asked professor gross a question c every faculty member has either asked professor miller a question or been asked a question by pro fessor miller d some student has not asked any faculty member a question e there is a faculty member who has never been asked a question by a student f some student has asked every faculty member a ques tion g there is a faculty member who has asked every other faculty member a question h some student has never been asked a question by a faculty member let i x be the statement x has an internet connection and c x y be the statement x and y have chatted over the internet where the domain for the variables x and y consists of all students in your class use quantifiers to express each of these statements a jerry does not have an internet connection b rachel has not chatted over the internet with chelsea c jan and sharon have never chatted over the internet d no one in the class has chatted with bob e sanjay has chatted with everyone except joseph f someone in your class does not have an internet con nection g not everyone in your class has an internet connec tion h exactly one student in your class has an internet con nection i everyone except one student in your class has an in ternet connection j everyone in your class with an internet connection has chatted over the internet with at least one other student in your class k someone in your class has an internet connection but has not chatted with anyone else in your class l there are two students in your class who have not chatted with each other over the internet m there is a student in your class who has chatted with everyone in your class over the internet n there are at least two students in your class who have not chatted with the same person in your class o there are two students in the class who between them have chatted with everyone else in the class let m x y be x has sent y an e mail message and t x y be x has telephoned y where the domain con sists of all students in your class use quantifiers to ex press each of these statements assume that all e mail messages that were sent are received which is not the way things often work a chou has never sent an e mail message to koko b arlene has never sent an e mail message to or tele phoned sarah c josé has never received an e mail message from deb orah d every student in your class has sent an e mail mes sage to ken e no one in your class has telephoned nina f everyone in your class has either telephoned avi or sent him an e mail message g there is a student in your class who has sent everyone else in your class an e mail message h there is someone in your class who has either sent an e mail message or telephoned everyone else in your class i there are two different students in your class who have sent each other e mail messages j there is a student who has sent himself or herself an e mail message k there is a student in your class who has not received an e mail message from anyone else in the class and who has not been called by any other student in the class l every student in the class has either received an e mail message or received a telephone call from an other student in the class m there are at least two students in your class such that one student has sent the other e mail and the second student has telephoned the first student n there are two different students in your class who between them have sent an e mail message to or tele phoned everyone else in the class use quantifiers and predicates with more than one vari able to express these statements a there is a student in this class who can speak hindi b every student in this class plays some sport c some student in this class has visited alaska but has not visited hawaii d all students in this class have learned at least one pro gramming language e there is a student in this class who has taken ev ery course offered by one of the departments in this school f some student in this class grew up in the same town as exactly one other student in this class g every student in this class has chatted with at least one other student in at least one chat group use quantifiers and predicates with more than one vari able to express these statements a every computer science student needs a course in dis crete mathematics b there is a student in this class who owns a personal computer c every student in this class has taken at least one com puter science course d there is a student in this class who has taken at least one course in computer science e every student in this class has been in every building on campus f there is a student in this class who has been in every room of at least one building on campus g every student in this class has been in at least one room of every building on campus a discrete mathematics class contains mathematics ma jor who is a freshman mathematics majors who are sophomores computer science majors who are sopho mores mathematics majors who are juniors computer science majors who are juniors and computer science major who is a senior express each of these statements in terms of quantifiers and then determine its truth value a there is a student in the class who is a junior b every student in the class is a computer science major c there is a student in the class who is neither a math ematics major nor a junior d every student in the class is either a sophomore or a computer science major e there is a major such that there is a student in the class in every year of study with that major express each of these system specifications using predi cates quantifiers and logical connectives if necessary a every user has access to exactly one mailbox b there is a process that continues to run during all error conditions only if the kernel is working correctly c all users on the campus network can access all web sites whose url has a edu extension d there are exactly two systems that monitor every re mote server express each of these system specifications using predi cates quantifiers and logical connectives if necessary a at least one console must be accessible during every fault condition b the e mail address of every user can be retrieved whenever the archive contains at least one message sent by every user on the system c for every security breach there is at least one mecha nism that can detect that breach if and only if there is a process that has not been compromised d there are at least two paths connecting every two dis tinct endpoints on the network e no one knows the password of every user on the sys tem except for the system administrator who knows all passwords express each of these statements using mathematical and logical operators predicates and quantifiers where the domain consists of all integers a the sum of two negative integers is negative b the difference of two positive integers is not neces sarily positive c the sum of the squares of two integers is greater than or equal to the square of their sum d the absolute value of the product of two integers is the product of their absolute values express each of these statements using predicates quan tifiers logical connectives and mathematical operators where the domain consists of all integers a the product of two negative integers is positive b the average of two positive integers is positive c the difference of two negative integers is not neces sarily negative d the absolute value of the sum of two integers does not exceed the sum of the absolute values of these integers use predicates quantifiers logical connectives and mathematical operators to express the statement that ev ery positive integer is the sum of the squares of four in tegers use predicates quantifiers logical connectives and mathematical operators to express the statement that there is a positive integer that is not the sum of three squares express each of these mathematical statements using predicates quantifiers logical connectives and mathe matical operators a the product of two negative real numbers is positive b the difference of a real number and itself is zero c every positive real number has exactly two square roots d a negative real number does not have a square root that is a real number translate each of these nested quantifications into an en glish statement that expresses a mathematical fact the domain in each case consists of all real numbers a x y x y y b x y x y x y c x y x y x y d x y x y xy translate each of these nested quantifications into an en glish statement that expresses a mathematical fact the domain in each case consists of all real numbers a x y xy y b x y x y xy c x y y x y d x y z x y z let q x y be the statement x y x y if the do main for both variables consists of all integers what are the truth values a q b q c yq y d xq x e x yq x y f x yq x y g y xq x y h y xq x y i x yq x y determine the truth value of each of these statements if the domain for all variables consists of all integers a n m m b n m n c n m n m d n m nm m e n m f n m g n m n m n m h n m n m n m i n m p p m n determine the truth value of each of these statements if the domain of each variable consists of all real numbers a x y y b x y x c x y xy d x y x y y x e x x y xy f x y y xy g x y x y h x y x i x y x y y j x y z z x y suppose the domain of the propositional function p x y consists of pairs x and y where x is or and y is or write out these propositions using disjunctions and conjunctions a x yp x y b x yp x y c x yp x y d y xp x y rewrite each of these statements so that negations ap pear only within predicates that is so that no negation is outside a quantifier or an expression involving logical connectives a y xp x y b x yp x y c y q y x r x y d y xr x y xs x y e y x zt x y z x zu x y z express the negations of each of these statements so that all negation symbols immediately precede predicates a x y zt x y z b x yp x y x yq x y c x y p x y zr x y z d x y p x y q x y express the negations of each of these statements so that all negation symbols immediately precede predicates a z y xt x y z b x yp x y x yq x y c x y q x y q y x d y x z t x y z q x y rewrite each of these statements so that negations ap pear only within predicates that is so that no negation is outside a quantifier or an expression involving logical connectives a x yp x y b y xp x y c y x p x y q x y d x y p x y x yq x y e x y zp x y z z yp x y z find a common domain for the variables x y and z for which the statement x y x y z z x z y is true and another domain for which it is false find a common domain for the variables x y z and w for which the statement x y z w w x w y w z is true and another common domain for these variables for which it is false express each of these statements using quantifiers then form the negation of the statement so that no negation is to the left of a quantifier next express the negation in simple english do not simply use the phrase it is not the case that a no one has lost more than one thousand dollars play ing the lottery b there is a student in this class who has chatted with exactly one other student c no student in this class has sent e mail to exactly two other students in this class d some student has solved every exercise in this book e no student has solved at least one exercise in every section of this book express each of these statements using quantifiers then form the negation of the statement so that no negation is to the left of a quantifier next express the negation in simple english do not simply use the phrase it is not the case that a every student in this class has taken exactly two math ematics classes at this school b someone has visited every country in the world except libya c no one has climbed every mountain in the himalayas d every movie actor has either been in a movie with kevin bacon or has been in a movie with someone who has been in a movie with kevin bacon express the negations of these propositions using quan tifiers and in english a every student in this class likes mathematics b there is a student in this class who has never seen a computer c there is a student in this class who has taken every mathematics course offered at this school d there is a student in this class who has been in at least one room of every building on campus find a counterexample if possible to these universally quantified statements where the domain for all variables consists of all integers a x y x y b x y x c x y xy x find a counterexample if possible to these universally quantified statements where the domain for all variables consists of all integers a x y y b x y y x determine the truth value of the statement x y xy if the domain for the variables consists of a the nonzero real numbers b the nonzero integers c the positive real numbers determine the truth value of the statement x y x if the domain for the variables consists of a the positive real numbers b the integers c the nonzero real numbers show that the two statements x yp x y and x y p x y where both quantifiers over the first vari able in p x y have the same domain and both quanti fiers over the second variable in p x y have the same domain are logically equivalent show that xp x xq x and x y p x q y where all quantifiers have the same nonempty domain are logically equivalent the new variable y is used to combine the quantifications correctly a show that xp x xq x is logically equivalent to x y p x q y where all quantifiers have the same nonempty domain b show that xp x xq x is equivalent to x y p x q y where all quantifiers have the same nonempty domain a statement is in prenex normal form pnf if and only if it is of the form qkxkp xk where each qi i k is either the existential quan tifier or the universal quantifier and p xk is a pred icate involving no quantifiers for example x y p x y q y is in prenex normal form whereas xp x xq x is not because the quantifiers do not all occur first every statement formed from propositional variables predicates t and f using logical connectives and quan tifiers is equivalent to a statement in prenex normal form exercise asks for a proof of this fact put these statements in prenex normal form hint use logical equivalence from tables and in section table in section example in section exercises and in section and exercises and c x y x a use quantifiers to express the associative law for multi plication of real numbers use quantifiers to express the distributive laws of multi plication over addition for real numbers use quantifiers and logical connectives to express the fact that every linear polynomial that is polynomial of de gree with real coefficients and where the coefficient of x is nonzero has exactly one real root use quantifiers and logical connectives to express the fact that a quadratic polynomial with real number coefficients has at most two real roots xp x xq x a where a is a proposition not involving any quantifiers b xp x xq x c xp x xq x show how to transform an arbitrary statement to a state ment in prenex normal form that is equivalent to the given statement note a formal solution of this exercise re quires use of structural induction covered in section express the quantification xp x introduced in sec tion using universal quantifications existential quan tifications and logical operators rules of inference introduction later in this chapter we will study proofs proofs in mathematics are valid arguments that estab lish the truth of mathematical statements by an argument we mean a sequence of statements that end with a conclusion by valid we mean that the conclusion or final statement of the argument must follow from the truth of the preceding statements or premises of the argument that is an argument is valid if and only if it is impossible for all the premises to be true and the conclusion to be false to deduce new statements from statements we already have we use rules of inference which are templates for constructing valid arguments rules of inference are our basic tools for establishing the truth of statements before we study mathematical proofs we will look at arguments that involve only compound propositions we will define what it means for an argument involving compound propositions to be valid then we will introduce a collection of rules of inference in propositional logic these rules of inference are among the most important ingredients in producing valid arguments after we illustrate how rules of inference are used to produce valid arguments we will describe some common forms of incorrect reasoning called fallacies which lead to invalid arguments after studying rules of inference in propositional logic we will introduce rules of inference for quantified statements we will describe how these rules of inference can be used to produce valid arguments these rules of inference for statements involving existential and universal quantifiers play an important role in proofs in computer science and mathematics although they are often used without being explicitly mentioned finally we will show how rules of inference for propositions and for quantified statements can be combined these combinations of rule of inference are often used together in complicated arguments valid arguments in propositional logic consider the following argument involving propositions which by definition is a sequence of propositions if you have a current password then you can log onto the network you have a current password therefore you can log onto the network we would like to determine whether this is a valid argument that is we would like to determine whether the conclusion you can log onto the network must be true when the premises if you have a current password then you can log onto the network and you have a current password are both true before we discuss the validity of this particular argument we will look at its form use p to represent you have a current password and q to represent you can log onto the network then the argument has the form p q p q where is the symbol that denotes therefore we know that when p and q are propositional variables the statement p q p q is a tautology see exercise c in section in particular when both p q and p are true we know that q must also be true we say this form of argument is valid because whenever all its premises all statements in the argument other than the final one the conclusion are true the conclusion must also be true now suppose that both if you have a current password then you can log onto the network and you have a current password are true statements when we replace p by you have a current password and q by you can log onto the network it necessarily follows that the conclusion you can log onto the network is true this argument is valid because its form is valid note that whenever we replace p and q by propositions where p q and p are both true then q must also be true what happens when we replace p and q in this argument form by propositions where not both p and p q are true for example suppose that p represents you have access to the network and q represents you can change your grade and that p is true but p q is false the argument we obtain by substituting these values of p and q into the argument form is if you have access to the network then you can change your grade you have access to the network you can change your grade the argument we obtained is a valid argument but because one of the premises namely the first premise is false we cannot conclude that the conclusion is true most likely this conclusion is false in our discussion to analyze an argument we replaced propositions by propositional vari ables this changed an argument to an argument form we saw that the validity of an argument follows from the validity of the form of the argument we summarize the terminology used to discuss the validity of arguments with our definition of the key notions definition from the definition of a valid argument form we see that the argument form with premises pn and conclusion q is valid when pn q is a tautology the key to showing that an argument in propositional logic is valid is to show that its argument form is valid consequently we would like techniques to show that argument forms are valid we will now develop methods for accomplishing this task rules of inference for propositional logic we can always use a truth table to show that an argument form is valid we do this by showing that whenever the premises are true the conclusion must also be true however this can be a tedious approach for example when an argument form involves different propositional variables to use a truth table to show this argument form is valid requires different rows fortunately we do not have to resort to truth tables instead we can first establish the validity of some relatively simple argument forms called rules of inference these rules of inference can be used as building blocks to construct more complicated valid argument forms we will now introduce the most important rules of inference in propositional logic the tautology p p q q is the basis of the rule of inference called modus po nens or the law of detachment modus ponens is latin for mode that affirms this tautology leads to the following valid argument form which we have already seen in our initial discussion about arguments where as before the symbol denotes therefore p p q q using this notation the hypotheses are written in a column followed by a horizontal bar followed by a line that begins with the therefore symbol and ends with the conclusion in particular modus ponens tells us that if a conditional statement and the hypothesis of this conditional statement are both true then the conclusion must also be true example illustrates the use of modus ponens example suppose that the conditional statement if it snows today then we will go skiing and its hypothesis it is snowing today are true then by modus ponens it follows that the conclusion of the conditional statement we will go skiing is true as we mentioned earlier a valid argument can lead to an incorrect conclusion if one or more of its premises is false we illustrate this again in example example determine whether the argument given here is valid and determine whether its conclusion must be true because of the validity of the argument solution let p be the proposition and q the proposition the premises of the argument are p q and p and q is its conclusion this argument is valid because it is constructed by using modus ponens a valid argument form however one of its premises is false consequently we cannot conclude that the conclusion is true furthermore note that the conclusion of this argument is false because there are many useful rules of inference for propositional logic perhaps the most widely used of these are listed in table exercises and in section ask for the verifications that these rules of inference are valid argument forms we now give examples of arguments that use these rules of inference in each argument we first use propositional variables to express the propositions in the argument we then show that the resulting argument form is a rule of inference from table table rules of inference rule of inference tautology name p p q q p p q q modus ponens q p q p q p q p modus tollens p q q r p r p q q r p r hypothetical syllogism p q p q p q p q disjunctive syllogism p p q p p q addition p q p p q p simplification p q p q p q p q conjunction p q p r q r p q p r q r resolution example state which rule of inference is the basis of the following argument it is below freezing now therefore it is either below freezing or raining now solution let p be the proposition it is below freezing now and q the proposition it is raining now then this argument is of the form p p q this is an argument that uses the addition rule example state which rule of inference is the basis of the following argument it is below freezing and raining now therefore it is below freezing now solution let p be the proposition it is below freezing now and let q be the proposition it is raining now this argument is of the form p q p this argument uses the simplification rule example state which rule of inference is used in the argument if it rains today then we will not have a barbecue today if we do not have a barbecue today then we will have a barbecue tomorrow therefore if it rains today then we will have a barbecue tomorrow solution let p be the proposition it is raining today let q be the proposition we will not have a barbecue today and let r be the proposition we will have a barbecue tomorrow then this argument is of the form p q q r p r hence this argument is a hypothetical syllogism using rules of inference to build arguments when there are many premises several rules of inference are often needed to show that an argument is valid this is illustrated by examples and where the steps of arguments are displayed on separate lines with the reason for each step explicitly stated these examples also show how arguments in english can be analyzed using rules of inference example show that the premises it is not sunny this afternoon and it is colder than yesterday we will go swimming only if it is sunny if we do not go swimming then we will take a canoe trip and if we take a canoe trip then we will be home by sunset lead to the conclusion we will be home by sunset solution let p be the proposition it is sunny this afternoon q the proposition it is colder than yesterday r the proposition we will go swimming the proposition we will take a canoe trip and t the proposition we will be home by sunset then the premises become p q r p r and t the conclusion is simply t we need to give a valid argument with premises p q r p r and t and conclusion t we construct an argument to show that our premises lead to the desired conclusion as follows step reason p q premise p simplification using r p premise r modus tollens using and r premise modus ponens using and t premise t modus ponens using and note that we could have used a truth table to show that whenever each of the four hypotheses is true the conclusion is also true however because we are working with five propositional variables p q r and t such a truth table would have rows example show that the premises if you send me an e mail message then i will finish writing the program if you do not send me an e mail message then i will go to sleep early and if i go to sleep early then i will wake up feeling refreshed lead to the conclusion if i do not finish writing the program then i will wake up feeling refreshed solution let p be the proposition you send me an e mail message q the proposition i will finish writing the program r the proposition i will go to sleep early and the proposition i will wake up feeling refreshed then the premises are p q p r and r the desired conclusion is q we need to give a valid argument with premises p q p r and r and conclusion q this argument form shows that the premises lead to the desired conclusion step reason p q premise q p contrapositive of p r premise q r hypothetical syllogism using and r premise q hypothetical syllogism using and resolution computer programs have been developed to automate the task of reasoning and proving theo rems many of these programs make use of a rule of inference known as resolution this rule of inference is based on the tautology p q p r q r exercise in section asks for the verification that this is a tautology the final disjunction in the resolution rule q r is called the resolvent when we let q r in this tautology we obtain p q p q q furthermore when we let r f we obtain p q p q because q f q which is the tautology on which the rule of disjunctive syllogism is based example use resolution to show that the hypotheses jasmine is skiing or it is not snowing and it is snowing or bart is playing hockey imply that jasmine is skiing or bart is playing hockey solution let p be the proposition it is snowing q the proposition jasmine is skiing and r the proposition bart is playing hockey we can represent the hypotheses as p q and p r respectively using resolution the proposition q r jasmine is skiing or bart is playing hockey follows resolution plays an important role in programming languages based on the rules of logic such as prolog where resolution rules for quantified statements are applied furthermore it can be used to build automatic theorem proving systems to construct proofs in propositional logic using resolution as the only rule of inference the hypotheses and the conclusion must be expressed as clauses where a clause is a disjunction of variables or negations of these variables we can replace a statement in propositional logic that is not a clause by one or more equivalent statements that are clauses for example suppose we have a statement of the form p q r because p q r p q p r we can replace the single statement p q r by two statements p q and p r each of which is a clause we can replace a statement of the form p q by the two statements p and q because de morgan law tells us that p q p q we can also replace a conditional statement p q with the equivalent disjunction p q example show that the premises p q r and r imply the conclusion p solution we can rewrite the premises p q r as two clauses p r and q r we can also replace r by the equivalent clause r using the two clauses p r and r we can use resolution to conclude p fallacies several common fallacies arise in incorrect arguments these fallacies resemble rules of infer ence but are based on contingencies rather than tautologies these are discussed here to show the distinction between correct and incorrect reasoning the proposition p q q p is not a tautology because it is false when p is false and q is true however there are many incorrect arguments that treat this as a tautology in other words they treat the argument with premises p q and q and conclusion p as a valid argument form which it is not this type of incorrect reasoning is called the fallacy of affirming the conclusion example is the following argument valid if you do every problem in this book then you will learn discrete mathematics you learned discrete mathematics therefore you did every problem in this book solution let p be the proposition you did every problem in this book let q be the proposition you learned discrete mathematics then this argument is of the form if p q and q then p this is an example of an incorrect argument using the fallacy of affirming the conclusion indeed it is possible for you to learn discrete mathematics in some way other than by doing every problem in this book you may learn discrete mathematics by reading listening to lectures doing some but not all the problems in this book and so on the proposition p q p q is not a tautology because it is false when p is false and q is true many incorrect arguments use this incorrectly as a rule of inference this type of incorrect reasoning is called the fallacy of denying the hypothesis example let p and q be as in example if the conditional statement p q is true and p is true is it correct to conclude that q is true in other words is it correct to assume that you did not learn discrete mathematics if you did not do every problem in the book assuming that if you do every problem in this book then you will learn discrete mathematics solution it is possible that you learned discrete mathematics even if you did not do every problem in this book this incorrect argument is of the form p q and p imply q which is an example of the fallacy of denying the hypothesis rules of inference for quantified statements we have discussed rules of inference for propositions we will now describe some important rules of inference for statements involving quantifiers these rules of inference are used extensively in mathematical arguments often without being explicitly mentioned universal instantiation is the rule of inference used to conclude that p c is true where c is a particular member of the domain given the premise xp x universal instantiation is used when we conclude from the statement all women are wise that lisa is wise where lisa is a member of the domain of all women table rules of inference for quantified statements rule of inference name xp x p c universal instantiation p c for an arbitrary c xp x universal generalization xp x p c for some element c existential instantiation p c for some element c xp x existential generalization universal generalization is the rule of inference that states that xp x is true given the premise that p c is true for all elements c in the domain universal generalization is used when we show that xp x is true by taking an arbitrary element c from the domain and showing that p c is true the element c that we select must be an arbitrary and not a specific element of the domain that is when we assert from xp x the existence of an element c in the domain we have no control over c and cannot make any other assumptions about c other than it comes from the domain universal generalization is used implicitly in many proofs in mathematics and is seldom mentioned explicitly however the error of adding unwarranted assumptions about the arbitrary element c when universal generalization is used is all too common in incorrect reasoning existential instantiation is the rule that allows us to conclude that there is an element c in the domain for which p c is true if we know that xp x is true we cannot select an arbitrary value of c here but rather it must be a c for which p c is true usually we have no knowledge of what c is only that it exists because it exists we may give it a name c and continue our argument existential generalization is the rule of inference that is used to conclude that xp x is true when a particular element c with p c true is known that is if we know one element c in the domain for which p c is true then we know that xp x is true we summarize these rules of inference in table we will illustrate how some of these rules of inference for quantified statements are used in examples and example show that the premises everyone in this discrete mathematics class has taken a course in computer science and marla is a student in this class imply the conclusion marla has taken a course in computer science solution let d x denote x is in this discrete mathematics class and let c x denote x has taken a course in computer science then the premises are x d x c x and d marla the conclusion is c marla the following steps can be used to establish the conclusion from the premises step reason x d x c x premise d marla c marla universal instantiation from d marla premise c marla modus ponens from and example show that the premises a student in this class has not read the book and everyone in this class passed the first exam imply the conclusion someone who passed the first exam has not read the book solution let c x be x is in this class b x be x has read the book and p x be x passed the first exam the premises are x c x b x and x c x p x the conclusion is x p x b x these steps can be used to establish the conclusion from the premises step reason x c x b x premise c a b a existential instantiation from c a simplification from x c x p x premise c a p a universal instantiation from p a modus ponens from and b a simplification from p a b a conjunction from and x p x b x existential generalization from combining rules of inference for propositions and quantified statements we have developed rules of inference both for propositions and for quantified statements note that in our arguments in examples and we used both universal instantiation a rule of inference for quantified statements and modus ponens a rule of inference for propositional logic we will often need to use this combination of rules of inference because universal instantiation and modus ponens are used so often together this combination of rules is sometimes called universal modus ponens this rule tells us that if x p x q x is true and if p a is true for a particular element a in the domain of the universal quantifier then q a must also be true to see this note that by universal instantiation p a q a is true then by modus ponens q a must also be true we can describe universal modus ponens as follows x p x q x p a where a is a particular element in the domain q a universal modus ponens is commonly used in mathematical arguments this is illustrated in example example assume that for all positive integers n if n is greater than then is less than is true use universal modus ponens to show that solution let p n denote n and q n denote the statement for all positive integers n if n is greater than then is less than can be represented by n p n q n where the domain consists of all positive integers we are assuming that n p n q n is true note that p is true because it follows by universal modus ponens that q is true namely that another useful combination of a rule of inference from propositional logic and a rule of inference for quantified statements is universal modus tollens universal modus tollens exercises combines universal instantiation and modus tollens and can be expressed in the following way x p x q x q a where a is a particular element in the domain p a the verification of universal modus tollens is left as exercise exercises develop additional combinations of rules of inference in propositional logic and quantified statements find the argument form for the following argument and determine whether it is valid can we conclude that the conclusion is true if the premises are true if socrates is human then socrates is mortal socrates is human socrates is mortal find the argument form for the following argument and determine whether it is valid can we conclude that the conclusion is true if the premises are true if george does not have eight legs then he is not a spider george is a spider george has eight legs what rule of inference is used in each of these argu ments a alice is a mathematics major therefore alice is ei ther a mathematics major or a computer science major b jerry is a mathematics major and a computer science major therefore jerry is a mathematics major c if it is rainy then the pool will be closed it is rainy therefore the pool is closed d if it snows today the university will close the uni versity is not closed today therefore it did not snow today e if i go swimming then i will stay in the sun too long if i stay in the sun too long then i will sunburn there fore if i go swimming then i will sunburn what rule of inference is used in each of these arguments a kangaroos live in australia and are marsupials there fore kangaroos are marsupials b it is either hotter than degrees today or the pollu tion is dangerous it is less than degrees outside today therefore the pollution is dangerous c linda is an excellent swimmer if linda is an excellent swimmer then she can work as a lifeguard therefore linda can work as a lifeguard d steve will work at a computer company this summer therefore this summer steve will work at a computer company or he will be a beach bum e if i work all night on this homework then i can an swer all the exercises if i answer all the exercises i will understand the material therefore if i work all night on this homework then i will understand the material use rules of inference to show that the hypotheses randy works hard if randy works hard then he is a dull boy and if randy is a dull boy then he will not get the job imply the conclusion randy will not get the job use rules of inference to show that the hypotheses if it does not rain or if it is not foggy then the sailing race will be held and the lifesaving demonstration will go on if the sailing race is held then the trophy will be awarded and the trophy was not awarded imply the conclusion it rained what rules of inference are used in this famous argu ment all men are mortal socrates is a man therefore socrates is mortal what rules of inference are used in this argument no man is an island manhattan is an island therefore man hattan is not a man for each of these collections of premises what relevant conclusion or conclusions can be drawn explain the rules of inference used to obtain each conclusion from the premises a if i take the day off it either rains or snows i took tuesday off or i took thursday off it was sunny on tuesday it did not snow on thursday b if i eat spicy foods then i have strange dreams i have strange dreams if there is thunder while i sleep i did not have strange dreams c i am either clever or lucky i am not lucky if i am lucky then i will win the lottery d every computer science major has a personal com puter ralph does not have a personal computer ann has a personal computer e what is good for corporations is good for the united states what is good for the united states is good for you what is good for corporations is for you to buy lots of stuff f all rodents gnaw their food mice are rodents rabbits do not gnaw their food bats are not ro dents for each of these sets of premises what relevant conclu sion or conclusions can be drawn explain the rules of in ference used to obtain each conclusion from the premises a if i play hockey then i am sore the next day i use the whirlpool if i am sore i did not use the whirlpool b if i work it is either sunny or partly sunny i worked last monday or i worked last friday it was not sunny on tuesday it was not partly sunny on friday c all insects have six legs dragonflies are insects spiders do not have six legs spiders eat dragon flies d every student has an internet account homer does not have an internet account maggie has an internet account e all foods that are healthy to eat do not taste good tofu is healthy to eat you only eat what tastes good you do not eat tofu cheeseburgers are not healthy to eat f i am either dreaming or hallucinating i am not dreaming if i am hallucinating i see elephants run ning down the road show that the argument form with premises pn and conclusion q r is valid if the argument form with premises pn q and conclusion r is valid show that the argument form with premises p t r q u t u p and and conclusion q r is valid by first using exercise and then us ing rules of inference from table for each of these arguments explain which rules of in ference are used for each step a doug a student in this class knows how to write programs in java everyone who knows how to write programs in java can get a high paying job there fore someone in this class can get a high paying job b somebody in this class enjoys whale watching ev ery person who enjoys whale watching cares about ocean pollution therefore there is a person in this class who cares about ocean pollution c each of the students in this class owns a personal computer everyone who owns a personal computer can use a word processing program therefore zeke a student in this class can use a word processing pro gram d everyone in new jersey lives within miles of the ocean someone in new jersey has never seen the ocean therefore someone who lives within miles of the ocean has never seen the ocean for each of these arguments explain which rules of in ference are used for each step a linda a student in this class owns a red convertible everyone who owns a red convertible has gotten at least one speeding ticket therefore someone in this class has gotten a speeding ticket b each of five roommates melissa aaron ralph ve neesha and keeshawn has taken a course in discrete mathematics every student who has taken a course in discrete mathematics can take a course in algorithms therefore all five roommates can take a course in algorithms next year c all movies produced by john sayles are wonder ful john sayles produced a movie about coal miners therefore there is a wonderful movie about coal min ers d there is someone in this class who has been to france everyone who goes to france visits the louvre therefore someone in this class has visited the louvre for each of these arguments determine whether the argu ment is correct or incorrect and explain why a all students in this class understand logic xavier is a student in this class therefore xavier understands logic b every computer science major takes discrete math ematics natasha is taking discrete mathematics therefore natasha is a computer science major c all parrots like fruit my pet bird is not a parrot there fore my pet bird does not like fruit d everyone who eats granola every day is healthy linda is not healthy therefore linda does not eat granola every day for each of these arguments determine whether the argu ment is correct or incorrect and explain why a everyone enrolled in the university has lived in a dor mitory mia has never lived in a dormitory therefore mia is not enrolled in the university b a convertible car is fun to drive isaac car is not a convertible therefore isaac car is not fun to drive c quincy likes all action movies quincy likes the movie eight men out therefore eight men out is an action movie d all lobstermen set at least a dozen traps hamilton is a lobsterman therefore hamilton sets at least a dozen traps what is wrong with this argument let h x be x is happy given the premise xh x we conclude that h lola therefore lola is happy what is wrong with this argument let s x y be x is shorter than y given the premise ss max it follows that s max max then by existential generalization it follows that xs x x so that someone is shorter than himself determine whether each of these arguments is valid if an argument is correct what rule of inference is being used if it is not what logical error occurs a if n is a real number such that n then suppose that then n b if n is a real number with n then suppose that then n c if n is a real number with n then suppose that n then determine whether these are valid arguments a if x is a positive real number then is a positive real number therefore if is positive where a is a real number then a is a positive real number b if where x is a real number then x let a be a real number with then a which rules of inference are used to establish the conclusion of lewis carroll argument described in example of section which rules of inference are used to establish the conclusion of lewis carroll argument described in example of section identify the error or errors in this argument that sup posedly shows that if xp x xq x is true then x p x q x is true xp x xq x premise xp x simplification from p c existential instantiation from xq x simplification from q c existential instantiation from p c q c conjunction from and x p x q x existential generalization identify the error or errors in this argument that sup posedly shows that if x p x q x is true then xp x xq x is true x p x q x premise p c q c universal instantiation from p c simplification from xp x universal generalization from q c simplification from xq x universal generalization from x p x xq x conjunction from and justify the rule of universal modus tollens by showing that the premises x p x q x and q a for a particular element a in the domain imply p a justify the rule of universal transitivity which states that if x p x q x and x q x r x are true then x p x r x is true where the domains of all quantifiers are the same use rules of inference to show that if x p x q x s x and x p x r x are true then x r x s x is true use rules of inference to show that if x p x q x and x p x q x r x are true then x r x p x is also true where the domains of all quantifiers are the same use rules of inference to show that if x p x q x x q x s x x r x s x and x p x are true then x r x is true use resolution to show the hypotheses allen is a bad boy or hillary is a good girl and allen is a good boy or david is happy imply the conclusion hillary is a good girl or david is happy use resolution to show that the hypotheses it is not rain ing or yvette has her umbrella yvette does not have her umbrella or she does not get wet and it is raining or yvette does not get wet imply that yvette does not get wet show that the equivalence p p f can be derived using resolution together with the fact that a condi tional statement with a false hypothesis is true hint let q r f in resolution use resolution to show that the compound propo sition p q p q p q p q is not satisfiable the logic problem taken from wff n proof the game of logic has these two assumptions logic is difficult or not many students like logic if mathematics is easy then logic is not difficult by translating these assumptions into statements involv ing propositional variables and logical connectives deter mine whether each of the following are valid conclusions of these assumptions a that mathematics is not easy if many students like logic b that not many students like logic if mathematics is not easy c that mathematics is not easy or logic is difficult d that logic is not difficult or mathematics is not easy e that if not many students like logic then either math ematics is not easy or logic is not difficult determine whether this argument taken from kalish and montague is valid if superman were able and willing to prevent evil he would do so if superman were unable to prevent evil he would be impotent if he were unwilling to prevent evil he would be malevolent superman does not prevent evil if superman exists he is nei ther impotent nor malevolent therefore superman does not exist introduction to proofs introduction in this section we introduce the notion of a proof and describe methods for constructing proofs a proof is a valid argument that establishes the truth of a mathematical statement a proof can use the hypotheses of the theorem if any axioms assumed to be true and previously proven theorems using these ingredients and rules of inference the final step of the proof establishes the truth of the statement being proved in our discussion we move from formal proofs of theorems toward more informal proofs the arguments we introduced in section to show that statements involving propositions and quantified statements are true were formal proofs where all steps were supplied and the rules for each step in the argument were given however formal proofs of useful theorems can be extremely long and hard to follow in practice the proofs of theorems designed for human consumption are almost always informal proofs where more than one rule of inference may be used in each step where steps may be skipped where the axioms being assumed and the rules of inference used are not explicitly stated informal proofs can often explain to humans why theorems are true while computers are perfectly happy producing formal proofs using automated reasoning systems the methods of proof discussed in this chapter are important not only because they are used to prove mathematical theorems but also for their many applications to computer science these applications include verifying that computer programs are correct establishing that operating systems are secure making inferences in artificial intelligence showing that system specifica tions are consistent and so on consequently understanding the techniques used in proofs is essential both in mathematics and in computer science some terminology formally a theorem is a statement that can be shown to be true in mathematical writing the term theorem is usually reserved for a statement that is considered at least somewhat important less important theorems sometimes are called propositions theorems can also be referred to as facts or results a theorem may be the universal quantification of a conditional statement with one or more premises and a conclusion however it may be some other type of logical statement as the examples later in this chapter will show we demonstrate that a theorem is true with a proof a proof is a valid argument that establishes the truth of a theorem the statements used in a proof can include axioms or postulates which are statements we assume to be true for example the axioms for the real numbers given in appendix and the axioms of plane geometry the premises if any of the theorem and previously proven theorems axioms may be stated using primitive terms that do not require definition but all other terms used in theorems and their proofs must be defined rules of inference together with definitions of terms are used to draw conclusions from other assertions tying together the steps of a proof in practice the final step of a proof is usually just the conclusion of the theorem however for clarity we will often recap the statement of the theorem as the final step of a proof a less important theorem that is helpful in the proof of other results is called a lemma plural lemmas or lemmata complicated proofs are usually easier to understand when they are proved using a series of lemmas where each lemma is proved individually a corollary is a theorem that can be established directly from a theorem that has been proved a conjecture is a statement that is being proposed to be a true statement usually on the basis of some partial evidence a heuristic argument or the intuition of an expert when a proof of a conjecture is found the conjecture becomes a theorem many times conjectures are shown to be false so they are not theorems understanding how theorems are stated before we introduce methods for proving theorems we need to understand how many math ematical theorems are stated many theorems assert that a property holds for all elements in a domain such as the integers or the real numbers although the precise statement of such theorems needs to include a universal quantifier the standard convention in mathematics is to omit it for example the statement if x y where x and y are positive real numbers then really means for all positive real numbers x and y if x y then furthermore when theorems of this type are proved the first step of the proof usually involves selecting a general element of the domain subsequent steps show that this element has the property in question finally universal generalization implies that the theorem holds for all members of the domain methods of proving theorems proving mathematical theorems can be difficult to construct proofs we need all available am munition including a powerful battery of different proof methods these methods provide the overall approach and strategy of proofs understanding these methods is a key component of learning how to read and construct mathematical proofs one we have chosen a proof method we use axioms definitions of terms previously proved results and rules of inference to com plete the proof note that in this book we will always assume the axioms for real numbers found in appendix we will also assume the usual axioms whenever we prove a result about geometry when you construct your own proofs be careful not to use anything but these axioms definitions and previously proved results as facts to prove a theorem of the form x p x q x our goal is to show that p c q c is true where c is an arbitrary element of the domain and then apply universal generalization in this proof we need to show that a conditional statement is true because of this we now focus on methods that show that conditional statements are true recall that p q is true unless p is true but q is false note that to prove the statement p q we need only show that q is true if p is true the following discussion will give the most common techniques for proving conditional statements later we will discuss methods for proving other types of statements in this section and in section we will develop a large arsenal of proof techniques that can be used to prove a wide variety of theorems when you read proofs you will often find the words obviously or clearly these words indicate that steps have been omitted that the author expects the reader to be able to fill in unfortunately this assumption is often not warranted and readers are not at all sure how to fill in the gaps we will assiduously try to avoid using these words and try not to omit too many steps however if we included all steps in proofs our proofs would often be excruciatingly long direct proofs a direct proof of a conditional statement p q is constructed when the first step is the assumption that p is true subsequent steps are constructed using rules of inference with the final step showing that q must also be true a direct proof shows that a conditional statement p q is true by showing that if p is true then q must also be true so that the combination p true and q false never occurs in a direct proof we assume that p is true and use axioms definitions and previously proven theorems together with rules of inference to show that q must also be true you will find that direct proofs of many results are quite straightforward with a fairly obvious sequence of steps leading from the hypothesis to the conclusion however direct proofs sometimes require particular insights and can be quite tricky the first direct proofs we present here are quite straightforward later in the text you will see some that are less obvious we will provide examples of several different direct proofs before we give the first example we need to define some terminology definition example give a direct proof of the theorem if n is an odd integer then is odd solution note that this theorem states np n q n where p n is n is an odd integer and q n is is odd as we have said we will follow the usual convention in mathematical proofs by showing that p n implies q n and not explicitly using universal instantiation to begin a direct proof of this theorem we assume that the hypothesis of this conditional statement is true namely we assume that n is odd by the definition of an odd integer it follows that n where k is some integer we want to show that is also odd we can square both sides of the equation n to obtain a new equation that expresses when we do this we find that by the definition of an odd integer we can conclude that is an odd integer it is one more than twice an integer consequently we have proved that if n is an odd integer then is an odd integer example give a direct proof that if m and n are both perfect squares then nm is also a perfect square an integer a is a perfect square if there is an integer b such that a solution to produce a direct proof of this theorem we assume that the hypothesis of this conditional statement is true namely we assume that m and n are both perfect squares by the definition of a perfect square it follows that there are integers and t such that m and n t the goal of the proof is to show that mn must also be a perfect square when m and n are looking ahead we see how we can show this by substituting for m and t for n into mn this tells us that mn hence mn ss tt st st st using commutativity and associativity of multiplication by the definition of perfect square it follows that mn is also a perfect square because it is the square of st which is an integer we have proved that if m and n are both perfect squares then mn is also a perfect square proof by contraposition direct proofs lead from the premises of a theorem to the conclusion they begin with the premises continue with a sequence of deductions and end with the conclusion however we will see that attempts at direct proofs often reach dead ends we need other methods of proving theorems of the form x p x q x proofs of theorems of this type that are not direct proofs that is that do not start with the premises and end with the conclusion are called indirect proofs an extremely useful type of indirect proof is known as proof by contraposition proofs by contraposition make use of the fact that the conditional statement p q is equivalent to its contrapositive q p this means that the conditional statement p q can be proved by showing that its contrapositive q p is true in a proof by contraposition of p q we take q as a premise and using axioms definitions and previously proven theorems together with rules of inference we show that p must follow we will illustrate proof by contraposition with two examples these examples show that proof by contraposition can succeed when we cannot easily find a direct proof example prove that if n is an integer and is odd then n is odd solution we first attempt a direct proof to construct a direct proof we first assume that is an odd integer this means that for some integer k can we use this fact to show that n is odd we see that but there does not seem to be any direct way to conclude that n is odd because our attempt at a direct proof failed we next try a proof by contraposition the first step in a proof by contraposition is to assume that the conclusion of the conditional statement if is odd then n is odd is false namely assume that n is even then by the definition of an even integer n for some integer k substituting for n we find that this tells us that is even because it is a multiple of and therefore not odd this is the negation of the premise of the theorem because the negation of the conclusion of the conditional statement implies that the hypothesis is false the original conditional statement is true our proof by contraposition succeeded we have proved the theorem if is odd then n is odd example prove that if n ab where a and b are positive integers then a n or b n solution because there is no obvious way of showing that a n or b n directly from the equation n ab where a and b are positive integers we attempt a proof by contraposition the first step in a proof by contraposition is to assume that the c onclusion of the conditional statement if n ab where a and b a re positive int egers then a n or b n is false that is we assume that the statement a n b n is false using t he meaning o f disjunction together with de morg an law we ee that this implies that both a n and b n are false this implies that a n and b n we can multiply these inequ alitie together using the fact that if t and u v then su tv to obtain ab that ab n which contradicts the statement n ab n n n this shows because the negation of the conclusion of the conditional statement implies that the hypoth esis is false the original conditional statement is true our proof by contrapos ition succe eded we have proved that if n ab where a and b are positive integers then a n or b n vacuous and trivial proofs we can quickly prove that a conditional statement p q is true when we know that p is false because p q must be true when p is false consequently if we can show that p is false then we have a proof called a vacuous proof of the conditional statement p q vacuous proofs are often used to establish special cases of theorems that state that a conditional statement is true for all positive integers i e a theorem of the kind np n where p n is a propositional function proof techniques for theorems of this kind will be discussed in section example show that the proposition p is true where p n is if n then n and the domain consists of all integers solution note that p is if then we can show p using a vacuous proof indeed the hypothesis is false this tells us that p is automatically true remark the fact that the conclusion of this conditional statement is false is irrelevant to the truth value of the conditional statement because a conditional statement with a false hypothesis is guaranteed to be true we can also quickly prove a conditional statement p q if we know that the conclusion q is true by showing that q is true it follows that p q must also be true a proof of p q that uses the fact that q is true is called a trivial proof trivial proofs are often important when special cases of theorems are proved see the discussion of proof by cases in section and in mathematical induction which is a proof technique discussed in section example let p n be if a and b are positive integers with a b then an bn where the domain consists of all nonnegative integers show that p is true solution the proposition p is if a b then because the conclusion of the conditional statement if a b then is true hence this conditional statement which is p is true this is an example of a trivial proof note that the hypothesis which is the statement a b was not needed in this proof a little proof strategy we have described two important approaches for proving theorems of the form x p x q x direct proof and proof by contraposition we have also given examples that show how each is used however when you are presented with a theorem of the form x p x q x which method should you use to attempt to prove it we will provide a few rules of thumb here in section we will discuss proof strategy at greater length when you want to prove a statement of the form x p x q x first evaluate whether a direct proof looks promising begin by expanding the definitions in the hypotheses start to reason using these hypotheses together with axioms and available theorems if a direct proof does not seem to go anywhere try the same thing with a proof by contraposition recall that in a proof by contraposition you assume that the conclusion of the conditional statement is false and use a direct proof to show this implies that the hypothesis must be false we illustrate this strategy in examples and before we present our next example we need a definition definition example prove that the sum of two rational numbers is rational note that if we include the implicit quantifiers here the theorem we want to prove is for every real number r and every real number if r and are rational numbers then r is rational solution we first attempt a direct proof to begin suppose that r and are rational numbers from the definition of a rational number it follows that there are integers p and q with q such that r p q and integers t and u with u such that t u can we use this information to show that r is rational the obvious next step is to add r p q and t u to obtain p t r pu qt because q and u it follows that qu consequently we have expressed r as the ratio of two integers pu qt and qu where qu this means that r is rational we have proved that the sum of two rational numbers is rational our attempt to find a direct proof succeeded example prove that if n is an integer and is odd then n is odd solution we first attempt a direct proof suppose that n is an integer and is odd then there exists an integer k such that can we use this information to show that n is odd there seems to be n o obvious approach to show that n is odd because solving for n produces the equation n which is not terribly useful because this attempt to use a direct proof did not bear fruit we next attempt a proof by contraposition we take as our hypothesis the statement that n is not odd because every integer is odd or even this means that n is even this implies that there exists an integer k such that n to prove the theorem we need to show that this hypothesis implies the conclusion that is not odd that is that is even can we use the equation n to achieve this by squaring both sides of this equation we obtain which implies that is also even because where t we have proved that if n is an integer and is odd then n is odd our attempt to find a proof by contraposition succeeded proofs by contradiction suppose we want to prove that a statement p is true furthermore suppose that we can find a contradiction q such that p q is true because q is false but p q is true we can conclude that p is false which means that p is true how can we find a contradiction q that might help us prove that p is true in this way because the statement r r is a contradiction whenever r is a proposition we can prove that p is true if we can show that p r r is true for some proposition r proofs of this type are called proofs by contradiction because a proof by contradiction does not prove a result directly it is another type of indirect proof we provide three examples of proof by contradiction the first is an example of an application of the pigeonhole principle a combinatorial technique that we will cover in depth in section example show that at least four of any days must fall on the same day of the week solution let p be the proposition at least four of chosen days fall on the same day of the week suppose that p is true this means that at most three of the days fall on the same day of the week because there are seven days of the week this implies that at most days could have been chosen as for each of the days of the week at most three of the chosen days could fall on that day this contradicts the premise that we have days under consideration that is if r is the statement that days are chosen then we have shown that p r r consequently we know that p is true we have proved that at least four of chosen days fall on the same day of the week example prove that is irrational by giving a proof by contradiction solution let p be the proposition is irrational to start a proof by co ntradiction we suppose that p i true note that p is the statement it is not the case that is irrational which says th at is rational we will show that assuming that p is true leads to a contradiction if is rational there exist integers a and b with a b where b and a and b have no common factors so that the fraction a b is in lowest terms h ere we are using the fact that every rational number can be written in lowest terms because sides of this equation are squared it follows that hence a b when both by the definition of an even integer it follows that is even we next use the fact that if is even a must also be even which follows by exercise furthermore because a is even by the definition of an even integer a for some integer c thus dividing both sides of this equation by gives by the definition of even this means that is even again using the fact that if the square of an integer is even then the integer itself must be even we conclude that b must be even as well we have now shown that the assumption of p leads to the equation a b where a and b have no common factors but both a and b are even that is divides both a and b note that the statement that a b where a and b have no common factors means in particular that does not divide both a and b because our assumption of p leads to the contradiction that divides both a and b and does not divide both a and b p must be false that is the statement p is irrational is true we have proved that is irrational proof by contradiction can be used to prove conditional statements in such proofs we first assume the negation of the conclusion we then use the premises of the theorem and the negation of the conclusion to arrive at a contradiction the reason that such proofs are valid rests on the logical equivalence of p q and p q f to see that these statements are equivalent simply note that each is false in exactly one case namely when p is true and q is false note that we can rewrite a proof by contraposition of a conditional statement as a proof by contradiction in a proof of p q by contraposition we assume that q is true we then show that p must also be true to rewrite a proof by contraposition of p q as a proof by contradiction we suppose that both p and q are true then we use the steps from the proof of q p to show that p is true this leads to the contradiction p p completing the proof example illustrates how a proof by contraposition of a conditional statement can be rewritten as a proof by contradiction example give a proof by contradiction of the theorem if is odd then n is odd solution let p be is odd and q be n is odd to construct a proof by contradiction assume that both p and q are true that is assume that is odd and that n is not odd because n is not odd we know that it is even because n is even there is an integer k such that n this implies that because is where t is even note that the statement is even is equivalent to the statement p because an integer is even if and only if it is not odd because both p and p are true we have a contradiction this completes the proof by contradiction proving that if is odd then n is odd note that we can also prove by contradiction that p q is true by assuming that p and q are true and showing that q must be also be true this implies that q and q are both true a contradiction this observation tells us that we can turn a direct proof into a proof by contradiction proofs of equivalence to prove a theorem that is a biconditional statement that is a statement of the form p q we show that p q and q p are both true the validity of this approach is based on the tautology p q p q q p example prove the theorem if n is an integer then n is odd if and only if is odd solution this theorem has the form p if and only if q where p is n is odd and q is is odd as usual we do not explicitly deal with the universal quantification to prove this theorem we need to show that p q and q p are true we have already shown in example that p q is true and in example that q p is true because we have shown that both p q and q p are true we have shown that the theorem is true sometimes a theorem states that several propositions are equivalent such a theorem states that propositions pn are equivalent this can be written as pn which states that all n propositions have the same truth values and consequently that for all i and j with i n and j n pi and pj are equivalent one way to prove these mutually equivalent is to use the tautology pn pn this shows that if the n conditional statements pn can be shown to be true then the propositions pn are all equivalent this is much more efficient than proving that pi pj for all i j with i n and j n note that there are n such conditional statements when we prove that a group of statements are equivalent we can establish any chain of conditional statements we choose as long as it is possible to work through the chain to go from any one of these statements to any other statement for example we can show that and are equivalent by showing that and example show that these statements about the integer n are equivalent n is even n is odd is even solution we will show that these three statements are equivalent by showing that the conditional statements and are true we use a direct proof to show that suppose that n is even then n for some integer k consequently n k this means that n is odd because it is of the form where m is the integer k we also use a direct proof to show that now suppose n is odd then n for some integer k hence n so that 8k this means that is twice the integer and hence is even to prove we use a proof by contraposition that is we prove that if n is not even then is not even this is the same as proving that if n is odd then is odd which we have already done in example this completes the proof counterexamples in section we stated that to show that a statement of the form xp x is false we need only find a counterexample that is an example x for which p x is false when presented with a statement of the form xp x which we believe to be false or which has resisted all proof attempts we look for a counterexample we illustrate the use of counterexamples in example example show that the statement every positive integer is the sum of the squares of two integers is false solution to show that this statement is false we look for a counterexample which is a particular integer that is not the sum of the squares of two integers it does not take long to find a counterex ample because cannot be written as the sum of the squares of two integers to show this is the case note that the only perfect squares not exceeding are and furthermore there is no way to get as the sum of two terms each of which is or consequently we have shown that every positive integer is the sum of the squares of two integers is false mistakes in proofs there are many common errors made in constructing mathematical proofs we will briefly describe some of these here among the most common errors are mistakes in arithmetic and basic algebra even professional mathematicians make such errors especially when working with complicated formulae whenever you use such computations you should check them as carefully as possible you should also review any troublesome aspects of basic algebra especially before you study section each step of a mathematical proof needs to be correct and the conclusion needs to follow logically from the steps that precede it many mistakes result from the introduction of steps that do not logically follow from those that precede it this is illustrated in examples example what is wrong with this famous supposed proof that proof we use these steps where a and b are two equal positive integers step reason a b given a ab multiply both sides of by a a ab subtract from both sides of a b a b b a b factor both sides of a b b divide both sides of by a b b replace a by b in because a b and simplify divide both sides of by b solution every step is valid except for one step where we divided both sides by a b the error is that a b equals zero division of both sides of an equation by the same quantity is valid as long as this quantity is not zero example what is wrong with this proof theorem if is positive then n is positive proof suppose that is positive because the conditional statement if n is positive then is positive is true we can conclude that n is positive solution let p n be n is positive and q n be is positive then our hypothesis is q n the statement if n is positive then is positive is the statement n p n q n from the hypothesis q n and the statement n p n q n we cannot conclude p n because we are not using a valid rule of inference instead this is an example of the fallacy of affirming the conclusion a counterexample is supplied by n for which is positive but n is negative example what is wrong with this proof theorem if n is not positive then is not positive this is the contrapositive of the theorem in example proof suppose that n is not positive because the conditional statement if n is positive then is positive is true we can conclude that is not positive solution let p n and q n be as in the solution of example then our hypothesis is p n and the statement if n is positive then is positive is the statement n p n q n from the hypothesis p n and the statement n p n q n we cannot conclude q n because we are not using a valid rule of inference instead this is an example of the fallacy of denying the hypothesis a counterexample is supplied by n as in example finally we briefly discuss a particularly nasty type of error many incorrect arguments are based on a fallacy called begging the question this fallacy occurs when one or more steps of a proof are based on the truth of the statement being proved in other words this fallacy arises when a statement is proved using itself or a statement equivalent to it that is why this fallacy is also called circular reasoning example is the following argument correct it supposedly shows that n is an even integer whenever is an even integer suppose that is even then for some integer k let n for some integer l this shows that n is even solution this argument is incorrect the statement let n for some integer l occurs in the proof no argument has been given to show that n can be written as for some integer l this is circular reasoning because this statement is equivalent to the statement being proved namely n is even of course the result itself is correct only the method of proof is wrong making mistakes in proofs is part of the learning process when you make a mistake that someone else finds you should carefully analyze where you went wrong and make sure that you do not make the same mistake again even professional mathematicians make mistakes in proofs more than a few incorrect proofs of important results have fooled people for many years before subtle errors in them were found just a beginning we have now developed a basic arsenal of proof methods in the next section we will introduce other important proof methods we will also introduce several important proof techniques in chapter including mathematical induction which can be used to prove results that hold for all positive integers in chapter we will introduce the notion of combinatorial proofs in this section we introduced several methods for proving theorems of the form x p x q x including direct proofs and proofs by contraposition there are many theorems of this type whose proofs are easy to construct by directly working through the hypotheses and def initions of the terms of the theorem however it is often difficult to prove a theorem without resorting to a clever use of a proof by contraposition or a proof by contradiction or some other proof technique in section we will address proof strategy we will describe various approaches that can be used to find proofs when straightforward approaches do not work con structing proofs is an art that can be learned only through experience including writing proofs having your proofs critiqued and reading and analyzing other proofs exercises use a direct proof to show that the sum of two odd integers is even use a direct proof to show that the sum of two even inte gers is even show that the square of an even number is an even number using a direct proof show that the additive inverse or negative of an even number is an even number using a direct proof prove that if m n and n p are even integers where m n and p are integers then m p is even what kind of proof did you use use a direct proof to show that the product of two odd numbers is odd use a direct proof to show that every odd integer is the difference of two squares prove that if n is a perfect square then n is not a perfect square use a proof by contradiction to prove that the sum of an irrational number and a rational number is irrational use a direct proof to show that the product of two rational numbers is rational prove or disprove that the product of two irrational num bers is irrational prove or disprove that the product of a nonzero rational number and an irrational number is irrational prove that if x is irrational then x is irrational prove that if x is rational and x then x is rational use a proof by contraposition to show that if x y where x and y are real numbers then x or y prove that if m and n are integers and mn is even then m is even or n is even show that if n is an integer and is odd then n is even using a a proof by contraposition b a proof by contradiction prove that if n is an integer and is even then n is show that at least ten of any days chosen must fall on the same day of the week show that at least three of any days chosen must fall in the same month of the year use a proof by contradiction to show that there is no ratio nal number r for which r hint assume that r a b is a root where a and b are integers and a b is in lowest terms obtain an equation involving integers by multiplying by then look at whether a and b are each odd or even prove that if n is a positive integer then n is even if and only if is even prove that if n is a positive integer then n is odd if and only if is odd prove that if and only if m n or m n prove or disprove that if m and n are integers such that mn then either m and n or else m and n show that these three statements are equivalent where a and b are real numbers i a is less than b ii the average of a and b is greater than a and iii the average of a and b is less than b show that these statements about the integer x are equiv alent i is even ii x is odd iii is even show that these statements about the real number x are equivalent i x is rational ii x is rational iii is rational show that these statements about the real number x are equivalent i x is irrational ii is irrational iii x is irrational is this reasoning for finding the solutions of the equa tion x correct x is given obtained by squaring both sides of obtained by subtracting from both sides of x x obtained by factor ing the left hand side of x x or x which follows because ab implies that a or even using a a proof by contraposition b are these steps fo r finding the solutions of x a a proof by contradiction x correct x x is given x x obtained by squaring both sides of prove the proposition p where p n is the proposi tion if n is a positive integer greater than then n what kind of proof did you use prove the proposition p where p n is the proposi tion if n is a positive integer then n what kind of proof did you use let p n be the proposition if a and b are positive real numbers then a b n an bn prove that p is true what kind of proof did you use show that if you pick three socks from a drawer contain ing just blue socks and black socks you must get either a pair of blue socks or a pair of black socks obtained by subtracting x from both sides of x x obtained by factoring the right hand side of x or x which follows from because ab implies that a or b show that the propositions and can be shown to be equivalent by showing that and show that the propositions and can be shown to be equivalent by proving that the conditional statements and are true find a counterexample to the statement that every posi tive integer can be written as the sum of the squares of three integers prove that at least one of the real numbers an is greater than or equal to the average of these numbers what kind of proof did you use use exercise to show that if the first positive inte gers are placed around a circle in any order there exist three integers in consecutive locations around the circle that have a sum greater than or equal to prove that if n is an integer these four statements are equivalent i n is even ii n is odd iii is odd iv is even prove that these four statements about the integer n are equivalent i is odd ii n is even iii is odd iv is even proof methods and strategy introduction in section we introduced many methods of proof and illustrated how each method can be used in this section we continue this effort we will introduce several other commonly used proof methods including the method of proving a theorem by considering different cases separately we will also discuss proofs where we prove the existence of objects with desired properties in section we briefly discussed the strategy behind constructing proofs this strategy includes selecting a proof method and then successfully constructing an argument step by step based on this method in this section after we have developed a versatile arsenal of proof methods we will study some aspects of the art and science of proofs we will provide advice on how to find a proof of a theorem we will describe some tricks of the trade including how proofs can be found by working backward and by adapting existing proofs when mathematicians work they formulate conjectures and attempt to prove or disprove them we will briefly describe this process here by proving results about tiling checkerboards with dominoes and other types of pieces looking at tilings of this kind we will be able to quickly formulate conjectures and prove theorems without first developing a theory we will conclude the section by discussing the role of open questions in particular we will discuss some interesting problems either that have been solved after remaining open for hundreds of years or that still remain open exhaustive proof and proof by cases sometimes we cannot prove a theorem using a single argument that holds for all possible cases we now introduce a method that can be used to prove a theorem by considering different cases separately this method is based on a rule of inference that we will now introduce to prove a conditional statement of the form pn q the tautology pn q q q pn q can be used as a rule of inference this shows that the original conditional statement with a hypothesis made up of a disjunction of the propositions pn can be proved by proving each of the n conditional statements pi q i n individually such an argument is called a proof by cases sometimes to prove that a conditional statement p q is true it is convenient to use a disjunction pn instead of p as the hypothesis of the conditional statement where p and pn are equivalent exhaustive proof some theorems can be proved by examining a relatively small number of examples such proofs are called exhaustive proofs or proofs by exhaustion because these proofs proceed by exhausting all possibilities an exhaustive proof is a special type of proof by cases where each case involves checking a single example we now provide some illustrations of exhaustive proofs example prove that n if n is a positive integer with n solution we use a proof by exhaustion we only need verify the inequality n when n and for n we have n and for n we have n and for n we have n and and for n we have n and in each of these four cases we see that n we have used the method of exhaustion to prove that n if n is a positive integer with n example prove that the only consecutive positive integers not exceeding that are perfect powers are and an integer is a perfect power if it equals na where a is an integer greater than proofs by exhaustion can tire out people and computers when the number of cases challenges the available processing power solution we use a proof by exhaustion in particular we can prove this fact by examining positive integers n not exceeding first checking whether n is a perfect power and if it is checking whether n is also a perfect power a quicker way to do this is simply to look at all perfect powers not exceeding and checking whether the next largest integer is also a perfect power the squares of positive integers not exceeding are and the cubes of positive integers not exceeding are and the fourth powers of positive integers not exceeding are and the fifth powers of positive integers not exceeding are and the sixth powers of positive integers not exceeding are and there are no powers of positive integers higher than the sixth power not exceeding other than looking at this list of perfect powers not exceeding we see that n is the only perfect power n for which n is also a perfect power that is and are the only two consecutive perfect powers not exceeding people can carry out exhaustive proofs when it is necessary to check only a relatively small number of instances of a statement computers do not complain when they are asked to check a much larger number of instances of a statement but they still have limitations note that not even a computer can check all instances when it is impossible to list all instances to check proof by cases a proof by cases must cover all possible cases that arise in a theorem we illustrate proof by cases with a couple of examples in each example you should check that all possible cases are covered example prove that if n is an integer then n solution we can prove that n for every integer by considering three cases when n when n and when n we split the proof into three cases because it is straightforward to prove the result by considering zero positive integers and negative integers separately case i when n because we see that it follows that n is true in this case case ii when n when we multiply both sides of the inequality n by the positive integer n we obtain n n n this implies that n for n case iii in this case n however it follows that n because the inequality n holds in all three cases we can conclude that if n is an integer then n example use a proof by cases to show that xy x y where x and y are real numbers recall that a the absolute value of a equals a when a and equals a when a solution in our proof of this theorem we remove absolute values using the fact that a a when a and a a when a because both x and y occur in our formula we will need four cases i x and y both nonnegative ii x nonnegative and y is negative iii x negative and y nonnegative and iv x negative and y negative we denote by and the proposition stating the assumption for each of these four cases respectively note that we can remove the absolute value signs by making the appropriate choice of signs within each case case i we see that q because xy when x and y so that xy xy x y case ii to see that q note that if x and y then xy so that xy xy x y x y here because y we have y y case iii to see that q we follow the same reasoning as the previous case with the roles of x and y reversed case iv to see that q note that when x and y it follows that xy hence xy xy x y x y because xy x y holds in each of the four cases and these cases exhaust all possibilities we can conclude that xy x y whenever x and y are real numbers leveraging proof by cases the examples we have presented illustrating proof by cases provide some insight into when to use this method of proof in particular when it is not possible to consider all cases of a proof at the same time a proof by cases should be considered when should you use such a proof generally look for a proof by cases when there is no obvious way to begin a proof but when extra information in each case helps move the proof forward example illustrates how the method of proof by cases can be used effectively example formulate a conjecture about the final decimal digit of the square of an integer and prove your result solution the smallest perfect squares are and so on we notice that the digits that occur as the final digit of a square are and with and never appearing as the final digit of a square we conjecture this theorem the final decimal digit of a perfect square is or how can we prove this theorem we first note that we can express an integer n as b where a and b are pos itive integers and b is or here a is the integer obtained by subtracting the final decimal digit of n from n and dividing by next note that b 10a2 so that the final decimal digit of is the same as the final decimal digit of furthermore note that the final decimal digit of is the same as the final decimal digit of b consequently we can reduce our proof to the consideration of six cases case i the final digit of n is or then the final decimal digit of is the final decimal digit of or namely case ii the final digit of n is or then the final decimal digit of n is the final decimal digit of or namely case iii the final digit of n is or then the final decimal digit of n is the final decimal digit of or namely case iv the final digit of n is or then the final decimal digit of n is the final decimal digit of or namely case v the final decimal digit of n is then the final decimal digit of n is the final decimal digit of namely case vi the final decimal digit of n is then the final decimal digit of is the final decimal digit of namely because we have considered all six cases we can conclude that the final decimal digit of where n is an integer is either or sometimes we can eliminate all but a few examples in a proof by cases as example illustrates example show that there are no solutions in integers x and y of solution we can quickly reduce a proof to checking just a few simple cases because when x and when y this leaves the cases when x equals or and y equals or we can finish using an exhaustive proof to dispense with the remaining cases we note that possible values for are and and possible values for are and and the largest sum of possible values for and is consequently it is impossible for to hold when x and y are integers without loss of generality in the proof in example we dismissed case iii where x and y because it is the same as case ii where x and y with the roles of x and y reversed to shorten the proof we could have proved cases ii and iii together by assuming without loss of generality that x and y implicit in this statement is that we can complete the case with x and y using the same argument as we used for the case with x and y but with the obvious changes in general when the phrase without loss of generality is used in a proof often abbreviated as wlog we assert that by proving one case of a theorem no additional argument is required to prove other specified cases that is other cases follow by making straightforward changes to the argument or by filling in some straightforward initial step proofs by cases can often be made much more efficient when the notion of without loss of generality is employed of course incorrect use of this principle can lead to unfortunate errors sometimes assumptions are made in a proof by cases be sure not to omit any cases and check that you have proved all cases correctly that lead to a loss in generality such assumptions can be made that do not take into account that one case may be substantially different from others this can lead to an incomplete and possibly unsalvageable proof in fact many incorrect proofs of famous theorems turned out to rely on arguments that used the idea of without loss of generality to establish cases that could not be quickly proved from simpler cases we now illustrate a proof where without loss of generality is used effectively together with other proof techniques example show that if x and y are integers and both xy and x y are even then both x and y are even solution we will use proof by contraposition the notion of without loss of generality and proof by cases first suppose that x and y are not both even that is assume that x is odd or that y is odd or both without loss of generality we assume that x is odd so that x for some integer k to complete the proof we need to show that xy is odd or x y is odd consider two cases i y even and ii y odd in i y for some integer n so that x y m n is odd in ii y for some integer n so that xy m n is odd this completes the proof by contraposition note that our use of without loss of generality within the proof is justified because the proof when y is odd can be obtained by simply interchanging the roles of x and y in the proof we have given common errors with exhaustive proof and proof by cases a common error of reasoning is to draw incorrect conclusions from examples no matter how many separate examples are considered a theorem is not proved by considering examples unless every possible case is covered the problem of proving a theorem is analogous to showing that a computer program always produces the output desired no matter how many input values are tested unless all input values are tested we cannot conclude that the program always produces the correct output example is it true that every positive integer is the sum of fourth powers of integers solution to determine whether a positive integer n can be written as the sum of fourth powers of integers we might begin by examining whether n is the sum of fourth powers of integers for the smallest positive integers because the fourth powers of integers are if we can select terms from these numbers that add up to n then n is the sum of fourth powers we can show that all positive integers up to can be written as the sum of fourth powers the details are left to the reader however if we decided this was enough checking we would come to the wrong conclusion it is not true that every positive integer is the sum of fourth powers because is not the sum of fourth powers as the reader can verify another common error involves making unwarranted assumptions that lead to incorrect proofs by cases where not all cases are considered this is illustrated in example example what is wrong with this proof theorem if x is a real number then is a positive real number proof let be x is positive let be x is negative and let q be is positive to show that q is true note that when x is positive is positive because it is the product of two positive numbers x and x to show that q note that when x is negative is positive because it is the product of two negative numbers x and x this completes the proof solution the problem with this proof is that we missed the case of x when x is not positive so the supposed theorem is false if p is x is a real number then we can prove results where p is the hypothesis with three cases and where is x is positive is x is negative and is x because of the equivalence p existence proofs many theorems are assertions that objects of a particular type exist a theorem of this type is a proposition of the form xp x where p is a predicate a proof of a proposition of the form xp x is called an existence proof there are several ways to prove a theorem of this type sometimes an existence proof of xp x can be given by finding an element a called a witness such that p a is true this type of existence proof is called constructive it is also possible to give an existence proof that is nonconstructive that is we do not find an element a such that p a is true but rather prove that xp x is true in some other way one common method of giving a nonconstructive existence proof is to use proof by contradiction and show that the negation of the existential quantification implies a contradiction the concept of a constructive existence proof is illustrated by example and the concept of a nonconstructive existence proof is illustrated by example example a constructive existence proof show that there is a positive integer that can be written as the sum of cubes of positive integers in two different ways solution after considerable computation such as a computer search we find that because we have displayed a positive integer that can be written as the sum of cubes in two different ways we are done there is an interesting story pertaining to this example the english mathematician g h hardy when visiting the ailing indian prodigy ramanujan in the hospital remarked that the number of the cab he took was rather dull ramanujan replied no it is a very interesting number it is the smallest number expressible as the sum of cubes in two different ways example a nonconstructive existence proof show that there exist irrational numbers x and y such that xy is rational solution by example in section we know that is irrational consider the number if it is rational we have two irrational numbers x and y with xy rational namely x and on the other hand if is irrational then we can let and so that xy this proof is an example of a nonconstructive existence proof because we have not found irrational numbers x and y such that xy is rational rather we have shown that either the pair x y or the pair x y have the desired property but we do not know which of these two pairs works godfrey harold hardy hardy born in cranleigh surrey england was the older of two children of isaac hardy and sophia hall hardy his father was the geography and drawing master at the cranleigh school and also gave singing lessons and played soccer his mother gave piano lessons and helped run a boardinghouse for young students hardy parents were devoted to their children education hardy demonstrated his numerical ability at the early age of two when he began writing down numbers into the millions he had a private mathematics tutor rather than attending regular classes at the cranleigh school he moved to winchester college a private high school when he was and was awarded a scholarship he excelled in his studies and demonstrated a strong interest in mathematics he entered trinity college cambridge in on a scholarship and won several prizes during his time there graduating in hardy held the position of lecturer in mathematics at trinity college at cambridge university from to when he was appointed to the sullivan chair of geometry at oxford he had become unhappy with cambridge over the dismissal of the famous philosopher and mathematician bertrand russell from trinity for antiwar activities and did not like a heavy load of administrative duties in he returned to cambridge as the sadleirian professor of pure mathematics where he remained until his retirement in he was a pure mathematician and held an elitist view of mathematics hoping that his research could never be applied ironically he is perhaps best known as one of the developers of the hardy weinberg law which predicts patterns of inheritance his work in this area appeared as a letter to the journal science in which he used simple algebraic ideas to demonstrate errors in an article on genetics hardy worked primarily in number theory and function theory exploring such topics as the riemann zeta function fourier series and the distribution of primes he made many important contributions to many important problems such as waring problem about representing positive integers as sums of kth powers and the problem of representing odd integers as sums of three primes hardy is also remembered for his collaborations with john e littlewood a colleague at cambridge with whom he wrote more than papers and the famous indian mathematical prodigy srinivasa ramanujan his collaboration with littlewood led to the joke that there were only three important english mathematicians at that time hardy littlewood and hardy littlewood although some people thought that hardy had invented a fictitious person littlewood because littlewood was seldom seen outside cambridge hardy had the wisdom of recognizing ramanujan genius from unconventional but extremely creative writings ramanujan sent him while other mathematicians failed to see the genius hardy brought ramanujan to cambridge and collaborated on important joint papers establishing new results on the number of partitions of an integer hardy was interested in mathematics education and his book a course of pure mathematics had a profound effect on undergraduate instruction in mathematics in the first half of the twentieth century hardy also wrote a mathematician apology in which he gives his answer to the question of whether it is worthwhile to devote one life to the study of mathematics it presents hardy view of what mathematics is and what a mathematician does hardy had a strong interest in sports he was an avid cricket fan and followed scores closely one peculiar trait he had was that he did not like his picture taken only five snapshots are known and disliked mirrors covering them with towels immediately upon entering a hotel room nonconstructive existence proofs often are quite subtle as example illustrates example chomp is a game played by two players in this game cookies are laid out on a rectangular grid the cookie in the top left position is poisoned as shown in figure a the two players take turns making moves at each move a player is required to eat a remaining cookie together with all cookies to the right and or below it see figure b for example the loser is the player who has no choice but to eat the poisoned cookie we ask whether one of the two players has a winning strategy that is can one of the players always make moves that are guaranteed to lead to a win solution we will give a nonconstructive existence proof of a winning strategy for the first player that is we will show that the first player always has a winning strategy without explicitly describing the moves this player must follow first note that the game ends and cannot finish in a draw because with each move at least one cookie is eaten so after no more than m n moves the game ends where the initial grid is m n now suppose that the first player begins the game by eating just the cookie in the bottom right corner there are two possibilities this is the first move of a winning strategy for the first player or the second player can make a move that is the first move of a winning strategy for the second player in this second case instead of eating just the cookie in the bottom right corner the first player could have made the same move that the second player made as the first srinivasa ramanujan the famous mathematical prodigy ramanujan was born and raised in southern india near the city of madras now called chennai his father was a clerk in a cloth shop his mother contributed to the family income by singing at a local temple ramanujan studied at the local english language school displaying his talent and interest for mathematics at the age of he mastered a textbook used by college students when he was a university student lent him a copy of synopsis of pure mathematics ramanujan decided to work out the over results in this book stated without proof or explanation writing on sheets later collected to form notebooks he graduated from high school in winning a scholarship to the university of madras enrolling in a fine arts curriculum he neglected his subjects other than mathematics and lost his scholarship he failed to pass examinations at the university four times from to doing well only in mathematics during this time he filled his notebooks with original writings sometimes rediscovering already published work and at other times making new discoveries without a university degree it was difficult for ramanujan to find a decent job to survive he had to depend on the goodwill of his friends he tutored students in mathematics but his unconventional ways of thinking and failure to stick to the syllabus caused problems he was married in in an arranged marriage to a young woman nine years his junior needing to support himself and his wife he moved to madras and sought a job he showed his notebooks of mathematical writings to his potential employers but the books bewildered them however a professor at the presidency college recognized his genius and supported him and in he found work as an accounts clerk earning a small salary ramanujan continued his mathematical work during this time and published his first paper in in an indian journal he realized that his work was beyond that of indian mathematicians and decided to write to leading english mathematicians the first mathematicians he wrote to turned down his request for help but in january he wrote to g h hardy who was inclined to turn ramanujan down but the mathematical statements in the letter although stated without proof puzzled hardy he decided to examine them closely with the help of his colleague and collaborator j e littlewood they decided after careful study that ramanujan was probably a genius because his statements could only be written down by a mathematician of the highest class they must be true because if they were not true no one would have the imagination to invent them hardy arranged a scholarship for ramanujan bringing him to england in hardy personally tutored him in mathematical analysis and they collaborated for five years proving significant theorems about the number of partitions of integers during this time ramanujan made important contributions to number theory and also worked on continued fractions infinite series and elliptic functions ramanujan had amazing insight involving certain types of functions and series but his purported theorems on prime numbers were often wrong illustrating his vague idea of what constitutes a correct proof he was one of the youngest members ever appointed a fellow of the royal society unfortunately in ramanujan became extremely ill at the time it was thought that he had trouble with the english climate and had contracted tuberculosis it is now thought that he suffered from a vitamin deficiency brought on by ramanujan strict vegetarianism and shortages in wartime england he returned to india in continuing to do mathematics even when confined to his bed he was religious and thought his mathematical talent came from his family deity namagiri he considered mathematics and religion to be linked he said that an equation for me has no meaning unless it expresses a thought of god his short life came to an end in april when he was years old ramanujan left several notebooks of unpublished results the writings in these notebooks illustrate ramanujan insights but are quite sketchy several mathematicians have devoted many years of study to explaining and justifying the results in these notebooks a b figure a chomp top left cookie poisoned b three possible moves move of a winning strategy and then continued to follow that winning strategy this would guarantee a win for the first player note that we showed that a winning strategy exists but we did not specify an actual winning strategy consequently the proof is a nonconstructive existence proof in fact no one has been able to describe a winning strategy for that chomp that applies for all rectangular grids by describing the moves that the first player should follow however winning strategies can be described for certain special cases such as when the grid is square and when the grid only has two rows of cookies see exercises and in section uniqueness proofs some theorems assert the existence of a unique element with a particular property in other words these theorems assert that there is exactly one element with this property to prove a statement of this type we need to show that an element with this property exists and that no other element has this property the two parts of a uniqueness proof are existence we show that an element x with the desired property exists uniqueness we show that if y x then y does not have the desired property equivalently we can show that if x and y both have the desired property then x y remark showing that there is a unique element x such that p x is the same as proving the statement x p x y y x p y we illustrate the elements of a uniqueness proof in example example show that if a and b are real numbers and a then there is a unique real number r such that ar b solution first note that the real number r b a is a solution of ar b because a b a b b b consequently a real number r exists for which ar b this is the existence part of the proof second suppose that is a real number such that as b then ar b as b where r b a subtracting b from both sides we find that ar as dividing both sides of this last equation by a which is nonzero we see that r this means that if r then as b this establishes the uniqueness part of the proof proof strategies finding proofs can be a challenging business when you are confronted with a statement to prove you should first replace terms by their definitions and then carefully analyze what the hypotheses and the conclusion mean after doing so you can attempt to prove the result using one of the available methods of proof generally if the statement is a conditional statement you should first try a direct proof if this fails you can try an indirect proof if neither of these approaches works you might try a proof by contradiction forward and backward reasoning whichever method you choose you need a starting point for your proof to begin a direct proof of a conditional statement you start with the premises using these premises together with axioms and known theorems you can construct a proof using a sequence of steps that leads to the conclusion this type of reasoning called forward reasoning is the most common type of reasoning used to prove relatively simple results similarly with indirect reasoning you can start with the negation of the conclusion and using a sequence of steps obtain the negation of the premises unfortunately forward reasoning is often difficult to use to prove more complicated results because the reasoning needed to reach the desired conclusion may be far from obvious in such cases it may be helpful to use backward reasoning to reason backward to prove a statement q we find a statement p that we can prove with the property that p q note that it is not helpful to find a statement r that you can prove such that q r because it is the fallacy of begging the question to conclude from q r and r that q is true backward reasoning is illustrated in examples and example given two posit ive real numbers x and y their arithmetic mean is x y and their geo positive real numbers we find that the arithmetic mean is always greate r than the geometric mean for example when x and y we have we prove that this inequality is always true can solution to prove that x y xy when x and y are distinct positive real numbers we can work backward we construct a sequence of equivalent inequalities the equivalent inequalities are x y xy x y xy x y x y because x y when x y it follows that the fi nal inequality is true because all these inequalities are equivalent it follows that x y xy when x y once we have carried out this backward reasoning we can easily reverse the steps to construct a proof using forward reasoning we now give this proof suppose that x and y are distinct positive real numbers then x y because the square of a nonzero real number is positive see appendix because x y this implies that adding to both sides we obtain because x y this means that x y dividing both sides of this equation by we see that x y xy finally taking square roots of both sides which preserves the inequality because both sides are positive yields x y xy we conclude that if x and y are distinct pos itive real numbers then their arithmetic mean x y is greater than their geometric mean xy example suppose that two people play a game taking turns removing one two or three stones at a time from a pile that begins with stones the person who removes the last stone wins the game show that the first player can win the game no matter what the second player does solution to prove that the first player can always win the game we work backward at the last step the first player can win if this player is left with a pile containing one two or three stones the second player will be forced to leave one two or three stones if this player has to remove stones from a pile containing four stones consequently one way for the first person to win is to leave four stones for the second player on the next to last move the first person can leave four stones when there are five six or seven stones left at the beginning of this player move which happens when the second player has to remove stones from a pile with eight stones consequently to force the second player to leave five six or seven stones the first player should leave eight stones for the second player at the second to last move for the first player this means that there are nine ten or eleven stones when the first player makes this move similarly the first player should leave twelve stones when this player makes the first move we can reverse this argument to show that the first player can always make moves so that this player wins the game no matter what the second player does these moves successively leave twelve eight and four stones for the second player adapting existing proofs an excellent way to look for possible approaches that can be used to prove a statement is to take advantage of existing proofs of similar results often an existing proof can be adapted to prove other facts even when this is not the case some of the ideas used in existing proofs may be helpful because existing proofs provide clues for new proofs you should read and understand the proofs you encounter in your studies this process is illustrated in example example in example of section we proved that is irrational we now conj ecture that is irrational can we adapt the proof in example in section to show that is irrational solution to adapt th e proof in examp le in section we beg in by mimicking the steps in that proof but with replaced with first we suppose that d c where the fraction c d is in lowest terms squaring both sides tells us that so that can we use this equation to show that must be a factor of both c and d similar to how we used the equation in example in section to show that must be a factor of both a and b recall that an integer is a factor of the integer t if t is an integer an integer n is even if and only if is a factor of n in turns out that we can but we need some ammunition from number theory which we will develop in chapter we sketch out the remainder of the proof but leave the justification of these steps until chapter because is a factor of it must also be a factor of c furthermore because is a factor of c is a factor of which means that is a factor of this implies that is a factor of which means that is a factor of that d this makes a factor of both c and d which contradicts the assumption that c d is in l owest terms after we have filled in the jus tification for these steps we will have shown that is irrational b y adapting the proof that is irrational note that this proof can be extended to show that n is irrational whenever n is a positive integer that is not a perfect square we leave the details of this to chapter a good tip is to look for existing proofs that you might adapt when you are confronted with proving a new theorem particularly when the new theorem seems similar to one you have already proved looking for counterexamples in section we introduced the use of counterexamples to show that certain statements are false when confronted with a conjecture you might first try to prove this conjecture and if your attempts are unsuccessful you might try to find a counterexample first by looking at the simplest smallest examples if you cannot find a counterexample you might again try to prove the statement in any case looking for counterexamples is an extremely important pursuit which often provides insights into problems we will illustrate the role of counterexamples in example example in example in section we showed that the statement every positive integer is the sum of two squares of integers is false by finding a counterexample that is there are positive integers that cannot be written as the sum of the squares of two integers although we cannot write every positive integer as the sum of the squares of two integers maybe we can write every positive integer as the sum of the squares of three integers that is is the statement every positive integer is the sum of the squares of three integers true or false solution because we know that not every positive integer can be written as the sum of two squares of integers we might initially be skeptical that every positive integer can be written as the sum of three squares of integers so we first look for a counterexample that is we can show that the statement every positive integer is the sum of three squares of integers is false if we can find a particular integer that is not the sum of the squares of three integers to look for a counterexample we try to write successive positive integers as a sum of three squares we find that 02 02 but we cannot find a way to write as the sum of three squares to show that there are not three squares that add up to we note that the only possible squares we can use are those not exceeding namely and because no three terms where each term is or add up to it follows that is a counterexample we conclude that the statement every positive integer is the sum of the squares of three integers is false we have shown that not every positive integer is the sum of the squares of three integers the next question to ask is whether every positive integer is the sum of the squares of four positive integers some experimentation provides evidence that the answer is yes for example and it turns out the conjecture every positive integer is the sum of the squares of four integers is true for a proof see proof strategy in action mathematics is generally taught as if mathematical facts were carved in stone mathematics texts including the bulk of this book formally present theorems and their proofs such presen tations do not convey the discovery process in mathematics this process begins with exploring concepts and examples asking questions formulating conjectures and attempting to settle these conjectures either by proof or by counterexample these are the day to day activities of math ematicians believe it or not the material taught in textbooks was originally developed in this way people formulate conjectures on the basis of many types of possible evidence the exam ination of special cases can lead to a conjecture as can the identification of possible patterns altering the hypotheses and conclusions of known theorems also can lead to plausible conjec tures at other times conjectures are made based on intuition or a belief that a result holds no matter how a conjecture was made once it has been formulated the goal is to prove or disprove it when mathematicians believe that a conjecture may be true they try to find a proof if they cannot find a proof they may look for a counterexample when they cannot find a coun terexample they may switch gears and once again try to prove the conjecture although many conjectures are quickly settled a few conjectures resist attack for hundreds of years and lead to figure the standard checkerboard figure two dominoes the development of new parts of mathematics we will mention a few famous conjectures later in this section tilings we can illustrate aspects of proof strategy through a brief study of tilings of checkerboards looking at tilings of checkerboards is a fruitful way to quickly discover many different results and construct their proofs using a variety of proof methods there are almost an endless number of conjectures that can be made and studied in this area too to begin we need to define some terms a checkerboard is a rectangle divided into squares of the same size by horizontal and vertical lines the game of checkers is played on a board with rows and columns this board is called the standard checkerboard and is shown in figure in this section we use the term board to refer to a checkerboard of any rectangular size as well as parts of checkerboards obtained by removing one or more squares a domino is a rectangular piece that is one square by two squares as shown in figure we say that a board is tiled by dominoes when all its squares are covered with no overlapping dominoes and no dominoes overhanging the board we now develop some results about tiling boards using dominoes example can we tile the standard checkerboard using dominoes solution we can find many ways to tile the standard checkerboard using dominoes for example we can tile it by placing dominoes horizontally as shown in figure the existence of one such tiling completes a constructive existence proof of course there are a large number of other ways to do this tiling we can place dominoes vertically on the board or we can place some tiles vertically and some horizontally but for a constructive existence proof we needed to find just one such tiling example can we tile a board obtained by removing one of the four corner squares of a standard checker board solution to answer this question note that a standard checkerboard has squares so removing a square produces a board with squares now suppose that we could tile a board obtained from the standard checkerboard by removing a corner square the board has an even number of figure tiling the standard checkerboard figure the standard checkerboard with the upper left and lower right squares removed squares because each domino covers two squares and no two dominoes overlap and no dominoes overhang the board consequently we can prove by contradiction that a standard checkerboard with one square removed cannot be tiled using dominoes because such a board has an odd number of squares we now consider a trickier situation example can we tile the board obtained by deleting the upper left and lower right corner squares of a standard checkerboard shown in figure solution a board obtained by deleting two squares of a standard checkerboard contains squares because is even we cannot quickly rule out the existence of a tiling of the standard checkerboard with its upper left and lower right squares removed unlike example where we ruled out the existence of a tiling of the standard checkerboard with one corner square removed trying to construct a tiling of this board by successively placing dominoes might be a first approach as the reader should attempt however no matter how much we try we cannot find such a tiling because our efforts do not produce a tiling we are led to conjecture that no tiling exists we might try to prove that no tiling exists by showing that we reach a dead end however we successively place dominoes on the board to construct such a proof we would have to consider all possible cases that arise as we run through all possible choices of successively placing dominoes for example we have two choices for covering the square in the second column of the first row next to the removed top left corner we could cover it with a horizontally placed tile or a vertically placed tile each of these two choices leads to further choices and so on it does not take long to see that this is not a fruitful plan of attack for a person although a computer could be used to complete such a proof by exhaustion exercise asks you to supply such a proof to show that a checkerboard with opposite corners removed cannot be tiled we need another approach perhaps there is an easier way to prove there is no tiling of a standard checkerboard with two opposite corners removed as with many proofs a key obser vation can help we color the squares of this checkerboard using alternating white and black squares as in figure observe that a domino in a tiling of such a board covers one white square and one black square next note that this board has unequal numbers of white square and black figure a right triomino and a straight triomino squares we can use these observations to prove by contradiction that a standard checkerboard with opposite corners removed cannot be tiled using dominoes we now present such a proof proof suppose we can use dominoes to tile a standard checkerboard with opposite corners removed note that the standard checkerboard with opposite corners removed contains squares the tiling would use dominoes note that each domino in this tiling covers one white and one black square consequently the tiling covers white squares and black squares however when we remove two opposite corner squares either of the remaining squares are white and are black or else are white and are black this contradicts the assumption that we can use dominoes to cover a standard checkerboard with opposite corners removed completing the proof we can use other types of pieces besides dominoes in tilings instead of dominoes we can study tilings that use identically shaped pieces constructed from congruent squares that are connected along their edges such pieces are called polyominoes a term coined in by the mathematician solomon golomb the author of an entertaining book about them we will consider two polyominoes with the same number of squares the same if we can rotate and or flip one of the polyominoes to get the other one for example there are two types of triominoes see figure which are polyominoes made up of three squares connected by their sides one type of triomino the straight triomino has three horizontally connected squares the other type right triominoes resembles the letter l in shape flipped and or rotated if necessary we will study the tilings of a checkerboard by straight triominoes here we will study tilings by right triominoes in section example can you use straight triominoes to tile a standard checkerboard solution the standard checkerboard contains squares and each triomino covers three squares consequently if triominoes tile a board the number of squares of the board must be a multiple of because is not a multiple of triominoes cannot be used to cover an checkerboard in example we consider the problem of using straight triominoes to tile a standard checkerboard with one corner missing example can we use straight triominoes to tile a standard checkerboard with one of its four corners removed an checkerboard with one corner removed contains squares any tiling by straight triominoes of one of these four boards uses triominoes however when we experiment we cannot find a tiling of one of these boards using straight triominoes a proof by exhaustion does not appear promising can we adapt our proof from example to prove that no such tiling exists solution we will color the squares of the checkerboard in an attempt to adapt the proof by contradiction we gave in example of the impossibility of using dominoes to tile a standard checkerboard with opposite corners removed because we are using straight triominoes rather than dominoes we color the squares using three colors rather than two colors as shown in figure note that there are blue squares black squares and white squares in this coloring next we make the crucial observation that when a straight triomino covers three squares of the checkerboard it covers one blue square one black square and one white square next note that each of the three colors appears in a corner square thus without loss of generality we may assume that we have rotated the coloring so that the missing square is colored blue therefore we assume that the remaining board contains blue squares black squares and white squares if we could tile this board using straight triominoes then we would use straight triominoes these triominoes would cover blue squares black squares and white figure coloring the squares of the standard checkerboard with three colors squares this contradicts the fact that this board contains blue squares black squares and white squares therefore we cannot tile this board using straight triominoes the role of open problems many advances in mathematics have been made by people trying to solve famous unsolved problems in the past years many unsolved problems have finally been resolved such as the proof of a conjecture in number theory made more than years ago this conjecture asserts the truth of the statement known as fermat last theorem theorem remark the equation z2 has infinitely many solutions in integers x y and z these solutions are called pythagorean triples and correspond to the lengths of the sides of right triangles with integer lengths see exercise this problem has a fascinating history in the seventeenth century fermat jotted in the margin of his copy of the works of diophantus that he had a wondrous proof that there are no integer solutions of xn yn zn when n is an integer greater than with xyz however he never published a proof fermat published almost nothing and no proof could be found in the papers he left when he died mathematicians looked for a proof for three centuries without success although many people were convinced that a relatively simple proof could be found proofs of special cases were found such as the proof of the case when n by euler and the proof of the n case by fermat himself over the years several established mathematicians thought that they had proved this theorem in the nineteenth century one of these failed attempts led to the development of the part of number theory called algebraic number theory a correct proof requiring hundreds of pages of advanced mathematics was not found until the when andrew wiles used recently developed ideas from a sophisticated area of number theory called the theory of elliptic curves to prove fermat last theorem wiles quest to find a proof of fermat last theorem using this powerful theory described in a program in the nova series on public television took close to ten years moreover his proof was based on major contributions of many mathematicians the interested reader should consult for more information about fermat last theorem and for additional references concerning this problem and its resolution we now state an open problem that is simple to describe but that seems quite difficult to resolve example the conjecture let t be the transformation that sends an even integer x to x and an odd integer x to a famous conjecture sometimes known as the conjec ture states that for all positive integers x when we repeatedly apply the transformation t we will eventually reach the integer for example starting with x we find t t t t t t t t and t the conjecture has been verified using computers for all integers x up to watch out working on the problem can be addictive the conjecture has an interesting history and has attracted the attention of mathe maticians since the the conjecture has been raised many times and goes by many other names including the collatz problem hasse algorithm ulam problem the syracuse prob lem and kakutani problem many mathematicians have been diverted from their work to spend time attacking this conjecture this led to the joke that this problem was part of a conspiracy to slow down american mathematical research see the article by jeffrey lagarias for a fascinating discussion of this problem and the results that have been found by mathematicians attacking it in chapter we will describe additional open questions about prime numbers students already familiar with the basic notions about primes might want to explore section where these open questions are discussed we will mention other important open questions throughout the book build up your arsenal of proof methods as you work through this book additional proof methods in this chapter we introduced the basic methods used in proofs we also described how to leverage these methods to prove a variety of results we will use these proof methods in all subsequent chapters in particular we will use them in chapters and to prove results about sets functions algorithms and number theory and in chapters and to prove results in graph theory among the theorems we will prove is the famous halting theorem which states that there is a problem that cannot be solved using any procedure however there are many important proof methods besides those we have covered we will introduce some of these methods later in this book in particular in section we will discuss mathematical induction which is an extremely useful method for proving statements of the form np n where the domain consists of all positive integers in section we will introduce structural induction which can be used to prove results about recursively defined sets we will use the cantor diagonalization method which can be used to prove results about the size of infinite sets in section in chapter we will introduce the notion of combinatorial proofs which can be used to prove results by counting arguments the reader should note that entire books have been devoted to the activities discussed in this section including many excellent works by george pólya finally note that we have not given a procedure that can be used for proving theorems in mathematics it is a deep theorem of mathematical logic that there is no such procedure exercises prove that when n is a positive integer with n prove that there are no positive perfect cubes less than that are the sum of the cubes of two positive integers prove that if x and y are real numbers then max x y min x y x y hint use a proof by cases with the two cases corresponding to x y and x y respec tively use a proof by cases to show that min a min b c min min a b c whenever a b and c are real numbers prove using the notion of without loss of generality that min x y x y x y and max x y x y x y whenever x and y are real numbers prove using the notion of without loss of generality that is an odd integer when x and y are integers of opposite parity prove the triangle inequality which states that if x and y are real numbers then x y x y where x represents the absolute value of x which equals x if x and equals x if x prove that there is a positive integer that equals the sum of the positive integers not exceeding it is your proof constructive or nonconstructive prove that there are consecutive positive integers that are not perfect squares is your proof constructive or non constructive prove that either or is not a perfect square is your proof constructive or nonconstruc tive prove that there exists a pair of consecutive integers such that one of these integers is a perfect square and the other is a perfect cube show that the product of two of the numbers 791212 and is nonnegative is your proof constructive or nonconstructive hint do not try to evaluate these numbers prove or disprove that there is a rational number x and an irrational number y such that xy is irrational prove or disprove that if a and b are rational numbers then ab is also rational show that each of these statements can be used to ex press the fact that there is a unique element x such that p x is true note that we can also write this statement as xp x a x y p y x y b xp x x y p x p y x y c x p x y p y x y show that if a b and c are real numbers and a then there is a unique solution of the equation ax b c suppose that a and b are odd integers with a b show there is a unique integer c such that a c b c show that if r is an irrational number there is a unique integer n such that the distance between r and n is less than show that if n is an odd integer then there is a unique integer k such that n is the sum of k and k prove that given a real number x there exist unique num bers n and e such that x n e n is an integer and e prove that given a real number x there exist unique num bers n and e such that x n e n is an integer and e use forward reasoning to show that if x is a nonzero real number then hint start with the in equality x x which holds for all nonzero real numbers x the harmonic mean of two real numbers x and y equals x y by computing the harmonic and geometric means of different pairs of positive real numbers formu late a conjecture about their relative sizes and prove your conjecture the quadratic mean of two real numbers x and y equals by computing the arithmetic and quadratic means of different pairs of positive real num bers formulate a conjecture about their relative sizes and prove your conjecture write the numbers on a blackboard where n is an odd integer pick any two of the numbers j and k write j k on the board and erase j and k continue this process until only one integer is written on the board prove that this integer must be odd suppose that five ones and four zeros are arranged around a circle between any two equal bits you insert a and between any two unequal bits you insert a to produce nine new bits then you erase the nine original bits show that when you iterate this procedure you can never get nine zeros hint work backward assuming that you did end up with nine zeros formulate a conjecture about the decimal digits that ap pear as the final decimal digit of the fourth power of an integer prove your conjecture using a proof by cases formulate a conjecture about the final two decimal digits of the square of an integer prove your conjecture using a proof by cases prove that there is no positive integer n such that prove that there are no solutions in integers x and y to the equation prove that there are no solutions in positive integers x and y to the equation prove that there are infinitely many solutions in posi tive integers x y and z to the equation y2 z2 hint let x y and z where m and n are integers key terms and results adapt the proof in example in section to prove that if n abc wh ere a b an d c are positive integers then prove that is irrational prove that between every two rational numbers there is an irrational number prove that between every rational number and every irra tional number there is an irrational number let s xnyn where xn and y2 yn are orderings of two different se quences of positive real numbers each containing n ele ments a show that s takes its maximum value over all order ings of the two sequences when both sequences are sorted so that the elements in each sequence are in nondecreasing order b show that s takes its minimum value over all order ings of the two sequences when one sequence is sorted into nondecreasing order and the other is sorted into nonincreasing order prove or disprove that if you have an gallon jug of wa ter and two empty jugs with capacities of gallons and gallons respectively then you can measure gallons by successively pouring some of or all of the water in a jug into another jug verify the conjecture for these integers a b c d verify the conjecture for these integers a b c d prove or disprove that you can use dominoes to tile the standard checkerboard with two adjacent corners re moved that is corners that are not opposite prove or disprove that you can use dominoes to tile a standard checkerboard with all four corners removed prove that you can use dominoes to tile a rectangular checkerboard with an even number of squares prove or disprove that you can use dominoes to tile a checkerboard with three corners removed use a proof by exhaustion to show that a tiling using dominoes of a checkerboard with opposite corners removed does not exist hint first show that you can assume that the squares in the upper left and lower right corners are removed number the squares of the original checkerboard from to starting in the first row mov ing right in this row then starting in the leftmost square in the second row and moving right and so on remove squares and to begin the proof note that square is covered either by a domino laid horizontally which cov ers squares and or vertically which covers squares and consider each of these cases separately and work through all the subcases that arise prove that when a white square and a black square are removed from an checkerboard colored as in the text you can tile the remaining squares of the checker board using dominoes hint show that when one black and one white square are removed each part of the parti tion of the remaining cells formed by inserting the barriers shown in the figure can be covered by dominoes show that by removing two white squares and two black squares from an checkerboard colored as in the text you can make it impossible to tile the remaining squares using dominoes find all squares if they exist on an checkerboard such that the board obtained by removing one of these square can be tiled using straight triominoes hint first use arguments based on coloring and rotations to elimi nate as many squares as possible from consideration a draw each of the five different tetrominoes where a tetromino is a polyomino consisting of four squares b for each of the five different tetrominoes prove or dis prove that you can tile a standard checkerboard using these tetrominoes prove or disprove that you can tile a checker board using straight tetrominoes key terms and results terms proposition a statement that is true or false propositional variable a variable that represents a proposi tion truth value true or false p negation of p the proposition with truth value opposite to the truth value of p logical operators operators used to combine propositions compound proposition a proposition constructed by combin ing propositions using logical operators truth table a table displaying all possible truth values of propositions p q disjunction of p and q the proposition p or q which is true if and only if at least one of p and q is true p q conjunction of p and q the proposition p and q which is true if and only if both p and q are true p q exclusive or of p and q the proposition p xor q which is true when exactly one of p and q is true p q p implies q the proposition if p then q which is false if and only if p is true and q is false converse of p q the conditional statement q p contrapositive of p q the conditional statement q p inverse of p q the conditional statement p q p q biconditional the proposition p if and only if q which is true if and only if p and q have the same truth value bit boolean variable a variable that has a value of or bit operation an operation on a bit or bits bit string a list of bits bitwise operations operations on bit strings that operate on each bit in one string and the corresponding bit in the other string logic gate a logic element that performs a logical operation on one or more bits to produce an output bit logic circuit a switching circuit made up of logic gates that produces one or more output bits tautology a compound proposition that is always true contradiction a compound proposition that is always false contingency a compound proposition that is sometimes true and sometimes false consistent compound propositions compound propositions for which there is an assignment of truth values to the vari ables that makes all these propositions true satisfiable compound proposition a compound proposition for which there is an assignment of truth values to its vari ables that makes it true logically equivalent compound propositions compound propositions that always have the same truth values predicate part of a sentence that attributes a property to the subject propositional function a statement containing one or more variables that becomes a proposition when each of its vari ables is assigned a value or is bound by a quantifier domain or universe of discourse the values a variable in a propositional function may take x p x existential quantification of p x the proposition that is true if and only if there exists an x in the domain such that p x is true xp x universal quantification of p x the proposition that is true if and only if p x is true for every x in the domain logically equivalent expressions expressions that have the same truth value no matter which propositional functions and domains are used free variable a variable not bound in a propositional function bound variable a variable that is quantified scope of a quantifier portion of a statement where the quan tifier binds its variable argument a sequence of statements argument form a sequence of compound propositions involv ing propositional variables premise a statement in an argument or argument form other than the final one conclusion the final statement in an argument or argument form valid argument form a sequence of compound propositions involving propositional variables where the truth of all the premises implies the truth of the conclusion valid argument an argument with a valid argument form rule of inference a valid argument form that can be used in the demonstration that arguments are valid fallacy an invalid argument form often used incorrectly as a rule of inference or sometimes more generally an incor rect argument circular reasoning or begging the question reasoning where one or more steps are based on the truth of the statement being proved theorem a mathematical assertion that can be shown to be true conjecture a mathematical assertion proposed to be true but that has not been proved proof a demonstration that a theorem is true axiom a statement that is assumed to be true and that can be used as a basis for proving theorems lemma a theorem used to prove other theorems corollary a proposition that can be proved as a consequence of a theorem that has just been proved vacuous proof a proof that p q is true based on the fact that p is false trivial proof a proof that p q is true based on the fact that q is true direct proof a proof that p q is true that proceeds by show ing that q must be true when p is true proof by contraposition a proof that p q is true that pro ceeds by showing that p must be false when q is false proof by contradiction a proof that p is true based on the truth of the conditional statement p q where q is a contradiction exhaustive proof a proof that establishes a result by checking a list of all possible cases proof by cases a proof broken into separate cases where these cases cover all possibilities without loss of generality an assumption in a proof that makes it possible to prove a theorem by reducing the number of cases to consider in the proof counterexample an element x such that p x is false constructive existence proof a proof that an element with a specified property exists that explicitly finds such an ele ment nonconstructive existence proof a proof that an element with a specified property exists that does not explicitly find such an element rational number a number that can be expressed as the ratio of two integers p and q such that q uniqueness proof a proof that there is exactly one element satisfying a specified property results the logical equivalences given in tables and in sec tion de morgan laws for quantifiers rules of inference for propositional calculus rules of inference for quantified statements review questions a define the negation of a proposition b what is the negation of this is a boring course a define using truth tables the disjunction conjunc tion exclusive or conditional and biconditional of the propositions p and q b what are the disjunction conjunction exclusive or conditional and biconditional of the propositions i ll go to the movies tonight and i ll finish my discrete mathematics homework a describe at least five different ways to write the con ditional statement p q in english b define the converse and contrapositive of a conditional statement c state the converse and the contrapositive of the con ditional statement if it is sunny tomorrow then i will go for a walk in the woods a what does it mean for two propositions to be logically equivalent b describe the different ways to show that two com pound propositions are logically equivalent c show in at least two different ways that the compound propositions p r q and p q r are equivalent depends on the exercise set in section a given a truth table explain how to use disjunctive nor mal form to construct a compound proposition with this truth table b explain why part a shows that the operators and are functionally complete c is there an operator such that the set containing just this operator is functionally complete what are the universal and existential quantifications of a predicate p x what are their negations a what is the difference between the quantification x yp x y and y xp x y where p x y is a predicate b give an example of a predicate p x y such that x yp x y and y xp x y have different truth values describe what is meant by a valid argument in proposi tional logic and show that the argument if the earth is flat then you can sail off the edge of the earth you can not sail off the edge of the earth therefore the earth is not flat is a valid argument use rules of inference to show that if the premises all zebras have stripes and mark is a zebra are true then the conclusion mark has stripes is true a describe what is meant by a direct proof a proof by contraposition and a proof by contradiction of a con ditional statement p q b give a direct proof a proof by contraposition and a proof by contradiction of the statement if n is even then n is even a describe a way to prove the biconditional p q b prove the statement the integer is odd if and only if the integer is even where n is an inte ger to prove that the statements and are equiva lent is it sufficient to show that the conditional statements and are valid if not pro vide another collection of conditional statements that can be used to show that the four statements are equivalent a suppose that a statement of the form xp x is false how can this be proved b show that the statement for every positive integer n is false what is the difference between a constructive and non constructive existence proof give an example of each what are the elements of a proof that there is a unique element x such that p x where p x is a propositional function explain how a proof by cases can be used to prove a result about absolute values such as the fact that xy x y for all real numbers x and y supplementary exercises let p be the proposition i will do every exercise in this book and q be the proposition i will get an a in this course express each of these as a combination of p and q a i will get an a in this course only if i do every exer cise in this book b i will get an a in this course and i will do every exercise in this book c either i will not get an a in this course or i will not do every exercise in this book d for me to get an a in this course it is necessary and sufficient that i do every exercise in this book find the truth table of the compound proposition p q p r show that these compound propositions are tautologies a q p q p b p q p q give the converse the contrapositive and the inverse of these conditional statements a if it rains today then i will drive to work b if x x then x c if n is greater than then n is greater than given a conditional statement p q find the converse of its inverse the converse of its converse and the con verse of its contrapositive given a conditional statement p q find the inverse of its inverse the inverse of its converse and the inverse of its contrapositive find a compound proposition involving the propositional variables p q r and that is true when exactly three of these propositional variables are true and is false other wise show that these statements are inconsistent if sergei takes the job offer then he will get a signing bonus if sergei takes the job offer then he will receive a higher salary if sergei gets a signing bonus then he will not receive a higher salary sergei takes the job offer show that these statements are inconsistent if miranda does not take a course in discrete mathematics then she will not graduate if miranda does not graduate then she is not qualified for the job if miranda reads this book then she is qualified for the job miranda does not take a course in discrete mathematics but she reads this book teachers in the middle ages supposedly tested the realtime propositional logic ability of a student via a technique known suppose that you meet three people aaron bohan and crystal can you determine what aaron bohan and crys tal are if aaron says all of us are knaves and bohan says exactly one of us is a knave suppose that you meet three people anita boris and carmen what are anita boris and carmen if anita says i am a knave and boris is a knight and boris says ex actly one of the three of us is a knight adapted from suppose that on an island there are three types of people knights knaves and normals also known as spies knights always tell the truth knaves always lie and normals sometimes lie and some times tell the truth detectives questioned three inhabi tants of the island amy brenda and claire as part of the investigation of a crime the detectives knew that one of the three committed the crime but not which one they also knew that the criminal was a knight and that the other two were not additionally the detectives recorded these statements amy i am innocent brenda what amy says is true claire brenda is not a normal af ter analyzing their information the detectives positively identified the guilty party who was it show that if s is a proposition where s is the conditional statement if s is true then unicorns live then uni corns live is true show that it follows that s cannot be a proposition this paradox is known as löb paradox show that the argument with premises the tooth fairy is a real person and the tooth fairy is not a real person and conclusion you can find gold at the end of the rainbow is a valid argument does this show that the conclusion is true suppose that the truth value of the proposition pi is t whenever i is an odd positive integer and is f when ever i is an even positive integer find the truth values of pi pi and pi pi is set and in each round the teacher gives the student succes sive assertions that the student must either accept or reject as they are given when the student accepts an assertion it is added as a commitment when the student rejects an assertion its negation is added as a commitment the student passes the test if the consistency of all commitments is maintained throughout the test suppose that in a three round obligato game the teacher first gives the student the proposition p q then the proposition p r q and finally the proposition q for which of the eight possible sequences of three answers will the student pass the test suppose that in a four round obligato game the teacher first gives the student the proposition p q r then the proposition p q then the proposition r and finally the proposition p r q p for which of the possible sequences of four answers will the student pass the test explain why every obligato game has a winning strategy exercises and are set on the island of knights and knaves described in example in section model sudoku puzzles with blocks as satisfiability problems let p x be the statement student x knows calculus and let q y be the statement class y contains a student who knows calculus express each of these as quantifications of p x and q y a some students know calculus b not every student knows calculus c every class has a student in it who knows calculus d every student in every class knows calculus e there is at least one class with no students who know calculus let p m n be the statement m divides n where the do main for both variables consists of all positive integers by m divides n we mean that n km for some integer k determine the truth values of each of these statements a p b p c m n p m n d m n p m n e n m p m n f np n find a domain for the quantifiers in x y x y z z x z y such that this statement is true find a domain for the quantifiers in x y x y z z x z y such that this statement is false use existential and universal quantifiers to express the statement no one has more than three grandmothers us ing the propositional function g x y which represents x is the grandmother of y use existential and universal quantifiers to express the statement everyone has exactly two biological parents using the propositional function p x y which repre sents x is the biological parent of y the quantifier n denotes there exists exactly n so that nxp x means there exist exactly n values in the do main such that p x is true determine the true value of these statements where the domain consists of all real numbers a b x c d x x express each of these statements using existential and universal quantifiers and propositional logic where n is defined in exercise a x b x c x d x let p x y be a propositional function show that x y p x y y x p x y is a tautology express this statement using quantifiers every student in this class has taken some course in every department in the school of mathematical sciences express this statement using quantifiers there is a build ing on the campus of some college in the united states in which every room is painted white express the statement there is exactly one student in this class who has taken exactly one mathematics class at this school using the uniqueness quantifier then express this statement using quantifiers without using the uniqueness quantifier describe a rule of inference that can be used to prove that there are exactly two elements x and y in a domain such that p x and p y are true express this rule of inference as a statement in english use rules of inference to show that if the premises x p x q x x q x r x and r a where a is in the domain are true then the conclusion p a is true prove that if is irrational then x is irrational prove that if x is irrational and x then x is irra tional prove that given a nonnegative integer n there is a unique nonnegative integer m such that n m let p x and q x be propositional functions show that x p x q x and x p x x q x always have the same truth value if y x p x y is true does it necessarily follow that x y p x y is true if x y p x y is true does it necessarily follow that x y p x y is true find the negations of these statements a if it snows today then i will go skiing tomorrow b every person in this class understands mathematical induction c some students in this class do not like discrete math ematics d in every mathematics class there is some student who falls asleep during lectures computer projects write programs with the specified input and output given the truth values of the propositions p and q find the truth values of the conjunction disjunction exclusive or conditional statement and biconditional of these proposi tions given two bit strings of length n find the bitwise and bitwise or and bitwise xor of these strings give a compound proposition determine whether it is sat isfiable by checking its truth value for all positive assign ments of truth values to its propositional variables prove that there exists an integer m such that is your proof constructive or nonconstructive prove that there is a positive integer that can be written as the sum of squares of positive integers in two differ ent ways use a computer or calculator to speed up your work disprove the statement that every positive integer is the sum of the cubes of eight nonnegative integers disprove the statement that every positive integer is the sum of at most two squares and a cube of nonnegative integers disprove the statement that every positive integer is the sum of fifth powers of nonnegative integers assuming the truth of the theorem that states that n is irrational whenever n is a positive integer that is not a perfect square prove that is irrational given the truth values of the propositions p and q in fuzzy logic find the truth value of the disjunction and the conjunction of p and q see exercises and of section given positive integers m and n interactively play the game of chomp given a portion of a checkerboard look for tilings of this checkerboard with various types of polyominoes including dominoes the two types of triominoes and larger polyomi noes computations and explorations use a computational program or programs you have written to do these exercises look for positive integers that are not the sum of the cubes of nine different positive integers look for positive integers greater than that are not the sum of the fourth powers of positive integers find as many positive integers as you can that can be writ ten as the sum of cubes of positive integers in two different ways sharing this property with try to find winning strategies for the game of chomp for different initial configurations of cookies construct the different pentominoes where a pentomino is a polyomino consisting of five squares find all the rectangles of squares that can be tiled using every one of the different pentominoes writing projects respond to these with essays using outside sources discuss logical paradoxes including the paradox of epi menides the cretan jourdain card paradox and the bar ber paradox and how they are resolved describe how fuzzy logic is being applied to practical ap plications consult one or more of the recent books on fuzzy logic written for general audiences describe some of the practical problems that can be mod eled as satisfiability problems describe some of the techniques that have been devised to help people solve sudoku puzzles without the use of a computer describe the basic rules of wff n proof the game of modern logic developed by layman allen give exam ples of some of the games included in wff n proof read some of the writings of lewis carroll on symbolic logic describe in detail some of the models he used to represent logical arguments and the rules of inference he used in these arguments extend the discussion of prolog given in section ex plaining in more depth how prolog employs resolution discuss some of the techniques used in computational logic including skolem rule automated theorem proving is the task of using com puters to mechanically prove theorems discuss the goals and applications of automated theorem proving and the progress made in developing automated theorem provers describe how dna computing has been used to solve instances of the satisfiability problem look up some of the incorrect proofs of famous open questions and open questions that were solved since and describe the type of error made in each proof discuss what is known about winning strategies in the game of chomp describe various aspects of proof strategy discussed by george pólya in his writings on reasoning including and describe a few problems and results about tilings with polyominoes as described in and for ex ample sets set operations functions sequences and summations cardinality of sets matrices uch of discrete mathematics is devoted to the study of discrete structures used to repre sent discrete objects many important discrete structures are built using sets which are collections of objects among the discrete structures built from sets are combinations unordered collections of objects used extensively in counting relations sets of ordered pairs that represent relationships between objects graphs sets of vertices and edges that connect vertices and finite state machines used to model computing machines these are some of the topics we will study in later chapters the concept of a function is extremely important in discrete mathematics a function assigns to each element of a first set exactly one element of a second set where the two sets are not necessarily distinct functions play important roles throughout discrete mathematics they are used to represent the computational complexity of algorithms to study the size of sets to count objects and in a myriad of other ways useful structures such as sequences and strings are special types of functions in this chapter we will introduce the notion of a sequence which represents ordered lists of elements furthermore we will introduce some important types of sequences and we will show how to define the terms of a sequence using earlier terms we will also address the problem of identifying a sequence from its first few terms in our study of discrete mathematics we will often add consecutive terms of a sequence of numbers because adding terms from a sequence as well as other indexed sets of numbers is such a common occurrence a special notation has been developed for adding such terms in this chapter we will introduce the notation used to express summations we will develop formulae for certain types of summations that appear throughout the study of discrete mathematics for instance we will encounter such summations in the analysis of the number of steps used by an algorithm to sort a list of numbers so that its terms are in increasing order the relative sizes of infinite sets can be studied by introducing the notion of the size or cardinality of a set we say that a set is countable when it is finite or has the same size as the set of positive integers in this chapter we will establish the surprising result that the set of rational numbers is countable while the set of real numbers is not we will also show how the concepts we discuss can be used to show that there are functions that cannot be computed using a computer program in any programming language matrices are used in discrete mathematics to represent a variety of discrete structures we will review the basic material about matrices and matrix arithmetic needed to represent relations and graphs the matrix arithmetic we study will be used to solve a variety of problems involving these structures sets introduction in this section we study the fundamental discrete structure on which all other discrete structures are built namely the set sets are used to group objects together often but not always the objects in a set have similar properties for instance all the students who are currently enrolled in your school make up a set likewise all the students currently taking a course in discrete mathematics at any school make up a set in addition those students enrolled in your school who are taking a course in discrete mathematics form a set that can be obtained by taking the elements common to the first two collections the language of sets is a means to study such collections in an organized fashion we now provide a definition of a set this definition is an intuitive definition which is not part of a formal theory of sets definition it is common for sets to be denoted using uppercase letters lowercase letters are usually used to denote elements of sets there are several ways to describe a set one way is to list all the members of a set when this is possible we use a notation where all members of the set are listed between braces for example the notation a b c d represents the set with the four elements a b c and d this way of describing a set is known as the roster method example the set v of all vowels in the english alphabet can be written as v a e i o u example the set o of odd positive integers less than can be expressed by o example although sets are usually used to group together elements with common properties there is nothing that prevents a set from having seemingly unrelated elements for instance a fred new jersey is the set containing the four elements a fred and new jersey sometimes the roster method is used to describe a set without listing all its members some members of the set are listed and then ellipses are used when the general pattern of the elements is obvious example the set of positive integers less than can be denoted by another way to describe a set is to use set builder notation we characterize all those elements in the set by stating the property or properties they must have to be members for instance the set o of all odd positive integers less than can be written as o x x is an odd positive integer less than or specifying the universe as the set of positive integers as o x z x is odd and x we often use this type of notation to describe sets when it is impossible to list all the elements of the set for instance the set q of all positive rational numbers can be written as q x r x p for some positive integers p and q beware that mathe maticians disagree whether is a natural number we consider it quite natural these sets each denoted using a boldface letter play an important role in discrete mathe matics n the set of natural numbers z the set of integers z the set of positive integers q p q p z q z and q the set of rational numbers r the set of real numbers r the set of positive real numbers c the set of complex numbers note that some people do not consider a natural number so be careful to check how the term natural numbers is used when you read other books recall the notation for intervals of real numbers when a and b are real numbers with a b we write a b x a x b a b x a x b a b x a x b a b x a x b note that a b is called the closed interval from a to b and a b is called the open interval from a to b sets can have other sets as members as example illustrates example the set n z q r is a set containing four elements each of which is a set the four elements of this set are n the set of natural numbers z the set of integers q the set of rational numbers and r the set of real numbers remark note that the concept of a datatype or type in computer science is built upon the concept of a set in particular a datatype or type is the name of a set together with a set of operations that can be performed on objects from that set for example boolean is the name of the set together with operators on one or more elements of this set such as and or and not because many mathematical statements assert that two differently specified collections of objects are really the same set we need to understand what it means for two sets to be equal definition example the sets and are equal because they have the same elements note that the order in which the elements of a set are listed does not matter note also that it does not matter if an element of a set is listed more than once so is the same as the set because they have the same elements georg cantor georg cantor was born in st petersburg russia where his father was a successful merchant cantor developed his interest in mathematics in his teens he began his university studies in zurich in but when his father died he left zurich he continued his university studies at the university of berlin in where he studied under the eminent mathematicians weierstrass kummer and kronecker he received his doctor degree in after having written a dissertation on number theory cantor assumed a position at the university of halle in where he continued working until his death cantor is considered the founder of set theory his contributions in this area include the discovery that the set of real numbers is uncountable he is also noted for his many important contributions to analysis cantor also was interested in philosophy and wrote papers relating his theory of sets with metaphysics cantor married in and had five children his melancholy temperament was balanced by his wife happy disposition although he received a large inheritance from his father he was poorly paid as a professor to mitigate this he tried to obtain a better paying position at the university of berlin his appointment there was blocked by kronecker who did not agree with cantor views on set theory cantor suffered from mental illness throughout the later years of his life he died in from a heart attack has one more element than the empty set there is a special set that has no elements this set is called the empty set or null set and is denoted by the empty set can also be denoted by that is we represent the empty set with a pair of braces that encloses all the elements in this set often a set of elements with certain properties turns out to be the null set for instance the set of all positive integers that are greater than their squares is the null set a set with one element is called a singleton set a common error is to confuse the empty set with the set which is a singleton set the single element of the set is the empty set itself a useful analogy for remembering this difference is to think of folders in a computer file system the empty set can be thought of as an empty folder and the set consisting of just the empty set can be thought of as a folder with exactly one folder inside namely the empty folder naive set theory note that the term object has been used in the definition of a set definition without specifying what an object is this description of a set as a collection of objects based on the intuitive notion of an object was first stated in by the german mathematician georg cantor the theory that results from this intuitive definition of a set and the use of the intuitive notion that for any property whatever there is a set consisting of exactly the objects with this property leads to paradoxes or logical inconsistencies this was shown by the english philosopher bertrand russell in see exercise for a description of one of these paradoxes these logical inconsistencies can be avoided by building set theory beginning with axioms however we will use cantor original version of set theory known as naive set theory in this book because all sets considered in this book can be treated consistently using cantor original theory students will find familiarity with naive set theory helpful if they go on to learn about axiomatic set theory they will also find the development of axiomatic set theory much more abstract than the material in this text we refer the interested reader to to learn more about axiomatic set theory venn diagrams sets can be represented graphically using venn diagrams named after the english mathemati cian john venn who introduced their use in in venn diagrams the universal set u which contains all the objects under consideration is represented by a rectangle note that the uni versal set varies depending on which objects are of interest inside this rectangle circles or other geometrical figures are used to represent sets sometimes points are used to represent the particular elements of the set venn diagrams are often used to indicate the relationships between sets we show how a venn diagram can be used in example example draw a venn diagram that represents v the set of vowels in the english alphabet solution we draw a rectangle to indicate the universal set u which is the set of the letters of the english alphabet inside this rectangle we draw a circle to represent v inside this circle we indicate the elements of v with points see figure figure venn diagram for the set of vowels subsets it is common to encounter situations where the elements of one set are also the elements of a second set we now introduce some terminology and notation to express such relationships between sets definition we see that a b if and only if the quantification x x a x b is true note that to show that a is not a subset of b we need only find one element x a with x b such an x is a counterexample to the claim that x a implies x b we have these useful rules for determining whether one set is a subset of another example the set of all odd positive integers less than is a subset of the set of all positive integers less than the set of rational numbers is a subset of the set of real numbers the set of all computer science majors at your school is a subset of the set of all students at your school and the set of all people in china is a subset of the set of all people in china that is it is a subset of itself each of these facts follows immediately by noting that an element that belongs to the first set in each pair of sets also belongs to the second set in that pair example the set of integers with squares less than is not a subset of the set of nonnegative integers because is in the former set as but not the later set the set of people who have taken discrete mathematics at your school is not a subset of the set of all computer science majors at your school if there is at least one student who has taken discrete mathematics who is not a computer science major bertrand russell bertrand russell was born into a prominent english family active in the progressive movement and having a strong commitment to liberty he became an orphan at an early age and was placed in the care of his father parents who had him educated at home he entered trinity college cambridge in where he excelled in mathematics and in moral science he won a fellowship on the basis of his work on the foundations of geometry in trinity college appointed him to a lectureship in logic and the philosophy of mathematics russell fought for progressive causes throughout his life he held strong pacifist views and his protests against world war i led to dismissal from his position at trinity college he was imprisoned for months in because of an article he wrote that was branded as seditious russell fought for women suffrage in great britain in at the age of he was imprisoned for the second time for his protests advocating nuclear disarmament russell greatest work was in his development of principles that could be used as a foundation for all of mathematics his most famous work is principia mathematica written with alfred north whitehead which attempts to deduce all of mathematics using a set of primitive axioms he wrote many books on philosophy physics and his political ideas russell won the nobel prize for literature in figure venn diagram showing that a is a subset of b theorem theorem shows that every nonempty set s is guaranteed to have at least two subsets the empty set and the set s itself that is s and s s proof we will prove i and leave the proof of ii as an exercise let s be a set to show that s we must show that x x x s is true because the empty set contains no elements it follows that x is always false it follows that the conditional statement x x s is always true because its hypothesis is always false and a conditional statement with a false hypothesis is true therefore x x x s is true this completes the proof of i note that this is an example of a vacuous proof when we wish to emphasize that a set a is a subset of a set b but that a b we write a b and say that a is a proper subset of b for a b to be true it must be the case that a b and there must exist an element x of b that is not an element of a that is a is a proper subset of b if and only if x x a x b x x b x a is true venn diagrams can be used to illustrate that a set a is a subset of a set b we draw the universal set u as a rectangle within this rectangle we draw a circle for b because a is a subset of b we draw the circle for a within the circle for b this relationship is shown in figure a useful way to show that two sets have the same elements is to show that each set is a subset of the other in other words we can show that if a and b are sets with a b and b a then a b that is a b if and only if x x a x b and x x b x a or equivalently if and only if x x a x b which is what it means for the a and b to be equal because this method of showing two sets are equal is so useful we highlight it here john venn john venn was born into a london suburban family noted for its philanthropy he attended london schools and got his mathematics degree from caius college cambridge in he was elected a fellow of this college and held his fellowship there until his death he took holy orders in and after a brief stint of religious work returned to cambridge where he developed programs in the moral sciences besides his mathematical work venn had an interest in history and wrote extensively about his college and family venn book symbolic logic clarifies ideas originally presented by boole in this book venn presents a systematic development of a method that uses geometric figures known now as venn diagrams today these diagrams are primarily used to analyze logical arguments and to illustrate relationships between sets in addition to his work on symbolic logic venn made contributions to probability theory described in his widely used textbook on that subject sets may have other sets as members for instance we have the sets a a b a b and b x x is a subset of the set a b note that these two sets are equal that is a b also note that a a but a a the size of a set sets are used extensively in counting problems and for such applications we need to discuss the sizes of sets definition remark the term cardinality comes from the common usage of the term cardinal number as the size of a finite set example let a be the set of odd positive integers less than then a example let s be the set of letters in the english alphabet then s example because the null set has no elements it follows that we will also be interested in sets that are not finite definition example the set of positive integers is infinite we will extend the notion of cardinality to infinite sets in section a challenging topic full of surprising results power sets many problems involve testing all combinations of elements of a set to see if they satisfy some property to consider all such combinations of elements of a set s we build a new set that has as its members all the subsets of s definition example what is the power set of the set solution the power set p is the set of all subsets of hence p note that the empty set and the set itself are members of this set of subsets example what is the power set of the empty set what is the power set of the set solution the empty set has exactly one subset namely itself consequently p the set has exactly two subsets namely and the set itself therefore p if a set has n elements then its power set has elements we will demonstrate this fact in several ways in subsequent sections of the text cartesian products the order of elements in a collection is often important because sets are unordered a different structure is needed to represent ordered collections this is provided by ordered n tuples definition we say that two ordered n tuples are equal if and only if each corresponding pair of their elements is equal in other words an bn if and only if ai bi for i n in particular ordered tuples are called ordered pairs the ordered pairs a b and c d are equal if and only if a c and b d note that a b and b a are not equal unless a b rené descartes rené descartes was born into a noble family near tours france about miles southwest of paris he was the third child of his father first wife she died several days after his birth because of rené poor health his father a provincial judge let his son formal lessons slide until at the age of rené entered the jesuit college at la flèche the rector of the school took a liking to him and permitted him to stay in bed until late in the morning because of his frail health from then on descartes spent his mornings in bed he considered these times his most productive hours for thinking descartes left school in moving to paris where he spent years studying mathematics he earned a law degree in from the university of poitiers at descartes became disgusted with studying and decided to see the world he moved to paris and became a successful gambler however he grew tired of bawdy living and moved to the suburb of saint germain where he devoted himself to mathematical study when his gambling friends found him he decided to leave france and undertake a military career however he never did any fighting one day while escaping the cold in an overheated room at a military encampment he had several feverish dreams which revealed his future career as a mathematician and philosopher after ending his military career he traveled throughout europe he then spent several years in paris where he studied mathemat ics and philosophy and constructed optical instruments descartes decided to move to holland where he spent years wandering around the country accomplishing his most important work during this time he wrote several books including the discours which contains his contributions to analytic geometry for which he is best known he also made fundamental contributions to philosophy in descartes was invited by queen christina to visit her court in sweden to tutor her in philosophy although he was reluctant to live in what he called the land of bears amongst rocks and ice he finally accepted the invitation and moved to sweden unfortunately the winter of was extremely bitter descartes caught pneumonia and died in mid february many of the discrete structures we will study in later chapters are based on the notion of the cartesian product of sets named after rené descartes we first define the cartesian product of two sets definition example let a represent the set of all students at a university and let b represent the set of all courses offered at the university what is the cartesian product a b and how can it be used solution the cartesian product a b consists of all the ordered pairs of the form a b where a is a student at the university and b is a course offered at the university one way to use the set a b is to represent all possible enrollments of students in courses at the university example what is the cartesian product of a and b a b c solution the cartesian product a b is a b a b c a b c note that the cartesian products a b and b a are not equal unless a or b so that a b or a b see exercises and this is illustrated in example example show that the cartesian product b a is not equal to the cartesian product a b where a and b are as in example solution the cartesian product b a is b a a a b b c c this is not equal to a b which was found in example the cartesian product of more than two sets can also be defined definition example what is the cartesian product a b c where a b and c solution the cartesian product a b c consists of all ordered triples a b c where a a b b and c c hence a b c remark note that when a b and c are sets a b c is not the same as a b c see exercise we use the notation to denote a a the cartesian product of the set a with itself similarly a a a a a a a and so on more generally an an ai a for i n example suppose that a it follows that and a subset r of the cartesian product a b is called a relation from the set a to the set b the elements of r are ordered pairs where the first element belongs to a and the second to b for example r a a a b b c c is a relation from the set a b c to the set a relation from a set a to itself is called a relation on a example what are the ordered pairs in the less than or equal to relation which contains a b if a b on the set solution the ordered pair a b belongs to r if and only if both a and b belong to and a b consequently the ordered pairs in r are and we will study relations and their properties at length in chapter using set notation with quantifiers sometimes we restrict the domain of a quantified statement explicitly by making use of a particular notation for example x s p x denotes the universal quantification of p x over all elements in the set s in other words x s p x is shorthand for x x s p x similarly x s p x denotes the existential quantification of p x over all elements in s that is x s p x is shorthand for x x s p x example what do the statements x r and x z mean solution the statement x r states that for every real number x this state ment can be expressed as the square of every real number is nonnegative this is a true statement the statement x z states that there exists an integer x such that this statement can be expressed as there is an integer whose square is this is also a true statement because x is such an integer as is truth sets and quantifiers we will now tie together concepts from set theory and from predicate logic given a predicate p and a domain d we define the truth set of p to be the set of elements x in d for which p x is true the truth set of p x is denoted by x d p x example what are the truth sets of the predicates p x q x and r x where the domain is the set of integers and p x is x q x is and r x is x x solution the truth set of p x z x is the set of integers for which x because x when x or x and for no other integers x we see that the truth set of p is the set the truth set of q x z is the set of integers for which this is the exercises empty set because there are no integers x for which the truth set of r x z x x is the set of integers for which x x because x x if and only if x it follows that the truth set of r is n the set of nonnegative integers note that xp x is true over the domain u if and only if the truth set of p is the set u likewise xp x is true over the domain u if and only if the truth set of p is nonempty list the members of these sets a x x is a real number such that b x x is a positive integer less than c x x is the square of an integer and x d x x is an integer such that use set builder notation to give a description of each of these sets a b c m n o p for each of these pairs of sets determine whether the first is a subset of the second the second is a subset of the first or neither is a subset of the other a the set of airline flights from new york to new delhi the set of nonstop airline flights from new york to new delhi b the set of people who speak english the set of people who speak chinese c the set of flying squirrels the set of living creatures that can fly for each of these pairs of sets determine whether the first is a subset of the second the second is a subset of the first or neither is a subset of the other a the set of people who speak english the set of people who speak english with an australian accent b the set of fruits the set of citrus fruits c the set of students studying discrete mathematics the set of students studying data structures determine whether each of these pairs of sets are equal a b c suppose that a b c and d determine which of these sets are subsets of which other of these sets for each of the following sets determine whether is an element of that set a x r x is an integer greater than b x r x is the square of an integer c d e f for each of the sets in exercise determine whether is an element of that set determine whether each of these statements is true or false a b c d e f g determine whether these statements are true or false a b c d e f g determine whether each of these statements is true or false a x x b x x c x x d x x e x f x use a venn diagram to illustrate the subset of odd integers in the set of all positive integers not exceeding use a venn diagram to illustrate the set of all months of the year whose names do not contain the letter r in the set of all months of the year use a venn diagram to illustrate the relationship a b and b c use a venn diagram to illustrate the relationships a b and b c use a venn diagram to illustrate the relationships a b and a c suppose that a b and c are sets such that a b and b c show that a c find two sets a and b such that a b and a b what is the cardinality of each of these sets a a b a c a a d a a a a what is the cardinality of each of these sets a b c d find the power set of each of these sets where a and b are distinct elements a a b a b c can you conclude that a b if a and b are two sets with the same power set how many elements does each of these sets have where a and b are distinct elements a p a b a b b p a a a c p p determine whether each of these sets is the power set of a set where a and b are distinct elements a b a c a a d a b a b prove that p a p b if and only if a b show that if a c and b d then a b c d let a a b c d and b y z find a a b b b a what is the cartesian product a b where a is the set of courses offered by the mathematics department at a university and b is the set of mathematics professors at this university give an example of how this cartesian product can be used what is the cartesian product a b c where a is the set of all airlines and b and c are both the set of all cities in the united states give an example of how this cartesian product can be used suppose that a b where a and b are sets what can you conclude let a be a set show that a a let a a b c b x y and c find a a b c b c b a c c a b d b b b find if a a b a a b find if a a a b a a how many different elements does a b have if a has m elements and b has n elements how many different elements does a b c have if a has m elements b has n elements and c has p elements how many different elements does an have when a has m elements and n is a positive integer show that a b b a when a and b are nonempty unless a b explain why a b c and a b c are not the same explain why a b c d and a b c d are not the same translate each of these quantifications into english and determine its truth value a x r b x z c x z d x r x translate each of these quantifications into english and determine its truth value a x r b x z x x c x z x z d x z z find the truth set of each of these predicates where the domain is the set of integers a p x b q x x c r x find the truth set of each of these predicates where the domain is the set of integers a p x b q x c r x x the defining property of an ordered pair is that two or dered pairs are equal if and only if their first elements are equal and their second elements are equal surpris ingly instead of taking the ordered pair as a primitive con cept we can construct ordered pairs using basic notions from set theory show that if we define the ordered pair a b to be a a b then a b c d if and only if a c and b d hint first show that a a b c c d if and only if a c and b d this exercise presents russell paradox let s be the set that contains a set x if the set x does not belong to itself so that s x x x a show the assumption that s is a member of s leads to a contradiction b show the assumption that s is not a member of s leads to a contradiction by parts a and b it follows that the set s cannot be de fined as it was this paradox can be avoided by restricting the types of elements that sets can have describe a procedure for listing all the subsets of a finite set set operations introduction two or more sets can be combined in many different ways for instance starting with the set of mathematics majors at your school and the set of computer science majors at your school we can form the set of students who are mathematics majors or computer science majors the set of students who are joint majors in mathematics and computer science the set of all students not majoring in mathematics and so on definition an element x belongs to the union of the sets a and b if and only if x belongs to a or x belongs to b this tells us that a b x x a x b the venn diagram shown in figure represents the union of two sets a and b the area that represents a b is the shaded area within either the circle representing a or the circle representing b we will give some examples of the union of sets example the union of the sets and is the set that is example the union of the set of all computer science majors at your school and the set of all mathe matics majors at your school is the set of students at your school who are majoring either in mathematics or in computer science or in both definition an element x belongs to the intersection of the sets a and b if and only if x belongs to a and x belongs to b this tells us that a b x x a x b a u b is shaded figure venn diagram of the union of a and b a n b is shaded figure venn diagram of the intersection of a and b the venn diagram shown in figure represents the intersection of two sets a and b the shaded area that is within both the circles representing the sets a and b is the area that represents the intersection of a and b we give some examples of the intersection of sets example the intersection of the sets and is the set that is example the intersection of the set of all computer science majors at your school and the set of all mathematics majors is the set of all students who are joint majors in mathematics and computer science definition example let a and b because a b a and b are disjoint we are often interested in finding the cardinality of a union of two finite sets a and b note be careful not to that a b counts each element that is in a but not in b or in b but not in a exactly once overcount and each element that is in both a and b exactly twice thus if the number of elements that are in both a and b is subtracted from a b elements in a b will be counted only once hence a b a b a b the generalization of this result to unions of an arbitrary number of sets is called the principle of inclusion exclusion the principle of inclusion exclusion is an important technique used in enumeration we will discuss this principle and other counting techniques in detail in chapters and there are other important ways to combine sets definition remark the difference of sets a and b is sometimes denoted by a b an element x belongs to the difference of a and b if and only if x a and x b this tells us that a b x x a x b the venn diagram shown in figure represents the difference of the sets a and b the shaded area inside the circle that represents a and outside the circle that represents b is the area that represents a b we give some examples of differences of sets example the difference of and is the set that is this is different from the difference of and which is the set example the difference of the set of computer science majors at your school and the set of mathematics majors at your school is the set of all computer science majors at your school who are not also mathematics majors a b is shaded figure venn diagram for the difference of a and b a is shaded figure venn diagram for the complement of the set a once the universal set u has been specified the complement of a set can be defined definition an element belongs to a if and only if x a this tells us that a x u x a in figure the shaded area outside the circle representing a is the area representing a we give some examples of the complement of a set example let a a e i o u where the universal set is the set of letters of the english alphabet then a b c d f g h j k l m n p q r t v w x y z example let a be the set of positive integers greater than with universal set the set of all positive integers then a it is left to the reader exercise to show that we can express the difference of a and b as the intersection of a and the complement of b that is a b a b set identities table lists the most important set identities we will prove several of these identities here using three different methods these methods are presented to illustrate that there are often many set identities and propositional equivalences are just special cases of identities for boolean algebra different approaches to the solution of a problem the proofs of the remaining identities will be left as exercises the reader should note the similarity between these set identities and the logical equivalences discussed in section compare table of section and table in fact the set identities given can be proved directly from the corresponding logical equivalences furthermore both are special cases of identities that hold for boolean algebra discussed in chapter one way to show that two sets are equal is to show that each is a subset of the other recall that to show that one set is a subset of a second set we can show that if an element belongs to the first set then it must also belong to the second set we generally use a direct proof to do this we illustrate this type of proof by establishing the first of de morgan laws table set identities identity name a u a a a identity laws a u u a domination laws a a a a a a idempotent laws a a complementation law a b b a a b b a commutative laws a b c a b c a b c a b c associative laws a b c a b a c a b c a b a c distributive laws a b a b a b a b de morgan laws a a b a a a b a absorption laws a a u a a complement laws example prove that a b a b this identity says that the complement of the intersection of two sets is the union of their complements solution we will prove that the two sets a b and a b are equal by showing that each set is a subset of the other first we will show that a b a b we do this by showing that if x is in a b then it must also be in a b now suppose that x a b by the definition of complement x a b using the definition of intersection we see that the proposition x a x b is true by applying de morgan law for propositions we see that x a or x b using the definition of negation of propositions we have x a or x b using the definition of the complement of a set we see that this implies that x a or x b consequently by the definition of union we see that x a b we have now shown that a b a b next we will show that a b a b we do this by showing that if x is in a b then it must also be in a b now suppose that x a b by the definition of union we know that x a or x b using the definition of complement we see that x a or x b consequently the proposition x a x b is true by de morgan law for propositions we conclude that x a x b is true by the definition of intersection it follows that x a b we now use the definition of complement to conclude that x a b this shows that a b a b because we have shown that each set is a subset of the other the two sets are equal and the identity is proved we can more succinctly express the reasoning used in example using set builder notation as example illustrates example use set builder notation and logical equivalences to establish the first de morgan law a b a b solution we can prove this identity with the following steps a b x x a b by definition of complement x x a b by definition of does not belong symbol x x a x b by definition of intersection x x a x b by the first de morgan law for logical equivalences x x a x b by definition of does not belong symbol x x a x b by definition of complement x x a b by definition of union a b by meaning of set builder notation note that besides the definitions of complement union set membership and set builder notation this proof uses the second de morgan law for logical equivalences proving a set identity involving more than two sets by showing each side of the identity is a subset of the other often requires that we keep track of different cases as illustrated by the proof in example of one of the distributive laws for sets example prove the second distributive law from table which states that a b c a b a c for all sets a b and c solution we will prove this identity by showing that each side is a subset of the other side suppose that x a b c then x a and x b c by the definition of union it follows that x a and x b or x c or both in other words we know that the compound proposition x a x b x c is true by the distributive law for conjunction over disjunction it follows that x a x b x a x c we conclude that either x a and x b or x a and x c by the definition of intersection it follows that x a b or x a c using the definition of union we conclude that x a b a c we conclude that a b c a b a c now suppose that x a b a c then by the definition of union x a b or x a c by the definition of intersection it follows that x a and x b or that x a and x c from this we see that x a and x b or x c consequently by the definition of union we see that x a and x b c furthermore by the definition of intersection it follows that x a b c we conclude that a b a c a b c this completes the proof of the identity set identities can also be proved using membership tables we consider each combination of sets that an element can belong to and verify that elements in the same combinations of sets belong to both the sets in the identity to indicate that an element is in a set a is used to indicate that an element is not in a set a is used the reader should note the similarity between membership tables and truth tables example use a membership table to show that a b c a b a c solution the membership table for these combinations of sets is shown in table this table has eight rows because the columns for a b c and a b a c are the same the identity is valid additional set identities can be established using those that we have already proved consider example table a membership table for the distributive property a b c b c a b c a b a c a b a c example let a b and c be sets show that a b c c b a solution we have a b c a b c by the first de morgan law a b c by the second de morgan law b c a by the commutative law for intersections c b a by the commutative law for unions generalized unions and intersections because unions and intersections of sets satisfy associative laws the sets a b c and a b c are well defined that is the meaning of this notation is unambiguous when a b and c are sets that is we do not have to use parentheses to indicate which operation comes first because a b c a b c and a b c a b c note that a b c contains those elements that are in at least one of the sets a b and c and that a b c contains those elements that are in all of a b and c these combinations of the three sets a b and c are shown in figure a a u b u c is shaded b a b c is shaded figure the union and intersection of a b and c example let a b and c what are a b c and a b c solution the set a b c contains those elements in at least one of a b and c hence a b c the set a b c contains those elements in all three of a b and c thus a b c we can also consider unions and intersections of an arbitrary number of sets we introduce these definitions definition we use the notation n an ai i to denote the union of the sets an definition we use the notation n an ai i to denote the intersection of the sets an we illustrate generalized unions and intersections with example example for i let ai i i i then n n ai i i i and i i n n n ai n i i i n n n an we can extend the notation we have introduced for unions and intersections to other families of sets in particular we use the notation an ai i to denote the union of the sets an similarly the intersection of these sets is denoted by an ai i the intersection and union of the sets ai for i i respectively note that we have i i ai example suppose that ai i for i then ai i z and i i n ai n i to see that the union of these sets is the set of positive integers note that every positive integer n is in at least one of the sets because it belongs to an n and every element of the sets in the union is a positive integer to see that the intersection of these sets is the set note that the only element that belongs to all the sets is to see this note that and ai for i computer representation of sets there are various ways to represent sets using a computer one method is to store the elements of the set in an unordered fashion however if this is done the operations of computing the union intersection or difference of two sets would be time consuming because each of these operations would require a large amount of searching for elements we will present a method for storing elements using an arbitrary ordering of the elements of the universal set this method of representing sets makes computing combinations of sets easy assume that the universal set u is finite and of reasonable size so that the number of elements of u is not larger than the memory size of the computer being used first specify an arbitrary ordering of the elements of u for instance an represent a subset a of u with the bit string of length n where the ith bit in this string is if ai belongs to a and is if ai does not belong to a example illustrates this technique example let u and the ordering of elements of u has the elements in increasing order that is ai i what bit strings represent the subset of all odd integers in u the subset of all even integers in u and the subset of integers not exceeding in u solution the bit string that represents the set of odd integers in u namely has a one bit in the first third fifth seventh and ninth positions and a zero elsewhere it is we have split this bit string of length ten into blocks of length four for easy reading similarly we represent the subset of all even integers in u namely by the string the set of all integers in u that do not exceed namely is represented by the string using bit strings to represent sets it is easy to find complements of sets and unions inter sections and differences of sets to find the bit string for the complement of a set from the bit string for that set we simply change each to a and each to because x a if and only if x a note that this operation corresponds to taking the negation of each bit when we associate a bit with a truth value with representing true and representing false example we have seen that the bit string for the set with universal set is what is the bit string for the complement of this set solution the bit string for the complement of this set is obtained by replacing with and vice versa this yields the string which corresponds to the set to obtain the bit string for the union and intersection of two sets we perform bitwise boolean operations on the bit strings representing the two sets the bit in the ith position of the bit string of the union is if either of the bits in the ith position in the two strings is or both are and is when both bits are hence the bit string for the union is the bitwise or of the bit strings for the two sets the bit in the ith position of the bit string of the intersection is when the bits in the corresponding position in the two strings are both and is when either of the two bits is or both are hence the bit string for the intersection is the bitwise and of the bit strings for the two sets example the bit strings for the sets and are and respectively use bit strings to find the union and intersection of these sets solution the bit string for the union of these sets is which corresponds to the set the bit string for the intersection of these sets is which corresponds to the set exercises let a be the set of students who live within one mile of school and let b be the set of students who walk to classes describe the students in each of these sets a a b b a b c a b d b a suppose that a is the set of sophomores at your school and b is the set of students in discrete mathematics at your school express each of these sets in terms of a and b a the set of sophomores taking discrete mathematics in your school b the set of sophomores at your school who are not tak ing discrete mathematics c the set of students at your school who either are sopho mores or are taking discrete mathematics d the set of students at your school who either are not sophomores or are not taking discrete mathematics let a and b find a a b b a b c a b d b a let a a b c d e and b a b c d e f g h a a b b a b c a b d b a in exercises assume that a is a subset of some underly ing universal set u prove the complementation law in table by showing that a a prove the identity laws in table by showing that a a a b a u a prove the domination laws in table by showing that a a u u b a prove the idempotent laws in table by showing that a a a a b a a a prove the complement laws in table by showing that a a a u b a a show that a a a b a let a and b be sets prove the commutative laws from table by showing that a a b b a b a b b a prove the first absorption law from table by showing that if a and b are sets then a a b a prove the second absorption law from table by showing that if a and b are sets then a a b a find the sets a and b if a b b a and a b prove the second de morgan law in table by showing that if a and b are sets then a b a b a by showing each side is a subset of the other side b using a membership table let a and b be sets show that a a b a b a a b c a b a d a b a e a b a a b show that if a b and c are sets then a b c a b c a by showing each side is a subset of the other side b using a membership table let a b and c be sets show that a a b a b c b a b c a b c a b c a c d a c c b e b a c a b c a show that if a and b are sets then a a b a b b a b a b a show that if a and b are sets with a b then a a b b b a b a prove the first associative law from table by show ing that if a b and c are sets then a b c a b c prove the second associative law from table by show ing that if a b and c are sets then a b c a b c prove the first distributive law from table by show ing that if a b and c are sets then a b c a b a c let a b and c be sets show that a b c a c b c let a b and c find a a b c b a b c c a b c d a b c draw the venn diagrams for each of these combinations of the sets a b and c a a b c b a b c c a b a c b c draw the venn diagrams for each of these combinations of the sets a b and c a a b c b a b a c c a b a c draw the venn diagrams for each of these combinations of the sets a b c and d a a b c d b a b c d c a b c d what can you say about the sets a and b if we know that a a b a b a b a c a b a d a b b a e a b b a can you conclude that a b if a b and c are sets such that a a c b c b a c b c let ai be the set of all nonempty bit strings that is bit strings of length at least one of length not exceeding i find n n c a c b c and a c b c let a and b be subsets of a universal set u show that a ai b ai i i a b if and only if b a the symmetric difference of a and b denoted by a b is find i a ai ai and i i i ai if for every positive integer i the set containing those elements in either a or b but not in both a and b b ai i i find the symmetric difference of and find the symmetric difference of the set of computer sci ence majors at a school and the set of mathematics majors c ai i that is the set of real numbers x with x i d ai i that is the set of real numbers x with x i at this school draw a venn diagram for the symmetric difference of the find i a a ai and n i ai if for every positive integer i sets a and b show that a b a b a b i i i i i b ai i i c ai i i that is the set of real numbers x with show that a b a b b a show that if a is a subset of a universal set u then a a a b a a c a u a d a a u show that if a and b are sets then a a b b a b a b b a what can you say about the sets a and b if a b a determine whether the symmetric difference is associa tive that is if a b and c are sets does it follow that a b c a b c suppose that a b and c are sets such that a c b c must it be the case that a b if a b c and d are sets does it follow that a b c d a c b d if a b c and d are sets does it follow that a b c d a d b c show that if a and b are finite sets then a b is a finite set show that if a is an infinite set then whenever b is a set a b is also an infinite set show that if a b and c are sets then a b c a b c a b a c b c a b c this is a special case of the inclusion exclusion princi ple which will be studied in chapter let ai i for i find i x i d ai i that is the set of real numbers x with x i suppose that the universal set is u express each of these sets with bit strings where the ith bit in the string is if i is in the set and otherwise a b c using the same universal set as in the last problem find the set specified by each of these bit strings a b c what subsets of a finite universal set do these bit strings represent a the string with all zeros b the string with all ones what is the bit string corresponding to the difference of two sets what is the bit string corresponding to the symmetric dif ference of two sets show how bitwise operations on bit strings can be used to find these combinations of a a b c d e b b c d g p t v c c e i o u x y z and d d e h i n o t u x y a a b b a b c a d b c d a b c d n a ai b i n ai i how can the union and intersection of n sets that all are subsets of the universal set u be found using bit strings the successor of the set a is the set a a let ai i find find the successors of the following sets n a ai b i n ai i a b c d how many elements does the successor of a set with n elements have sometimes the number of times that an element occurs in an unordered collection matters multisets are unordered collec tions of elements where an element can occur as a member more than once the notation mr ar denotes the multiset with element occurring times el ement occurring times and so on the numbers mi i r are called the multiplicities of the elements ai i r let p and q be multisets the union of the multisets p and q is the multiset where the multiplicity of an element is the maximum of its multiplicities in p and q the intersec tion of p and q is the multiset where the multiplicity of an element is the minimum of its multiplicities in p and q the difference of p and q is the multiset where the multiplicity of an element is the multiplicity of the element in p less its multiplicity in q unless this difference is negative in which case the multiplicity is the sum of p and q is the multiset where the multiplicity of an element is the sum of multiplic ities in p and q the union intersection and difference of p and q are denoted by p q p q and p q respec tively where these operations should not be confused with the analogous operations for sets the sum of p and q is denoted by p q let a and b be the multisets a b c and a b d respectively find a a b b a b c a b d b a e a b suppose that a is the multiset that has as its elements the types of computer equipment needed by one depart ment of a university and the multiplicities are the number of pieces of each type needed and b is the analogous multiset for a second department of the university for instance a could be the multiset personal comput ers routers servers and b could be the multiset personal computers routers mainframes a what combination of a and b represents the equip ment the university should buy assuming both depart ments use the same equipment b what combination of a and b represents the equip ment that will be used by both departments if both departments use the same equipment c what combination of a and b represents the equip ment that the second department uses but the first de partment does not if both departments use the same equipment d what combination of a and b represents the equip ment that the university should purchase if the depart ments do not share equipment fuzzy sets are used in artificial intelligence each element in the universal set u has a degree of membership which is a real number between and including and in a fuzzy set s the fuzzy set s is denoted by listing the elements with their degrees of membership elements with degree of membership are not listed for instance we write alice brian fred oscar rita for the set f of fa mous people to indicate that alice has a degree of mem bership in f brian has a degree of membership in f fred has a degree of membership in f oscar has a degree of membership in f and rita has a degree of membership in f so that brian is the most famous and oscar is the least famous of these people also suppose that r is the set of rich people with r alice brian fred oscar rita the complement of a fuzzy set s is the set s with the degree of the membership of an element in s equal to minus the degree of membership of this element in s find f the fuzzy set of people who are not famous and r the fuzzy set of people who are not rich the union of two fuzzy sets s and t is the fuzzy set s t where the degree of membership of an element in s t is the maximum of the degrees of membership of this element in s and in t find the fuzzy set f r of rich or famous people the intersection of two fuzzy sets s and t is the fuzzy set s t where the degree of membership of an element in s t is the minimum of the degrees of membership of this element in s and in t find the fuzzy set f r of rich and famous people functions introduction in many instances we assign to each element of a set a particular element of a second set which may be the same as the first for example suppose that each student in a discrete mathematics class is assigned a letter grade from the set a b c d f and suppose that the grades are a for adams c for chou b for goodfriend a for rodriguez and f for stevens this assignment of grades is illustrated in figure this assignment is an example of a function the concept of a function is extremely impor tant in mathematics and computer science for example in discrete mathematics functions are used in the definition of such discrete structures as sequences and strings functions are also used to represent how long it takes a computer to solve problems of a given size many computer programs and subroutines are designed to calculate values of functions recursive functions adams a chou b goodfriend c rodriguez d stevens f figure assignment of grades in a discrete mathematics class which are functions defined in terms of themselves are used throughout computer science they will be studied in chapter this section reviews the basic concepts involving functions needed in discrete mathematics definition remark functions are sometimes also called mappings or transformations functions are specified in many different ways sometimes we explicitly state the assign ments as in figure often we give a formula such as f x x to define a function other times we use a computer program to specify a function a function f a b can also be defined in terms of a relation from a to b recall from section that a relation from a to b is just a subset of a b a relation from a to b that contains one and only one ordered pair a b for every element a a defines a function f from a to b this function is defined by the assignment f a b where a b is the unique ordered pair in the relation that has a as its first element definition figure represents a function f from a to b when we define a function we specify its domain its codomain and the mapping of elements of the domain to elements in the codomain two functions are equal when they have the same domain have the same codomain and map each element of their common domain to the same element in their common codomain note that if we change either the domain or the codomain figure the function f maps a to b of a function then we obtain a different function if we change the mapping of elements then we also obtain a different function examples provide examples of functions in each case we describe the domain the codomain the range and the assignment of values to elements of the domain example what are the domain codomain and range of the function that assigns grades to students described in the first paragraph of the introduction of this section solution let g be the function that assigns a grade to a student in our discrete mathematics class note that g adams a for instance the domain of g is the set adams chou goodfriend rodriguez stevens and the codomain is the set a b c d f the range of g is the set a b c f because each grade except d is assigned to some student example let r be the relation with ordered pairs abdul brenda carla desire eddie and felicia here each pair consists of a graduate student and this student age specify a function determined by this relation solution if f is a function specified by r then f abdul f brenda f carla f desire f eddie and f felicia here f x is the age of x where x is a student for the domain we take the set abdul brenda carla desire eddie felicia we also need to specify a codomain which needs to contain all possible ages of students because it is highly likely that all students are less than years old we can take the set of positive integers less than as the codomain note that we could choose a different codomain such as the set of all positive integers or the set of positive integers between and but that would change the function using this codomain will also allow us to extend the function by adding the names and ages of more students later the range of the function we have specified is the set of different ages of these students which is the set example let f be the function that assigns the last two bits of a bit string of length or greater to that string for example f then the domain of f is the set of all bit strings of length or greater and both the codomain and range are the set example let f z z assign the square of an integer to this integer then f x where the domain of f is the set of all integers the codomain of f is the set of all integers and the range of f is the set of all integers that are perfect squares namely example the domain and codomain of functions are often specified in programming languages for instance the java statement int floor float real and the c function statement int function float x both tell us that the domain of the floor function is the set of real numbers represented by floating point numbers and its codomain is the set of integers a function is called real valued if its codomain is the set of real numbers and it is called integer valued if its codomain is the set of integers two real valued functions or two integer valued functions with the same domain can be added as well as multiplied definition note that the functions and have been defined by specifying their values at x in terms of the values of and at x example let and be functions from r to r such that x and x x what are the functions and solution from the definition of the sum and product of functions it follows that x x x x x and x x when f is a function from a to b the image of a subset of a can also be defined definition remark the notation f s for the image of the set s under the function f is potentially ambiguous here f s denotes a set and not the value of the function f for the set s example let a a b c d e and b with f a f b f c f d and f e the image of the subset s b c d is the set f s one to one and onto functions some functions never assign the same value to two different domain elements these functions are said to be one to one definition a b c d figure a one to one function note that a function f is one to one if and only if f a f b whenever a b this way of expressing that f is one to one is obtained by taking the contrapositive of the implication in the definition remark we can express that f is one to one using quantifiers as a b f a f b a b or equivalently a b a b f a f b where the universe of discourse is the domain of the function we illustrate this concept by giving examples of functions that are one to one and other functions that are not one to one example determine whether the function f from a b c d to with f a f b f c and f d is one to one solution the function f is one to one because f takes on different values at the four elements of its domain this is illustrated in figure example determine whether the function f x from the set of integers to the set of integers is one to one solution the function f x is not one to one because for instance f f but note that the function f x with its domain restricted to z is one to one techni cally when we restrict the domain of a function we obtain a new function whose values agree with those of the original function for the elements of the restricted domain the restricted function is not defined for elements of the original domain outside of the restricted domain example determine whether the function f x x from the set of real numbers to itself is one to one solution the function f x x is a one to one function to demonstrate this note that x y when x y example suppose that each worker in a group of employees is assigned a job from a set of possible jobs each to be done by a single worker in this situation the function f that assigns a job to each worker is one to one to see this note that if x and y are two different workers then f x f y because the two workers x and y must be assigned different jobs we now give some conditions that guarantee that a function is one to one a b c d figure an onto function definition remark a function f is increasing if x y x y f x f y strictly increasing if x y x y f x f y decreasing if x y x y f x f y and strictly de creasing if x y x y f x f y where the universe of discourse is the domain of f from these definitions it can be shown see exercises and that a function that is either strictly increasing or strictly decreasing must be one to one however a function that is increasing but not strictly increasing or decreasing but not strictly decreasing is not one to one for some functions the range and the codomain are equal that is every member of the codomain is the image of some element of the domain functions with this property are called onto functions definition remark a function f is onto if y x f x y where the domain for x is the domain of the function and the domain for y is the codomain of the function we now give examples of onto functions and functions that are not onto example let f be the function from a b c d to defined by f a f b f c and f d is f an onto function solution because all three elements of the codomain are images of elements in the domain we see that f is onto this is illustrated in figure note that if the codomain were then f would not be onto example is the function f x from the set of integers to the set of integers onto solution the function f is not onto because there is no integer x with for instance example is the function f x x from the set of integers to the set of integers onto a one to one not onto b onto not one to one c one to one and onto d neither one to one nor onto e not a function a a a a a b b b b b c c c c c d d d figure examples of different types of correspondences solution this function is onto because for every integer y there is an integer x such that f x y to see this note that f x y if and only if x y which holds if and only if x y example consider the function f in example that assigns jobs to workers the function f is onto if for every job there is a worker assigned this job the function f is not onto when there is at least one job that has no worker assigned it definition examples and illustrate the concept of a bijection example let f be the function from a b c d to with f a f b f c and f d is f a bijection solution the function f is one to one and onto it is one to one because no two values in the domain are assigned the same function value it is onto because all four elements of the codomain are images of elements in the domain hence f is a bijection figure displays four functions where the first is one to one but not onto the second is onto but not one to one the third is both one to one and onto and the fourth is neither one to one nor onto the fifth correspondence in figure is not a function because it sends an element to two different elements suppose that f is a function from a set a to itself if a is finite then f is one to one if and only if it is onto this follows from the result in exercise this is not necessarily the case if a is infinite as will be shown in section example let a be a set the identity function on a is the function ιa a a where ιa x x for all x a in other words the identity function ιa is the function that assigns each element to itself the function ιa is one to one and onto so it is a bijection note that ι is the greek letter iota for future reference we summarize what needs be to shown to establish whether a function is one to one and whether it is onto it is instructive to review examples in light of this summary inverse functions and compositions of functions now consider a one to one correspondence f from the set a to the set b because f is an onto function every element of b is the image of some element in a furthermore because f is also a one to one function every element of b is the image of a unique element of a consequently we can define a new function from b to a that reverses the correspondence given by f this leads to definition definition remark be sure not to confuse the function f with the function f which is the function that assigns to each x in the domain the value f x notice that the latter makes sense only when f x is a non zero real number figure illustrates the concept of an inverse function if a function f is not a one to one correspondence we cannot define an inverse function of f when f is not a one to one correspondence either it is not one to one or it is not onto if f is not one to one some element b in the codomain is the image of more than one element in the domain if f is not onto for some element b in the codomain no element a in the domain exists for which f a b consequently if f is not a one to one correspondence we cannot assign to each element b in the codomain a unique element a in the domain such that f a b because for some b there is either more than one such a or no such a a one to one correspondence is called invertible because we can define an inverse of this function a function is not invertible if it is not a one to one correspondence because the inverse of such a function does not exist f b f figure the function f is the inverse of function f example let f be the function from a b c to such that f a f b and f c is f invertible and if it is what is its inverse solution the function f is invertible because it is a one to one correspondence the in verse function f reverses the correspondence given by f so f c f a and f b example let f z z be such that f x x is f invertible and if it is what is its inverse solution the function f has an inverse because it is a one to one correspondence as follows from examples and to reverse the correspondence suppose that y is the image of x so that y x then x y this means that y is the unique element of z that is sent to y by f consequently f y y example let f be the function from r to r with f x is f invertible solution because f f f is not one to one if an inverse function were defined it would have to assign two elements to hence f is not invertible note we can also show that f is not invertible because it is not onto sometimes we can restrict the domain or the codomain of a function or both to obtain an invertible function as example illustrates example show that if we restrict the function f x in example to a function from the set of all nonnegative real numbers to the set of all nonnegative real numbers then f is invertible solution the function f x from the set of nonnegative real numbers to the set of non negative real numbers is one to one to see this note that if f x f y then y2 so y2 x y x y this means that x y or x y so x y or x y because both x and y are nonnegative we must have x y so this function is one to one furthermore f x is onto when the codomain is the set of all nonnegative real numbers because each nonnegative real number has a square root that is if y is a nonnegative real number there exists a nonnegative real number x such that x y which means that y because the function f x from the set of nonnegative real numbers to the set of non negative real numbers is one to one and onto it is invertible its inverse is given by the rule f y y definition in other words f g is the function that assigns to the element a of a the element assigned by f to g a that is to find f g a we first apply the function g to a to obtain g a and then we apply the function f to the result g a to obtain f g a f g a note that the composition f g cannot be defined unless the range of g is a subset of the domain of f in figure the composition of functions is shown f g a f g figure the composition of the functions f and g example let g be the function from the set a b c to itself such that g a b g b c and g c a let f be the function from the set a b c to the set such that f a f b and f c what is the composition of f and g and what is the composition of g and f solution the composition f g is defined by f g a f g a f b f g b f g b f c and f g c f g c f a note that g f is not defined because the range of f is not a subset of the domain of g example let f and g be the functions from the set of integers to the set of integers defined by f x and g x what is the composition of f and g what is the com position of g and f solution both the compositions f g and g f are defined moreover f g x f g x f and g f x g f x g remark note that even though f g and g f are defined for the functions f and g in example f g and g f are not equal in other words the commutative law does not hold for the composition of functions when the composition of a function and its inverse is formed in either order an identity function is obtained to see this suppose that f is a one to one correspondence from the set a to the set b then the inverse function f exists and is a one to one correspondence from b to a the inverse function reverses the correspondence of the original function so f b a when f a b and f a b when f b a hence f f a f f a f b a and f f b f f b f a b consequently f f ιa and f f ιb where ιa and ιb are the identity functions on the sets a and b respectively that is f f the graphs of functions we can associate a set of pairs in a b to each function from a to b this set of pairs is called the graph of the function and is often displayed pictorially to aid in understanding the behavior of the function definition from the definition the graph of a function f from a to b is the subset of a b containing the ordered pairs with the second entry equal to the element of b assigned by f to the first entry also note that the graph of a function f from a to b is the same as the relation from a to b determined by the function f as described on page example display the graph of the function f n from the set of integers to the set of integers solution the graph of f is the set of ordered pairs of the form n where n is an integer this graph is displayed in figure example display the graph of the function f x from the set of integers to the set of integers solution the graph of f is the set of ordered pairs of the form x f x x where x is an integer this graph is displayed in figure some important functions next we introduce two important functions in discrete mathematics namely the floor and ceiling functions let x be a real number the floor function rounds x down to the closest integer less than or equal to x and the ceiling function rounds x up to the closest integer greater than or equal to x these functions are often used when objects are counted they play an important role in the analysis of the number of steps used by procedures to solve problems of a particular size figure the graph of f n from z to z figure the graph of f x from z to z definition remark the floor function is often also called the greatest integer function it is often denoted by x example these are some values of the floor and ceiling functions j j we display the graphs of the floor and ceiling functions in figure in figure a we display the graph of the floor function x note that this function has the same value throughout the interval n n namely n and then it jumps up to n when x n in figure b we display the graph of the ceiling function x note that this function has the same value throughout the interval n n namely n and then jumps to n when x is a little larger than n the floor and ceiling functions are useful in a wide variety of applications including those involving data storage and data transmission consider examples and typical of basic calculations done when database and data communications problems are studied example data stored on a computer disk or transmitted over a data network are usually represented as a string of bytes each byte is made up of bits how many bytes are required to encode bits of data solution to determine the number of bytes needed we determine the smallest integer that is at least as large as the quotient when is divided by the number of bits in a byte consequently bytes are required example in asynchronous transfer mode atm a communications protocol used on backbone networks data are organized into cells of bytes how many atm cells can be transmitted in minute over a connection that transmits data at the rate of kilobits per second solution in minute this connection can transmit bits each atm cell is bytes long which means that it is bits long to determine the number a y x b y x figure graphs of the a floor and b ceiling functions table useful properties of the floor and ceiling functions n is an integer x is a real number xj n if and only if n x n n if and only if n x n xj n if and only if x n x n if and only if x n x x xj x x xj x xj x nj xj n n n of cells that can be transmitted in minute we determine the largest integer not exceeding the quotient when is divided by consequently 754 atm cells can be transmitted in minute over a kilobit per second connection table with x denoting a real number displays some simple but important properties of the floor and ceiling functions because these functions appear so frequently in discrete mathematics it is useful to look over these identities each property in this table can be established using the definitions of the floor and ceiling functions properties and follow directly from these definitions for example states that x n if and only if the integer n is less than or equal to x and n is larger than x this is precisely what it means for n to be the greatest integer not exceeding x which is the definition of x n properties 1c and can be established similarly we will prove property using a direct proof proof suppose that x m where m is a positive integer by property it follows that m x m adding n to all three quantities in this chain of two inequalities shows that m n x n m n using property again we see that x n m n x n this completes the proof proofs of the other properties are left as exercises the floor and ceiling functions enjoy many other useful properties besides those displayed in table there are also many statements about these functions that may appear to be correct but actually are not we will consider statements about the floor and ceiling functions in examples and a useful approach for considering statements about the floor function is to let x n e where n xj is an integer and e the fractional part of x satisfies the inequality e similarly when considering statements about the ceiling function it is useful to write x n e where n is an integer and e example prove that if x is a real number then xj x j solution to prove this statement we let x n e where n is an integer and e there are two cases to consider depending on whether e is less than or greater than or equal to the reason we choose these two cases will be made clear in the proof we first consider the case when e in this case and because similarly x n e so x j n because e consequently and xj x j n n next we consider the case when e in this case because 2e it follows that because x j n e j n e j and e it follows that x j n consequently and xj x j n n this con cludes the proof example prove or disprove that y for all real numbers x and y solution although this statement may appear reasonable it is false a counterexample is sup plied by x and y with these values we find that y but there are certain types of functions that will be used throughout the text these include polynomial logarithmic and exponential functions a brief review of the properties of these functions needed in this text is given in appendix in this book the notation log x will be used to denote the logarithm to the base of x because is the base that we will usually use for logarithms we will denote logarithms to the base b where b is any real number greater than by logb x and the natural logarithm by ln x another function we will use throughout this text is the factorial function f n z denoted by f n n the value of f n n is the product of the first n positive integers so f n n n and f example we have f f f and f example illustrates that the factorial function grows extremely rapidly as n grows the rapid growth of the factorial functio n is made clearer by stirling formula a result from higher mathematics that tell us that n n e n here we have used the notation f n g n which means that the ratio f n g n approaches as n grows without bound that is limn f n g n the symbol is read is asymptotic to stirling formula is named after james stirling a scottish mathematician of the eighteenth century james stirling james stirling was born near the town of stirling scotland his family strongly supported the jacobite cause of the stuarts as an alternative to the british crown the first information known about james is that he entered balliol college oxford on a scholarship in however he later lost his scholarship when he refused to pledge his allegiance to the british crown the first jacobean rebellion took place in and stirling was accused of communicating with rebels he was charged with cursing king george but he was acquitted of these charges even though he could not graduate from oxford because of his politics he remained there for several years stirling published his first work which extended newton work on plane curves in he traveled to venice where a chair of mathematics had been promised to him an appointment that unfortunately fell through nevertheless stirling stayed in venice continuing his mathematical work he attended the university of padua in and in he returned to glasgow stirling apparently fled italy after learning the secrets of the italian glass industry avoiding the efforts of italian glass makers to assassinate him to protect their secrets in late stirling moved to london staying there years teaching mathematics and actively engaging in research in he published methodus differentialis his most important work presenting results on infinite series summations interpolation and quadrature it is in this book that his asymptotic formula for n appears stirling also worked on gravitation and the shape of the earth he stated but did not prove that the earth is an oblate spheroid stirling returned to scotland in when he was appointed manager of a scottish mining company he was very successful in this role and even published a paper on the ventilation of mine shafts he continued his mathematical research but at a reduced pace during his years in the mining industry stirling is also noted for surveying the river clyde with the goal of creating a series of locks to make it navigable in the citizens of glasgow presented him with a silver teakettle as a reward for this work partial functions a program designed to evaluate a function may not produce the correct value of the function for all elements in the domain of this function for example a program may not produce a correct value because evaluating the function may lead to an infinite loop or an overflow similarly in abstract mathematics we often want to discuss functions that are defined only for a subset of the real numbers such as x x and arcsin x also we may want to use such notions as the youngest child function which is undefined for a couple having no children or the time of sunrise which is undefined for some days above the arctic circle to study such situations we use the concept of a partial function definition remark we write f a b to denote that f is a partial function from a to b note that this is the same notation as is used for functions the context in which the notation is used determines whether f is a partial function or a total function example the function f z r where f n n is a partial function from z to r where the domain of definition is the set of nonnegative integers note that f is undefined for negative integers exercises why is f not a function from r to r if a f x x b f x x c f x determine whether f is a function from z to r if a f n n b f n c f n determine whether f is a function from the set of all bit strings to the set of integers if a f s is the position of a bit in s b f s is the number of bits in s c f s is the smallest integer i such that the ith bit of s is and f s when s is the empty string the string with no bits find the domain and range of these functions note that in each case to find the domain determine the set of elements assigned values by the function a the function that assigns to each nonnegative integer its last digit b the function that assigns the next largest integer to a positive integer c the function that assigns to a bit string the number of one bits in the string d the function that assigns to a bit string the number of bits in the string find the domain and range of these functions note that in each case to find the domain determine the set of elements assigned values by the function a the function that assigns to each bit string the number of ones in the string minus the number of zeros in the string b the function that assigns to each bit string twice the number of zeros in that string c the function that assigns the number of bits left over when a bit string is split into bytes which are blocks of bits d the function that assigns to each positive integer the largest perfect square not exceeding this integer find the domain and range of these functions a the function that assigns to each pair of positive inte gers the first integer of the pair b the function that assigns to each positive integer its largest decimal digit c the function that assigns to a bit string the number of ones minus the number of zeros in the string d the function that assigns to each positive integer the largest integer not exceeding the square root of the integer e the function that assigns to a bit string the longest string of ones in the string find the domain and range of these functions a the function that assigns to each pair of positive inte gers the maximum of these two integers b the function that assigns to each positive integer the number of the digits that do not appear as decimal digits of the integer c the function that assigns to a bit string the number of times the block appears d the function that assigns to a bit string the numerical position of the first in the string and that assigns the value to a bit string consisting of all find these values a b c d e f g j h j find these values a b j c d j a office b assigned bus to chaperone in a group of buses taking students on a field trip c salary d social security number specify a codomain for each of the functions in exercise under what conditions is each of these functions with the codomain you specified onto specify a codomain for each of the functions in exercise under what conditions is each of the functions with the codomain you specified onto give an example of a function from n to n that is a one to one but not onto b onto but not one to one c both onto and one to one but different from the iden tity function d neither one to one nor onto give an explicit formula for a function from the set of e f integers to the set of positive integers that is a one to one but not onto g j h jj determine whether each of these functions from a b c d to itself is one to one a f a b f b a f c c f d d b f a b f b b f c d f d c c f a d f b b f c c f d d which functions in exercise are onto determine whether each of these functions from z to z is one to one a f n n b f n c f n d f n which functions in exercise are onto determine whether f z z z is onto if a f m n n b f m n c f m n m n d f m n m n e f m n determine whether the function f z z z is onto a f m n n b onto but not one to one c one to one and onto d neither one to one nor onto determine whether each of these functions is a bijection from r to r a f x b f x c f x x x d f x determine whether each of these functions is a bijection from r to r a f x b f x c f x d f x let f r r and let f x for all x r show that f x is strictly increasing if and only if the func tion g x f x is strictly decreasing let f r r and let f x for all x r show that f x is strictly decreasing if and only if the func b f m n m c f m n m d f m n n n tion g x f x is strictly increasing a prove that a strictly increasing function from r to it self is one to one e f m n m n consider these functions from the set of students in a discrete mathematics class under what conditions is the function one to one if it assigns to a student his or her a mobile phone number b student identification number c final grade in the class d home town consider these functions from the set of teachers in a school under what conditions is the function one to one if it assigns to a teacher his or her b give an example of an increasing function from r to itself that is not one to one a prove that a strictly decreasing function from r to itself is one to one b give an example of a decreasing function from r to itself that is not one to one show that the function f x ex from the set of real numbers to the set of real numbers is not invertible but if the codomain is restricted to the set of positive real numbers the resulting function is invertible show that the function f x x from the set of real numbers to the set of nonnegative real numbers is not invertible but if the domain is restricted to the set of non negative real numbers the resulting function is invertible let s find f s if a f x b f x c f x d f x let f x find f s if a s b s c s d s let f x where the domain is the set of real num bers what is a f z b f n c f r suppose that g is a function from a to b and f is a inverse of the invertible function f notice also that f s the inverse image of the set s makes sense for all functions f not just invertible functions let f be the function from r to r defined by f x find a f b f x x c f x x let g x xj find a g b g c g x x let f be a function from a to b let s and t be subsets of b show that a f s t f s f t b f s t f s f t let f be a function from a to b let s be a subset of b show that f s f s show that x j is the closest integer to the number x function from b to c except when x is midway between two integers when it a show that if both f and g are one to one functions then f g is also one to one is the larger of these two integers show that is the closest integer to the number x b show that if both f and g are onto functions then except when x is midway between two integers when it f g is also onto if f and f g are one to one does it follow that g is one to one justify your answer if f and f g are onto does it follow that g is onto justify your answer find f g and g f where f x and g x x are functions from r to r find f g and fg for the functions f and g given in exercise let f x ax b and g x cx d where a b c and d are constants determine necessary and suffi cient conditions on the constants a b c and d so that f g g f show that the function f x ax b from r to r is invertible where a and b are constants with a and find the inverse of f let f be a function from the set a to the set b let s and t be subsets of a show that a f s t f s f t b f s t f s f t a give an example to show that the inclusion in part b in exercise may be proper b show that if f is one to one the inclusion in part b in exercise is an equality let f be a function from the set a to the set b let s be a subset of b we define the inverse image of s to be the subset of a whose elements are precisely all pre images of all ele ments of s we denote the inverse image of s by f s so f s a a f a s beware the notation f is used in two different ways do not confuse the notation intro duced here with the notation f y for the value at y of the is the smaller of these two integers show that if x is a real number then xj if x is not an integer and xj if x is an integer show that if x is a real number then x xj x x show that if x is a real number and m is an integer then m m show that if x is a real number and n is an integer then a x n if and only if xj n b n x if and only if n show that if x is a real number and n is an integer then a x n if and only if n b n x if and only if n xj prove that if n is an integer then n n if n is even and n if n is odd prove that if x is a real number then xj and x xj the function int is found on some calculators where int x xj when x is a nonnegative real number and int x when x is a negative real number show that this int function satisfies the identity int x int x let a and b be real numbers with a b use the floor and or ceiling functions to express the number of inte gers n that satisfy the inequality a n b let a and b be real numbers with a b use the floor and or ceiling functions to express the number of inte gers n that satisfy the inequality a n b how many bytes are required to encode n bits of data where n equals a b c d how many bytes are required to encode n bits of data where n equals a b c d how many atm cells described in example can be transmitted in seconds over a link operating at the fol lowing rates a kilobits per second kilobit bits b kilobits per second c megabit per second megabit bits data are transmitted over a particular ethernet network suppose that f is a function from a to b where a and b are finite sets with a b show that f is one to one if and only if it is onto prove or disprove each of these statements about the floor and ceiling functions a x x for all real numbers x b x whenever x is a real number c x y x y or whenever x and y are real numbers d for all real numbers x and y data over this ethernet network note that a byte is a synonym for an octet a kilobyte is bytes and a megabyte is bytes a kilobytes of data b kilobytes of data c megabytes of data d megabytes of data draw the graph of the function f n from z to z draw the graph of the function f x from r to r draw the graph of the function f x x from r to r draw the graph of the function f x xj x draw the graph of the function f x x draw graphs of each of these functions a f x x j b f x c f x d f x x prove or disprove each of these statements about the floor and ceiling functions a j for all real numbers x b x yj xj yj for all real numbers x and y c for all real numbers x d j x j for all positive real numbers x e xj yj x yj for all real prove that if x is a positive real number then a xj j x j let x be a real number show that xj x j x j for each of these partial functions determine its domain codomain domain of definition and the set of values for which it is undefined also determine whether it is a total function a f z r f n n b f z z f n c f z z q f m n m n e f x x d f z z z f m n mn f f x g f x x j draw graphs of each of these functions a f x b f x c f x xj d f x e f z z z f m n m n if m n a show that a partial function from a to b can be viewed as a function f from a to b u where u is not an element of b and e f x x g f x j f f x x f a if a belongs to the domain find the inverse function of f x f a of definition of f suppose that f is an invertible function from y to z and g is an invertible function from x to y show that the inverse of the composition f g is given by f g g f let s be a subset of a universal set u the characteristic function fs of s is the function from u to the set such that fs x if x belongs to s and fs x if x does not belong to s let a and b be sets show that for all x u a fa b x fa x fb x b fa b x fa x fb x fa x fb x c fa x fa x d fa b x fa x fb x x fb x u if f is undefined at a b using the construction in a find the function f corresponding to each partial function in exercise a show that if a set s has cardinality m where m is a positive integer then there is a one to one correspon dence between s and the set m b show that if s and t are two sets each with m ele ments where m is a positive integer then there is a one to one correspondence between s and t show that a set s is infinite if and only if there is a proper subset a of s such that there is a one to one correspon dence between a and s sequences and summations introduction sequences are ordered lists of elements used in discrete mathematics in many ways for ex ample they can be used to represent solutions to certain counting problems as we will see in chapter they are also an important data structure in computer science we will often need to work with sums of terms of sequences in our study of discrete mathematics this section reviews the use of summation notation basic properties of summations and formulas for the sums of terms of some particular types of sequences the terms of a sequence can be specified by providing a formula for each term of the sequence in this section we describe another way to specify the terms of a sequence using a recurrence relation which expresses each term as a combination of the previous terms we will introduce one method known as iteration for finding a closed formula for the terms of a sequence specified via a recurrence relation identifying a sequence when the first few terms are provided is a useful skill when solving problems in discrete mathematics we will provide some tips including a useful tool on the web for doing so sequences a sequence is a discrete structure used to represent an ordered list for example is a sequence with five terms and is an infinite sequence definition we use the notation an to describe the sequence note that an represents an individual term of the sequence an be aware that the notation an for a sequence conflicts with the notation for a set however the context in which we use this notation will always make it clear when we are dealing with sets and when we are dealing with sequences moreover although we have used the letter a in the notation for a sequence other letters or expressions may be used depending on the sequence under consideration that is the choice of the letter a is arbitrary we describe sequences by listing the terms of the sequence in order of increasing subscripts example consider the sequence an where an n the list of the terms of this sequence beginning with namely starts with definition remark a geometric progression is a discrete analogue of the exponential function f x arx example the sequences bn with bn n cn with cn and dn with dn n are geometric progressions with initial term and common ratio equal to and and and and respectively if we start at n the list of terms begins with the list of terms begins with and the list of terms begins with definition remark an arithmetic progression is a discrete analogue of the linear function f x dx a example the sequences sn with sn and tn with tn are both arithmetic progres sions with initial terms and common differences equal to and and and respectively if we start at n the list of terms begins with and the list of terms begins with sequences of the form an are often used in computer science these finite sequences are also called strings this string is also denoted by an recall that bit strings which are finite sequences of bits were introduced in section the length of a string is the number of terms in this string the empty string denoted by λ is the string that has no terms the empty string has length zero example the string abcd is a string of length four recurrence relations in examples we specified sequences by providing explicit formulas for their terms there are many other ways to specify a sequence for example another way to specify a sequence is definition to provide one or more initial terms together with a rule for determining subsequent terms from those that precede them example let an be a sequence that satisfies the recurrence relation an an for n and suppose that what are and solution we see from the recurrence relation that it then follows that and example let an be a sequence that satisfies the recurrence relation an an an for n and suppose that and what are and solution we see from the recurrence relation that and we can find and each successive term in a similar way the initial conditions for a recursively defined sequence specify the terms that precede the first term where the recurrence relation takes effect for instance the initial condition in example is and the initial conditions in example are and using mathematical induction a proof technique introduced in chapter it can be shown that a recurrence relation hop along to chapter to learn how to find a formula for the fibonacci numbers definition together with its initial conditions determines a unique solution next we define a particularly useful sequence defined by a recurrence relation known as the fibonacci sequence after the italian mathematician fibonacci who was born in the century see chapter for his biography we will study this sequence in depth in chapters and where we will see why it is important for many applications including modeling the population growth of rabbits example find the fibonacci numbers and solution the recurrence relation for the fibonacci sequence tells us that we find successive terms by adding the previous two terms because the initial conditions tell us that and using the recurrence relation in the definition we find that f5 example suppose that an is the sequence of integers defined by an n the value of the factorial function at the integer n where n because n n n n n n nan we see that the sequence of factorials satisfies the recurrence relation an nan together with the initial condition we say that we have solved the recurrence relation together with the initial conditions when we find an explicit formula called a closed formula for the terms of the sequence example determine whether the sequence an where an for every nonnegative integer n is a solution of the recurrence relation an an for n answer the same question where an and where an solution suppose that an for every nonnegative integer n then for n we see that an n n an therefore an where an is a so lution of the recurrence relation suppose that an for every nonnegative integer n note that and because we see that an where an is not a solution of the recurrence relation suppose that an for every nonnegative integer n then for n we see that an an an therefore an where an is a solution of the recur rence relation many methods have been developed for solving recurrence relations here we will introduce a straightforward method known as iteration via several examples in chapter we will study recurrence relations in depth in that chapter we will show how recurrence relations can be used to solve counting problems and we will introduce several powerful methods that can be used to solve many different recurrence relations example solve the recurrence relation and initial condition in example solution we can successively apply the recurrence relation in example starting with the initial condition and working upward until we reach an to deduce a closed formula for the sequence we see that an an n n we can also successively apply the recurrence relation in example starting with the term an and working downward until we reach the initial condition to deduce this same formula the steps are an an an an an an n n n at each iteration of the recurrence relation we obtain the next term in the sequence by adding to the previous term we obtain the nth term after n iterations of the recurrence relation hence we have added n to the initial term to obtain an this gives us the closed formula an n note that this sequence is an arithmetic progression the technique used in example is called iteration we have iterated or repeatedly used the recurrence relation the first approach is called forward substitution we found successive terms beginning with the initial condition and ending with an the second approach is called backward substitution because we began with an and iterated to express it in terms of falling terms of the sequence until we found it in terms of note that when we use iteration we essential guess a formula for the terms of the sequence to prove that our guess is correct we need to use mathematical induction a technique we discuss in chapter in chapter we will show that recurrence relations can be used to model a wide variety of problems we provide one such example here showing how to use a recurrence relation to find compound interest example compound interest suppose that a person deposits in a savings account at a bank yielding per year with interest compounded annually how much will be in the account after years solution to solve this problem let pn denote the amount in the account after n years because the amount in the account after n years equals the amount in the account after n years plus interest for the nth year we see that the sequence pn satisfies the recurrence relation pn pn pn the initial condition is we can use an iterative approach to find a formula for pn note that p3 pn pn when we insert the initial condition the formula pn is obtained inserting n into the formula pn shows that after years the account contains special integer sequences a common problem in discrete mathematics is finding a closed formula a recurrence relation or some other type of general rule for constructing the terms of a sequence sometimes only a few terms of a sequence solving a problem are known the goal is to identify the sequence even though the initial terms of a sequence do not determine the entire sequence after all there are infinitely many different sequences that start with any finite set of initial terms knowing the first few terms may help you make an educated conjecture about the identity of your sequence once you have made this conjecture you can try to verify that you have the correct sequence when trying to deduce a possible formula recurrence relation or some other type of rule for the terms of a sequence when given the initial terms try to find a pattern in these terms you might also see whether you can determine how a term might have been produced from those preceding it there are many questions you could ask but some of the more useful are are there runs of the same value that is does the same value occur many times in a row are terms obtained from previous terms by adding the same amount or an amount that depends on the position in the sequence are terms obtained from previous terms by multiplying by a particular amount are terms obtained by combining previous terms in a certain way are there cycles among the terms example find formulae for the sequences with the following first five terms a b c solution a we recognize that the denominators are powers of the sequence with an n is a possible match this proposed sequence is a geometric progression with a and r b we note that each term is obtained by adding to the previous term the sequence with an n is a possible match this proposed sequence is an arithmetic progression with a and d n c the terms alternate between and the sequence with an n is a possible match this proposed sequence is a geometric progression with a and r examples illustrate how we can analyze sequences to find how the terms are con structed example how can we produce the terms of a sequence if the first terms are solution in this sequence the integer appears once the integer appears twice the integer appears three times and the integer appears four times a reasonable rule for generating this sequence is that the integer n appears exactly n times so the next five terms of the sequence would all be the following six terms would all be and so on the sequence generated this way is a possible match example how can we produce the terms of a sequence if the first terms are solution note that each of the first terms of this sequence after the first is obtained by adding to the previous term we could see this by noticing that the difference between consecutive terms is consequently the nth term could be produced by starting with and adding a total of n times that is a reasonable guess is that the nth term is n this is an arithmetic progression with a and d example how can we produce the terms of a sequence if the first terms are solution observe that each successive term of this sequence starting with the third term is the sum of the two previous terms that is and so on consequently if ln is the nth term of this sequence we guess that the sequence is determined by the recurrence relation ln ln ln with initial conditions and the table some useful sequences nth term first terms 2401 19683 n fn 40320 same recurrence relation as the fibonacci sequence but with different initial conditions this sequence is known as the lucas sequence after the french mathematician françois édouard lucas lucas studied this sequence and the fibonacci sequence in the nineteenth century another useful technique for finding a rule for generating the terms of a sequence is to compare the terms of a sequence of interest with the terms of a well known integer sequence such as terms of an arithmetic progression terms of a geometric progression perfect squares perfect cubes and so on the first terms of some sequences you may want to keep in mind are displayed in table example conjecture a simple formula for an if the first terms of the sequence an are 6559 check out the puzzles at solution to attack this problem we begin by looking at the difference of consecutive terms but we do not see a pattern when we form the ratio of consecutive terms to see whether each term is a multiple of the previous term we find that this ratio although not a constant is close to so it is reasonable to suspect that the terms of this sequence are generated by a formula involving comparing these terms with the corresponding terms of the sequence we notice that the nth term is less than the corresponding power of we see that an for n and conjecture that this formula holds for all n we will see throughout this text that integer sequences appear in a wide range of contexts in discrete mathematics sequences we have encountered or will encounter include the sequence of prime numbers chapter the number of ways to order n discrete objects chapter the number of moves required to solve the famous tower of hanoi puzzle with n disks chapter and the number of rabbits on an island after n months chapter the oeis site integer sequences appear in an amazingly wide range of subject areas besides discrete mathematics including biology engineering chemistry and physics as well as in puzzles an amazing database of over different integer sequences can be found in the on line encyclopedia of integer sequences oeis this database was originated by neil sloane in the the last printed version of this database was published in the current encyclopedia would occupy more than volumes of the size of the book with more than new submissions a year there is also a program accessible via the web that you can use to find sequences from the encyclopedia that match initial terms you provide summations next we consider the addition of the terms of a sequence for this we introduce summation notation we begin by describing the notation used to express the sum of the terms am am an from the sequence an we use the notation n aj n aj or m j n aj j m read as the sum from j m to j n of aj to represent am am an here the variable j is called the index of summation and the choice of the letter j as the variable is arbitrary that is we could have used any other letter such as i or k or in notation n n n aj ai ak here the index of summation runs through all integers starting with its lower limit m and ending with its upper limit n a large uppercase greek letter sigma is used to denote summation the usual laws for arithmetic apply to summations for example when a and b are real numbers we have n axj byj a n xj b n yj where xn and proof can be constructed using mathematical induction a proof method we introduce in chap ter the proof also uses the commutative and associative laws for addition and the distributive law of multiplication over addition we give some examples of summation notation example use summation notation to express the sum of the first terms of the sequence aj where aj j for j solution the lower limit for the index of summation is and the upper limit is we write this sum as j j neil sloane born neil sloane studied mathematics and electrical engineering at the uni versity of melbourne on a scholarship from the australian state telephone company he mastered many telephone related jobs such as erecting telephone poles in his summer work after graduating he designed minimal cost telephone networks in australia in he came to the united states and studied electri cal engineering at cornell university his ph d thesis was on what are now called neural networks he took a job at bell labs in working in many areas including network design coding theory and sphere packing he now works for at t labs moving there from bell labs when at t split up in one of his favorite problems is the kissing problem a name he coined which asks how many spheres can be arranged in n dimensions so that they all touch a central sphere of the same size in two dimensions the answer is because pennies can be placed so that they touch a central penny in three dimensions billiard balls can be placed so that they touch a central billiard ball two billiard balls that just touch are said to kiss giving rise to the terminology kissing problem and kissing number sloane together with andrew odlyzko showed that in and dimensions the optimal kissing numbers are respectively and 560 the kissing number is known in dimensions and but not in any other dimensions sloane books include sphere packings lattices and groups ed with john conway the theory of error correcting codes with jessie macwilliams the encyclopedia of integer sequences with simon plouffe which has grown into the famous oeis website and the rock climbing guide to new jersey crags with paul nick the last book demonstrates his interest in rock climbing it includes more than climbing sites in new jersey example what is the value of j solution we have j j example what is the value of k solution we have k k sometimes it is useful to shift the index of summation in a sum this is often done when two sums need to be added but their indices of summation do not match when shifting an index of summation it is important to make the appropriate changes in the corresponding summand this is illustrated by example example suppose we have the sum j j but want the index of summation to run between and rather than from to to do this we let k j then the new summation index runs from because k when j to because k when j and the term j becomes k hence j k it is easily checked that both sums are sums of terms of geometric progressions commonly arise such sums are called geometric series theorem gives us a formula for the sum of terms of a geometric progression theorem proof let n sn arj j to compute s first multiply both sides of the equality by r and then manipulate the resulting sum as follows n rsn r arj substituting summation formula for s j n arj by the distributive property j n ar shifting the index of summation with k j k n k ark arn a removing k n term and adding k term sn arn a substituting s for summation formula from these equalities we see that rsn sn arn a solving for sn shows that if r then arn a sn r if r then the sn n arj n a n a example double summations arise in many contexts as in the analysis of nested loops in computer programs an example of a double summation is ij i j to evaluate the double sum first expand the inner summation and then continue by computing the outer summation ij i i j i i we can also use summation notation to add all values of a function or terms of an indexed set where the index of summation runs over all values in a set that is we write f s to represent the sum of the values f for all members of s table some useful summation formulae sum closed form n ark r k n k k n k n k xk x k kxk x k arn a r r n n n n n x x example what is the value of solution because represents the sum of the values of for all the members of the set it follows that certain sums arise repeatedly throughout discrete mathematics having a collection of formulae for such sums can be useful table provides a small table of formulae for commonly occurring sums we derived the first formula in this table in theorem the next three formulae give us the sum of the first n positive integers the sum of their squares and the sum of their cubes these three formulae can be derived in many different ways for example see exercises and also note that each of these formulae once known can easily be proved using mathematical induction the subject of section the last two formulae in the table involve infinite series and will be discussed shortly example illustrates how the formulae in table can be useful example find solution first note that because k k we have using the formula n n n from table and proved in exercise 925 some infinite series although most of the summations in this book are finite sums infinite series are important in some parts of discrete mathematics infinite series are usually studied in a course in calculus and even the definition of these series requires the use of calculus but sometimes they arise in discrete mathematics because discrete mathematics deals with infi nite collections of discrete elements in particular in our future studies in discrete mathematics we will find the closed forms for the infinite series in examples and to be quite useful example requires calculus let x be a real number with x find n xn k solution by theorem with a and r x we see that k x xk approaches as k approaches infinity it follows that xn x because x xn n lim k xk x x x we can produce new summation formulae by differentiating or integrating existing formulae example requires calculus differentiating both sides of the equation xk from example we find that kx x exercises k this differentiation is valid for x by a theorem about infinite series find these terms of the sequence an where an n a b c d what is the term of the sequence an if an equals a b c n d n what are the terms and of the sequence an where an equals a b n n c n d n what are the terms and of the sequence an where an equals a n b c d n list the first terms of each of these sequences a the sequence that begins with and in which each successive term is more than the preceding term b the sequence that lists each positive integer three times in increasing order c the sequence that lists the odd positive integers in in creasing order listing each odd integer twice d the sequence whose nth term is n e the sequence that begins with where each succeed ing term is twice the preceding term f the sequence whose first term is second term is and each succeeding term is the sum of the two pre ceding terms g the sequence whose nth term is the number of bits in the binary expansion of the number n defined in section h the sequence where the nth term is the number of let ters in the english word for the index n list the first terms of each of these sequences a the sequence obtained by starting with and obtain ing each term by subtracting from the previous term b the sequence whose nth term is the sum of the first n positive integers c the sequence whose nth term is d the sequence whose nth term is nj e the sequence whose first two terms are and and each succeeding term is the sum of the two previous terms f the sequence whose nth term is the largest integer whose binary expansion defined in section has n bits write your answer in decimal notation g the sequence whose terms are constructed sequen tially as follows start with then add then multiply by then add then multiply by and so on h the sequence whose nth term is the largest integer k such that k n find at least three different sequences beginning with the terms whose terms are generated by a simple for mula or rule find at least three different sequences beginning with the terms whose terms are generated by a simple for mula or rule find the first five terms of the sequence defined by each of these recurrence relations and initial conditions a an c an n n d an n find the solution to each of these recurrence relations with the given initial conditions use an iterative approach such as that used in example a an an b an an c an an n d an e an n an f an g an an n find the solution to each of these recurrence relations and initial conditions use an iterative approach such as that used in example a an b an an b an c a a n c a an a a d n n n d an an an nan e a a e an an an n n a a find the first six terms of the sequence defined by each of these recurrence relations and initial conditions a an an an an an d an nan n n an nan h an a person deposits in an account that yields interest compounded annually a set up a recurrence relation for the amount in the ac count at the end of n years e an an n an n b find an explicit formula for the amount in the account let an for n a find and b show that 6a1 and show that an for all integers n with n show that the sequence an is a solution of the recurrence relation an if at the end of n years c how much money will the account contain after years suppose that the number of bacteria in a colony triples every hour a set up a recurrence relation for the number of bacteria after n hours have elapsed b if bacteria are used to begin a new colony how a an n b an n many bacteria will be in the colony in hours c an d an is the sequence an a solution of the recurrence relation an if a an b an c an d an e an f an g an n h an for each of these sequences find a recurrence relation satisfied by this sequence the answers are not unique because there are infinitely many different recurrence relations satisfied by any sequence a an b an c an d an e an f an n g an n n h an n show that the sequence an is a solution of the recurrence relation an an if a an n b an n n assume that the population of the world in was billion and is growing at the rate of a year a set up a recurrence relation for the population of the world n years after b find an explicit formula for the population of the world n years after c what will the population of the world be in a factory makes custom sports cars at an increasing rate in the first month only one car is made in the second month two cars are made and so on with n cars made in the nth month a set up a recurrence relation for the number of cars produced in the first n months by this factory b how many cars are produced in the first year c find an explicit formula for the number of cars pro duced in the first n months by this factory an employee joined a company in with a starting salary of every year this employee receives a raise of plus of the salary of the previous year a set up a recurrence relation for the salary of this em ployee n years after what are the values of these sums where s a j b j c find an explicit formula for the salary of this em c j d find a recurrence relation for the balance b k owed at the end of k months on a loan of at a rate of what is the value of each of these sums of terms of a geometric progression if a payment of is made each month hint ex press b k in terms of b k the monthly interest is b k a find a recurrence relation for the balance b k owed at the end of k months on a loan at a rate of r if a payment a b j c j d j j j j p is made on the loan each month hint express find the value of each of these sums b k in terms of b k and note that the monthly interest rate is r b determine what the monthly payment p should be so that the loan is paid off after t months for each of these lists of integers provide a simple for a j b j c d j j j mula or rule that generates the terms of an integer se quence that begins with the given list assuming that your compute each of these double sums a i j b of the sequence a c i d ij c d compute each of these double sums a i j b f g c j d h 40321 i j show that n i j aj aj an where quence that begins with the given list assuming that your formula or rule is correct determine the next three terms of sum is called telescoping use the identity k k n k k and a b c sum both sides of the identity k from k to k n and use exercise to find a a formula for n the sum of the first n d odd natural numbers e 2186 f 2027025 b a formula for n k result of exercise to derive the formula for n g h show that if an denotes the nth po sitive integer that is not given in table hint take ak sum in exercise find use table k in the telescoping a perfect square then an n n where x denotes k find use table the integer closest to the real number x k a k b j there is also a special notation for products the product of n k c i d j j am am an is represented by uct from j m to j n of aj j tt m aj read as the prod what are the values of the following products express n using product notation a i b i c i d find j recall that the value of the factorial function at a positive in teger n denoted by n is the product of the positive integers from to n inclusive also we specify that find j cardinality of sets introduction in definition of section we defined the cardinality of a finite set as the number of elements in the set we use the cardinalities of finite sets to tell us when they have the same size or when one is bigger than the other in this section we extend this notion to infinite sets that is we will define what it means for two infinite sets to have the same cardinality providing us with a way to measure the relative sizes of infinite sets we will be particularly interested in countably infinite sets which are sets with the same cardinality as the set of positive integers we will establish the surprising result that the set of rational numbers is countably infinite we will also provide an example of an uncountable set when we show that the set of real numbers is not countable the concepts developed in this section have important applications to computer science a function is called uncomputable if no computer program can be written to find all its values even with unlimited time and memory we will use the concepts in this section to explain why uncomputable functions exist we now define what it means for two sets to have the same size or cardinality in section we discussed the cardinality of finite sets and we defined the size or cardinality of such sets in exercise of section we showed that there is a one to one correspondence between any two finite sets with the same number of elements we use this observation to extend the concept of cardinality to all sets both finite and infinite definition for infinite sets the definition of cardinality provides a relative measure of the sizes of two sets rather than a measure of the size of one particular set we can also define what it means for one set to have a smaller cardinality than another set definition countable sets we will now split infinite sets into two groups those with the same cardinality as the set of natural numbers and those with a different cardinality figure a one to one correspondence between z and the set of odd positive integers definition we illustrate how to show a set is countable in the next example example show that the set of odd positive integers is a countable set solution to show that the set of odd positive integers is countable we will exhibit a one to one correspondence between this set and the set of positive integers consider the function f n from z to the set of odd positive integers we show that f is a one to one correspondence by showing that it is both one to one and onto to see that it is one to one suppose that f n f m then so n m to see that it is onto suppose that t is an odd positive integer then t is less than an even integer where k is a natural number hence t f k we display this one to one correspondence in figure an infinite set is countable if and only if it is possible to list the elements of the set in a sequence indexed by the positive integers the reason for this is that a one to one correspon dence f from the set of positive integers to a set s can be expressed in terms of a sequence you can always get a room at hilbert grand hotel an where f f an f n hilbert s grand hotel we now describe a paradox that shows that something impos sible with finite sets may be possible with infinite sets the famous mathematician david hilbert invented the notion of the grand hotel which has a countably infinite number of rooms each occupied by a guest when a new guest arrives at a hotel with a finite number of rooms and all rooms are occupied this guest cannot be accommodated without evicting a current guest however we can always accommodate a new guest at the grand hotel even when all rooms are already occupied as we show in example exercises and ask you to show that we can accommodate a finite number of new guests and a countable number of new guests respectively at the fully occupied grand hotel david hilbert hilbert born in königsberg the city famous in mathematics for its seven bridges was the son of a judge during his tenure at göttingen university from to he made many fundamental contributions to a wide range of mathematical subjects he almost always worked on one area of mathematics at a time making important contributions then moving to a new mathematical subject some areas in which hilbert worked are the calculus of variations geometry algebra number theory logic and mathematical physics besides his many outstanding original contributions hilbert is remembered for his famous list of difficult problems he described these problems at the international congress of mathematicians as a challenge to mathematicians at the birth of the twentieth century since that time they have spurred a tremendous amount and variety of research although many of these problems have now been solved several remain open including the riemann hypothesis which is part of problem on hilbert list hilbert was also the author of several important textbooks in number theory and geometry manager newguest figure a new guest arrives at hilbert grand hotel example how can we accommodate a new guest arriving at the fully occupied grand hotel without removing any of the current guests solution because the rooms of the grand hotel are countable we can list them as room room room and so on when a new guest arrives we move the guest in room to room the guest in room to room and in general the guest in room n to room n for all positive integers n this frees up room which we assign to the new guest and all the current guests still have rooms we illustrate this situation in figure when there are finitely many room in a hotel the notion that all rooms are occupied is equivalent to the notion that no new guests can be accommodated however hilbert paradox of the grand hotel can be explained by noting that this equivalence no longer holds when there are infinitely many room examples of countable and uncountable sets we will now show that cer tain sets of numbers are countable we begin with the set of all integers note that we can show that the set of all integers is countable by listing its members example show that the set of all integers is countable solution we can list all integers in a sequence by starting with and alternating between positive and negative integers alternatively we could find a one to one correspondence between the set of positive integers and the set of all integers we leave it to the reader to show that the function f n n when n is even and f n n when n is odd is such a function consequently the set of all integers is countable it is not surprising that the set of odd integers and the set of all integers are both countable sets as shown in examples and many people are amazed to learn that the set of rational numbers is countable as example demonstrates example show that the set of positive rational numbers is countable solution it may seem surprising that the set of positive rational numbers is countable but we will show how we can list the positive rational numbers as a sequence rn first note that every positive rational number is the quotient p q of two positive integers we can terms not circled are not listed because they repeat previously listed terms figure the positive rational numbers are countable arrange the positive rational numbers by listing those with denominator q in the first row those with denominator q in the second row and so on as displayed in figure the key to listing the rational numbers in a sequence is to first list the positive rational numbers p q with p q followed by those with p q followed by those with p q and so on following the path shown in figure whenever we encounter a number p q that is already listed we do not list it again for example when we come to we do not list it because we have already listed the initial terms in the list of positive rational numbers we have constructed are and so on these numbers are shown circled the uncircled numbers in the list are those we leave out because they are already listed because all positive rational numbers are listed once as the reader can verify we have shown that the set of positive rational numbers is countable not all infinite sets have the same size an uncountable set we have seen that the set of positive rational numbers is a countable set do we have a promising candidate for an uncountable set the first place we might look is the set of real numbers in example we use an important proof method introduced in by georg cantor and known as the cantor diagonalization argument to prove that the set of real numbers is not countable this proof method is used extensively in mathematical logic and in the theory of computation example show that the set of real numbers is an uncountable set solution to show that the set of real numbers is uncountable we suppose that the set of real numbers is countable and arrive at a contradiction then the subset of all real numbers that fall between and would also be countable because any subset of a countable set is also countable see exercise under this assumption the real numbers between and can be listed in some order say let the decimal representation of these real numbers be r4 where dij for example if we have and so on then form a new real number with decimal expansion r where the decimal digits are determined by the following rule d if dii if dii a number with a decimal expansion that terminates has a second decimal expansion ending with an infinite sequence of because as an example suppose that r4 and so on then we have r 4544 where because because because because and so on every real number has a unique decimal expansion when the possibility that the expansion has a tail end that consists entirely of the digit is excluded therefore the real number r is not equal to any of because the decimal expansion of r differs from the decimal expansion of ri in the ith place to the right of the decimal point for each i because there is a real number r between and that is not in the list the assumption that all the real numbers between and could be listed must be false therefore all the real numbers between and cannot be listed so the set of real numbers between and is uncountable any set with an uncountable subset is uncountable see exercise hence the set of real numbers is uncountable results about cardinality we will now discuss some results about the cardinality of sets first we will prove that the union of two countable sets is also countable theorem this proof uses wlog proof suppose that a and b are both countable sets without loss of generality we can assume that a and b are disjoint if they are not we can replace b by b a because a b a and a b a a b furthermore without loss of generality if one of the two sets is and cases countably infinite and other finite we can assume that b is the one that is finite there are three cases to consider i a and b are both finite ii a is infinite and b is finite and iii a and b are both countably infinite case i note that when a and b are finite a b is also finite and therefore countable case ii because a is countably infinite its elements can be listed in an infinite sequence an and because b is finite its terms can be listed as bm for some positive integer m we can list the elements of a b as bm an this means that a b is countably infinite case iii because both a and b are countably infinite we can list their elements as an and bn respectively by alternating terms of these two sequences we can list the elements of a b in the infinite sequence an bn this means a b must be countably infinite we have completed the proof as we have shown that a b is countable in all three cases because of its importance we now state a key theorem in the study of cardinality theorem because theorem seems to be quite straightforward we might expect that it has an easy proof however even though it can be proved without using advanced mathematics no known proof is easy to explain consequently we omit a proof here we refer the interested reader to and for a proof this result is called the schröder bernstein theorem after ernst schröder who published a flawed proof of it in and felix bernstein a student of georg cantor who presented a proof in however a proof of this theorem was found in notes of richard dedekind dated dedekind was a german mathematician who made important contributions to the foundations of mathematics abstract algebra and number theory we illustrate the use of theorem with an example example show that the solution it is not at all obvious how to find a one to one correspondence between and to show that fortunately we can use the schröder bernstein theorem instead finding a one to one function from to is simple because f x x is a one to one function from to finding a one to one function from to is also not difficult the function g x x is clearly one to one and maps to as we have found one to one functions from to and from to the schröder bernstein theorem tells us that uncomputable functions we will now describe an important application of the concepts of this section to computer science in particular we will show that there are functions whose values cannot be computed by any computer program definition to show that there are uncomputable functions we need to establish two results first we need to show that the set of all computer programs in any particular programming language is countable this can be proved by noting that a computer programs in a particular language can be thought of as a string of characters from a finite alphabet see exercise next we show that there are uncountably many different functions from a particular countably infinite set to itself in particular exercise shows that the set of functions from the set of positive integers to itself is uncountable this is a consequence of the uncountability of the real numbers between and see example putting these two results together exercise shows that there are uncomputable functions c is the lowercase the continuum hypothesis we conclude this section with a brief discussion of a famous open question about cardinality it can be shown that the power set of z and the set of real numbers r have the same cardinality see exercise in other words we know that z r c where c denotes the cardinality of the set of real numbers an important theorem of cantor exercise states that the cardinality of a set is always less than the cardinality of its power set hence z z we can rewrite this as using the notation s to denote the cardinality of the power set of the set s also note that the relationship z r can be expressed as c this leads us to the famous continuum hypothesis which asserts that there is no cardinal number x between and c in other words the continuum hypothesis states that there is no set fraktur c a such that the cardinality of the set of positive integers is less than a and a is less than c the cardinality of the set of real numbers it can be shown that the smallest infinite cardinal numbers form an infinite sequence ℵ if we assume that the continuum hypothesis is true it would follow that c so that the continuum hypothesis was stated by cantor in he labored unsuccessfully to prove it becoming extremely dismayed that he could not by settling the continuum hypothesis was considered to be among the most important unsolved problems in mathematics it was the first problem posed by david hilbert in his famous list of open problems in mathematics the continuum hypothesis is still an open question and remains an area for active research however it has been shown that it can be neither proved nor disproved under the standard set theory axioms in modern mathematics the zermelo fraenkel axioms the zermelo fraenkel axioms were formulated to avoid the paradoxes of naive set theory such as russell paradox but there is much controversy whether they should be replaced by some other set of axioms for set theory exercises determine whether each of these sets is finite countably infinite or uncountable for those that are countably in finite exhibit a one to one correspondence between the set of positive integers and that set a the negative integers b the even integers c the integers less than d the real numbers between and e the positive integers less than f the integers that are multiples of determine whether each of these sets is finite countably infinite or uncountable for those that are countably in finite exhibit a one to one correspondence between the set of positive integers and that set a the integers greater than b the odd negative integers c the integers with absolute value less than d the real numbers between and e the set a z where a f the integers that are multiples of determine whether each of these sets is countable or un countable for those that are countably infinite exhibit a one to one correspondence between the set of positive integers and that set a all bit strings not containing the bit b all positive rational numbers that cannot be written with denominators less than c the real numbers not containing in their decimal representation d the real numbers containing only a finite number of in their decimal representation determine whether each of these sets is countable or un countable for those that are countably infinite exhibit a one to one correspondence between the set of positive integers and that set a integers not divisible by b integers divisible by but not by c the real numbers with decimal representations con sisting of all d the real numbers with decimal representations of all or show that a finite group of guests arriving at hilbert fully occupied grand hotel can be given rooms without evicting any current guest suppose that hilbert grand hotel is fully occupied but the hotel closes all the even numbered rooms for mainte nance show that all guests can remain in the hotel suppose that hilbert grand hotel is fully occupied on the day the hotel expands to a second building which also contains a countably infinite number of rooms show that the current guests can be spread out to fill every room of the two buildings of the hotel show that a countably infinite number of guests arriv ing at hilbert fully occupied grand hotel can be given rooms without evicting any current guest suppose that a countably infinite number of buses each containing a countably infinite number of guests arrive at hilbert fully occupied grand hotel show that all the arriving guests can be accommodated without evicting any current guest give an example of two uncountable sets a and b such that a b is a finite b countably infinite c uncountable give an example of two uncountable sets a and b such that a b is a finite b countably infinite c uncountable show that if a and b are sets and a b then a b explain why the set a is countable if and only if a z show that if a and b are sets with the same cardinality then a b and b a show that if a and b are sets a is uncountable and a b then b is uncountable show that a subset of a countable set is also countable if a is an uncountable set and b is a countable set must a b be uncountable show that if a and b are sets a b then p a p b show that if a b c and d are sets with a b and c d then a c b d show that if a b and b c then a c show that if a b and c are sets such that a b and b c then a c suppose that a is a countable set show that the set b is also countable if there is an onto function f from a to b show that if a is an infinite set then it contains a count ably infinite subset show that there is no infinite set a such that a z prove that if it is possible to label each element of an infinite set s with a finite string of keyboard characters from a finite list characters where no two elements of s have the same label then s is a countably infinite set use exercise to provide a proof different from that in the text that the set of rational numbers is countable hint show that you can express a rational number as a string of digits with a slash and possibly a minus sign show that the union of a countable number of countable sets is countable show that the set z z is countable show that the set of all finite bit strings is countable show that the set of real numbers that are solutions of quadratic equations bx c where a b and c are integers is countable show that z z is countable by showing that the polynomial function f z z z with f m n m n m n m is one to one and onto show that when you substitute for each occur rence of n and for each occurrence of m in the right hand side of the formula for the function f m n in exercise you obtain a one to one polynomial func tion z z z it is an open question whether there is a one to one polynomial function q q q use the schröder bernstein theorem to show that and have the same cardinality show that and r have the same cardinality hint use the schröder bernstein theorem show that there is no one to one correspondence from the set of positive integers to the power set of the set of positive integers hint assume that there is such a one to one correspondence represent a subset of the set of positive integers as an infinite bit string with ith bit if i belongs to the subset and otherwise suppose that you can list these infinite strings in a sequence indexed by the positive integers construct a new bit string with its ith bit equal to the complement of the ith bit of the ith string in the list show that this new bit string cannot appear in the list show that there is a one to one correspondence from the set of subsets of the positive integers to the set real num bers between and use this result and exercises and to conclude that z r hint look at the first part of the hint for exercise show that the set of all computer programs in a partic ular programming language is countable hint a com puter program written in a programming language can be thought of as a string of symbols from a finite alphabet show that the set of functions from the positive inte gers to the set is uncountable hint first set up a one to one correspondence between the set of real numbers between and and a subset of these functions do this by associating to the real number dn the function f with f n dn we say that a function is computable if there is a com puter program that finds the values of this function use exercises and to show that there are functions that are not computable show that if s is a set then there does not exist an onto function f from s to s the power set of s con clude that s s this result is known as cantor theorem hint suppose such a function f existed let t s f and show that no element can exist for which f t matrices introduction matrices are used throughout discrete mathematics to express relationships between elements in sets in subsequent chapters we will use matrices in a wide variety of models for instance matrices will be used in models of communications networks and transportation systems many algorithms will be developed that use these matrix models this section reviews matrix arithmetic that will be used in these algorithms definition example the matrix matrix we now introduce some terminology about matrices boldface uppercase letters will be used to represent matrices definition definition matrix arithmetic the basic operations of matrix arithmetic will now be discussed beginning with a definition of matrix addition example the sum of two matrices of the same size is obtained by adding elements in the corresponding positions matrices of different sizes cannot be added because the sum of two matrices is defined only when both matrices have the same number of rows and the same number of columns we now discuss matrix products a product of two matrices is defined only when the number of columns in the first matrix equals the number of rows of the second matrix definition in figure the colored row of a and the colored column of b are used to compute the element cij of ab the product of two matrices is not defined when the number of columns in the first matrix and the number of rows in the second matrix are not the same we now give some examples of matrix products example let find ab if it is defined solution because a matrix and b matrix the product ab is defined and is matrix to find the elements of ab the corresponding elements of the rows of a and the columns of b are first multiplied and then these products are added for instance the element in the th position of ab is the sum of the products of the corresponding elements of the third row of a and the first column of b namely when all the elements of ab are computed we see that ab matrix multiplication is not commutative that is if a and b are two matrices it is not necessarily true that ab and ba are the same in fact it may be that only one of these two products is defined for instance if a is and b is then ab is defined and is however ba is not defined because it is impossible to multiply a matrix and a matrix in general suppose that a is an m n matrix and b is an r matrix then ab is defined only when n r and ba is defined only when m moreover even when ab and ba are b1n aik cij bkj bkn cmn amk figure the product of a aij and b bij both defined they will not be the same size unless m n r hence if both ab and ba are defined and are the same size then both a and b must be square and of the same size furthermore even with a and b both n n matrices ab and ba are not necessarily equal as example demonstrates example let a and b does ab ba solution we find that ab and ba hence ab ba transposes and powers of matrices we now introduce an important matrix with entries that are zeros and ones definition multiplying a matrix by an appropriately sized identity matrix does not change this matrix in other words when a is an m n matrix we have ain ima a powers of square matrices can be defined when a is an n n matrix we have in ar a aa a the operation of interchanging the rows and columns of a square matrix arises in many contexts definition example the transpose of the matrix is the matrix matrices that do not change when their rows and columns are interchanged are often im portant definition note that a matrix is symmetric if and only if it is square and it is symmetric with respect to its main diagonal which consists of entries that are in the ith row and ith column for some i this symmetry is displayed in figure figure a symmetric matrix zero one matrices a matrix all of whose entries are either or is called a zero one matrix zero one matrices are often used to represent discrete structures as we will see in chapters and algorithms using these structures are based on boolean arithmetic with zero one matrices this arithmetic is based on the boolean operations and which operate on pairs of bits defined by b b if otherwise b b if or otherwise definition example find the join and meet of the zero one matrices a b solution we find that the join of a and b is a b the meet of a and b is a b we now define the boolean product of two matrices definition note that the boolean product of a and b is obtained in an analogous way to the ordinary product of these matrices but with addition replaced with the operation and with multiplication replaced with the operation we give an example of the boolean products of matrices example find the boolean product of a and b where a b solution the boolean product a b is given by a we can also define the boolean powers of a square zero one matrix these powers will be used in our subsequent studies of paths in graphs which are used to model such things as communications paths in computer networks definition solution we find that a a a we also find that a a a a a a additional computation shows that a exercises the reader can now see that a n a for all positive integers n with n b a a what size is a b what is the third column of a c what is the second row of a b find ab if d what is the element of a in the th position a a b e what is at find a b where b a b a a c a b b find the product ab where let b a b find a formula for an whenever n is a positive integer show that at t a c a b let a and b be two n n matrices show that a a b t at bt find a matrix a such that b ab t bt at if a and b are n n matrices with ab ba in then b is called the inverse of a this terminology is appropriate be a cause such a matrix b is unique and a is said to be invertible the notation b a denotes that b is the inverse of a show that hint finding a requires that you solve systems of linear equations find a matrix a such that is the inverse of a let a be an m n matrix and let be the m n matrix that has all entries equal to zero show that a a a show that matrix addition is commutative that is show that if a and b are both m n matrices then a b b a show that matrix addition is associative that is show that if a b and c are all m n matrices then let a be the matrix a a show that if ad bc then a b c a b c let a matrix b matrix and c be a matrix determine which of the following products are defined and find the size of those that are defined a ab b ba c ac d ca e bc f cb what do we know about the sizes of the matrices a and let a d c ad bc b a ad bc b if both of the products ab and ba are defined in this exercise we show that matrix multiplication is dis tributive over matrix addition a suppose that a and b are m k matrices and that c is a k n matrix show that a b c ac bc b suppose that c is an m k matrix and that a and b are k n matrices show that c a b ca cb in this exercise we show that matrix multiplication is associative suppose that a is an m p matrix b is a p k matrix and c is a k n matrix show that a bc ab c the n n matrix a aij is called a diagonal matrix if aij when i j show that the product of two n n diagonal matrices is again a diagonal matrix give a sim ple rule for determining this product a a find a hint use exercise b find c find a d use your answers to b and c to show that a is the inverse of let a be an invertible matrix show that an a n whenever n is a positive integer let a be a matrix show that the matrix aat is symmet ric hint show that this matrix equals its transpose with the help of exercise suppose that a is an n n matrix where n is a positive integer show that a a is symmetric a show that the system of simultaneous linear equations find the boolean product of a and b where a and b annxn bn in the variables xn can be expressed as ax b where a aij x is an n matrix with xi the entry in its ith row and b is an n matrix with bi the entry in its ith row b show that if the matrix a aij is invertible as let find a defined in the preamble to exercise then the so lution of the system in part a can be found using the equation x a use exercises and to solve the system let a and b a a b a c a a a let a be a zero one matrix show that a a a a b a a a in this exercise we show that the meet and join opera tions are commutative let a and b be m n zero one matrices show that a a b b a b b a a b in this exercise we show that the meet and join opera tions are associative let a b and c be m n zero one matrices show that a a b c a b c find b a b c a b c we will establish distributive laws of the meet over the join operation in this exercise let a b and c be m n zero one matrices show that a a b b a b c a b let a a b c a b a c b a b c a b a c let a be an n n zero one matrix let i be the n n a and b identity matrix show that a i i a a find in this exercise we will show that the boolean prod uct of zero one matrices is associative assume that a is an m p zero one matrix b is a p k zero one a a b b a b c a b key terms and results terms set a collection of distinct objects axiom a basic assumption of a theory paradox a logical inconsistency element member of a set an object in a set roster method a method that describes a set by listing its elements set builder notation the notation that describes a set by stating a property an element must have to be a member empty set null set the set with no members universal set the set containing all objects under considera tion venn diagram a graphical representation of a set or sets s t set equality s and t have the same elements matrix and c is a k n zero one matrix show that a b c a b c s t s is a subset of t every element of s is also an element of t s t s is a proper subset of t s is a subset of t and s t finite set a set with n elements where n is a nonnegative integer infinite set a set that is not finite s the cardinality of s the number of elements in s p s the power set of s the set of all subsets of s a b the union of a and b the set containing those ele ments that are in at least one of a and b a b the intersection of a and b the set containing those elements that are in both a and b a b the difference of a and b the set containing those j n ai the sum an a the complement of a the set of elements in the universal set that are not in a a b the symmetric difference of a and b the set con taining those elements in exactly one of a and b membership table a table displaying the membership of ele ments in sets function from a to b an assignment of exactly one element of b to each element of a domain of f the set a where f is a function from a to b codomain of f the set b where f is a function from a to b b is the image of a under f b f a a is a pre image of b under f f a b range of f the set of images of f onto function surjection a function from a to b such that every element of b is the image of some element in a one to one function injection a function such that the im ages of elements in its domain are distinct one to one correspondence bijection a function that is both one to one and onto inverse of f the function that reverses the correspondence given by f when f is a bijection f g composition of f and g the function that assigns f g x to x x floor function the largest integer not exceeding x x ceiling function the smallest integer greater than or equal to x partial function an assignment to each element in a subset of the domain a unique element in the codomain sequence a function with domain that is a subset of the set of cardinality two sets a and b have the same cardinality if there is a one to one correspondence from a to b countable set a set that either is finite or can be placed in one to one correspondence with the set of positive integers uncountable set a set that is not countable aleph null the cardinality of a countable set c the cardinality of the set of real numbers cantor diagonalization argument a proof technique used to show that the set of real numbers is uncountable computable function a function for which there is a com puter program in some programming language that finds its values uncomputable function a function for which no computer program in a programming language exists that finds its values continuum hypothesis the statement there no set a exists such that a c matrix a rectangular array of numbers matrix addition see page matrix multiplication see page in identity matrix of order n the n n matrix that has entries equal to on its diagonal and elsewhere at transpose of a the matrix obtained from a by interchang ing the rows and columns symmetric matrix a matrix is symmetric if it equals its trans pose zero one matrix a matrix with each entry equal to either or integers a b the join of a and b see page geometric progression a sequence of the form a ar ar where a and r are real numbers arithmetic progression a sequence of the form a a d a where a and d are real numbers string a finite sequence empty string a string of length zero recurrence relation a equation that expresses the nth term an of a sequence in terms of one or more of the previous terms of the sequence for all integers n greater than a particular integer a b the meet of a and b see page a b the boolean product of a and b see page results the set identities given in table in section the summation formulae in table in section the set of rational numbers is countable the set of real numbers is uncountable review questions explain what it means for one set to be a subset of another set how do you prove that one set is a subset of another set what is the empty set show that the empty set is a subset of every set a define s the cardinality of the set s b give a formula for a b where a and b are sets a define the power set of a set s b when is the empty set in the power set of a set s c how many elements does the power set of a set s with n elements have a define the union intersection difference and sym metric difference of two sets b what are the union intersection difference and sym metric difference of the set of positive integers and the set of odd integers a explain what it means for two sets to be equal b describe as many of the ways as you can to show that two sets are equal c show in at least two different ways that the sets a b c and a b a c are equal explain the relationship between logical equivalences and set identities a define the domain codomain and range of a function b let f n be the function from the set of integers to the set of integers such that f n what are the domain codomain and range of this function a define what it means for a function from the set of positive integers to the set of positive integers to be one to one b define what it means for a function from the set of positive integers to the set of positive integers to be onto c give an example of a function from the set of posi tive integers to the set of positive integers that is both one to one and onto d give an example of a function from the set of positive integers to the set of positive integers that is one to one but not onto e give an example of a function from the set of posi tive integers to the set of positive integers that is not one to one but is onto f give an example of a function from the set of positive integers to the set of positive integers that is neither one to one nor onto a define the inverse of a function b when does a function have an inverse c does the function f n n from the set of inte gers to the set of integers have an inverse if so what is it a define the floor and ceiling functions from the set of real numbers to the set of integers b for which real numbers x is it true that xj conjecture a formula for the terms of the sequence that begins and find the next three terms of your sequence suppose that an an for n find a for mula for an what is the sum of the terms of the geometric progression a ar arn when r show that the set of odd integers is countable give an example of an uncountable set define the product of two matrices a and b when is this product defined show that matrix multiplication is not commutative supplementary exercises let a be the set of english words that contain the letter x and let b be the set of english words that contain the letter q express each of these sets as a combination of a and b a the set of english words that do not contain the letter x b the set of english words that contain both an x and a q c the set of english words that contain an x but not a q d the set of english words that do not contain either an x or a q e the set of english words that contain an x or a q but not both show that if a is a subset of b then the power set of a is a subset of the power set of b suppose that a and b are sets such that the power set of a is a subset of the power set of b does it follow that a is a subset of b let e denote the set of even integers and o denote the set of odd integers as usual let z denote the set of all integers determine each of these sets a e o b e o c z e d z o show that if a and b are sets then a a b a b let a and b be sets show that a b if and only if a b a let a b and c be sets show that a b c is not necessarily equal to a b c suppose that a b and c are sets prove or disprove that a b c a c b suppose that a b c and d are sets prove or disprove that a b c d a c b d show that if a and b are finite sets then a b a b determine when this relationship is an equality let a and b be sets in a finite universal set u list the following in order of increasing size a a a b a b u b a b a b a b a b let a and b be subsets of the finite universal set u show that a b u a b a b let f and g be functions from to a b c d and from a b c d to respectively with f d f c f a and f b and g a g b g c and g d a is f one to one is g one to one b is f onto is g onto c does either f or g have an inverse if so find this inverse suppose that f is a function from a to b where a and b are finite sets explain why f s s for all subsets s suppose that f is a function from a to b where a and b are finite sets explain why f s s for all subsets s of a if and only if f is one to one suppose that f is a function from a to b we define the func tion sf from p a to p b by the rule sf x f x for each subset x of a similarly we define the function sf from b to a by the rule sf y f y for each subset y of b here we are using definition and the defi nition of the inverse image of a set found in the preamble to exercise both in section suppose that f is a function from the set a to the set b prove that a if f is one to one then sf is a one to one function from a to b b if f is onto function then sf is an onto function from p a to p b c if f is onto function then sf is a one to one func tion from p b to p a d if f is one to one then sf is an onto function from b to a e if f is a one to one correspondence then sf is a one to one correspondence from a to b and sf is a one to one correspondence from b to a hint use parts a d prove that if f and g are functions from a to b and sf sg using the definition in the preamble to exercise then f x g x for all x a show that if n is an integer then n n for which real numbers x and y is it true that x yj xj yj for which real numbers x and y is it true that y 1y for which real numbers x and y is it true that y yj prove that n for all integers n prove that if m is an integer then xj m xj m unless x is an integer in which case it equals m prove that if x is a real number then x x prove that if n is an odd integer then prove that if m and n are positive integers and x is a real number then xj n x n we define the ulam numbers by setting and furthermore after determining whether the in tegers less than n are ulam numbers we set n equal to the next ulam number if it can be written uniquely as the sum of two different ulam numbers note that and a find the first ulam numbers b prove that there are infinitely many ulam numbers determine the value of k the notation used here for products is defined in the preamble to exercise in section determine a rule for generating the terms of the sequence that begins and find the next four terms of the sequence determine a rule for generating the terms of the sequence that begins and find the next four terms of the sequence show that the set of irrational numbers is an uncountable set show that the set s is a countable set if there is a func tion f from s to the positive integers such that f j is countable whenever j is a positive integer show that the set of all finite subsets of the set of positive integers is a countable set show that r r r hint use the schröder bernstein theorem to show that to construct an injection from to suppose that x y map x y to the number with decimal expansion formed by alter nating between the digits in the decimal expansions of x and y which do not end with an infinite string of show that c the set of complex numbers has the same cardinality as r the set of real numbers find an if a is show that if a ci where c is a real number and i is the n n identity matrix then ab ba whenever b is an n n matrix show that if a matrix such that ab ba when ever b is a matrix then a ci where c is a real m m number and i is the identity matrix prove that if m is a positive integer and x is a real number then show that if a and b are invertible matrices and ab exists then ab b let a be an n n matrix and let be the n n matrix x m a a a b a a a c a a computer projects computer projects write programs with the specified input and output given subsets a and b of a set with n elements use bit strings to find a a b a b a b and a b given multisets a and b from the same universal set find a b a b a b and a b see preamble to exer cise of section given fuzzy sets a and b find a a b and a b see preamble to exercise of section given a function f from n to the set of integers determine whether f is one to one given a function f from n to itself determine whether f is onto given a bijection f from the set n to itself find f given an m k matrix a and a k n matrix b find ab given a square matrix a and a positive integer n find an given a square matrix determine whether it is symmetric given two m n boolean matrices find their meet and join given an m k boolean matrix a and a k n boolean matrix b find the boolean product of a and b given a square boolean matrix a and a positive integer n find a n computations and explorations use a computational program or programs you have written to do these exercises given two finite sets list all elements in the cartesian prod uct of these two sets given a finite set list all elements of its power set calculate the number of one to one functions from a set s to a set t where s and t are finite sets of various sizes can you determine a formula for the number of such functions we will find such a formula in chapter calculate the number of onto functions from a set s to a set t where s and t are finite sets of various sizes can you determine a formula for the number of such functions we will find such a formula in chapter develop a collection of different rules for generating the terms of a sequence and a program for randomly selecting one of these rules and the particular sequence generated using these rules make this part of an interactive program that prompts for the next term of the sequence and deter mines whether the response is the intended next term writing projects respond to these with essays using outside sources discuss how an axiomatic set theory can be developed to avoid russell paradox see exercise of section research where the concept of a function first arose and describe how this concept was first used explain the different ways in which the encyclopedia of integer sequences has been found useful also describe a few of the more unusual sequences in this encyclopedia and how they arise define the recently invented ekg sequence and describe some of its properties and open questions about it look up the definition of a transcendental number explain how to show that such numbers exist and how such num bers can be constructed which famous numbers can be shown to be transcendental and for which famous numbers is it still unknown whether they are transcendental expand the discussion of the continuum hypothesis in the text algorithms the growth of functions complexity of algorithms any problems can be solved by considering them as special cases of general problems for instance consider the problem of locating the largest integer in the sequence this is a specific case of the problem of locating the largest integer in a sequence of integers to solve this general problem we must give an algorithm which specifies a sequence of steps used to solve this general problem we will study algorithms for solving many different types of problems in this book for example in this chapter we will introduce algorithms for two of the most important problems in computer science searching for an element in a list and sorting a list so its elements are in some prescribed order such as increasing decreasing or alphabetic later in the book we will develop algorithms that find the greatest common divisor of two integers that generate all the orderings of a finite set that find the shortest path between nodes in a network and for solving many other problems we will also introduce the notion of an algorithmic paradigm which provides a general method for designing algorithms in particular we will discuss brute force algorithms which find solutions using a straightforward approach without introducing any cleverness we will also discuss greedy algorithms a class of algorithms used to solve optimization problems proofs are important in the study of algorithms in this chapter we illustrate this by proving that a particular greedy algorithm always finds an optimal solution one important consideration concerning an algorithm is its computational complexity which measures the processing time and computer memory required by the algorithm to solve problems of a particular size to measure the complexity of algorithms we use big o and big theta notation which we develop in this chapter we will illustrate the analysis of the complexity of algorithms in this chapter focusing on the time an algorithm takes to solve a problem fur thermore we will discuss what the time complexity of an algorithm means in practical and theoretical terms algorithms introduction there are many general classes of problems that arise in discrete mathematics for instance given a sequence of integers find the largest one given a set list all its subsets given a set of integers put them in increasing order given a network find the shortest path between two vertices when presented with such a problem the first thing to do is to construct a model that translates the problem into a mathematical context discrete structures used in such models include sets sequences and functions structures discussed in chapter as well as such other structures as permutations relations graphs trees networks and finite state machines concepts that will be discussed in later chapters setting up the appropriate mathematical model is only part of the solution to complete the solution a method is needed that will solve the general problem using the model ideally what is required is a procedure that follows a sequence of steps that leads to the desired answer such a sequence of steps is called an algorithm definition the term algorithm is a corruption of the name al khowarizmi a mathematician of the ninth century whose book on hindu numerals is the basis of modern decimal notation originally the word algorism was used for the rules for performing arithmetic using decimal notation algorism evolved into the word algorithm by the eighteenth century with the growing interest in computing machines the concept of an algorithm was given a more general meaning to include all definite procedures for solving problems not just the procedures for performing arithmetic we will discuss algorithms for performing arithmetic with integers in chapter in this book we will discuss algorithms that solve a wide variety of problems in this section we will use the problem of finding the largest integer in a finite sequence of integers to illustrate the concept of an algorithm and the properties algorithms have also we will describe algorithms for locating a particular element in a finite set in subsequent sections procedures for finding the greatest common divisor of two integers for finding the shortest path between two points in a network for multiplying matrices and so on will be discussed example describe an algorithm for finding the maximum largest value in a finite sequence of integers even though the problem of finding the maximum element in a sequence is relatively trivial it provides a good illustration of the concept of an algorithm also there are many instances where the largest integer in a finite sequence of integers is required for instance a university may need to find the highest score on a competitive exam taken by thousands of students or a sports organization may want to identify the member with the highest rating each month we want to develop an algorithm that can be used whenever the problem of finding the largest element in a finite sequence of integers arises we can specify a procedure for solving this problem in several ways one method is simply to use the english language to describe the sequence of steps used we now provide such a solution solution of example we perform the following steps set the temporary maximum equal to the first integer in the sequence the temporary maximum will be the largest integer examined at any stage of the procedure compare the next integer in the sequence to the temporary maximum and if it is larger than the temporary maximum set the temporary maximum equal to this integer repeat the previous step if there are more integers in the sequence stop when there are no integers left in the sequence the temporary maximum at this point is the largest integer in the sequence an algorithm can also be described using a computer language however when that is done only those instructions permitted in the language can be used this often leads to a description of the algorithm that is complicated and difficult to understand furthermore because many programming languages are in common use it would be undesirable to choose one particular language so instead of using a particular computer language to specify algorithms a form of pseudocode described in appendix will be used in this book we will also describe algorithms using the english language pseudocode provides an intermediate step between abu ja far mohammed ibn musa al khowarizmi c c al khowarizmi an as tronomer and mathematician was a member of the house of wisdom an academy of scientists in baghdad the name al khowarizmi means from the town of kowarzizm which was then part of persia but is now called khiva and is part of uzbekistan al khowarizmi wrote books on mathematics astronomy and geography western europeans first learned about algebra from his works the word algebra comes from al jabr part of the title of his book kitab al jabr w al muquabala this book was translated into latin and was a widely used textbook his book on the use of hindu numerals describes procedures for arithmetic operations using these numerals european authors used a latin corruption of his name which later evolved to the word algorithm to describe the subject of arithmetic with hindu numerals an english language description of an algorithm and an implementation of this algorithm in a programming language the steps of the algorithm are specified using instructions resembling those used in programming languages however in pseudocode the instructions used can include any well defined operations or statements a computer program can be produced in any computer language using the pseudocode description as a starting point the pseudocode used in this book is designed to be easily understood it can serve as an intermediate step in the construction of programs implementing algorithms in one of a variety of different programming languages although this pseudocode does not follow the syntax of java c c or any other programming language students familiar with a modern programming language will find it easy to follow a key difference between this pseudocode and code in a programming language is that we can use any well defined instruction even if it would take many lines of code to implement this instruction the details of the pseudocode used in the text are given in appendix the reader should refer to this appendix whenever the need arises a pseudocode description of the algorithm for finding the maximum element in a finite sequence follows this algorithm first assigns the initial term of the sequence to the variable max the for loop is used to successively examine terms of the sequence if a term is greater than the current value of max it is assigned to be the new value of max properties of algorithms there are several properties that algorithms generally share they are useful to keep in mind when algorithms are described these properties are input an algorithm has input values from a specified set output from each set of input values an algorithm produces output values from a spec ified set the output values are the solution to the problem definiteness the steps of an algorithm must be defined precisely correctness an algorithm should produce the correct output values for each set of input values finiteness an algorithm should produce the desired output after a finite but perhaps large number of steps for any input in the set effectiveness it must be possible to perform each step of an algorithm exactly and in a finite amount of time generality the procedure should be applicable for all problems of the desired form not just for a particular set of input values example show that algorithm for finding the maximum element in a finite sequence of integers has all the properties listed solution the input to algorithm is a sequence of integers the output is the largest integer in the sequence each step of the algorithm is precisely defined because only assignments a finite loop and conditional statements occur to show that the algorithm is correct we must show that when the algorithm terminates the value of the variable max equals the maximum of the terms of the sequence to see this note that the initial value of max is the first term of the sequence as successive terms of the sequence are examined max is updated to the value of a term if the term exceeds the maximum of the terms previously examined this informal argument shows that when all the terms have been examined max equals the value of the largest term a rigorous proof of this requires techniques developed in section the algorithm uses a finite number of steps because it terminates after all the integers in the sequence have been examined the algorithm can be carried out in a finite amount of time because each step is either a comparison or an assignment there are a finite number of these steps and each of these two operations takes a finite amount of time finally algorithm is general because it can be used to find the maximum of any finite sequence of integers searching algorithms the problem of locating an element in an ordered list occurs in many contexts for instance a program that checks the spelling of words searches for them in a dictionary which is just an ordered list of words problems of this kind are called searching problems we will discuss several algorithms for searching in this section we will study the number of steps used by each of these algorithms in section the general searching problem can be described as follows locate an element x in a list of distinct elements an or determine that it is not in the list the solution to this search problem is the location of the term in the list that equals x that is i is the solution if x ai and is if x is not in the list the linear search the first algorithm that we will present is called the linear search or sequential search algorithm the linear search algorithm begins by comparing x and when x the solution is the location of namely when x compare x with if x the solution is the location of namely when x compare x with continue this process comparing x successively with each term of the list until a match is found where the solution is the location of that term unless no match occurs if the entire list has been searched without locating x the solution is the pseudocode for the linear search algorithm is displayed as algorithm the binary search we will now consider another searching algorithm this algorithm can be used when the list has terms occurring in order of increasing size for instance if the terms are numbers they are listed from smallest to largest if they are words they are listed in lexicographic or alphabetic order this second searching algorithm is called the binary search algorithm it proceeds by comparing the element to be located to the middle term of the list the list is then split into two smaller sublists of the same size or where one of these smaller lists has one fewer term than the other the search continues by restricting the search to the appropriate sublist based on the comparison of the element to be located and the middle term in section it will be shown that the binary search algorithm is much more efficient than the linear search algorithm example demonstrates how a binary search works example to search for in the list first split this list which has terms into two smaller lists with eight terms each namely then compare and the largest term in the first list because the search for can be restricted to the list containing the through the terms of the original list next split this list which has eight terms into the two smaller lists of four terms each namely because comparing with the largest term of the first list the search is restricted to the second of these lists which contains the through the terms of the original list the list is split into two lists namely because is not greater than the largest term of the first of these two lists which is also the search is restricted to the first list which contains the and terms of the original list next this list of two terms is split into two lists of one term each and because the search is restricted to the second list the list containing the term of the list which is now that the search has been narrowed down to one term a comparison is made and is located as the term in the original list we now specify the steps of the binary search algorithm to search for the integer x in the list an where an begin by comparing x with the middle term am of the list where m n recall that x is the greatest integer not exceeding x if x am the search for x is restricted to the second half of the list which is am am an if x is not greater than am the search for x is restricted to the first half of the list which is am the search has now been restricted to a list with no more than n elements recall that x is the smallest integer greater than or equal to x using the same procedure compare x to the middle term of the restricted list then restrict the search to the first or second half of the list repeat this process until a list with one term is obtained then determine whether this term is x pseudocode for the binary search algorithm is displayed as algorithm algorithm proceeds by successively narrowing down the part of the sequence being searched at any given stage only the terms from ai to aj are under consideration in other words i and j are the smallest and largest subscripts of the remaining terms respectively algorithm continues narrowing the part of the sequence being searched until only one term of the sequence remains when this is done a comparison is made to see whether this term equals x sorting is thought to hold the record as the problem solved by the most fundamentally different algorithms sorting ordering the elements of a list is a problem that occurs in many contexts for example to produce a telephone directory it is necessary to alphabetize the names of subscribers similarly producing a directory of songs available for downloading requires that their titles be put in alphabetic order putting addresses in order in an e mail mailing list can determine whether there are duplicated addresses creating a useful dictionary requires that words be put in alphabetical order similarly generating a parts list requires that we order them according to increasing part number suppose that we have a list of elements of a set furthermore suppose that we have a way to order elements of the set the notion of ordering elements of sets will be discussed in detail in section sorting is putting these elements into a list in which the elements are in increasing order for instance sorting the list produces the list sorting the list d h c a f using alphabetical order produces the list a c d f h an amazingly large percentage of computing resources is devoted to sorting one thing or another hence much effort has been devoted to the development of sorting algorithms a surprisingly large number of sorting algorithms have been devised using distinct strate gies with new ones introduced regularly in his fundamental work the art of computer programming donald knuth devotes close to pages to sorting covering around different sorting algorithms in depth more than sorting algorithms have been de vised and it is surprising how often new sorting algorithms are developed among the newest sorting algorithms that have caught on is the the library sort also known as the gapped insertion sort invented as recently as there are many reasons why sort ing algorithms interest computer scientists and mathematicians among these reasons are that some algorithms are easier to implement some algorithms are more efficient either in general or when given input with certain characteristics such as lists slightly out of order some algorithms take advantage of particular computer architectures and some al gorithms are particularly clever in this section we will introduce two sorting algorithms the bubble sort and the insertion sort two other sorting algorithms the selection sort and the binary insertion sort are introduced in the exercises and the shaker sort is in troduced in the supplementary exercises in section we will discuss the merge sort and introduce the quick sort in the exercises in that section the tournament sort is in troduced in the exercise set in section we cover sorting algorithms both because sorting is an important problem and because these algorithms can serve as examples for many important concepts the bubble sort the bubble sort is one of the simplest sorting algorithms but not one of the most efficient it puts a list into increasing order by successively comparing adjacent elements interchanging them if they are in the wrong order to carry out the bubble sort we perform the basic operation that is interchanging a larger element with a smaller one following it starting at the beginning of the list for a full pass we iterate this procedure until the sort is complete pseudocode for the bubble sort is given as algorithm we can imagine the elements in the list placed in a column in the bubble sort the smaller elements bubble to the top as they are interchanged with larger elements the larger elements sink to the bottom this is illustrated in example example use the bubble sort to put into increasing order first pass second pass an interchange figure the steps of a bubble sort pair in correct order numbers in color guaranteed to be in correct order solution the steps of this algorithm are illustrated in figure begin by comparing the first two elements and because interchange and producing the list because continue by comparing and because interchange and producing the list because the first pass is complete the first pass guarantees that the largest element is in the correct position the second pass begins by comparing and because these are in the correct order and are compared because these numbers are interchanged producing because these numbers are in the correct order it is not necessary to do any more comparisons for this pass because is already in the correct position the second pass guarantees that the two largest elements and are in their correct positions the third pass begins by comparing and these are interchanged because produc ing because these two elements are in the correct order it is not necessary to do any more comparisons for this pass because and are already in the correct positions the third pass guarantees that the three largest elements and are in their correct positions the fourth pass consists of one comparison namely the comparison of and because these elements are in the correct order this completes the bubble sort the insertion sort the insertion sort is a simple sorting algorithm but it is usually not the most efficient to sort a list with n elements the insertion sort begins with the second element the insertion sort compares this second element with the first element and inserts it before the first element if it does not exceed the first element and after the first element if it exceeds the first element at this point the first two elements are in the correct order the third element is then compared with the first element and if it is larger than the first element it is compared with the second element it is inserted into the correct position among the first three elements in general in the j th step of the insertion sort the j th element of the list is inserted into the correct position in the list of the previously sorted j elements to insert the j th element in the list a linear search technique is used see exercise the j th element is successively compared with the already sorted j elements at the start of the list until the first element that is not less than this element is found or until it has been compared with all j elements the j th element is inserted in the correct position so that the first j elements are sorted the algorithm continues until the last element is placed in the correct position relative to the already sorted list of the first n elements the insertion sort is described in pseudocode in algorithm example use the insertion sort to put the elements of the list in increasing order solution the insertion sort first compares and because it places in the first position producing the list the sorted part of the list is shown in color at this point and are in the correct order next it inserts the third element into the already sorted part of the list by making the comparisons and because remains in the third position at this point the list is and we know that the ordering of the first three elements is correct next we find the correct place for the fourth element among the already sorted elements because we obtain the list finally we insert into the correct position by successively comparing it to and because it stays at the end of the list producing the correct order for the entire list greed is good greed is right greed works greed clarifies spoken by the character gordon gecko in the film wall street you have to prove that a greedy algorithm always finds an optimal solution greedy algorithms many algorithms we will study in this book are designed to solve optimization problems the goal of such problems is to find a solution to the given problem that either minimizes or maximizes the value of some parameter optimization problems studied later in this text include finding a route between two cities with smallest total mileage determining a way to encode messages using the fewest bits possible and finding a set of fiber links between network nodes using the least amount of fiber surprisingly one of the simplest approaches often leads to a solution of an optimization problem this approach selects the best choice at each step instead of considering all sequences of steps that may lead to an optimal solution algorithms that make what seems to be the best choice at each step are called greedy algorithms once we know that a greedy algorithm finds a feasible solution we need to determine whether it has found an optimal solution note that we call the algoritm greedy whether or not it finds an optimal solution to do this we either prove that the solution is optimal or we show that there is a counterexample where the algorithm yields a nonoptimal solution to make these concepts more concrete we will consider an algorithm that makes change using coins example consider the problem of making n cents change with quarters dimes nickels and pennies and using the least total number of coins we can devise a greedy algorithm for making change for n cents by making a locally optimal choice at each step that is at each step we choose the coin of the largest denomination possible to add to the pile of change without exceeding n cents for example to make change for cents we first select a quarter leaving cents we next select a second quarter leaving cents followed by a dime leaving cents followed by a nickel leaving cents followed by a penny leaving cent followed by a penny we display a greedy change making algorithm for n cents using any set of denominations of coins as algorithm we have described a greedy algorithm for making change using any finite set of coins with denominations cr in the particular case where the four denominations are quarters dimes nickels and pennies we have and for this case we will show that this algorithm leads to an optimal solution in the sense that it uses the fewest coins possible before we embark on our proof we show that there are sets of coins for which the greedy algorithm algorithm does not necessarily produce change using the fewest coins possible for example if we have only quarters dimes and pennies and no nickels to use the greedy algorithm would make change for cents using six coins a quarter and five pennies whereas we could have used three coins namely three dimes lemma proof we use a proof by contradiction we will show that if we had more than the specified number of coins of each type we could replace them using fewer coins that have the same value we note that if we had three dimes we could replace them with a quarter and a nickel if we had two nickels we could replace them with a dime if we had five pennies we could replace them with a nickel and if we had two dimes and a nickel we could replace them with a quarter because we can have at most two dimes one nickel and four pennies but we cannot have two dimes and a nickel it follows that cents is the most money we can have in dimes nickels and pennies when we make change using the fewest number of coins for n cents theorem proof we will use a proof by contradiction suppose that there is a positive integer n such that there is a way to make change for n cents using quarters dimes nickels and pennies that uses fewer coins than the greedy algorithm finds we first note that qt the number of quarters used in this optimal way to make change for n cents must be the same as q the number of quarters used by the greedy algorithm to show this first note that the greedy algorithm uses the most quarters possible so qt q however it is also the case that qt cannot be less than q if it were we would need to make up at least cents from dimes nickels and pennies in this optimal way to make change but this is impossible by lemma because there must be the same number of quarters in the two ways to make change the value of the dimes nickels and pennies in these two ways must be the same and these coins are worth no more than cents there must be the same number of dimes because the greedy algorithm used the most dimes possible and by lemma when change is made using the fewest coins possible at most one nickel and at most four pennies are used so that the most dimes possible are also used in the optimal way to make change similarly we have the same number of nickels and finally the same number of pennies a greedy algorithm makes the best choice at each step according to a specified criterion the next example shows that it can be difficult to determine which of many possible criteria to choose example suppose we have a group of proposed talks with preset start and end times devise a greedy algorithm to schedule as many of these talks as possible in a lecture hall under the assumptions that once a talk starts it continues until it ends no two talks can proceed at the same time and a talk can begin at the same time another one ends assume that talk j begins at time sj where stands for start and ends at time ej where e stands for end solution to use a greedy algorithm to schedule the most talks that is an optimal schedule we need to decide how to choose which talk to add at each step there are many criteria we could use to select a talk at each step where we chose from the talks that do not overlap talks already selected for example we could add talks in order of earliest start time we could add talks in order of shortest time we could add talks in order of earliest finish time or we could use some other criterion we now consider these possible criteria suppose we add the talk that starts earliest among the talks compatible with those already selected we can construct a counterexample to see that the resulting algorithm does not always produce an optimal schedule for instance suppose that we have three talks talk starts at a m and ends at noon talk starts at a m and ends at a m and talk starts at a m and ends at noon we first select the talk because it starts earliest but once we have selected talk we cannot select either talk or talk because both overlap talk hence this greedy algorithm selects only one talk this is not optimal because we could schedule talk and talk which do not overlap now suppose we add the talk that is shortest among the talks that do not overlap any of those already selected again we can construct a counterexample to show that this greedy algorithm does not always produce an optimal schedule so suppose that we have three talks talk starts at a m and ends at a m talk starts at a m and ends at a m and talk starts at a m and ends at a m we select talk because it is shortest requiring one hour once we select talk we cannot select either talk or talk because neither is compatible with talk hence this greedy algorithm selects only one talk however it is possible to select two talks talk and talk which are compatible however it can be shown that we schedule the most talks possible if in each step we select the talk with the earliest ending time among the talks compatible with those already selected we will prove this in chapter using the method of mathematical induction the first step we will make is to sort the talks according to increasing finish time after this sorting we relabel the talks so that en the resulting greedy algorithm is given as algorithm the halting problem we will now describe a proof of one of the most famous theorems in computer science we will show that there is a problem that cannot be solved using any procedure that is we will show there are unsolvable problems the problem we will study is the halting problem it asks whether there is a procedure that does this it takes as input a computer program and input to the program and determines whether the program will eventually stop when run with this input it would be convenient to have such a procedure if it existed certainly being able to test whether a program entered into an infinite loop would be helpful when writing and debugging programs however in alan turing showed that no such procedure exists see his biography in section before we present a proof that the halting problem is unsolvable first note that we cannot simply run a program and observe what it does to determine whether it terminates when run with the given input if the program halts we have our answer but if it is still running after any fixed length of time has elapsed we do not know whether it will never halt or we just did not wait long enough for it to terminate after all it is not hard to design a program that will stop only after more than a billion years has elapsed we will describe turing proof that the halting problem is unsolvable it is a proof by contradiction the reader should note that our proof is not completely rigorous because we have not explicitly defined what a procedure is to remedy this the concept of a turing machine is needed this concept is introduced in section proof assume there is a solution to the halting problem a procedure called h p i the procedure h p i takes two inputs one a program p and the other i an input to the program p h p i generates the string halt as output if h determines that p stops when given i as input otherwise h p i generates the string loops forever as output we will now derive a contradiction when a procedure is coded it is expressed as a string of characters this string can be interpreted as a sequence of bits this means that a program itself can be used as data therefore a program can be thought of as input to another program or even itself hence h can take a program p as both of its inputs which are a program and input to this program h should be able to determine whether p will halt when it is given a copy of itself as input to show that no procedure h exists that solves the halting problem we construct a simple procedure k p which works as follows making use of the output h p p if the output of h p p is loops forever which means that p loops forever when given a copy of itself as input then k p halts if the output of h p p is halt which means that p halts when given a copy of itself as input then k p loops forever that is k p does the opposite of what the output of h p p specifies see figure now suppose we provide k as input to k we note that if the output of h k k is loops forever then by the definition of k we see that k k halts otherwise if the output of h k k input program p p as program p as input output h p p if h p p halts then loop forever if h p p loops forever then halt figure showing that the halting problem is unsolvable is halt then by the definition of k we see that k k loops forever in violation of what h tells us in both cases we have a contradiction thus h cannot always give the correct answers consequently there is no procedure that solves the halting problem exercises list all the steps used by algorithm to find the maximum of the list determine which characteristics of an algorithm de scribed in the text after algorithm the following pro cedures have and which they lack a procedure double n positive integer while n n b procedure divide n positive integer while n m n n n c procedure sum n positive integer sum while i sum sum i d procedure choose a b integers x either a or b devise an algorithm that finds the sum of all the integers in a list describe an algorithm that takes as input a list of n in tegers and produces as output the largest difference ob tained by subtracting an integer in the list from the one following it describe an algorithm that takes as input a list of n inte gers in nondecreasing order and produces the list of all values that occur more than once recall that a list of integers is nondecreasing if each integer in the list is at least as large as the previous integer in the list describe an algorithm that takes as input a list of n in tegers and finds the number of negative integers in the list describe an algorithm that takes as input a list of n inte gers and finds the location of the last even integer in the list or returns if there are no even integers in the list describe an algorithm that takes as input a list of n dis tinct integers and finds the location of the largest even integer in the list or returns if there are no even integers in the list a palindrome is a string that reads the same forward and backward describe an algorithm for determining whether a string of n characters is a palindrome devise an algorithm to compute xn where x is a real number and n is an integer hint first give a procedure for computing xn when n is nonnegative by successive multiplication by x starting with then extend this pro cedure and use the fact that x n xn to compute xn when n is negative describe an algorithm that interchanges the values of the variables x and y using only assignments what is the minimum number of assignment statements needed to do this describe an algorithm that uses only assignment state ments that replaces the triple x y z with y z x what is the minimum number of assignment statements needed list all the steps used to search for in the sequence using a a linear search b a binary search list all the steps used to search for in the sequence given in exercise for both a linear search and a binary search describe an algorithm that inserts an integer x in the ap propriate position into the list an of integers that are in increasing order describe an algorithm for finding the smallest integer in a finite sequence of natural numbers describe an algorithm that locates the first occurrence of the largest element in a finite list of integers where the integers in the list are not necessarily distinct describe an algorithm that locates the last occurrence of the smallest element in a finite list of integers where the integers in the list are not necessarily distinct describe an algorithm that produces the maximum me dian mean and minimum of a set of three integers the median of a set of integers is the middle element in the list when these integers are listed in order of increasing size the mean of a set of integers is the sum of the integers divided by the number of integers in the set describe an algorithm for finding both the largest and the smallest integers in a finite sequence of integers describe an algorithm that puts the first three terms of a sequence of integers of arbitrary length in increasing order describe an algorithm to find the longest word in an en glish sentence where a sentence is a sequence of symbols either a letter or a blank which can then be broken into alternating words and blanks describe an algorithm that determines whether a function from a finite set of integers to another finite set of integers is onto describe an algorithm that determines whether a function from a finite set to another finite set is one to one describe an algorithm that will count the number of in a bit string by examining each bit of the string to deter mine whether it is a bit change algorithm so that the binary search procedure compares x to am at each stage of the algorithm with the algorithm terminating if x am what advantage does this version of the algorithm have the ternary search algorithm locates an element in a list of increasing integers by successively splitting the list into three sublists of equal or as close to equal as possible size and restricting the search to the appropriate piece specify the steps of this algorithm specify the steps of an algorithm that locates an element in a list of increasing integers by successively splitting the list into four sublists of equal or as close to equal as possible size and restricting the search to the appropriate piece in a list of elements the same element may appear several times a mode of such a list is an element that occurs at least as often as each of the other elements a list has more than one mode when more than one element appears the maximum number of times devise an algorithm that finds a mode in a list of nonde creasing integers recall that a list of integers is nonde creasing if each term is at least as large as the preceding term devise an algorithm that finds all modes recall that a list of integers is nondecreasing if each term of the list is at least as large as the preceding term devise an algorithm that finds the first term of a se quence of integers that equals some previous term in the sequence devise an algorithm that finds all terms of a finite se quence of integers that are greater than the sum of all previous terms of the sequence devise an algorithm that finds the first term of a sequence of positive integers that is less than the immediately pre ceding term of the sequence use the bubble sort to sort showing the lists obtained at each step use the bubble sort to sort showing the lists obtained at each step use the bubble sort to sort d f k m a b showing the lists obtained at each step adapt the bubble sort algorithm so that it stops when no interchanges are required express this more efficient version of the algorithm in pseudocode use the insertion sort to sort the list in exercise show ing the lists obtained at each step use the insertion sort to sort the list in exercise show ing the lists obtained at each step use the insertion sort to sort the list in exercise show ing the lists obtained at each step the selection sort begins by finding the least element in the list this element is moved to the front then the least element among the remaining elements is found and put into the sec ond position this procedure is repeated until the entire list has been sorted sort these lists using the selection sort a b c write the selection sort algorithm in pseudocode describe an algorithm based on the linear search for de termining the correct position in which to insert a new element in an already sorted list describe an algorithm based on the binary search for de termining the correct position in which to insert a new element in an already sorted list how many comparisons does the insertion sort use to sort the list n how many comparisons does the insertion sort use to sort the list n n the binary insertion sort is a variation of the insertion sort that uses a binary search technique see exercise rather than a linear search technique to insert the ith element in the correct place among the previously sorted elements show all the steps used by the binary insertion sort to sort the list compare the number of comparisons used by the inser tion sort and the binary insertion sort to sort the list express the binary insertion sort in pseudocode a devise a variation of the insertion sort that uses a lin ear search technique that inserts the j th element in the correct place by first comparing it with the j st element then the j th element if necessary and so on b use your algorithm to sort c answer exercise using this algorithm d answer exercise using this algorithm when a list of elements is in close to the correct order would it be better to use an insertion sort or its variation described in exercise use the greedy algorithm to make change using quarters dimes nickels and pennies for a cents b cents c cents d cents use the greedy algorithm to make change using quarters dimes nickels and pennies for a cents b cents c cents d cents use the greedy algorithm to make change using quar ters dimes and pennies but no nickels for each of the amounts given in exercise for which of these amounts does the greedy algorithm use the fewest coins of these denominations possible use the greedy algorithm to make change using quar ters dimes and pennies but no nickels for each of the amounts given in exercise for which of these amounts does the greedy algorithm use the fewest coins of these denominations possible show that if there were a coin worth cents the greedy algorithm using quarters cent coins dimes nickels and pennies would not always produce change using the fewest coins possible use algorithm to schedule the largest number of talks in a lecture hall from a proposed set of talks if the starting and ending times of the talks are a m and a m a m and a m a m and a m a m and a m a m and a m a m and a m a m and a m a m and a m a m and a m a m and a m a m and a m show that a greedy algorithm that schedules talks in a lec ture hall as described in example by selecting at each step the talk that overlaps the fewest other talks does not always produce an optimal schedule a devise a greedy algorithm that determines the fewest lecture halls needed to accommodate n talks given the starting and ending time for each talk b prove that your algorithm is optimal suppose we have men ms and women ws we wish to match each person with a member of the opposite gender furthermore suppose that each person ranks in order of preference with no ties the people of the opposite gender we say that a matching of people of opposite genders to form couples is stable if we cannot find a man m and a woman w who are not assigned to each other such that m prefers w over his assigned partner and w prefers m to her assigned partner suppose we have three men and and three women and furthermore suppose that the preference rankings of the men for the three women from highest to lowest are and the preference rankings of the women for the three men from highest to lowest are for each of the six possible matchings of men and women to form three couples determine whether this matching is stable the deferred acceptance algorithm also known as the gale shapley algorithm can be used to construct a stable matching of men and women in this algorithm members of one gender are the suitors and members of the other gender the suitees the algorithm uses a sequence of rounds in each round every suitor whose proposal was rejected in the previous round pro poses to his or her highest ranking suitee who has not already rejected a proposal from this suitor a suitee rejects all pro posals except that from the suitor that this suitee ranks highest among all the suitors who have proposed to this suitee in this round or previous rounds the proposal of this highest ranking suitor remains pending and is rejected in a later round if a more appealing suitor proposes in that round the series of rounds ends when every suitor has exactly one pending proposal all pending proposals are then accepted write the deferred acceptance algorithm in pseudocode show that the deferred acceptance algorithm terminates show that the deferred acceptance always terminates with a stable assignment show that the problem of determining whether a program with a given input ever prints the digit is unsolvable show that the following problem is solvable given two programs with their inputs and the knowledge that exactly one of them halts determine which halts show that the problem of deciding whether a specific program with a specific input halts is solvable the growth of functions introduction in section we discussed the concept of an algorithm we introduced algorithms that solve a variety of problems including searching for an element in a list and sorting a list in section we will study the number of operations used by these algorithms in particular we will estimate the number of comparisons used by the linear and binary search algorithms to find an element in a sequence of n elements we will also estimate the number of comparisons used by the bubble sort and by the insertion sort to sort a list of n elements the time required to solve a problem depends on more than only the number of operations it uses the time also depends on the hardware and software used to run the program that implements the algorithm however when we change the hardware and software used to implement an algorithm we can closely approximate the time required to solve a problem of size n by multiplying the previous time required by a constant for example on a supercomputer we might be able to solve a problem of size n a million times faster than we can on a pc however this factor of one million will not depend on n except perhaps in some minor ways one of the advantages of using big o notation which we introduce in this section is that we can estimate the growth of a function without worrying about constant multipliers or smaller order terms this means that using big o notation we do not have to worry about the hardware and software used to implement an algorithm furthermore using big o notation we can assume that the different operations used in an algorithm take the same time which simplifies the analysis considerably big o notation is used extensively to estimate the number of operations an algorithm uses as its input grows with the help of this notation we can determine whether it is practical to use a particular algorithm to solve a problem as the size of the input increases furthermore using big o notation we can compare two algorithms to determine which is more efficient as the size of the input grows for instance if we have two algorithms for solving a problem one using operations and the other using operations big o notation can help us see that the first algorithm uses far fewer operations when n is large even though it uses more operations for small values of n such as n this section introduces big o notation and the related big omega and big theta notations we will explain how big o big omega and big theta estimates are constructed and establish estimates for some important functions that are used in the analysis of algorithms big o notation the growth of functions is often described using a special notation definition describes this notation definition remark intuitively the definition that f x is o g x says that f x grows slower that some fixed multiple of g x as x grows without bound the constants c and k in the definition of big o notation are called witnesses to the relationship f x is o g x to establish that f x is o g x we need only one pair of witnesses to this relationship that is to show that f x is o g x we need find only one pair of constants c and k the witnesses such that f x c g x whenever x k note that when there is one pair of witnesses to the relationship f x is o g x there are infinitely many pairs of witnesses to see this note that if c and k are one pair of witnesses then any pair ct and kt where c ct and k kt is also a pair of witnesses because f x c g x ct g x whenever x kt k the history of big o notation big o notation has been used in mathematics for more than a century in computer science it is widely used in the analysis of algorithms as will be seen in section the german mathematician paul bachmann first introduced big o notation in in an important book on number theory the big o symbol is sometimes called a landau symbol after the german mathematician edmund landau who used this notation throughout his work the use of big o notation in computer science was popularized by donald knuth who also introduced the big q and big notations defined later in this section working withthe definition of big o notation a useful approach for find ing a pair of witnesses is to first select a value of k for which the size of f x can be readily estimated when x k and to see whether we can use this estimate to find a value of c for which f x c g x for x k this approach is illustrated in example example show that f x is o solution we observe that we can readily estimate the size of f x when x because x and when x it follows that whenever x as shown in figure consequently we can take c and k as witnesses to show that f x is o that is f x whenever x note that it is not necessary to use absolute values here because all functions in these equalities are positive when x is positive alternatively we can estimate the size of f x when x when x we have and consequently if x we have it follows that c and k are also witnesses to the relation f x is o the part of the graph of f x x x that satisfies f x x is shown in blue figure the function is o observe that in the relationship f x is o can be replaced by any function with larger values than for example f x is o f x is o x and so on it is also true that is o because whenever x this means that c and k are witnesses to the relationship is o note that in example we have two functions f x and g x such that f x is o g x and g x is o f x the latter fact following from the inequality which holds for all nonnegative real numbers x we say that two func tions f x and g x that satisfy both of these big o relationships are of the same order we will return to this notion later in this section remark the fact that f x is o g x is sometimes written f x o g x however the equals sign in this notation does not represent a genuine equality rather this notation tells us that an inequality holds relating the values of the functions f and g for sufficiently large numbers in the domains of these functions however it is acceptable to write f x o g x because o g x represents the set of functions that are o g x when f x is o g x and h x is a function that has larger absolute values than g x does for sufficiently large values of x it follows that f x is o h x in other words the function g x in the relationship f x is o g x can be replaced by a function with larger absolute values to see this note that if f x c g x if x k and if h x g x for all x k then f x c h x if x k hence f x is o h x when big o notation is used the function g in the relationship f x is o g x is chosen to be as small as possible sometimes from a set of reference functions such as functions of the form xn where n is a positive integer paul gustav heinrich bachmann paul bachmann the son of a lutheran pastor shared his father pious lifestyle and love of music his mathematical talent was discovered by one of his teachers even though he had difficulties with some of his early mathematical studies after recuperating from tuberculosis in switzerland bachmann studied mathematics first at the university of berlin and later at göttingen where he attended lectures presented by the famous number theorist dirichlet he received his doctorate under the german number theorist kummer in his thesis was on group theory bachmann was a professor at breslau and later at münster after he retired from his professorship he continued his mathematical writing played the piano and served as a music critic for newspapers bachmann mathematical writings include a five volume survey of results and methods in number theory a two volume work on elementary number theory a book on irrational numbers and a book on the famous conjecture known as fermat last theorem he introduced big o notation in his book analytische zahlentheorie edmund landau edmund landau the son of a berlin gynecologist attended high school and university in berlin he received his doctorate in under the direction of frobenius landau first taught at the university of berlin and then moved to göttingen where he was a full professor until the nazis forced him to stop teaching landau main contributions to mathematics were in the field of analytic number theory in particular he established several important results concerning the distribution of primes he authored a three volume exposition on number theory as well as other books on number theory and mathematical analysis cg x the part of the graph of f x that satisfies x cg x is shown in color k figure the function f x is o g x in subsequent discussions we will almost always deal with functions that take on only positive values all references to absolute values can be dropped when working with big o estimates for such functions figure illustrates the relationship f x is o g x example illustrates how big o notation is used to estimate the growth of functions example show that is o solution note that when x we have we can obtain this inequality by multiplying both sides of x by consequently we can take c and k as witnesses to establish donald e knuth born knuth grew up in milwaukee where his father taught bookkeeping at a lutheran high school and owned a small printing business he was an excellent student earning academic achievement awards he applied his intelligence in unconventional ways winning a contest when he was in the eighth grade by finding over words that could be formed from the letters in ziegler giant bar this won a television set for his school and a candy bar for everyone in his class knuth had a difficult time choosing physics over music as his major at the case institute of technology he then switched from physics to mathematics and in he received his bachelor of science degree simultane ously receiving a master of science degree by a special award of the faculty who considered his work outstanding at case he managed the basketball team and applied his talents by constructing a formula for the value of each player this novel approach was covered by newsweek and by walter cronkite on the cbs television network knuth began graduate work at the california institute of technology in and received his ph d there in during this time he worked as a consultant writing compilers for different computers knuth joined the staff of the california institute of technology in where he remained until when he took a job as a full professor at stanford university he retired as professor emeritus in to concentrate on writing he is especially interested in updating and completing new volumes of his series the art of computer programming a work that has had a profound influence on the development of computer science which he began writing as a graduate student in focusing on compilers in common jargon knuth referring to the art of computer programming has come to mean the reference that answers all questions about such topics as data structures and algorithms knuth is the founder of the modern study of computational complexity he has made fundamental contributions to the subject of compilers his dissatisfaction with mathematics typography sparked him to invent the now widely used tex and metafont systems tex has become a standard language for computer typography two of the many awards knuth has received are the turing award and the national medal of technology awarded to him by president carter knuth has written for a wide range of professional journals in computer science and in mathematics however his first publication in when he was a college freshman was a parody of the metric system called the potrzebie systems of weights and measures which appeared in mad magazine and has been in reprint several times he is a church organist as his father was he is also a composer of music for the organ knuth believes that writing computer programs can be an aesthetic experience much like writing poetry or composing music knuth pays for the first person to find each error in his books and for significant suggestions if you send him a letter with an error you will need to use regular mail because he has given up reading e mail he will eventually inform you whether you were the first person to tell him about this error be prepared for a long wait because he receives an overwhelming amount of mail the author received a letter years after sending an error report to knuth noting that this report arrived several months after the first report of this error the relationship is o alternatively when x we have so that c and k are also witnesses to the relationship is o example illustrates how to show that a big o relationship does not hold example show that is not o n solution to show that is not o n we must show that no pair of witnesses c and k exist such that cn whenever n k we will use a proof by contradiction to show this suppose that there are constants c and k for which cn whenever n k observe that when n we can divide both sides of the inequality cn by n to obtain the equivalent inequality n c however no matter what c and k are the inequality n c cannot hold for all n with n k in particular once we set a value of k we see that when n is larger than the maximum of k and c it is not true that n c even though n k this contradiction shows that in not o n example example shows that is o is it also true that is o solution to determine whether is o we need to determine whether witnesses c and k exist so that c whenever x k we will show that no such witnesses exist using a proof by contradiction if c and k are witnesses the inequality c holds for all x k observe that the inequality c is equivalent to the inequality x which follows by dividing both sides by the positive quantity however no matter what c is it is not the case that x for all x k no matter what k is because x can be made arbitrarily large it follows that no witnesses c and k exist for this proposed big o relationship hence is not o big o estimates for some important functions polynomials can often be used to estimate the growth of functions instead of analyzing the growth of polynomials each time they occur we would like a result that can always be used to estimate the growth of a polynomial theorem does this it shows that the leading term of a polynomial dominates its growth by asserting that a polynomial of degree n or less is o xn theorem proof using the triangle inequality see exercise in section if x we have f x anxn an an xn an xn x xn an an x xn xn xn an an this shows that f x cxn where c an an whenever x hence the witnesses c an an and k show that f x is o xn we now give some examples involving functions that have the set of positive integers as their domains example how can big o notation be used to estimate the sum of the first n positive integers solution because each of the integers in the sum of the first n positive integers does not exceed n it follows that n n n n from this inequality it follows that n is o taking c and k as witnesses in this example the domains of the functions in the big o relationship are the set of positive integers in example big o estimates will be developed for the factorial function and its loga rithm these estimates will be important in the analysis of the number of steps used in sorting procedures example give big o estimates for the factorial function and the logarithm of the factorial function where the factorial function f n n is defined by n n whenever n is a positive integer and for example note that the function n grows rapidly for instance 902 solution a big o estimate for n can be obtained by noting that each term in the product does not exceed n hence n n n n n n nn this inequality shows that n is o nn taking c and k as witnesses taking logarithms of both sides of the inequality established for n we obtain log n log nn n log n this implies that log n is o n log n again taking c and k as witnesses example in section we will show that n whenever n is a positive integer show that this inequality implies that n is o and use this inequality to show that log n is o n solution using the inequality n we quickly can conclude that n is o by taking k c as witnesses note that because the logarithm function is increasing taking logarithms base of both sides of this inequality shows that log n n it follows that log n is o n again we take c k as witnesses if we have logarithms to a base b where b is different from we still have logb n is o n because log log n n n whenever n is a positive integer we take c log b and k as witnesses we have used theorem in appendix to see that logb n log n log b as mentioned before big o notation is used to estimate the number of operations needed to solve a problem using a specified procedure or algorithm the functions used in these estimates often include the following log n n n log n n using calculus it can be shown that each function in the list is smaller than the succeeding function in the sense that the ratio of a function and the succeeding function tends to zero as n grows without bound figure displays the graphs of these functions using a scale for the values of the functions that doubles for each successive marking on the graph that is the vertical scale in this graph is logarithmic figure a display of the growth of functions commonly used in big o estimates useful big o estimates involving logarithms powers and exponen tial functions we now give some useful facts that help us determine whether big o relationships hold between pairs of functions when each of the functions is a power of a loga rithm a power or an exponential function of the form bn where b their proofs are left as exercises for readers skilled with calculus theorem shows that if f n is a polynomial of degree d then f n is o nd applying this theorem we see that if d c then nc is o nd we leave it to the reader to show that the reverse of this relationship does not hold putting these facts together we see that if d c then nc is o nd but nd is not o nc in example we showed that logb n is o n whenever b more generally whenever b and c and d are positive we have logb n c is o nd but nd is not o logb n c this tells us that every positive power of the logarithm of n to the base b where b is big o of every positive power of n but the reverse relationship never holds in example we also showed that n is o more generally whenever d is positive and b we have nd is o bn but bn is not o nd this tells us that every power of n is big o of every exponential function of n with a base that is greater than one but the reverse relationship never holds furthermore we have when c b bn is o cn but cn is not o bn this tells us that if we have two exponential functions with different bases greater than one one of these functions is big o of the other if and only if its base is smaller or equal the growth of combinations of functions many algorithms are made up of two or more separate subprocedures the number of steps used by a computer to solve a problem with input of a specified size using such an algorithm is the sum of the number of steps used by these subprocedures to give a big o estimate for the number of steps needed it is necessary to find big o estimates for the number of steps used by each subprocedure and then combine these estimates big o estimates of combinations of functions can be provided if care is taken when different big o estimates are combined in particular it is often necessary to estimate the growth of the sum and the product of two functions what can be said if big o estimates for each of two functions are known to see what sort of estimates hold for the sum and the product of two functions suppose that x is o x and x is o x from the definition of big o notation there are constants and such that x x when x and x x when x to estimate the sum of x and x note that x x x x x using the triangle inequality a b a b when x is greater than both and it follows from the inequalities for x and x that theorem x x x x g x g x g x c g x where c and g x max x x here max a b denotes the maximum or larger of a and b this inequality shows that x c g x whenever x k where k max we state this useful result as theorem we often have big o estimates for and in terms of the same function g in this situation theorem can be used to show that x is also o g x because max g x g x g x this result is stated in corollary corollary theorem in a similar way big o estimates can be derived for the product of the functions and when x is greater than max it follows that x x x x x x c x where c from this inequality it follows that x x is o x because there are constants c and k namely c and k max such that x c x x whenever x k this result is stated in theorem the goal in using big o notation to estimate functions is to choose a function g x as simple as possible that grows relatively slowly so that f x is o g x examples and illustrate how to use theorems and to do this the type of analysis given in these examples is often used in the analysis of the time used to solve problems using computer programs example give a big o estimate for f n log n log n where n is a positive integer solution first the product log n will be estimated from example we know that log n is o n log n using this estimate and the fact that is o n theorem gives the estimate that log n is o log n next the product log n will be estimated because when n it follows that is o thus from theorem it follows that log n is o log n using theorem to combine the two big o estimates for the products shows that f n log n log n is o log n example give a big o estimate for f x x log solution first a big o estimate for x log will be found note that x is o x furthermore when x hence log log log log log log x log x if x this shows that log is o log x from theorem it follows that x log is o x log x because is o theorem tells us that f x is o max x log x because x log x for x it follows that f x is o q and are the greek uppercase letters omega and theta respectively big omega and big theta notation big o notation is used extensively to describe the growth of functions but it has limitations in particular when f x is o g x we have an upper bound in terms of g x for the size of f x for large values of x however big o notation does not provide a lower bound for the size of f x for large x for this we use big omega big q notation when we want to give both an upper and a lower bound on the size of a function f x relative to a reference function g x we use big theta big notation both big omega and big theta notation were introduced by donald knuth in the his motivation for introducing these notations was the common misuse of big o notation when both an upper and a lower bound on the size of a function are needed we now define big omega notation and illustrate its use after doing so we will do the same for big theta notation definition there is a strong connection between big o and big omega notation in particular f x is q g x if and only if g x is o f x we leave the verification of this fact as a straightforward exercise for the reader example the function f x is q g x where g x is the function g x this is easy to see because f x for all positive real numbers x this is equivalent to saying that g x is o which can be established directly by turning the inequality around often it is important to know the order of growth of a function in terms of some relatively simple reference function such as xn when n is a positive integer or cx where c knowing the order of growth requires that we have both an upper bound and a lower bound for the size of the function that is given a function f x we want a reference function g x such that f x is o g x and f x is q g x big theta notation defined as follows is used to express both of these relationships providing both an upper and a lower bound on the size of a function definition when f x is g x it is also the case that g x is f x also note that f x is g x if and only if f x is o g x and g x is o f x see exercise furthermore note that f x is g x if and only if there are real numbers and and a positive real number k such that g x f x g x whenever x k the existence of the constants and k tells us that f x is q g x and that f x is o g x respectively usually when big theta notation is used the function g x in g x is a relatively simple reference function such as xn cx log x and so on while f x can be relatively complicated example we showed in example that the sum of the first n positive integers is o is this sum of order solution let f n n because we already know that f n is o to show that f n is of order we need to find a positive constant c such that f n for sufficiently large integers n to obtain a lower bound for this sum we can ignore the first half of the terms summing only the terms greater than n we find that n n n n n n n n n n n n this shows that f n is q we conclude that f n is of order or in symbols f n is example show that log x is solution because log x it follows that log x for x consequently log x is o clearly is o log x consequently log x is one useful fact is that the leading term of a polynomial determines its order for example if f x then f x is of order this is stated in theorem whose proof is left as exercise theorem example the polynomials 1444 and are of orders and respectively unfortunately as knuth observed big o notation is often used by careless writers and speakers as if it had the same meaning as big theta notation keep this in mind when you see big o notation used the recent trend has been to use big theta notation whenever both upper and lower bounds on the size of a function are needed exercises in exercises to establish a big o relationship find wit nesses c and k such that f x c g x whenever x k determine whether each of these functions is o x a f x b f x c f x x d f x log x e f x lxj f f x x determine whether each of these functions is o a f x b f x c f x x log x d f x e f x f f x lxj use the definition of f x is o g x to show that is o use the definition of f x is o g x to show that is o show that x log x is o but that is not o x log x show that is o but that is not o note that this is a special case of exercise determine whether is o g x for each of these func tions g x a g x b g x c g x d g x e g x f g x explain what it means for a function to be o show that if f x is o x then f x is o suppose that f x g x and h x are functions such that f x is o g x and g x is o h x show that f x is o h x let k be a positive integer show that nk is o nk show that x is o x show that is o find the least integer n such that f x is o xn for each of these functions a f x log x determine whether each of the functions and is o determine whether each of the functions log n and log is o log n arrange the functions n log n n log n and in a list so that each function is big o b f x log x c f x of the next function arrange the function n log n n log n d f x log x find the least integer n such that f x is o xn for each of these functions a f x log x b f x log x c f x d f x log x show that is o but that is not o show that is o but that is not o show that is o and is o n and in a list so that each function is big o of the next function suppose that you have two different algorithms for solv ing a problem to solve a problem of size n the first algorithm uses exactly n log n operations and the sec ond algorithm uses exactly operations as n grows which algorithm uses fewer operations suppose that you have two different algorithms for solv ing a problem to solve a problem of size n the first algorithm uses exactly operations and the second algorithm uses exactly n operations as n grows which algorithm uses fewer operations give as good a big o estimate as possible for each of these functions a n b n log n c n log give a big o estimate for each of these functions for the function g in your estimate f x is o g x use a simple function g of smallest order a log n log n log n b c nn n give a big o estimate for each of these functions for the function g in your estimate that f x is o g x use a simple function g of the smallest order a n log log n b n log n log n c nn for each function in exercise determine whether that function is q x and whether it is x for each function in exercise determine whether that function is q and whether it is show that each of these pairs of functions are of the same order a x b x c lx x d log x e x x show that f x is g x if and only if f x is o g x and g x is o f x show that if f x and g x are functions from the set of real numbers to the set of real numbers then f x is o g x if and only if g x is q f x show that if f x and g x are functions from the set of real numbers to the set of real numbers then f x is g x if and only if there are positive constants k and c such that c g x f x c g x when show that if f and g are real valued functions such that f x is o g x then for every positive integer n fn x is o gn x note that fn x f x n show that for all real numbers a and b with a and b if f x is o logb x then f x is o loga x suppose that f x is o g x where f and g are in creasing and unbounded functions show that log f x is o log g x suppose that f x is o g x does it follow that x is o x let x and x be functions from the set of real numbers to the set of positive real numbers show that if x and x are both g x where g x is a func tion from the set of real numbers to the set of positive real numbers then x x is g x is this still true if x and x can take negative values suppose that f x g x and h x are functions such that f x is g x and g x is h x show that f x is h x if x and x are functions from the set of positive integers to the set of positive real numbers and x and x are both g x is x also g x either prove that it is or give a counterexample show that if x and x are functions from the set of positive integers to the set of real numbers and x is x and x is x then x is x find functions f and g from the set of positive integers to the set of real numbers such that f n is not o g n and g n is not o f n express the relationship f x is q g x using a picture show the graphs of the functions f x and cg x as well as the constant k on the real axis show that if x is x x is x and x and x for all real numbers x then x is x show that if f x anxn an where an and an are real numbers and an then f x is xn ever x k a show that x is by directly finding the constants k and in exercise b express the relationship in part a using a picture showing the functions x and and the constant k on the x axis where and k are the constants you found in part a big o big theta and big omega notation can be extended to functions in more than one variable for example the state ment f x y is o g x y means that there exist constants c and such that f x y c g x y whenever x and y define the statement f x y is g x y define the statement f x y is q g x y to show that x is show that xy x log y is o express the relationship f x is g x using a picture show that is q show the graphs of the functions f x g x and g x as well as the constant k on the x axis explain what it means for a function to be q explain what it means for a function to be give a big o estimate of the product of the first n odd positive integers show that lxyj is o xy show that is q xy requires calculus show that if c d then nd is o nc but nc is not o nd requires calculus show that if b and c and d are positive then logb n c is o nd but nd is not o logb n c requires calculus show that if d is positive and b then nd is o bn but bn is not o nd requires calculus show that if c b then bn is o cn but cn is not o bn the following problems deal with another type of asymptotic notation called little o notation because little o notation is based on the concept of limits a knowledge of calculus is needed for these problems we say that f x is o g x read f x is little oh of g x when f x requires calculus show that if x is o g x and x is o g x then x x is o g x requires calculus let hn be the nth harmonic number hn n show that hn is o log n hint first establish the in equality lim x g x n dx requires calculus show that a is o b x log x is o j x j c is o d x is not o requires calculus a show that if f x and g x are functions such that f x is o g x and c is a constant then cf x is o g x where cf x cf x show that if x x and g x are functions such that x is o g x and x is o g x then x is o g x where x x x requires calculus represent pictorially that x log x is o by graphing x log x and x log x explain how this picture shows that x log x is o requires calculus express the relationship f x is o g x using a picture show the graphs of f x g x and f x g x requires calculus suppose that f x is o g x does it follow that x is o x requires calculus suppose that f x is o g x does it follow that log f x is o log g x requires calculus the two parts of this exercise describe the relationship between little o and big o notation a show that if f x and g x are functions such that f x is o g x then f x is o g x b show that if f x and g x are functions such that f x is o g x then it does not necessarily follow that f x is o g x requires calculus show that if f x is a polynomial of by showing that the sum of the areas of the rectangles of height j with base from j to j for j n is less than the area under the curve y x from to n show that n log n is o log n determine whether log n is n log n justify your an swer show that log n is greater than n log n for n hint begin with the inequality n n n n n let f x and g x be functions from the set of real num bers to the set of real numbers we say that the func tions f and g are asymptotic and write f x g x if limx f x g x requires calculus for each of these pairs of functions determine whether f and g are asymptotic a f x g x b f x log x g x c f x log g x 17x d f x x g x x requires calculus for each of these pairs of functions determine whether f and g are asymptotic a f x log g x log x b f x g x c f x g x degree n and g x is a polynomial of degree m where m n then f x is o g x d f x x g x complexity of algorithms introduction when does an algorithm provide a satisfactory solution to a problem first it must always produce the correct answer how this can be demonstrated will be discussed in chapter second it should be efficient the efficiency of algorithms will be discussed in this section how can the efficiency of an algorithm be analyzed one measure of efficiency is the time used by a computer to solve a problem using the algorithm when input values are of a specified size a second measure is the amount of computer memory required to implement the algorithm when input values are of a specified size questions such as these involve the computational complexity of the algorithm an analysis of the time required to solve a problem of a particular size involves the time complexity of the algorithm an analysis of the computer memory required involves the space complexity of the algorithm considerations of the time and space complexity of an algorithm are essential when algorithms are implemented it is obviously important to know whether an algorithm will produce an answer in a microsecond a minute or a billion years likewise the required memory must be available to solve a problem so that space complexity must be taken into account considerations of space complexity are tied in with the particular data structures used to implement the algorithm because data structures are not dealt with in detail in this book space complexity will not be considered we will restrict our attention to time complexity time complexity the time complexity of an algorithm can be expressed in terms of the number of operations used by the algorithm when the input has a particular size the operations used to measure time complexity can be the comparison of integers the addition of integers the multiplication of integers the division of integers or any other basic operation time complexity is described in terms of the number of operations required instead of actual computer time because of the difference in time needed for different computers to perform basic operations moreover it is quite complicated to break all operations down to the basic bit oper ations that a computer uses furthermore the fastest computers in existence can perform basic bit operations for instance adding multiplying comparing or exchanging two bits in second picoseconds but personal computers may require second nanoseconds which is times as long to do the same operations we illustrate how to analyze the time complexity of an algorithm by considering algorithm of section which finds the maximum of a finite set of integers example describe the time complexity of algorithm of section for finding the maximum element in a finite set of integers solution the number of comparisons will be used as the measure of the time complexity of the algorithm because comparisons are the basic operations used to find the maximum element of a set with n elements listed in an arbitrary order the temporary maximum is first set equal to the initial term in the list then after a comparison i n has been done to determine that the end of the list has not yet been reached the temporary maximum and second term are compared updating the temporary maximum to the value of the second term if it is larger this procedure is continued using two additional comparisons for each term of the list one i n to determine that the end of the list has not been reached and another max ai to determine whether to update the temporary maximum because two comparisons are used for each of the second through the nth elements and one more comparison is used to exit the loop when i n exactly n comparisons are used whenever this algorithm is applied hence the algorithm for finding the maximum of a set of n elements has time complexity n measured in terms of the number of comparisons used note that for this algorithm the number of comparisons is independent of particular input of n numbers next we will analyze the time complexity of searching algorithms example describe the time complexity of the linear search algorithm specified as algortihm in section solution the number of comparisons used by algorithm in section will be taken as the measure of the time complexity at each step of the loop in the algorithm two comparisons are performed one i n to see whether the end of the list has been reached and one x ai to compare the element x with a term of the list finally one more comparison i n is made outside the loop consequently if x ai comparisons are used the most comparisons are required when the element is not in the list in this case comparisons are used to determine that x is not ai for i n an additional comparison is used to exit the loop and one comparison is made outside the loop so when x is not in the list a total of comparisons are used hence a linear search requires n comparisons in the worst case because is n worst case complexity the type of complexity analysis done in example is a worst case analysis by the worst case performance of an algorithm we mean the largest number of operations needed to solve the given problem using this algorithm on input of specified size worst case analysis tells us how many operations an algorithm requires to guarantee that it will produce a solution example describe the time complexity of the binary search algorithm specified as algorithm in section in terms of the number of comparisons used and ignoring the time required to compute m l i j in each iteration of the loop in the algorithm solution for simplicity assume there are n elements in the list an where k is a nonnegative integer note that k log n if n the number of elements in the list is not a power of the list can be considered part of a larger list with elements where n here is the smallest power of larger than n at each stage of the algorithm i and j the locations of the first term and the last term of the restricted list at that stage are compared to see whether the restricted list has more than one term if i j a comparison is done to determine whether x is greater than the middle term of the restricted list at the first stage the search is restricted to a list with terms so far two comparisons have been used this procedure is continued using two comparisons at each stage to restrict the search to a list with half as many terms in other words two comparisons are used at the first stage of the algorithm when the list has elements two more when the search has been reduced to a list with elements two more when the search has been reduced to a list with elements and so on until two comparisons are used when the search has been reduced to a list with elements finally when one term is left in the list one comparison tells us that there are no additional terms left and one more comparison is used to determine if this term is x hence at most log n comparisons are required to perform a binary search when the list being searched has elements if n is not a power of the original list is expanded to a list with terms where k log n and the search requires at most log n comparisons it follows that in the worst case binary search requires o log n comparisons note that in the worst case log n comparisons are used by the binary search hence the binary search uses log n comparisons in the worst case because log n log n from this analysis it follows that in the worst case the binary search algorithm is more efficient than the linear search algorithm because we know by example that the linear search algorithm has n worst case time complexity average case complexity another important type of complexity analysis besides worst case analysis is called average case analysis the average number of operations used to solve the problem over all possible inputs of a given size is found in this type of analysis average case time complexity analysis is usually much more complicated than worst case analysis however the average case analysis for the linear search algorithm can be done without difficulty as shown in example example describe the average case performance of the linear search algorithm in terms of the average number of comparisons used assuming that the integer x is in the list and it is equally likely that x is in any position solution by hypothesis the integer x is one of the integers an in the list if x is the first term of the list three comparisons are needed one i n to determine whether the end of the list has been reached one x ai to compare x and the first term and one i n outside the loop if x is the second term of the list two more comparisons are needed so that a total of five comparisons are used in general if x is the ith term of the list ai two comparisons will be used at each of the i steps of the loop and one outside the loop so that a total of comparisons are needed hence the average number of comparisons used equals n n using the formula from line of table in section and see exercise b of section n n n hence the average number of comparisons used by the linear search algorithm when x is known to be in the list is n n n which is n remark in the analysis in example we assumed that x is in the list being searched it is also possible to do an average case analysis of this algorithm when x may not be in the list see exercise remark although we have counted the comparisons needed to determine whether we have reached the end of a loop these comparisons are often not counted from this point on we will ignore such comparisons worst case complexity of two sorting algorithms we analyze the worst case complexity of the bubble sort and the insertion sort in examples and example what is the worst case complexity of the bubble sort in terms of the number of comparisons made solution the bubble sort described before example in section sorts a list by performing a sequence of passes through the list during each pass the bubble sort successively compares adjacent elements interchanging them if necessary when the ith pass begins the i largest elements are guaranteed to be in the correct positions during this pass n i comparisons are used consequently the total number of comparisons used by the bubble sort to order a list of n elements is n n n n using a summation formula from line in table in section and exercise b in section note that the bubble sort always uses this many comparisons because it con tinues even if the list becomes completely sorted at some intermediate step consequently the bubble sort uses n n comparisons so it has worst case complexity in terms of the number of comparisons used example what is the worst case complexity of the insertion sort in terms of the number of comparisons made solution the insertion sort described in section inserts the j th element into the correct position among the first j elements that have already been put into the correct order it does this by using a linear search technique successively comparing the j th element with successive terms until a term that is greater than or equal to it is found or it compares aj with itself and stops because aj is not less than itself consequently in the worst case j comparisons are required to insert the j th element into the correct position therefore the total number of comparisons used by the insertion sort to sort a list of n elements is n n n using the summation formula for the sum of consecutive integers in line of table of section and see exercise b of section and noting that the first term is missing in this sum note that the insertion sort may use considerably fewer comparisons if the smaller elements started out at the end of the list we conclude that the insertion sort has worst case complexity in examples and we showed that both the bubble sort and the insertion sort have worst case time complexity however the most efficient sorting algorithms can sort n items in o n log n time as we will show in sections and using techniques we develop in those sections from this point on we will assume that sorting n items can be done in o n log n time complexity of matrix multiplication the definition of the product of two matrices can be expressed as an algorithm for computing the product of two matrices suppose that c cij is the m n matrix that is the product of the m k matrix a aij and the k n matrix b bij the algorithm based on the definition of the matrix product is expressed in pseudocode in algorithm we can determine the complexity of this algorithm in terms of the number of additions and multiplications used example how many additions of integers and multiplications of integers are used by algorithm to multiply two n n matrices with integer entries solution there are entries in the product of a and b to find each entry requires a total of n multiplications and n additions hence a total of multiplications and n additions are used surprisingly there are more efficient algorithms for matrix multiplication than that given in algorithm as example shows multiplying two n n matrices directly from the definition requires o multiplications and additions using other algorithms two n n matrices can be multiplied using o n multiplications and additions details of such algorithms can be found in we can also analyze the complexity of the algorithm we described in chapter for computing the boolean product of two matrices which we display as algorithm the number of bit operations used to find the boolean product of two n n matrices can be easily determined example how many bit operations are used to find a b where a and b are n n zero one matrices solution there are entries in a b using algorithm a total of n ors and n ands are used to find an entry of a b hence bit operations are used to find each entry therefore bit operations are required to compute a b using algorithm matrix chain multiplication there is another important problem involving the complexity of the multiplication of matrices how should the matrix chain an be com puted using the fewest multiplications of integers where an are mn mn matrices respectively and each has integers as entries because matrix multiplication is associative as shown in exercise in section the order of the mul tiplication used does not change the product note that multiplications of integers are performed to multiply an matrix and an matrix using algorithm example illustrates this problem example in which order should the matrices and where is is and is all with integer entries be multiplied to use the least number of multiplications of integers solution there are two possible ways to compute these are and if and are first multiplied a total of multiplications of inte gers are used to obtain the matrix then to multiply and requires multiplications hence a total of 6000 multiplications are used on the other hand if and are first multiplied then multiplications are used to obtain the matrix then to multiply and requires multiplications hence a total of multiplications are used clearly the first method is more efficient we will return to this problem in exercise in section algorithms for determining the most efficient way to carry out matrix chain multiplication are discussed in algorithmic paradigms in section we introduced the basic notion of an algorithm we provided examples of many different algorithms including searching and sorting algorithms we also introduced the concept of a greedy algorithm giving examples of several problems that can be solved by greedy algo rithms greedy algorithms provide an example of an algorithmic paradigm that is a general approach based on a particular concept that can be used to construct algorithms for solving a variety of problems in this book we will construct algorithms for solving many different problems based on a variety of algorithmic paradigms including the most widely used algorithmic paradigms these paradigms can serve as the basis for constructing efficient algorithms for solving a wide range of problems some of the algorithms we have already studied are based on an algorithmic paradigm known as brute force which we will describe in this section algorithmic paradigms studied later in this book include divide and conquer algorithms studied in chapter dynamic programming also studied in chapter backtracking studied in chapter and probabilistic algorithms studied in chapter there are many important algorithmic paradigms besides those described in this book consult books on algorithm design such as to learn more about them brute force algorithms brute force is an important and basic algorithmic paradigm in a brute force algorithm a problem is solved in the most straightforward manner based on the statement of the problem and the definitions of terms brute force algorithms are designed to solve problems without regard to the computing resources required for example in some brute force algorithms the solution to a problem is found by examining every possible solution looking for the best possible in general brute force algorithms are naive approaches for solving problems that do not take advantage of any special structure of the problem or clever ideas note that algorithm in section for finding the maximum number in a sequence is a brute force algorithm because it examines each of the n numbers in a sequence to find the maximum term the algorithm for finding the sum of n numbers by adding one additional number at a time is also a brute force algorithm as is the algorithm for matrix multiplication based on its definition algorithm the bubble insertion and selection sorts described in section in algorithms and and in exercise respectively are also considered to be brute force algorithms all three of these sorting algorithms are straightforward approaches much less efficient than other sorting algorithms such as the merge sort and the quick sort discussed in chapters and although brute force algorithms are often inefficient they are often quite useful a brute force algorithm may be able to solve practical instances of problems particularly when the input is not too large even if it is impractical to use this algorithm for larger inputs furthermore when designing new algorithms to solve a problem the goal is often to find a new algorithm that is more efficient than a brute force algorithm one such problem of this type is described in example example construct a brute force algorithm for finding the closest pair of points in a set of n points in the plane and provide a worst case big o estimate for the number of bit operations used by the algorithm solution suppose that we are given as input the points y2 xn yn recall that the distance between xi yi and xj yj is xj xi yj yi a brute force algo rithm can find the closest pair of these points by computing the distances between all pairs of the n points and determining the smallest distance we can make one small simplification to make the computation easier we can compute the square of the distance between pairs of points to find the closest pair rather than the distance between these points we can do this because the square of the distance between a pair of points is smallest when the distance between these points is smallest to estimate the number of operations used by the algorithm first note that there are n n pairs of points xi yi xj yj that we loop through as the reader should verify for each such pair we compute xj xi yj yi compare it with the current value of min and if it is smaller than min replace the current value of min by this new value it follows that this algorithm uses operations in terms of arithmetic operations and comparisons in chapter we will devise an algorithm that determines the closest pair of points when given n points in the plane as input that has o n log n worst case complexity the original discovery of such an algorithm much more efficient than the brute force approach was considered quite surprising understanding the complexity of algorithms table displays some common terminology used to describe the time complexity of algorithms for example an algorithm that finds the largest of the first terms of a list of n elements by applying algorithm to the sequence of the first terms where n is an integer with n has constant complexity because it uses comparisons no matter what n is as the reader can verify the linear search algorithm has linear worst case or average case complexity and the binary search algorithm has logarithmic worst case complexity many important algorithms have n log n or linearithmic worst case complexity such as the merge sort which we will introduce in chapter the word linearithmic is a combination of the words linear and logarithmic table commonly used terminology for the complexity of algorithms complexity terminology constant complexity log n logarithmic complexity n linear complexity n log n linearithmic complexity nb polynomial complexity bn where b exponential complexity n factorial complexity an algorithm has polynomial complexity if it has complexity nb where b is an integer with b for example the bubble sort algorithm is a polynomial time algorithm because it uses comparisons in the worst case an algorithm has exponential complexity if it has time complexity bn where b the algorithm that determines whether a compound proposition in n variables is satisfiable by checking all possible assignments of truth variables is an algorithm with exponential complexity because it uses operations finally an algorithm has factorial complexity if it has n time complexity the algorithm that finds all orders that a traveling salesperson could use to visit n cities has factorial complexity we will discuss this algorithm in chapter tractability a problem that is solvable using an algorithm with polynomial worst case complexity is called tractable because the expectation is that the algorithm will produce the solution to the problem for reasonably sized input in a relatively short time however if the polynomial in the big estimate has high degree such as degree or if the coefficients are extremely large the algorithm may take an extremely long time to solve the problem consequently that a problem can be solved using an algorithm with polynomial worst case time complexity is no guarantee that the problem can be solved in a reasonable amount of time for even relatively small input values fortunately in practice the degree and coefficients of polynomials in such estimates are often small the situation is much worse for problems that cannot be solved using an algorithm with worst case polynomial time complexity such problems are called intractable usually but not always an extremely large amount of time is required to solve the problem for the worst cases of even small input values in practice however there are situations where an algorithm with a certain worst case time complexity may be able to solve a problem much more quickly for most cases than for its worst case when we are willing to allow that some perhaps small number of cases may not be solved in a reasonable amount of time the average case time complexity is a better measure of how long an algorithm takes to solve a problem many problems important in industry are thought to be intractable but can be practically solved for essentially all sets of input that arise in daily life another way that intractable problems are handled when they arise in practical applications is that instead of looking for exact solutions of a problem approximate solutions are sought it may be the case that fast algorithms exist for finding such approximate so lutions perhaps even with a guarantee that they do not differ by very much from an exact solution some problems even exist for which it can be shown that no algorithm exists for solving them such problems are called unsolvable as opposed to solvable problems that can be solved using an algorithm the first proof that there are unsolvable problems was provided by the great english mathematician and computer scientist alan turing when he showed that the halting problem is unsolvable recall that we proved that the halting problem is unsolvable in section a biography of alan turing and a description of some of his other work can be found in chapter p versus np the study of the complexity of algorithms goes far beyond what we can describe here note however that many solvable problems are believed to have the property that no algorithm with polynomial worst case time complexity solves them but that a solution if known can be checked in polynomial time problems for which a solution can be checked in polynomial time are said to belong to the class np tractable problems are said to belong to class p the abbreviation np stands for nondeterministic polynomial time the satisfiability problem discussed in section is an example of an np problem we can quickly verify that an assignment of truth values to the variables of a compound proposition makes it true but no polynomial time algorithm has been discovered for finding such an assignment of truth values for example an exhaustive search of all possible truth values requires q bit operations where n is the number of variables in the compound proposition there is also an important class of problems called np complete problems with the property that if any of these problems can be solved by a polynomial worst case time algorithm then all problems in the class np can be solved by polynomial worst case time algorithms the satisfiability problem is also an example of an np complete problem it is an np problem and if a polynomial time algorithm for solving it were known there would be polynomial time algorithms for all problems known to be in this class of problems and there are many important problems in this class this last statement follows from the fact that every problem in np can be reduced in polynomial time to the satisfiability problem although more than np complete problems are now known the satisfiability problem was the first problem shown to be np complete the theorem that asserts this is known as the cook levin theorem after stephen cook and leonid levin who independently proved it in the early the p versus np problem asks whether np the class of problems for which it is possible to check solutions in polynomial time equals p the class of tractable problems if p np there would be some problems that cannot be solved in polynomial time but whose solutions could be verified in polynomial time the concept of np completeness is helpful in research aimed at solving the p versus np problem because np complete problems are the problems in np considered most likely not to be in p as every problem in np can be reduced to an np complete problem in polynomial time a large majority of theoretical computer scientists believe that p np which would mean that no np complete problem can be solved in polynomial time one reason for this belief is that despite extensive research no one has succeeded in showing that p np in particular no one has been able to find an algorithm with worst case polynomial time complexity that solves any np complete problem the p versus np problem is one of the most famous unsolved problems in the mathematical sciences which include theoretical computer science it is one of the seven famous millennium prize problems of which six remain unsolved a prize of is offered by the clay mathematics institute for its solution stephen cook born stephen cook was born in buffalo where his father worked as an industrial chemist and taught university courses his mother taught english courses in a community college while in high school cook developed an interest in electronics through his work with a famous local inventor noted for inventing the first implantable cardiac pacemaker cook was a mathematics major at the university of michigan graduating in he did graduate work at harvard receiving a master degree in and a ph d in cook was appointed an assistant professor in the mathematics department at the university of california berkeley in he was not granted tenure there possibly because the members of the mathematics department did not find his work on what is now considered to be one of the most important areas of theoretical computer science of sufficient interest in he joined the university of toronto as an assistant professor holding a joint appointment in the computer science department and the mathematics department he has remained at the university of toronto where he was appointed a university professor in cook is considered to be one of the founders of computational complexity theory his paper the complexity of theorem proving procedures formalized the notions of np completeness and polynomial time reduction showed that np complete problems exist by showing that the satisfiability problem is such a problem and introduced the notorious p versus np problem cook has received many awards including the turing award he is married and has two sons among his interests are playing the violin and racing sailboats for more information about the complexity of algorithms consult the references including for this section listed at the end of this book also for a more formal discussion of computational complexity in terms of turing machines see section practical considerations note that a big estimate of the time complexity of an algorithm expresses how the time required to solve the problem increases as the input grows in size in practice the best estimate that is with the smallest reference function that can be shown is used however big estimates of time complexity cannot be directly translated into the actual amount of computer time used one reason is that a big estimate f n is g n where f n is the time complexity of an algorithm and g n is a reference function means that n f n n when n k where and k are constants so without knowing the constants c2 and k in the inequality this estimate cannot be used to determine a lower bound and an upper bound on the number of operations used in the worst case as remarked before the time required for an operation depends on the type of operation and the computer being used often instead of a big estimate on the worst case time complexity of an algorithm we have only a big o estimate note that a big o estimate on the time complexity of an algorithm provides an upper but not a lower bound on the worst case time required for the algorithm as a function of the input size nevertheless for simplicity we will often use big o estimates when describing the time complexity of algorithms with the understanding that big estimates would provide more information table displays the time needed to solve problems of various sizes with an algorithm using the indicated number n of bit operations assuming that each bit operation takes seconds a reasonable estimate of the time required for a bit operation using the fastest computers available today times of more than years are indicated with an asterisk in the future these times will decrease as faster computers are developed we can use the times shown in table to see whether it is reasonable to expect a solution to a problem of a specified size using an algorithm with known worst case time complexity when we run this algorithm on a modern computer note that we cannot determine the exact time a computer uses to solve a problem with input of a particular size because of a myriad of issues involving computer hardware and the particular software implementation of the algorithm it is important to have a reasonable estimate for how long it will take a computer to solve a problem for instance if an algorithm requires approximately hours it may be worthwhile to spend the computer time and money required to solve this problem but if an algorithm requires approximately billion years to solve a problem it would be unreasonable to use resources to implement this algorithm one of the most interesting phenomena of modern technology is the tremendous increase in the speed and memory space of computers another important factor that decreases the time needed to solve problems on computers is parallel processing which is the technique of performing sequences of operations simultaneously efficient algorithms including most algorithms with polynomial time complexity benefit most from significant technology improvements however these technology improvements table the computer time used by algorithms problem size bit operations used n log n n n log n n min yr offer little help in overcoming the complexity of algorithms of exponential or factorial time complexity because of the increased speed of computation increases in computer memory and the use of algorithms that take advantage of parallel processing many problems that were con sidered impossible to solve five years ago are now routinely solved and certainly five years from now this statement will still be true this is even true when the algorithms used are intractable exercises give a big o estimate for the number of operations where an operation is an addition or a multiplication used in this segment of an algorithm t for i to for j to t t ij give a big o estimate for the number additions used in this segment of an algorithm t for i to n for j to n t t i j give a big o estimate for the number of operations where an operation is a comparison or a multiplication used in this segment of an algorithm ignoring compar isons used to test the conditions in the for loops where an are positive real numbers m for i to n for j i to n m max aiaj m give a big o estimate for the number of operations where an operation is an addition or a multiplication used in this segment of an algorithm ignoring comparisons used to test the conditions in the while loop i t while i n t t i i how many comparisons are used by the algorithm given in exercise of section to find the smallest natural number in a sequence of n natural numbers a use pseudocode to describe the algorithm that puts the first four terms of a list of real numbers of arbitrary length in increasing order using the insertion sort b show that this algorithm has time complexity o in terms of the number of comparisons used suppose that an element is known to be among the first four elements in a list of elements would a lin ear search or a binary search locate this element more rapidly given a real number x and a positive integer k determine the number of multiplications used to find starting with x and successively squaring to find and so on is this a more efficient way to find than by mul tiplying x by itself the appropriate number of times give a big o estimate for the number of comparisons used by the algorithm that determines the number of in a bit string by examining each bit of the string to deter mine whether it is a bit see exercise of section a show that this algorithm determines the number of bits in the bit string s procedure bit count s bit string count while s count count s s s return count count is the number of in s here s is the bit string obtained by changing the rightmost bit of s to a and all the bits to the right of this to recall that s s is the bitwise and of s and s b how many bitwise and operations are needed to find the number of bits in a string s using the algorithm in part a a suppose we have n subsets sn of the set n express a brute force algorithm that de termines whether there is a disjoint pair of these sub sets hint the algorithm should loop through the subsets for each subset si it should then loop through all other subsets and for each of these other subsets sj it should loop through all elements k in si to de termine whether k also belongs to sj b give a big o estimate for the number of times the algorithm needs to determine whether an integer is in one of the subsets consider the following algorithm which takes as input a sequence of n integers an and produces as out put a matrix m mij where mij is the minimum term in the sequence of integers ai ai aj for j i and mij otherwise initialize m so that mij ai if j i and mij otherwise for i to n for j i to n for k i to j mij min mij ak return m mij mij is the minimum term of a a a i i j a show that this algorithm uses o comparisons to compute the matrix m b show that this algorithm uses q comparisons to compute the matrix m using this fact and part a conclude that the algorithms uses comparisons hint only consider the cases where i n and a log n b c d e f g h what is the largest n for which one can solve within a minute using an algorithm that requires f n bit op j in the two outer loops in the algorithm the conventional algorithm for evaluating a polynomial anxn an at x c can be ex pressed in pseudocode by procedure polynomial c an real numbers power y for i to n power power c y y ai power return y y ancn an where the final value of y is the value of the polynomial at x c a evaluate x at x by working through each step of the algorithm showing the values assigned at each assignment step b exactly how many multiplications and additions are used to evaluate a polynomial of degree n at x c do not count additions used to increment the loop variable there is a more efficient algorithm in terms of the num ber of multiplications and additions used for evaluating polynomials than the conventional algorithm described in the previous exercise it is called horner method this pseudocode shows how to use this method to find the value of anxn an at x c procedure horner c an real numbers y an for i to n y y c an i return y y ancn an a evaluate x at x by working through each step of the algorithm showing the values assigned at each assignment step b exactly how many multiplications and additions are used by this algorithm to evaluate a polynomial of degree n at x c do not count additions used to increment the loop variable what is the largest n for which one can solve within one second a problem using an algorithm that requires f n bit operations where each bit operation is carried out in seconds with these functions f n a log n b n c n log n d e f n what is the largest n for which one can solve within a day using an algorithm that requires f n bit operations where each bit operation is carried out in seconds with these functions f n erations where each bit operation is carried out in seconds with these functions f n a log log n b log n c log n d e f g how much time does an algorithm take to solve a prob lem of size n if this algorithm uses operations each requiring seconds with these values of n a b c d how much time does an algorithm using operations need if each operation takes these amounts of time a b c what is the effect in the time required to solve a prob lem when you double the size of the input from n to assuming that the number of milliseconds the algorithm uses to solve the problem with input size n is each of these function express your answer in the simplest form pos sible either as a ratio or a difference your answer may be a function of n or a constant a log log n b log n c d n log n e f g what is the effect in the time required to solve a problem when you increase the size of the input from n to n assuming that the number of milliseconds the algorithm uses to solve the problem with input size n is each of these function express your answer in the simplest form pos sible either as a ratio or a difference your answer may be a function of n or a constant a log n b c d e f g n determine the least number of comparisons or best case performance a required to find the maximum of a sequence of n in tegers using algorithm of section b used to locate an element in a list of n terms with a linear search c used to locate an element in a list of n terms using a binary search analyze the average case performance of the linear search algorithm if exactly half the time the element x is not in the list and if x is in the list it is equally likely to be in any position an algorithm is called optimal for the solution of a prob lem with respect to a specified operation if there is no algorithm for solving this problem using fewer opera tions a show that algorithm in section is an optimal algorithm with respect to the number of comparisons of integers note comparisons used for bookkeep ing in the loop are not of concern here b is the linear search algorithm optimal with respect to the number of comparisons of integers not including comparisons used for bookkeeping in the loop describe the worst case time complexity measured in terms of comparisons of the ternary search algorithm described in exercise of section describe the worst case time complexity measured in terms of comparisons of the search algorithm described in exercise of section analyze the worst case time complexity of the algorithm you devised in exercise of section for locating a mode in a list of nondecreasing integers analyze the worst case time complexity of the algorithm you devised in exercise of section for locating all modes in a list of nondecreasing integers analyze the worst case time complexity of the algorithm you devised in exercise of section for finding the first term of a sequence of integers equal to some previous term analyze the worst case time complexity of the algorithm you devised in exercise of section for finding all terms of a sequence that are greater than the sum of all previous terms analyze the worst case time complexity of the algorithm you devised in exercise of section for finding the first term of a sequence less than the immediately preced ing term determine the worst case complexity in terms of com parisons of the algorithm from exercise in section for determining all values that occur more than once in a sorted list of integers determine the worst case complexity in terms of compar isons of the algorithm from exercise in section for determining whether a string of n characters is a palin drome how many comparisons does the selection sort see preamble to exercise in section use to sort n items use your answer to give a big o estimate of the complexity of the selection sort in terms of number of comparisons for the selection sort find a big o estimate for the worst case complexity in terms of number of comparisons used and the number of terms swapped by the binary insertion sort described in the preamble to exercise in section show that the greedy algorithm for making change for n cents using quarters dimes nickels and pennies has o n complexity measured in terms of comparisons needed exercises and deal with the problem of scheduling the most talks possible given the start and end times of n talks find the complexity of a brute force algorithm for scheduling the talks by examining all possible subsets of the talks hint use the fact that a set with n elements has subsets find the complexity of the greedy algorithm for schedul ing the most talks by adding at each step the talk with the earliest end time compatible with those already scheduled algorithm in section assume that the talks are not already sorted by earliest end time and assume that the worst case time complexity of sorting is o n log n describe how the number of comparisons used in the worst case changes when these algorithms are used to search for an element of a list when the size of the list doubles from n to where n is a positive integer a linear search b binary search describe how the number of comparisons used in the worst case changes when the size of the list to be sorted doubles from n to where n is a positive integer when these sorting algorithms are used a bubble sort b insertion sort c selection sort described in the preamble to exer cise in section d binary insertion sort described in the preamble to ex ercise in section an n n matrix is called upper triangular if aij when ever i j from the definition of the matrix product describe an algorithm in english for computing the product of two upper triangular matrices that ignores those products in the computation that are automatically equal to zero give a pseudocode description of the algorithm in exer cise for multiplying two upper triangular matrices how many multiplications of entries are used by the al gorithm found in exercise for multiplying two n n upper triangular matrices in exercises assume that the number of multiplications of entries used to multiply a p q matrix and a q r matrix is pqr what is the best order to form the product abc if a b and c are matrices with dimensions and respectively what is the best order to form the product abcd if a b c and d are matrices with dimensions and respectively in this exercise we deal with the problem of string match ing a explain how to use a brute force algorithm to find the first occurrence of a given string of m characters called the target in a string of n characters where m n called the text hint think in terms of find ing a match for the first character of the target and checking successive characters for a match and if they do not all match moving the start location one character to the right b express your algorithm in pseudocode c give a big o estimate for the worst case time com plexity of the brute force algorithm you described key terms and results terms algorithm a finite sequence of precise instructions for per forming a computation or solving a problem searching algorithm the problem of locating an element in a list linear search algorithm a procedure for searching a list ele ment by element binary search algorithm a procedure for searching an or dered list by successively splitting the list in half sorting the reordering of the elements of a list into prescribed order f x is o g x the fact that f x c g x for all x k for some constants c and k witness to the relationship f x is o g x a pair c and k such that f x c g x whenever x k f x is q g x the fact that f x c g x for all x k for some positive constants c and k f x is g x the fact that f x is both o g x and q g x time complexity the amount of time required for an algorithm to solve a problem space complexity the amount of space in computer memory required for an algorithm to solve a problem worst case time complexity the greatest amount of time re quired for an algorithm to solve a problem of a given size average case time complexity the average amount of time required for an algorithm to solve a problem of a given size algorithmic paradigm a general approach for constructing algorithms based on a particular concept brute force the algorithmic paradigm based on constructing algorithms for solving problems in a naive manner from the statement of the problem and definitions greedy algorithm an algorithm that makes the best choice at each step according to some specified condition tractable problem a problem for which there is a worst case polynomial time algorithm that solves it intractable problem a problem for which no worst case polynomial time algorithm exists for solving it solvable problem a problem that can be solved by an algo rithm unsolvable problem a problem that cannot be solved by an algorithm results linear and binary search algorithms given in section bubble sort a sorting that uses passes where successive items are interchanged if they in the wrong order insertion sort a sorting that at the j th step inserts the j th el ement into the correct position in in the list when the first j elements of the list are already sorted the linear search has o n worst case time complexity the binary search has o log n worst case time complexity the bubble and insertion sorts have o worst case time complexity log n is o n log n if x is o x and x is o x then x is o max x g2 x and x is o x if an are real numbers with an then anxn an is xn and hence o n and review questions a define the term algorithm b what are the different ways to describe algorithms c what is the difference between an algorithm for solv ing a problem and a computer program that solves this problem a describe using english an algorithm for finding the largest integer in a list of n integers b express this algorithm in pseudocode c how many comparisons does the algorithm use a state the definition of the fact that f n is o g n where f n and g n are functions from the set of positive integers to the set of real numbers b use the definition of the fact that f n is o g n directly to prove or disprove that is o c use the definition of the fact that f n is o g n directly to prove or disprove that is o list these functions so that each function is big o of the next function in the list log n n n a how can you produce a big o estimate for a function that is the sum of different terms where each term is the product of several functions b give a big o estimate for the function f n n nn for the function g in your estimate f x is o g x use a simple function of smallest possible order a define what the worst case time complexity average case time complexity and best case time complexity in terms of comparisons mean for an algorithm that finds the smallest integer in a list of n integers b what are the worst case average case and best case time complexities in terms of comparisons of the al gorithm that finds the smallest integer in a list of n integers by comparing each of the integers with the smallest integer found so far a describe the linear search and binary search algorithm for finding an integer in a list of integers in increasing order b compare the worst case time complexities of these two algorithms c is one of these algorithms always faster than the other measured in terms of comparisons a describe the bubble sort algorithm b use the bubble sort algorithm to sort the list c give a big o estimate for the number of comparisons used by the bubble sort a describe the insertion sort algorithm b use the insertion sort algorithm to sort the list c give a big o estimate for the number of comparisons used by the insertion sort a explain the concept of a greedy algorithm b provide an example of a greedy algorithm that pro duces an optimal solution and explain why it produces an optimal solution c provide an example of a greedy algorithm that does not always produce an optimal solution and explain why it fails to do so define what it means for a problem to be tractable and what it means for a problem to be solvable supplementary exercises a describe an algorithm for locating the last occurrence of the largest number in a list of integers b estimate the number of comparisons used a describe an algorithm for finding the first and second largest elements in a list of integers b estimate the number of comparisons used a give an algorithm to determine whether a bit string contains a pair of consecutive zeros b how many comparisons does the algorithm use a suppose that a list contains integers that are in order of largest to smallest and an integer can appear repeat edly in this list devise an algorithm that locates all occurrences of an integer x in the list b estimate the number of comparisons used a adapt algorithm in section to find the maxi mum and the minimum of a sequence of n elements by employing a temporary maximum and a temporary minimum that is updated as each successive element is examined b describe the algorithm from part a in pseudocode c how many comparisons of elements in the sequence are carried out by this algorithm do not count com parisons used to determine whether the end of the se quence has been reached a describe in detail and in english the steps of an al gorithm that finds the maximum and minimum of a sequence of n elements by examining pairs of suc cessive elements keeping track of a temporary maxi mum and a temporary minimum if n is odd both the temporary maximum and temporary minimum should initially equal the first term and if n is even the tem porary minimum and temporary maximum should be found by comparing the initial two elements the tem porary maximum and temporary minimum should be updated by comparing them with the maximum and minimum of the pair of elements being examined c how many comparisons of elements of the sequence are carried out by this algorithm do not count com parisons used to determine whether the end of the se quence has been reached how does this compare to the number of comparisons used by the algorithm in exercise show that the worst case complexity in terms of compar isons of an algorithm that finds the maximum and mini mum of n elements is at least devise an efficient algorithm for finding the second largest element in a sequence of n elements and deter mine the worst case complexity of your algorithm devise an algorithm that finds all equal pairs of sums of two terms of a sequence of n numbers and determine the worst case complexity of your algorithm devise an algorithm that finds the closest pair of integers in a sequence of n integers and determine the worst case complexity of your algorithm hint sort the sequence use the fact that sorting can be done with worst case time complexity o n log n the shaker sort or bidirectional bubble sort successively compares pairs of adjacent elements exchanging them if they are out of order and alternately passing through the list from the beginning to the end and then from the end to the beginning until no exchanges are needed show the steps used by the shaker sort to sort the list express the shaker sort in pseudocode show that the shaker sort has o complexity measured in terms of the number of comparisons it uses explain why the shaker sort is efficient for sorting lists that are already in close to the correct order show that n log n is o show that 12x log x is o give a big o estimate for x log x find a big o estimate for n j j b express the algorithm described in part a in pseu j show that n is not o docode show that nn is not o n find all pairs of functions of the same order in this list of functions log n n log n n and n log n find all pairs of functions of the same order in this list of functions n and find an integer n with n for which find an integer n with n for which log n n arrange the functions nn log n n n and n log n in a list so that each function is big o of the next function hint to determine the relative size of some of these functions take logarithms arrange the function nlog n n log n log log n n log n and log n in a list so that each function is big o of the next function hint to determine the relative size of some of these functions take logarithms give an example of two increasing functions f n and g n from the set of positive integers to the set of posi tive integers such that neither f n is o g n nor g n is o f n show that if the denominations of coins are ck where k is a positive integer and c is a positive integer c the greedy algorithm always produces change us ing the fewest coins possible a use pseudocode to specify a brute force algorithm that determines when given as input a sequence of n pos itive integers whether there are two distinct terms of the sequence that have as sum a third term the algo rithm should loop through all triples of terms of the sequence checking whether the sum of the first two terms equals the third b give a big o estimate for the complexity of the brute force algorithm from part a a devise a more efficient algorithm for solving the prob lem described in exercise that first sorts the in put sequence and then checks for each pair of terms whether their difference is in the sequence b give a big o estimate for the complexity of this al gorithm is it more efficient than the brute force algo rithm from exercise suppose we have men and women each with their prefer ence lists for the members of the opposite gender as described in the preamble to exercise in section we say that a woman w is a valid partner for a man m if there is some sta ble matching in which they are paired similarly a man m is a valid partner for a woman w if there is some stable matching in which they are paired a matching in which each man is as signed his valid partner ranking highest on his preference list is called male optimal and a matching in which each woman is assigned her valid partner ranking lowest on her preference list is called female pessimal find all valid partners for each man and each woman if there are three men and and three women with these preference rankings of the men for the women from highest to lowest and with these preference rank ings of the women for the men from highest to lowest w3 m3 show that the deferred acceptance algorithm given in the preamble to exercise of section always produces a male optimal and female pessimal matching define what it means for a matching to be female optimal and for a matching to be male pessimal show that when woman do the proposing in the deferred acceptance algorithm the matching produced is female optimal and male pessimal in exercises and we consider variations on the problem of finding stable matchings of men and women described in the preamble to exercise in section in this exercise we consider matching problems where there may be different numbers of men and women so that it is impossible to match everyone with a member of the opposite gender a extend the definition of a stable matching from that given in the preamble to exercise in section to cover the case where there are unequal numbers of men and women avoid all cases where a man and a woman would prefer each other to their current sit uation including those involving unmatched people assume that an unmatched person prefers a match with a member of the opposite gender to remaining unmatched b adapt the deferred acceptance algorithm to find sta ble matchings using the definition of stable matchings from part a when there are different numbers of men and women c prove that all matchings produced by the algorithm from part b are stable according to the definition from part a in this exercise we consider matching problems where some man woman pairs are not allowed a extend the definition of a stable matching to cover the situation where there are the same number of men and women but certain pairs of men and women are forbidden avoid all cases where a man and a woman would prefer each other to their current situation in cluding those involving unmatched people b adapt the deferred acceptance algorithm to find stable matchings when there are the same number of men and women but certain man woman pairs are forbid den be sure to consider people who are unmatched at the end of the algorithm assume that an unmatched person prefers a match with a member of the opposite gender who is not a forbidden partner to remaining unmatched c prove that all matchings produced by the algorithm from b are stable according to the definition in part a exercises deal with the problem of scheduling n jobs on a single processor to complete job j the processor must run job j for time tj without interruption each job has a dead line dj if we start job j at time sj it will be completed at time ej sj tj the lateness of the job measures how long it finishes after its deadline that is the lateness of job j is max ej dj we wish to devise a greedy algorithm that minimizes the maximum lateness of a job among the n jobs suppose we have five jobs with specified required times and deadlines find the maximum lateness of any job when the jobs are scheduled in this order and they start at time job job job job job answer the same question for the schedule job job job job job the slackness of a job requiring time t and with deadline d is d t the difference between its deadline and the time it requires find an example that shows that schedul ing jobs by increasing slackness does not always yield a schedule with the smallest possible maximum lateness find an example that shows that scheduling jobs in or sleeping bag an kg tent a kg food pack a kg container of water and an kg portable stove in exercises we will study the problem of load balanc ing the input to the problem is a collection of p processors and n jobs tj is the time required to run job j jobs run without interruption on a single machine until finished and a proces sor can run only one job at a time the load lk of processor k is the sum over all jobs assigned to processor k of the times required to run these jobs the makespan is the maximum load over all the p processors the load balancing problem asks for an assignment of jobs to processors to minimize the makespan suppose we have three processors and five jobs requiring times and solve the load balancing problem for this input by finding the assignment of the five jobs to the three processors that minimizes the makespan suppose that l is the minimum makespan when p pro cessors are given n jobs where tj is the time required to run job j a show that l maxj n tj b show that l n tj schedule with the smallest possible maximum lateness prove that scheduling jobs in order of increasing deadlines always produces a schedule that minimizes the maximum lateness of a job hint first show that for a schedule to be optimal jobs must be scheduled with no idle time be tween them and so that no job is scheduled before another with an earlier deadline suppose that we have a knapsack with total capacity of w kg we also have n items where item j has mass wj the knapsack problem asks for a subset of these n items with the largest possible total mass not exceeding w a devise a brute force algorithm for solving the knap sack problem b solve the knapsack problem when the capacity of the knapsack is kg and there are five items a kg write out in pseudocode the greedy algorithm that goes through the jobs in order and assigns each job to the pro cessor with the smallest load at that point in the algorithm run the algorithm from exercise on the input given in exercise an approximation algorithm for an optimization problem produces a solution guaranteed to be close to an optimal so lution more precisely suppose that the optimization problem asks for an input s that minimizes f x where f is some function of the input x if an algorithm always finds an input t with f t cf s where c is a fixed positive real number the algorithm is called a c approximation algorithm for the problem prove that the algorithm from exercise is a approximation algorithm for the load balancing problem hint use both parts of exercise computer projects write programs with these inputs and outputs given a list of n integers find the largest integer in the list given a list of n integers find the first and last occurrences of the largest integer in the list given a list of n distinct integers determine the position of an integer in the list using a linear search given an ordered list of n distinct integers determine the position of an integer in the list using a binary search given a list of n integers sort them using a bubble sort given a list of n integers sort them using an insertion sort given an integer n use the greedy algorithm to find the change for n cents using quarters dimes nickels and pennies given the starting and ending times of n talks use the appropriate greedy algorithm to schedule the most talks possible in a single lecture hall given an ordered list of n integers and an integer x in the list find the number of comparisons used to determine the position of x in the list using a linear search and using a binary search given a list of integers determine the number of compar isons used by the bubble sort and by the insertion sort to sort this list computations and explorations use a computational program or programs you have written to do these exercises we know that nb is o dn when b and d are positive numbers with d give values of the constants c and k such that nb cdn whenever x k for each of these sets of values b d b d b d compute the change for different values of n with coins of different denominations using the greedy algorithm and determine whether the smallest number of coins was used can you find conditions so that the greedy algorithm is guaranteed to use the fewest coins possible using a generator of random orderings of the integers n find the number of comparisons used by the bubble sort insertion sort binary insertion sort and selection sort to sort these integers writing projects respond to these with essays using outside sources examine the history of the word algorithm and describe the use of this word in early writings look up bachmann original introduction of big o no tation explain how he and others have used this notation explain how sorting algorithms can be classified into a taxonomy based on the underlying principle on which they are based describe the radix sort algorithm describe the historic trends in how quickly processors can perform operations and use these trends to estimate how quickly processors will be able to perform operations in the next twenty years develop a detailed list of algorithmic paradigms and pro vide examples using each of these paradigms explain what the turing award is and describe the criteria used to select winners list six past winners of the award and why they received the award describe what is meant by a parallel algorithm explain how the pseudocode used in this book can be extended to handle parallel algorithms explain how the complexity of parallel algorithms can be measured give some examples to illustrate this concept showing how a parallel algorithm can work more quickly than one that does not operate in parallel describe six different np complete problems demonstrate how one of the many different np complete problems can be reduced to the satisfiability problem divisibility and modular arithmetic integer repre sentations and algorithms primes and greatest common divisors solving congruences applications of congruences cryptography he part of mathematics devoted to the study of the set of integers and their properties is known as number theory in this chapter we will develop some of the important concepts of number theory including many of those used in computer science as we develop number theory we will use the proof methods developed in chapter to prove many theorems we will first introduce the notion of divisibility of integers which we use to introduce modular or clock arithmetic modular arithmetic operates with the remainders of integers when they are divided by a fixed positive integer called the modulus we will prove many important results about modular arithmetic which we will use extensively in this chapter integers can be represented with any positive integer b greater than as a base in this chapter we discuss base b representations of integers and give an algorithm for finding them in particular we will discuss binary octal and hexadecimal base and representations we will describe algorithms for carrying out arithmetic using these representations and study their complexity these algorithms were the first procedures called algorithms we will discuss prime numbers the positive integers that have only and themselves as positive divisors we will prove that there are infinitely many primes the proof we give is considered to be one of the most beautiful proofs in mathematics we will discuss the distribution of primes and many famous open questions concerning primes we will introduce the concept of greatest common divisors and study the euclidean algorithm for computing them this algorithm was first described thousands of years ago we will introduce the fundamental theorem of arithmetic a key result which tells us that every positive integer has a unique factorization into primes we will explain how to solve linear congruences as well as systems of linear congruences which we solve using the famous chinese remainder theorem we will introduce the notion of pseudoprimes which are composite integers masquerading as primes and show how this notion can help us rapidly generate prime numbers this chapter introduces several important applications of number theory in particular we will use number theory to generate pseudorandom numbers to assign memory locations to computer files and to find check digits used to detect errors in various kinds of identification numbers we also introduce the subject of cryptography number theory plays an essentially role both in classical cryptography first used thousands of years ago and modern cryptography which plays an essential role in electronic communication we will show how the ideas we develop can be used in cryptographical protocols introducing protocols for sharing keys and for sending signed messages number theory once considered the purest of subjects has become an essential tool in providing computer and internet security divisibility and modular arithmetic introduction the ideas that we will develop in this section are based on the notion of divisibility division of an integer by a positive integer produces a quotient and a remainder working with these remainders leads to modular arithmetic which plays an important role in mathematics and which is used throughout computer science we will discuss some important applications of modular arithmetic later in this chapter including generating pseudorandom numbers assigning computer memory locations to files constructing check digits and encrypting messages division when one integer is divided by a second nonzero integer the quotient may or may not be an integer for example is an integer whereas is not this leads to definition definition remark we can express a b using quantifiers as c ac b where the universe of discourse is the set of integers in figure a number line indicates which integers are divisible by the positive integer d example determine whether and whether solution we see that because is not an integer on the other hand because example let n and d be positive integers how many positive integers not exceeding n are divisible by d solution the positive integers divisible by d are all the integers of the form dk where k is a positive integer hence the number of positive integers divisible by d that do not exceed n equals the number of integers k with dk n or with k n d therefore there are ln d positive integers not exceeding n that are divisible by d some of the basic properties of divisibility of integers are given in theorem theorem proof we will give a direct proof of i suppose that a b and a c then from the definition of divisibility it follows that there are integers and t with b as and c at hence b c as at a t d d figure integers divisible by the positive integer d therefore a divides b c this establishes part i of the theorem the proofs of parts ii and iii are left as exercises and theorem has this useful consequence corollary proof we will give a direct proof by part ii of theorem we see that a mb and a nc whenever m and n are integers by part i of theorem it follows that a mb nc the division algorithm when an integer is divided by a positive integer there is a quotient and a remainder as the division algorithm shows theorem we defer the proof of the division algorithm to section see example and exercise remark theorem is not really an algorithm why not nevertheless we use its traditional name definition remark note that both a div d and a mod d for a fixed d are functions on the set of inte gers furthermore when a is an integer and d is a positive integer we have a div d la d and a mod d a d see exercise examples and illustrate the division algorithm example what are the quotient and remainder when is divided by solution we have hence the quotient when is divided by is div and the remainder is mod example what are the quotient and remainder when is divided by solution we have hence the quotient when is divided by is div and the remainder is mod note that the remainder cannot be negative consequently the remainder is not even though because r does not satisfy r note that the integer a is divisible by the integer d if and only if the remainder is zero when a is divided by d remark a programming language may have one or possibly two operators for modular arith metic denoted by mod in basic maple mathematica excel and sql in c c java and python rem in ada and lisp or something else be careful when using them because for a some of these operators return a m a m instead of a mod m a m a m as shown in exercise also unlike a mod m some of these operators are defined when m and even when m modular arithmetic in some situations we care only about the remainder of an integer when it is divided by some specified positive integer for instance when we ask what time it will be on a hour clock hours from now we care only about the remainder when plus the current hour is divided by because we are often interested only in remainders we have special notations for them we have already introduced the notation a mod m to represent the remainder when an integer a is divided by the positive integer m we now introduce a different but related notation that indicates that two integers have the same remainder when they are divided by the positive integer m definition although both notations a b mod m and a mod m b include mod they represent fundamentally different concepts the first represents a relation on the set of integers whereas the second represents a function however the relation a b mod m and the mod m function are closely related as described in theorem theorem the proof of theorem is left as exercises and recall that a mod m and b mod m are the remainders when a and b are divided by m respectively consequently theorem also says that a b mod m if and only if a and b have the same remainder when divided by m example determine whether is congruent to modulo and whether and are congruent modulo solution because divides we see that mod however because is not divisible by we see that mod the great german mathematician karl friedrich gauss developed the concept of congru ences at the end of the eighteenth century the notion of congruences has played an important role in the development of number theory theorem provides a useful way to work with congruences theorem proof if a b mod m by the definition of congruence definition we know that m a b this means that there is an integer k such that a b km so that a b km conversely if there is an integer k such that a b km then km a b hence m divides a b so that a b mod m the set of all integers congruent to an integer a modulo m is called the congruence class of a modulo m in chapter we will show that there are m pairwise disjoint equivalence classes modulo m and that the union of these equivalence classes is the set of integers theorem shows that additions and multiplications preserve congruences karl friedrich gauss karl friedrich gauss the son of a bricklayer was a child prodigy he demonstrated his potential at the age of when he quickly solved a problem assigned by a teacher to keep the class busy the teacher asked the students to find the sum of the first positive integers gauss realized that this sum could be found by forming pairs each with the sum this brilliance attracted the sponsorship of patrons including duke ferdinand of brunswick who made it possible for gauss to attend caroline college and the university of göttingen while a student he invented the method of least squares which is used to estimate the most likely value of a variable from experimental results in gauss made a fundamental discovery in geometry advancing a subject that had not advanced since ancient times he showed that a sided regular polygon could be drawn using just a ruler and compass in gauss presented the first rigorous proof of the fundamental theorem of algebra which states that a polynomial of degree n has exactly n roots counting multiplicities gauss achieved worldwide fame when he successfully calculated the orbit of the first asteroid discovered ceres using scanty data gauss was called the prince of mathematics by his contemporary mathematicians although gauss is noted for his many discoveries in geometry algebra analysis astronomy and physics he had a special interest in number theory which can be seen from his statement mathematics is the queen of the sciences and the theory of numbers is the queen of mathematics gauss laid the foundations for modern number theory with the publication of his book disquisitiones arithmeticae in theorem proof we use a direct proof because a b mod m and c d mod m by theorem there are integers and t with b a sm and d c tm hence b d a sm c tm a c m t and bd a sm c tm ac m at cs stm hence a c b d mod m and ac bd mod m example because mod and mod it follows from theorem that mod and that mod we must be careful working with congruences some properties we may expect to be true you cannot always divide both sides of a congruence by the same number are not valid for example if ac bc mod m the congruence a b mod m may be false similarly if a b mod m and c d mod m the congruence ac bd mod m may be false see exercise corollary shows how to find the values of the mod m function at the sum and product of two integers using the values of this function at each of these integers we will use this result in section corollary proof by the definitions of mod m and of congruence modulo m we know that a a mod m mod m and b b mod m mod m hence theorem tells us that a b a mod m b mod m mod m and ab a mod m b mod m mod m the equalities in this corollary follow from these last two congruences by theorem arithmetic modulo m we can define arithmetic operations on zm the set of nonnegative integers less than m that is the set m in particular we define addition of these integers denoted by m by a m b a b mod m where the addition on the right hand side of this equation is the ordinary addition of integers and we define multiplication of these integers denoted by m by a m b a b mod m where the multiplication on the right hand side of this equation is the ordinary multiplication of integers the operations m and m are called addition and multiplication modulo m and when we use these operations we are said to be doing arithmetic modulo m example use the definition of addition and multiplication in zm to find and solution using the definition of addition modulo we find that mod mod and mod mod hence and the operations m and m satisfy many of the same properties of ordinary addition and multiplication of integers in particular they satisfy these properties closure if a and b belong to zm then a m b and a m b belong to zm associativity if a b and c belong to zm then a m b m c a m b m c and a m b m c a m b m c commutativity if a and b belong to zm then a m b b m a and a m b b m a identity elements the elements and are identity elements for addition and multiplication modulo m respectively that is if a belongs to zm then a m m a a and a m m a a additive inverses if a belongs to zm then m a is an additive inverse of a modulo m and is its own additive inverse that is a m m a and m distributivity if a b and c belong to zm then a m b m c a m b m a m c and a m b m c a m c m b m c these properties follow from the properties we have developed for congruences and remainders modulo m together with the properties of integers we leave their proofs as exercises note that we have listed the property that every element of zm has an additive inverse but no analogous property for multiplicative inverses has been included this is because multiplicative inverses do not always exists modulo m for instance there is no multiplicative inverse of modulo as the reader can verify we will return to the question of when an integer has a multiplicative inverse modulo m later in this chapter remark because zm with the operations of addition and multiplication modulo m satisfies the properties listed zm with modular addition is said to be a commutative group and zm with both of these operations is said to be a commutative ring note that the set of integers with ordinary addition and multiplication also forms a commutative ring groups and rings are studied in courses that cover abstract algebra remark in exercise and in later sections we will use the notations and for m and m without the subscript m on the symbol for the operator whenever we work with zm exercises does divide each of these numbers a b c d prove that if a is an integer other than then a divides a b a divides prove that part ii of theorem is true prove that part iii of theorem is true show that if a b and b a where a and b are integers then a b or a b show that if a b c and d are integers where a such that a c and b d then ab cd show that if a b and c are integers where a and c such that ac bc then a b prove or disprove that if a bc where a b and c are pos itive integers and a then a b or a c what are the quotient and remainder when suppose that a and b are integers a mod and b mod find the integer c with c such that a c mod b c mod c c a b mod d c mod e c mod f c b3 mod suppose that a and b are integers a mod and b mod find the integer c with c such that a c mod b c mod c c a b mod d c mod e c mod a is divided by b is divided by c is divided by d is divided by e is divided by f is divided by g is divided by h is divided by what are the quotient and remainder when a is divided by b is divided by c is divided by d is divided by e is divided by f is divided by g is divided by h is divided by what time does a hour clock read a hours after it reads b hours before it reads c hours after it reads what time does a hour clock read a hours after it reads b hours before it reads c hours after it reads f c mod let m be a positive integer show that a b mod m if a mod m b mod m let m be a positive integer show that a mod m b mod m if a b mod m show that if n and k are positive integers then in k l n k show that if a is an integer and d is an inte ger greater than then the quotient and remain der obtained when a is divided by d are la d and a dla d respectively find a formula for the integer with smallest absolute value that is congruent to an integer a modulo m where m is a positive integer evaluate these quantities a mod b mod c mod d mod evaluate these quantities a mod b mod c mod d mod find a div m and a mod m when a a m b a m c a m d a m find a div m and a mod m when a a m b a m c a m d a m find the integer a such that a a mod and a b a mod and a c a mod and a find the integer a such that a a mod and a b a mod and a c a mod and a list five integers that are congruent to modulo list all integers between and that are congruent to modulo decide whether each of these integers is congruent to modulo a b c d decide whether each of these integers is congruent to modulo a b c d find each of these values a mod mod mod b mod mod mod find each of these values a mod mod mod b mod mod mod find each of these values a mod mod b mod mod c mod mod d mod mod find each of these values a mod mod b mod mod c mod mod d mod mod show that if a b mod m and c d mod m where a b c d and m are integers with m then a c b d mod m show that if n m where n and m are integers greater than and if a b mod m where a and b are integers then a b mod n show that if a b c and m are integers such that m c and a b mod m then ac bc mod mc find counterexamples to each of these statements about congruences a if ac bc mod m where a b c and m are integers with m then a b mod m b if a b mod m and c d mod m where a b c d and m are integers with c and d positive and m then ac bd mod m show that if n is an integer then or mod use exercise to show that if m is a positive integer of the form for some nonnegative integer k then m is not the sum of the squares of two integers prove that if n is an odd positive integer then mod show that if a b k and m are integers such that k m and a b mod m then ak bk mod m show that zm with addition modulo m where m is an integer satisfies the closure associative and commu tative properties is an additive identity and for every nonzero a zm m a is an inverse of a modulo m show that zm with multiplication modulo m where m is an integer satisfies the closure associative and commutativity properties and is a multiplicative iden tity show that the distributive property of multiplication over addition holds for zm where m is an integer write out the addition and multiplication tables for where by addition and multiplication we mean and write out the addition and multiplication tables for where by addition and multiplication we mean and determine whether each of the functions f a a div d and g a a mod d where d is a fixed positive integer from the set of integers to the set of integers is one to one and determine whether each of these functions is onto integer representations and algorithms introduction integers can be expressed using any integer greater than one as a base as we will show in this section although we commonly use decimal base representations binary base octal base and hexadecimal base representations are often used especially in computer science given a base b and an integer n we will show how to construct the base b representation of this integer we will also explain how to quickly covert between binary and octal and between binary and hexadecimal notations as mentioned in section the term algorithm originally referred to procedures for per forming arithmetic operations using the decimal representations of integers these algorithms adapted for use with binary representations are the basis for computer arithmetic they provide good illustrations of the concept of an algorithm and the complexity of algorithms for these reasons they will be discussed in this section we will also introduce an algorithm for finding a div d and a mod d where a and d are integers with d finally we will describe an efficient algorithm for modular exponentiation which is a particularly important algorithm for cryptography as we will see in section representations of integers in everyday life we use decimal notation to express integers for example is used to denote however it is often convenient to use bases other than in particular computers usually use binary notation with as the base when carrying out arithmetic and octal base or hexadecimal base notation when expressing characters such as letters or digits in fact we can use any integer greater than as the base when expressing integers this is stated in theorem theorem a proof of this theorem can be constructed using mathematical induction a proof method that is discussed in section it can also be found in the representation of n given in theorem is called the base b expansion of n the base b expansion of n is denoted by akak b for instance represents typically the sub script is omitted for base expansions of integers because base or decimal expansions are commonly used to represent integers binary expansions choosing as the base gives binary expansions of integers in binary notation each digit is either a or a in other words the binary expansion of an integer is just a bit string binary expansions and related expansions that are variants of binary expansions are used by computers to represent and do arithmetic with integers example what is the decimal expansion of the integer that has as its binary expansion solution we have octal and hexadecimal expansions among the most important bases in com puter science are base base and base base expansions are called octal expansions and base expansions are hexadecimal expansions example what is the decimal expansion of the number with octal expansion solution using the definition of a base b expansion with b tells us that sixteen different digits are required for hexadecimal expansions usually the hexadecimal digits used are a b c d e and f where the letters a through f represent the digits corresponding to the numbers through in decimal notation example what is the decimal expansion of the number with hexadecimal expansion solution using the definition of a base b expansion with b tells us that 163 175627 each hexadecimal digit can be represented using four bits for instance we see that because e and bytes which are bit strings of length eight can be represented by two hexadecimal digits base conversion we will now describe an algorithm for constructing the base b expan sion of an integer n first divide n by b to obtain a quotient and remainder that is n b the remainder is the rightmost digit in the base b expansion of n next divide by b to obtain b we see that is the second digit from the right in the base b expansion of n continue this process successively dividing the quotients by b obtaining additional base b digits as the remainders this process terminates when we obtain a quotient equal to zero it produces the base b digits of n from the right to the left example find the octal expansion of solution first divide by to obtain successively dividing quotients by gives the successive remainders that we have found and are the digits from the right to the left of in base hence example find the hexadecimal expansion of solution first divide by to obtain successively dividing quotients by gives the successive remainders that we have found give us the digits from the right to the left of in the hexadecimal base expansion of it follows that recall that the integers and correspond to the hexadecimal digits a b and e respectively example find the binary expansion of solution first divide by to obtain successively dividing quotients by gives the successive remainders that we have found are the digits from the right to the left in the binary base expansion of hence the pseudocode given in algorithm finds the base b expansion ak b of the integer n table hexadecimal octal and binary representation of the integers through decimal hexadecimal a b c d e f octal binary in algorithm q represents the quotient obtained by successive divisions by b starting with q n the digits in the base b expansion are the remainders of these divisions and are given by q mod b the algorithm terminates when a quotient q is reached remark note that algorithm can be thought of as a greedy algorithm as the base b digits are taken as large as possible in each step conversion between binary octal and hexadecimal expansions conversion between binary and octal and between binary and hexadecimal expansions is ex tremely easy because each octal digit corresponds to a block of three binary digits and each hexadecimal digit corresponds to a block of four binary digits with these correspondences shown in table without initial shown we leave it as exercises to show that this is the case this conversion is illustrated in example example find the octal and hexadecimal expansions of and the binary expansions of and solution to convert into octal notation we group the binary dig its into blocks of three adding initial zeros at the start of the leftmost block if necessary these blocks from left to right are and corresponding to and respectively consequently 37274 to convert into hexadecimal notation we group the binary digits into blocks of four adding initial zeros at the start of the leftmost block if necessary these blocks from left to right are and corresponding to the hexadecimal digits e b and c respectively consequently 1100 3ebc to convert into binary notation we replace each octal digit by a block of three binary digits these blocks are and hence to convert into binary notation we replace each hexadecimal digit by a block of four binary digits these blocks are and hence algorithms for integer operations the algorithms for performing operations with integers using their binary expansions are ex tremely important in computer arithmetic we will describe algorithms for the addition and the multiplication of two integers expressed in binary notation we will also analyze the compu tational complexity of these algorithms in terms of the actual number of bit operations used throughout this discussion suppose that the binary expansions of a and b are a an b bn so that a and b each have n bits putting bits equal to at the beginning of one of these expansions if necessary we will measure the complexity of algorithms for integer arithmetic in terms of the number of bits in these numbers addition algorithm consider the problem of adding two integers in binary notation a procedure to perform addition can be based on the usual method for adding numbers with pencil and paper this method proceeds by adding pairs of binary digits together with carries when they occur to compute the sum of two integers this procedure will now be specified in detail to add a and b first add their rightmost bits this gives where is the rightmost bit in the binary expansion of a b and is the carry which is either or then add the next pair of bits and the carry where is the next bit from the right in the binary expansion of a b and is the carry continue this process adding the corresponding bits in the two binary expansions and the carry to determine the next bit from the right in the binary expansion of a b at the last stage add an bn and cn to obtain cn sn the leading bit of the sum is sn cn this procedure produces the binary expansion of the sum namely a b snsn example add a and b solution following the procedure specified in the algorithm first note that so that and then because it follows that and continuing figure adding and so that and finally because b3 c2 follows that and this means that therefore a b this addition is displayed in figure where carries are shown in blue the algorithm for addition can be described using pseudocode as follows next the number of additions of bits used by algorithm will be analyzed example how many additions of bits are required to use algorithm to add two integers with n bits or less in their binary representations solution two integers are added by successively adding pairs of bits and when it occurs a carry adding each pair of bits and the carry requires two additions of bits thus the total number of additions of bits used is less than twice the number of bits in the expansion hence the number of additions of bits used by algorithm to add two n bit integers is o n multiplication algorithm next consider the multiplication of two n bit integers a and b the conventional algorithm used when multiplying with pencil and paper works as follows using the distributive law we see that ab a bn a a a bn we can compute ab using this equation we first note that abj a if bj and abj if bj each time we multiply a term by we shift its binary expansion one place to the left and add a zero at the tail end of the expansion consequently we can obtain abj by shifting the binary expansion of abj j places to the left adding j zero bits at the tail end of this binary expansion finally we obtain ab by adding the n integers abj 2j j n algorithm displays this procedure for multiplication example illustrates the use of this algorithm example find the product of a and b solution first note that and figure multiplying and to find the product add and carrying out these additions us ing algorithm including initial zero bits when necessary shows that ab this multiplication is displayed in figure next we determine the number of additions of bits and shifts of bits used by algorithm to multiply two integers example how many additions of bits and shifts of bits are used to multiply a and b using algorithm solution algorithm computes the products of a and b by adding the partial products c2 and cn when bj we compute the partial product cj by shifting the binary expansion of a by j bits when bj no shifts are required because cj hence to find all n of the integers abj 2j j n requires at most n shifts hence by example in section the number of shifts required is o to add the integers abj from j to j n requires the addition of an n bit integer an n bit integer and a bit integer we know from example that each of these additions requires o n additions of bits consequently a total of o additions of bits are required for all n additions surprisingly there are more efficient algorithms than the conventional algorithm for mul tiplying integers one such algorithm which uses o bit operations to multiply n bit numbers will be described in section algorithm for div and mod given integers a and d d we can find q a div d and r a mod d using algorithm in this brute force algorithm when a is pos itive we subtract d from a as many times as necessary until what is left is less than d the number of times we perform this subtraction is the quotient and what is left over after all these subtractions is the remainder algorithm also covers the case where a is negative this algo rithm finds the quotient q and remainder r when a is divided by d then when a and r it uses these to find the quotient q and remainder d r when a is divided by d we leave it to the reader exercise to show that assuming that a d this algorithm uses o q log a bit operations there are more efficient algorithms than algorithm for determining the quotient q a div d and the remainder r a mod d when a positive integer a is divided by a positive integer d see for details these algorithms require o log a log d bit operations if both of the binary expansions of a and d contain n or fewer bits then we can replace log a log d by this means that we need o bit operations to find the quotient and remainder when a is divided by d modular exponentiation in cryptography it is important to be able to find bn mod m efficiently where b n and m are large integers it is impractical to first compute bn and then find its remainder when divided by m because bn will be a huge number instead we can use an algorithm that employs the binary expansion of the exponent n before we present this algorithm we illustrate its basic idea we will explain how to use the binary expansion of n say n ak to compute bn first note that bn bak bak ba0 this shows that to compute bn we need only compute the values of b once we have these values we multiply the terms in this list where a for efficiency after multiplying by each term we reduce the result modulo m this gives us bn for example to compute we first note that so that by successively squaring we find that and consequently 6561 the algorithm successively finds b mod m mod m mod m mod m and multiplies together those terms mod m where a finding the remainder be sure to reduce modulo m after each multiplication of the product when divided by m after each multiplication pseudocode for this algorithm is shown in algorithm note that in algorithm we can use the most efficient algorithm available to compute values of the mod function not necessarily algorithm we illustrate how algorithm works in example example use algorithm to find mod solution algorithm initially sets x and power mod in the computation of mod this algorithm determines mod for j by successively squaring and reducing modulo if aj where aj is the bit in the j th position in the binary expansion of which is it multiplies the current value of x by mod and reduces the result modulo here are the steps used this shows that following the steps of algorithm produces the result mod algorithm is quite efficient it uses o log m log n bit operations to find bn mod m see exercise exercises convert the decimal expansion of each of these integers to a binary expansion a b c convert the decimal expansion of each of these integers to a binary expansion a b c convert the binary expansion of each of these integers to a decimal expansion a b c d 0001 convert the binary expansion of each of these integers to a decimal expansion a b c d 1100 0001 convert the octal expansion of each of these integers to a binary expansion a b c d convert the binary expansion of each of these integers to an octal expansion a b c d 0101 convert the hexadecimal expansion of each of these in tegers to a binary expansion a b c abba d defaced convert badfaced from its hexadecimal expan sion to its binary expansion convert abcdef from its hexadecimal expansion to its binary expansion convert each of the integers in exercise from a binary expansion to a hexadecimal expansion convert 1011 1011 from its binary expansion to its hexadecimal expansion convert from its binary expansion to its hexadecimal expansion show that the hexadecimal expansion of a positive integer can be obtained from its binary expansion by grouping to gether blocks of four binary digits adding initial zeros if necessary and translating each block of four binary digits into a single hexadecimal digit show that the binary expansion of a positive integer can be obtained from its hexadecimal expansion by translat ing each hexadecimal digit into a block of four binary digits show that the octal expansion of a positive integer can be obtained from its binary expansion by grouping together blocks of three binary digits adding initial zeros if nec essary and translating each block of three binary digits into a single octal digit show that the binary expansion of a positive integer can be obtained from its octal expansion by translating each octal digit into a block of three binary digits convert to its binary expansion and 1011 1011 to its octal expansion give a procedure for converting from the hexadecimal ex pansion of an integer to its octal expansion using binary notation as an intermediate step give a procedure for converting from the octal expansion of an integer to its hexadecimal expansion using binary notation as an intermediate step explain how to convert from binary to base expan sions and from base expansions to binary expansions and from octal to base expansions and from base expansions to octal expansions find the sum and the product of each of these pairs of numbers express your answers as a binary expansion a 0111 b 1011 c d 0000 0001 find the sum and product of each of these pairs of num bers express your answers as a base expansion a b c d find the sum and product of each of these pairs of num bers express your answers as an octal expansion a b c d find the sum and product of each of these pairs of num bers express your answers as a hexadecimal expan sion a bbc b c abcde d baaa use algorithm to find mod use algorithm to find mod use algorithm to find mod use algorithm to find mod show that every positive integer can be represented uniquely as the sum of distinct powers of hint con sider binary expansions of integers it can be shown that every integer can be uniquely repre sented in the form e e e e than one complement representations to represent an inte ger x with x for a specified positive integer n a total of n bits is used the leftmost bit is used to where ej or for j k expan sions of this type are called balanced ternary expan sions find the balanced ternary expansions of a b c d show that a positive integer is divisible by if and only if the sum of its decimal digits is divisible by show that a positive integer is divisible by if and only if the difference of the sum of its decimal digits in even numbered positions and the sum of its decimal digits in odd numbered positions is divisible by show that a positive integer is divisible by if and only if the difference of the sum of its binary digits in even numbered positions and the sum of its binary digits in odd numbered positions is divisible by one complement representations of integers are used to simplify computer arithmetic to represent positive and nega tive integers with absolute value less than a total of n bits integers and a bit in this position is used for negative inte gers just as in one complement expansions for a positive integer the remaining bits are identical to the binary expan sion of the integer for a negative integer the remaining bits are the bits of the binary expansion of x two com plement expansions of integers are often used by computers because addition and subtraction of integers can be performed easily using these expansions where these integers can be ei ther positive or negative answer exercise but this time find the two comple ment expansion using bit strings of length six answer exercise if each expansion is a two comple ment expansion of length five answer exercise for two complement expansions answer exercise for two complement expansions answer exercise for two complement expansions show that the integer m with two complement representation a a a a can be found us is used the leftmost bit is used to represent the sign a bit ing the equation n n in this position is used for positive integers and a bit in this m an an position is used for negative integers for positive integers the remaining bits are identical to the binary expansion of the integer for negative integers the remaining bits are obtained by first finding the binary expansion of the absolute value of the integer and then taking the complement of each of these bits where the complement of a is a and the complement isa find the one complement representations using bit strings of length six of the following integers a b c d what integer does each of the following one comple ment representations of length five represent a b c d if m is a positive integer less than how is the one complement representation of m obtained from the one complement of m when bit strings of length n are used how is the one complement representation of the sum of two integers obtained from the one complement rep resentations of these integers how is the one complement representation of the differ ence of two integers obtained from the one complement representations of these integers show that the integer m with one complement representation an can be found us ing the equation m an an two complement representations of integers are also used to simplify computer arithmetic and are used more commonly give a simple algorithm for forming the two comple ment representation of an integer from its one comple ment representation sometimes integers are encoded by using four digit bi nary expansions to represent each decimal digit this pro duces the binary coded decimal form of the integer for instance is encoded in this way by how many bits are required to represent a number with n decimal digits using this type of encoding a cantor expansion is a sum of the form ann an n where ai is an integer with ai i for i n find the cantor expansions of a b c d e f describe an algorithm that finds the cantor expansion of an integer describe an algorithm to add two integers from their can tor expansions add and by working through each step of the algorithm for addition given in the text multiply and by working through each step of the algorithm for multiplication given in the text describe an algorithm for finding the difference of two binary expansions estimate the number of bit operations used to subtract two binary expansions devise an algorithm that given the binary expansions of the integers a and b determines whether a b a b or a b how many bit operations does the comparison algo rithm from exercise use when the larger of a and b has n bits in its binary expansion estimate the complexity of algorithm for finding the base b expansion of an integer n in terms of the number of divisions used show that algorithm uses o log m log n bit opera tions to find bn mod m show that algorithm uses o q log a bit operations assuming that a d primes and greatest common divisors introduction in section we studied the concept of divisibility of integers one important concept based on divisibility is that of a prime number a prime is an integer greater than that is divisible by no positive integers other than and itself the study of prime numbers goes back to ancient times thousands of years ago it was known that there are infinitely many primes the proof of this fact found in the works of euclid is famous for its elegance and beauty we will discuss the distribution of primes among the integers we will describe some of the results about primes found by mathematicians in the last years in particular we will introduce an important theorem the fundamental theorem of arithmetic this theorem which asserts that every positive integer can be written uniquely as the product of primes in nondecreasing order has many interesting consequences we will also discuss some of the many old conjectures about primes that remain unsettled today primes have become essential in modern cryptographic systems and we will develop some of their properties important in cryptography for example finding large primes is essential in modern cryptography the length of time required to factor large integers into their prime factors is the basis for the strength of some important modern cryptographic systems in this section we will also study the greatest common divisor of two integers as well as the least common multiple of two integers we will develop an important algorithm for computing greatest common divisors called the euclidean algorithm primes every integer greater than is divisible by at least two integers because a positive integer is divisible by and by itself positive integers that have exactly two different positive integer factors are called primes definition remark the integer n is composite if and only if there exists an integer a such that a n and a n example the integer is prime because its only positive factors are and whereas the integer is composite because it is divisible by the primes are the building blocks of positive integers as the fundamental theorem of arithmetic shows the proof will be given in section theorem example gives some prime factorizations of integers example the prime factorizations of and 1024 are given by 641 1024 theorem trial division it is often important to show that a given integer is prime for instance in cryptology large primes are used in some methods for making messages secret one procedure for showing that an integer is prime is based on the following observation proof if n is composite by the definition of a composite integer we know that it has a factor a with a n hence by the definition of a factor of a posi tive integer we have n ab wher e b is a positive intege r greater than we will show that a n or b n if a n and b n then ab n n n which is a contradiction consequently a n or b n because both a and b are divisors of n we see that n has a positive divisor not exceeding n this divisor is either prime or by the fundamental theorem of arithmeti c has a prime divisor less than itself in either case n has a prime divisor less than or equal to n from theorem it follows that an integer is prime if it is not divisible by any prime less than or equal to its square root this leads to the brute force al gorithm known as trial division to use trial division we divide n by all primes not exceeding n and conclude that n is prime if it is not divisible by any of these primes in example we use trial division to show that is prime example show that is prime solution the only primes not exceeding are and because is not divisible by or the quotient of and each of these integers is not an integer it follows that is prime because every integer has a prime factorization it would be useful to have a procedure for finding this prime factorization consider the problem of finding the prime factorization of n begin by dividing n by successive primes starting with the smallest prime if n has a prime factor then by theorem a prime factor p not exceeding n will be found so if no prime factor not exceeding n is found then n is prime otherwise if a prime factor p is found continue by factoring n p note that n p has no prime factors less than p again if n p has no prime factor greater than or equal to p and not exceeding its square root then it is prime otherwise if it has a prime factor q continue by factoring n pq this procedure is continued until the factorization has been reduced to a prime this procedure is illustrated in example example find the prime factorization of solution to find the prime factorization of first perform divisions of by succes sive primes beginning with none of the primes and divides however di vides with next divide by successive primes beginning with it is immediately seen that also divides because continue by divid ing by successive primes beginning with although does not divide does divide and because is prime the procedure is completed it follows that consequently the prime factorization of is prime numbers were studied in ancient times for philosophical reasons today there are highly practical reasons for their study in particular large primes play a crucial role in cryp tography as we will see in section the sieve of eratosthenes note that composite integers not exceeding must have a prime factor not exceeding because the only primes less than are and the primes not exceeding are these four primes and those positive integers greater than and not exceeding that are divisible by none of or the sieve of eratosthenes is used to find all primes not exceeding a specified positive integer for instance the following procedure is used to find the primes not exceeding we begin with the list of all integers between and to begin the sieving process the integers that are divisible by other than are deleted because is the first integer greater than that is left all those integers divisible by other than are deleted because is the next integer left after those integers divisible by other than are deleted the next integer left is so those integers divisible by other than are deleted because all composite integers not exceeding are divisible by or all remaining integers except are prime in table the panels display those integers deleted at each stage where each integer divisible by other than is underlined in the first panel each integer divisible by other than is underlined in the second panel each integer divisible by other than is underlined in the third panel and each integer divisible by other than is underlined in the fourth panel the integers not underlined are the primes not exceeding we conclude that the primes less than are and the infinitude of primes it has long been known that there are infinitely many primes this means that whenever pn are the n smallest primes we know there is a larger eratosthenes b c e b c e it is known that eratosthenes was born in cyrene a greek colony west of egypt and spent time studying at plato academy in athens we also know that king ptolemy ii invited eratosthenes to alexandria to tutor his son and that later eratosthenes became chief librarian at the famous library at alexandria a central repository of ancient wisdom eratosthenes was an extremely versatile scholar writing on mathematics geography astronomy history philosophy and literary criticism besides his work in mathematics he is most noted for his chronology of ancient history and for his famous measurement of the size of the earth table the sieve of eratosthenes integers divisible by other than integers divisible by other than receive an underline receive an underline 86 integers divisible by other than integers divisible by other than receive receive an underline an underline integers in color are prime 79 86 90 86 90 94 96 94 96 prime not listed we will prove this fact using a proof given by euclid in his famous mathematics text the elements this simple yet elegant proof is considered by many mathematicians to be among the most beautiful proofs in mathematics it is the first proof presented in the book proofs from the book where the book refers to the imagined collection of perfect proofs that the famous mathematician paul erdo claimed is maintained by god by the way there are a vast number of different proofs than there are an infinitude of primes and new ones are published surprisingly frequently theorem proof we will prove this theorem using a proof by contradiction we assume that there are only finitely many primes pn let q pn by the fundamental theorem of arithmetic q is prime or else it can be written as the product of two or more primes however none of the primes pj divides q for if pj q then pj divides q pn hence there is a prime not in the list pn this prime is either q if it is prime or a prime factor of q this is a contradiction because we assumed that we have listed all the primes consequently there are infinitely many primes remark note that in this proof we do not state that q is prime furthermore in this proof we have given a nonconstructive existence proof that given any n primes there is a prime not in this list for this proof to be constructive we would have had to explicitly give a prime not in our original list of n primes because there are infinitely many primes given any positive integer there are primes greater than this integer there is an ongoing quest to discover larger and larger prime numbers for almost all the last years the largest prime known has been an integer of the special form where p is also prime note that cannot be prime when n is not prime see exercise such primes are called mersenne primes after the french monk marin mersenne who studied them in the seventeenth century the reason that the largest known prime has usually been a mersenne prime is that there is an extremely efficient test known as the lucas lehmer test for determining whether is prime furthermore it is not currently possible to test numbers not of this or certain other special forms anywhere near as quickly to determine whether they are prime example the numbers and are mersenne primes while is not a mersenne prime because progress in finding mersenne primes has been steady since computers were invented as of early mersenne primes were known with found since the largest mersenne prime known again as of early is 112 a number with nearly million decimal digits which was shown to be prime in a communal effort the great internet mersenne prime search gimps is devoted to the search for new mersenne primes you can join this search and if you are lucky find a new mersenne prime and possibly even win a cash prize by the way even the search for mersenne primes has practical implications one quality control test for supercomputers has been to replicate the lucas lehmer test that establishes the primality of a large mersenne prime see for more information about the quest for finding mersenne primes the distribution of primes theorem tells us that there are infinitely many primes however how many primes are less than a positive number x this question interested mathe maticians for many years in the late eighteenth century mathematicians produced large tables marin mersenne mersenne was born in maine france into a family of laborers and attended the college of mans and the jesuit college at la flèche he continued his education at the sor bonne studying theology from to he joined the religious order of the minims in a group whose name comes from the word minimi the members of this group were extremely humble they consid ered themselves the least of all religious orders besides prayer the members of this group devoted their energy to scholarship and study in he became a priest at the place royale in paris between and he taught philosophy at the minim convent at nevers he returned to paris in where his cell in the minims de l annociade became a place for meetings of french scientists philosophers and mathe maticians including fermat and pascal mersenne corresponded extensively with scholars throughout europe serving as a clearinghouse for mathematical and scientific knowledge a function later served by mathematical journals and today also by the internet mersenne wrote books covering mechanics mathematical physics mathematics music and acoustics he studied prime numbers and tried unsuccessfully to construct a formula representing all primes in mersenne claimed that is prime for p but is composite for all other primes less than it took over years to determine that mersenne claim was wrong five times specifically is not prime for p and p but is prime for p p and p it is also noteworthy that mersenne defended two of the most famous men of his time descartes and galileo from religious critics he also helped expose alchemists and astrologers as frauds of prime numbers to gather evidence concerning the distribution of primes using this evidence the great mathematicians of the day including gauss and legendre conjectured but did not prove theorem theorem the prime number theorem was first proved in by the french mathematician jacques hadamard and the belgian mathematician charles jean gustave nicholas de la vallée poussin using the theory of complex variables although proofs not using complex variables have been found all known proofs of the prime number theorem are quite complicated we can use the prime number theorem to estimate the odds that a randomly chosen number is prime the prime number theorem tells us that the number of primes not exceeding x can be approximated by x ln x consequently the odds that a randomly selected positive integer less than n is prime are approximately n ln n n ln n sometimes we need to find a prime with a particular number of digits we would like an estimate of how many integers with a particular number of digits we need to select before we encounter a prime using the prime number theorem and calculus it can be shown that the probability that an integer n is prime is also approximately ln n for example the odds that an integer near is prime are approximately ln which is approximately of course by choosing only odd numbers we double our chances of finding a prime using trial division with theorem gives procedures for factoring and for primality testing however these procedures are not efficient algorithms many much more practical and efficient algorithms for these tasks have been developed factoring and primality testing have become important in the applications of number theory to cryptography this has led to a great interest in developing efficient algorithms for both tasks clever procedures have been devised in the last years for efficiently generating large primes moreover in an important theoretical discovery was made by manindra agrawal neeraj kayal and nitin saxena they showed there is a polynomial time algorithm in the number of bits in the binary expansion of an integer for determining whether a positive integer is prime algorithms based on their work use o log n bit operations to determine whether a positive integer n is prime however even though powerful new factorization methods have been developed in the same time frame factoring large numbers remains extraordinarily more time consuming than primality testing no polynomial time algorithm for factoring integers is known nevertheless the challenge of factoring large numbers interests many people there is a communal effort on the internet to factor large numbers especially those of the special form kn where k is a small positive integer and n is a large positive integer such numbers are called cunningham numbers at any given time there is a list of the ten most wanted large numbers of this type awaiting factorization primes and arithmetic progressions every odd integer is in one of the two arithmetic progressions or k because we know that there are infinitely many primes we can ask whether there are infinitely many primes in both of these arithmetic progressions the primes are in the arithmetic progression the primes are in the arithmetic progression looking at the evidence hints that there may be infinitely many primes in both progressions what about other arithmetic progressions ak b k where no integer greater than one divides both a and b do they contain infinitely many primes the answer was provided by the german mathematician g lejeune dirichlet who proved that every such arithmetic progression contains infinitely many primes his proof and all proofs found later are beyond the scope of this book however it is possible to prove special cases of dirichlet theorem using the ideas developed in this book for example exercises and ask for proofs that there are infinitely many primes in the arithmetic progressions and where k is a positive integer the hint for each of these exercises supplies the basic idea needed for the proof we have explained that every arithmetic progression ak b k where a and b have no common factor greater than one contains infinitely many primes but are there long arithmetic progressions made up of just primes for example some exploration shows that is an arithmetic progression of five primes and 829 1669 is an arithmetic progression of ten primes in the the famous mathematician paul erdo conjectured that for every positive integer n greater than two there is an arithmetic progression of length n made up entirely of primes in ben green and terence tao were able to prove this conjecture their proof considered to be a mathematical tour de force is a nonconstructive proof that combines powerful ideas from several advanced areas of mathematics conjectures and open problems about primes number theory is noted as a subject for which it is easy to formulate conjectures some of which are difficult to prove and others that remained open problems for many years we will describe some conjectures in number theory and discuss their status in examples example it would be useful to have a function f n such that f n is prime for all positive integers n if we had such a function we could find large primes for use in cryptography and other applications looking for such a function we might check out different polynomial functions as some mathematicians did several hundred years ago after a lot of computation we may encounter the polynomial f n n this polynomial has the interesting property that f n is prime for all positive integers n not exceeding we have f f f f and so on this can lead us to the conjecture that f n is prime for all positive integers n can we settle this conjecture solution perhaps not surprisingly this conjecture turns out to be false we do not have to look far to find a positive integer n for which f n is composite because f because f n n is prime for all positive integers n with n we might terence tao born tao was born in australia his father is a pediatrician and his mother taught mathematics at a hong kong secondary school tao was a child prodigy teaching himself arithmetic at the age of two at he became the youngest contestant at the international mathematical olympiad imo he won an imo gold medal at tao received his bachelors and masters degrees when he was and began graduate studies at princeton receiving his ph d in three years in he became a faculty member at ucla where he continues to work tao is extremely versatile he enjoys working on problems in diverse areas including harmonic analy sis partial differential equations number theory and combinatorics you can follow his work by reading his blog where he discusses progress on various problems his most famous result is the green tao theorem which says that there are arbitrarily long arithmetic progressions of primes tao has made important contributions to the applications of mathematics such as developing a method for reconstructing digital images using the least possible amount of information tao has an amazing reputation among mathematicians he has become a mr fix it for researchers in mathematics the well known mathematician charles fefferman himself a child prodigy has said that if you re stuck on a problem then one way out is to interest terence tao in tao was awarded a fields medal the most prestigious award for mathematicians under the age of he was also awarded a macarthur fellowship in and in he received the allan t waterman award which came with a cash prize to support research work of scientists early in their career tao wife laura is an engineer at the jet propulsion laboratory be tempted to find a different polynomial with the property that f n is prime for all positive integers n however there is no such polynomial it can be shown that for every polynomial f n with integer coefficients there is a positive integer y such that f y is composite see exercise in the supplementary exercises many famous problems about primes still await ultimate resolution by clever people we describe a few of the most accessible and better known of these open problems in examples number theory is noted for its wealth of easy to understand conjectures that resist attack by all but the most sophisticated techniques or simply resist all attacks we present these conjectures to show that many questions that seem relatively simple remain unsettled even in the twenty first century example goldbach conjecture in christian goldbach in a letter to leonhard euler conjec tured that every odd integer n n is the sum of three primes euler replied that this conjecture is equivalent to the conjecture that every even integer n n is the sum of two primes see exercise in the supplementary exercises the conjecture that every even integer n n is the sum of two primes is now called goldbach conjecture we can check this conjecture for small even numbers for example and so on goldbach conjecture was verified by hand calculations for numbers up to the mil lions prior to the advent of computers with computers it can be checked for extremely large numbers as of mid the conjecture has been checked for all positive even integers up to although no proof of goldbach conjecture has been found most mathematicians believe it is true several theorems have been proved using complicated methods from analytic number theory far beyond the scope of this book establishing results weaker than goldbach conjecture among these are the result that every even integer greater than is the sum of at most six primes proved in by o ramaré and that every sufficiently large positive integer is the sum of a prime and a number that is either prime or the product of two primes proved in by j r chen perhaps goldbach conjecture will be settled in the not too distant future example there are many conjectures asserting that there are infinitely many primes of certain special forms a conjecture of this sort is the conjecture that there are infinitely many primes of the form where n is a positive integer for example and so on the best result currently known is that there are infinitely many positive integers n such that is prime or the product of at most two primes proved by henryk iwaniec in using advanced techniques from analytic number theory far beyond the scope of this book example the twin prime conjecture twin primes are pairs of primes that differ by such as and and and and and and the twin prime conjecture asserts that there are infinitely many twin primes the strongest result proved concerning twin primes is that there are infinitely many pairs p and p where p is prime and p is prime or the product of two primes proved by j r chen in the world record for twin primes as of mid consists of the numbers 333 which have decimal digits christian goldbach christian goldbach was born in königsberg prussia the city noted for its famous bridge problem which will be studied in section he became professor of mathematics at the academy in st petersburg in in goldbach went to moscow to tutor the son of the tsar he entered the world of politics when in he became a staff member in the russian ministry of foreign affairs goldbach is best known for his correspondence with eminent mathematicians including euler and bernoulli for his famous conjectures in number theory and for several contributions to analysis greatest common divisors and least common multiples the largest integer that divides both of two integers is called the greatest common divisor of these integers definition the greatest common divisor of two integers not both zero exists because the set of common divisors of these integers is nonempty and finite one way to find the greatest common divisor of two integers is to find all the positive common divisors of both integers and then take the largest divisor this is done in examples and later a more efficient method of finding greatest common divisors will be given example what is the greatest common divisor of and solution the positive common divisors of and are and hence gcd example what is the greatest common divisor of and solution the integers and have no positive common divisors other than so that gcd because it is often important to specify that two integers have no common positive divisor other than we have definition definition example by example it follows that the integers and are relatively prime because gcd because we often need to specify that no two integers in a set of integers have a common positive divisor greater than we make definition definition example determine whether the integers and are pairwise relatively prime and whether the integers and are pairwise relatively prime solution because gcd gcd and gcd we conclude that and are pairwise relatively prime because gcd we see that and are not pairwise relatively prime another way to find the greatest common divisor of two positive integers is to use the prime factorizations of these integers suppose that the prime factorizations of the positive integers a and b are a pan b pbn where each exponent is a nonnegative integer and where all primes occurring in the prime factorization of either a or b are included in both factorizations with zero exponents if necessary then gcd a b is given by gcd a b pmin pmin pmin an bn where min x y represents the minimum of the two numbers x and y to show that this formula for gcd a b is valid we must show that the integer on the right hand side divides both a and b and that no larger integer also does this integer does divide both a and b because the power of each prime in the factorization does not exceed the power of this prime in either the factorization of a or that of b further no larger integer can divide both a and b because the exponents of the primes in this factorization cannot be increased and no other primes can be included example because the prime factorizations of and are and the greatest common divisor is gcd 3min prime factorizations can also be used to find the least common multiple of two integers definition the least common multiple exists because the set of integers divisible by both a and b is nonempty as ab belongs to this set for instance and every nonempty set of positive integers has a least element by the well ordering property which will be discussed in section suppose that the prime factorizations of a and b are as before then the least common multiple of a and b is given by lcm a b pmax pmax pmax an bn where max x y denotes the maximum of the two numbers x and y this formula is valid because a common multiple of a and b has at least max ai bi factors of pi in its prime factorization and the least common multiple has no other prime factors besides those in a and b example what is the least common multiple of and solution we have lcm 3max theorem gives the relationship between the greatest common divisor and least common multiple of two integers it can be proved using the formulae we have derived for these quantities the proof of this theorem is left as exercise theorem the euclidean algorithm computing the greatest common divisor of two integers directly from the prime factorizations of these integers is inefficient the reason is that it is time consuming to find prime factoriza tions we will give a more efficient method of finding the greatest common divisor called the euclidean algorithm this algorithm has been known since ancient times it is named after the ancient greek mathematician euclid who included a description of this algorithm in his book the elements before describing the euclidean algorithm we will show how it is used to find gcd first divide the larger of the two integers by the smaller to obtain any divisor of and must also be a divisor of also any divisor of and must also be a divisor of hence the greatest common divisor of and is the same as the greatest common divisor of and this means that the problem of finding gcd has been reduced to the problem of finding gcd next divide by to obtain because any common divisor of and also divides and any common divisor of and divides it follows that gcd gcd continue by dividing by to obtain because divides it follows that gcd furthermore because gcd gcd gcd the original problem has been solved we now describe how the euclidean algorithm works in generality we will use successive divisions to reduce the problem of finding the greatest common divisor of two positive integers to the same problem with smaller integers until one of the integers is zero the euclidean algorithm is based on the following result about greatest common divisors and the division algorithm euclid b c e b c e euclid was the author of the most successful mathematics book ever written the elements which appeared in over different editions from ancient to modern times little is known about euclid life other than that he taught at the famous academy at alexandria in egypt apparently euclid did not stress applications when a student asked what he would get by learning geometry euclid explained that knowledge was worth acquiring for its own sake and told his servant to give the student a coin because he must make a profit from what he learns lemma proof if we can show that the common divisors of a and b are the same as the common divisors of b and r we will have shown that gcd a b gcd b r because both pairs must have the same greatest common divisor so suppose that d divides both a and b then it follows that d also divides a bq r from theorem of section hence any common divisor of a and b is also a common divisor of b and r likewise suppose that d divides both b and r then d also divides bq r a hence any common divisor of b and r is also a common divisor of a and b consequently gcd a b gcd b r suppose that a and b are positive integers with a b let a and b when we successively apply the division algorithm we obtain rn rn rn rn rn rn rnqn eventually a remainder of zero occurs in this sequence of successive divisions because the sequence of remainders a cannot contain more than a terms fur thermore it follows from lemma that gcd a b gcd gcd gcd rn rn gcd rn rn gcd rn rn hence the greatest common divisor is the last nonzero remainder in the sequence of divisions example find the greatest common divisor of and using the euclidean algorithm solution successive uses of the division algorithm give 248 hence gcd 662 because is the last nonzero remainder the euclidean algorithm is expressed in pseudocode in algorithm in algorithm the initial values of x and y are a and b respectively at each stage of the procedure x is replaced by y and y is replaced by x mod y which is the remainder when x is divided by y this process is repeated as long as y the algorithm terminates when y and the value of x at that point the last nonzero remainder in the procedure is the greatest common divisor of a and b we will study the time complexity of the euclidean algorithm in section where we will show that the number of divisions required to find the greatest common divisor of a and b where a b is o log b gcds as linear combinations an important result we will use throughout the remainder of this section is that the greatest common divisor of two integers a and b can be expressed in the form sa tb where and t are integers in other words gcd a b can be expressed as a linear combination with integer coefficients of a and b for example gcd and we state this fact as theorem theorem étienne bézout bézout was born in nemours france where his father was a magistrate reading the writings of the great mathematician leonhard euler enticed him to become a mathematician in he was appointed to a position at the académie des sciences in paris in he was appointed examiner of the gardes de la marine where he was assigned the task of writing mathematics textbooks this assignment led to a four volume textbook completed in bézout is well known for his six volume comprehensive textbook on mathematics his textbooks were extremely popular and were studied by many generations of students hoping to enter the école polytechnique the famous engineering and science school his books were translated into english and used in north america including at harvard his most important original work was published in in the book théorie générale des équations algébriques where he introduced important methods for solving simultaneous polynomial equations in many unknowns the most well known result in this book is now called bézout theorem which in its general form tells us that the number of common points on two plane algebraic curves equals the product of the degrees of these curves bézout is also credited with inventing the determinant which was called the bézoutian by the great english mathematician james joseph sylvester he was considered to be a kind person with a warm heart although he had a reserved and somber personality he was happily married and a father definition we will not give a formal proof of theorem here see exercise in section and for proofs we will provide an example of a general method that can be used to find a linear combination of two integers equal to their greatest common divisor in this section we will assume that a linear combination has integer coefficients the method proceeds by working backward through the divisions of the euclidean algorithm so this method requires a forward pass and a backward pass through the steps of the euclidean algorithm in the exercises we will describe an algorithm called the extended euclidean algorithm which can be used to express gcd a b as a linear combination of a and b using a single pass through the steps of the euclidean algorithm see the preamble to exercise example express gcd as a linear combination of and solution to show that gcd the euclidean algorithm uses these divisions using the next to last division the third division we can express gcd as a linear combination of and we find that the second division tells us that substituting this expression for into the previous equation we can express as a linear combination of and we have the first division tells us that substituting this expression for into the previous equation we can express as a linear combination of and we conclude that 198 completing the solution we will use theorem to develop several useful results one of our goals will be to prove the part of the fundamental theorem of arithmetic asserting that a positive integer has at most one prime factorization we will show that if a positive integer has a factorization into primes where the primes are written in nondecreasing order then this factorization is unique first we need to develop some results about divisibility lemma proof because gcd a b by bézout theorem there are integers and t such that sa tb multiplying both sides of this equation by c we obtain sac tbc c we can now use theorem of section to show that a c by part ii of that theorem a tbc because a sac and a tbc by part i of that theorem we conclude that a divides sac tbc because sac tbc c we conclude that a c completing the proof we will use the following generalization of lemma in the proof of uniqueness of prime factorizations the proof of lemma is left as exercise in section because it can be most easily carried out using the method of mathematical induction covered in that section lemma we can now show that a factorization of an integer into primes is unique that is we will show that every integer can be written as the product of primes in nondecreasing order in at most one way this is part of the fundamental theorem of arithmetic we will prove the other part that every integer has a factorization into primes in section proof of the uniqueness of the prime factorization of a positive integer we will use a proof by contradiction suppose that the positive integer n can be written as the product of primes in two different ways say n ps and n qt each pi and qj are primes such that ps and qt when we remove all common primes from the two factorizations we have piu qjv where no prime occurs on both sides of this equation and u and v are positive integers by lemma it follows that divides qjk for some k because no prime divides another prime this is impossible consequently there can be at most one factorization of n into primes in nondecreasing order lemma can also be used to prove a result about dividing both sides of a congruence by the same integer we have shown theorem in section that we can multiply both sides of a congruence by the same integer however dividing both sides of a congruence by an integer does not always produce a valid congruence as example shows example the congruence mod holds but both sides of this congruence cannot be divided by to produce a valid congruence because and but mod although we cannot divide both sides of a congruence by any integer to produce a valid congruence we can if this integer is relatively prime to the modulus theorem establishes this important fact we use lemma in the proof theorem exercises proof because ac bc mod m m ac bc c a b by lemma because gcd c m it follows that m a b we conclude that a b mod m determine whether each of these integers is prime a b c d e f determine whether each of these integers is prime a b c d e f find the prime factorization of each of these integers a 88 b c d e f find the prime factorization of each of these integers a b c d e f find the prime factorization of how many zeros are there at the end of express in pseudocode the trial division algorithm for determining whether an integer is prime express in pseudocode the algorithm described in the text for finding the prime factorization of an integer show that if am is composite if a and m are integers greater than and m is odd hint show that x is a factor of the polynomial xm if m is odd which positive integers less than are relatively prime to which positive integers less than are relatively prime to determine whether the integers in each of these sets are pairwise relatively prime a b c d determine whether the integers in each of these sets are pairwise relatively prime a b c d we call a positive integer perfect if it equals the sum of its positive divisors other than itself a show that and are perfect b show that is a perfect number when is prime show that if is prime then n is prime hint use the identity b b determine whether each of these integers is prime veri fying some of mersenne claims a b show that if is an odd prime then m for some nonnegative integer n hint first show that the polynomial identity xm xk xk t xk t xk holds where m kt and t c d the value of the euler φ function at the positive integer n is defined to be the number of positive integers less than or equal to n that are relatively prime to n note φ is the greek is odd show that is an irrational number recall that an ir rational number is a real number x that cannot be written as the ratio of two integers prove that for every positive integer n there are n con secutive composite integers hint consider the n con secutive integers starting with n prove or disprove that there are three consecutive odd positive integers that are primes that is odd primes of the form p p and p letter phi find these values of the euler φ function a φ b φ c φ show that n is prime if and only if φ n n what is the value of φ pk when p is prime and k is a positive integer what are the greatest common divisors of these pairs of integers a b c d e f what are the greatest common divisors of these pairs of integers a b c d using the method followed in example express the greatest common divisor of each of these pairs of integers as a linear combination of these integers a b c d e f g h i the extended euclidean algorithm can be used to express gcd a b as a linear combination with integer coefficients of e 212 f 1111 the integers a and b we set and let and what is the least common multiple of each pair in exer cise what is the least common multiple of each pair in exer cise find gcd and lcm and verify that gcd lcm 625 find gcd and lcm and verify that gcd lcm hint first find the prime factorizations of and 123552 if the product of two integers is and their great est common divisor is what is their least common multiple show that if a and b are positive integers then ab gcd a b lcm a b hint use the prime factorizations of a and b and the formulae for gcd a b and lcm a b in terms of these factorizations use the euclidean algorithm to find a gcd b gcd c gcd d gcd e gcd f gcd use the euclidean algorithm to find a gcd b gcd c gcd d gcd e gcd f gcd how many divisions are required to find gcd us ing the euclidean algorithm how many divisions are required to find gcd us ing the euclidean algorithm show that if a and b are both positive integers then mod mod b use exercise to show that if a and b are posi tive integers then gcd a b hint show that the remainders obtained when the eu clidean algorithm is used to compute gcd are of the form where r is a remainder arising when the euclidean algorithm is used to find gcd a b use exercise to show that the integers and are pairwise relatively prime using the method followed in example express the greatest common divisor of each of these pairs of integers as a linear combination of these integers a b c d e f g h i sj sj qj and tj tj qj for j n where the qj are the quotients in the di visions used when the euclidean algorithm finds gcd a b as shown in the text it can be shown see that gcd a b sna tnb the main advantage of the extended euclidean algorithm is that it uses one pass through the steps of the euclidean algorithm to find bézout coefficients of a and b unlike the method in the text which uses two passes use the extended euclidean algorithm to express gcd as a linear combination of and use the extended euclidean algorithm to express gcd as a linear combination of and use the extended euclidean algorithm to express gcd as a linear combination of and use the extended euclidean algorithm to express gcd as a linear combination of and describe the extended euclidean algorithm using pseu docode find the smallest positive integer with exactly n different positive factors when n is a b c d e can you find a formula or rule for the nth term of a se quence related to the prime numbers or prime factoriza tions so that the initial terms of the sequence have these values a b c d e f 510510 can you find a formula or rule for the nth term of a se quence related to the prime numbers or prime factoriza tions so that the initial terms of the sequence have these values a b c d e f 361 1369 prove that the product of any three consecutive integers is divisible by show that if a b and m are integers such that m and a b mod m then gcd a m gcd b m prove or disprove that is prime when ever n is a positive integer prove or disprove that pn is prime for every positive integer n where pn are the n small est prime numbers show that there is a composite integer in every arithmetic progression ak b k where a and b are pos itive integers adapt the proof in the text that there are infinitely many primes to prove that there are infinitely many primes of the form where k is a nonnegative inte ger hint suppose that there are only finitely many of the form where k is a nonnegative inte ger hint suppose that there are only finitely many such primes qn and consider the number qn prove that the set of positive rational numbers is countable by setting up a function that assigns to a rational num ber p q with gcd p q the base number formed by the decimal representation of p followed by the base digit a which corresponds to the decimal number followed by the decimal representation of q prove that the set of positive rational numbers is countable by showing that the function k is a one to one correspon dence between the set of positive rational numbers and the set of positive integers if k m n qn 1q2b2 where gcd m n adapt the proof in the text that there are infinitely many primes to prove that there are infinitely many primes and the prime power factorizations of m and n are m pas and n qbt solving congruences introduction solving linear congruences which have the form ax b mod m is an essential task in the study of number theory and its applications just as solving linear equations plays an important role in calculus and linear algebra to solve linear congruences we employ inverses modulo m we explain how to work backwards through the steps of the euclidean algorithm to find inverses modulo m once we have found an inverse of a modulo m we solve the congruence ax b mod m by multiplying both sides of the congruence by this inverse simultaneous systems of linear congruence have been studied since ancient times for example the chinese mathematician sun tsu studied them in the first century we will show how to solve systems of linear congruences modulo pairwise relatively prime moduli the result we will prove is called the chinese remainder theorem and our proof will give a method to find all solutions of such systems of congruences we will also show how to use the chinese remainder theorem as a basis for performing arithmetic with large integers we will introduce a useful result of fermat known as fermat little theorem which states that if p is prime and p does not divide a then ap mod p we will examine the converse of this statement which will lead us to the concept of a pseudoprime a pseudoprime m to the base a is a composite integer m that masquerades as a prime by satisfying the congruence am mod m we will also give an example of a carmichael number which is a composite integer that is a pseudoprime to all bases a relatively prime to it we also introduce the notion of discrete logarithms which are analogous to ordinary loga rithms to define discrete logarithms we must first define primitive roots a primitive root of a prime p is an integer r such that every integer not divisible by p is congruent to a power of r modulo p if r is a primitive root of p and re a mod p then e is the discrete logarithm of a modulo p to the base r finding discrete logarithms turns out to be an extremely difficult prob lem in general the difficulty of this problem is the basis for the security of many cryptographic systems linear congruences a congruence of the form ax b mod m where m is a positive integer a and b are integers and x is a variable is called a linear congruence such congruences arise throughout number theory and its applications how can we solve the linear congruence ax b mod m that is how can we find all integers x that satisfy this congruence one method that we will describe uses an integer a such that aa mod m if such an integer exists such an integer a is said to be an inverse of a modulo m theorem guarantees that an inverse of a modulo m exists whenever a and m are relatively prime theorem proof by theorem of section because gcd a m there are integers and t such that sa tm this implies that sa tm mod m because tm mod m it follows that sa mod m consequently is an inverse of a modulo m that this inverse is unique modulo m is left as exercise using inspection to find an inverse of a modulo m is easy when m is small to find this inverse we look for a multiple of a that exceeds a multiple of m by for example to find an inverse of modulo we can find j for j stopping when we find a multiple of that is one more than a multiple of we can speed this approach up if we note that mod this means that mod hence mod so is an inverse of modulo we can design a more efficient algorithm than brute force to find an inverse of a modulo m when gcd a m using the steps of the euclidean algorithm by reversing these steps as in example of section we can find a linear combination sa tm where and t are integers reducing both sides of this equation modulo m tells us that is an inverse of a modulo m we illustrate this procedure in example example find an inverse of modulo by first finding bézout coefficients of and note that we have already shown that is an inverse of modulo by inspection solution because gcd theorem tells us that an inverse of modulo exists the euclidean algorithm ends quickly when used to find the greatest common divisor of and from this equation we see that this shows that and are bézout coefficients of and we see that is an inverse of modulo note that every integer congruent to modulo is also an inverse of such as and so on example find an inverse of modulo solution for completeness we present all steps used to compute an inverse of modulo only the last step goes beyond methods developed in section and illustrated in example in that section first we use the euclidean algorithm to show that gcd then we will reverse the steps to find bézout coefficients a and b such that it will then follow that a is an inverse of modulo the steps used by the euclidean algorithm to find gcd are because the last nonzero remainder is we know that gcd we can now find the bézout coefficients for and by working backwards through these steps expressing gcd in terms of each successive pair of remainders in each step we eliminate the remainder by expressing it as a linear combination of the divisor and the dividend we obtain that tells us that and are bézout coefficients of and and is an inverse of modulo 4620 once we have an inverse a of a modulo m we can solve the congruence ax b mod m by multiplying both sides of the linear congruence by a as example illustrates example what are the solutions of the linear congruence mod solution by example we know that is an inverse of modulo multiplying both sides of the congruence by shows that mod because mod and mod it follows that if x is a solution then x mod we need to determine whether every x with x mod is a solution assume that x mod then by theorem of section it follows that mod which shows that all such x satisfy the congruence we conclude that the solutions to the congruence are the integers x such that x mod namely and the chinese remainder theorem systems of linear congruences arise in many contexts for example as we will see later they are the basis for a method that can be used to perform arithmetic with large integers such systems can even be found as word puzzles in the writings of ancient chinese and hindu mathematicians such as that given in example example in the first century the chinese mathematician sun tsu asked there are certain things whose number is unknown when divided by the remainder is when divided by the remainder is and when divided by the remainder is what will be the number of things this puzzle can be translated into the following question what are the solutions of the systems of congruences x mod x mod x mod we will solve this system and with it sun tsu puzzle later in this section the chinese remainder theorem named after the chinese heritage of problems involving systems of linear congruences states that when the moduli of a system of linear congruences are pairwise relatively prime there is a unique solution of the system modulo the product of the moduli theorem proof to establish this theorem we need to show that a solution exists and that it is unique modulo m we will show that a solution exists by describing a way to construct this solution showing that the solution is unique modulo m is exercise to construct a simultaneous solution first let mk m mk for k n that is mk is the product of the moduli except for mk because mi and mk have no common factors greater than when i k it follows that gcd mk mk conse quently by theorem we know that there is an integer yk an inverse of mk modulo mk such that mkyk mod mk to construct a simultaneous solution form the sum x anmnyn we will now show that x is a simultaneous solution first note that because mj mod mk whenever j k all terms except the kth term in this sum are congruent to modulo mk because mkyk mod mk we see that x akmkyk ak mod mk for k n we have shown that x is a simultaneous solution to the n congruences example illustrates how to use the construction given in our proof of the chinese remainder theorem to solve a system of congruences we will solve the system given in example arising in sun tsu puzzle example to solve the system of congruences in example first let m m m and m we see that is an inverse of modulo because mod is an inverse of modulo because mod and is an inverse of mod because mod the solutions to this system are those x such that x mod it follows that is the smallest positive integer that is a simultaneous solution we conclude that is the smallest positive integer that leaves a remainder of when divided by a remainder of when divided by and a remainder of when divided by although the construction in theorem provides a general method for solving systems of linear congruences with pairwise relatively prime moduli it can be easier to solve a system using a different method example illustrates the use of a method known as back substitution example use the method of back substitution to find all integers x such that x mod x mod and x mod solution by theorem in section the first congruence can be rewritten as an equality x where t is an integer substituting this expression for x into the second congruence tells us that mod which can be easily solved to show that t mod as the reader should verify using theorem in section again we see that t where u is an integer substituting this expression for t back into the equation x tells us that x we insert this into the third equation to obtain mod solving this congruence tells us that u mod as the reader should verify hence theo rem in section tells us that u where v is an integer substituting this expression for u into the equation x tells us that x trans lating this back into a congruence we find the solution to the simultaneous congruences x mod computer arithmetic with large integers suppose that mn are pairwise relatively prime moduli and let m be their product by the chinese remainder theorem we can show see exercise that an integer a with a m can be uniquely represented by the n tuple consisting of its remainders upon division by mi i n that is we can uniquely represent a by a mod a mod a mod mn example what are the pairs used to represent the nonnegative integers less than when they are rep resented by the ordered pair where the first component is the remainder of the integer upon division by and the second component is the remainder of the integer upon division by solution we have the following representations obtained by finding the remainder of each integer when it is divided by and by to perform arithmetic with large integers we select moduli mn where each mi is an integer greater than gcd mi mj whenever i j and m mn is greater than the results of the arithmetic operations we want to carry out once we have selected our moduli we carry out arithmetic operations with large integers by performing componentwise operations on the n tuples representing these integers using their remainders upon division by mi i n once we have computed the value of each component in the result we recover its value by solving a system of n congruences modulo mi i n this method of performing arithmetic with large integers has several valu able features first it can be used to perform arithmetic with integers larger than can ordinarily be carried out on a computer second computations with respect to the different moduli can be done in parallel speeding up the arithmetic example suppose that performing arithmetic with integers less than on a certain processor is much quicker than doing arithmetic with larger integers we can restrict almost all our computations to integers less than if we represent integers using their remainders modulo pairwise relatively prime integers less than for example we can use the moduli of and these integers are relatively prime pairwise because no two have a common factor greater than by the chinese remainder theorem every nonnegative integer less than can be represented uniquely by its remainders when divided by these four mod uli for example we represent as because mod mod mod and mod similarly we represent as 92 to find the sum of and we work with these tuples instead of these two integers directly we add the tuples componentwise and reduce each component with respect to the appropriate modulus this yields 92 mod mod mod mod to find the sum that is the integer represented by we need to solve the system of congruences x mod x mod x mod x mod it can be shown see exercise that is the unique nonnegative solution of this system less than 403 consequently is the sum note that it is only when we have to recover the integer represented by that we have to do arithmetic with integers larger than particularly good choices for moduli for arithmetic with large integers are sets of integers of the form where k is a positive integer because it is easy to do binary arithmetic modulo such integers and because it is easy to find sets of such integers that are pairwise relatively prime the second reason is a consequence of the fact that gcd a b as exercise in section shows suppose for instance that we can do arithmetic with integers less than easily on our computer but that working with larger integers requires special procedures we can use pairwise relatively prime moduli less than to perform arithmetic with integers as large as their product for example as exercise in section shows the integers and are pairwise relatively prime because the product of these six moduli exceeds we can perform arithmetic with integers as large as as long as the results do not exceed this number by doing arithmetic modulo each of these six moduli none of which exceeds fermat little theorem the great french mathematician pierre de fermat made many important discoveries in number theory one of the most useful of these states that p divides ap whenever p is prime and a is an integer not divisible by p fermat announced this result in a letter to one of his correspondents however he did not include a proof in the letter stating that he feared the proof would be too long although fermat never published a proof of this fact there is little doubt that he knew how to prove it unlike the result known as fermat last theorem the first published proof is credited to leonhard euler we now state this theorem in terms of congruences theorem remark fermat little theorem tells us that if a zp then ap in zp the proof of theorem is outlined in exercise fermat little theorem is extremely useful in computing the remainders modulo p of large powers of integers as example illustrates example find mod solution we can use fermat little theorem to evaluate mod rather than using the fast modular exponentiation algorithm by fermat little theorem we know that mod so k mod for every positive integer k to take advantage of this last congruence we divide the exponent by finding that we now see that 2272 mod it follows that mod example illustrated how we can use fermat little theorem to compute an mod p where p is prime and p a first we use the division algorithm to find the quotient q and remainder r when n is divided by p so that n q p r where r p it follows that an aq p r ap qar ar mod p hence to find an mod p we only need to compute ar mod p we will take advantage of this simplification many times in our study of number theory pseudoprimes in se ction we showed that an integer n is prime when it is not divisible by any prime p with p n unfortunately using this criterion to sh ow that a given integer is prime is inefficient it requires that we find all primes not exceeding n and that we carry out trial division by each such prime to see whether it divides n are there more efficient ways to determine whether an integer is prime according to some sources ancient chinese mathematicians believed that n was an odd prime if and only if mod n if this were true it would provide an efficient primality test why did they believe this congruence could be used to determine whether an integer n is prime first they observed that the congruence holds whenever n is an odd prime for example is prime and mod by fermat little theorem we know that this observation was correct that is mod n whenever n is an odd prime second they never found a composite integer n for which the congruence holds however the ancient chinese were only partially correct they were correct in thinking that the congruence holds whenever n is prime but they were incorrect in concluding that n is necessarily prime if the congruence holds unfortunately there are composite integers n such that mod n such integers are called pseudoprimes to the base example the integer is a pseudoprime to the base because it is composite and as exercise shows mod we can use an integer other than as the base when we study pseudoprimes definition given a positive integer n determining whether mod n is a useful test that pro vides some evidence concerning whether n is prime in particular if n satisfies this congruence then it is either prime or a pseudoprime to the base if n does not satisfy this congruence it is composite we can perform similar tests using bases b other than and obtain more evidence as to whether n is prime if n passes all such tests it is either prime or a pseudoprime to all the bases b we have chosen furthermore among the positive integers not exceeding x where x is a positive real number compared to primes there are relatively few pseudoprimes to the base b where b is a positive integer for example among the positive integers less than 1010 there are 512 primes but only pseudoprimes to the base unfortunately we pierre de fermat 1665 pierre de fermat one of the most important mathematicians of the seventeenth century was a lawyer by profession he is the most famous amateur mathematician in history fermat published little of his mathematical discoveries it is through his correspondence with other mathematicians that we know of his work fermat was one of the inventors of analytic geometry and developed some of the fundamental ideas of calculus fermat along with pascal gave probability theory a mathematical basis fermat formulated what was the most famous unsolved problem in mathematics he asserted that the equation xn yn zn has no nontrivial positive integer solutions when n is an integer greater than for more than years no proof or counterexample was found in his copy of the works of the ancient greek mathematician diophantus fermat wrote that he had a proof but that it would not fit in the margin because the first proof found by andrew wiles in relies on sophisticated modern mathematics most people think that fermat thought he had a proof but that the proof was incorrect however he may have been tempting others to look for a proof not being able to find one himself cannot distinguish between primes and pseudoprimes just by choosing sufficiently many bases because there are composite integers n that pass all tests with bases b such that gcd b n this leads to definition definition example the integer is a carmichael number to see this first note that is composite be cause next note that if gcd b then gcd b gcd b gcd b using fermat little theorem we find that b2 mod mod and mod it follows that b2 mod mod mod by exercise it follows that mod for all positive integers b with gcd b hence is a carmichael number although there are infinitely many carmichael numbers more delicate tests described in the exercise set can be devised that can be used as the basis for efficient probabilistic primality tests such tests can be used to quickly show that it is almost certainly the case that a given integer is prime more precisely if an integer is not prime then the probability that it passes a series of tests is close to we will describe such a test in chapter and discuss the notions from probability theory that this test relies on these probabilistic primality tests can be used and are used to find large primes extremely rapidly on computers primitive roots and discrete logarithms in the set of positive real numbers if b and x by we say that y is the logarithm of x to the base b here we will show that we can also define the concept of logarithms modulo p of positive integers where p is a prime before we do so we need a definition definition robert daniel carmichael robert daniel carmichael was born in alabama he re ceived his undergraduate degree from lineville college in and his ph d in from princeton carmichael held positions at indiana university from until and at the university of illinois from until carmichael was an active researcher in a wide variety of areas including number theory real analysis differential equations mathematical physics and group theory his ph d thesis written under the direction of g d birkhoff is considered the first significant american contribution to the subject of differential equations example determine whether and are primitive roots modulo solution when we compute the powers of in we obtain because every element of is a power of is a primitive root of when we compute the powers of modulo we obtain we note that this pattern repeats when we compute higher powers of because not all elements of are powers of we conclude that is not a primitive root of an important fact in number theory is that there is a primitive root modulo p for every prime p we refer the reader to for a proof of this fact suppose that p is prime and r is a primitive root modulo p if a is an integer between and p that is an element of zp we know that there is an unique exponent e such that re a in zp that is re mod p a definition example find the discrete logarithms of and modulo to the base solution when we computed the powers of modulo in example we found that and in hence the discrete logarithms of and modulo to the base are and respectively these are the powers of that equal and respectively in we write and where the modulus is understood and not explicitly noted in the notation the discrete logarithm the discrete logarithm problem takes as input a prime p a primitive root r modulo p problem is hard and a positive integer a zp its output is the discrete logarithm of a modulo p to the base r although this problem might seem not to be that difficult it turns out that no polynomial time algorithm is known for solving it the difficulty of this problem plays an important role in cryptography as we will see in section exercises show that is an inverse of modulo show that is an inverse of modulo by inspection as discussed prior to example find an inverse of modulo by inspection as discussed prior to example find an inverse of modulo find an inverse of a modulo m for each of these pairs of relatively prime integers using the method followed in example a a m b a m c a m d a m find an inverse of a modulo m for each of these pairs of relatively prime integers using the method followed in example a a m b a m c a m d a m show that if a and m are relatively prime positive inte gers then the inverse of a modulo m is unique modulo m hint assume that there are two solutions b and c of the congruence ax mod m use theorem of section to show that b c mod m show that an inverse of a modulo m where a is an in teger and m is a positive integer does not exist if gcd a m solve the congruence mod using the inverse of modulo found in part a of exercise solve the congruence mod using the inverse of modulo found in part a of exercise solve each of these congruences using the modular in verses found in parts b c and d of exercise a mod b mod c mod solve each of these congruences using the modular in verses found in parts b c and d of exercise a mod b mod c mod find the solutions of the congruence mod hint show the congruence is equivalent to the congruence mod factor the left hand side of the congruence show that a solution of the quadratic congruence is a solution of one of the two different linear congruences find the solutions of the congruence mod hint show the congruence is equivalence to the congruence mod fac tor the left hand side of the congruence show that a so lution of the quadratic congruence is a solution of one of two different linear congruences show that if m is an integer greater than and ac bc mod m then a b mod m gcd c m a show that the positive integers less than except and can be split into pairs of integers such that each pair consists of integers that are inverses of each other modulo b use part a to show that mod show that if p is prime the only solutions of mod p are integers x such that x mod p or x mod p a generalize the result in part a of exercise that is show that if p is a prime the positive integers less than p except and p can be split into p pairs of integers such that each pair consists of inte gers that are inverses of each other hint use the result of exercise b from part a conclude that p mod p whenever p is prime this result is known as wilson theorem c what can we conclude if n is a positive integer such that n mod n this exercise outlines a proof of fermat little theorem a suppose that a is not divisible by the prime p show that no two of the integers a a p a are congruent modulo p b conclude from part a that the product of p is congruent modulo p to the prod uct of a p a use this to show that p ap p mod p c use theorem of section to show from part b that ap mod p if p a hint use lemma of section to show that p does not divide p and then use theorem of section alternatively use wilson theorem from exercise b d use part c to show that ap a mod p for all in tegers a use the construction in the proof of the chinese remainder theorem to find all solutions to the system of congruences x mod x mod and x mod use the construction in the proof of the chinese remain der theorem to find all solutions to the system of congru ences x mod x mod x mod and x mod solve the system of congruence x mod and x mod using the method of back substitution solve the system of congruences in exercise using the method of back substitution solve the system of congruences in exercise using the method of back substitution write out in pseudocode an algorithm for solving a si multaneous system of linear congruences based on the construction in the proof of the chinese remainder theo rem find all solutions if any to the system of congruences x mod x mod and x mod find all solutions if any to the system of congruences x mod x mod and x mod use the chinese remainder theorem to show that an integer a with a m mn where the positive integers mn are pairwise relatively prime can be represented uniquely by the n tuple a mod a mod a mod mn let mn be pairwise relatively prime integers greater than or equal to show that if a b mod mi for i n then a b mod m where m mn this result will be used in exercise to prove the chinese remainder theorem consequently do not use the chinese remainder theorem to prove it complete the proof of the chinese remainder theorem by showing that the simultaneous solution of a system of linear congruences modulo pairwise relatively prime moduli is unique modulo the product of these moduli hint assume that x and y are two simultaneous solu tions show that mi x y for all i using exercise conclude that m mn x y which integers leave a remainder of when divided by and also leave a remainder of when divided by which integers are divisible by but leave a remainder of when divided by use fermat little theorem to find mod use fermat little theorem to find mod use fermat little theorem to show that if p is prime and p a then ap is an inverse of a modulo p use exercise to find an inverse of modulo a show that mod by fermat little theo rem and noting that b show that mod using the fact that 3268 c conclude from parts a and b that mod a use fermat little theorem to compute mod mod and mod b use your results from part a and the chinese re mainder theorem to find mod note that a use fermat little theorem to compute mod mod and mod b use your results from part a and the chinese re mainder theorem to find mod note that show with the help of fermat little theorem that if n is a positive integer then divides n show that if p is an odd prime then every divisor of the mersenne number is of the form where k is a nonnegative integer hint use fermat little the orem and exercise of section use exercise to determine whether and 607 are prime use exercise to determine whether and 131 are prime let n be a positive integer and let n where is a nonnegative integer and t is an odd positive integer we say that n passes miller test for the base b if either bt mod n or t mod n for some j with j it can be shown see that a composite integer n passes miller test for fewer than n bases b with b n a composite positive integer n that passes miller test to the base b is called a strong pseudoprime to the base b show that if n is prime and b is a positive integer with n b then n passes miller test to the base b show that is a strong pseudoprime to the base by showing that it passes miller test to the base but is composite show that is a carmichael number show that if n pk where pk are distinct primes that satisfy pj n for j k then n is a carmichael number a use exercise to show that every integer of the form where m is a positive integer and and are all primes is a carmichael number b use part a to show that is a car michael number find the nonnegative integer a less than represented by each of these pairs where each pair represents a mod show that is a primitive root of find the discrete logarithms of and to the base mod ulo let p be an odd prime and r a primitive root of p show that if a and b are positive integers in zp then logr ab logr a logr b mod p write out a table of discrete logarithms modulo with respect to the primitive root if m is a positive integer the integer a is a quadratic residue of m if gcd a m and the congruence a mod m has a solution in other words a quadratic residue of m is an integer relatively prime to m that is a perfect square mod ulo m if a is not a quadratic residue of m and gcd a m we say that it is a quadratic nonresidue of m for exam ple is a quadratic residue of because gcd and mod and is a quadratic nonresidue of because gcd and mod has no solution which integers are quadratic residues of show that if p is an odd prime and a is an integer not divisible by p then the congruence a mod p has either no solutions or exactly two incongruent solutions modulo p show that if p is an odd prime then there are exactly p quadratic residues of p among the integers p if p is an odd prime and a is an integer not divisible by p the legendre symbol a is defined to be if a is a quadratic p residue of p and otherwise show that if p is an odd prime and a and b are integers with a b mod p then a b prove euler criterion which states that if p is an odd prime and a is a positive integer not divisible by p then a a p mod p hint if a is a quadratic residue modulo p apply fer mat little theorem otherwise apply wilson theorem given in exercise b use exercise to show that if p is an odd prime and a and b are integers not divisible by p then a mod ab a b a b c d e f g h i express each nonnegative integer a less than as a pair a mod a mod explain how to use the pairs found in exercise to add and solve the system of congruences that arises in example p p p show that if p is an odd prime then is a quadratic residue of p if p mod and is not a quadratic residue of p if p mod hint use exercise find all solutions of the congruence mod hint find the solutions of this congruence modulo and modulo and then use the chinese remainder theorem find all solutions of the congruence mod hint find the solutions of this congruence modulo modulo and modulo and then use the chinese re mainder theorem describe a brute force algorithm for solving the discrete logarithm problem and find the worst case and average case time complexity of this algorithm applications of congruences congruences have many applications to discrete mathematics computer science and many other disciplines we will introduce three applications in this section the use of congruences to assign memory locations to computer files the generation of pseudorandom numbers and check digits suppose that a customer identification number is ten digits long to retrieve customer files quickly we do not want to assign a memory location to a customer record using the ten digit identification number instead we want to use a smaller integer associated to the identification number this can be done using what is known as a hashing function in this section we will show how we can use modular arithmetic to do hashing constructing sequences of random numbers is important for randomized algorithms for simulations and for many other purposes constructing a sequence of truly random numbers is extremely difficult or perhaps impossible because any method for generating what are supposed to be random numbers may generate numbers with hidden patterns as a consequence methods have been developed for finding sequences of numbers that have many desirable properties of random numbers and which can be used for various purposes in place of random numbers in this section we will show how to use congruences to generate sequences of pseudorandom numbers the advantage is that the pseudorandom numbers so generated are constructed quickly the disadvantage is that they have too much predictability to be used for many tasks congruences also can be used to produce check digits for identification numbers of various kinds such as code numbers used to identify retail products numbers used to identify books airline ticket numbers and so on we will explain how to construct check digits using congru ences for a variety of types of identification numbers we will show that these check digits can be used to detect certain kinds of common errors made when identification numbers are printed hashing functions the central computer at an insurance company maintains records for each of its customers how can memory locations be assigned so that customer records can be retrieved quickly the solution to this problem is to use a suitably chosen hashing function records are identified using a key which uniquely identifies each customer records for instance customer records are often identified using the social security number of the customer as the key a hashing function h assigns memory location h k to the record that has k as its key in practice many different hashing functions are used one of the most common is the function h k k mod m where m is the number of available memory locations hashing functions should be easily evaluated so that files can be quickly located the hashing function h k k mod m meets this requirement to find h k we need only compute the remainder when k is divided by m furthermore the hashing function should be onto so that all memory locations are possible the function h k k mod m also satisfies this property example find the memory locations assigned by the hashing function h k k mod to the records of customers with social security numbers and solution the record of the customer with social security number is assigned to memory location because h mod similarly because h mod the record of the customer with social security number is assigned to memory location because a hashing function is not one to one because there are more possible keys than memory locations more than one file may be assigned to a memory location when this happens we say that a collision occurs one way to resolve a collision is to assign the first free location following the occupied memory location assigned by the hashing function example after making the assignments of records to memory locations in example assign a memory location to the record of the customer with social security number solution first note that the hashing function h k k mod maps the social security number to location because h mod however this location is already occupied by the file of the customer with social security number but because memory location the first location following memory location is free we assign the record of the customer with social security number to this location in example we used a linear probing function namely h k i h k i mod m to look for the first free memory location where i runs from to m there are many other ways to resolve collisions that are discussed in the references on hashing functions given at the end of the book pseudorandom numbers randomly chosen numbers are often needed for computer simulations different methods have been devised for generating numbers that have properties of randomly chosen numbers because numbers generated by systematic methods are not truly random they are called pseudorandom numbers the most commonly used procedure for generating pseudorandom numbers is the linear congruential method we choose four integers the modulus m multiplier a increment c and seed with a m c m and m we generate a se quence of pseudorandom numbers xn with xn m for all n by successively using the recursively defined function xn axn c mod m this is an example of a recursive definition discussed in section in that section we will show that such sequences are well defined many computer experiments require the generation of pseudorandom numbers between and to generate such numbers we divide numbers generated with a linear congruential generator by the modulus that is we use the numbers xn m example find the sequence of pseudorandom numbers generated by the linear congruential method with modulus m multiplier a increment c and seed solution we compute the terms of this sequence by successively using the recursively defined function xn mod beginning by inserting the seed to find we find that mod mod mod 7x1 mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod because and because each term depends only on the previous term we see that the sequence is generated this sequence contains nine different numbers before repeating most computers do use linear congruential generators to generate pseudorandom numbers often a linear congruential generator with increment c is used such a generator is called a pure multiplicative generator for example the pure multiplicative generator with modulus and multiplier is widely used with these values it can be shown that numbers are generated before repetition begins pseudorandom numbers generated by linear congruential generators have long been used for many tasks unfortunately it has been shown that sequences of pseudorandom numbers gen erated in this way do not share some important statistical properties that true random numbers have because of this it is not advisable to use them for some tasks such as large simulations for such sensitive tasks other methods are used to produce sequences of pseudorandom num bers either using some sort of algorithm or sampling numbers arising from a random physical phenomenon for more details on pseudorandom number see and check digits congruences are used to check for errors in digit strings a common technique for detecting errors in such strings is to add an extra digit at the end of the string this final digit or check digit is calculated using a particular function then to determine whether a digit string is correct a check is made to see whether this final digit has the correct value we begin with an application of this idea for checking the correctness of bit strings example parity check bits digital information is represented by bit string split into blocks of a specified size before each block is stored or transmitted an extra bit called a parity check bit can be appended to each block the parity check bit xn for the bit string xn is defined by xn xn mod it follows that xn is if there are an even number of bits in the block of n bits and it is if there are an odd number of bits in the block of n bits when we examine a string that includes a parity check bit we know that there is an error in it if the parity check bit is wrong however when the parity check bit is correct there still may be an error a parity check can detect an odd number of errors in the previous bits but not an even number of errors see exercise suppose we receive in a transmission the bit strings and each ending with a parity check bit should we accept these bit strings as correct solution before accepting these strings as correct we examine their parity check bits the parity check bit of the first string is because mod the parity check bit is correct the parity check bit of the second string is we find that mod so the parity check is incorrect we conclude that the first string may have been transmitted correctly and we know for certain that the second string was transmitted incorrectly we accept the first string as correct even though it still may contain an even number of errors but we reject the second string check bits computed using congruences are used extensively to verify the correctness of various kinds of identification numbers examples and show how check bits are computed for codes that identify products universal product codes and books international standard book numbers the preambles to exercises and introduce the use of congruences to find and use check digits in money order numbers airline ticket numbers and identification numbers for periodicals respectively note that congruences are also used to compute check digits for bank account numbers drivers license numbers credit card numbers and many other types of identification numbers example upcs retail products are identified by their universal product codes upcs the most common form of a upc has decimal digits the first digit identifies the product category the next five digits identify the manufacturer the following five identify the particular product and the last digit is a check digit the check digit is determined by the congruence mod answer these questions a suppose that the first digits of a upc are what is the check digit b is a valid upc solution a we insert the digits of into the congruence for upc check digits this gives mod simplifying we have mod hence mod it follows that x12 mod so the check digit is b to check whether is valid we insert the digits into the congruence these digits must satisfy this gives mod hence is not a valid upc example isbns all books are identified by an international standard book number isbn a remember that the check digit of an isbn can be an x digit code assigned by the publisher recently a digit code known as isbn was introduced to identify a larger number of published works see the preamble to exercise in the supplementary exercises an isbn consists of blocks identifying the language the publisher the number assigned to the book by its publishing company and finally a check digit that is either a digit or the letter x used to represent this check digit is selected so that ixi mod i or equivalently so that ixi mod i answer these questions about isbn a the first nine digits of the isbn of the sixth edition of this book are what is the check digit b is a valid isbn solution a the check digit is determined by the congruence ixi mod inserting mod this means that mod so 189 mod hence b to see whether is a valid isbn we see if ixi mod we 81 mod hence is not a valid isbn publishers sometimes do not calculate isbns correctly for their books as was done for an earlier edition of this text several kinds of errors often arise in identification numbers a single error an error in one digit of an identification number is perhaps the most common type of error another common kind of error is a transposition error which occurs when two digits are accidentally inter changed for each type of identification number including a check digit we would like to be able to detect these common types of errors as well as other types of errors we will investigate whether the check digit for isbns can detect single errors and transposition errors whether check digits for upcs can detect these kinds of errors is left as exercises and suppose that is a valid isbn so that xi mod we will show that we can detect a single error and a transposition of two digits where we include the possibility that one of the two digits is the check digit x representing suppose that this isbn has been printed with a single error as if there is a single error then for some integer j yi xi for i j and yj xj a where a and a note that a yj xj is the error in the j th place it then follows that iyi ixi ja ja mod these last two congruences hold because xi mod and ja because j error is not a valid isbn so we have detected the single now suppose that two unequal digits have been transposed it follows that there are distinct integers j and k such that yj xk and yk xj and yi xi for i j and i k hence iyi ixi jxk jxj kxj kxk j k xk xj mod i i because xi mod and j k and xk xj we see that is not a valid isbn thus we can detect the interchange of two unequal digits exercises which memory locations are assigned by the hashing function h k k mod to the records of insurance company customers with these social security numbers a b c d which memory locations are assigned by the hashing function h k k mod to the records of insurance company customers with these social security numbers a b c d a parking lot has visitor spaces numbered from to visitors are assigned parking spaces using the hashing function h k k mod where k is the number formed from the first three digits on a visitor license plate a which spaces are assigned by the hashing function to cars that have these first three digits on their license plates b describe a procedure visitors should follow to find a free parking space when the space they are assigned is occupied another way to resolve collisions in hashing is to use double hashing we use an initial hashing function h k k mod p where p is prime we also use a second hashing function g k k mod p when a collision occurs we use a probing sequence h k i h k i g k mod p use the double hashing procedure we have described with p to assign memory locations to files for em ployees with social security numbers 509496993 034367980 329938157 325510778 053708912 what sequence of pseudorandom numbers is gener ated using the linear congruential generator xn mod with seed what sequence of pseudorandom numbers is gener ated using the linear congruential generator xn mod with seed what sequence of pseudorandom numbers is gener ated using the pure multiplicative generator xn mod with seed write an algorithm in pseudocode for generating a se quence of pseudorandom numbers using a linear congru ential generator the middle square method for generating pseudorandom numbers begins with an n digit integer this number is squared initial zeros are appended to ensure that the result has digits and its middle n digits are used to form the next number in the sequence this process is repeated to generate additional terms find the first eight terms of the sequence of four digit pseudorandom numbers generated by the middle square method starting with explain why both and would be bad choices for the initial term of a sequence of four digit pseudoran dom numbers generated by the middle square method the power generator is a method for generating pseudoran dom numbers to use the power generator parameters p and d are specified where p is a prime d is a positive integer such that p d and a seed is specified the pseudorandom num bers are generated using the recursive definition xn xd mod p find the sequence of pseudorandom numbers generated by the power generator with p d and seed find the sequence of pseudorandom numbers generated by the power generator with p d and seed suppose you received these bit strings over a communi cations link where the last bit is a parity check bit in which string are you sure there is an error a b c d prove that a parity check bit can detect an error in a string if and only if the string contains an odd number of errors the first nine digits of the isbn of the european ver sion of the fifth edition of this book are what is the check digit for that book the isbn of the sixth edition of elementary number theory and its applications is where q is a digit find the value of q determine whether the check digit of the isbn for this textbook the seventh edition of discrete mathemat ics and its applications was computed correctly by the publisher the united states postal service usps sells money orders identified by an digit number the first ten dig its identify the money order is a check digit that satisfies mod find the check digit for the usps money orders that have identification number that start with these ten digits a b c d determine whether each of these numbers is a valid usps money order identification number a b c d one digit in each of these identification numbers of a postal money order is smudged can you recover the smudged digit indicated by a q in each of these num bers a b c d one digit in each of these identification numbers of a postal money order is smudged can you recover the smudged digit indicated by a q in each of these num bers a b c d determine which single digit errors are detected by the usps money order code determine which transposition errors are detected by the usps money order code determine the check digit for the upcs that have these initial digits a b c d determine whether each of the strings of digits is a valid upc code a b c d does the check digit of a upc code detect all single er rors prove your answer or find a counterexample determine which transposition errors the check digit of a upc code finds some airline tickets have a digit identification number where is a check digit that equals mod find the check digit that follows each of these initial digits of an airline ticket identification number a b c d determine whether each of these digit numbers is a valid airline ticket identification number a b c d which errors in a single digit of a digit airline ticket identification number can be detected can the accidental transposition of two consecutive dig its in an airline ticket identification number be detected using the check digit periodicals are identified using an international standard serial number issn an issn consists of two blocks of four digits the last digit in the second block is a check digit this check digit is determined by the congruence 5d3 9d7 mod when mod we use the letter x to represent in the code for each of these initial seven digits of an issn deter mine the check digit which may be the letter x a b c d are each of these eight digit codes possible issns that is do they end with a correct check digit a b c d does the check digit of an issn detect every single error in an issn justify your answer with either a proof or a counterexample does the check digit of an issn detect every error where two consecutive digits are accidentally interchanged jus tify your answer with either a proof or a counterexample cryptography introduction number theory plays a key role in cryptography the subject of transforming information so that it cannot be easily recovered without special knowledge number theory is the basis of many classical ciphers first used thousands of years ago and used extensively until the century these ciphers encrypt messages by changing each letter to a different letter or each block of letters to a different block of letters we will discuss some classical ciphers including shift ciphers which replace each letter by the letter a fixed number of positions later in the alphabet wrapping around to the beginning of the alphabet when necessary the classical ciphers we will discuss are examples of private key ciphers where knowing how to encrypt allows someone to also decrypt messages with a private key cipher two parties who wish to communicate in secret must share a secret key the classical ciphers we will discuss are also vulnerable to cryptanalysis which seeks to recover encrypted information without access to the secret information used to encrypt the message we will show how to cryptanalyze messages sent using shift ciphers number theory is also important in public key cryptography a type of cryptography invented in the in public key cryptography knowing how to encrypt does not also tell someone how to decrypt the most widely used public key system called the rsa cryptosystem encrypts messages using modular exponentiation where the modulus is the product of two large primes knowing how to encrypt requires that someone know the modulus and an exponent it does not require that the two prime factors of the modulus be known as far as it is known knowing how to decrypt requires someone to know how to invert the encryption function which can only be done in a practical amount of time when someone knows these two large prime factors in this chapter we will explain how the rsa cryptosystem works including how to encrypt and decrypt messages the subject of cryptography also includes the subject of cryptographic protocols which are exchanges of messages carried out by two or more parties to achieve a specific security goal we will discuss two important protocols in this chapter one allows two people to share a common secret key the other can be used to send signed messages so that a recipient can be sure that they were sent by the purported sender classical cryptography one of the earliest known uses of cryptography was by julius caesar he made messages secret by shifting each letter three letters forward in the alphabet sending the last three letters of the alphabet to the first three for instance using this scheme the letter b is sent to e and the letter x is sent to a this is an example of encryption that is the process of making a message secret to express caesar encryption process mathematically first replace each letter by an ele ment of that is an integer from to equal to one less than its position in the alphabet for example replace a by k by and z by caesar encryption method can be represented by the function f that assigns to the nonnegative integer p p the integer f p in the set with f p p mod in the encrypted version of the message the letter represented by p is replaced with the letter represented by p mod example what is the secret message produced from the message meet you in the park using the caesar cipher solution first replace the letters in the message with numbers this produces now replace each of these numbers p by f p p mod this gives translating this back to letters produces the encrypted message phhw brx lq wkh sdun to recover the original message from a secret message encrypted by the caesar cipher the function f the inverse of f is used note that the function f sends an integer p from to f p p mod in other words to find the original message each letter is shifted back three letters in the alphabet with the first three letters sent to the last three letters of the alphabet the process of determining the original message from the encrypted message is called decryption there are various ways to generalize the caesar cipher for example instead of shifting the numerical equivalent of each letter by we can shift the numerical equivalent of each letter by k so that f p p k mod such a cipher is called a shift cipher note that decryption can be carried out using f p p k mod here the integer k is called a key we illustrate the use of a shift cipher in examples and example encrypt the plaintext message stop global warming using the shift cipher with shift k solution to encrypt the message stop global warming we first translate each letter to the corresponding element of this produces the string we now apply the shift f p p mod to each number in this string we obtain translating this last string back to letters we obtain the ciphertext deza rwzmlw hlcx tyr example decrypt the ciphertext message lewlyplujl pz h nylha alhjoly that was en crypted with the shift cipher with shift k solution to decrypt the ciphertext lewlyplujl pz h nylha alhjoly we first translate the letters back to elements of we obtain next we shift each of these numbers by k modulo to obtain finally we translate these numbers back to letters to obtain the plaintext we obtain experience is a great teacher we can generalize shift ciphers further to slightly enhance security by using a function of the form f p ap b mod where a and b are integers chosen so that f is a bijection the function f p ap b mod is a bijection if and only if gcd a such a mapping is called an affine transformation and the resulting cipher is called an affine cipher example what letter replaces the letter k when the function f p mod is used for en cryption solution first note that represents k then using the encryption function specified it follows that f mod because represents v k is replaced by v in the encrypted message mathematicians make the best code breakers their work in world war ii changed the course of the war we will now show how to decrypt messages encrypted using an affine cipher suppose that c ap b mod with gcd a to decrypt we need to show how to express p in terms of c to do this we apply the encrypting congruence c ap b mod and solve it for p to do this we first subtract b from both sides to obtain c b ap mod because gcd a we know that there is an inverse a of a modulo multiplying both sides of the last equation by a gives us a c b aap mod because aa mod this tells us that p a c b mod this determines p because p belongs to cryptanalysis the process of recovering plaintext from ciphertext without knowledge of both the encryption method and the key is known as crytanalysis or breaking codes in general cryptanalysis is a difficult process especially when the encryption method is unknown we will not discuss cryptanalysis in general but we will explain how to break messages that were encrypted using a shift cipher if we know that a ciphertext message was produced by enciphering a message using a shift cipher we can try to recover the message by shifting all characters of the ciphertext by each of the possible shifts including a shift of zero characters one of these is guaranteed to be the plaintext however we can use a more intelligent approach which we can build upon to cryptanalyze ciphertext resulting from other ciphers the main tool for cryptanalyzing ciphertext encrypted using a shift cipher is the count of the frequency of letters in the ciphertext the nine most common letters in english text and their approximate relative frequencies are e t a o i n s h and r to cryptanalyze ciphertext that we know was produced using a shift cipher we first find the relative frequencies of letters in the ciphertext we list the most common letters in the ciphertext in frequency order we hypothesize that the most common letter in the ciphertext is produced by encrypting e then we determine the value of the shift under this hypothesis say k if the message produced by shifting the ciphertext by k makes sense we presume that our hypothesis is correct and that we have the correct value of k if it does not make sense we next consider the hypothesis that the most common letter in the ciphertext is produced by encrypting t the second most common letter in english we find k under this hypothesis shift the letters of the message by k and see whether the resulting message makes sense if it does not we continue the process working our way through the letters from most common to least common example suppose that we intercepted the ciphertext message znk kgxre hoxj mkzy znk cuxs that we know was produced by a shift cipher what was the original plaintext message solution because we know that the intercepted ciphertext message was encrypted using a shift cipher we begin by calculating the frequency of letters in the ciphertext we find that the most common letter in the ciphertext is k so we hypothesize that the shift cipher sent the plaintext letter e to the ciphertext letter k if this hypothesis is correct we know that k mod so k next we shift the letters of the message by obtaining the early bird gets the worm because this message makes sense we assume that the hypothesis that k is correct block ciphers shift ciphers and affine ciphers proceed by replacing each letter of the alphabet by another letter in the alphabet because of this these ciphers are called character or monoalphabetic ciphers encryption methods of this kind are vulnerable to attacks based on the analysis of letter frequency in the ciphertext as we just illustrated we can make it harder to successfully attack ciphertext by replacing blocks of letters with other blocks of letters instead of replacing individual characters with individual characters such ciphers are called block ciphers we will now introduce a simple type of block cipher called the transposition cipher as a key we use a permutation σ of the set m for some positive integer m that is a one to one function from m to itself to encrypt a message we first split its letters into blocks of size m if the number of letters in the message is not divisible by m we add some random letters at the end to fill out the final block we encrypt the block pm as cm pσ pσ pσ m to decryt a ciphertext block cm we transpose its letters using the permutation σ the inverse of σ example illustrates encryption and decryption for a transposition cipher example using the transposition cipher based on the permutation σ of the set with σ o σ and σ a encrypt the plaintext message pirate attack b decrypt the ciphertext message swue trae oehs which was encrypted using this cipher solution a we first split the letters of the plaintext into blocks of four letters we obtain pira teat tack to encrypt each block we send the first letter to the third position the second letter to the first position the third letter to the fourth position and the fourth letter to the second position we obtain iapr etta aktc b we note that σ the inverse of σ sends to sends to sends to and sends to applying σ m to each block gives us the plaintext usew ater hose grouping together these letters to form common words we surmise that the plaintext is use water hose cryptosystems we have defined two families of ciphers shift ciphers and affine ciphers we now introduce the notion of a cryptosystem which provides a general structure for defining new families of ciphers definition we now illustrate the use of the definition of a cryptosystem example describe the family of shift ciphers as a cryptosytem solution to encrypt a string of english letters with a shift cipher we first translate each letter to an integer between and that is to an element of we then shift each of these integers by a fixed integer modulo and finally we translate the integers back to letters to apply the defintion of a cryptosystem to shift ciphers we assume that our messages are already integers that is elements of that is we assume that the translation between letters and integers is outside of the cryptosystem consequently both the set of plaintext strings p and the set of ciphertext strings c are the set of strings of elements of the set of keys k is the set of possible shifts so k the set e consists of functions of the form ek p p k mod and the set d of decryption functions is the same as the set of encrypting functions where dk p p k mod the concept of a cryptosystem is useful in the discussion of additional families of ciphers and is used extensively in cryptography public key cryptography all classical ciphers including shift ciphers and affine ciphers are examples of private key cryptosystems in a private key cryptosystem once you know an encryption key you can quickly find the decryption key so knowing how to encrypt messages using a particular key allows you to decrypt messages that were encrypted using this key for example when a shift cipher is used with encryption key k the plaintext integer p is sent to c p k mod decryption is carried out by shifting by k that is p c k mod so knowing how to encrypt with a shift cipher also tells you how to decrypt when a private key cryptosystem is used two parties who wish to communicate in secret must share a secret key because anyone who knows this key can both encrypt and decrypt messages two people who want to communicate securely need to securely exchange this key we will introduce a method for doing this later in this section the shift cipher and affine cipher cryptosystems are private key cryptosystems they are quite simple and are extremely vulnerable to cryptanalysis however the same is not true of many modern private key cryptosystems in particular the current us government standard for private key cryptography the advanced encryption standard aes is extremely complex and is considered to be highly resistant to cryptanalysis see for details on aes and other modern private key cryptosystems aes is widely used in government and commercial communications however it still shares the property that for secure communications keys be shared furthermore for extra security a new key is used for each communication session between two parties which requires a method for generating keys and securely sharing them to avoid the need for keys to be shared by every pair of parties that wish to communicate securely in the cryptologists introduced the concept of public key cryptosystems when such cryptosystems are used knowing how to send an encrypted message does not help decrypt messages in such a system everyone can have a publicly known encryption key only the decryption keys are kept secret and only the intended recipient of a message can decrypt it because as far as it is currently known knowledge of the encryption key does not let someone recover the plaintext message without an extraordinary amount of work such as billions of years of computer time m i t is also known as the rsa cryptosystem in three researchers at the massachusetts institute of technology ronald rivest adi shamir and leonard adleman introduced to the world a public key cryptosystem known as the the tute rsa system from the initials of its inventors as often happens with cryptographic discoveries the rsa system had been discovered several years earlier in secret government research in the united kingdom clifford cocks working in secrecy at the united kingdom government communications headquarters gchq had discovered this cryptosystem in however his invention was unknown to the outside world until the late when he was allowed to share classified gchq documents from the early an excellent account of this earlier discovery as well as the work of rivest shamir and adleman can be found in unfortunately no one calls this the cocks cryptosystem in the rsa cryptosystem each individual has an encryption key n e where n pq the modulus is the product of two large primes p and q say with digits each and an exponent e that is relatively prime to p q to produce a usable key two large primes must be found this can be done quickly on a computer using probabilistic primality tests referred to earlier in this section however the product of these primes n pq with approximately digits cannot as far as is currently known be factored in a reasonable length of time as we will see this is an important reason why decryption cannot as far as is currently known be done quickly without a separate decryption key rsa encryption to encrypt messages using a particular key n e we first translate a plaintext message m into sequences of integers to do this we first translate each plaintext letter into a two digit number using the same translation we employed for shift ciphers with one key difference that is we include an initial zero for the letters a through j so that a is translated into b into and j into then we concatenate these two digit numbers into strings of digits next we divide this string into equally sized blocks of digits where is the largest even number such that the number with digits does not exceed n when necessary we pad the plaintext message with dummy xs to make the last block the same size as all other blocks after these steps we have translated the plaintext message m into a sequence of integers mk for some integer k encryption proceeds by transforming each block mi to a ciphertext block ci this is done using the function c me mod n to perform the encryption we use an algorithm for fast modular exponentiation such as algorithm in section we leave the encrypted message as blocks of numbers and send these to the intended recipient because the rsa cryptosystem encrypts blocks of characters into blocks of characters it is a block cipher clifford cocks born clifford cocks born in cheshire england was a talented mathematics student in he won a silver medal at the international mathematical olympiad cocks attended king college cambridge studying mathematics he also spent a short time at oxford university working in number theory in he decided not to complete his graduate work instead taking a mathematical job at the govern ment communications headquarters gchq of british intelligence two months after joining gchq cocks learned about public key cryptography from an internal gchq report written by james ellis cocks used his number theory knowledge to invent what is now called the rsa cryptosystem he quickly realized that a public key cryptosystem could be based on the difficulty of reversing the process of multiplying two large primes in he was allowed to reveal declassified gchq internal documents describing his discovery cocks is also known for his invention of a secure identity based encryption scheme which uses information about a user identity as a public key in cocks became the chief mathematician at gchq he has also set up the heilbronn institute for mathematical research a partnership between gchq and the university of bristol example illustrates how rsa encryption is performed for practical reasons we use small primes p and q in this example rather than primes with or more digits although the cipher described in this example is not secure it does illustrate the techniques used in the rsa cipher example encrypt the message stop using the rsa cryptosystem with key note that p and q are primes and gcd e p q gcd solution to encrypt we first translate the letters in stop into their numerical equivalents we then group these numbers into blocks of four digits because to obtain we encrypt each block using the mapping c mod computations using fast modular multiplication show that mod and mod the encrypted message is 2182 rsa decryption the plaintext message can be quickly recovered from a ciphertext message when the decryp tion key d an inverse of e modulo p q is known such an inverse exists because gcd e p q to see this note that if de mod p q there is an integer k such that de k p q it follows that cd me d mde k p q mod n ronald rivest born ronald rivest received a b a from yale in and his ph d in computer science from stanford in rivest is a computer science professor at m i t and was a cofounder of rsa data security which held the patent on the rsa cryptosystem that he invented together with adi shamir and leonard adleman areas that rivest has worked in besides cryptography include machine learning vlsi design and computer algorithms he is a coauthor of a popular text on algorithms adi shamir born adi shamir was born in tel aviv israel his undergraduate degree is from tel aviv university and his ph d is from the weizmann institute of science shamir was a research assistant at the university of warwick and an assistant professor at m i t he is currently a profes sor in the applied mathematics department at the weizmann institute and leads a group studying computer security shamir contributions to cryptography besides the rsa cryptosystem include cracking knapsack cryptosystems cryptanalysis of the data encryption standard des and the design of many cryptographic protocols leonard adleman born leonard adleman was born in san francisco california he received a b s in mathematics and his ph d in computer science from the university of california berkeley adleman was a member of the mathematics faculty at m i t from until where he was a coinventor of the rsa cryptosystem and in he took a position in the computer science department at the university of southern california usc he was appointed to a chaired position at usc in adleman has worked on computer security computational complexity immunology and molecular biology he invented the term computer virus adleman recent work on dna computing has sparked great interest he was a technical adviser for the movie sneakers in which computer security played an important role by fermat little theorem assuming that gcd m p gcd m q which holds except in rare cases which we cover in exercise it follows that mp mod p and mq mod q consequently cd m mp k q m m mod p and cd m mq k p m m mod q because gcd p q it follows by the chinese remainder theorem that cd m mod pq example illustrates how to decrypt messages sent using the rsa cryptosystem example we receive the encrypted message what is the decrypted message if it was encrypted using the rsa cipher from example solution the message was encrypted using the rsa cryptosystem with n and expo nent as exercise in section shows d is an inverse of modulo we use as our decryption exponent consequently to decrypt a block c we compute m mod to decrypt the message we use the fast modular exponentiation algorithm to compute mod and mod consequently the numerical version of the original message is 1115 translating this back to english letters we see that the message is help rsa as a public key system why is the rsa cryptosystem suitable for public key cryptography first it is possible to rapidly construct a public key by finding two large primes p and q each with more than digits and to find an integer e relatively prime to p q when we know the factorization of the modulus n that is when we know p and q we can quickly find an inverse d of e modulo p q this is done by using the euclidean algorithm to find bézout coefficients and t for d and p q which shows that the inverse of d modulo p q is mod p q knowing d lets us decrypt messages sent using our key however no method is known to decrypt messages that is not based on finding a factorization of n or that does not also lead to the factorization of n factorization is believed to be a difficult problem as opposed to finding large primes p and q which can be done quickly the most efficient factorization methods known as of require billions of years to factor digit integers consequently when p and q are digit primes it is believed that messages encrypted using n pq as the modulus cannot be found in a reasonable time unless the primes p and q are known although no polynomial time algorithm is known for factoring large integers active re search is under way to find new ways to efficiently factor integers integers that were thought as recently as several years ago to be far too large to be factored in a reasonable amount of time can now be factored routinely integers with more than digits as well as some with more than digits have been factored using team efforts when new factorization techniques are found it will be necessary to use larger primes to ensure secrecy of messages unfortunately messages that were considered secure earlier can be saved and subsequently decrypted by unintended recipients when it becomes feasible to factor the n pq in the key used for rsa encryption the rsa method is now widely used however the most commonly used cryptosystems are private key cryptosystems the use of public key cryptography via the rsa system is growing nevertheless there are applications that use both private key and public key systems for example a public key cryptosystem such as rsa can be used to distribute private keys to pairs of individuals when they wish to communicate these people then use a private key system for encryption and decryption of messages cryptographic protocols so far we have shown how cryptography can be used to make messages secure however there are many other important applications of cryptography among these applications are cryptographic protocols which are exchanges of messages carried out by two or more parties to achieve a particular security goal in particular we will show how cryptography can be used to allow two people to exchange a secret key over an insecure communication channel we will also show how cryptography can be used to send signed secret messages so that the recipient can be sure that the message came from the purported sender we refer the reader to for thorough discussions of a variety of cryptographic protocols key exchange we now discuss a protocol that two parties can use to exchange a secret key over an insecure communications channel without having shared any information in the past generating a key that two parties can share is important for many applications of cryp tography for example for two people to send secure messages to each other using a private key cryptosystem they need to share a common key the protocol we will describe is known as the diffie hellman key agreement protocol after whitfield diffie and martin hellman who described it in however this protocol was invented in by malcolm williamson in secret work at the british gchq it was not until that his discovery was made public suppose that alice and bob want to share a common key the protocol follows these steps where the computations are done in zp alice and bob agree to use a prime p and a primitive root a of p alice chooses a secret integer and sends mod p to bob bob chooses a secret integer and sends mod p to alice alice computes mod p bob computes mod p at the end of this protocol alice and bob have computed their shared key namely mod p mod p to analyze the security of this protocol note that the messages sent in steps and are not assumed to be sent securely we can even assume that these communications were in the clear and that their contents are public information so p a mod p and mod p are assumed to be public information the protocol ensures that and the common key mod p mod p are kept secret to find the secret information from this pub lic information requires that an adversary solves instances of the discrete logarithm problem because the adversary would need to find and from mod p and mod p respec tively furthermore no other method is known for finding the shared key using just the public information we have remarked that this is thought to be computationally infeasible when p and a are sufficiently large with the computing power available now this system is considered unbreakable when p has more than decimal digits and and have more than decimal digits each digital signatures not only can cryptography be used to secure the confidentiality of a message but it also can be used so that the recipient of the message knows that it came from the person they think it came from we first show how a message can be sent so that a recipient of the message will be sure that the message came from the purported sender of the message in particular we can show how this can be accomplished using the rsa cryptosystem to apply a digital signature to a message suppose that alice rsa public key is n e and her private key is d alice encrypts a plain text message x using the encryption function e n e x xe mod n she decrypts a ciphertext message y using the decryption function d n e xd mod n alice wants to send the message m so that everyone who receives the message knows that it came from her just as in rsa en cryption she translates the letters into their numerical equivalents and splits the resulting string into blocks mk such that each block is the same size which is as large as possible so that mi n for i k she then applies her decryption function d n e to each block obtaining dn e mi i k she sends the result to all intended recipients of the message when a recipient receives her message they apply alice encryption function e n e to each block which everyone has available because alice key n e is public information the result is the original plaintext block because e n e d n e x x so alice can send her message to as many people as she wants and by signing it in this way every recipient can be sure it came from alice example illustrates this protocol example suppose alice public rsa cryptosystem key is the same as in example that is n and e her decryption key is d as described in example she wants to send the message meet at noon to her friends so that they are sure it came from her what should she send solution alice first translates the message into blocks of digits obtaining 1314 as the reader should verify she then applies her decryption transformation d x mod to each block using fast modular exponentiation with the help of a computational aid she finds that mod mod mod mod and mod so the message she sends split into blocks is 1310 1026 when one of her friends gets this message they apply her encryption transformation e to each block when they do this they obtain the blocks of digits of the original message which they translate back to english letters we have shown that signed messages can be sent using the rsa cryptosystem we can extend this by sending signed secret messages to do this the sender applies rsa encryption using the publicly known encryption key of an intended recipient to each block that was encrypted using sender decryption transformation the recipient then first applies his private decryption transformation and then the sender public encryption transformation exercise asks for this protocol to be carried out exercises encrypt the message do not pass go by translating the letters into numbers applying the given encryption function and then translating the numbers back into let ters a f p p mod the caesar cipher b f p p mod c f p mod encrypt the message stop pollution by translating the letters into numbers applying the given encryption function and then translating the numbers back into let ters a f p p mod b f p p mod c f p mod encrypt the message watch your step by translat ing the letters into numbers applying the given encryp tion function and then translating the numbers back into letters a f p p mod b f p mod c f p mod decrypt these messages that were encrypted using the caesar cipher a eoxh mhdqv b whvw wrgdb c hdw glp vxp decrypt these messages encrypted using the shift cipher f p p mod a cebboxnob xyg b lo wi pbsoxn c dswo pyb pex suppose that when a long string of text is encrypted using a shift cipher f p p k mod the most common letter in the ciphertext is x what is the most likely value for k assuming that the distribution of letters in the text is typical of english text suppose that when a string of english text is encrypted us ing a shift cipher f p p k mod the resulting ciphertext is dy cvooz zobmrkxmo dy nbokw what was the original plaintext string suppose that the ciphertext dve cfmv kf nfeuvi reu kyrk zj kyv jvvu fw jtzvetv was pro duced by encrypting a plaintext message using a shift cipher what is the original plaintext suppose that the ciphertext erc wyjjmgmirxpc ehzergih xiglrspskc mw mrhmwxm rkymwlefpi jvsq qekmg was produced by en crypting a plaintext message using a shift cipher what is the original plaintext determine whether there is a key for which the encipher ing function for the shift cipher is the same as the deci phering function what is the decryption function for an affine cipher if the encryption function is c mod find all pairs of integers keys a b for affine ciphers for which the encryption function c ap b mod is the same as the corresponding decryption function suppose that the most common letter and the second most common letter in a long ciphertext produced by encrypting a plaintext using an affine cipher f p ap b mod are z and j respectively what are the most likely values of a and b encrypt the message grizzly bears using blocks of five letters and the transposition cipher based on the permutation of with σ σ σ σ and σ for this exercise use the letter x as many times as necessary to fill out the final block of fewer then five letters decrypt the message eabw efro atmr asin which is the ciphertext produced by encrypting a plaintext mes sage using the transposition cipher with blocks of four letters and the permutation σ of defined by o σ σ and σ suppose that you know that a ciphertext was produced by encrypting a plaintext message with a transposition cipher how might you go about breaking it suppose you have intercepted a ciphertext message and when you determine the frequencies of letters in this mes sage you find the frequencies are similar to the frequency of letters in english text which type of cipher do you sus pect was used the vigenère cipher is a block cipher with a key that is a string of letters with numerical equivalents km where ki for i m suppose that the numerical equivalents of the letters of a plaintext block are pm the corresponding numerical ciphertext block is mod mod pm km mod finally we translate back to letters for example suppose that the key string is red with numerical equivalents then the plaintext orange with numerical equivalents is encrypted by first splitting it into two blocks and then in each block we shift the first letter by the second by and the third by we obtain and the cipherext is fvdekh use the vigenère cipher with key blue to encrypt the message snowfall the ciphertext oikywvhbx was produced by encrypt ing a plaintext message using the vigenère cipher with key hot what is the plaintext message express the vigenère cipher as a cryptosystem to break a vigenère cipher by recovering a plaintext message from the ciphertext message without having the key the first step is to figure out the length of the key string the second step is to figure out each character of the key string by determining the correspond ing shift exercises and deal with these two aspects suppose that when a long string of text is encrypted using a vigenère cipher the same string is found in the ciphertext starting at several different positions explain how this infor mation can be used to help determine the length of the key once the length of the key string of a vigènere cipher is known explain how to determine each of its characters assume that the plaintext is long enough so that the frequency of its let ters is reasonably close to the frequency of letters in typical english text show that we can easily factor n when we know that n is the product of two primes p and q and we know the value of p q in exercises first express your answers without computing modular exponentiations then use a computational aid to com plete these computations encrypt the message attack using the rsa system with n and e translating each letter into integers and grouping together pairs of integers as done in example encrypt the message upload using the rsa system with n and e translating each letter into integers and grouping together pairs of integers as done in example what is the original message encrypted using the rsa sys tem with n and e if the encrypted message is to decrypt first find the decryption exponent d which is the inverse of e modulo what is the original message encrypted using the rsa system with n and e if the encrypted message is to decrypt first find the decryption exponent d which is the inverse of e modulo suppose that n e is an rsa encryption key with n pq where p and q are large primes and gcd e p q furthermore suppose that d describe the steps that alice and bob follow when they use the diffie hellman key exchange protocol to generate a shared key assume that they use the prime p and take a which is a primitive root of and that alice selects and bob selects you may want to use some compu tational aid describe the steps that alice and bob follow when they use the diffie hellman key exchange protocol to generate a shared key assume that they use the prime p and take a which is a primitive root of and that alice selects and bob selects you may want to use some compu tational aid in exercises suppose that alice and bob have these public keys and corresponding private keys nalice ealice dalice and nbob ebob dbob first express your an swers without carrying out the calculations then using a com putational aid if available perform the calculation to get the numerical answers alice wants to send to all her friends including bob the mes sage sell everything so that he knows that she sent it what should she send to her friends assuming she signs the message using the rsa cryptosystem alice wants to send to bob the message buy now so that he knows that she sent it and so that only bob can read it what should she send to bob assuming she signs the message and then encrypts it using bob public key we describe a basic key exchange protocol using private key cryptography upon which more sophisticated protocols for key exchange are based encryption within the protocol is done using a private key cryptosystem such as aes that is considered secure the protocol involves three parties alice and bob who wish to exchange a key and a trusted third party cathy assume that alice has a secret key kalice that only she and cathy know and bob has a secret key kbob which only he and cathy know the protocol has three steps i alice sends the trusted third party cathy the message re quest a shared key with bob encrypted using alice key kalice ii cathy sends back to alice a key kalice bob which she gen erates encrypted using the key is an inverse of e modulo p q suppose that c me mod pq in the text we showed that rsa de cryption that is the congruence cd m mod pq holds when gcd m pq show that this decryption congruence also holds when gcd m pq hint use congruences modulo p and modulo q and apply the chinese remainder theorem kalice followed by this same key kalice bob encrypted using bob key kbob iii alice sends to bob the key kalice bob encrypted using kbob known only to bob and to cathy explain why this protocol allows alice and bob to share the secret key kalice bob known only to them and to cathy key terms and results terms a b a divides b there is an integer c such that b ac a and b are congruent modulo m m divides a b modular arithmetic arithmetic done modulo an integer m prime an integer greater than with exactly two positive integer divisors composite an integer greater than that is not prime mersenne prime a prime of the form where p is prime gcd a b greatest common divisor of a and b the largest integer that divides both a and b relatively prime integers integers a and b such that gcd a b pairwise relatively prime integers a set of integers with the property that every pair of these integers is relatively prime lcm a b least common multiple of a and b the smallest positive integer that is divisible by both a and b a mod b the remainder when the integer a is divided by the positive integer b a b mod m a is congruent to b modulo m a b is divisible by m n akak b the base b representation of n binary representation the base representation of an integer octal representation the base representation of an integer hexadecimal representation the base representation of an integer linear combination of a and b with integer coefficients an expression of the form sa tb where and t are integers bézout coefficients of a and b integers and t such that the bézout identity sa tb gcd a b holds inverse of a modulo m an integer a such that aa mod m linear congruence a congruence of the form ax b mod m where x is an integer variable pseudoprime to the base b a composite integer n such that bn mod n carmichael number a composite integer n such that n is a pseudoprime to the base b for all positive integers b with gcd b n primitive root of a prime p an integer r in zp such that every integer not divisible by p is congruent modulo p to a power of r discrete logarithm of a to the base r modulo p the integer e with e p such that re a mod p encryption the process of making a message secret decryption the process of returning a secret message to its original form encryption key a value that determines which of a family of encryption functions is to be used shift cipher a cipher that encrypts the plaintext letter p as p k mod m for an integer k affine cipher a cipher that encrypts the plaintext letter p as ap b mod m for integers a and b with gcd a character cipher a cipher that encrypts characters one by one block cipher a cipher that encrypts blocks of characters of a fixed size crytanalysis the process of recovering the plaintext from ci phertext without knowledge of the encryption method or with knowledge of the encryption method but not the key cryptosystem a five tuple where is the set of plaintext messages is the set of ciphertext messages is the set of keys is the set of encryption functions and is the set of decryption functions private key encryption encryption where both encryption keys and decryption keys must be kept secret public key encryption encryption where encryption keys are public knowledge but decryption keys are kept secret rsa cryptosystem the cryptosystem where and are both is the set of pairs k n e where n pq where p and q are large primes and e is a positive integer ek p pe mod n and dk c cd mod n where d is the inverse of e modulo p q key exchange protocol a protocol used for two parties to generate a shared key digital signature a method that a recipient can use to deter mine that the purported sender of a message actually sent the message results division algorithm let a and d be integers with d positive then there are unique integers q and r with r d such that a dq r let b be an integer greater than then if n is a pos itive integer it can be expressed uniquely in the form n akbk ak the algorithm for finding the base b expansion of an integer see algorithm in section the conventional algorithms for addition and multiplication of integers given in section the modular exponentiation algorithm see algorithm in section euclidean algorithm for finding greatest common divisors by successively using the division algorithm see algorithm in section bézout theorem if a and b are positive integers then gcd a b is a linear combination of a and b sieve of eratosthenes a procedure for finding all primes not exceeding a specified number n described in section fundamental theorem of arithmetic every positive integer can be written uniquely as the product of primes where the prime factors are written in order of increasing size if a and b are positive integers then ab gcd a b lcm a b if m is a positive integer and gcd a m then a has a unique inverse modulo m chinese remainder theorem a system of linear congruences modulo pairwise relatively prime integers has a unique so lution modulo the product of these moduli fermat little theorem if p is prime and p a then ap mod p review questions find div and mod a define what it means for a and b to be congruent mod ulo b which pairs of the integers and are congruent modulo c show that if a and b are congruent modulo then and are also congruent modulo show that if a b mod m and c d mod m then a c b d mod m describe a procedure for converting decimal base expansions of integers into hexadecimal expansions convert 1101 1001 0101 1011 to octal and hexadeci mal representations convert and to a binary representation state the fundamental theorem of arithmetic a describe a procedure for finding the prime factoriza tion of an integer b use this procedure to find the prime factorization of 707 a define the greatest common divisor of two integers b describe at least three different ways to find the great est common divisor of two integers when does each method work best c find the greatest common divisor of and supplementary exercises b express gcd as a linear combination of and a what does it mean for a to be an inverse of a modulo m b how can you find an inverse of a modulo m when m is a positive integer and gcd a m c find an inverse of modulo a how can an inverse of a modulo m be used to solve the congruence ax b mod m when gcd a m b solve the linear congruence mod a state the chinese remainder theorem b find the solutions to the system x mod x mod and x mod suppose that mod n is n necessarily prime use fermat little theorem to evaluate mod explain how the check digit is found for a digit isbn encrypt the message apples and oranges using a shift cipher with key k a what is the difference between a public key and a pri vate key cryptosystem b explain why using shift ciphers is a private key sys tem c explain why the rsa cryptosystem is a public key 654 system d find the greatest common divisor of and a how can you find a linear combination with integer coefficients of two integers that equals their greatest common divisor supplementary exercises the odometer on a car goes to up miles the present owner of a car bought it when the odometer read miles he now wants to sell it when you examine the car for possible purchase you notice that the odome ter reads 697 miles what can you conclude about how many miles he drove the car assuming that the odometer always worked correctly a explain why n div equals the number of complete weeks in n days b explain why n div equals the number of complete days in n hours find four numbers congruent to modulo show that if a and d are positive integers then there are integers q and r such that a dq r where d r d show that if ac bc mod m where a b c and m are integers with m and d gcd m c then a b mod m d show that the sum of the squares of two odd integers cannot be the square of an integer explain how encryption and decryption are done in the rsa cryptosystem describe how two parties can share a secret key using the diffie hellman key exchange protocol show that if is a perfect square where n is an integer then n is even prove that there are no solutions in integers x and y to the equation 5y2 hint consider this equation modulo develop a test for divisibility of a positive integer n by based on the binary expansion of n develop a test for divisibility of a positive integer n by based on the binary expansion of n devise an algorithm for guessing a number between and by successively guessing each bit in its binary expansion determine the complexity in terms of the number of guesses needed to determine a number between and by successively guessing the bits in its binary ex pansion show that an integer is divisible by if and only if the sum of its decimal digits is divisible by show that if a and b are positive irrational numbers such that a b then every positive integer can be uniquely expressed as either ka or kb for some posi tive integer k prove there are infinitely many primes by showing that qn n must have a prime factor greater than n whenever n is a positive integer find a positive integer n for which qn n is not prime use dirichlet theorem which states there are infinitely many primes in every arithmetic progression ak b where gcd a b to show that there are infinitely many primes that have a decimal expansion ending with a prove that if n is a positive integer such that the sum of the divisors of n is n then n is prime show that every integer greater than is the sum of two composite integers find the five smallest consecutive composite integers show that goldbach conjecture which states that ev ery even integer greater than is the sum of two primes is equivalent to the statement that every integer greater than is the sum of three primes find an arithmetic progression of length six beginning with that contains only primes prove that if f x is a nonconstant polynomial with inte ger coefficients then there is an integer y such that f y is composite hint assume that f p is prime show that p divides f kp for all integers k obtain a con tradiction of the fact that a polynomial of degree n where n takes on each value at most n times how many zeros are at the end of the binary expansion of use the euclidean algorithm to find the greatest common divisor of and how many divisions are required to find gcd 233 using the euclidean algorithm find gcd where n is a positive integer hint use the euclidean algorithm a show that if a and b are positive inte gers with a b then gcd a b a if a b gcd a b gcd a b if a and b are even gcd a b gcd a b if a is even and b is odd and gcd a b gcd a b b if both a and b are odd b explain how to use a to construct an algorithm for computing the greatest common divisor of two posi tive integers that uses only comparisons subtractions and shifts of binary expansions without using any divisions c find gcd using this algorithm adapt the proof that there are infinitely many primes the orem in section to show that are infinitely many primes in the arithmetic progression k explain why you cannot directly adapt the proof that there are infinitely many primes theorem in section to show that there are infinitely many primes in the arith metic progression k explain why you cannot directly adapt the proof that there are infinitely many primes theorem in section to show that are infinitely many primes in the arithmetic progression k show that if the smallest prime factor p of the positive integer n is larger than n then n p is prime or equal to a set of integers is called mutually relatively prime if the greatest common divisor of these integers is determine whether the integers in each of these sets are mutually relatively prime a b c d find a set of four mutually relatively prime integers such that no two of them are relatively prime for which positive integers n is prime show that the system of congruences x mod and x mod has no solutions find all solutions of the system of congruences x mod and x mod a show that the system of congruences x mod and x mod m2 where and m2 are integers with and m2 has a solution if and only if gcd m2 b show that if the system in part a has a solution then it is unique modulo lcm m1 m2 prove that divides n for every nonnegative integer n prove that is divisible by for every integer n for which gcd n show that if p and q are distinct prime numbers then pq qp mod pq the check digit for an isbn with initial digits is determined by the congruence a12 mod determine whether each of these digit numbers is a valid isbn a b c 148410 d show that the check digit of an isbn can always detect a single error show that there are transpositions of two digits that are not detected by an isbn a routing transit number rtn is a bank code used in the united states which appears on the bottom of checks the most common form of an rtn has nine digits where the last digit is a check digit if is a valid rtn computer projects the congruence mod must hold show that if is a valid rtn then d9 d4 d2 d8 d6 mod fur thermore use this formula to find the check digit that follows the eight digits in a valid rtn show that the check digit of an rtn can detect all single errors and determine which transposition errors an rtn check digit can catch and which ones it cannot catch the encrypted version of a message is ljmkg mg mxf qexmw if it was encrypted using the affine ci pher f p mod what was the original message autokey ciphers are ciphers where the nth letter of the plain text is shifted by the numerical equivalent of the nth letter of a keystream the keystream begins with a seed letter its sub sequent letters are constructed using either the plaintext or the ciphertext when the plaintext is used each character of the keystream after the first is the previous letter of the plaintext when the ciphertext is used each subsequent character of the keystream after the first is the previous letter of the ciphertext computed so far in both cases plaintext letters are encrypted by shifting each character by the numerical equivalent of the corresponding keystream letter use the autokey cipher to encrypt the message now is the time to decide ignoring spaces using a the keystream with seed x followed by letters of the plaintext b the keystream with seed x followed by letters of the ciphertext use the autokey cipher to encrypt the message the dream of reason ignoring spaces using a the keystream with seed x followed by letters of the plaintext b the keystream with seed x followed by letters of the ciphertext computer projects write programs with these inputs and outputs given integers n and b each greater than find the base b expansion of this integer given the positive integers a b and m with m find ab mod m given a positive integer find the cantor expansion of this integer see the preamble to exercise of section given a positive integer determine whether it is prime using trial division given a positive integer find the prime factorization of this integer given two positive integers find their greatest common divisor using the euclidean algorithm given two positive integers find their least common mul tiple given positive integers a and b find bézout coefficients and t of a and b given relatively prime positive integers a and b find an inverse of a modulo b given n linear congruences modulo pairwise relatively prime moduli find the simultaneous solution of these con gruences modulo the product of these moduli given a positive integer n a modulus m a multiplier a an increment c and a seed where a m c m and m generate the sequence of n pseudo random numbers using the linear congruential generator xn axn c mod m given a set of identification numbers use a hash func tion to assign them to memory locations where there are k memory locations compute the check digit when given the first nine digits of an isbn given a message and a positive integer k less than encrypt this message using the shift cipher with key k and given a message encrypted using a shift cipher with key k decrypt this message given a message and positive integers a and b less than with gcd a encrypt this message using an affine cipher with key a b and given a message encrypted us ing the affine cipher with key a b decrypt this message by first finding the decryption key and then applying the appropriate decryption transformation find the original plaintext message from the ciphertext message produced by encrypting the plaintext message using a shift cipher do this using a frequency count of letters in the ciphertext construct a valid rsa encryption key by finding two primes p and q with digits each and an integer e relatively prime to p q given a message and an integer n pq where p and q are odd primes and an integer e relatively prime to p q encrypt the message using the rsa cryptosystem with key n e given a valid rsa key n e and the primes p and q with n pq find the associated decryption key d given a message encrypted using the rsa cryptosystem with key n e and the associated decryption key d de crypt this message generate a shared key using the diffie hellman key ex change protocol given the rsa public and private keys of two parties send a signed secret message from one of the parties to the other computations and explorations use a computational program or programs you have written to do these exercises determine whether is prime for each of the primes not exceeding test a range of large mersenne numbers to de termine whether they are prime you may want to use software from the gimps project determine whether qn pn is prime where pn are the n smallest primes for as many positive integer n as possible look for polynomials in one variables whose values at long runs of consecutive integers are all primes find as many primes of the form where n is a pos itive integer as you can it is not known whether there are infinitely many such primes find different primes each with digits how many primes are there less than less than and less than can you propose an estimate for the number of primes less than x where x is a positive integer find a prime factor of each of different digit odd integers selected at random keep track of how long it takes to find a factor of each of these integers do the same thing for different digit odd integers dif ferent digit odd integers and so on continuing as long as possible find all pseudoprimes to the base that do not exceed writing projects respond to these with essays using outside sources describe the lucas lehmer test for determining whether a mersenne number is prime discuss the progress of the gimps project in finding mersenne primes using this test explain how probabilistic primality tests are used in prac tice to produce extremely large numbers that are almost certainly prime do such tests have any potential draw backs the question of whether there are infinitely many carmichael numbers was solved recently after being open for more than years describe the ingredients that went into the proof that there are infinitely many such numbers summarize the current status of factoring algorithms in terms of their complexity and the size of numbers that can currently be factored when do you think that it will be feasible to factor digit numbers describe the algorithms that are actually used by modern computers to add subtract multiply and divide positive integers describe the history of the chinese remainder theorem describe some of the relevant problems posed in chi nese and hindu writings and how the chinese remainder theorem applies to them when are the numbers of a sequence truly random num bers and not pseudorandom what shortcomings have been observed in simulations and experiments in which pseudorandom numbers have been used what are the properties that pseudorandom numbers can have that ran dom numbers should not have explain how a check digit is found for an international bank account number iban and discuss the types of errors that can be found using this check digit describe the luhn algorithm for finding the check digit of a credit card number and discuss the types of errors that can found using this check digit show how a congruence can be used to tell the day of the week for any given date describe how public key cryptography is being applied are the ways it is applied secure given the status of fac toring algorithms will information kept secure using public key cryptography become insecure in the future describe how public key cryptography can be used to produce signed secret messages so that the recipient is relatively sure the message was sent by the person ex pected to have sent it describe the rabin public key cryptosystem explaining how to encrypt and how to decrypt messages and why it is suitable for use as a public key cryptosystem explain why it would not be suitable to use p where p is a large prime as the modulus for encryption in the rsa cryptosystem that is explain how someone could without excessive computation find a private key from the corresponding public key if the modulus were a large prime rather than the product of two large primes explain what is meant by a cryptographic hash function what are the important properties such a function must have mathematical induction strong induction and well ordering recursive definitions and structural induction recursive algorithms program correctness any mathematical statements assert that a property is true for all positive integers examples of such statements are that for every positive integer n n nn n is divisible by a set with n elements has subsets and the sum of the first n positive integers is n n a major goal of this chapter and the book is to give the student a thorough understanding of mathematical induction which is used to prove results of this kind proofs using mathematical induction have two parts first they show that the statement holds for the positive integer second they show that if the statement holds for a positive integer then it must also hold for the next larger integer mathematical induction is based on the rule of inference that tells us that if p and k p k p k are true for the domain of positive integers then np n is true mathematical induction can be used to prove a tremendous variety of results understanding how to read and construct proofs by mathematical induction is a key goal of learning discrete mathematics in chapter we explicitly defined sets and functions that is we described sets by listing their elements or by giving some property that characterizes these elements we gave formulae for the values of functions there is another important way to define such objects based on mathematical induction to define functions some initial terms are specified and a rule is given for finding subsequent values from values already known we briefly touched on this sort of definition in chapter when we showed how sequences can be defined using recurrence relations sets can be defined by listing some of their elements and giving rules for constructing elements from those already known to be in the set such definitions called recursive definitions are used throughout discrete mathematics and computer science once we have defined a set recursively we can use a proof method called structural induction to prove results about this set when a procedure is specified for solving a problem this procedure must always solve the problem correctly just testing to see that the correct result is obtained for a set of input values does not show that the procedure always works correctly the correctness of a procedure can be guaranteed only by proving that it always yields the correct result the final section of this chapter contains an introduction to the techniques of program verification this is a formal technique to verify that procedures are correct program verification serves as the basis for attempts under way to prove in a mechanical fashion that programs are correct mathematical induction introduction suppose that we have an infinite ladder as shown in figure and we want to know whether we can reach every step on this ladder we know two things we can reach the first rung of the ladder if we can reach a particular rung of the ladder then we can reach the next rung can we conclude that we can reach every rung by we know that we can reach the first rung of the ladder moreover because we can reach the first rung by we can also reach the second rung it is the next rung after the first rung applying again because we can reach the second rung we can also reach the third rung continuing in this way we can show that we we can reach step k if we can reach step k step k step k step we can reach step step step step figure climbing an infinite ladder can reach the fourth rung the fifth rung and so on for example after uses of we know that we can reach the rung but can we conclude that we are able to reach every rung of this infinite ladder the answer is yes something we can verify using an important proof technique called mathematical induction that is we can show that p n is true for every positive integer n where p n is the statement that we can reach the nth rung of the ladder mathematical induction is an extremely important proof technique that can be used to prove assertions of this type as we will see in this section and in subsequent sections of this chapter and later chapters mathematical induction is used extensively to prove results about a large variety of discrete objects for example it is used to prove results about the complexity of algorithms the correctness of certain types of computer programs theorems about graphs and trees as well as a wide range of identities and inequalities in this section we will describe how mathematical induction can be used and why it is a valid proof technique it is extremely important to note that mathematical induction can be used only to prove results obtained in some other way it is not a tool for discovering formulae or theorems mathematical induction in general mathematical induction can be used to prove statements that assert that p n is true for all positive integers n where p n is a propositional function a proof by mathematical unfortunately using the terminology mathematical induction clashes with the terminology used to describe different types of reasoning in logic deductive reasoning uses rules of inference to draw conclusions from premises whereas inductive reasoning makes conclusions only supported but not ensured by evidence mathematical proofs including arguments that use mathematical induction are deductive not inductive induction has two parts a basis step where we show that p is true and an inductive step where we show that for all positive integers k if p k is true then p k is true to complete the inductive step of a proof using the principle of mathematical induction we assume that p k is true for an arbitrary positive integer k and show that under this assumption p k must also be true the assumption that p k is true is called the inductive hypothesis once we complete both steps in a proof by mathematical induction we have shown that p n is true for all positive integers that is we have shown that np n is true where the quantification is over the set of positive integers in the inductive step we show that k p k p k is true where again the domain is the set of positive integers expressed as a rule of inference this proof technique can be stated as p k p k p k np n when the domain is the set of positive integers because mathematical induction is such an important technique it is worthwhile to explain in detail the steps of a proof using this technique the first thing we do to prove that p n is true for all positive integers n is to show that p is true this amounts to showing that the particular statement obtained when n is replaced by in p n is true then we must show that p k p k is true for every positive integer k to prove that this conditional statement is true for every positive integer k we need to show that p k cannot be false when p k is true this can be accomplished by assuming that p k is true and showing that under this hypothesis p k must also be true remark in a proof by mathematical induction it is not assumed that p k is true for all positive integers it is only shown that if it is assumed that p k is true then p k is also true thus a proof by mathematical induction is not a case of begging the question or circular reasoning when we use mathematical induction to prove a theorem we first show that p is true then we know that p is true because p implies p further we know that p is true because p implies p continuing along these lines we see that p n is true for every positive integer n historical note the first known use of mathematical induction is in the work of the sixteenth century mathematician francesco maurolico maurolico wrote extensively on the works of classical mathematics and made many contributions to geometry and optics in his book arithmeticorum libri duo maurolico presented a variety of properties of the integers together with proofs of these properties to prove some of these properties he devised the method of mathematical induction his first use of mathematical induction in this book was to prove that the sum of the first n odd positive integers equals augustus de morgan is credited with the first presentation in of formal proofs using mathematical induction as well as introducing the terminology mathematical induction maurolico proofs were informal and he never used the word induction see to learn more about the history of the method of mathematical induction figure illustrating how mathematical induction works using dominoes ways to remember how mathematical induction works thinking of the infinite ladder and the rules for reaching steps can help you remember how mathematical induction works note that statements and for the infinite ladder are exactly the basis step and inductive step respectively of the proof that p n is true for all positive integers n where p n is the statement that we can reach the nth rung of the ladder consequently we can invoke mathematical induction to conclude that we can reach every rung another way to illustrate the principle of mathematical induction is to consider an infinite row of dominoes labeled n where each domino is standing up let p n be the proposition that domino n is knocked over if the first domino is knocked over i e if p is true and if whenever the kth domino is knocked over it also knocks the k st domino over i e if p k p k is true for all positive integers k then all the dominoes are knocked over this is illustrated in figure why mathematical induction is valid why is mathematical induction a valid proof technique the reason comes from the well ordering property listed in appendix as an axiom for the set of positive integers which states that every nonempty subset of the set of positive integers has a least element so suppose we know that p is true and that the proposition p k p k is true for all positive integers k to show that p n must be true for all positive integers n assume that there is at least one positive integer for which p n is false then the set s of positive integers for which p n is false is nonempty thus by the well ordering property s has a least element which will be denoted by m we know that m cannot be because p is true because m is positive and greater than m is a positive integer furthermore because m is less than m it is not in s so p m must be true because the conditional statement p m p m is also true it must be the case that p m is true this contradicts the choice of m hence p n must be true for every positive integer n the good and the bad of mathematical induction an important point needs to be made about mathematical induction before we commence a study of its use the good thing about mathematical induction is that it can be used to prove you can prove a theorem by mathematical induction even if you do not have the slightest idea why it is true a conjecture once it is has been made and is true the bad thing about it is that it cannot be used to find new theorems mathematicians sometimes find proofs by mathematical in duction unsatisfying because they do not provide insights as to why theorems are true many theorems can be proved in many ways including by mathematical induction proofs of these the orems by methods other than mathematical induction are often preferred because of the insights they bring examples of proofs by mathematical induction many theorems assert that p n is true for all positive integers n where p n is a propositional function mathematical induction is a technique for proving theorems of this kind in other words mathematical induction can be used to prove statements of the form n p n where the domain is the set of positive integers mathematical induction can be used to prove an extremely wide variety of theorems each of which is a statement of this form remember many mathematical assertions include an implicit universal quantifier the statement if n is a positive integer then n is divisible by is an example of this making the implicit universal quantifier explicit yields the statement for every positive integer n n is divisible by we will use how theorems are proved using mathematical induction the theorems we will prove include summation formulae inequalities identities for combinations of sets divisibility results theorems about algorithms and some other creative results in this section and in later sections we will employ mathematical induction to prove many other types of results including the correctness of computer programs and algorithms mathematical induction can be used to prove a wide variety of theorems not just summation formulae inequalities and other types of examples we illustrate here for proofs by mathematical induction of many more interesting and diverse results see the handbook of mathematical induction by david gunderson this book is part of the extensive crc series in discrete mathematics many of which may be of interest to readers the author is the series editor of these books note that there are many opportunities for errors in induction proofs we will describe some incorrect proofs by mathematical induction at the end of this section and in the exercises to avoid making errors in proofs by mathematical induction try to follow the guidelines for such proofs given at the end of this section ih look for the symbol to see where the inductive hypothesis is used seeingwherethe inductive hypothesis is used to help the reader understand each of the mathematical induction proofs in this section we will note where the inductive hypothesis is used we indicate this use in three different ways by explicit mention in the text by inserting the acronym ih for inductive hypothesis over an equals sign or a sign for an inequality or by specifying the inductive hypothesis as the reason for a step in a multi line display proving summation formulae we begin by using mathematical induction to prove several summation formulae as we will see mathematical induction is particularly well suited for proving that such formulae are valid however summation formulae can be proven in other ways this is not surprising because there are often different ways to prove a theorem the major disadvantage of using mathematical induction to prove a summation formula is that you cannot use it to derive this formula that is you must already have the formula before you attempt to prove it by mathematical induction examples illustrate how to use mathematical induction to prove summation formulae the first summation formula we will prove by mathematical induction in example is a closed formula for the sum of the smallest n positive integers example show that if n is a positive integer then n n n if you are rusty simplifying algebraic expressions this is the time to do some reviewing solution let p n be the proposition that the sum of the first n positive integers n n n is n n we must do two things to prove that p n is true for n namely we must show that p is true and that the conditional statement p k implies p k is true for k basis step p is true because the left hand side of this equation is because is the sum of the first positive integer the right hand side is found by substituting for n in n n inductive step for the inductive hypothesis we assume that p k holds for an arbitrary positive integer k that is we assume that k k k under this assumption it must be shown that p k is true namely that k k k k k k is also true when we add k to both sides of the equation in p k we obtain ih k k k k k k k k k k this last equation shows that p k is true under the assumption that p k is true this completes the inductive step we have completed the basis step and the inductive step so by mathematical induction we know that p n is true for all positive integers n that is we have proven that n n n for all positive integers n as we noted mathematical induction is not a tool for finding theorems about all positive integers rather it is a proof method for proving such results once they are conjectured in example using mathematical induction to prove a summation formula we will both formulate and then prove a conjecture example conjecture a formula for the sum of the first n positive odd integers then prove your conjecture using mathematical induction solution the sums of the first n positive odd integers for n are from these values it is reasonable to conjecture that the sum of the first n positive odd integers is that is we need a method to prove that this conjecture is correct if in fact it is let p n denote the proposition that the sum of the first n odd positive integers is our conjecture is that p n is true for all positive integers to use mathematical induction to prove this conjecture we must first complete the basis step that is we must show that p is true then we must carry out the inductive step that is we must show that p k is true when p k is assumed to be true we now attempt to complete these two steps basis step p states that the sum of the first one odd positive integer is this is true because the sum of the first odd positive integer is the basis step is complete inductive step to complete the inductive step we must show that the proposition p k p k is true for every positive integer k to do this we first assume the inductive hypothesis the inductive hypothesis is the statement that p k is true for an arbitrary positive integer k that is note that the kth odd positive integer is because this integer is obtained by adding a total of k times to to show that k p k p k is true we must show that if p k is true the inductive hypothesis then p k is true note that p k is the statement that k so assuming that p k is true it follows that i h k this shows that p k follows from p k note that we used the inductive hypothesis p k in the second equality to replace the sum of the first k odd positive integers by we have now completed both the basis step and the inductive step that is we have shown that p is true and the conditional statement p k p k is true for all positive integers k consequently by the principle of mathematical induction we can conclude that p n is true for all positive integers n that is we know that for all positive integers n often we will need to show that p n is true for n b b b where b is an integer other than we can use mathematical induction to accomplish this as long as we change the basis step by replacing p with p b in other words to use mathematical induction to show that p n is true for n b b b where b is an integer other than we show that p b is true in the basis step in the inductive step we show that the conditional statement p k p k is true for k b b b note that b can be negative zero or positive following the domino analogy we used earlier imagine that we begin by knocking down the bth domino the basis step and as each domino falls it knocks down the next domino the inductive step we leave it to the reader to show that this form of induction is valid see exercise we illustrate this notion in example which states that a summation formula is valid for all nonnegative integers in this example we need to prove that p n is true for n so the basis step in example shows that p is true example use mathematical induction to show that for all nonnegative integers n solution let p n be the proposition that for the integer n basis step p is true because this completes the basis step inductive step for the inductive hypothesis we assume that p k is true for an arbitrary nonnegative integer k that is we assume that to carry out the inductive step using this assumption we must show that when we assume that p k is true then p k is also true that is we must show that k assuming the inductive hypothesis p k under the assumption of p k we see that i h note that we used the inductive hypothesis in the second equation in this string of equalities to replace by we have completed the inductive step because we have completed the basis step and the inductive step by mathematical induction we know that p n is true for all nonnegative integers n that is for all nonnegative integers n the formula given in example is a special case of a general result for the sum of terms of a geometric progression theorem in section we will use mathematical induction to provide an alternative proof of this formula example sums of geometric progressions use mathematical induction to prove this formula for the sum of a finite number of terms of a geometric progression with initial term a and common ratio r j arj a ar arn arn a r when r where n is a nonnegative integer solution to prove this formula using mathematical induction let p n be the statement that the sum of the first n terms of a geometric progression in this formula is correct basis step p is true because a ar a a r r r a r inductive step the inductive hypothesis is the statement that p k is true where k is an arbitrary nonnegative integer that is p k is the statement that a ar ark ark a r to complete the inductive step we must show that if p k is true then p k is also true to show that this is the case we first add ark to both sides of the equality asserted by p k we find that a ar ark ar ih ark a ar r k rewriting the right hand side of this equation shows that ark a r ark ark a r ark ark r ark a r combining these last two equations gives a ar ark ar k ark a r this shows that if the inductive hypothesis p k is true then p k must also be true this completes the inductive argument we have completed the basis step and the inductive step so by mathematical induction p n is true for all nonnegative integers n this shows that the formula for the sum of the terms of a geometric series is correct as previously mentioned the formula in example is the case of the formula in example with a and r the reader should verify that putting these values for a and r into the general formula gives the same formula as in example proving inequalities mathematical induction can be used to prove a variety of inequalities that hold for all positive integers greater than a particular positive integer as examples illustrate example use mathematical induction to prove the inequality n for all positive integers n solution let p n be the proposition that n basis step p is true because this completes the basis step inductive step we first assume the inductive hypothesis that p k is true for anarbitrary positive integer k that is the inductive hypothesis p k is the statement that k to complete the inductive step we need to show that if p k is true then p k which is the statement that k is true that is we need to show that if k then k to show that this conditional statement is true for the positive integer k we first add to both sides of k and then note that this tells us that ih k this shows that p k is true namely that k based on the assumption that p k is true the induction step is complete therefore because we have completed both the basis step and the inductive step by the principle of mathematical induction we have shown that n is true for all positive integers n example use mathematical induction to prove that n for every integer n with n note that this inequality is false for n and solution let p n be the proposition that n basis step to prove the inequality for n requires that the basis step be p note that p is true because inductive step for the inductive step we assume that p k is true for an arbitrary integer k with k that is we assume that k for the positive integer k with k we must show that under this hypothesis p k is also true that is we must show that if k for an arbitrary positive integer k where k then k we have by definition of exponent k by the inductive hypothesis k k because k k by definition of factorial function this shows that p k is true when p k is true this completes the inductive step of the proof we have completed the basis step and the inductive step hence by mathematical induction p n is true for all integers n with n that is we have proved that n is true for all integers n with n an important inequality for the sum of the reciprocals of a set of positive integers will be proved in example example an inequality for harmonic numbers the harmonic numbers hj j are defined by hj j for instance use mathematical induction to show that n whenever n is a nonnegative integer n solution to carry out the proof let p n be the proposition that basis step p is true because h inductive step the inductive hypothesis is the statement that p k is true that is k where k is an arbitrary nonnegative integer we must show that if p k is true then p k which states that hypothesis it follows that k is also true so assuming the inductive by the definition of harmonic by the definition of th harmonic number k by the inductive hypothesis k because there are terms k each k this establishes the inductive step of the proof we have completed the basis step and the inductive step thus by mathematical induction p n is true for all nonnegative integers n that is the inequality n for the harmonic numbers holds for all nonnegative integers n remark the inequality established here shows that the harmonic series n is a divergent infinite series this is an important example in the study of infinite series proving divisibility results mathematical induction can be used to prove divisibil ity results about integers although such results are often easier to prove using basic results in number theory it is instructive to see how to prove such results using mathematical induction as examples and illustrate example use mathematical induction to prove that n is divisible by whenever n is a posi tive integer note that this is the statement with p of fermat little theorem which is theorem of section solution to construct the proof let p n denote the proposition n is divisible by basis step the statement p is true because is divisible by this completes the basis step inductive step for the inductive hypothesis we assume that p k is true that is we assume that k is divisible by for an arbitrary positive integer k to complete the inductive step we must show that when we assume the inductive hypothesis it follows that p k the statement that k k is divisible by is also true that is we must show that k k is divisible by note that k k k k k using the inductive hypothesis we conclude that the first term k is divisible by the second term is divisible by because it is times an integer so by part i of theorem in section we know that k k is also divisible by this completes the inductive step because we have completed both the basis step and the inductive step by the prin ciple of mathematical induction we know that n is divisible by whenever n is a positive integer the next example presents a more challenging proof by mathematical induction of a divis ibility result example use mathematical induction to prove that is divisible by for every nonnegative integer n solution to construct the proof let p n denote the proposition is divisible by basis step to complete the basis step we must show that p is true because we want to prove that p n is true for every nonnegative integer we see that p is true because 82 81 is divisible by this completes the basis step inductive step for the inductive hypothesis we assume that p k is true for an arbitrary nonnegative integer k that is we assume that is divisible by to complete the inductive step we must show that when we assume that the inductive hypothesis p k is true then p k the statement that k 82 k is divisible by is also true the difficult part of the proof is to see how to use the inductive hypothesis to take advantage of the inductive hypothesis we use these steps k 82 k 82 we can now use the inductive hypothesis which states that 7k is divisible by we will use parts i and ii of theorem in section by part ii of this theorem and the inductive hypothesis we conclude that the first term in this last sum 7k is divisible by by part ii of this theorem the second term in this sum is divisible by hence by part i of this theorem we conclude that 7k 82k 7k 82k is divisible by this completes the inductive step because we have completed both the basis step and the inductive step by the principle of mathematical induction we know that is divisible by for every nonnegative integer n proving results about sets mathematical induction can be used to prove many results about sets in particular in example we prove a formula for the number of subsets of a finite set and in example we establish a set identity figure generating subsets of a set with k elements here t s a example the number of subsets of a finite set use mathematical induction to show that if s is a finite set with n elements where n is a nonnegative integer then s has subsets we will prove this result directly in several ways in chapter solution let p n be the proposition that a set with n elements has subsets basis step p is true because a set with zero elements the empty set has exactly subset namely itself inductive step for the inductive hypothesis we assume that p k is true for an arbitrary nonnegative integer k that is we assume that every set with k elements has subsets it must be shown that under this assumption p k which is the statement that every set with k elements has subsets must also be true to show this let t be a set with k elements then it is possible to write t s a where a is one of the elements of t and s t a and hence s k the subsets of t can be obtained in the following way for each subset x of s there are exactly two subsets of t namely x and x a this is illustrated in figure these constitute all the subsets of t and are all distinct we now use the inductive hypothesis to conclude that s has subsets because it has k elements we also know that there are two subsets of t for each subset of s therefore there are subsets of t this finishes the inductive argument because we have completed the basis step and the inductive step by mathematical induction it follows that p n is true for all nonnegative integers n that is we have proved that a set with n elements has subsets whenever n is a nonnegative integer example use mathematical induction to prove the following generalization of one of de morgan laws n aj j n aj j whenever an are subsets of a universal set u and n solution let p n be the identity for n sets basis step the statement p asserts that this is one of de morgan laws it was proved in example of section inductive step the inductive hypothesis is the statement that p k is true where k is an arbitrary integer with k that is it is the statement that k aj j k aj j whenever ak are subsets of the universal set u to carry out the inductive step we need to show that this assumption implies that p k is true that is we need to show that if this equality holds for every collection of k subsets of u then it must also hold for every collection of k subsets of u suppose that ak ak are subsets of u when the inductive hypothesis is assumed to hold it follows that k j k aj j k j aj ak by the definition of intersection aj ak by de morgan law where the two sets are nk aj and ak k k aj by the definition of union j this completes the inductive step because we have completed both the basis step and the inductive step by mathematical induction we know that p n is true whenever n is a positive integer n that is we know that n aj j n aj j whenever an are subsets of a universal set u and n proving results about algorithms next we provide an example somewhat more difficult than previous examples that illustrates one of many ways mathematical induction is used in the study of algorithms we will show how mathematical induction can be used to prove that a greedy algorithm we introduced in section always yields an optimal solution example recall the algorithm for scheduling talks discussed in example of section the input to this algorithm is a group of m proposed talks with preset starting and ending times the goal is to schedule as many of these lectures as possible in the main lecture hall so that no two talks overlap suppose that talk tj begins at time sj and ends at time ej no two lectures can proceed in the main lecture hall at the same time but a lecture in this hall can begin at the same time another one ends without loss of generality we assume that the talks are listed in order of nondecreasing ending time so that em the greedy algorithm proceeds by selecting at each stage a talk with the earliest ending time among all those talks that begin no sooner than when the last talk scheduled in the main lecture hall has ended note that a talk with the earliest end time is always selected first by the algorithm we will show that this greedy algorithm is optimal in the sense that it always schedules the most talks possible in the main lecture hall to prove the optimality of this algorithm we use mathematical induction on the variable n the number of talks scheduled by the algorithm we let p n be the proposition that if the greedy algorithm schedules n talks in the main lecture hall then it is not possible to schedule more than n talks in this hall basis step suppose that the greedy algorithm managed to schedule just one talk in the main lecture hall this means that no other talk can start at or after the end time of otherwise the first such talk we come to as we go through the talks in order of nondecreasing end times could be added hence at time each of the remaining talks needs to use the main lecture hall because they all start before and end after it follows that no two talks can be scheduled because both need to use the main lecture hall at time this shows that p is true and completes the basis step inductive step the inductive hypothesis is that p k is true where k is an arbitrary positive integer that is that the greedy algorithm always schedules the most possible talks when it selects k talks where k is a positive integer given any set of talks no matter how many we must show that p k follows from the assumption that p k is true that is we must show that under the assumption of p k the greedy algorithm always schedules the most possible talks when it selects k talks now suppose that the greedy algorithm has selected k talks our first step in completing the inductive step is to show there is a schedule including the most talks possible that contains talk a talk with the earliest end time this is easy to see because a schedule that begins with the talk ti in the list where i can be changed so that talk replaces talk ti to see this note that because ei all talks that were scheduled to follow talk ti can still be scheduled once we included talk scheduling the talks so that as many as possible are scheduled is reduced to scheduling as many talks as possible that begin at or after time so if we have scheduled as many talks as possible the schedule of talks other than talk is an optimal schedule of the original talks that begin once talk has ended because the greedy algorithm schedules k talks when it creates this schedule we can apply the inductive hypothesis to conclude that it has scheduled the most possible talks it follows that the greedy algorithm has scheduled the most possible talks k when it produced a schedule with k talks so p k is true this completes the inductive step we have completed the basis step and the inductive step so by mathematical induction we know that p n is true for all positive integers n this completes the proof of optimality that is we have proved that when the greedy algorithm schedules n talks when n is a positive integer then it is not possible to schedule more than n talks creative uses of mathematical induction mathematical induction can often be used in unexpected ways we will illustrate two particularly clever uses of mathematical induction here the first relating to survivors in a pie fight and the second relating to tilings with regular triominoes of checkerboards with one square missing example odd pie fights an odd number of people stand in a yard at mutually distinct distances at the same time each person throws a pie at their nearest neighbor hitting this person use mathematical induction to show that there is at least one survivor that is at least one person who is not hit by a pie this problem was introduced by carmony note that this result is false when there are an even number of people see exercise solution let p n be the statement that there is a survivor whenever people stand in a yard at distinct mutual distances and each person throws a pie at their nearest neighbor to prove this result we will show that p n is true for all positive integers n this follows because as n runs through all positive integers runs through all odd integers greater than or equal to note that one person cannot engage in a pie fight because there is no one else to throw the pie at basis step when n there are people in the pie fight of the three people suppose that the closest pair are a and b and c is the third person because distances between pairs of people are different the distance between a and c and the distance between b and c are both different from and greater than the distance between a and b it follows that a and b throw pies at each other while c throws a pie at either a or b whichever is closer hence c is not hit by a pie this shows that at least one of the three people is not hit by a pie completing the basis step inductive step for the inductive step assume that p k is true for an arbitrary odd integer k with k that is assume that there is at least one survivor whenever people stand in a yard at distinct mutual distances and each throws a pie at their nearest neighbor we must show that if the inductive hypothesis p k is true then p k the statement that there is at least one survivor whenever k people stand in a yard at distinct mutual distances and each throws a pie at their nearest neighbor is also true so suppose that we have k people in a yard with distinct distances between pairs of people let a and b be the closest pair of people in this group of people when each person throws a pie at the nearest person a and b throw pies at each other we have two cases to consider i when someone else throws a pie at either a or b and ii when no one else throws a pie at either a or b case i because a and b throw pies at each other and someone else throws a pie at either a and b at least three pies are thrown at a and b and at most pies are thrown at the remaining people this guarantees that at least one person is a survivor for if each of these people was hit by at least one pie a total of at least pies would have to be thrown at them the reasoning used in this last step is an example of the pigeonhole principle discussed further in section case ii no one else throws a pie at either a and b besides a and b there are people because the distances between pairs of these people are all different we can use the inductive hypothesis to conclude that there is at least one survivor s when these people each throws a pie at their nearest neighbor furthermore s is also not hit by either the pie thrown by a or the pie thrown by b because a and b throw their pies at each other so s is a survivor because s is not hit by any of the pies thrown by these people we have completed both the basis step and the inductive step using a proof by cases so by mathematical induction it follows that p n is true for all positive integers n we conclude that whenever an odd number of people located in a yard at distinct mutual distances each throws a pie at their nearest neighbor there is at least one survivor in section we discussed the tiling of checkerboards by polyominoes example illus trates how mathematical induction can be used to prove a result about covering checkerboards with right triominoes pieces shaped like the letter l example let n be a positive integer show that every checkerboard with one square removed can be tiled using right triominoes where these pieces cover three squares at a time as shown in figure solution let p n be the proposition that every checkerboard with one square removed can be tiled using right triominoes we can use mathematical induction to prove that p n is true for all positive integers n figure a right triomino basis step p is true because each of the four checkerboards with one square removed can be tiled using one right triomino as shown in figure figure tiling checkerboards with one square removed inductive step the inductive hypothesis is the assumption that p k is true for the positive integer k that is it is the assumption that every checkerboard with one square removed can be tiled using right triominoes it must be shown that under the assumption of the inductive hypothesis p k must also be true that is any checkerboard with one square removed can be tiled using right triominoes to see this consider a checkerboard with one square removed split this checkerboard into four checkerboards of size by dividing it in half in both directions this is illustrated in figure no square has been removed from three of these four checker boards the fourth checkerboard has one square removed so we now use the inductive hypothesis to conclude that it can be covered by right triominoes now temporarily remove the square from each of the other three checkerboards that has the center of the original larger checkerboard as one of its corners as shown in figure by the inductive hypothesis each of these three checkerboards with a square removed can be tiled by right triomi noes furthermore the three squares that were temporarily removed can be covered by one right triomino hence the entire checkerboard can be tiled with right triominoes we have completed the basis step and the inductive step therefore by mathemati cal induction p n is true for all positive integers n this shows that we can tile every checkerboard where n is a positive integer with one square removed using right triominoes figure dividing a checkerboard into four 2k 2k checkerboards figure tiling the 2k 2k checkerboard with one square removed consult common errors in discrete mathematics on this book website for more basic mistakes mistaken proofs by mathematical induction as with every proof method there are many opportunities for making errors when using mathe matical induction many well known mistaken and often entertaining proofs by mathematical induction of clearly false statements have been devised as exemplified by example and exercises often it is not easy to find where the error in reasoning occurs in such mis taken proofs to uncover errors in proofs by mathematical induction remember that in every such proof both the basis step and the inductive step must be done correctly not completing the basis step in a supposed proof by mathematical induction can lead to mistaken proofs of clearly ridiculous statements such as n n whenever n is a positive integer we leave it to the reader to show that it is easy to construct a correct inductive step in an attempted proof of this statement locating the error in a faulty proof by mathematical induction as example illustrates can be quite tricky especially when the error is hidden in the basis step example find the error in this proof of the clearly false claim that every set of lines in the plane no two of which are parallel meet in a common point proof let p n be the statement that every set of n lines in the plane no two of which are parallel meet in a common point we will attempt to prove that p n is true for all positive integers n basis step the statement p is true because any two lines in the plane that are not parallel meet in a common point by the definition of parallel lines inductive step the inductive hypothesis is the statement that p k is true for the positive integer k that is it is the assumption that every set of k lines in the plane no two of which are parallel meet in a common point to complete the inductive step we must show that if p k is true then p k must also be true that is we must show that if every set of k lines in the plane no two of which are parallel meet in a common point then every set of k lines in the plane no two of which are parallel meet in a common point so consider a set of k distinct lines in the plane by the inductive hypothesis the first k of these lines meet in a common point moreover by the inductive hypothesis the last k of these lines meet in a common point we will show that and must be the same point if and were different points all lines containing both of them must be the same line because two points determine a line this contradicts our assumption that all these lines are distinct thus and are the same point we conclude that the point lies on all k lines we have shown that p k is true assuming that p k is true that is we have shown that if we assume that every k k distinct lines meet in a common point then every k distinct lines meet in a common point this completes the inductive step we have completed the basis step and the inductive step and supposedly we have a correct proof by mathematical induction solution examining this supposed proof by mathematical induction it appears that everything is in order however there is an error as there must be the error is rather subtle carefully looking at the inductive step shows that this step requires that k we cannot show that p implies p when k our goal is to show that every three distinct lines meet in a common point the first two lines must meet in a common point and the last two lines must meet in a common point but in this case and do not have to be the same because only the second line is common to both sets of lines here is where the inductive step fails guidelines for proofs by mathematical induction examples illustrate proofs by mathematical induction of a diverse collection of theorems each of these examples includes all the elements needed in a proof by mathematical induction we have provided an example of an invalid proof by mathematical induction summarizing what we have learned from these examples we can provide some useful guidelines for constructing correct proofs by mathematical induction we now present these guidelines it is worthwhile to revisit each of the mathematical induction proofs in examples to see how these steps are completed it will be helpful to follow these guidelines in the solutions of the exercises that ask for proofs by mathematical induction the guidelines that we presented can be adapted for each of the variants of mathematical induction that we introduce in the exercises and later in this chapter exercises there are infinitely many stations on a train route sup pose that the train stops at the first station and suppose that if the train stops at a station then it stops at the next station show that the train stops at all stations suppose that you know that a golfer plays the first hole of a golf course with an infinite number of holes and that if this golfer plays one hole then the golfer goes on to play the next hole prove that this golfer plays every hole on the course use mathematical induction in exercises to prove sum mation formulae be sure to identify where you use the in ductive hypothesis let p n be the statement that n n for the positive integer n a what is the statement p b show that p is true completing the basis step of the proof c what is the inductive hypothesis d what do you need to prove in the inductive step e complete the inductive step identifying where you use the inductive hypothesis f explain why these steps show that this formula is true whenever n is a positive integer let p n be the statement that n n for the positive integer n a what is the statement p b show that p is true completing the basis step of the proof c what is the inductive hypothesis d what do you need to prove in the inductive step e complete the inductive step identifying where you use the inductive hypothesis f explain why these steps show that this formula is true whenever n is a positive integer prove that n whenever n is a nonnegative integer prove that n n n whenever n is a positive integer prove that whenever n is a nonnegative integer prove that n n whenever n is a nonnegative integer a find a formula for the sum of the first n even positive integers b prove the formula that you conjectured in part a a find a formula for c what is the inductive hypothesis d what do you need to prove in the inductive step e complete the inductive step f explain why these steps show that this inequality is true whenever n is an integer greater than n n prove that n if n is an integer greater than prove that if n is an integer greater than by examining the values of this expression for small values of n b prove the formula you conjectured in part a a find a formula for for which nonnegative integers n is n prove your answer for which nonnegative integers n is prove your answer prove that by examining the values of this expression for small values of n b prove the formula you conjectured in part a prove that whenever n is a positive integer prove that if h then nh h n for all non negative integers n this is called bernoulli inequality suppose that a and b are real numbers with b a j n prove that if n is a positive integer then an bn whenever n is a nonnegative integer prove that n n n n whenever n is a positive integer prove that n n prove that for every positive integer n n is nonnegative whenever n is an n n k integer with n in exercises and hn denotes the nth harmonic number prove that for every positive integer n n n n n n prove that for every positive integer n n n n n n n n prove that n j n n is a positive integer use mathematical induction to prove the inequalities in exer cises let p n be the statement that n nn where n is an integer greater than a what is the statement p b show that p is true completing the basis step of the proof c what is the inductive hypothesis d what do you need to prove in the inductive step e complete the inductive step f explain why these steps show that this inequality is true whenever n is an integer greater than let p n be the statement that prove that n whenever n is a nonnegative in teger prove that h2 hn n hn n use mathematical induction in exercises to prove di visibility facts prove that divides n whenever n is a positive in teger prove that divides whenever n is a positive integer prove that divides n whenever n is a nonnegative integer prove that divides n whenever n is a nonnegative integer prove that is divisible by whenever n is an odd positive integer prove that divides whenever n is a pos itive integer prove that if n is a positive integer then divides use mathematical induction in exercises to prove re sults about sets prove that if a a a and b b b are sets n n where n is an integer greater than such that aj bj for j n then a what is the statement p n n b show that p is true completing the basis step of aj bj prove that if an and bn are sets such that aj bj for j n then inductive step assume that p k is true so that all the horses in any set of k horses are the same color n n consider any k horses number these as horses aj bj j j prove that if an and b are sets then an b b b an b prove that if an and b are sets then k k now the first k of these horses all must have the same color and the last k of these must also have the same color because the set of the first k horses and the set of the last k horses overlap all k must be the same color this shows that p k is true and finishes the proof by induction what is wrong with this proof theorem for every positive integer n n i an b b b an b n basis step the formula is true for n i prove that if an and b are sets then b b an b inductive step suppose that n i n then n i n i n by the induc an b prove that if an are subsets of a universal set u then tive hypothesis n i n n n n n n n n completing the induc tive step n n k k ak what is wrong with this proof theorem for every positive integer n if x and y are positive integers with max x y n then x y prove that if an and b are sets then b b an b an b prove that a set with n elements has n n subsets containing exactly two elements whenever n is an integer greater than or equal to prove that a set with n elements has n n n subsets containing exactly three elements whenever n is an integer greater than or equal to in exercises and we consider the problem of placing towers along a straight road so that every building on the road receives cellular service assume that a building receives cellular service if it is within one mile of a tower devise a greedy algorithm that uses the minimum number of towers possible to provide cell service to d buildings located at positions xd from the start of the road hint at each step go as far as possible along the road before adding a tower so as not to leave any buildings without coverage use mathematical induction to prove that the algorithm you devised in exercise produces an optimal solution that is that it uses the fewest towers possible to provide basis step suppose that n if max x y and x and y are positive integers we have x and y inductive step let k be a positive integer assume that whenever max x y k and x and y are positive inte gers then x y now let max x y k where x and y are positive integers then max x y k so by the inductive hypothesis x y it follows that x y completing the inductive step suppose that m and n are positive integers with m n and f is a function from m to n use mathematical induction on the variable n to show that f is not one to one use mathematical induction to show that n people can di vide a cake where each person gets one or more separate pieces of the cake so that the cake is divided fairly that is in the sense that each person thinks he or she got at least n th of the cake hint for the inductive step take a fair division of the cake among the first k people have each person divide their share into what this per son thinks are k equal portions and then have the k st person select a portion from each of the k peo ple when showing this produces a fair division for k people suppose that person k thinks that person i got cellular service to all buildings exercises present incorrect proofs using mathemati pi of the cake where k pi cal induction you will need to identify an error in reasoning in each exercise what is wrong with this proof that all horses are the same color let p n be the proposition that all the horses in a set of n horses are the same color basis step clearly p is true use mathematical induction to show that given a set of n positive integers none exceeding there is at least one integer in this set that divides another integer in the set a knight on a chessboard can move one space horizon tally in either direction and two spaces vertically in either direction or two spaces horizontally in either di rection and one space vertically in either direction suppose that we have an infinite chessboard made up of all squares m n where m and n are nonnegative inte gers that denote the row number and the column number of the square respectively use mathematical induction to show that a knight starting at can visit every square using a finite sequence of moves hint use induction on the variable m n suppose that a a 07 where a and b are real numbers show that an an for every positive integer n requires calculus use mathematical induction to prove that the derivative of f x xn equals nxn whenever n is a positive integer for the inductive step use the product rule for derivatives suppose that a and b are square matrices with the prop erty ab ba show that abn bna for every positive integer n suppose that m is a positive integer use mathematical induction to prove that if a and b are integers with a b mod m then ak bk mod m whenever k is a nonneg ative integer use mathematical induction to show that pn is equivalent to pn whenever pn are propositions show that pn pn pn pn is a tautology whenever pn are propositions where n show that n lines separate the plane into n regions if no two of these lines are parallel and no three pass through a common point let an be positive real numbers the arith metic mean of these numbers is defined by a an n and the geometric mean of these numbers is defined by g an n use mathematical induction to prove that a g use mathematical induction to prove lemma of section which states that if p is a prime and p an where ai is an integer for i n then p ai for some integer i show that if n is a positive integer then use the well ordering property to show that the follow ing form of mathematical induction is a valid method to prove that p n is true for all positive integers n basis step p and p are true inductive step for each positive integer k if p k and p k are both true then p k is true show that if an are sets where n and for all pairs of integers i and j with i j n either ai is a subset of aj or aj is a subset of ai then there is an integer i i n such that ai is a subset of aj for all integers j with j n a guest at a party is a celebrity if this person is known by every other guest but knows none of them there is at most one celebrity at a party for if there were two they would know each other a particular party may have no celebrity your assignment is to find the celebrity if one exists at a party by asking only one type of question asking a guest whether they know a second guest ev eryone must answer your questions truthfully that is if alice and bob are two people at the party you can ask al ice whether she knows bob she must answer correctly use mathematical induction to show that if there are n people at the party then you can find the celebrity if there is one with n questions hint first ask a question to eliminate one person as a celebrity then use the inductive hypothesis to identify a potential celebrity finally ask two more questions to determine whether that person is actually a celebrity suppose there are n people in a group each aware of a scandal no one else in the group knows about these people commu nicate by telephone when two people in the group talk they share information about all scandals each knows about for example on the first call two people share information so by the end of the call each of these people knows about two scandals the gossip problem asks for g n the minimum number of telephone calls that are needed for all n people to learn about all the scandals exercises deal with the gossip problem find g g g and g use mathematical induction to prove that g n for n hint in the inductive step have a new person call a particular person at the start and at the end prove that g n for n show that it is possible to arrange the numbers n in a row so that the average of any two of these numbers never appears between them hint show that it suffices to prove this fact when n is a power of then use math ematical induction to prove the result when n is a power of show that if in is a collection of open in n tervals on the real number line n and every pair k ii ij whenever i n and j n then here the sum is over all nonempty subsets of the set of the n smallest positive integers the intersection of all these sets is nonempty that is in recall that an open interval is the set of real numbers x with a x b where a and b are real numbers with a b sometimes we cannot use mathematical induction to prove a result we believe to be true but we can use mathematical induction to prove a stronger result because the inductive hy pothesis of the stronger result provides more to work with this process is called inductive loading we use inductive loading in exercise suppose that we want to prove that person throws a pie at their nearest neighbor it is possible that everyone is hit by a pie construct a tiling using right triominoes of the checkerboard with the square in the upper left corner re moved construct a tiling using right triominoes of the checkerboard with the square in the upper left corner re moved prove or disprove that all checkerboards of these shapes can be completely covered using right triominoes when ever n is a positive integer a b c d for all positive integers n a show that if we try to prove this inequality using math ematical induction the basis step works but the in ductive step fails b show that mathematical induction can be used to prove the stronger inequality 79 show that a three dimensional checker board with one cube missing can be completely covered by cubes with one cube re moved show that an n n checkerboard with one square re moved can be completely covered using right triominoes if n n is odd and n 81 show that a checkerboard with a corner square re moved can be tiled using right triominoes 82 find a checkerboard with a square removed that for all integers greater than which together with a verification for the case where n establishes the weaker inequality we originally tried to prove using mathematical induction let n be an even positive integer show that when n peo ple stand in a yard at mutually distinct distances and each cannot be tiled using right triominoes prove that such a tiling does not exist for this board use the principle of mathematical induction to show that p n is true for n b b b where b is an integer if p b is true and the conditional statement p k p k is true for all integers k with k b strong induction and well ordering introduction in section we introduced mathematical induction and we showed how to use it to prove a variety of theorems in this section we will introduce another form of mathematical induction called strong induction which can often be used when we cannot easily prove a result using mathematical induction the basis step of a proof by strong induction is the same as a proof of the same result using mathematical induction that is in a strong induction proof that p n is true for all positive integers n the basis step shows that p is true however the inductive steps in these two proof methods are different in a proof by mathematical induction the inductive step shows that if the inductive hypothesis p k is true then p k is also true in a proof by strong induction the inductive step shows that if p j is true for all positive integers not exceeding k then p k is true that is for the inductive hypothesis we assume that p j is true for j k the validity of both mathematical induction and strong induction follow from the well ordering property in appendix in fact mathematical induction strong induction and well ordering are all equivalent principles as shown in exercises and that is the validity of each can be proved from either of the other two this means that a proof using one of these two principles can be rewritten as a proof using either of the other two principles just as it is sometimes the case that it is much easier to see how to prove a result using strong induction rather than mathematical induction it is sometimes easier to use well ordering than one of the two forms of mathematical induction in this section we will give some examples of how the well ordering property can be used to prove theorems strong induction before we illustrate how to use strong induction we state this principle again note that when we use strong induction to prove that p n is true for all positive integers n our inductive hypothesis is the assumption that p j is true for j k that is the inductive hypothesis includes all k statements p p p k because we can use all k statements p p p k to prove p k rather than just the statement p k as in a proof by mathematical induction strong induction is a more flexible proof technique because of this some mathematicians prefer to always use strong induction instead of mathematical induction even when a proof by mathematical induction is easy to find you may be surprised that mathematical induction and strong induction are equivalent that is each can be shown to be a valid proof technique assuming that the other is valid in particular any proof using mathematical induction can also be considered to be a proof by strong induction because the inductive hypothesis of a proof by mathematical induction is part of the inductive hypothesis in a proof by strong induction that is if we can complete the inductive step of a proof using mathematical induction by showing that p k follows from p k for every positive integer k then it also follows that p k follows from all the statements p p p k because we are assuming that not only p k is true but also more namely that the k statements p p p k are true however it is much more awkward to convert a proof by strong induction into a proof using the principle of mathematical induction see exercise strong induction is sometimes called the second principle of mathematical induction or complete induction when the terminology complete induction is used the principle of mathematical induction is called incomplete induction a technical term that is a somewhat unfortunate choice because there is nothing incomplete about the principle of mathematical induction after all it is a valid proof technique strong induction and the infinite ladder to better understand strong in duction consider the infinite ladder in section strong induction tells us that we can reach all rungs if we can reach the first rung and for every integer k if we can reach all the first k rungs then we can reach the k st rung that is if p n is the statement that we can reach the nth rung of the ladder by strong induction we know that p n is true for all positive integers n because tells us p is true completing the basis step and tells us that p p p k implies p k completing the inductive step example illustrates how strong induction can help us prove a result that cannot easily be proved using the principle of mathematical induction example suppose we can reach the first and second rungs of an infinite ladder and we know that if we can reach a rung then we can reach two rungs higher can we prove that we can reach every rung using the principle of mathematical induction can we prove that we can reach every rung using strong induction solution we first try to prove this result using the principle of mathematical induction basis step the basis step of such a proof holds here it simply verifies that we can reach the first rung attempted inductive step the inductive hypothesis is the statement that we can reach the kth rung of the ladder to complete the inductive step we need to show that if we assume the inductive hypothesis for the positive integer k namely if we assume that we can reach the kth rung of the ladder then we can show that we can reach the k st rung of the ladder however there is no obvious way to complete this inductive step because we do not know from the given information that we can reach the k st rung from the kth rung after all we only know that if we can reach a rung we can reach the rung two higher now consider a proof using strong induction basis step the basis step is the same as before it simply verifies that we can reach the first rung inductive step the inductive hypothesis states that we can reach each of the first k rungs to complete the inductive step we need to show that if we assume that the inductive hypothesis is true that is if we can reach each of the first k rungs then we can reach the k st rung we already know that we can reach the second rung we can complete the inductive step by noting that as long as k we can reach the k st rung from the k st rung because we know we can climb two rungs from a rung we can already reach and because k k by the inductive hypothesis we can reach the k st rung this completes the inductive step and finishes the proof by strong induction we have proved that if we can reach the first two rungs of an infinite ladder and for every positive integer k if we can reach all the first k rungs then we can reach the k st rung then we can reach all rungs of the ladder examples of proofs using strong induction now that we have both mathematical induction and strong induction how do we decide which method to apply in a particular situation although there is no cut and dried answer we can supply some useful pointers in practice you should use mathematical induction when it is straightforward to prove that p k p k is true for all positive integers k this is the case for all the proofs in the examples in section in general you should restrict your use of the principle of mathematical induction to such scenarios unless you can clearly see that the inductive step of a proof by mathematical induction goes through you should attempt a proof by strong induction that is use strong induction and not mathematical induction when you see how to prove that p k is true from the assumption that p j is true for all positive integers j not exceeding k but you cannot see how to prove that p k follows from just p k keep this in mind as you examine the proofs in this section for each of these proofs consider why strong induction works better than mathematical induction we will illustrate how strong induction is employed in examples in these examples we will prove a diverse collection of results pay particular attention to the inductive step in each of these examples where we show that a result p k follows under the assumption that p j holds for all positive integers j not exceeding k where p n is a propositional function we begin with one of the most prominent uses of strong induction the part of the fundamental theorem of arithmetic that tells us that every positive integer can be written as the product of primes example show that if n is an integer greater than then n can be written as the product of primes solution let p n be the proposition that n can be written as the product of primes basis step p is true because can be written as the product of one prime itself note that p is the first case we need to establish inductive step the inductive hypothesis is the assumption that p j is true for all integers j with j k that is the assumption that j can be written as the product of primes whenever j is a positive integer at least and not exceeding k to complete the inductive step it must be shown that p k is true under this assumption that is that k is the product of primes there are two cases to consider namely when k is prime and when k is composite if k is prime we immediately see that p k is true otherwise k is composite and can be written as the product of two positive integers a and b with a b k because both a and b are integers at least and not exceeding k we can use the inductive hypothesis to write both a and b as the product of primes thus if k is composite it can be written as the product of primes namely those primes in the factorization of a and those in the factorization of b remark because can be thought of as the empty product of no primes we could have started the proof in example with p as the basis step we chose not to do so because many people find this confusing example completes the proof of the fundamental theorem of arithmetic which asserts that every nonnegative integer can be written uniquely as the product of primes in nondecreasing order we showed in section that an integer has at most one such factorization into primes example shows there is at least one such factorization next we show how strong induction can be used to prove that a player has a winning strategy in a game example consider a game in which two players take turns removing any positive number of matches they want from one of two piles of matches the player who removes the last match wins the game show that if the two piles contain the same number of matches initially the second player can always guarantee a win solution let n be the number of matches in each pile we will use strong induction to prove p n the statement that the second player can win when there are initially n matches in each pile basis step when n the first player has only one choice removing one match from one of the piles leaving a single pile with a single match which the second player can remove to win the game inductive step the inductive hypothesis is the statement that p j is true for all j with j k that is the assumption that the second player can always win whenever there are j matches where j k in each of the two piles at the start of the game we need to show that p k is true that is that the second player can win when there are initially k matches in each pile under the assumption that p j is true for j k so suppose that there are k matches in each of the two piles at the start of the game and suppose that the first player removes r matches r k from one of the piles leaving k r matches in this pile by removing the same number of matches from the other pile the second player creates the situation where there are two piles each with k r matches because k r k we can now use the inductive hypothesis to conclude that the second player can always win we complete the proof by noting that if the first player removes all k matches from one of the piles the second player can win by removing all the remaining matches using the principle of mathematical induction instead of strong induction to prove the results in examples and is difficult however as example shows some results can be readily proved using either the principle of mathematical induction or strong induction before we present example note that we can slightly modify strong induction to handle a wider variety of situations in particular we can adapt strong induction to handle cases where the inductive step is valid only for integers greater than a particular integer let b be a fixed integer and j a fixed positive integer the form of strong induction we need tells us that p n is true for all integers n with n b if we can complete these two steps basis step we verify that the propositions p b p b p b j are true inductive step we show that p b p b p k p k is true for every integer k b j we will use this alternative form in the strong induction proof in example that this alternative form is equivalent to strong induction is left as exercise example prove that every amount of postage of cents or more can be formed using just cent and cent stamps solution we will prove this result using the principle of mathematical induction then we will present a proof using strong induction let p n be the statement that postage of n cents can be formed using cent and cent stamps we begin by using the principle of mathematical induction basis step postage of cents can be formed using three cent stamps inductive step the inductive hypothesis is the statement that p k is true that is under this hypothesis postage of k cents can be formed using cent and cent stamps to complete the inductive step we need to show that when we assume p k is true then p k is also true where k that is we need to show that if we can form postage of k cents then we can form postage of k cents so assume the inductive hypothesis is true that is assume that we can form postage of k cents using cent and cent stamps we consider two cases when at least one cent stamp has been used and when no cent stamps have been used first suppose that at least one cent stamp was used to form postage of k cents then we can replace this stamp with a cent stamp to form postage of k cents but if no cent stamps were used we can form postage of k cents using only cent stamps moreover because k we needed at least three cent stamps to form postage of k cents so we can replace three cent stamps with four cent stamps to form postage of k cents this completes the inductive step because we have completed the basis step and the inductive step we know that p n is true for all n that is we can form postage of n cents where n using just cent and cent stamps this completes the proof by mathematical induction next we will use strong induction to prove the same result in this proof in the basis step we show that p p p and p are true that is that postage of or cents can be formed using just cent and cent stamps in the inductive step we show how to get postage of k cents for k from postage of k cents basis step we can form postage of and cents using three cent stamps two cent stamps and one cent stamp one cent stamp and two cent stamps and three cent stamps respectively this shows that p p p and p are true this completes the basis step inductive step the inductive hypothesis is the statement that p j is true for j k where k is an integer with k to complete the inductive step we assume that we can form postage of j cents where j k we need to show that under the assumption that p k is true we can also form postage of k cents using the inductive hypothesis we can assume that p k is true because k that is we can form postage of k cents using just cent and cent stamps to form postage of k cents we need only add another cent stamp to the stamps we used to form postage of k cents that is we have shown that if the inductive hypothesis is true then p k is also true this completes the inductive step because we have completed the basis step and the inductive step of a strong induction proof we know by strong induction that p n is true for all integers n with n that is we know that every postage of n cents where n is at least can be formed using cent and cent stamps this finishes the proof by strong induction there are other ways to approach this problem besides those described here can you find a solution that does not use mathematical induction using strong induction in computational geometry our next example of strong induction will come from computational geometry the part of discrete mathematics that studies computational problems involving geometric objects compu tational geometry is used extensively in computer graphics computer games robotics scientific calculations and a vast array of other areas before we can present this result we introduce some terminology possibly familiar from earlier studies in geometry a polygon is a closed geometric figure consisting of a sequence of line segments sn called sides each pair of consecutive sides si and si i n as well as the last side sn and the first side of the polygon meet at a common endpoint called a vertex a polygon is called simple if no two nonconsecutive sides intersect every sim ple polygon divides the plane into two regions its interior consisting of the points inside the curve and its exterior consisting of the points outside the curve this last fact is surprisingly complicated to prove it is a special case of the famous jordan curve theorem which tells us that every simple curve divides the plane into two regions see for example a polygon is called convex if every line segment connecting two points in the interior of the polygon lies entirely inside the polygon a polygon that is not convex is said to be nonconvex figure displays some polygons polygons a and b are convex but polygons c and d are not a diagonal of a simple polygon is a line segment connecting two nonconsecutive vertices of the polygon and a diagonal is called an interior diagonal if it lies entirely inside the polygon ex cept for its endpoints for example in polygon d the line segment connecting a and f is an inte rior diagonal but the line segment connecting a and d is a diagonal that is not an interior diagonal one of the most basic operations of computational geometry involves dividing a simple polygon into triangles by adding nonintersecting diagonals this process is called triangulation note that a simple polygon can have many different triangulations as shown in figure perhaps the most basic fact in computational geometry is that it is possible to triangulate every simple a a d a f b e a g f b e d af is an interior diagonal b c c d b d c ad is not an interior diagonal a b c d figure convex and nonconvex polygons two different triangulations of a simple polygon with seven sides into five triangles shown with dotted lines and with dashed lines respectively figure triangulations of a polygon polygon as we state in theorem furthermore this theorem tells us that every triangulation of a simple polygon with n sides includes n triangles theorem it seems obvious that we should be able to triangulate a simple polygon by successively adding interior diagonals consequently a proof by strong induction seems promising however such a proof requires this crucial lemma lemma although lemma seems particularly simple it is surprisingly tricky to prove in fact as recently as years ago a variety of incorrect proofs thought to be correct were commonly seen in books and articles we defer the proof of lemma until after we prove theorem it is not uncommon to prove a theorem pending the later proof of an important lemma proof of theorem we will prove this result using strong induction let t n be the statement that every simple polygon with n sides can be triangulated into n triangles basis step t is true because a simple polygon with three sides is a triangle we do not need to add any diagonals to triangulate a triangle it is already triangulated into one triangle itself consequently every simple polygon with n has can be triangulated into n triangle inductive step for the inductive hypothesis we assume that t j is true for all integers j with j k that is we assume that we can triangulate a simple polygon with j sides into j triangles whenever j k to complete the inductive step we must show that when we assume the inductive hypothesis p k is true that is that every simple polygon with k sides can be triangulated into k k triangles so suppose that we have a simple polygon p with k sides because k lemma tells us that p has an interior diagonal ab now ab splits p into two simple polygons q with sides and r with t sides the sides of q and r are the sides of p together with the side ab which is a side of both q and r note that k and t k because both q and r have at least one fewer side than p does after all each of these is formed from p by deleting at least two sides and replacing these sides by the diagonal ab furthermore the number of sides of p is two less than the sum of the numbers of sides of q and the number of a b e t is the triangle abc p is the vertex of p inside t such that the bap is smallest bp must be an interior diagonal of p c figure constructing an interior diagonal of a simple polygon sides of r because each side of p is a side of either q or of r but not both and the diagonal ab is a side of both q and r but not p that is k t we now use the inductive hypothesis because both k and t k by the induc tive hypothesis we can triangulate q and r into and t triangles respectively next note that these triangulations together produce a triangulation of p each diagonal added to triangulate one of these smaller polygons is also a diagonal of p consequently we can trian gulate p into a total of t t k triangles this completes the proof by strong induction that is we have shown that every simple polygon with n sides where n can be triangulated into n triangles we now return to our proof of lemma we present a proof published by chung wu ho note that although this proof may be omitted without loss of continuity it does provide a correct proof of a result proved incorrectly by many mathematicians proof suppose that p is a simple polygon drawn in the plane furthermore suppose that b is the point of p or in the interior of p with the least y coordinate among the vertices with the smallest x coordinate then b must be a vertex of p for if it is an interior point there would have to be a vertex of p with a smaller x coordinate two other vertices each share an edge with b say a and c it follows that the angle in the interior of p formed by ab and bc must be less than degrees otherwise there would be points of p with smaller x coordinates than b now let t be the triangle abc if there are no vertices of p on or inside t we can connect a and c to obtain an interior diagonal on the other hand if there are vertices of p inside t we will find a vertex p of p on or inside t such that bp is an interior diagonal this is the tricky part ho noted that in many published proofs of this lemma a vertex p was found such that bp was not necessarily an interior diagonal of p see exercise the key is to select a vertex p such that the angle bap is smallest to see this note that the ray starting at a and passing through p hits the line segment bc at a point say q it then follows that the triangle baq cannot contain any vertices of p in its interior hence we can connect b and p to produce an interior diagonal of p locating this vertex p is illustrated in figure proofs using the well ordering property the validity of both the principle of mathematical induction and strong induction follows from a fundamental axiom of the set of integers the well ordering property see appendix the well ordering property states that every nonempty set of nonnegative integers has a least element we will show how the well ordering property can be used directly in proofs furthermore it can be shown see exercises and that the well ordering property the principle of mathematical induction and strong induction are all equivalent that is the validity of each of these three proof techniques implies the validity of the other two techniques in section we showed that the principle of mathematical induction follows from the well ordering property the other parts of this equivalence are left as exercises and the well ordering property every nonempty set of nonnegative integers has a least element the well ordering property can often be used directly in proofs example use the well ordering property to prove the division algorithm recall that the division algorithm states that if a is an integer and d is a positive integer then there are unique integers q and r with r d and a dq r solution let s be the set of nonnegative integers of the form a dq where q is an integer this set is nonempty because dq can be made as large as desired taking q to be a negative integer with large absolute value by the well ordering property s has a least element r a the integer r is nonnegative it is also the case that r d if it were not then there would be a smaller nonnegative element in s namely a d to see this suppose that r d because a r it follows that a d a d r d conse quently there are integers q and r with r d the proof that q and r are unique is left as exercise example in a round robin tournament every player plays every other player exactly once and each match has a winner and a loser we say that the players pm form a cycle if beats p2 p2 beats pm beats pm and pm beats use the well ordering principle to show that if there is a cycle of length m m among the players in a round robin tournament there must be a cycle of three of these players exercises solution we assume that there is no cycle of three players because there is at least one cycle in the round robin tournament the set of all positive integers n for which there is a cycle of length n is nonempty by the well ordering property this set of positive integers has a least element k which by assumption must be greater than three consequently there exists a cycle of players p2 pk and no shorter cycle exists because there is no cycle of three players we know that k consider the first three elements of this cycle p2 and there are two possible outcomes of the match between and if beats it follows that p2 is a cycle of length three contradicting our assumption that there is no cycle of three players consequently it must be the case that beats this means that we can omit p2 from the cycle p2 p3 pk to obtain the cycle p3 pk of length k contradicting the assumption that the smallest cycle has length k we conclude that there must be a cycle of length three use strong induction to show that if you can run one mile or two miles and if you can always run two more miles once you have run a specified number of miles then you can run any number of miles use strong induction to show that all dominoes fall in an infinite arrangement of dominoes if you know that the first three dominoes fall and that when a domino falls the domino three farther down in the arrangement also falls let p n be the statement that a postage of n cents can be formed using just cent stamps and cent stamps the parts of this exercise outline a strong induction proof that p n is true for n a show that the statements p p and p are true completing the basis step of the proof b what is the inductive hypothesis of the proof c what do you need to prove in the inductive step d complete the inductive step for k e explain why these steps show that this statement is true whenever n let p n be the statement that a postage of n cents can be formed using just cent stamps and cent stamps the parts of this exercise outline a strong induction proof that p n is true for n a show statements p p p and p are true completing the basis step of the proof b what is the inductive hypothesis of the proof c what do you need to prove in the inductive step d complete the inductive step for k e explain why these steps show that this statement is true whenever n a determine which amounts of postage can be formed using just cent and cent stamps b prove your answer to a using the principle of math ematical induction be sure to state explicitly your inductive hypothesis in the inductive step c prove your answer to a using strong induction how does the inductive hypothesis in this proof differ from that in the inductive hypothesis for a proof using math ematical induction a determine which amounts of postage can be formed using just cent and cent stamps b prove your answer to a using the principle of math ematical induction be sure to state explicitly your inductive hypothesis in the inductive step c prove your answer to a using strong induction how does the inductive hypothesis in this proof differ from that in the inductive hypothesis for a proof using math ematical induction which amounts of money can be formed using just two dollar bills and five dollar bills prove your answer using strong induction suppose that a store offers gift certificates in denomina tions of dollars and dollars determine the possible total amounts you can form using these gift certificates prove your answer using strong induction use strong induction to prove t hat is irrational hint use strong induction to show that every positive integer n can be written as a sum of distinct powers of two that is as a sum of a subset of the integers and so on hint for the inductive step separately con sider the case where k is even and where it is odd when it is even note that k is an integer a jigsaw puzzle is put together by successively joining pieces that fit together into blocks a move is made each time a piece is added to a block or when two blocks are joined use strong induction to prove that no matter how the moves are carried out exactly n moves are required to assemble a puzzle with n pieces suppose you begin with a pile of n stones and split this pile into n piles of one stone each by successively split ting a pile of stones into two smaller piles each time you split a pile you multiply the number of stones in each of the two smaller piles you form so that if these piles have r and stones in them respectively you compute rs show that no matter how you split the piles the sum of the products computed at each step equals n n prove that the first player has a winning strategy for the game of chomp introduced in example in section if the initial board is square hint use strong induction to show that this strategy works for the first move the first player chomps all cookies except those in the left and top edges on subsequent moves after the second player has chomped cookies on either the top or left edge the first player chomps cookies in the same relative positions in the left or top edge respectively prove that the first player has a winning strategy for the game of chomp introduced in example in section if the initial board is two squares wide that is a n board hint use strong induction the first move of the first player should be to chomp the cookie in the bottom row at the far right use strong induction to show that if a simple polygon with at least four sides is triangulated then at least two of the triangles in the triangulation have two sides that let p n be the statement that integer b n b for any positive border the exterior of the polygon use strong induction to show that when a simple poly assume that a chocolate bar consists of n squares ar ranged in a rectangular pattern the entire bar a smaller rectangular piece of the bar can be broken along a vertical or a horizontal line separating the squares assuming that only one piece can be broken at a time determine how many breaks you must successively make to break the bar into n separate squares use strong induction to prove your answer consider this variation of the game of nim the game begins with n matches two players take turns removing matches one two or three at a time the player remov ing the last match loses use strong induction to show that if each player plays the best strategy possible the first player wins if n or for some nonnegative integer j and the second player wins in the remaining case when n for some nonnegative integer j gon p with consecutive vertices vn is trian gulated into n triangles the n triangles can be numbered n so that vi is a vertex of triangle i for i n pick theorem says that the area of a simple poly gon p in the plane with vertices that are all lattice points that is points with integer coordinates equals i p b p where i p and b p are the number of lattice points in the interior of p and on the boundary of p respectively use strong induction on the number of vertices of p to prove pick theorem hint for the basis step first prove the theorem for rectangles then for right triangles and finally for all triangles by noting that the area of a triangle is the area of a larger rectangle containing it with the areas of at most three tri angles subtracted for the inductive step take advantage of lemma suppose that p is a simple polygon with vertices vn listed so that consecutive vertices are con nected by an edge and and vn are connected by an edge a vertex vi is called an ear if the line segment connecting the two vertices adjacent to vi is an interior diagonal of the simple polygon two ears vi and vj are called nonover lapping if the interiors of the triangles with vertices vi and its two adjacent vertices and vj and its two adjacent vertices do not intersect prove that every simple polygon with at least four vertices has at least two nonoverlapping ears in the proof of lemma we mentioned that many in correct methods for finding a vertex p such that the line segment bp is an interior diagonal of p have been published this exercise presents some of the incorrect ways p has been chosen in these proofs show by con sidering one of the polygons drawn here that for each of these choices of p the line segment bp is not necessarily an interior diagonal of p a p is the vertex of p such that the angle abp is small est b p is the vertex of p with the least x coordinate other than b c p is the vertex of p that is closest to b b b e a explain where a proof using strong induction that e n is true for all integers n runs into difficulties b show that we can prove that e n is true for all inte gers n by proving by strong induction the stronger statement t n for all integers n which states that in every triangulation of a simple polygon at least two of the triangles in the triangulation have two sides bor dering the exterior of the polygon a stable assignment defined in the preamble to exer cise in section is called optimal for suitors if no stable assignment exists in which a suitor is paired with a suitee whom this suitor prefers to the person to whom this suitor is paired in this stable assignment use strong induction to show that the deferred acceptance algorithm produces a stable assignment that is optimal for suitors suppose that p n is a propositional function determine for which positive integers n the statement p n must be true and justify your answer if a p is true for all positive integers n if p n is true then p n is true b p and p are true for all positive integers n if p n and p n are true then p n is true c p is true for all positive integers n if p n is true then p is true a d p is true for all positive integers n if p n is true then p n is true suppose that p n is a propositional function determine for which nonnegative integers n the statement p n must be true if a p is true for all nonnegative integers n if p n is true then p n is true b p is true for all nonnegative integers n if p n is true then p n is true c p and p are true for all nonnegative integers n if p n and p n are true then p n is true exercises and present examples that show inductive loading can be used to prove results in computational geom etry let p n be the statement that when nonintersecting di agonals are drawn inside a convex polygon with n sides at least two vertices of the polygon are not endpoints of any of these diagonals a show that when we attempt to prove p n for all inte gers n with n using strong induction the inductive step does not go through b show that we can prove that p n is true for all inte gers n with n by proving by strong induction the stronger assertion q n for n where q n states that whenever nonintersecting diagonals are drawn in side a convex polygon with n sides at least two non adjacent vertices are not endpoints of any of these diagonals let e n be the statement that in a triangulation of a sim ple polygon with n sides at least one of the triangles in the triangulation has two sides bordering the exterior of the polygon d p is true for all nonnegative integers n if p n is true then p n and p n are true show that if the statement p n is true for infinitely many positive integers n and p n p n is true for all positive integers n then p n is true for all positive inte gers n let b be a fixed integer and j a fixed positive inte ger show that if p b p b p b j are true and p b p b p k p k is true for every integer k b j then p n is true for all integers n with n b what is wrong with this proof by strong induction theorem for every nonnegative integer n 5n basis step inductive step suppose that for all nonneg ative integers j with j k write k i j where i and j are natural numbers less than k by the inductive hypothesis k i j find the flaw with the following proof that an for all nonnegative integers n whenever a is a nonzero real number basis step is true by the definition of inductive step assume that aj for all nonnegative integers j with j k then note that the set of positive integers of the form as bt where and t are integers a show that s is nonempty b use the well ordering property to show that s has a smallest element c c show that if d is a common divisor of a and b then d is a divisor of c ak ak ak ak d show that c a and c b hint first assume that c a then a qc r where r c show that r s contradicting the choice of c show that strong induction is a valid method of proof by showing that it follows from the well ordering property find the flaw with the following proof that every postage of three cents or more can be formed using just three cent and four cent stamps basis step we can form postage of three cents with a single three cent stamp and we can form postage of four cents using a single four cent stamp inductive step assume that we can form postage of j cents for all nonnegative integers j with j k us ing just three cent and four cent stamps we can then form postage of k cents by replacing one three cent stamp with a four cent stamp or by replacing two four cent stamps by three three cent stamps show that we can prove that p n k is true for all pairs of positive integers n and k if we show a p is true and p n k p n k p n k is true for all positive integers n and k b p k is true for all positive integers k and p n k p n k is true for all positive inte gers n and k c p n is true for all positive integers n and p n k p n k is true for all positive inte gers n and k e conclude from c and d that the greatest common divisor of a and b exists finish the proof by showing that this greatest common divisor is unique let a be an integer and d be a positive integer show that the integers q and r with a dq r and r d which were shown to exist in example are unique use mathematical induction to show that a rectangu lar checkerboard with an even number of cells and two squares missing one white and one black can be covered by dominoes can you use the well ordering property to prove the state ment every positive integer can be described using no more than fifteen english words assume the words come from a particular dictionary of english hint sup pose that there are positive integers that cannot be de scribed using no more than fifteen english words by well ordering the smallest positive integer that cannot be described using no more than fifteen english words would then exist use the well ordering principle to show that if x and y are real numbers with x y then there is a rational number r with x r y hint use the archimedean property given in appendix to find a positive integer a with a y x then show that there is a rational number r with denominator a between x and n n n n j j j j j k k k for all positive inte y by looking at the numbers x j a where j is a positive integer n gers k and n hint use a technique from exercise show that if an are n distinct real numbers ex actly n multiplications are used to compute the prod show that the well ordering property can be proved when the principle of mathematical induction is taken as an ax iom uct of these n numbers no matter how parentheses are inserted into their product hint use strong induction and consider the last multiplication the well ordering property can be used to show that there is a unique greatest common divisor of two positive in tegers let a and b be positive integers and let s be show that the principle of mathematical induction and strong induction are equivalent that is each can be shown to be valid from the other show that we can prove the well ordering property when we take strong induction as an axiom instead of taking the well ordering property as an axiom recursive definitions and structural induction introduction sometimes it is difficult to define an object explicitly however it may be easy to define this object in terms of itself this process is called recursion for instance the picture shown in figure is produced recursively first an original picture is given then a process of successively superimposing centered smaller pictures on top of the previous pictures is carried out figure a recursively defined picture we can use recursion to define sequences functions and sets in section and in most beginning mathematics courses the terms of a sequence are specified using an explicit formula for instance the sequence of powers of is given by an for n recall from section that we can also define a sequence recursively by specifying how terms of the sequence are found from previous terms the sequence of powers of can also be defined by giving the first term of the sequence namely and a rule for finding a term of the sequence from the previous one namely an for n when we define a sequence recursively by specifying how terms of the sequence are found from previous terms we can use induction to prove results about the sequence when we define a set recursively we specify some initial elements in a basis step and provide a rule for constructing new elements from those we already have in the recur sive step to prove results about recursively defined sets we use a method called structural induction recursively defined functions we use two steps to define a function with the set of nonnegative integers as its domain basis step specify the value of the function at zero recursive step give a rule for finding its value at an integer from its values at smaller integers such a definition is called a recursive or inductive definition note that a function f n from the set of nonnegative integers to the set of a real numbers is the same as a sequence where ai is a real number for every nonnegative integer i so defining a real valued sequence using a recurrence relation as was done in section is the same as defining a function from the set of nonnegative integers to the set of real numbers example suppose that f is defined recursively by f f n n find f f f and f solution from the recursive definition it follows that f f f f 93 recursively defined functions are well defined that is for every positive integer the value of the function at this integer is determined in an unambiguous way this means that given any positive integer we can use the two parts of the definition to find the value of the function at that integer and that we obtain the same value no matter how we ap ply the two parts of the definition this is a consequence of the principle of mathemati cal induction see exercise additional examples of recursive definitions are given in examples and example give a recursive definition of an where a is a nonzero real number and n is a nonnegative integer solution the recursive definition contains two parts first is specified namely then the rule for finding an from an namely an a an for n is given these two equations uniquely define an for all nonnegative integers n example give a recursive definition of n ak k solution the first part of the recursive definition is ak k the second part is n k ak n k ak an in some recursive definitions of functions the values of the function at the first k positive integers are specified and a rule is given for determining the value of the function at larger inte gers from its values at some or all of the preceding k integers that recursive definitions defined in this way produce well defined functions follows from strong induction see exercise recall from section that the fibonacci numbers are defined by the equations and fn fn fn for n we can think of the fibonacci number fn either as the nth term of the sequence of fibonacci numbers or as the value at the integer n of a function f n we can use the recursive definition of the fibonacci numbers to prove many properties of these numbers we give one such property in example example show that whenever n fn αn where α solution we can use strong induction to prove this inequality let p n be the statement fn αn we want to show that p n is true whenever n is an integer greater than or equal to basis step first note that α so p and p are true inductive step assume that p j is true namely that fj αj for all integers j with j k where k we must show that p k is true that is that fk αk because α is a solution of x as the quadratic formula shows it follows that α therefore αk αk α αk α αk αk αk αk by the inductive hypothesis because k we have fk αk fk αk therefore it follows that fk fk fk αk αk αk hence p k is true this completes the proof remark the inductive step shows that whenever k p k follows from the assumption that p j is true for j k hence the inductive step does not show that p p therefore we had to show that p is true separately theorem we can now show that the euclidean algorithm introduced in section uses o log b divisions to find the greatest common divisor of the positive integers a and b where a b proof recall that when the euclidean algorithm is applied to find gcd a b with a b this sequence of equations where a and b is obtained r2 r2q2 r2 rn rn rn rn rn rn rnqn here n divisions have been used to find rn gcd a b note that the quotients qn are all at least moreover qn because rn rn this implies that rn rn rn rn rn r2 r4 fn fn fn b r1 r2 fn fn fn it follows that if n divisions are used by the euclidean algorithm to find gcd a b with a b then b fn by example we know that fn αn for n where α therefore it follows that b αn furthermore because α we see that b n α n hence n b now suppose that b has k decimal digits then b and b k it follows that n and because k is an integer it follows that n this finishes the proof because the number of decimal digits in b which equals b is less than or equal to b theorem tells us that the number of divisions required to find gcd a b with a b is less than or equal to b because b is o log b we see that o log b divisions are used by the euclidean algorithm to find gcd a b whenever a b recursively defined sets and structures we have explored how functions can be defined recursively we now turn our attention to how sets can be defined recursively just as in the recursive definition of functions recursive definitions of sets have two parts a basis step and a recursive step in the basis step an initial collection of elements is specified in the recursive step rules for forming new elements in the set from those already known to be in the set are provided recursive definitions may also include an exclusion rule which specifies that a recursively defined set contains nothing other than those elements specified in the basis step or generated by applications of the recursive step in our discussions we will always tacitly assume that the exclusion rule holds and no element belongs to a recursively defined set unless it is in the initial collection specified in the basis step or can be generated using the recursive step one or more times later we will see how we can use a technique known as structural induction to prove results about recursively defined sets examples and illustrate the recursive definition of sets in each example we show those elements generated by the first few applications of the recursive step example consider the subset s of the set of integers recursively defined by basis step s recursive step if x s and y s then x y s the new elements found to be in s are by the basis step at the first application of the recursive step and at the second application of the recursive step and so on we will show in example that s is the set of all positive multiples of recursive definitions play an important role in the study of strings see chapter for an introduction to the theory of formal languages for example recall from section that a string over an alphabet is a finite sequence of symbols from we can define the set of strings over recursively as definition shows definition gabriel lamé gabriel lamé entered the école polytechnique in graduating in he continued his education at the école des mines graduating in in lamé went to russia where he was appointed director of the schools of highways and trans portation in st petersburg not only did he teach but he also planned roads and bridges while in russia he returned to paris in where he helped found an engineering firm however he soon left the firm accepting the chair of physics at the école polytechnique which he held until while holding this position he was active outside academia as an engineering consultant serving as chief engineer of mines and participating in the building of railways lamé contributed original work to number theory applied mathematics and thermodynamics his best known work involves the introduction of curvilinear coordinates his work on number theory includes proving fermat last theorem for n as well as providing the upper bound for the number of divisions used by the euclidean algorithm given in this text in the opinion of gauss one of the most important mathematicians of all time lamé was the foremost french mathematician of his time however french mathematicians considered him too practical whereas french scientists considered him too theoretical the basis step of the recursive definition of strings says that the empty string belongs to the recursive step states that new strings are produced by adding a symbol from to the end of strings in at each application of the recursive step strings containing one additional symbol are generated example if the strings found to be in the set of all bit strings are λ specified to be in in the basis step and formed during the first application of the recursive step and formed during the second application of the recursive step and so on recursive definitions can be used to define operations or functions on the elements of recursively defined sets this is illustrated in definition of the concatenation of two strings and example concerning the length of a string definition the concatenation of the strings and is often written as rather than by repeated application of the recursive definition it follows that the concatenation of two strings and w2 consists of the symbols in followed by the symbols in w2 for instance the concatenation of abra and w2 cadabra is abracadabra example length of a string give a recursive definition of l w the length of the string w solution the length of a string can be recursively defined by l λ l wx l w if w and x another important use of recursive definitions is to define well formed formulae of various types this is illustrated in examples and example well formed formulae in propositional logic we can define the set of well formed for mulae in propositional logic involving t f propositional variables and operators from the set basis step t f and where is a propositional variable are well formed formulae recursive step if e and f are well formed formulae then e e f e f e f and e f are well formed formulae for example by the basis step we know that t f p and q are well formed formulae where p and q are propositional variables from an initial application of the recursive step we know that p q p f f q and q f are well formed formulae a sec ond application of the recursive step shows that p q q f q p q and p f t are well formed formulae we leave it to the reader to show that p q pq and pq are not well formed formulae by showing that none can be obtained using the basis step and one or more applications of the recursive step example well formed formulae of operators and operands we can define the set of well formed formulae consisting of variables numerals and operators from the set where denotes multiplication and denotes exponentiation recursively basis step x is a well formed formula if x is a numeral or a variable recursive step if f and g are well formed formulae then f g f g f g f g and f g are well formed formulae for example by the basis step we see that x y and are well formed formulae as is any variable or numeral well formed formulae generated by applying the recursive step once include x y x y x y x y x and applying the recursive step twice shows that formulae such as x and x y are well formed formulae note that is a well formed formula because we are concerned only with syntax matters here we leave it to the reader to show that each of the formulae y x and x y is not a well formed formula by showing that none of them can be obtained from the basis step and one or more applications of the recursive step we will study trees extensively in chapter a tree is a special type of a graph a graph is made up of vertices and edges connecting some pairs of vertices we will study graphs in chapter we will briefly introduce them here to illustrate how they can be defined recursively definition in figure we illustrate some of the rooted trees formed starting with the basis step and applying the recursive step one time and two times note that infinitely many rooted trees are formed at each application of the recursive definition basis step step step figure building up rooted trees basis step step step step figure building up extended binary trees binary trees are a special type of rooted trees we will provide recursive definitions of two types of binary trees full binary trees and extended binary trees in the recursive step of the definition of each type of binary tree two binary trees are combined to form a new tree with one of these trees designated the left subtree and the other the right subtree in extended binary trees the left subtree or the right subtree can be empty but in full binary trees this is not possible binary trees are one of the most important types of structures in computer science in chapter we will see how they can be used in searching and sorting algorithms in algorithms for compressing data and in many other applications we first define extended binary trees definition figure shows how extended binary trees are built up by applying the recursive step from one to three times we now show how to define the set of full binary trees note that the difference between this recursive definition and that of extended binary trees lies entirely in the basis step basis step step step figure building up full binary trees definition figure shows how full binary trees are built up by applying the recursive step one and two times structural induction to prove results about recursively defined sets we generally use some form of mathematical induction example illustrates the connection between recursively defined sets and mathe matical induction example show that the set s defined in example by specifying that s and that if x s and y s then x y s is the set of all positive integers that are multiples of solution let a be the set of all positive integers divisible by to prove that a s we must show that a is a subset of s and that s is a subset of a to prove that a is a subset of s we must show that every positive integer divisible by is in s we will use mathematical induction to prove this let p n be the statement that belongs to s the basis step holds because by the first part of the recursive definition of s is in s to establish the inductive step assume that p k is true namely that is in s because is in s and because is in s it follows from the second part of the recursive definition of s that k is also in s to prove that s is a subset of a we use the recursive definition of s first the basis step of the definition specifies that is in s because all elements specified to be in s in this step are divisible by and are therefore in a to finish the proof we must show that all integers in s generated using the second part of the recursive definition are in a this consists of showing that x y is in a whenever x and y are elements of s also assumed to be in a now if x and y are both in a it follows that x and y by part i of theorem of section it follows that x y completing the proof in example we used mathematical induction over the set of positive integers and a recursive definition to prove a result about a recursively defined set however instead of using mathematical induction directly to prove results about recursively defined sets we can use a more convenient form of induction known as structural induction a proof by structural induction consists of two parts these parts are basis step show that the result holds for all elements specified in the basis step of the recursive definition to be in the set recursive step show that if the statement is true for each of the elements used to construct new elements in the recursive step of the definition the result holds for these new elements the validity of structural induction follows from the principle of mathematical induction for the nonnegative integers to see this let p n state that the claim is true for all elements of the set that are generated by n or fewer applications of the rules in the recursive step of a recursive definition we will have established that the principle of mathematical induction implies the principle of structural induction if we can show that p n is true whenever n is a positive integer in the basis step of a proof by structural induction we show that p is true that is we show that the result is true of all elements specified to be in the set in the basis step of the definition a consequence of the recursive step is that if we assume p k is true it follows that p k is true when we have completed a proof using structural induction we have shown that p is true and that p k implies p k by mathematical induction it follows that p n is true for all nonnegative integers n this also shows that the result is true for all elements generated by the recursive definition and shows that structural induction is a valid proof technique examples of proofs using structural induction structural induction can be used to prove that all members of a set constructed recursively have a particular property we will illustrate this idea by using structural induction to prove results about well formed formulae strings and binary trees for each proof we have to carry out the appropriate basis step and the appropriate recursive step for example to use structural induction to prove a result about the set of well formed formulae defined in example where we specify that t f and every propositional variable are well formed formulae and where we specify that if e and f are well formed formulae then e e f e f e f and e f are well formed formulae we need to complete this basis step and this recursive step basis step show that the result is true for t f and whenever is a propositional variable recursive step show that if the result is true for the compound propositions p and q it is also true for p p q p q p q and p q example illustrates how we can prove results about well formed formulae using structural induction example show that every well formed formula for compound propositions as defined in example contains an equal number of left and right parentheses solution basis step each of the formula t f and contains no parentheses so clearly they contain an equal number of left and right parentheses recursive step assume p and q are well formed formulae each containing an equal number of left and right parentheses that is if lp and lq are the number of left parentheses in p and q respectively and rp and rq are the number of right parentheses in p and q respec tively then lp rp and lq rq to complete the inductive step we need to show that each of p p q p q p q and p q also contains an equal number of left and right parentheses the number of left parentheses in the first of these compound propositions equals lp and in each of the other compound propositions equals lp lq similarly the number of right parentheses in the first of these compound propositions equals rp and in each of the other compound propositions equals rp rq because lp rp and lq rq it follows that each of these compound expressions contains the same number of left and right parentheses this completes the proof by structural induction suppose that p w is a propositional function over the set of strings w to use structural induction to prove that p w holds for all strings w we need to complete both a basis step and a recursive step these steps are basis step show that p λ is true recursive step assume that p w is true where w show that if x then p wx must also be true example illustrates how structural induction can be used in proofs about strings example use structural induction to prove that l xy l x l y where x and y belong to the set of strings over the alphabet solution we will base our proof on the recursive definition of the set given in definition and the definition of the length of a string in example which specifies that l λ and l wx l w when w and x let p y be the statement that l xy l x l y whenever x belongs to basis step to complete the basis step we must show that p λ is true that is we must show that l xλ l x l λ for all x because l xλ l x l x l x l λ for every string x it follows that p λ is true recursive step to complete the inductive step we assume that p y is true and show that this implies that p ya is true whenever a what we need to show is that l xya l x l ya for every a to show this note that by the recursive definition of l w given in example we have l xya l xy and l ya l y and by the inductive hypothesis l xy l x l y we conclude that l xya l x l y l x l ya we can prove results about trees or special classes of trees using structural induction for example to prove a result about full binary trees using structural induction we need to complete this basis step and this recursive step basis step show that the result is true for the tree consisting of a single vertex recursive step show that if the result is true for the trees and then it is true for tree consisting of a root r which has as its left subtree and as its right subtree before we provide an example showing how structural induction can be used to prove a result about full binary trees we need some definitions we will recursively define the height h t and the number of vertices n t of a full binary tree t we begin by defining the height of a full binary tree definition if we let n t denote the number of vertices in a full binary tree we observe that n t satisfies the following recursive formula basis step the number of vertices n t of the full binary tree t consisting of only a root r is n t recursive step if and are full binary trees then the number of vertices of the full binary tree t is n t n n we now show how structural induction can be used to prove a result about full binary trees theorem proof we prove this inequality using structural induction basis step for the full binary tree consisting of just the root r the result is true because n t and h t so that n t recursive step for the inductive hypothesis we assume that n and n whenever and are full binary trees by the recursive formulae for n t and h t we have n t n n and h t max h h we find that n t n n by the recursive formula for n t by the inductive hypothesis max because the sum of two terms is at most times the larger h h because max 2y x y t by the recursive definition of h t 2h t this completes the recursive step generalized induction we can extend mathematical induction to prove results about other sets that have the well ordering property besides the set of integers although we will discuss this concept in detail in section we provide an example here to illustrate the usefulness of such an approach as an example note that we can define an ordering on n n the ordered pairs of non negative integers by specifying that is less than or equal to y2 if either or and y2 this is called the lexicographic ordering the set n n with this ordering has the property that every subset of n n has a least element see exercise in section this implies that we can recursively define the terms am n with m n and n n and prove results about them using a variant of mathematical induction as illustrated in example example suppose that am n is defined recursively for m n n n by and am n am n if n and m am n n if n show that am n m n n for all m n n n that is for all pairs of nonnegative integers solution we can prove that am n m n n using a generalized version of mathematical induction the basis step requires that we show that this formula is valid when m n the induction step requires that we show that if the formula holds for all pairs smaller than m n in the lexicographic ordering of n n then it also holds for m n basis step let m n then by the basis case of the recursive definition of am n we have furthermore when m n m n n this completes the basis step inductive step suppose that ami ni mi ni ni whenever mi ni is less than m n in the lexicographic ordering of n n by the recursive definition if n then am n am n because m n is smaller than m n the inductive hypoth esis tells us that am n m n n so that am n m n n m n n giving us the desired equality now suppose that n so am n am n n because m n is smaller than m n the inductive hypothesis tells us that am n m n n so am n m n n n m n m n n this finishes the inductive step exercises as mentioned we will justify this proof technique in section find f f f and f if f n is defined recur sively by f and for n a f n f n b f n n c f n n d f n f n f n find f f f f and f if f n is defined recursively by f and for n a f n n b f n n c f n f n n d f n 3f n find f f f and f if f is defined recur sively by f f and for n a f n f n 3f n b f n f n f n c f n 3f n 4f n d f n f n f n find f f f and f if f is defined recur sively by f f and for n a f n f n f n b f n f n f n c f n f n f n d f n f n f n determine whether each of these proposed definitions is a valid recursive definition of a function f from the set of nonnegative integers to the set of integers if f is well defined find a formula for f n when n is a nonnegative integer and prove that your formula is valid a f f n n for n b f f n f n for n c f f f n f n for n d f f f n n for n e f f n 3f n if n is odd and n and f n n if n is even and n determine whether each of these proposed definitions is a valid recursive definition of a function f from the set of nonnegative integers to the set of integers if f is well defined find a formula for f n when n is a nonnegative integer and prove that your formula is valid a f f n f n for n b f f f f n n for n c f f f n n for n d f f f n 2f n for n e f f n f n if n is odd and n and f n 2f n if n give a recursive definition of the sequence an n if a an b an c an d an give a recursive definition of the sequence an n if a an b an n c an n n d an let f be the function such that f n is the sum of the first n positive integers give a recursive definition of f n give a recursive definition of sm n the sum of the inte ger m and the nonnegative integer n give a recursive definition of pm n the product of the integer m and the nonnegative integer n in exercises fn is the nth fibonacci number prove that f f f fnfn when n is a give a recursive definition of the set of positive integers that are multiples of give a recursive definition of a the set of odd positive integers b the set of positive integer powers of c the set of polynomials with integer coefficients give a recursive definition of a the set of even integers b the set of positive integers congruent to modulo c the set of positive integers not divisible by let s be the subset of the set of ordered pairs of integers defined recursively by basis step s recursive step if a b s then a b s and a b s n positive integer prove that f3 when n is a pos itive integer show that fn f n when n is a positive integer a list the elements of s produced by the first five ap plications of the recursive definition b use strong induction on the number of applications of the recursive step of the definition to show that a b when a b s show that 1f2n f is a positive integer when n c use structural induction to show that a b when a b s show that f2n when n is a positive integer determine the number of divisions used by the euclidean algorithm to find the greatest common divisor of the fi bonacci numbers fn and fn where n is a nonnegative integer verify your answer using mathematical induction let a show that an fn fn fn fn let s be the subset of the set of ordered pairs of integers defined recursively by basis step s recursive step if a b s then a b s a b s and a b s a list the elements of s produced by the first four ap plications of the recursive definition b use strong induction on the number of applications of the recursive step of the definition to show that a whenever a b s c use structural induction to show that a when ever a b s when n is a positive integer by taking determinants of both sides of the equation in exercise prove the identity given in exercise re a b c d ad bc give a recursive definition of the functions max and min so that max an and min an are the maximum and minimum of the n numbers an respectively let an and b2 bn be real numbers use the recursive definitions that you gave in exercise to prove these a max an min an b max b2 an bn max an max b2 bn min b2 an bn min an min b2 bn show that the set s defined by s and t s when ever s and t s is the set of positive integers give a recursive definition of each of these sets of ordered pairs of positive integers hint plot the points in the set in the plane and look for lines containing points in the set a s a b a z b z and a b is odd b s a b a z b z and a b c s a b a z b z and a b give a recursive definition of each of these sets of or dered pairs of positive integers use structural induction to prove that the recursive definition you found is correct hint to find a recursive definition plot the points in the set in the plane and look for patterns a s a b a z b z and a b is even b s a b a z b z and a or b is odd c s a b a z b z a b is odd and b prove that in a bit string the string occurs at most one more time than the string define well formed formulae of sets variables represent ing sets and operators from a give a recursive definition of the function ones which counts the number of ones in a bit string b use structural induction to prove that ones st ones ones t a give a recursive definition of the function m which equals the smallest digit in a nonempty string of dec imal digits b use structural induction to prove that m st min m m t the reversal of a string is the string consisting of the symbols of the string in reverse order the reversal of the string w is denoted by wr find the reversal of the following bit strings a 0101 b 1011 c 1001 0111 give a recursive definition of the reversal of a string hint first define the reversal of the empty string then write a string w of length n as xy where x is a string of length n and express the reversal of w in terms of xr and y use structural induction to prove that r wrwr use generalized induction as was done in example to show that if am n is defined recursively by and am n if n and m am n if n then am n m n for all m n n n use generalized induction as was done in example to show that if am n is defined recursively by and am n if n and m am n if n then am n m n for all m n z z a partition of a positive integer n is a way to write n as a sum of positive integers where the order of terms in the sum does not matter for instance is a partition of let pm equal the number of different partitions of m and let pm n be the number of different ways to express m as the sum of positive integers not exceeding n a show that pm m pm give a recursive definition of wi where w is a string and i is a nonnegative integer here wi represents the con catenation of i copies of the string w give a recursive definition of the set of bit strings that are b show that the following recursive definition for pm n is correct if m if n palindromes when does a string belong to the set a of bit strings de fined recursively by pm n pm m if m n pm m if m n pm n pm n n if m n λ a a if x a where λ is the empty string recursively define the set of bit strings that have more zeros than ones use exercise and mathematical induction to show that l wi i l w where w is a string and i is a nonnegative integer show that wr i wi r whenever w is a string and i is a nonnegative integer that is show that the ith power of the reversal of a string is the reversal of the ith power of the string use structural induction to show that n t 2h t c find the number of partitions of and of using this recursive definition consider an inductive definition of a version of ackermann function this function was named after wilhelm ackermann a german mathematician who was a student of the great math ematician david hilbert ackermann function plays an im portant role in the theory of recursive functions and in the study of the complexity of certain algorithms involving set unions there are several different variants of this function all are called ackermann function and have similar properties even though their values do not always agree if m if m and n where t is a full binary tree n t equals the number of vertices of t and h t is the height of t the set of leaves and the set of internal vertices of a full binary a m n if m and n a m a m n if m and n tree can be defined recursively basis step the root r is a leaf of the full binary tree with exactly one vertex r this tree has no internal vertices recursive step the set of leaves of the tree t is the union of the sets of leaves of and of the inter nal vertices of t are the root r of t and the union of the set of internal vertices of and the set of internal vertices of use structural induction to show that l t the number of leaves of a full binary tree t is more than i t the number of internal vertices of t exercises involve this version of ackermann func tion find these values of ackermann function a a b a c a d a show that a m whenever m show that a n whenever n find these values of ackermann function a a b a find a prove that a m n a m n whenever m and n are nonnegative integers log k n if k log log k n if log k n is defined prove that a m n a m n whenever m and n are n and positive nonnegative integers prove that a i j j whenever i and j are nonnegative undefined otherwise use mathematical induction to prove that a function f defined by specifying f and a rule for obtaining f n from f n is well defined use strong induction to prove that a function f defined by specifying f and a rule for obtaining f n from the values f k for k n is well defined show that each of these proposed recursive definitions of a function on the set of positive integers does not produce a well defined function a f n f n for n and f b f n f n for n f and f f n f n for n f and f f n f n if n is even and n f n f n if n is odd and f f n f n if n is even and n f n f if n is odd and n and f is the smallest nonnegative integer k such that log k n find these values a log b log 256 c log 265536 d log 2265536 find the value of log n for these values of n a b c d e 256 f g find the largest integer n such that log n determine the number of decimal digits in this number exercises deal with values of iterated functions sup pose that f n is a function from the set of real numbers or positive real numbers or some other set of real numbers to the set of real numbers such that f n is monotonically in creasing that is f n f m when n m and f n n for all n in the domain of f the function f k n is defined recursively by n if k show that each of these proposed recursive definitions of a function on the set of positive integers does not produce f k n f f k n if k a well defined function a f n f n for n and b f furthermore let c be a positive real number the iterated function f c is the number of iterations of f required to reduce its argument to c or less so f c n is the smallest nonnegative f n f n for n and f c f n f n for n f f and f f n f n if n is even and n f n f n if n is odd and f integer k such that f k n c let f n n a where a is a positive integer find a formula for f k n what is the value of f n when n is a positive integer let f n n find a formula for f k n what is the f n f f n if n and f exercises deal with iterations of the logarithm function let log n denote the logarithm of n to the base as usual the function log k n is defined recursively by value of f n when n is a positive integer let f n n find a formula for f k n what is the value of f n when n is a positive integer recursive algorithms introduction here a famous humorous quote to understand recursion you must first understand recursion sometimes we can reduce the solution to a problem with a particular set of input values to the solution of the same problem with smaller input values for instance the problem of finding the greatest common divisor of two positive integers a and b where b a can be reduced to finding the greatest common divisor of a pair of smaller integers namely b mod a and a because gcd b mod a a gcd a b when such a reduction can be done the solution to the original problem can be found with a sequence of reductions until the problem has been reduced to some initial case for which the solution is known for instance for finding the greatest common divisor the reduction continues until the smaller of the two numbers is zero because gcd a a when a we will see that algorithms that successively reduce a problem to the same problem with smaller input are used to solve a wide variety of problems definition we will describe a variety of different recursive algorithms in this section example give a recursive algorithm for computing n where n is a nonnegative integer solution we can build a recursive algorithm that finds n where n is a nonnegative integer based on the recursive definition of n which specifies that n n n when n is a positive integer and that to find n for a particular integer we use the recursive step n times each time replacing a value of the factorial function with the value of the factorial function at the next smaller integer at this last step we insert the value of the recursive algorithm we obtain is displayed as algorithm to help understand how this algorithm works we trace the steps used by the algorithm to compute first we use the recursive step to write we then use the recursive step repeatedly to write and inserting the value of and working back through the steps we see that and example shows how a recursive algorithm can be constructed to evaluate a function from its recursive definition example give a recursive algorithm for computing an where a is a nonzero real number and n is a nonnegative integer solution we can base a recursive algorithm on the recursive definition of an this definition states that an a an for n and the initial condition to find an successively use the recursive step to reduce the exponent until it becomes zero we give this procedure in algorithm next we give a recursive algorithm for finding greatest common divisors example give a recursive algorithm for computing the greatest common divisor of two nonnegative integers a and b with a b solution we can base a recursive algorithm on the reduction gcd a b gcd b mod a a and the condition gcd b b when b this produces the procedure in algorithm which is a recursive version of the euclidean algorithm we illustrate the workings of algorithm with a trace when the input is a b with this input the algorithm uses the else clause to find that gcd gcd mod gcd it uses this clause again to find that gcd gcd mod gcd then to get gcd gcd mod gcd then to get gcd gcd mod gcd finally to find gcd it uses the first step with a to find that gcd consequently the algorithm finds that gcd example devise a recursive algorithm for computing bn mod m where b n and m are integers with m n and b m solution we can base a recursive algorithm on the fact that bn mod m b bn mod m mod m which follows by corollary in section and the initial condition b0 mod m we leave this as exercise for the reader however we can devise a much more efficient recursive algorithm based on the observation that bn mod m bn mod m mod m when n is even and bn mod m b n mod m mod m b mod m mod m when n is odd which we describe in pseudocode as algorithm we trace the execution of algorithm with input b n and m to illustrate how it works first because n is odd we use the else clause to see that mpower mpower mod mod mod we next use the else if clause to see that mpower mpower mod using the else clause again we see that mpower mpower mod mod mod finally using the if clause we see that mpower working backwards it follows that mpower mod mod mod so mpower mod and finally mpower mod mod mod we will now give recursive versions of searching algorithms that were introduced in section example express the linear search algorithm as a recursive procedure solution to search for the first occurrence of x in the sequence an at the ith step of the algorithm x and ai are compared if x equals ai then the algorithm returns i the location of x in the sequence otherwise the search for the first occurrence of x is reduced to a search in a sequence with one fewer element namely the sequence ai an the algorithm returns when x is never found in the sequence after all terms have been examined we can now give a recursive procedure which is displayed as pseudocode in algorithm let search i j x be the procedure that searches for the first occurrence of x in the sequence ai ai aj the input to the procedure consists of the triple n x the algorithm terminates at a step if the first term of the remaining sequence is x or if there is only one term of the sequence and this is not x if x is not the first term and there are additional terms the same procedure is carried out but with a search sequence of one fewer term obtained by deleting the first term of the search sequence if the algorithm terminates without x having been found the algorithm returns the value example construct a recursive version of a binary search algorithm solution suppose we want to locate x in the sequence an of integers in increasing order to perform a binary search we begin by comparing x with the middle term a n our algorithm will terminate if x equals this term and return the location of this term in the sequence otherwise we reduce the search to a smaller search sequence namely the first half of the sequence if x is smaller than the middle term of the original sequence and the second half otherwise we have reduced the solution of the search problem to the solution of the same problem with a sequence at most half as long if have we never encountered the search term x our algorithm returns the value we express this recursive version of a binary search algorithm as algorithm proving recursive algorithms correct mathematical induction and its variant strong induction can be used to prove that a recursive algorithm is correct that is that it produces the desired output for all possible input values examples and illustrate how mathematical induction or strong induction can be used to prove that recursive algorithms are correct first we will show that algorithm is correct example prove that algorithm which computes powers of real numbers is correct solution we use mathematical induction on the exponent n basis step if n the first step of the algorithm tells us that power a this is correct because for every nonzero real number a this completes the basis step inductive step the inductive hypothesis is the statement that power a k ak for all a for an arbitrary nonnegative integer k that is the inductive hypothesis is the statement that the algorithm correctly computes ak to complete the inductive step we show that if the inductive hypothesis is true then the algorithm correctly computes ak because k is a positive integer when the algorithm computes ak the algorithm sets power a k a power a k by the inductive hypothesis we have power a k ak so power a k a power a k a ak ak this completes the inductive step we have completed the basis step and the inductive step so we can conclude that algorithm always computes an correctly when a and n is a nonnegative integer generally we need to use strong induction to prove that recursive algorithms are correct rather than just mathematical induction example illustrates this it shows how strong induction can be used to prove that algorithm is correct example prove that algorithm which computes modular powers is correct solution we use strong induction on the exponent n basis step let b be an integer and m an integer with m when n the algorithm sets mpower b n m equal to this is correct because b0 mod m the basis step is complete inductive step for the inductive hypothesis we assume that mpower b j m bj mod m for all integers j k whenever b is a positive integer and m is an integer with m to complete the inductive step we show that if the inductive hypothesis is correct then mpower b k m bk mod m because the recursive algorithm handles odd and even values of k differently we split the inductive step into two cases when k is even we have mpower b k m mpower b k m mod m bk mod m mod m bk mod m where we have used the inductive hypothesis to replace mpower b k m by bk mod m when k is odd we have mpower b k m mpower b k m mod m b mod m mod m b k mod m mod m b mod m mod m b2 k mod m bk mod m using corollary in section because k k k when k is odd here we have used the inductive hypothesis to replace mpower b k m by b k mod m this completes the inductive step we have completed the basis step and the inductive step so by strong induction we know that algorithm is correct recursion and iteration a recursive definition expresses the value of a function at a positive integer in terms of the values of the function at smaller integers this means that we can devise a recursive algorithm to evaluate a recursively defined function at a positive integer instead of successively reducing the computation to the evaluation of the function at smaller integers we can start with the value of the function at one or more integers the base cases and successively apply the recursive definition to find the values of the function at successive larger integers such a procedure is called iterative often an iterative approach for the evaluation of a recursively defined sequence requires much less computation than a procedure using recursion unless special purpose recursive machines are used this is illustrated by the iterative and recursive procedures for finding the nth fibonacci number the recursive procedure is given first when we use a recursive procedure to find fn we first express fn as fn fn then we replace both of these fibonacci numbers by the sum of two previous fibonacci numbers and so on when or arises it is replaced by its value note that at each stage of the recursion until or is obtained the number of fibonacci numbers to be evaluated has doubled for instance when we find using this recursive algo rithm we must carry out all the computations illustrated in the tree diagram in figure this figure evaluating recursively tree consists of a root labeled with and branches from the root to vertices labeled with the two fibonacci numbers f3 and that occur in the reduction of the computation of each subsequent reduction produces two branches in the tree this branching ends when and are reached the reader can verify that this algorithm requires fn additions to find fn now consider the amount of computation required to find fn using the iterative approach in algorithm this procedure initializes x as and y as when the loop is traversed the sum of x and y is assigned to the auxiliary variable z then x is assigned the value of y and y is assigned the value of the auxiliary variable z therefore after going through the loop the first time it follows that x equals and y equals furthermore after going through the loop n times x equals fn and y equals fn the reader should verify this statement only n additions have been used to find fn with this iterative approach when n consequently this algorithm requires far less computation than does the recursive algorithm we have shown that a recursive algorithm may require far more computation than an iterative one when a recursively defined function is evaluated it is sometimes preferable to use a recursive procedure even if it is less efficient than the iterative procedure in particular this is true when the recursive approach is easily implemented and the iterative approach is not also machines designed to handle recursion may be available that eliminate the advantage of using iteration figure the merge sort of the merge sort we now describe a recursive sorting algorithm called the merge sort algorithm we will demon strate how the merge sort algorithm works with an example before describing it in generality example use the merge sort to put the terms of the list in increasing order solution a merge sort begins by splitting the list into individual elements by successively splitting lists in two the progression of sublists for this example is represented with the balanced binary tree of height shown in the upper half of figure sorting is done by successively merging pairs of lists at the first stage pairs of individual elements are merged into lists of length two in increasing order then successive merges of pairs of lists are performed until the entire list is put into increasing order the succession of merged lists in increasing order is represented by the balanced binary tree of height shown in the lower half of figure note that this tree is displayed upside down in general a merge sort proceeds by iteratively splitting lists into two sublists of equal length or where one sublist has one more element than the other until each sublist contains one element this succession of sublists can be represented by a balanced binary tree the procedure continues by successively merging pairs of lists where both lists are in increasing order into a larger list with elements in increasing order until the original list is put into increasing order the succession of merged lists can be represented by a balanced binary tree we can also describe the merge sort recursively to do a merge sort we split a list into two sublists of equal or approximately equal size sorting each sublist using the merge sort algorithm and then merging the two lists the recursive version of the merge sort is given in algorithm this algorithm uses the subroutine merge which is described in algorithm an efficient algorithm for merging two ordered lists into a larger ordered list is needed to implement the merge sort we will now describe such a procedure example merge the two lists and solution table illustrates the steps we use first compare the smallest elements in the two lists and respectively because is the smaller put it at the beginning of the merged list and remove it from the second list at this stage the first list is the second is and the combined list is next compare and the smallest elements of the two lists because is the smaller add it to the combined list and remove it from the first list at this stage the first list is the second is and the combined list is continue by comparing and the smallest elements of their respective lists because is the smaller of these two elements add it to the combined list and remove it from the first list at this stage the first list is and the second is the combined list is then compare and the smallest elements in the two lists because is the smaller of these two elements add it to the combined list and remove it from the second list at this stage the first list is the second list is empty and the combined list is finally because the second list is empty all elements of the first list can be appended to the end of the combined list in the order they occur in the first list this produces the ordered list we will now consider the general problem of merging two ordered lists and into an ordered list l we will describe an algorithm for solving this problem start with an empty list l compare the smallest elements of the two lists put the smaller of these two elements at the right end of l and remove it from the list it was in next if one of and is empty append the other nonempty list to l which completes the merging if neither nor is empty repeat this process algorithm gives a pseudocode description of this procedure table merging the two sorted lists and first list second list merged list comparison 356 we will need estimates for the number of comparisons used to merge two ordered lists in the analysis of the merge sort we can easily obtain such an estimate for algorithm each time a comparison of an element from and an element from is made an additional element is added to the merged list l however when either or is empty no more comparisons are needed hence algorithm is least efficient when m n comparisons are carried out where m and n are the number of elements in and respectively leaving one element in each of and the next comparison will be the last one needed because it will make one of these lists empty hence algorithm uses no more than m n comparisons lemma summarizes this estimate lemma sometimes two sorted lists of length m and n can be merged using far fewer than m n comparisons for instance when m a binary search procedure can be applied to put the one element in the first list into the second list this requires only log n comparisons which is much smaller than m n n for m on the other hand for some values of m and n lemma gives the best possible bound that is there are lists with m and n elements that cannot be merged using fewer than m n comparisons see exercise we can now analyze the complexity of the merge sort instead of studying the general problem we will assume that n the number of elements in the list is a power of say this will make the analysis less complicated but when this is not the case various modifications can be applied that will yield the same estimate at the first stage of the splitting procedure the list is split into two sublists of elements each at level of the tree generated by the splitting this process continues splitting the two sublists with elements into four sublists of elements each at level and so on in general there are 2k lists at level k each with k elements these lists at level k are split into 2k lists at level k each with k elements at the end of this process we have lists each with one element at level m we start merging by combining pairs of the lists of one element into lists at level m each with two elements to do this pairs of lists with one element each are merged the merger of each pair requires exactly one comparison the procedure continues so that at level k k m m m 2k lists each with k elements are merged into 2k lists each with k elements at level k to do this a total of 2k mergers of two lists each with k elements are needed but by lemma each of these mergers can be carried out using at most k k k comparisons hence going from level k to k can be accomplished using at most 2k k comparisons summing all these estimates shows that the number of comparisons required for the merge sort is at most m m m 2k k 2k n log n n because m log n and n we evaluated m by noting that it is the sum of m identical terms each equal to we evaluated k using the formula for the sum of the terms of a geometric progression from theorem of section theorem summarizes what we have discovered about the worst case complexity of the merge sort algorithm theorem in chapter we will show that the fastest comparison based sorting algorithm have o n log n time complexity a comparison based sorting algorithm has the comparison of two elements as its basic operation theorem tells us that the merge sort achieves this best possible big o estimate for the complexity of a sorting algorithm we describe another efficient algorithm the quick sort in the preamble to exercise exercises trace algorithm when it is given n as input that is show all steps used by algorithm to find as is done in example to find trace algorithm when it is given n as input that is show all steps used by algorithm to find as is done in example to find trace algorithm when it finds gcd that is show all the steps used by algorithm to find gcd trace algorithm when it finds gcd that is show all the steps used by algorithm to find gcd trace algorithm when it is given m n and b as input that is show all the steps algorithm give a recursive algorithm for finding the maximum of a finite set of integers making use of the fact that the maximum of n integers is the larger of the last integer in the list and the maximum of the first n integers in the list give a recursive algorithm for finding the minimum of a finite set of integers making use of the fact that the min imum of n integers is the smaller of the last integer in the list and the minimum of the first n integers in the list devise a recursive algorithm for finding xn mod m when ever n x and m are positive integers based on the fact that xn mod m xn mod m x mod m mod m give a recursive algorithm for finding n mod m when uses to find ever n and m are positive integers trace algorithm when it is given m n and b as input that is show all the steps algorithm uses to find give a recursive algorithm for computing nx whenever n is a positive integer and x is an integer using just addition give a recursive algorithm for finding the sum of the first n positive integers give a recursive algorithm for finding the sum of the first n odd positive integers give a recursive algorithm for finding a mode of a list of integers a mode is an element in the list that occurs at least as often as every other element devise a recursive algorithm for computing the greatest common divisor of two nonnegative integers a and b with a b using the fact that gcd a b gcd a b a prove that the recursive algorithm for finding the sum of the first n positive integers you found in exercise is correct describe a recursive algorithm for multiplying two non negative integers x and y based on the fact that xy x y when y is even and xy x y x when y is odd together with the initial condition xy when y prove that algorithm for computing n when n is a non negative integer is correct prove that algorithm for computing gcd a b when a and b are positive integers with a b is correct prove that the algorithm you devised in exercise is correct prove that the recursive algorithm that you found in ex ercise is correct prove that the recursive algorithm that you found in ex ercise is correct devise a recursive algorithm for computing where n is a nonnegative integer using the fact that n then prove that this algorithm is correct devise a recursive algorithm to find where a is a real number and n is a positive integer hint use the equality how does the number of multiplications used by the al gorithm in exercise compare to the number of multi plications used by algorithm to evaluate use the algorithm in exercise to devise an algo rithm for evaluating an when n is a nonnegative integer hint use the binary expansion of n how does the number of multiplications used by the al gorithm in exercise compare to the number of multi plications used by algorithm to evaluate an how many additions are used by the recursive and itera tive algorithms given in algorithms and respectively to find the fibonacci number devise a recursive algorithm to find the nth term of the se quence defined by and an an an give a recursive algorithm for finding the string wi the concatenation of i copies of w when w is a bit string prove that the recursive algorithm for finding the reversal of a bit string that you gave in exercise is correct prove that the recursive algorithm for finding the concate nation of i copies of a bit string that you gave in exercise is correct give a recursive algorithm for tiling a checker board with one square missing using right triominoes give a recursive algorithm for triangulating a simple poly gon with n sides using lemma in section give a recursive algorithm for computing values of the ackermann function hint see the preamble to exer cise in section use a merge sort to sort into increasing order show all the steps used by the algorithm use a merge sort to sort b d a f g h z p o k into al phabetic order show all the steps used by the algorithm how many comparisons are required to merge these pairs of lists using algorithm a b c show that for all positive integers m and n there are sorted lists with m elements and n elements respectively such that algorithm uses m n comparisons to merge them into one sorted list what is the least number of comparisons needed to merge any two lists in increasing order into one list in increasing order when the number of elements in the two lists are a b c d prove that the merge sort algorithm is correct the quick sort is an efficient algorithm to sort a this algorithm begins by taking the first for n devise an iterative algorithm to find the nth term of the element n and forming two sublists the first contain sequence defined in exercise is the recursive or the iterative algorithm for finding the sequence in exercise more efficient devise a recursive algorithm to find the nth term of the sequence defined by a0 and an an an an for n devise an iterative algorithm to find the nth term of the sequence defined in exercise is the recursive or the iterative algorithm for finding the sequence in exercise more efficient give iterative and recursive algorithms for finding the nth term of the sequence defined by a0 ing those elements that are less than in the order they arise and the second containing those elements greater than in the order they arise then is put at the end of the first sublist this procedure is repeated recursively for each sublist until all sublists contain one item the or dered list of n items is obtained by combining the sublists of one item in the order they occur sort using the quick sort let an be a list of n distinct real numbers how many comparisons are needed to form two sublists from this list the first containing elements less than and the second containing elements greater than describe the quick sort algorithm using pseudocode and an an n which is more efficient what is the largest number of comparisons needed to or give a recursive algorithm to find the number of parti tions of a positive integer based on the recursive definition given in exercise in section give a recursive algorithm for finding the reversal of a bit string see the definition of the reversal of a bit string in the preamble of exercise in section der a list of four elements using the quick sort algorithm what is the least number of comparisons needed to order a list of four elements using the quick sort algorithm determine the worst case complexity of the quick sort algorithm in terms of the number of comparisons used program correctness introduction suppose that we have designed an algorithm to solve a problem and have written a program to implement it how can we be sure that the program always produces the correct answer after all the bugs have been removed so that the syntax is correct we can test the program with sample input it is not correct if an incorrect result is produced for any sample input but even if the program gives the correct answer for all sample input it may not always produce the correct answer unless all possible input has been tested we need a proof to show that the program always gives the correct output program verification the proof of correctness of programs uses the rules of inference and proof techniques described in this chapter including mathematical induction because an incor rect program can lead to disastrous results a large amount of methodology has been constructed for verifying programs efforts have been devoted to automating program verification so that it can be carried out using a computer however only limited progress has been made toward this goal indeed some mathematicians and theoretical computer scientists argue that it will never be realistic to mechanize the proof of correctness of complex programs some of the concepts and methods used to prove that programs are correct will be introduced in this section many different methods have been devised for proving that programs are correct we will discuss a widely used method for program verification introduced by tony hoare in this section several other methods are also commonly used furthermore we will not develop a complete methodology for program verification in this book this section is meant to be a brief introduction to the area of program verification which ties together the rules of logic proof techniques and the concept of an algorithm program verification a program is said to be correct if it produces the correct output for every possible input a proof that a program is correct consists of two parts the first part shows that the correct answer is obtained if the program terminates this part of the proof establishes the partial correctness of the program the second part of the proof shows that the program always terminates to specify what it means for a program to produce the correct output two propositions are used the first is the initial assertion which gives the properties that the input values must have the second is the final assertion which gives the properties that the output of the program should have if the program did what was intended the appropriate initial and final assertions must be provided when a program is checked definition note the notation p s q is known as a hoare triple tony hoare introduced the concept of partial correctness note that the notion of partial correctness has nothing to do with whether a program termi nates it focuses only on whether the program does what it is expected to do if it terminates a simple example illustrates the concepts of initial and final assertions example show that the program segment y z x y is correct with respect to the initial assertion p x and the final assertion q z solution suppose that p is true so that x as the program begins then y is assigned the value and z is assigned the sum of the values of x and y which is hence s is correct with respect to the initial assertion p and the final assertion q thus p s q is true rules of inference a useful rule of inference proves that a program is correct by splitting the program into a sequence of subprograms and then showing that each subprogram is correct suppose that the program s is split into subprograms and write s to indicate that s is made up of followed by suppose that the correctness of with respect to the initial assertion p and final assertion q and the correctness of with respect to the initial assertion q and the final assertion r have been established it follows that if p is true and is executed and terminates then q is true and if q is true and executes and terminates then r is true thus if p is true and s is executed and terminates then r is true this rule of inference called the composition rule can be stated as p q q r p r this rule of inference will be used later in this section next some rules of inference for program segments involving conditional statements and loops will be given because programs can be split into segments for proofs of correctness this will let us verify many different programs conditional statements first rules of inference for conditional statements will be given suppose that a program segment has the form where s is a block of statements then s is executed if condition is true and it is not executed when condition is false to verify that this segment is correct with respect to the initial assertion p and final assertion q two things must be done first it must be shown that when p is true and condition is also true then q is true after s terminates second it must be shown that when p is true and condition is false then q is true because in this case s does not execute this leads to the following rule of inference p condition s q p condition q p if condition then s q example illustrates how this rule of inference is used example verify that the program segment is correct with respect to the initial assertion t and the final assertion y x solution when the initial assertion is true and x y the assignment y x is carried out hence the final assertion which asserts that y x is true in this case moreover when the initial assertion is true and x y is false so that x y the final assertion is again true hence using the rule of inference for program segments of this type this program is correct with respect to the given initial and final assertions similarly suppose that a program has a statement of the form if condition is true then executes if condition is false then executes to verify that this program segment is correct with respect to the initial assertion p and the final assertion q two things must be done first it must be shown that when p is true and condition is true then q is true after terminates second it must be shown that when p is true and condition is false then q is true after terminates this leads to the following rule of inference p condition q p condition q p if condition then else q c anthony r hoare born tony hoare was born in colombo ceylon now known as sri lanka where his father was a civil servant of the british empire and his mother father owned a plantation he spent his early childhood in ceylon moving to england in hoare studied philosophy together with the classics at the university of oxford where he became interested in computing as a result of his fascination with the power of mathematical logic and the certainty of mathematical truth he received his bachelors degree from oxford in hoare learned russian during his service in the royal navy and latter studied the computer translation of natural languages at moscow state university he returned to england in taking a job at a small computer manufacturer where he wrote a compiler for the programming language algol in he became professor of computing science at the queen university belfast in he moved to the university of oxford as professor of computing he is now professor emeritus he is a fellow of the royal society and also holds a position at microsoft research in cambridge hoare has made many contributions to the theory of programming languages and to programming methodology he was first to define a programming language based on how programs could be proved correct with respect to their specifications hoare also invented quick sort one of the most commonly used sorting algorithms see the preamble to exercise in section he received the acm turing award in and in he was knighted for services to education and computer science hoare is a noted writer in the technical and social aspects of computer science example illustrates how this rule of inference is used example verify that the program segment is correct with respect to the initial assertion t and the final assertion abs x solution two things must be demonstrated first it must be shown that if the initial assertion is true and x then abs x this is correct because when x the assignment statement abs x sets abs x which is x by definition when x second it must be shown that if the initial assertion is true and x is false so that x then abs x this is also correct because in this case the program uses the assignment statement abs x and x is x by definition when x so abs x hence using the rule of inference for program segments of this type this segment is correct with respect to the given initial and final assertions loop invariants next proofs of correctness of while loops will be described to develop a rule of inference for program segments of the type note that s is repeatedly executed until condition becomes false an assertion that remains true each time s is executed must be chosen such an assertion is called a loop invariant in other words p is a loop invariant if p condition s p is true suppose that p is a loop invariant it follows that if p is true before the program segment is executed p and condition are true after termination if it occurs this rule of inference is p condition s p p while condition s condition p the use of a loop invariant is illustrated in example example a loop invariant is needed to verify that the program segment terminates with factorial n when n is a positive integer let p be the assertion factorial i and i n we first prove that p is a loop invariant suppose that at the beginning of one execution of the while loop p is true and the condition of the while loop holds in other words assume that factorial i and that i n the new values inew and factorialnew of i and factorial are inew i and factorialnew factorial i i inew because i n we also have inew i n thus p is true at the end of the execution of the loop this shows that p is a loop invariant now we consider the program segment just before entering the loop i n and factorial i both hold so p is true because p is a loop invariant the rule of infer ence just introduced implied that if the while loop terminates it terminates with p true and with i n false in this case at the end factorial i and i n are true but i n is false in other words i n and factorial i n as desired finally we need to check that the while loop actually terminates at the beginning of the program i is assigned the value so after n traversals of the loop the new value of i will be n and the loop terminates at that point a final example will be given to show how the various rules of inference can be used to verify the correctness of a longer program example we will outline how to verify the correctness of the program s for computing the product of two integers the goal is to prove that after s is executed product has the value mn the proof of correctness can be carried out by splitting s into four segments with s as shown in the listing of s the rule of composition can be used to build the correctness proof here is how the argument proceeds the details will be left as an exercise for the reader let p be the initial assertion m and n are integers then it can be shown that p q is true when q is the proposition p a n next let r be the proposition q k x it is easily verified that q r is true it can be shown that x mk and k a is an invariant for the loop in furthermore it is easy to see that the loop terminates after a iterations with k a so x ma at this point because r implies that x m and a the loop invariant is true before the loop is entered because the loop terminates with k a it follows that r is true where is the proposition x ma and a n finally it can be shown that is correct with respect to the initial assertion and final assertion t where t is the proposition product mn putting all this together because p q q r r and t are all true it fol lows from the rule of composition that p s t is true furthermore because all four segments terminate s does terminate this verifies the correctness of the program review questions exercises prove that the program segment y z x y is correct with respect to the initial assertion x and the final assertion z verify that the program segment if x then x is correct with respect to the initial assertion t and the final assertion x verify that the program segment x z x y if y then z z else z is correct with respect to the initial assertion y and the final assertion z verify that the program segment if x y then min x else min y is correct with respect to the initial assertion t and the final assertion x y min x x y min y devise a rule of inference for verification of partial cor rectness of statements of the form if condition then else if condition then else sn where sn are blocks use the rule of inference developed in exercise to verify that the program key terms and results terms sequence a function with domain that is a subset of the set of integers geometric progression a sequence of the form a ar where a and r are real numbers arithmetic progression a sequence of the form a a d a where a and d are real numbers if x then y x x else if x then y x x else if x then y is correct with respect to the initial assertion t and the final assertion y use a loop invariant to prove that the following program segment for computing the nth power where n is a posi tive integer of a real number x is correct power i while i n power power x i i prove that the iterative program for finding fn given in section is correct provide all the details in the proof of correctness given in example suppose that both the conditional statement and the program assertion s q are true show that s q also must be true suppose that both the program assertion p s and the conditional statement are true show that p s also must be true this program computes quotients and remainders r a q while r d r r d q q verify that it is partially correct with respect to the ini tial assertion a and d are positive integers and the final assertion q and r are integers such that a dq r and r d use a loop invariant to verify that the euclidean algorithm algorithm in section is partially correct with re spect to the initial assertion a and b are positive integers and the final assertion x gcd a b the principle of mathematical induction the statement n p n is true if p is true and k p k p k is true basis step the proof of p in a proof by mathematical in duction of np n inductive step the proof of p k p k for all pos itive integers k in a proof by mathematical induction of np n strong induction the statement np n is true if p is true and k p p k p k is true well ordering property every nonempty set of nonnegative integers has a least element recursive definition of a function a definition of a function that specifies an initial set of values and a rule for obtaining values of this function at integers from its values at smaller integers recursive definition of a set a definition of a set that specifies an initial set of elements in the set and a rule for obtaining other elements from those in the set structural induction a technique for proving results about recursively defined sets recursive algorithm an algorithm that proceeds by reducing a problem to the same problem with smaller input merge sort a sorting algorithm that sorts a list by splitting it in two sorting each of the two resulting lists and merging the results into a sorted list iteration a procedure based on the repeated use of operations in a loop program correctness verification that a procedure always produces the correct result loop invariant a property that remains true during every traversal of a loop initial assertion the statement specifying the properties of the input values of a program final assertion the statement specifying the properties the out put values should have if the program worked correctly review questions a can you use the principle of mathematical induction to find a formula for the sum of the first n terms of a sequence b can you use the principle of mathematical induction to determine whether a given formula for the sum of the first n terms of a sequence is correct c find a formula for the sum of the first n even positive integers and prove it using mathematical induction a for which positive integers n is b prove the conjecture you made in part a using math ematical induction a which amounts of postage can be formed using only cent and cent stamps b prove the conjecture you made using mathematical induction c prove the conjecture you made using strong induction d find a proof of your conjecture different from the ones you gave in b and c give two different examples of proofs that use strong in duction a state the well ordering property for the set of positive integers b use this property to show that every positive inte ger greater than one can be written as the product of primes a explain why a function f from the set of positive in tegers to the set of real numbers is well defined if it is defined recursively by specifying f and a rule for finding f n from f n b provide a recursive definition of the function f n n a give a recursive definition of the fibonacci numbers b show that fn αn whenever n where fn is the nth term of the fibonacci sequence and α a explain why a sequence an is well defined if it is de fined recursively by specifying and and a rule for finding an from an for n b find the value of an if and an an an for n give two examples of how well formed formulae are de fined recursively for different sets of elements and oper ators a give a recursive definition of the length of a string b use the recursive definition from part a and struc tural induction to prove that l xy l x l y a what is a recursive algorithm b describe a recursive algorithm for computing the sum of n numbers in a sequence describe a recursive algorithm for computing the greatest common divisor of two positive integers a describe the merge sort algorithm b use the merge sort algorithm to put the list in increasing order c give a big o estimate for the number of comparisons used by the merge sort a does testing a computer program to see whether it produces the correct output for certain input values verify that the program always produces the correct output b does showing that a computer program is partially correct with respect to an initial assertion and a final assertion verify that the program always produces the correct output if not what else is needed what techniques can you use to show that a long computer program is partially correct with respect to an initial as sertion and a final assertion what is a loop invariant how is a loop invariant used supplementary exercises use mathematical induction to show that uct rule and the fact that f i x ex to prove that whenever n g n x x n ex whenever n is a positive integer use mathematical induction to show that n whenever n is a positive integer use mathematical induction to show that n n whenever n is a positive integer use mathematical induction to show that requires calculus suppose that f x ex and g x ecx where c is a constant use mathematical induction to gether with the chain rule and the fact that f i x ex to prove that g n cnecx whenever n is a positive integer formulate a conjecture about which fibonacci numbers are even and use a form of mathematical induction to prove your conjecture n determine which fibonacci numbers are divisible by whenever n is a positive integer show that n whenever n is a positive integer use mathematical induction to show that n whenever n is an integer greater than use mathematical induction to show that when ever n is an integer greater than find an integer such that whenever is greater use a form of mathematical induction to prove your con jecture prove that fkfn fk fn k for all nonnega tive integers n and k where fi denotes the ith fibonacci number recall from example of section that the sequence of lucas numbers is defined by and ln ln ln for n show that fn fn ln whenever n is a positive in teger where fi and li are the ith fibonacci number and ith lucas number respectively show that lnln whenever n is than n prove that your result is correct using mathemat ical induction use mathematical induction to prove that a b is a factor of an bn whenever n is a positive integer use mathematical induction to prove that divides n3 n n whenever n is a nonnegative integer use mathematical induction to prove that divides a nonnegative integer and li is the ith lucas number use mathematical induction to show that the product of any n consecutive positive integers is divisible by n hint use the identity m m m n n m m m m n n m m m n n use mathematical induction to show that cos x for every positive integer n i sin x n cos nx i sin nx whenever n is a positive in teger here i is the square root of hint use use mathematical induction to prove that divides for every positive integer n the identities cos a b cos a cos b sin a sin b and sin a b sin a cos b cos a sin b use mathematical induction to prove this formula for the use mathematical induction to show that n cos jx sum of the terms of an arithmetic progression a a d a nd cos n x sin nx sin x positive integer and sin x j whenever n is a suppose that aj bj mod m for j n use n for every positive inte ger n mathematical induction to prove that a aj bj mod m requires calculus suppose that the sequence xn is recursively defined by and b aj j bj mod m j a use mathematical induction to show that xn that is the sequence xn is monoton show that if n is a positive integer then n k n for which positive integers n is n prove your answer using mathematical induction requires calculus suppose that f x ex and g x ically increasing b use mathematical induction to prove that xn for n show if n is a positive integer with n then n 3n use mathematical induction to prove theorem in sec tion that is show if b is an integer where b and n is a positive integer then n can be expressed uniquely group that can complete a lap by obtaining gas from other cars as it travels around the track show that if n is a positive integer then in the form n akbk ak a0 a lattice point in the plane is a point x y where both x and y are integers use mathematical induction to show that at least n straight lines are needed to ensure j 2j k j k n n that every lattice point x y with x y and x y n lies on one of these lines requires calculus use mathematical induction and the product rule to show that if n is a positive integer and x x fn x are all differentiable functions then x x fn x i use mathematical induction to show that if a b and c are the lengths of the sides of a right triangle where c is the length of the hypotenuse then an bn cn for all integers n with n use mathematical induction to show that if n is a posi tive integers the sequence mod n mod n mod n x f2 x fn x x x fni x mod n is eventually constant that is all terms after a finite number of terms are all the same x f2 x fn x a unit or egyptian fraction is a fraction of the form n where n is a positive integer in this exercise we requires material in section suppose that b mam where a and b are n n matrices and m is invertible show that bk makm for all positive in tegers k consult both the text of section and the preamble to exercise of section use mathematical induction to show that if you draw lines in the plane you only need two colors to color the regions formed so that no two regions that have an edge in com mon have a common color show that n can be represented as the sum of n of its distinct positive divisors whenever n hint use in ductive loading first try to prove this result using mathe matical induction by examining where your proof fails find a stronger statement that you can easily prove using mathematical induction use mathematical induction to prove that if xn are positive real numbers with n then will use strong induction to show that a greedy algorithm can be used to express every rational number p q with p q as the sum of distinct unit fractions at each step of the algorithm we find the smallest positive integer n such that n can be added to the sum without exceed ing p q for example to express we first start the sum with because we add to the sum because is the smallest positive integer k such that k because the algo rithm terminates showing that let t p be the statement that this algorithm terminates for all rational numbers p q with p q we will prove that the algorithm always terminates by showing that t p holds for all positive integers p a show that the basis step t holds b suppose that t k holds for positive integers k with k p that is assume that the algorithm terminates for all rational numbers k r where k p show xn x that if we start with p q and the fraction n is se lected in the first step of the algorithm then p q xn x xn pi qi n where pi np q and qi nq after considering the case where p q n use the in use mathematical induction to prove that if n people stand in a line where n is a positive integer and if the first per son in the line is a woman and the last person in line is a man then somewhere in the line there is a woman directly in front of a man suppose that for every pair of cities in a country there is a direct one way road connecting them in one direction or the other use mathematical induction to show that there is a city that can be reached from every other city either directly or via exactly one other city use mathematical induction to show that when n circles divide the plane into regions these regions can be col ored with two different colors such that no regions with a common boundary are colored the same suppose that among a group of cars on a circular track there is enough fuel for one car to complete a lap use mathematical induction to show that there is a car in the ductive hypothesis to show that the greedy algorithm terminates when it begins with pi qi and complete the inductive step the mccarthy function defined by john mccarthy one of the founders of artificial intelligence is defined using the rule n if n m m n if n for all positive integers n by successively using the defining rule for m n find a m b m c m d m e m f m 76 show that the function m n is a well defined function from the set of positive integers to the set of positive inte gers hint prove that m n for all positive integers n with n is this proof that element a then show that a a is a smaller positive n n n whenever n is a positive integer correct justify your an swer basis step the result is true when n because integer of this form a set is well ordered if every nonempty subset of this set has a least element determine whether each of the following sets is well ordered a the set of integers b the set of integers greater than inductive step assume that the result is true for n then n n n n c the set of positive rationals d the set of positive rationals with denominator less than a show that if an are positive integers then gcd an an gcd an gcd an an b use part a together with the euclidean algorithm to n n n develop a recursive algorithm for computing the great est common divisor of a set of n positive integers n hence the result is true for n if it is true for n this completes the proof suppose that a are a collection of sets describe a recursive algorithm for writing the greatest common divisor of n positive integers as a linear combi nation of these integers find an explicit formula for f n if f and f n n suppose that and rk rk ak for k n use mathematical induction to prove that x rn if and only if x belongs to an odd number of the sets an recall that s t is the symmetric difference of the sets s and t defined in the preamble to exercise of section show that n circles divide the plane into n re gions if every two circles intersect in exactly two points and no three circles contain a common point show that n planes divide three dimensional space into n3 5n regions if any three of these planes have exactly one point in common and no four contain a com mon point use the well ordering property to show that is ir rational hint assume that is rational show that the set of positive integers of the form b has a least f n for n prove your result using mathematical induction give a recursive definition of the set of bit strings that contain twice as many as let s be the set of bit strings defined recursively by λ s and s s if x s where λ is the empty string a find all strings in s of length not exceeding five b give an explicit description of the elements of s let s be the set of strings defined recursively by abc s bac s and acb s where a b and c are fixed letters and for all x s abcx s abxc s axbc s and xabc s where x is a variable representing a string of letters a find all elements of s of length eight or less b show that every element of s has a length divisible by three john mccarthy born john mccarthy was born in boston he grew up in boston and in los angeles he studied mathematics as both an undergraduate and a graduate student receiving his b s in from the california institute of technology and his ph d in from princeton after graduating from princeton mccarthy held positions at princeton stanford dartmouth and m i t he held a position at stanford from until and is now an emeritus professor there at stanford he was the director of the artificial intelligence laboratory held a named chair in the school of engineering and was a senior fellow in the hoover institution mccarthy was a pioneer in the study of artificial intelligence a term he coined in he worked on problems related to the reasoning and information needs required for intelligent computer behavior mc carthy was among the first computer scientists to design time sharing computer systems he developed lisp a programming language for computing using symbolic expressions he played an important role in using logic to verify the correctness of computer programs mccarthy has also worked on the social implications of computer technology he is currently working on the problem of how people and computers make conjectures through assumptions that complications are absent from situations mccarthy is an advocate of the sustainability of human progress and is an optimist about the future of humanity he has also begun writing science fiction stories some of his recent writing explores the possibility that the world is a computer program written by some higher force among the awards mccarthy has won are the turing award from the association for computing machinery the research excellence award of the international conference on artificial intelligence the kyoto prize and the national medal of science the set b of all balanced strings of parentheses is defined recursively by λ b where λ is the empty string x b xy b if x y b show that is a balanced string of parentheses and is not a balanced string of parentheses find all balanced strings of parentheses with exactly six symbols find all balanced strings of parentheses with four or fewer symbols use induction to show that if x is a balanced string of parentheses then the number of left parentheses equals the number of right parentheses in x define the function n on the set of strings of parentheses by n λ n n devise a recursive algorithm that counts the number of times the integer occurs in a list of integers exercises deal with some unusual sequences in formally called self generating sequences produced by simple recurrence relations or rules in particular exer cises deal with the sequence a n defined by a n n a a n for n and a this se quence as well as those in exercises and are defined in douglas hofstader fascinating book gödel escher bach find the first terms of the sequence a n defined in the preamble to this exercise prove that this sequence is well defined that is show that a n is uniquely defined for all nonnegative integers n prove that a n n μ where μ n uv n u n v where λ is the empty string and u and v are strings it can be shown that n is well defined find a n b n c n d n show that a string w of parentheses is balanced if and only if n w and n u whenever u is a prefix of w that is w uv give a recursive algorithm for finding all balanced strings of parentheses containing n or fewer symbols give a recursive algorithm for finding gcd a b where a and b are nonnegative integers not both zero based on these facts gcd a b gcd b a if a b gcd b b gcd a b gcd a b if a and b are even gcd a b gcd a b if a is even and b is odd and gcd a b gcd a b a verify the program segment if x y then x y with respect to the initial assertion t and the final asser tion x y 68 develop a rule of inference for verifying recursive pro hint first show for all n that μn μn then show for all real numbers α with α and α μ that μ α α μ considering the cases α μ and μ α separately use the formula from exercise to show that a n a n if μn μn μ and a n a n otherwise find the first terms of each of the following self generating sequences a a n n a a a n for n a b a n n a a a a n for n a c a n a n a n a n a n for n a and a find the first terms of both the sequences m n and f n defined by the following pair of interwoven recur rence relations m n n f m n f n n m f n for n f and m golomb self generating sequence is the unique nonde creasing sequence of positive integers that has the property that it contains exactly ak occurrences of k for each positive integer k 76 find the first terms of golomb self generating se quence 77 show that if f n is the largest integer m such that am n where am is the mth term of golomb puting factorials given as algorithm in section computer projects write programs with these input and output f f n n kak k given a checkerboard with one square missing construct a tiling of this checkerboard using right triomi noes generate all well formed formulae for expressions in volving the variables x y and z and the operators with n or fewer symbols generate all well formed formulae for propositions with n or fewer symbols where each symbol is t f one of the propositional variables p and q or an operator from given a string find its reversal given a real number a and a nonnegative integer n find an using recursion given a real number a and a nonnegative integer n find using recursion writing projects given a real number a and a nonnegative integer n find an using the binary expansion of n and a recursive algorithm for computing given two integers not both zero find their greatest com mon divisor using recursion given a list of integers and an element x locate x in this list using a recursive implementation of a linear search given a list of integers and an element x locate x in this list using a recursive implementation of a binary search given a nonnegative integer n find the nth fibonacci number using iteration given a nonnegative integer n find the nth fibonacci number using recursion given a positive integer find the number of partitions of this integer see exercise of section given positive integers m and n find a m n the value of ackermann function at the pair m n see the pream ble to exercise of section given a list of n integers sort these integers using the merge sort computations and explorations use a computational program or programs you have written to do these exercises what are the largest values of n for which n has fewer than decimal digits and fewer than decimal digits determine which fibonacci numbers are divisible by which are divisible by and which are divisible by prove that your conjectures are correct construct tilings using right triominoes of various and checkerboards with one square miss ing explore which m n checkerboards can be completely writing projects respond to these with essays using outside sources describe the origins of mathematical induction who were the first people to use it and to which problems did they apply it explain how to prove the jordan curve theorem for sim ple polygons and describe an algorithm for determining whether a point is in the interior or exterior of a simple polygon describe how the triangulation of simple polygons is used in some key algorithms in computational geometry covered by right triominoes can you make a conjecture that answers this question implement an algorithm for determining whether a point is in the interior or exterior of a simple polygon implement an algorithm for triangulating a simple poly gon which values of ackermann function are small enough that you are able to compute them compare either the number of operations or the time needed to compute fibonacci numbers recursively versus that needed to compute them iteratively describe a variety of different applications of the fibonacci numbers to the biological and the physical sciences discuss the uses of ackermann function both in the theory of recursive definitions and in the analysis of the complex ity of algorithms for set unions discuss some of the various methodologies used to es tablish the correctness of programs and compare them to hoare methods described in section explain how the ideas and concepts of program correctness can be extended to prove that operating systems are secure the basics of counting the pigeonhole principle permutations and combinations binomial coefficients and identities generalized permutations and combinations generating permutations and combinations ombinatorics the study of arrangements of objects is an important part of discrete math ematics this subject was studied as long ago as the seventeenth century when combi natorial questions arose in the study of gambling games enumeration the counting of objects with certain properties is an important part of combinatorics we must count objects to solve many different types of problems for instance counting is used to determine the complexity of algorithms counting is also required to determine whether there are enough telephone numbers or internet protocol addresses to meet demand recently it has played a key role in mathematical biology especially in sequencing dna furthermore counting techniques are used extensively when probabilities of events are computed the basic rules of counting which we will study in section can solve a tremendous variety of problems for instance we can use these rules to enumerate the different telephone numbers possible in the united states the allowable passwords on a computer system and the different orders in which the runners in a race can finish another important combinatorial tool is the pigeonhole principle which we will study in section this states that when objects are placed in boxes and there are more objects than boxes then there is a box containing at least two objects for instance we can use this principle to show that among a set of or more students at least were born on the same day of the week we can phrase many counting problems in terms of ordered or unordered arrangements of the objects of a set with or without repetitions these arrangements called permutations and combinations are used in many counting problems for instance suppose the top finishers on a competitive exam taken by students are invited to a banquet we can count the possible sets of students that will be invited as well as the ways in which the top prizes can be awarded another problem in combinatorics involves generating all the arrangements of a specified kind this is often important in computer simulations we will devise algorithms to generate arrangements of various types the basics of counting introduction suppose that a password on a computer system consists of six seven or eight characters each of these characters must be a digit or a letter of the alphabet each password must contain at least one digit how many such passwords are there the techniques needed to answer this question and a wide variety of other counting problems will be introduced in this section counting problems arise throughout mathematics and computer science for example we must count the successful outcomes of experiments and all the possible outcomes of these experiments to determine probabilities of discrete events we need to count the number of operations used by an algorithm to study its time complexity we will introduce the basic techniques of counting in this section these methods serve as the foundation for almost all counting techniques basic counting principles we first present two basic counting principles the product rule and the sum rule then we will show how they can be used to solve many different counting problems the product rule applies when a procedure is made up of separate tasks examples show how the product rule is used example a new company with just two employees sanchez and patel rents a floor of a building with offices how many ways are there to assign different offices to these two employees solution the procedure of assigning offices to these two employees consists of assigning an office to sanchez which can be done in ways then assigning an office to patel different from the office assigned to sanchez which can be done in ways by the product rule there are ways to assign offices to these two employees example the chairs of an auditorium are to be labeled with an uppercase english letter followed by a positive integer not exceeding what is the largest number of chairs that can be labeled differently solution the procedure of labeling a chair consists of two tasks namely assigning to the seat one of the uppercase english letters and then assigning to it one of the possible integers the product rule shows that there are different ways that a chair can be labeled therefore the largest number of chairs that can be labeled differently is example there are microcomputers in a computer center each microcomputer has ports how many different ports to a microcomputer in the center are there solution the procedure of choosing a port consists of two tasks first picking a microcomputer and then picking a port on this microcomputer because there are ways to choose the micro computer and ways to choose the port no matter which microcomputer has been selected the product rule shows that there are ports an extended version of the product rule is often useful suppose that a procedure is carried out by performing the tasks tm in sequence if each task ti i n can be done in ni ways regardless of how the previous tasks were done then there are nm ways to carry out the procedure this version of the product rule can be proved by mathematical induction from the product rule for two tasks see exercise 72 example how many different bit strings of length seven are there solution each of the seven bits can be chosen in two ways because each bit is either or therefore the product rule shows there are a total of different bit strings of length seven example how many different license plates can be made if each plate contains a sequence of three uppercase english letters followed by three digits and no sequences of letters are prohibited even if they are obscene choices for each letter choices for each digit solution there are choices for each of the three uppercase english letters and ten choices for each of the three digits hence by the product rule there are a total of possible license plates example counting functions how many functions are there from a set with m elements to a set with n elements solution a function corresponds to a choice of one of the n elements in the codomain for each of the m elements in the domain hence by the product rule there are n n n nm functions from a set with m elements to one with n elements for example there are different functions from a set with three elements to a set with five elements example counting one to one functions how many one to one functions are there from a set with m elements to one with n elements counting the number of onto functions is harder we ll do this in chapter solution first note that when m n there are no one to one functions from a set with m elements to a set with n elements now let m n suppose the elements in the domain are am there are n ways to choose the value of the function at because the function is one to one the value of the function at can be picked in n ways because the value used for cannot be used again in general the value of the function at ak can be chosen in n k ways by the product rule there are n n n n m one to one functions from a set with m elements to one with n elements for example there are one to one functions from a set with three elements to a set with five elements example the telephone numbering plan the north american numbering plan nanp specifies the format of telephone numbers in the u s canada and many other parts of north america a telephone number in this plan consists of digits which are split into a three digit area code a three digit office code and a four digit station code because of signaling considerations there are certain restrictions on some of these digits to specify the allowable format let x denote a digit that can take any of the values through let n denote a digit that can take any of the values through and let y denote a digit that must be a or a two numbering plans which will be called the old plan and the new plan will be discussed the old plan in use in the has been replaced by the new plan but the recent rapid growth in demand for new current projections are that by it will be necessary to add one or more digits to north american telephone numbers numbers for mobile phones and devices will eventually make even this new plan obsolete in this example the letters used to represent digits follow the conventions of the north american numbering plan as will be shown the new plan allows the use of more numbers in the old plan the formats of the area code office code and station code are nyx nnx and xxxx respectively so that telephone numbers had the form nyx nnx xxxx in the new plan the formats of these codes are nxx nxx and xxxx respectively so that telephone numbers have the form nxx nxx xxxx how many different north american telephone numbers are possible under the old plan and under the new plan solution by the product rule there are area codes with format nyx and area codes with format nxx similarly by the product rule there are office codes with format nnx the product rule also shows that there are station codes with format xxxx note that we have ignored restrictions that rule out station codes for most area codes consequently applying the product rule again it follows that under the old plan there are 640 different numbers available in north america under the new plan there are 400 different numbers available example what is the value of k after the following code where nm are positive integers has been executed solution the initial value of k is zero each time the nested loop is traversed is added to k let ti be the task of traversing the ith loop then the number of times the loop is traversed is the number of ways to do the tasks t2 tm the number of ways to carry out the task tj j m is nj because the j th loop is traversed once for each integer ij with ij nj by the product rule it follows that the nested loop is traversed nm times hence the final value of k is nm example counting subsets of a finite set use the product rule to show that the number of different subsets of a finite set s is s solution let s be a finite set list the elements of s in arbitrary order recall from section that there is a one to one correspondence between subsets of s and bit strings of length s namely a subset of s is associated with the bit string in the ith position if the ith element in the list is in the subset in this position otherwise by the product rule there are s bit strings of length s hence p s s recall that we used mathematical induction to prove this fact in example of section the product rule is often phrased in terms of sets in this way if am are finite sets then the number of elements in the cartesian product of these sets is the product of the number of elements in each set to relate this to the product rule note that the task of choosing an element in the cartesian product am is done by choosing an element in an element in and an element in am by the product rule it follows that am am example dna and genomes the hereditary information of a living organism is encoded using de oxyribonucleic acid dna or in certain viruses ribonucleic acid rna dna and rna are extremely complex molecules with different molecules interacting in a vast variety of ways to soon it won t be that costly to have your own genetic code found enable living process for our purposes we give only the briefest description of how dna and rna encode genetic information dna molecules consist of two strands consisting of blocks known as nucleotides each nucleotide contains subcomponents called bases each of which is adenine a cytosine c guanine g or thymine t the two strands of dna are held together by hydrogen bonds connecting different bases with a bonding only with t and c bonding only with g unlike dna rna is single stranded with uracil u replacing thymine as a base so in dna the possible base pairs are a t and c g while in rna they are a u and c g the dna of a living creature consists of multiple pieces of dna forming separate chromosomes a gene is a segment of a dna molecule that encodes a particular protein the entirety of genetic information of an organism is called its genome sequences of bases in dna and rna encode long chains of proteins called amino acids there are essential amino acids for human beings we can quickly see that a sequence of at least three bases are needed to encode these different amino acid first note that because there are four possibilities for each base in dna a c g and t by the product rule there are 22 different sequences of two bases however there are 64 different sequences of three bases which provide enough different sequences to encode the 22 different amino acids even after taking into account that several different sequences of three bases encode the same amino acid the dna of simple living creatures such as algae and bacteria have between and links where each link is one of the four possible bases more complex organisms such as in sects birds and mammals have between and 1010 links in their dna so by the product rule there are at least different sequences of bases in the dna of simple organisms and at least different sequences of bases in the dna of more complex organisms these are both incredibly huge numbers which helps explain why there is such tremendous variability among living organisms in the past several decades techniques have been developed for determining the genome of different organisms the first step is to locate each gene in the dna of an or ganism the next task called gene sequencing is the determination of the sequence of links on each gene of course the specific sequence of kinks on these genes depends on the partic ular individual representative of a species whose dna is analyzed for example the human genome includes approximately genes each with or more links gene sequencing techniques take advantage of many recently developed algorithms and are based on numerous new ideas in combinatorics many mathematicians and computer scientists work on problems involving genomes taking part in the fast moving fields of bioinformatics and computational biology we now introduce the sum rule example illustrates how the sum rule is used example suppose that either a member of the mathematics faculty or a student who is a mathematics major is chosen as a representative to a university committee how many different choices are there for this representative if there are members of the mathematics faculty and mathematics majors and no one is both a faculty member and a student solution there are ways to choose a member of the mathematics faculty and there are ways to choose a student who is a mathematics major choosing a member of the mathematics faculty is never the same as choosing a student who is a mathematics major because no one is both a faculty member and a student by the sum rule it follows that there are 83 possible ways to pick this representative we can extend the sum rule to more than two tasks suppose that a task can be done in one of ways in one of ways or in one of nm ways where none of the set of ni ways of doing the task is the same as any of the set of nj ways for all pairs i and j with i j m then the number of ways to do the task is nm this extended version of the sum rule is often useful in counting problems as examples and show this version of the sum rule can be proved using mathematical induction from the sum rule for two sets this is exercise example a student can choose a computer project from one of three lists the three lists contain and possible projects respectively no project is on more than one list how many possible projects are there to choose from solution the student can choose a project by selecting a project from the first list the second list or the third list because no project is on more than one list by the sum rule there are ways to choose a project example what is the value of k after the following code where nm are positive integers has been executed solution the initial value of k is zero this block of code is made up of m different loops each time a loop is traversed is added to k to determine the value of k after this code has been executed we need to determine how many times we traverse a loop note that there are ni ways to traverse the ith loop because we only traverse one loop at a time the sum rule shows that the final value of k which is the number of ways to traverse one of the m loops is nm the sum rule can be phrased in terms of sets as if am are pairwise disjoint finite sets then the number of elements in the union of these sets is the sum of the numbers of elements in the sets to relate this to our statement of the sum rule note there are ai ways to choose an element from ai for i m because the sets are pairwise disjoint when we select an element from one of the sets ai we do not also select an element from a different set aj consequently by the sum rule because we cannot select an element from two of these sets at the same time the number of ways to choose an element from one of the sets which is the number of elements in the union is am am when ai aj for all i j this equality applies only when the sets in question are pairwise disjoint the situation is much more complicated when these sets have elements in common that situation will be briefly discussed later in this section and discussed in more depth in chapter more complex counting problems many counting problems cannot be solved using just the sum rule or just the product rule however many complicated counting problems can be solved using both of these rules in combination we begin by counting the number of variable names in the programming language basic in the exercises we consider the number of variable names in java then we will count the number of valid passwords subject to a particular set of restrictions example in a version of the computer language basic the name of a variable is a string of one or two alphanumeric characters where uppercase and lowercase letters are not distinguished an alphanumeric character is either one of the english letters or one of the digits moreover a variable name must begin with a letter and must be different from the five strings of two characters that are reserved for programming use how many different variable names are there in this version of basic solution let v equal the number of different variable names in this version of basic let be the number of these that are one character long and be the number of these that are two characters long then by the sum rule v note that because a one character variable name must be a letter furthermore by the product rule there are strings of length two that begin with a letter and end with an alphanumeric character however five of these are excluded so hence there are v different names for variables in this version of basic example each user on a computer system has a password which is six to eight characters long where each character is an uppercase letter or a digit each password must contain at least one digit how many possible passwords are there solution let p be the total number of possible passwords and let and denote the number of possible passwords of length and respectively by the sum rule p we will now find and finding directly is difficult to find it is easier to find the number of strings of uppercase letters and digits that are six characters long including those with no digits and subtract from this the number of strings with no digits by the product rule the number of strings of six characters is and the number of strings with no digits is hence 176 308 560 similarly we have 176 353 and 456 827 consequently p 684 483 example counting internet addresses in the internet which is made up of interconnected physical networks of computers each computer or more precisely each network connection of a com puter is assigned an internet address in version of the internet protocol now in use bit number class a netid hostid class b netid hostid class c netid hostid class d multicast address class e address figure internet addresses the lack of available address has become a crisis an address is a string of bits it begins with a network number netid the netid is followed by a host number hostid which identifies a computer as a member of a particular network three forms of addresses are used with different numbers of bits used for netids and hostids class a addresses used for the largest networks consist of followed by a bit netid and a bit hostid class b addresses used for medium sized networks consist of followed by a bit netid and a bit hostid class c addresses used for the smallest networks consist of followed by a bit netid and an bit hostid there are several restrictions on addresses because of special uses is not available as the netid of a class a network and the hostids consisting of all and all are not available for use in any network a computer on the internet has either a class a a class b or a class c address besides class a b and c addresses there are also class d addresses reserved for use in multicasting when mul tiple computers are addressed at a single time consisting of 1110 followed by bits and class e addresses reserved for future use consisting of followed by bits neither class d nor class e addresses are assigned as the address of a computer on the internet figure illustrates addressing limitations on the number of class a and class b netids have made addressing inadequate a new version of ip uses bit addresses to solve this problem how many different addresses are available for computers on the internet solution let x be the number of available addresses for computers on the internet and let xa xb and xc denote the number of class a class b and class c addresses available respectively by the sum rule x xa xb xc to find xa note that there are class a netids recalling that the netid overcounting is perhaps the most common enumeration error is unavailable for each netid there are hostids recalling that the hostids consisting of all and all are unavailable consequently xa to find xb and xc note that there are class b netids and 152 class c netids for each class b netid there are 534 hostids and for each class c netid there are hostids recalling that in each network the hostids consisting of all and all are unavailable consequently xb and xc we conclude that the total number of addresses available is x xa xb xc 130 178 073 056 842 the subtraction rule inclusion exclusion for two sets suppose that a task can be done in one of two ways but some of the ways to do it are common to both ways in this situation we cannot use the sum rule to count the number of ways to do the task if we add the number of ways to do the tasks in these two ways we get an overcount of the total number of ways to do it because the ways to do the task that are common to the two ways are counted twice to correctly count the number of ways to do the two tasks we must subtract the number of ways that are counted twice this leads us to an important counting rule the subtraction rule is also known as the principle of inclusion exclusion especially when it is used to count the number of elements in the union of two sets suppose that and are sets then there are ways to select an element from and ways to select an element from the number of ways to select an element from or from that is the number of ways to select an element from their union is the sum of the number of ways to select an element from and the number of ways to select an element from minus the number of ways to select an element that is in both and because there are ways to select an element in either or in and ways to select an element common to both sets we have this is the formula given in section for the number of elements in the union of two sets example illustrates how we can solve counting problems using the subtraction principle example how many bit strings of length eight either start with a bit or end with the two bits ways 64 ways ways solution we can construct a bit string of length eight that either starts with a bit or ends with the two bits by constructing a bit string of length eight beginning with a bit or by constructing a bit string of length eight that ends with the two bits we can construct a bit string of length eight that begins with a in ways this follows by the product rule because the first bit can be chosen in only one way and each of the other seven bits can be chosen in two ways similarly we can construct a bit string of length eight ending with the two bits in 64 ways this follows by the product rule because each of the first six bits can be chosen in two ways and the last two bits can be chosen in only one way some of the ways to construct a bit string of length eight starting with a are the same as the ways to construct a bit string of length eight that ends with the two bits there are ways to construct such a string this follows by the product rule because the first bit can be chosen in only one way each of the second through the sixth bits can be chosen in two ways and the last two bits can be chosen in one way consequently the number of bit strings of length eight that begin with a or end with a which equals the number of ways to construct a bit string of length eight that begins with a or that ends with equals 128 64 160 we present an example that illustrates how the formulation of the principle of inclusion exclusion can be used to solve counting problems example a computer company receives applications from computer graduates for a job planning a line of new web servers suppose that of these applicants majored in computer science majored in business and majored both in computer science and in business how many of these applicants majored neither in computer science nor in business solution to find the number of these applicants who majored neither in computer science nor in business we can subtract the number of students who majored either in computer science or in business or both from the total number of applicants let be the set of students who majored in computer science and the set of students who majored in business then is the set of students who majored in computer science or business or both and is the set of students who majored both in computer science and in business by the subtraction rule the number of students who majored either in computer science or in business or both equals a2 a2 147 316 we conclude that 316 of the applicants majored neither in computer science nor in business the subtraction rule or the principle of inclusion exclusion can be generalized to find the number of ways to do one of n different tasks or equivalently to find the number of elements in the union of n sets whenever n is a positive integer we will study the inclusion exclusion principle and some of its many applications in chapter the division rule we have introduced the product sum and subtraction rules for counting you may wonder whether there is also a division rule for counting in fact there is such a rule which can be useful when solving certain types of enumeration problems we can restate the division rule in terms of sets if the finite set a is the union of n pairwise disjoint subsets each with d elements then n a d we can also formulate the division rule in terms of functions if f is a function from a to b where a and b are finite sets and that for every value y b there are exactly d values x a such that f x y in which case we say that f is d to one then b a d we illustrate the use of the division rule for counting with an example example how many different ways are there to seat four people around a circular table where two seatings are considered the same when each person has the same left neighbor and the same right neighbor bit bit bit bit solution we arbitrarily select a seat at the table and label it seat we number the rest of the seats in numerical order proceeding clockwise around the table note that are four ways to select the person for seat three ways to select the person for seat two ways to select the person for seat and one way to select the person for seat thus there are ways to order the given four people for these seats however each of the four choices for seat leads to the same arrangement as we distinguish two arrangements only when one of the people has a different immediate left or immediate right neighbor because there are four ways to choose the person for seat by the division rule there are different seating arrangements of four people around the circular table tree diagrams counting problems can be solved using tree diagrams a tree consists of a root a number of branches leaving the root and possible additional branches leaving the endpoints of other branches we will study trees in detail in chapter to use trees in counting we use a branch figure bit strings of length four without consecutive to represent each possible choice we represent the possible outcomes by the leaves which are the endpoints of branches not having other branches starting at them note that when a tree diagram is used to solve a counting problem the number of choices of which branch to follow to reach a leaf can vary see example for example game game game game game figure best three games out of five playoffs example how many bit strings of length four do not have two consecutive solution the tree diagram in figure displays all bit strings of length four without two con secutive we see that there are eight bit strings of length four without two consecutive example 22 a playoff between two teams consists of at most five games the first team that wins three games wins the playoff in how many different ways can the playoff occur solution the tree diagram in figure displays all the ways the playoff can proceed with the winner of each game shown we see that there are different ways for the playoff to occur example suppose that i love new jersey t shirts come in five different sizes s m l xl and xxl further suppose that each size comes in four colors white red green and black except for xl which comes only in red green and black and xxl which comes only in green and black how many different shirts does a souvenir shop have to stock to have at least one of each available size and color of the t shirt solution the tree diagram in figure displays all possible size and color pairs it follows that the souvenir shop owner needs to stock different t shirts w white r red g green b black w r g b w r g b w r g b r g b g b figure counting varieties of t shirts exercises there are mathematics majors and computer sci ence majors at a college a in how many ways can two representatives be picked so that one is a mathematics major and the other is a computer science major b in how many ways can one representative be picked who is either a mathematics major or a computer sci ence major an office building contains floors and has offices on each floor how many offices are in the building a multiple choice test contains questions there are four possible answers for each question a in how many ways can a student answer the questions on the test if the student answers every question b in how many ways can a student answer the questions on the test if the student can leave answers blank a particular brand of shirt comes in colors has a male version and a female version and comes in three sizes for each sex how many different types of this shirt are made six different airlines fly from new york to denver and seven fly from denver to san francisco how many dif ferent pairs of airlines can you choose on which to book a trip from new york to san francisco via denver when you pick an airline for the flight to denver and an airline for the continuation flight to san francisco there are four major auto routes from boston to detroit and six from detroit to los angeles how many major auto routes are there from boston to los angeles via de troit how many different three letter initials can people have how many different three letter initials with none of the letters repeated can people have how many different three letter initials are there that be gin with an a how many bit strings are there of length eight how many bit strings of length ten both begin and end with a how many bit strings are there of length six or less not counting the empty string how many bit strings with length not exceeding n where n is a positive integer consist entirely of not counting the empty string how many bit strings of length n where n is a positive integer start and end with how many strings are there of lowercase letters of length four or less not counting the empty string how many strings are there of four lowercase letters that have the letter x in them how many strings of five ascii characters contain the character at sign at least once note there are 128 different ascii characters how many element dna sequences a end with a b start with t and end with g c contain only a and t d do not contain c how many element rna sequences a do not contain u b end with gu c start with c d contain only a or u how many positive integers between and a are divisible by which integers are these b are divisible by which integers are these c are divisible by and by which integers are these how many positive integers between and a are divisible by which integers are these b are divisible by which integers are these c are divisible by both and which integers are these 22 how many positive integers less than a are divisible by b are divisible by but not by c are divisible by both and d are divisible by either or e are divisible by exactly one of and f are divisible by neither nor g have distinct digits h have distinct digits and are even how many positive integers between and inclu sive a are divisible by b are odd c have the same three decimal digits d are not divisible by e are divisible by or f are not divisible by either or g are divisible by but not by h are divisible by and how many positive integers between and in clusive a are divisible by b are even c have distinct digits d are not divisible by e are divisible by or f are not divisible by either or g are divisible by but not by h are divisible by and how many strings of three decimal digits a do not contain the same digit three times b begin with an odd digit c have exactly two digits that are how many strings of four decimal digits a do not contain the same digit twice b end with an even digit c have exactly three digits that are a committee is formed consisting of one representative from each of the states in the united states where the representative from a state is either the governor or one of the two senators from that state how many ways are there to form this committee how many license plates can be made using either three digits followed by three uppercase english letters or three uppercase english letters followed by three digits how many license plates can be made using either two uppercase english letters followed by four digits or two digits followed by four uppercase english letters how many license plates can be made using either three uppercase english letters followed by three digits or four uppercase english letters followed by two digits how many license plates can be made using either two or three uppercase english letters followed by either two or three digits how many strings of eight uppercase english letters are there a if letters can be repeated b if no letter can be repeated c that start with x if letters can be repeated d that start with x if no letter can be repeated e that start and end with x if letters can be repeated f that start with the letters bo in that order if letters can be repeated g that start and end with the letters bo in that order if letters can be repeated h that start or end with the letters bo in that order if letters can be repeated how many strings of eight english letters are there a that contain no vowels if letters can be repeated b that contain no vowels if letters cannot be repeated c that start with a vowel if letters can be repeated d that start with a vowel if letters cannot be repeated e that contain at least one vowel if letters can be re peated f that contain exactly one vowel if letters can be re peated g that start with x and contain at least one vowel if letters can be repeated h that start and end with x and contain at least one vowel if letters can be repeated how many different functions are there from a set with elements to sets with the following numbers of ele ments a b c d how many one to one functions are there from a set with five elements to sets with the following number of ele ments a b c d how many functions are there from the set n where n is a positive integer to the set how many functions are there from the set n where n is a positive integer to the set a that are one to one b that assign to both and n c that assign to exactly one of the positive integers less than n how many partial functions see section are there from a set with five elements to sets with each of these number of elements a b c d how many partial functions see definition of section are there from a set with m elements to a set with n elements where m and n are positive integers how many subsets of a set with elements have more than one element a palindrome is a string whose reversal is identical to the string how many bit strings of length n are palindromes how many element dna sequences a do not contain the base t b contain the sequence acg c contain all four bases a t c and g d contain exactly three of the four bases a t c and g how many element rna sequences a contain the base u b do not contain the sequence cug c do not contain all four bases a u c and g d contain exactly two of the four bases a u c and g how many ways are there to seat four of a group of ten people around a circular table where two seatings are con sidered the same when everyone has the same immediate left and immediate right neighbor how many ways are there to seat six people around a cir cular table where two seatings are considered the same when everyone has the same two neighbors without re gard to whether they are right or left neighbors in how many ways can a photographer at a wedding ar range people in a row from a group of people where the bride and the groom are among these people if a the bride must be in the picture b both the bride and groom must be in the picture c exactly one of the bride and the groom is in the pic ture in how many ways can a photographer at a wedding ar range six people in a row including the bride and groom if a the bride must be next to the groom b the bride is not next to the groom c the bride is positioned somewhere to the left of the groom how many bit strings of length seven either begin with two or end with three how many bit strings of length either begin with three or end with two how many bit strings of length contain either five con secutive or five consecutive how many bit strings of length eight contain either three consecutive or four consecutive every student in a discrete mathematics class is either a computer science or a mathematics major or is a joint major in these two subjects how many students are in the class if there are computer science majors includ ing joint majors mathematics majors including joint majors and joint majors how many positive integers not exceeding are divis ible either by or by how many different initials can someone have if a person has at least two but no more than five different initials assume that each initial is one of the uppercase letters of the english language suppose that a password for a computer system must have at least but no more than characters where each character in the password is a lowercase english letter an uppercase english letter a digit or one of the six spe cial characters and a how many different passwords are available for this computer system b how many of these passwords contain at least one oc currence of at least one of the six special characters c using your answer to part a determine how long it takes a hacker to try every possible password assum ing that it takes one nanosecond for a hacker to check each possible password the name of a variable in the c programming language is a string that can contain uppercase letters lowercase let ters digits or underscores further the first character in the string must be a letter either uppercase or lowercase or an underscore if the name of a variable is determined by its first eight characters how many different variables can be named in c note that the name of a variable may contain fewer than eight characters the name of a variable in the java programming lan guage is a string of between and 535 characters inclusive where each character can be an uppercase or a lowercase letter a dollar sign an underscore or a digit except that the first character must not be a digit deter mine the number of different variable names in java the international telecommunications union itu specifies that a telephone number must consist of a coun try code with between and digits except that the code is not available for use as a country code followed by a number with at most digits how many available possible telephone numbers are there that satisfy these restrictions suppose that at some future time every telephone in the world is assigned a number that contains a country code to digits long that is of the form x xx or xxx followed by a digit telephone number of the form nxx nxx xxxx as described in example how many different telephone numbers would be available worldwide under this numbering plan a key in the vigenère cryptosystem is a string of english letters where the case of the letters does not matter how many different keys for this cryptosystem are there with three four five or six letters a wired equivalent privacy wep key for a wireless fi delity wifi network is a string of either or hexadecimal digits how many different wep keys are there suppose that p and q are prime numbers and that n pq use the principle of inclusion exclusion to find the num ber of positive integers not exceeding n that are relatively prime to n use the principle of inclusion exclusion to find the num ber of positive integers less than 000 that are not divisible by either or by 64 use a tree diagram to find the number of bit strings of length four with no three consecutive how many ways are there to arrange the letters a b c and d such that a is not followed immediately by b use a tree diagram to find the number of ways that the world series can occur where the first team that wins four games out of seven wins the series use a tree diagram to determine the number of subsets of with the property that the sum of the elements in the subset is less than 68 a suppose that a store sells six varieties of soft drinks cola ginger ale orange root beer lemonade and cream soda use a tree diagram to determine the num ber of different types of bottles the store must stock to have all varieties available in all size bottles if all vari eties are available in ounce bottles all but lemon ade are available in ounce bottles only cola and ginger ale are available in ounce bottles and all but lemonade and cream soda are available in 64 ounce bottles b answer the question in part a using counting rules a suppose that a popular style of running shoe is avail able for both men and women the woman shoe comes in sizes and and the man shoe comes in sizes and the man shoe comes in white and black while the woman shoe comes in white red and black use a tree diagram to determine the number of different shoes that a store has to stock to have at least one pair of this type of running shoe for all available sizes and colors for both men and women b answer the question in part a using counting rules use the product rule to show that there are different truth tables for propositions in n variables use mathematical induction to prove the sum rule for m tasks from the sum rule for two tasks 72 use mathematical induction to prove the product rule for m tasks from the product rule for two tasks how many diagonals does a convex polygon with n sides have recall that a polygon is convex if every line seg ment connecting two points in the interior or boundary of the polygon lies entirely within this set and that a diago nal of a polygon is a line segment connecting two vertices that are not adjacent 74 data are transmitted over the internet in datagrams which are structured blocks of bits each datagram con tains header information organized into a maximum of different fields specifying many things including the source and destination addresses and a data area that contains the actual data that are transmitted one of the header fields is the header length field denoted by hlen which is specified by the protocol to be bits long and that specifies the header length in terms of bit blocks of bits for example if hlen the header is made up of six bit blocks another of the header fields is the bit long total length field denoted by total length which specifies the length in bits of the entire datagram including both the header fields and the data area the length of the data area is the total length of the datagram minus the length of the header a the largest possible value of total length which is bits long determines the maximum total length in octets blocks of bits of an internet datagram what is this value b the largest possible value of hlen which is bits long determines the maximum total header length in bit blocks what is this value what is the maximum total header length in octets c the minimum and most common header length is octets what is the maximum total length in octets of the data area of an internet datagram d how many different strings of octets in the data area can be transmitted if the header length is octets and the total length is as long as possible the pigeonhole principle introduction suppose that a flock of pigeons flies into a set of pigeonholes to roost because there are pigeons but only pigeonholes a least one of these pigeonholes must have at least two pigeons in it to see why this is true note that if each pigeonhole had at most one pigeon in it at most pigeons one per hole could be accommodated this illustrates a general principle called the pigeonhole principle which states that if there are more pigeons than pigeonholes then there must be at least one pigeonhole with at least two pigeons in it see figure of course this principle applies to other objects besides pigeons and pigeonholes theorem a b c figure there are more pigeons than pigeonholes proof we prove the pigeonhole principle using a proof by contraposition suppose that none of the k boxes contains more than one object then the total number of objects would be at most k this is a contradiction because there are at least k objects the pigeonhole principle is also called the dirichlet drawer principle after the nineteenth century german mathematician g lejeune dirichlet who often used this principle in his work dirichlet was not the first person to use this principle a demonstration that there were at least two parisians with the same number of hairs on their heads dates back to the century see exercise it is an important additional proof technique supplementing those we have developed in earlier chapters we introduce it in this chapter because of its many important applications to combinatorics we will illustrate the usefulness of the pigeonhole principle we first show that it can be used to prove a useful corollary about functions corollary proof suppose that for each element y in the codomain of f we have a box that contains all elements x of the domain of f such that f x y because the domain contains k or more elements and the codomain contains only k elements the pigeonhole principle tells us that one of these boxes contains two or more elements x of the domain this means that f cannot be one to one examples show how the pigeonhole principle is used example among any group of people there must be at least two with the same birthday because there are only possible birthdays example in any group of english words there must be at least two that begin with the same letter because there are letters in the english alphabet example how many students must be in a class to guarantee that at least two students receive the same score on the final exam if the exam is graded on a scale from to points solution there are possible scores on the final the pigeonhole principle shows that among any students there must be at least students with the same score g lejeune dirichlet g lejeune dirichlet was born into a belgian family living near cologne germany his father was a postmaster he became passionate about mathematics at a young age he was spending all his spare money on mathematics books by the time he entered secondary school in bonn at the age of at he entered the jesuit college in cologne and at 16 he began his studies at the university of paris in he returned to germany and was appointed to a position at the university of breslau in he moved to the university of berlin in he was chosen to succeed gauss at the university of göttingen dirichlet is said to be the first person to master gauss disquisitiones arithmeticae which appeared years earlier he is said to have kept a copy at his side even when he traveled dirichlet made many important discoveries in number theory including the theorem that there are infinitely many primes in arithmetical progressions an b when a and b are relatively prime he proved the n case of fermat last theorem that there are no nontrivial solutions in integers to dirichlet also made many contributions to analysis dirichlet was considered to be an excellent teacher who could explain ideas with great clarity he was married to rebecca mendelssohn one of the sisters of the composer frederick mendelssohn the pigeonhole principle is a useful tool in many proofs including proofs of surprising results such as that given in example example show that for every integer n there is a multiple of n that has only and in its decimal expansion solution let n be a positive integer consider the n integers where the last integer in this list is the integer with n in its decimal expansion note that there are n possible remainders when an integer is divided by n because there are n integers in this list by the pigeonhole principle there must be two with the same remainder when divided by n the larger of these integers less the smaller one is a multiple of n which has a decimal expansion consisting entirely of and the generalized pigeonhole principle the pigeonhole principle states that there must be at least two objects in the same box when there are more objects than boxes however even more can be said when the number of objects exceeds a multiple of the number of boxes for instance among any set of decimal digits there must be that are the same this follows because when objects are distributed into boxes one box must have more than objects theorem proof we will use a proof by contraposition suppose that none of the boxes contains more than in k objects then the total number of objects is at most k n k n n where the inequality n k n k has been used this is a contradiction because there are a total of n objects a common type of problem asks for the minimum number of objects such that at least r of these objects must be in one of k boxes when these objects are distributed among the boxes when we have n objects the generalized pigeonhole principle tells us there must be at least r objects in one of the boxes as long as n k r the smallest integer n with n k r namely n k r is the smallest integer satisfying the inequality n k r could a smaller value of n suffice the answer is no because if we had k r objects we could put r of them in each of the k boxes and no box would have at least r objects when thinking about problems of this type it is useful to consider how you can avoid having at least r objects in one of the boxes as you add successive objects to avoid adding a rth object to any box you eventually end up with r objects in each box there is no way to add the next object without putting an rth object in that box examples illustrate how the generalized pigeonhole principle is applied example among people there are at least who were born in the same month example what is the minimum number of students required in a discrete mathematics class to be sure that at least six will receive the same grade if there are five possible grades a b c d and f solution the minimum number of students needed to ensure that at least six students receive the same grade is the smallest integer n such that n the smallest such integer is n if you have only students it is possible for there to be five who have re ceived each grade so that no six students have received the same grade thus is the minimum number of students needed to ensure that at least six students will receive the same grade example a how many cards must be selected from a standard deck of cards to guarantee that at least three cards of the same suit are chosen b how many must be selected to guarantee that at least three hearts are selected a standard deck of cards has kinds of cards with four cards of each of kind one in each of the four suits hearts diamonds spades and clubs solution a suppose there are four boxes one for each suit and as cards are selected they are placed in the box reserved for cards of that suit using the generalized pigeonhole principle we see that if n cards are selected there is at least one box containing at least n cards consequently we know that at least three cards of one suit are selected if n the smallest integer n such that n is n so nine cards suffice note that if eight cards are selected it is possible to have two cards of each suit so more than eight cards are needed consequently nine cards must be selected to guarantee that at least three cards of one suit are chosen one good way to think about this is to note that after the eighth card is chosen there is no way to avoid having a third card of some suit b we do not use the generalized pigeonhole principle to answer this question because we want to make sure that there are three hearts not just three cards of one suit note that in the worst case we can select all the clubs diamonds and spades cards in all before we select a single heart the next three cards will be all hearts so we may need to select cards to get three hearts example what is the least number of area codes needed to guarantee that the million phones in a state can be assigned distinct digit telephone numbers assume that telephone numbers are of the form nxx nxx xxxx where the first three digits form the area code n represents a digit from to inclusive and x represents any digit solution there are eight million different phone numbers of the form nxx xxxx as shown in example of section hence by the generalized pigeonhole principle among million telephones at least 000 000 000 000 of them must have identical phone numbers hence at least four area codes are required to ensure that all digit numbers are different example although not an application of the generalized pigeonhole principle makes use of similar principles example suppose that a computer science laboratory has workstations and servers a cable can be used to directly connect a workstation to a server for each server only one direct connection to that server can be active at any time we want to guarantee that at any time any set of or fewer workstations can simultaneously access different servers via direct connections although we could do this by connecting every workstation directly to every server using connections what is the minimum number of direct connections needed to achieve this goal solution suppose that we label the workstations and the servers furthermore suppose that we connect wk to sk for k and each of w14 and to all servers we have a total of direct connections clearly any set of or fewer workstations can simultaneously access different servers we see this by noting that if workstation wj is included with j it can access server sj and for each workstation wk with k included there must be a corresponding workstation wj with j not included so wk can access server sj this follows because there are at least as many available servers sj as there are workstations wj with j not included now suppose there are fewer than direct connections between workstations and servers then some server would be connected to at most workstations if all servers were connected to at least six workstations there would be at least direct connections this means that the remaining nine servers are not enough to allow the other workstations to simultaneously access different servers consequently at least direct connections are needed it follows that is the answer some elegant applications of the pigeonhole principle in many interesting applications of the pigeonhole principle the objects to be placed in boxes must be chosen in a clever way a few such applications will be described here example during a month with days a baseball team plays at least one game a day but no more than games show that there must be a period of some number of consecutive days during which the team must play exactly games solution let aj be the number of games played on or before the j th day of the month then is an increasing sequence of distinct positive integers with aj more over is also an increasing sequence of distinct positive integers with aj the positive integers a2 are all less than or equal to 59 hence by the pigeonhole principle two of these integers are equal because the integers aj j are all distinct and the integers aj j are all distinct there must be indices i and j with ai aj this means that exactly games were played from day j to day i example show that among any n positive integers not exceeding there must be an integer that divides one of the other integers solution write each of the n integers a2 an as a power of times an odd integer in other words let aj for j n where kj is a nonnegative integer and qj is odd the integers qn are all odd positive integers less than because there are only n odd positive integers less than it follows from the pigeonhole principle that two of the integers qn must be equal therefore there are distinct integers i and j such that qi qj let q be the common value of qi and qj then ai and aj it follows that if ki kj then ai divides aj while if ki kj then aj divides ai a clever application of the pigeonhole principle shows the existence of an increasing or a decreasing subsequence of a certain length in a sequence of distinct integers we review some definitions before this application is presented suppose that a2 an is a sequence of real numbers a subsequence of this sequence is a sequence of the form ai2 aim where im n hence a subsequence is a sequence obtained from the original sequence by including some of the terms of the original sequence in their original order and perhaps not including other terms a sequence is called strictly increasing if each term is larger than the one that precedes it and it is called strictly decreasing if each term is smaller than the one that precedes it theorem we give an example before presenting the proof of theorem example the sequence contains terms note that there are four strictly increasing subsequences of length four namely and there is also a strictly decreasing subsequence of length four namely the proof of the theorem will now be given proof let a2 be a sequence of distinct real numbers associate an ordered pair with each term of the sequence namely associate ik dk to the term ak where ik is the length of the longest increasing subsequence starting at ak and dk is the length of the longest decreasing subsequence starting at ak suppose that there are no increasing or decreasing subsequences of length n then ik and dk are both positive integers less than or equal to n for k hence by the product rule there are possible ordered pairs for ik dk by the pigeonhole principle two of these ordered pairs are equal in other words there exist terms as and at with t such that is it and ds dt we will show that this is impossible because the terms of the sequence are distinct either as at or as at if as at then because is it an increasing subsequence of length it can be built starting at as by taking as followed by an increasing subsequence of length it beginning at at this is a contradiction similarly if as at the same reasoning shows that ds must be greater than dt which is a contradiction the final example shows how the generalized pigeonhole principle can be applied to an im portant part of combinatorics called ramsey theory after the english mathematician f p ram sey in general ramsey theory deals with the distribution of subsets of elements of sets example assume that in a group of six people each pair of individuals consists of two friends or two enemies show that there are either three mutual friends or three mutual enemies in the group solution let a be one of the six people of the five other people in the group there are either three or more who are friends of a or three or more who are enemies of a this follows from the generalized pigeonhole principle because when five objects are divided into two sets one of the sets has at least elements in the former case suppose that b c and d are friends of a if any two of these three individuals are friends then these two and a form a group of three mutual friends otherwise b c and d form a set of three mutual enemies the proof in the latter case when there are three or more enemies of a proceeds in a similar manner the ramsey number r m n where m and n are positive integers greater than or equal to denotes the minimum number of people at a party such that there are either m mutual friends or n mutual enemies assuming that every pair of people at the party are friends or enemies example shows that r we conclude that r because in a group of five frank plumpton ramsey frank plumpton ramsey son of the president of magdalene college cambridge was educated at winchester and trinity colleges after graduating in he was elected a fellow of king college cambridge where he spent the remainder of his life ramsey made important contributions to mathematical logic what we now call ramsey theory began with his clever combinatorial arguments published in the paper on a problem of formal logic ramsey also made contributions to the mathematical theory of economics he was noted as an excellent lecturer on the foundations of mathematics according to one of his brothers he was interested in almost everything including english literature and politics ramsey was married and had two daughters his death at the age of resulting from chronic liver problems deprived the mathematical community and cambridge university of a brilliant young scholar people where every two people are friends or enemies there may not be three mutual friends or three mutual enemies see exercise it is possible to prove some useful properties about ramsey numbers but for the most part it is difficult to find their exact values note that by symmetry it can be shown that r m n r n m see exercise we also have r n n for every positive integer n see exercise the exact values of only nine ramsey numbers r m n with m n are known including r only bounds are known for many other ramsey numbers in cluding r which is known to satisfy r the reader interested in learning more about ramsey numbers should consult or exercises show that in any set of six classes each meeting regu larly once a week on a particular day of the week there must be two that meet on the same day assuming that no classes are held on weekends show that if there are students in a class then at least two have last names that begin with the same letter a drawer contains a dozen brown socks and a dozen black socks all unmatched a man takes socks out at random in the dark a how many socks must he take out to be sure that he has at least two socks of the same color b how many socks must he take out to be sure that he has at least two black socks a bowl contains red balls and blue balls a woman selects balls at random without looking at them a how many balls must she select to be sure of having at least three balls of the same color b how many balls must she select to be sure of having at least three blue balls show that among any group of five not necessarily con secutive integers there are two with the same remainder when divided by let d be a positive integer show that among any group of d not necessarily consecutive integers there are two with exactly the same remainder when they are divided by d let n be a positive integer show that in any set of n consecutive integers there is exactly one divisible by n show that if f is a function from s to t where s and t are finite sets with s t then there are elements and in s such that f f or in other words f is not one to one what is the minimum number of students each of whom comes from one of the states who must be enrolled in a university to guarantee that there are at least who come from the same state let xi yi i be a set of five distinct points with integer coordinates in the xy plane show that the midpoint of the line joining at least one pair of these points has integer coordinates let xi yi zi i be a set of nine distinct points with integer coordinates in xyz space show that the midpoint of at least one pair of these points has integer coordinates how many ordered pairs of integers a b are needed to guarantee that there are two ordered pairs and a2 b2 such that mod a2 mod and mod b2 mod a show that if five integers are selected from the first eight positive integers there must be a pair of these integers with a sum equal to b is the conclusion in part a true if four integers are selected rather than five a show that if seven integers are selected from the first positive integers there must be at least two pairs of these integers with the sum b is the conclusion in part a true if six integers are selected rather than seven how many numbers must be selected from the set to guarantee that at least one pair of these numbers add up to 16 how many numbers must be selected from the set to guarantee that at least one pair of these numbers add up to 16 a company stores products in a warehouse storage bins in this warehouse are specified by their aisle location in the aisle and shelf there are aisles horizontal locations in each aisle and shelves throughout the ware house what is the least number of products the company can have so that at least two products must be stored in the same bin suppose that there are nine students in a discrete mathe matics class at a small college a show that the class must have at least five male stu dents or at least five female students b show that the class must have at least three male stu dents or at least seven female students suppose that every student in a discrete mathematics class of students is a freshman a sophomore or a junior a show that there are at least nine freshmen at least nine sophomores or at least nine juniors in the class b show that there are either at least three freshmen at least sophomores or at least five juniors in the class find an increasing subsequence of maximal length and a decreasing subsequence of maximal length in the se quence 22 construct a sequence of 16 positive integers that has no increasing or decreasing subsequence of five terms 22 show that if there are people of different heights standing in a line it is possible to find people in the order they are standing in the line with heights that are either increasing or decreasing show that whenever girls and boys are seated around a circular table there is always a person both of whose neighbors are boys suppose that girls and boys enter a mathemat ics competition furthermore suppose that each entrant solves at most six questions and for every boy girl pair there is at least one question that they both solved show that there is a question that was solved by at least three girls and at least three boys describe an algorithm in pseudocode for producing the largest increasing or decreasing subsequence of a se quence of distinct integers show that in a group of five people where any two people are either friends or enemies there are not necessarily three mutual friends or three mutual enemies show that in a group of people where any two people are either friends or enemies there are either three mu tual friends or four mutual enemies and there are either three mutual enemies or four mutual friends use exercise to show that among any group of people where any two people are either friends or ene mies there are either four mutual friends or four mutual enemies show that if n is an integer with n then the ramsey number r n equals n recall that ramsey numbers were discussed after example in section show that if m and n are integers with m and n then the ramsey numbers r m n and r n m are equal recall that ramsey numbers were discussed after exam ple in section show that there are at least six people in california pop ulation million with the same three initials who were born on the same day of the year but not necessarily in the same year assume that everyone has three initials show that if there are 000 000 wage earners in the united states who earn less than 000 000 dollars but at least a penny then there are two who earned exactly the same amount of money to the penny last year in the century there were more than 800 000 inhab itants of paris at the time it was believed that no one had more than 000 hairs on their head assuming these numbers are correct and that everyone has at least one hair on their head that is no one is completely bald use the pigeonhole principle to show as the french writer pierre nicole did that there had to be two parisians with the same number of hairs on their heads then use the gener alized pigeonhole principle to show that there had to be at least five parisians at that time with the same number of hairs on their heads assuming that no one has more than 000 000 hairs on the head of any person and that the population of new york city was 008 in show there had to be at least nine people in new york city in with the same number of hairs on their heads there are different time periods during which classes at a university can be scheduled if there are different classes how many different rooms will be needed a computer network consists of six computers each com puter is directly connected to at least one of the other com puters show that there are at least two computers in the network that are directly connected to the same number of other computers a computer network consists of six computers each com puter is directly connected to zero or more of the other computers show that there are at least two computers in the network that are directly connected to the same num ber of other computers hint it is impossible to have a computer linked to none of the others and a computer linked to all the others find the least number of cables required to connect eight computers to four printers to guarantee that for every choice of four of the eight computers these four com puters can directly access four different printers justify your answer find the least number of cables required to connect computers to printers to guarantee that subset of computers can directly access different printers here the assumptions about cables and computers are the same as in example justify your answer prove that at a party where there are at least two people there are two people who know the same number of other people there an arm wrestler is the champion for a period of 75 hours here by an hour we mean a period starting from an exact hour such as p m until the next hour the arm wrestler had at least one match an hour but no more than total matches show that there is a period of consec utive hours during which the arm wrestler had exactly matches is the statement in exercise true if is replaced by a b c d show that if f is a function from s to t where s and t are nonempty finite sets and m s t then there are at least m elements of s mapped to the same value of t that is show that there are distinct elements sm of s such that f f f sm there are houses on a street each house has an address between and inclusive show that at least two houses have addresses that are consecutive integers let x be an irrational number show that for some positive integer j not exceeding the positive integer n the absolute value of the difference between jx and the nearest integer to jx is less than n let nt be positive integers show that if nt t objects are placed into t boxes then for some i i 2 t the ith box con tains at least ni objects an alternative proof of theorem based on the general ized pigeonhole principle is outlined in this exercise the notation used is the same as that used in the proof in the text a assume that ik n for k 2 use the generalized pigeonhole principle to show that there are n terms akn with ikn where k1 kn b show that akj akj for j 2 n hint as sume that akj akj and show that this implies that ikj ikj which is a contradiction c use parts a and b to show that if there is no increas ing subsequence of length n then there must be a decreasing subsequence of this length permutations and combinations introduction many counting problems can be solved by finding the number of ways to arrange a specified number of distinct elements of a set of a particular size where the order of these elements matters many other counting problems can be solved by finding the number of ways to select a particular number of elements from a set of a particular size where the order of the elements selected does not matter for example in how many ways can we select three students from a group of five students to stand in line for a picture how many different committees of three students can be formed from a group of four students in this section we will develop methods to answer questions such as these permutations we begin by solving the first question posed in the introduction to this section as well as related questions example in how many ways can we select three students from a group of five students to stand in line for a picture in how many ways can we arrange all five of these students in a line for a picture solution first note that the order in which we select the students matters there are five ways to select the first student to stand at the start of the line once this student has been selected there are four ways to select the second student in the line after the first and second students have been selected there are three ways to select the third student in the line by the product rule there are 60 ways to select three students from a group of five students to stand in line for a picture to arrange all five students in a line for a picture we select the first student in five ways the second in four ways the third in three ways the fourth in two ways and the fifth in one way consequently there are 2 ways to arrange all five students in a line for a picture example illustrates how ordered arrangements of distinct objects can be counted this leads to some terminology a permutation of a set of distinct objects is an ordered arrangement of these objects we also are interested in ordered arrangements of some of the elements of a set an ordered arrangement of r elements of a set is called an r permutation example 2 let s 2 the ordered arrangement 2 is a permutation of s the ordered arrangement 2 is a 2 permutation of s the number of r permutations of a set with n elements is denoted by p n r we can find p n r using the product rule example let s a b c the 2 permutations of s are the ordered arrangements a b a c b a b c c a and c b consequently there are six 2 permutations of this set with three elements there are always six 2 permutations of a set with three elements there are three ways to choose the first element of the arrangement there are two ways to choose the second element of the arrangement because it must be different from the first element hence by the product rule we see that p 2 2 the first element by the product rule it follows that p 2 2 we now use the product rule to find a formula for p n r whenever n and r are positive integers with r n theorem corollary proof we will use the product rule to prove that this formula is correct the first element of the permutation can be chosen in n ways because there are n elements in the set there are n ways to choose the second element of the permutation because there are n elements left in the set after using the element picked for the first position similarly there are n 2 ways to choose the third element and so on until there are exactly n r n r ways to choose the rth element consequently by the product rule there are n n n 2 n r r permutations of the set note that p n whenever n is a nonnegative integer because there is exactly one way to order zero elements that is there is exactly one list with no elements in it namely the empty list we now state a useful corollary of theorem proof when n and r are integers with r n by theorem we have p n r n n n 2 n r n because n n whenever n is a nonnegative integer we see that the formula n n p n r n also holds when r n r by theorem we know that if n is a positive integer then p n n n we will illustrate this result with some examples example how many ways are there to select a first prize winner a second prize winner and a third prize winner from different people who have entered a contest solution because it matters which person wins which prize the number of ways to pick the three prize winners is the number of ordered selections of three elements from a set of elements that is the number of permutations of a set of elements consequently the answer is p 99 970 example suppose that there are eight runners in a race the winner receives a gold medal the second place finisher receives a silver medal and the third place finisher receives a bronze medal how many different ways are there to award these medals if all possible outcomes of the race can occur and there are no ties solution the number of different ways to award the medals is the number of permutations of a set with eight elements hence there are p 336 possible ways to award the medals example suppose that a saleswoman has to visit eight different cities she must begin her trip in a specified city but she can visit the other seven cities in any order she wishes how many possible orders can the saleswoman use when visiting these cities solution the number of possible paths between the cities is the number of permutations of seven elements because the first city is determined but the remaining seven can be ordered arbitrarily consequently there are 2 ways for the saleswoman to choose her tour if for instance the saleswoman wishes to find the path between the cities with minimum distance and she computes the total distance for each possible path she must consider a total of paths example how many permutations of the letters abcdefgh contain the string abc solution because the letters abc must occur as a block we can find the answer by finding the number of permutations of six objects namely the block abc and the individual letters d e f g and h because these six objects can occur in any order there are 720 permutations of the letters abcdefgh in which abc occurs as a block combinations we now turn our attention to counting unordered selections of objects we begin by solving a question posed in the introduction to this section of the chapter example how many different committees of three students can be formed from a group of four students solution to answer this question we need only find the number of subsets with three elements from the set containing the four students we see that there are four such subsets one for each of the four students because choosing three students is the same as choosing one of the four students to leave out of the group this means that there are four ways to choose the three students for the committee where the order in which these students are chosen does not matter example illustrates that many counting problems can be solved by finding the number of subsets of a particular size of a set with n elements where n is a positive integer an r combination of elements of a set is an unordered selection of r elements from the set thus an r combination is simply a subset of the set with r elements example let s be the set 2 then is a combination from s note that is the same combination as because the order in which the elements of a set are listed does not matter the number of r combinations of a set with n distinct elements is denoted by c n r note that c n r is also denoted by n and is called a binomial coefficient we will learn where this terminology comes from in section example we see that c 2 because the 2 combinations of a b c d are the six subsets a b a c a d b c b d and c d we can determine the number of r combinations of a set with n elements using the formula for the number of r permutations of a set to do this note that the r permutations of a set can be obtained by first forming r combinations and then ordering the elements in these combinations the proof of theorem 2 which gives the value of c n r is based on this observation theorem 2 proof the p n r r permutations of the set can be obtained by forming the c n r r combinations of the set and then ordering the elements in each r combination which can be done in p r r ways consequently by the product rule p n r c n r p r r this implies that c n r p n r n n r n p r r r r r r n r we can also use the division rule for counting to construct a proof of this theorem because the order of elements in a combination does not matter and there are p r r ways to order r elements in an r combination of n elements each of the c n r r combinations of a set with n elements corresponds to exactly p r r r permutations hence by the division rule c n r p n r which implies as before that c n r r n r the formula in theorem 2 although explicit is not helpful when c n r is computed for large values of n and r the reasons are that it is practical to compute exact values of factorials exactly only for small integer values and when floating point arithmetic is used the formula in theorem 2 may produce a value that is not an integer when computing c n r first note that when we cancel out n r from the numerator and denominator of the expression for c n r in theorem 2 we obtain c n r n n n n r r n r r consequently to compute c n r you can cancel out all the terms in the larger factorial in the denominator from the numerator and denominator then multiply all the terms that do not cancel in the numerator and finally divide by the smaller factorial in the denominator when doing this calculation by hand instead of by machine it is also worthwhile to factor out common factors in the numerator n n n r and in the denominator r note that many calculators have a built in function for c n r that can be used for relatively small values of n and r and many computational programs can be used to find c n r such functions may be called choose n k or binom n k example illustrates how c n k is computed when k is relatively small compared to n and when k is close to n it also illustrates a key identity enjoyed by the numbers c n k example how many poker hands of five cards can be dealt from a standard deck of cards also how many ways are there to select cards from a standard deck of cards solution because the order in which the five cards are dealt from a deck of cards does not matter there are c different hands of five cards that can be dealt to compute the value of c first divide the numerator and denominator by to obtain c 2 this expression can be simplified by first dividing the factor in the denominator into the factor in the numerator to obtain a factor in the numerator then dividing the factor in the denominator into the factor in the numerator to obtain a factor of in the numerator then dividing the factor in the denominator into the factor in the numerator to obtain a factor of in the numerator and finally dividing the factor 2 in the denominator into the factor in the numerator to obtain a factor of in the numerator we find that c 26 2 consequently there are 2 different poker hands of five cards that can be dealt from a standard deck of cards note that there are c different ways to select cards from a standard deck of cards we do not need to compute this value because c c only the order of the factors and is different in the denominators in the formulae for these quantities it follows that there are also 2 different ways to select cards from a standard deck of cards in example we observed that c c this is a special case of the useful identity for the number of r combinations of a set given in corollary 2 corollary 2 proof from theorem 2 it follows that c n r n r n r and n n c n n r n r n n r hence c n r c n n r n r r we can also prove corollary 2 without relying on algebraic manipulation instead we can use a combinatorial proof we describe this important type of proof in definition definition combinatorial proofs are almost always much shorter and provide more insights than proofs based on algebraic manipulation many identities involving binomial coefficients can be proved using combinatorial proofs we now show how to prove corollary 2 using a combinatorial proof we will provide both a double counting proof and a bijective proof both based on the same basic idea proof we will use a bijective proof to show that c n r c n n r for all integers n and r with r n suppose that s is a set with n elements the function that maps a subset a of s to a is a bijection between subsets of s with r elements and subsets with n r elements as the reader should verify the identity c n r c n n r follows because when there is a bijection between two finite sets the two sets must have the same number of elements alternatively we can reformulate this argument as a double counting proof by definition the number of subsets of s with r elements equals c n r but each subset a of s is also determined by specifying which elements are not in a and so are in a because the complement of a subset of s with r elements has n r elements there are also c n n r subsets of s with r elements it follows that c n r c n n r example how many ways are there to select five players from a member tennis team to make a trip to a match at another school solution the answer is given by the number of combinations of a set with elements by theorem 2 the number of such combinations is c example a group of people have been trained as astronauts to go on the first mission to mars how many ways are there to select a crew of six people to go on this mission assuming that all crew members have the same job solution the number of ways to select a crew of six from the pool of people is the number of combinations of a set with elements because the order in which these people are chosen does not matter by theorem 2 the number of such combinations is c 26 593 2 example how many bit strings of length n contain exactly r solution the positions of r in a bit string of length n form an r combination of the set 2 n hence there are c n r bit strings of length n that contain exactly r example suppose that there are faculty members in the mathematics department and in the computer science department how many ways are there to select a committee to develop a discrete mathematics course at a school if the committee is to consist of three faculty members from the mathematics department and four from the computer science department solution by the product rule the answer is the product of the number of combinations of a set with nine elements and the number of combinations of a set with elements by theorem 2 the number of ways to select the committee is c c 9 330 720 exercises list all the permutations of a b c 2 how many different permutations are there of the set a b c d e f g how many permutations of a b c d e f g end with a let s 2 a list all the permutations of s b list all the combinations of s find the value of each of these quantities a p b p c p d p e p f p 9 find the value of each of these quantities a c b c c c d c 8 8 e c 8 f c find the number of permutations of a set with nine el ements 8 in how many different orders can five runners finish a race if no ties are allowed 9 how many possibilities are there for the win place and show first second and third positions in a horse race with horses if all orders of finish are possible there are six different candidates for governor of a state in how many different orders can the names of the can didates be printed on a ballot how many bit strings of length contain a exactly four b at most four c at least four d an equal number of and how many bit strings of length contain a exactly three b at most three c at least three d an equal number of and a group contains n men and n women how many ways are there to arrange these people in a row if the men and women alternate in how many ways can a set of two positive integers less than be chosen in how many ways can a set of five letters be selected from the english alphabet 16 how many subsets with an odd number of elements does a set with elements have how many subsets with more than two elements does a set with elements have a coin is flipped eight times where each flip comes up either heads or tails how many possible outcomes a are there in total b contain exactly three heads c contain at least three heads d contain the same number of heads and tails a coin is flipped times where each flip comes up either heads or tails how many possible outcomes a are there in total b contain exactly two heads c contain at most three tails d contain the same number of heads and tails how many bit strings of length have a exactly three b more than c at least seven d at least three how many permutations of the letters abcdefg con tain a the string bcd b the string cfga c the strings ba and gf d the strings abc and de e the strings abc and cde f the strings cba and bed 22 how many permutations of the letters abcdefgh con tain a the string ed b the string cde c the strings ba and fgh d the strings ab de and gh e the strings cab and bed f the strings bca and abf how many ways are there for eight men and five women to stand in a line so that no two women stand next to each other hint first position the men and then consider possible positions for the women how many ways are there for women and six men to stand in a line so that no two men stand next to each other hint first position the women and then consider possible positions for the men one hundred tickets numbered 2 are sold to different people for a drawing four different prizes are awarded including a grand prize a trip to tahiti how many ways are there to award the prizes if a there are no restrictions b the person holding ticket wins the grand prize c the person holding ticket wins one of the prizes d the person holding ticket does not win a prize e the people holding tickets and both win prizes f the people holding tickets and all win prizes g the people holding tickets and all win prizes h none of the people holding tickets and wins a prize i the grand prize winner is a person holding ticket or j the people holding tickets and win prizes but the people holding tickets and do not win prizes 26 thirteen people on a softball team show up for a game a how many ways are there to choose players to take the field b how many ways are there to assign the positions by selecting players from the people who show up c of the people who show up three are women how many ways are there to choose players to take the field if at least one of these players must be a woman a club has members a how many ways are there to choose four members of the club to serve on an executive committee b how many ways are there to choose a president vice president secretary and treasurer of the club where no person can hold more than one office a professor writes discrete mathematics true false questions of the statements in these questions are true if the questions can be positioned in any order how many different answer keys are possible how many permutations of the positive integers not ex ceeding contain three consecutive integers k k k 2 in the correct order a where these consecutive integers can perhaps be sep arated by other integers in the permutation b where they are in consecutive positions in the permu tation seven women and nine men are on the faculty in the mathematics department at a school a how many ways are there to select a committee of five members of the department if at least one woman must be on the committee b how many ways are there to select a committee of five members of the department if at least one woman and at least one man must be on the committee the english alphabet contains consonants and five vowels how many strings of six lowercase letters of the english alphabet contain a exactly one vowel b exactly two vowels c at least one vowel d at least two vowels how many strings of six lowercase letters from the en glish alphabet contain a the letter a b the letters a and b c the letters a and b in consecutive positions with a preceding b with all the letters distinct d the letters a and b where a is somewhere to the left of b in the string with all the letters distinct suppose that a department contains men and women how many ways are there to form a commit tee with six members if it must have the same number of men and women suppose that a department contains men and women how many ways are there to form a commit tee with six members if it must have more women than men how many bit strings contain exactly eight and if every must be immediately followed by a how many bit strings contain exactly five and if every must be immediately followed by two how many bit strings of length contain at least three and at least three how many ways are there to select countries in the united nations to serve on a council if are selected from a block of are selected from a block of and the others are selected from the remaining countries how many license plates consisting of three letters fol lowed by three digits contain no letter or digit twice a circular r permutation of n people is a seating of r of these n people around a circular table where seatings are con sidered to be the same if they can be obtained from each other by rotating the table find the number of circular permutations of people find a formula for the number of circular r permutations of n people find a formula for the number of ways to seat r of n people around a circular table where seatings are considered the same if every person has the same two neighbors without regard to which side these neighbors are sitting on how many ways are there for a horse race with three horses to finish if ties are possible note two or three horses may tie how many ways are there for a horse race with four horses to finish if ties are possible note any number of the four horses may tie there are six runners in the yard dash how many ways are there for three medals to be awarded if ties are possible the runner or runners who finish with the fastest time receive gold medals the runner or runners who finish with exactly one runner ahead receive silver medals and the runner or runners who finish with exactly two runners ahead receive bronze medals this procedure is used to break ties in games in the cham pionship round of the world cup soccer tournament each team selects five players in a prescribed order each of these players takes a penalty kick with a player from the first team followed by a player from the second team and so on following the order of players specified if the score is still tied at the end of the penalty kicks this proce dure is repeated if the score is still tied after penalty kicks a sudden death shootout occurs with the first team scoring an unanswered goal victorious a how many different scoring scenarios are possible if the game is settled in the first round of penalty kicks where the round ends once it is impossible for a team to equal the number of goals scored by the other team b how many different scoring scenarios for the first and second groups of penalty kicks are possible if the game is settled in the second round of penalty kicks c how many scoring scenarios are possible for the full set of penalty kicks if the game is settled with no more than total additional kicks after the two rounds of five kicks for each team binomial coefficients and identities as we remarked in section the number of r combinations from a set with n elements is occur as coefficients in the expansion of powers of binomial expressions such as a b we will discuss the binomial theorem which gives a power of a binomial expression as a sum of terms involving binomial coefficients we will prove this theorem using a combinatorial proof we will also show how combinatorial proofs can be used to establish some of the many different identities that express relationships among binomial coefficients the binomial theorem the binomial theorem gives the coefficients of the expansion of powers of binomial expressions a binomial expression is simply the sum of two terms such as x y the terms can be products of constants and variables but that does not concern us here example illustrates how the coefficients in a typical expansion can be found and prepares us for the statement of the binomial theorem example the expansion of x y can be found using combinatorial reasoning instead of multiplying the three terms out when x y 3 x y x y x y is expanded all products of a term in the first sum a term in the second sum and a term in the third sum are added terms of the form and arise to obtain a term of the form an x must be chosen in each of the sums and this can be done in only one way thus the term in the product has a coefficient of to obtain a term of the form an x must be chosen in two of the three sums and consequently a y in the other sum hence the number of such terms is the number of 2 combinations of three objects namely 3 similarly the number of terms of the form is the number of ways to pick one of the three sums to obtain an x and consequently take a y from each of the other two sums this can be done in 3 ways finally the only way to obtain a term is to choose the y for each of the three sums in the product and this can be done in exactly one way consequently it follows that x y 3 x y x y x y xx xy yx yy x y xxx xxy xyx xyy yxx yxy yyx yyy y3 we now state the binomial theorem theorem proof we use a combinatorial proof the terms in the product when it is expanded are of the form xn jyj for j 2 n to count the number of terms of the form xn jyj note that to obtain such a term it is necessary to choose n j xs from the n su ms so that the some computational uses of the binomial theorem are illustrated in examples 2 example 2 what is the expansion of x y solution from the binomial theorem it follows that x y j x 4 jyj 4 4 x3y 4 x2y2 4 xy3 4 y4 4x3y y4 example 3 what is the coefficient of in the expansion of x y solution from the binomial theorem it follows that this coefficient is example 4 what is the coefficient of in the expansion of 3y solution first note that this expression equals 3y by the binomial theorem we have consequently the coefficient of in the expansion is obtained when j namely 212 3 212313 we can prove some useful identities using the binomial theorem as corollaries 2 and 3 demonstrate corollary proof using the binomial theorem with x and y we see that n k n k k n this is the desired result there is also a nice combinatorial proof of corollary which we now present proof a set with n elements has a total of different subsets each subset has zero elements subsets with one element n subsets with two elements and n subsets with n elements k n counts the total number of subsets of a set with n elements by equating the two formulas we have for the number of subsets of a set with n elements we see that k n corollary 2 proof when we use the binomial theorem with x and y we see that 0n n this proves the corollary k n k1n k k n k remark corollary 2 implies that n n n n n n corollary 3 proof we recognize that the left hand side of this formula is the expansion of 2 n provided by the binomial theorem therefore by the binomial theorem we see that 2 n hence k n k n 2k k 2k n 3n pascal identity and triangle the binomial coefficients satisfy many different identities we introduce one of the most im portant of these now theorem 2 proof we will use a combinatorial proof suppose that t is a set containing n elements let a be an element in t and let s t a note that there are n subsets of t containing k of s or contains k elements of s and does not contain a because there are n n k subsets of k of s consequently n n k n k k k remark it is also possible to prove this identity by algebraic manipulation from the formula n r 2 2 2 by pascal identity 2 3 3 3 3 3 3 4 4 2 3 4 4 4 4 4 4 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 56 8 a b figure pascal triangle can be used to recursively define binomial coefficients this recursive definition is useful in the computation of binomial coefficients because only addition and not multiplication of integers is needed to use this recursive definition pascal identity is the basis for a geometric arrangement of the binomial coefficients in a triangle as shown in figure the nth row in the triangle consists of the binomial coefficients n k n this triangle is known as pascal triangle pascal identity shows that when two adjacent binomial coefficients in this triangle are added the binomial coefficient in the next row between these two coefficients is produced blaise pascal blaise pascal exhibited his talents at an early age although his father who had made discoveries in analytic geometry kept mathematics books away from him to encourage other interests at 16 pascal discovered an important result concerning conic sections at he designed a calculating machine which he built and sold pascal along with fermat laid the foundations for the modern theory of probability in this work he made new discoveries concerning what is now called pascal triangle in pascal abandoned his mathematical pursuits to devote himself to theology after this he returned to mathematics only once one night distracted by a severe toothache he sought comfort by studying the mathematical properties of the cycloid miraculously his pain subsided which he took as a sign of divine approval of the study of mathematics other identities involving binomial coefficients we conclude this section with combinatorial proofs of two of the many identities enjoyed by the binomial coefficients theorem 3 remark this identity was discovered by mathematician alexandre théophile vandermonde in the eighteenth century proof suppose that there are m items in one set and n items in a second set then the total number of ways to pick r elements from the union of these sets is m n another way to pick r elements from the union is to pick k elements from the second set k r k m n total number of ways to pick r elements from the union also equals r m n we have found two expressions for the number of ways to pick r elements from the union of a set with m items and a set with n items equating them gives us vandermonde identity corollary 4 follows from vandermonde identity corollary 4 proof we use vandermonde identity with m r n to obtain k n n k n k n 2 the last equality was obtained using the identity n n alexandre théophile vandermonde because alexandre théophile vandermonde was a sickly child his physician father directed him to a career in music however he later developed an interest in mathematics his complete mathematical work consists of four papers published in these papers include fundamental contributions on the roots of equations on the theory of determinants and on the knight tour problem introduced in the exercises in section vandermonde interest in mathematics lasted for only 2 years afterward he published papers on harmony experiments with cold and the manufacture of steel he also became interested in politics joining the cause of the french revolution and holding several different positions in government theorem 4 we can prove combinatorial identities by counting bit strings with different properties as the proof of theorem 4 will demonstrate proof we use a combinatorial proof by example in section 3 the left hand side n we show that the right hand side counts the same objects by considering the cases corre sponding to the possible locations of the final in a string with r ones this final one must occur at position r r 2 or n furthermore if the last one is the kth bit there must be ones among the first positions consequently by example in section 3 there are k such bit strings summing over k with r k n we find that there are n k r k j r j exercises bit strings of length n containing exactly r ones note that the last step follows from the change of variables j k because the left hand side and the right hand side count the same objects they are equal this completes the proof find the expansion of x y 4 a using combinatorial reasoning as in example b using the binomial theorem 2 find the expansion of x y what is the row of pascal triangle containing the bino mial coefficients 9 k 9 show that if n is a positive integer then n n l n j i n n n b using the binomial theorem 3 find the expansion of x y 4 find the coefficient of in x y show that n for all positive integers n and all in tegers k with k n 16 a use exercise 14 and corollary to show that if n is after like terms are collected what is the coefficient of in x 7 what is the coefficient of in 2 x b conclude from part a that if n is a positive integer then 4n 8 9 s h ow that if n and k are integers with k n then 9 what is the coefficient of in the expansion of 3y k suppose that b is an integer with b 7 use the bino mial theorem and the appropriate row of pascal triangle to find the base b expansion of 4 that is the fourth give a formula for the coefficient of x in the expansion power of the number b in base notation of x x where k is an integer b b of x where k is an integer the row of pascal triangle containing the binomial co suppose that k and n are integers with k n prove the hexagon identity efficients k is k 120 252 210 120 n k n k n k n k n k n k use pascal identity to produce the row immediately fol lowing this row in pascal s triangle which relates terms in pascal s triangle that form a hexagon prove that if and are integers with then n n k k a using a combinatorial proof hint show that the two sides of the identity count the number of ways to select a subset with k elements from a set with n elements and then an element of this subset b using an algebraic proof based on the formula for n given in theorem 2 in section 3 22 prove the identity n r n n k whenever n r and in this exercise we will count the number of paths in the xy plane between the origin and point m n where m and n are nonnegative integers such that each path is made up of a series of steps where each step is a move one unit to the right or a move one unit upward no moves to the left or downward are allowed two such paths from to 3 are illustrated here 3 are nonnegative integers with r n and k r a using a combinatorial argument b using an argument based on the formula for the num ber of r combinations of a set with n elements show that if n and k are positive integers then n n n k 3 use this identity to construct an inductive definition of the binomial coefficients show that if p is a prime and k is an integer such that p k let n be a positive integer show that 2n 2 2 0 a show that each path of the type described can be rep resented by a bit string consisting of m and n where a 0 represents a move one unit to the right and a represents a move one unit upward 26 let n and k be integers with k n show that b conclude from part a that there are m n paths of n n 2n 2 2n the desired type k k k n 2 n use exercise to give an alternative proof of corollary 2 in section 3 which states that n n whenever k prove the hockeystick identity hint consider the number of paths of the type described in exercise from 0 0 k 0 n k n r to n k k and from 0 0 to k n k use exercise to prove theorem 4 hint count the number of paths with n steps of the type described in ex whenever n and r are positive integers a using a combinatorial argument b using pascal s identity show that if n is a positive integer then 2n 2 n ercise every such path must end at one of the points n k k for k 0 2 n use exercise to prove pascal s identity hint show that a path of the type described in exercise a using a combinatorial argument b by algebraic manipulation give a combinatorial proof that n 2 2 k n from 0 0 to n k k passes through either n k k or n k k but not through both use exercise to prove the hockeystick identity from hint count in two ways the number of ways to select a committee and to then select a leader of the committee paths from 0 0 to n r equals n r sec ond count the number of paths by summing the num give a combinatorial proof that n k n 2 n 2n ber of these paths that start by going k units upward for committee with n members from a group of n mathemat ics professors and n computer science professors such that the chairperson of the committee is a mathematics professor show that a nonempty set has the same number of subsets with an odd number of elements as it does subsets with an even number of elements prove the binomial theorem using mathematical induc tion give a combinatorial proof that if n is a positive inte ger then n n n n 2n 2 hint show that both sides count the ways to select a subset of a set of n ele ments together with two not necessarily distinct elements from this subset furthermore express the right hand side as n n 2n 2 n2n 39 determine a formula involving binomial coefficients for the nth term of a sequence if its initial terms are those listed hint looking at pascal s triangle will be helpful although infinitely many sequences start with a specified set of terms each of the following lists is the start of a sequence of the type desired a 3 55 66 b 4 35 56 120 220 c 2 252 48620 d 2 3 35 70 126 e 3 35 9 f 3 116280 generalized permutations and combinations introduction in many counting problems elements may be used repeatedly for instance a letter or digit may be used more than once on a license plate when a dozen donuts are selected each variety can be chosen repeatedly this contrasts with the counting problems discussed earlier in the chapter where we considered only permutations and combinations in which each item could be used at most once in this section we will show how to solve counting problems where elements may be used more than once also some counting problems involve indistinguishable elements for instance to count the number of ways the letters of the word success can be rearranged the placement of identical letters must be considered this contrasts with the counting problems discussed earlier where all elements were considered distinguishable in this section we will describe how to solve counting problems in which some elements are indistinguishable moreover in this section we will explain how to solve another important class of counting problems problems involving counting the ways distinguishable elements can be placed in boxes an example of this type of problem is the number of different ways poker hands can be dealt to four players taken together the methods described earlier in this chapter and the methods introduced in this section form a useful toolbox for solving a wide range of counting problems when the additional methods discussed in chapter 8 are added to this arsenal you will be able to solve a large percentage of the counting problems that arise in a wide range of areas of study permutations with repetition counting permutations when repetition of elements is allowed can easily be done using the product rule as example shows example how many strings of length r can be formed from the uppercase letters of the english alphabet solution by the product rule because there are 26 uppercase english letters and because each letter can be used repeatedly we see that there are strings of uppercase english letters of length r the number of r permutations of a set with n elements when repetition is allowed is given in theorem theorem proof there are n ways to select an element of the set for each of the r positions in the r permutation when repetition is allowed because for each choice all n objects are available hence by the product rule there are nr r permutations when repetition is allowed combinations with repetition consider these examples of combinations with repetition of elements allowed example 2 how many ways are there to select four pieces of fruit from a bowl containing apples oranges and pears if the order in which the pieces are selected does not matter only the type of fruit and not the individual piece matters and there are at least four pieces of each type of fruit in the bowl solution to solve this problem we list all the ways possible to select the fruit there are ways the solution is the number of 4 combinations with repetition allowed from a three element set apple orange pear to solve more complex counting problems of this type we need a general method for counting the r combinations of an n element set in example 3 we will illustrate such a method example 3 how many ways are there to select five bills from a cash box containing bills 2 bills bills bills bills bills and bills assume that the order in which the bills are chosen does not matter that the bills of each denomination are indistinguishable and that there are at least five bills of each type solution because the order in which the bills are selected does not matter and seven dif ferent types of bills can be selected as many as five times this problem involves counting combinations with repetition allowed from a set with seven elements listing all possibilities would be tedious because there are a large number of solutions instead we will illustrate the use of a technique for counting combinations with repetition allowed suppose that a cash box has seven compartments one to hold each type of bill as illustrated in figure these compartments are separated by six dividers as shown in the picture the choice of five bills corresponds to placing five markers in the compartments holding different types of bills figure 2 illustrates this correspondence for three different ways to select five bills where the six dividers are represented by bars and the five bills by stars the number of ways to select five bills corresponds to the number of ways to arrange six bars and five stars in a row with a total of positions consequently the number of ways to select the five bills is the number of ways to select the positions of the five stars from the positions this corresponds to the number of unordered selections of objects from a set of 2 figure cash box with seven types of bills figure 2 examples of ways to select five bills objects which can be done in c ways consequently there are c 462 ways to choose five bills from the cash box with seven types of bills theorem 2 generalizes this discussion theorem 2 proof each r combination of a set with n elements when repetition is allowed can be rep resented by a list of n bars and r stars the n bars are used to mark off n different cells with the ith cell containing a star for each time the ith element of the set occurs in the combination for instance a combination of a set with four elements is represented with three bars and six stars here represents the combination containing exactly two of the first element one of the second element none of the third element and three of the fourth element of the set as we have seen each different list containing n bars and r stars corresponds to an r combination of the set with n elements when repetition is allowed the number of such lists is c n r r because each list corresponds to a choice of the r positions to place the r stars from the n r positions that contain r stars and n bars the number of such lists is also equal to c n r n because each list corresponds to a choice of the n positions to place the n bars examples 4 show how theorem 2 is applied example 4 suppose that a cookie shop has four different kinds of cookies how many different ways can six cookies be chosen assume that only the type of cookie and not the individual cookies or the order in which they are chosen matters solution the number of ways to choose six cookies is the number of combinations of a set with four elements from theorem 2 this equals c 4 c 9 because c 9 c 9 3 9 8 7 84 2 3 there are 84 different ways to choose the six cookies theorem 2 can also be used to find the number of solutions of certain linear equations where the variables are integers subject to constraints this is illustrated by example example how many solutions does the equation have where and are nonnegative integers solution to count the number of solutions we note that a solution corresponds to a way of selecting items from a set with three elements so that items of type one items of type two and items of type three are chosen hence the number of solutions is equal to the number of combinations with repetition allowed from a set with three elements from theorem 2 it follows that there are c 3 c c 2 78 solutions the number of solutions of this equation can also be found when the variables are subject to constraints for instance we can find the number of solutions where the variables are inte gers with 2 and 3 a solution to the equation subject to these constraints corresponds to a selection of 11 items with items of type one items of type two and items of type three where in addition there is at least one item of type one two items of type two and three items of type three so a solution corresponds to a choice of one item of type one two of type two and three of type three together with a choice of five additional items of any type by theorem 2 this can be done in c 3 c 7 c 7 2 7 ways thus there are solutions of the equation subject to the given constraints example shows how counting the number of combinations with repetition allowed arises in determining the value of a variable that is incremented each time a certain type of nested loop is traversed table combinations and permutations with and without repetition type repetition allowed formula r permutations no n n r r combinations no n r n r r permutations yes nr r combinations yes n r r n example what is the value of k after the following pseudocode has been executed solution note that the initial value of k is 0 and that is added to k each time the nested loop is traversed with a sequence of integers i2 im such that im im i1 n the number of such sequences of integers is the number of ways to choose m integers from 2 n with repetition allowed to see this note that once such a sequence has been selected if we order the integers in the sequence in nondecreasing order this uniquely defines an assignment of im im i1 conversely every such assignment corresponds to a unique unordered set hence from theorem 2 it follows that k c n m m after this code has been executed the formulae for the numbers of ordered and unordered selections of r elements chosen with and without repetition allowed from a set with n elements are shown in table permutations with indistinguishable objects some elements may be indistinguishable in counting problems when this is the case care must be taken to avoid counting things more than once consider example 7 example 7 how many different strings can be made by reordering the letters of the word success solution because some of the letters of success are the same the answer is not given by the number of permutations of seven letters this word contains three ss two cs one u and one e to determine the number of different strings that can be made by reordering the letters first note that the three ss can be placed among the seven positions in c 7 3 different ways leaving four positions free then the two cs can be placed in c 4 2 ways leaving two free positions the u can be placed in c 2 ways leaving just one position free hence e can be placed in c way consequently from the product rule the number of different strings that can be made is c 7 3 c 4 2 c 2 c 7 3 4 4 2 2 2 0 7 3 2 420 we can prove theorem 3 using the same sort of reasoning as in example 7 theorem 3 proof to determine the number of permutations first note that the objects of type one can be placed among the n positions in c n ways leaving n positions free then the objects of type two can be placed in c n ways leaving n positions free continue placing the objects of type three type k until at the last stage nk objects of type k can be placed in c n nk nk ways hence by the product rule the total number of different permutations is c n c n c n nk nk n n n nk n n n nk distributing objects into boxes nk 0 many counting problems can be solved by enumerating the ways objects can be placed into boxes where the order these objects are placed into the boxes does not matter the objects can be either distinguishable that is different from each other or indistinguishable that is considered identical distinguishable objects are sometimes said to be labeled whereas indistinguishable objects are said to be unlabeled similarly boxes can be distinguishable that is different or indinguishable that is identical distinguishable boxes are often said to be labeled while indistinguishable boxes are said to be unlabeled when you solve a counting problem using the model of distributing objects into boxes you need to determine whether the objects are distinguishable and whether the boxes are distinguishable although the context of the counting problem makes these two decisions clear counting problems are sometimes ambiguous and it may be unclear which model applies in such a case it is best to state whatever assumptions you are making and explain why the particular model you choose conforms to your assumptions we will see that there are closed formulae for counting the ways to distribute objects distinguishable or indistinguishable into distinguishable boxes we are not so lucky when we count the ways to distribute objects distinguishable or indistinguishable into indistinguishable boxes there are no closed formulae to use in these cases distinguishable objects and distinguishable boxes we first consider the case when distinguishable objects are placed into distinguishable boxes consider example 8 in which the objects are cards and the boxes are hands of players example 8 how many ways are there to distribute hands of cards to each of four players from the standard deck of cards solution we will use the product rule to solve this problem to begin note that the first player can be dealt cards in c ways the second player can be dealt cards in c ways because only cards are left the third player can be dealt cards in c ways finally the fourth player can be dealt cards in c ways hence the total number of ways to deal four players cards each is c 5 c 5 c 5 c 5 5 5 5 5 5 5 5 5 remark the solution to example 8 equals the number of permutations of objects with 5 in distinguishable objects of each of four different types and 32 objects of a fifth type this equality can be seen by defining a one to one correspondence between permutations of this type and dis tributions of cards to the players to define this correspondence first order the cards from to then cards dealt to the first player correspond to the cards in the positions assigned to objects of the first type in the permutation similarly cards dealt to the second third and fourth players re spectively correspond to cards in the positions assigned to objects of the second third and fourth type respectively the cards not dealt to any player correspond to cards in the positions assigned to objects of the fifth type the reader should verify that this is a one to one correspondence example 8 is a typical problem that involves distributing distinguishable objects into dis tinguishable boxes the distinguishable objects are the cards and the five distinguishable boxes are the hands of the four players and the rest of the deck counting problems that involve distributing distinguishable objects into boxes can be solved using theorem 4 theorem 4 theorem 4 can be proved using the product rule we leave the details as exercise it can also be proved see exercise by setting up a one to one correspondence between the permutations counted by theorem 3 and the ways to distribute objects counted by theorem 4 indistinguishable objects and distinguishable boxes counting the number of ways of placing n indistinguishable objects into k distinguishable boxes turns out to be the same as counting the number of n combinations for a set with k elements when repeti tions are allowed the reason behind this is that there is a one to one correspondence between n combinations from a set with k elements when repetition is allowed and the ways to place n indistinguishable balls into k distinguishable boxes to set up this correspondence we put a ball in the ith bin each time the ith element of the set is included in the n combination example 9 how many ways are there to place indistinguishable balls into eight distinguishable bins solution the number of ways to place indistinguishable balls into eight bins equals the num ber of combinations from a set with eight elements when repetition is allowed consequently there are c 8 c 7 this means that there are c n r n ways to place r indistinguishable objects into n distinguishable boxes distinguishable objects and indistinguishable boxes counting the ways to place n distinguishable objects into k indistinguishable boxes is more difficult than counting the ways to place objects distinguishable or indistinguishable objects into distinguishable boxes we illustrate this with an example example how many ways are there to put four different employees into three indistinguishable offices when each office can contain any number of employees solution we will solve this problem by enumerating all the ways these employees can be placed into the offices we represent the four employees by a b c and d first we note that we can distribute employees so that all four are put into one office three are put into one office and a fourth is put into a second office two employees are put into one office and two put into a second office and finally two are put into one office and one each put into the other two offices each way to distribute these employees to these offices can be represented by a way to partition the elements a b c and d into disjoint subsets we can put all four employees into one office in exactly one way represented by a b c d we can put three employees into one office and the fourth employee into a different office in exactly four ways represented by a b c d a b d c a c d b and b c d a we can put two employees into one office and two into a second office in exactly three ways represented by a b c d a c b d and a d b c finally we can put two employees into one office and one each into each of the remaining two offices in six ways represented by a b c d a c b d a d b c b c a d b d a c and c d a b counting all the possibilities we find that there are 14 ways to put four different employees into three indistinguishable offices another way to look at this problem is to look at the number of offices into which we put employees note that there are six ways to put four different employees into three indistinguishable offices so that no office is empty seven ways to put four different employees into two indistinguishable offices so that no office is empty and one way to put four employees into one office so that it is not empty there is no simple closed formula for the number of ways to distribute n distinguishable objects into j indistinguishable boxes however there is a formula involving a summation which we will now describe let s n j denote the number of ways to distribute n distinguish able objects into j indistinguishable boxes so that no box is empty the numbers s n j are called stirling numbers of the second kind for instance example shows that s 4 3 s 4 2 7 and s 4 we see that the number of ways to distribute n distinguishable objects into k indistinguishable boxes where the number of boxes that are nonempty equals k k 2 or equals k s n j for instance following the reasoning in example the number of ways to distribute four distinguishable objects into three indistinguishable boxes equals s 4 s 4 2 s 4 3 7 14 using the inclusion exclusion principle see section 8 it can be shown that s n j j j i 0 i j j i n consequently the number of ways to distribute n distinguishable objects into k indistinguishable boxes equals s n j i j j i n j j j i 0 i remark the reader may be curious about the stirling numbers of the first kind a combinatorial definition of the signless stirling numbers of the first kind the absolute values of the stirling numbers of the first kind can be found in the preamble to exercise 47 in the supplementary exercises for the definition of stirling numbers of the first kind for more information about stirling numbers of the second kind and to learn more about stirling numbers of the first kind and the relationship between stirling numbers of the first and second kind see combinatorics textbooks such as and and chapter in indistinguishable objects and indistinguishable boxes some counting problems can be solved by determining the number of ways to distribute indistinguishable objects into indistinguishable boxes we illustrate this principle with an example example 11 how many ways are there to pack six copies of the same book into four identical boxes where a box can contain as many as six books solution we will enumerate all ways to pack the books for each way to pack the books we will list the number of books in the box with the largest number of books followed by the numbers of books in each box containing at least one book in order of decreasing number of books in a box the ways we can pack the books are 6 5 4 2 4 3 3 3 2 3 1 1 1 2 2 2 2 2 1 1 for example 4 1 1 indicates that one box contains four books a second box contains a single book and a third box contains a single book and the fourth box is empty we conclude that there are nine allowable ways to pack the books because we have listed them all observe that distributing n indistinguishable objects into k indistinguishable boxes is the same as writing n as the sum of at most k positive integers in nonincreasing order if a2 aj n where a2 aj are positive integers with a2 aj we say that a2 aj is a partition of the positive integer n into j positive integers we see that if pk n is the number of partitions of n into at most k positive integers then there are pk n ways to distribute n indistinguishable objects into k indistinguishable boxes no simple closed formula exists for this number for more information about partitions of positive integers see exercises 1 in how many different ways can five elements be selected in order from a set with three elements when repetition is allowed 2 in how many different ways can five elements be selected in order from a set with five elements when repetition is allowed 3 how many strings of six letters are there 4 every day a student randomly chooses a sandwich for lunch from a pile of wrapped sandwiches if there are six kinds of sandwiches how many different ways are there for the student to choose sandwiches for the seven days of a week if the order in which the sandwiches are chosen matters 5 how many ways are there to assign three jobs to five employees if each employee can be given more than one job 6 how many ways are there to select five unordered ele ments from a set with three elements when repetition is allowed 7 how many ways are there to select three unordered el ements from a set with five elements when repetition is allowed 8 how many different ways are there to choose a dozen donuts from the 21 varieties at a donut shop 9 a bagel shop has onion bagels poppy seed bagels egg bagels salty bagels pumpernickel bagels sesame seed bagels raisin bagels and plain bagels how many ways are there to choose a six bagels b a dozen bagels c two dozen bagels d a dozen bagels with at least one of each kind e a dozen bagels with at least three egg bagels and no more than two salty bagels a croissant shop has plain croissants cherry croissants chocolate croissants almond croissants apple croissants and broccoli croissants how many ways are there to choose a a dozen croissants b three dozen croissants c two dozen croissants with at least two of each kind d two dozen croissants with no more than two broccoli croissants e two dozen croissants with at least five chocolate crois sants and at least three almond croissants f two dozen croissants with at least one plain croissant at least two cherry croissants at least three choco late croissants at least one almond croissant at least two apple croissants and no more than three broccoli croissants 11 how many ways are there to choose eight coins from a piggy bank containing identical pennies and iden tical nickels how many different combinations of pennies nickels dimes quarters and half dollars can a piggy bank con tain if it has coins in it a book publisher has copies of a discrete mathemat ics book how many ways are there to store these books in their three warehouses if the copies of the book are indistinguishable 14 how many solutions are there to the equation 17 where and are nonnegative integers how many solutions are there to the equation x4 21 where xi i 1 2 3 4 5 is a nonnegative integer such that a 1 b xi 2 for i 1 2 3 4 5 c 0 d 0 3 1 4 and 16 how many solutions are there to the equation x4 where xi i 1 2 3 4 5 6 is a nonnegative integer such that a xi 1 for i 1 2 3 4 5 6 b 1 2 3 x4 4 5 and 6 c 5 d 8 and 8 17 how many strings of ternary digits 0 1 or 2 are there that contain exactly two three and five how many strings of decimal digits are there that con tain two four three one 3 two three two and three suppose that a large family has 14 children including two sets of identical triplets three sets of identical twins and two individual children how many ways are there to seat these children in a row of chairs if the identical triplets or twins cannot be distinguished from one another how many solutions are there to the inequality 11 where and are nonnegative integers hint in troduce an auxiliary variable x4 such that x4 11 21 how many ways are there to distribute six indistinguish able balls into nine distinguishable bins 22 how many ways are there to distribute indistinguish able balls into six distinguishable bins how many ways are there to distribute distinguishable objects into six distinguishable boxes so that two objects are placed in each box how many ways are there to distribute distinguish able objects into five distinguishable boxes so that the boxes have one two three four and five objects in them respectively how many positive integers less than 1 000 000 have the sum of their digits equal to 19 26 how many positive integers less than 1 000 000 have ex actly one digit equal to 9 and have a sum of digits equal to 27 there are questions on a discrete mathematics final exam how many ways are there to assign scores to the problems if the sum of the scores is and each question is worth at least 5 points show that there are c n r qr 1 n qr different unordered selec how many ways are there to deal hands of seven cards to each of five players from a standard deck of cards 42 in bridge the cards of a standard deck are dealt to four players how many different ways are there to deal bridge hands to four players 43 how many ways are there to deal hands of five cards to each of six players from a deck containing different cards in how many ways can a dozen books be placed on four distinguishable shelves a if the books are indistinguishable copies of the same title tions of n objects of r different types that include at least objects of type one objects of type two and qr objects of type r how many different bit strings can be transmitted if the string must begin with a 1 bit must include three ad ditional 1 bits so that a total of four 1 bits is sent must include a total of 0 bits and must have at least two 0 bits following each 1 bit 30 how many different strings can be made from the letters in mississippi using all the letters how many different strings can be made from the letters in abracadabra using all the letters 32 how many different strings can be made from the letters in aardvark using all the letters if all three as must be consecutive how many different strings can be made from the letters in orono using some or all of the letters 34 how many strings with five or more characters can be formed from the letters in seeress 35 how many strings with seven or more characters can be formed from the letters in evergreen how many different bit strings can be formed using six and eight a student has three mangos two papayas and two kiwi fruits if the student eats one piece of fruit each day and only the type of fruit matters in how many different ways can these fruits be consumed a professor packs her collection of issues of a mathe matics journal in four boxes with issues per box how many ways can she distribute the journals if a each box is numbered so that they are distinguish able b the boxes are identical so that they cannot be distin guished 39 how many ways are there to travel in xyz space from the origin 0 0 0 to the point 4 3 5 by taking steps one unit in the positive x direction one unit in the positive y direction or one unit in the positive z direction moving in the negative x y or z direction is prohibited so that no backtracking is allowed how many ways are there to travel in xyzw space from the origin 0 0 0 0 to the point 4 3 5 4 by taking steps one unit in the positive x positive y positive z or positive w direction b if no two books are the same and the positions of the books on the shelves matter hint break this into tasks placing each book separately start with the sequence 1 2 3 4 to represent the shelves repre sent the books by bi i 1 2 place b1 to the right of one of the terms in 1 2 3 4 then successively place b2 b3 and 45 how many ways can n books be placed on k distinguish able shelves a if the books are indistinguishable copies of the same title b if no two books are the same and the positions of the books on the shelves matter a shelf holds books in a row how many ways are there to choose five books so that no two adjacent books are chosen hint represent the books that are chosen by bars and the books not chosen by stars count the number of sequences of five bars and seven stars so that no two bars are adjacent 47 use the product rule to prove theorem 4 by first placing objects in the first box then placing objects in the second box and so on prove theorem 4 by first setting up a one to one cor respondence between permutations of n objects with ni indistinguishable objects of type i i 1 2 3 k and the distributions of n objects in k boxes such that ni ob jects are placed in box i i 1 2 3 k and then ap plying theorem 3 49 in this exercise we will prove theorem 2 by set ting up a one to one correspondence between the set of r combinations with repetition allowed of s 1 2 3 n and the set of r combinations of the set t 1 2 3 n r 1 a arrange the elements in an r combination with rep etition allowed of s into an increasing sequence xr show that the sequence formed by adding k 1 to the kth term is strictly increasing conclude that this sequence is made up of r distinct elements from t b show that the procedure described in a defines a one to one correspondence between the set of r combinations with repetition allowed of s and the r combinations of t hint show the corre spondence can be reversed by associating to the r combination x2 xr of t with 1 x2 xr n r 1 the r combination with repetition allowed from s formed by subtracting k 1 from the kth element c conclude that there are c n r 1 r r combinations with repetition allowed from a set with n elements how many ways are there to distribute five distinguish able objects into three indistinguishable boxes how many ways are there to distribute six distinguishable objects into four indistinguishable boxes so that each of the boxes contains at least one object how many ways are there to put five temporary employ ees into four identical offices how many ways are there to put six temporary employ ees into four identical offices so that there is at least one temporary employee in each of these four offices how many ways are there to distribute five indistinguish able objects into three indistinguishable boxes 55 how many ways are there to distribute six indistinguish able objects into four indistinguishable boxes so that each of the boxes contains at least one object 56 how many ways are there to pack eight identical dvds into five indistinguishable boxes so that each box contains at least one dvd how many ways are there to pack nine identical dvds into three indistinguishable boxes so that each box con tains at least two dvds c the balls are unlabeled but the boxes are labeled d both the balls and boxes are unlabeled 60 suppose that a basketball league has 32 teams split into two conferences of 16 teams each each conference is split into three divisions suppose that the north central division has five teams each of the teams in the north central division plays four games against each of the other teams in this division three games against each of the 11 remaining teams in the conference and two games against each of the 16 teams in the other conference in how many different orders can the games of one of the teams in the north central division be scheduled suppose that a weapons inspector must inspect each of five different sites twice visiting one site per day the inspector is free to select the order in which to visit these sites but cannot visit site x the most suspicious site on two consecutive days in how many different orders can the inspector visit these sites 62 how many different terms are there in the expansion of x2 xm n after all terms with identical sets of exponents are added prove the multinomial theorem if n is a positive inte ger then x2 xm n c n nm xnm how many ways are there to distribute five balls into seven boxes if each box must have at most one ball in it if nm n where 1 2 m a both the balls and boxes are labeled c n n n b the balls are labeled but the boxes are unlabeled c the balls are unlabeled but the boxes are labeled m is a multinomial coefficient nm d both the balls and boxes are unlabeled 59 how many ways are there to distribute five balls into three boxes if each box must have at least one ball in it if a both the balls and boxes are labeled b the balls are labeled but the boxes are unlabeled 64 find the expansion of x y z 4 65 find the coefficient of in x y z 66 how many terms are there in the expansion of x y z 6 6 generating permutations and combinations introduction methods for counting various types of permutations and combinations were described in the previous sections of this chapter but sometimes permutations or combinations need to be gener ated not just counted consider the following three problems first suppose that a salesperson must visit six different cities in which order should these cities be visited to minimize total travel time one way to determine the best order is to determine the travel time for each of the 6 720 different orders in which the cities can be visited and choose the one with the smallest travel time second suppose we are given a set of six positive integers and wish to find a subset of them that has as their sum if such a subset exists one way to find these numbers is to generate all 26 64 subsets and check the sum of their elements third suppose a laboratory has employees a group of of these employees with a particular set of 25 skills is needed for a project each employee can have one or more of these skills one way to find such a set of employees is to generate all sets of of these employees and check whether they have the desired skills these examples show that it is often necessary to generate permutations and combinations to solve problems generating permutations any set with n elements can be placed in one to one correspondence with the set 1 2 3 n we can list the permutations of any set of n elements by generating the permutations of the n smallest positive integers and then replacing these integers with the corresponding elements many different algorithms have been developed to generate the n permutations of this set we will describe one of these that is based on the lexicographic or dictionary ordering of the set of permutations of 1 2 3 n in this ordering the permutation an precedes the permutation of bn if for some k with 1 k n b1 a2 b2 ak 1 bk 1 and ak bk in other words a permutation of the set of the n smallest positive integers precedes in lexicographic order a second permutation if the number in this permutation in the first position where the two permutations disagree is smaller than the number in that position in the second permutation example 1 the permutation of the set 1 2 3 4 5 precedes the permutation because these permutations agree in the first two positions but the number in the third position in the first permutation 4 is smaller than the number in the third position in the second permutation 5 similarly the permutation precedes an algorithm for generating the permutations of 1 2 n can be based on a proce dure that constructs the next permutation in lexicographic order following a given permutation an we will show how this can be done first suppose that an 1 an interchange an 1 and an to obtain a larger permutation no other permutation is both larger than the original per mutation and smaller than the permutation obtained by interchanging an 1 and an for instance the next larger permutation after is on the other hand if an 1 an then a larger permutation cannot be obtained by interchanging these last two terms in the permutation look at the last three integers in the permutation if an 2 an 1 then the last three integers in the permutation can be rearranged to obtain the next largest permutation put the smaller of the two integers an 1 and an that is greater than an 2 in position n 2 then place the remaining integer and an 2 into the last two positions in increasing order for instance the next larger permutation after is on the other hand if an 2 an 1 and an 1 an then a larger permutation cannot be obtained by permuting the last three terms in the permutation based on these observations a general method can be described for producing the next larger permutation in increasing order following a given permutation an first find the integers aj and aj 1 with aj aj 1 aj 1 aj 2 an that is the last pair of adjacent integers in the permutation where the first integer in the pair is smaller than the second then the next larger permutation in lexicographic order is obtained by putting in the j th position the least integer among aj 1 aj 2 and an that is greater than aj and listing in increasing order the rest of the integers aj aj 1 an in positions j 1 to n it is easy to see that there is no other permutation larger than the permutation an but smaller than the new permutation produced the verification of this fact is left as an exercise for the reader example 2 what is the next permutation in lexicographic order after solution the last pair of integers aj and aj 1 where aj aj 1 is 2 and 5 the least integer to the right of 2 that is greater than 2 in the permutation is 4 hence 4 is placed in the third position then the integers 2 5 and 1 are placed in order in the last three positions giving as the last three positions of the permutation hence the next permutation is to produce the n permutations of the integers 1 2 3 n begin with the smallest permu tation in lexicographic order namely n and successively apply the procedure described for producing the next larger permutation of n 1 times this yields all the permutations of the n smallest integers in lexicographic order example 3 generate the permutations of the integers 1 2 3 in lexicographic order solution begin with the next permutation is obtained by interchanging 3 and 2 to obtain next because 3 2 and 1 3 permute the three integers in put the smaller of 3 and 2 in the first position and then put 1 and 3 in increasing order in positions 2 and 3 to obtain this is followed by obtained by interchanging 1 and 3 because 1 3 the next larger permutation has 3 in the first position followed by 1 and 2 in increasing order namely finally interchange 1 and 2 to obtain the last permutation we have generated the permutations of 1 2 3 in lexicographic order they are 132 231 and algorithm 1 displays the procedure for finding the next permutation in lexicographic order after a permutation that is not n n 1 n 2 2 1 which is the largest permutation generating combinations how can we generate all the combinations of the elements of a finite set because a combination is just a subset we can use the correspondence between subsets of a2 an and bit strings of length n recall that the bit string corresponding to a subset in position k if ak is in the subset and in this position if ak is not in the subset if all the bit strings of length n can be listed then by the correspondence between subsets and bit strings a list of all the subsets is obtained recall that a bit string of length n is also the binary expansion of an integer between 0 and 2n 1 the 2n bit strings can be listed in order of their increasing size as integers in their binary expansions to produce all binary expansions of length n start with the bit string 000 00 with n zeros then successively find the next expansion until the bit string 11 is obtained at each stage the next binary expansion is found by locating the first position from the right that is not a 1 then changing all the to the right of this position to and making this first 0 from the right a 1 example 4 find the next bit string after 0010 0111 solution the first bit from the right that is not a 1 is the fourth bit from the right change this bit to a 1 and change all the following bits to this produces the next larger bit string 0010 the procedure for producing the next larger bit string after bn 2 is given as algorithm 2 next an algorithm for generating the r combinations of the set 1 2 3 n will be given an r combination can be represented by a sequence containing the elements in the subset in increasing order the r combinations can be listed using lexicographic order on these sequences in this lexicographic ordering the first r combination is 1 2 r 1 r and the last r combination is n r 1 n r 2 n 1 n the next r combination after ar can be obtained in the following way first locate the last element ai in the sequence such that ai n r i then replace ai with ai 1 and aj with ai j i 1 for j i 1 i 2 r it is left for the reader to show that this produces the next larger r combination in lexicographic order this procedure is illustrated with example 5 example 5 find the next larger 4 combination of the set 1 2 3 4 5 6 after 1 2 5 6 solution the last term among the terms ai with 1 a2 2 5 and 6 such that ai 6 4 i is a2 2 to obtain the next larger 4 combination increment a2 by 1 to obtain a2 3 then set 3 1 4 and 3 2 5 hence the next larger 4 combination is 1 3 4 5 algorithm 3 displays pseudocode for this procedure exercises 1 place these permutations of 1 2 3 4 5 in lexico graphic order 23451 45213 2 place these permutations of 1 2 3 4 5 6 in lexico graphic order 156423 314562 3 the name of a file in a computer directory consists of three uppercase letters followed by a digit where each letter is either a b or c and each digit is either 1 or 2 list the name of these files in lexicographic order where we order letters using the usual alphabetic order of letters 4 suppose that the name of a file in a computer directory consists of three digits followed by two lowercase letters and each digit is 0 1 or 2 and each letter is either a or b list the name of these files in lexicographic order where we order letters using the usual alphabetic order of letters 5 find the next larger permutation in lexicographic order after each of these permutations a b c d e f 6 find the next larger permutation in lexicographic order after each of these permutations a b c d e f 7 use algorithm 1 to generate the 24 permutations of the first four positive integers in lexicographic order 8 use algorithm 2 to list all the subsets of the set 1 2 3 4 9 use algorithm 3 to list all the 3 combinations of 1 2 3 4 5 10 show that algorithm 1 produces the next larger permu tation in lexicographic order 11 show that algorithm 3 produces the next larger r combination in lexicographic order after a given r combination develop an algorithm for generating the r permutations of a set of n elements 13 list all 3 permutations of 1 2 3 4 5 the remaining exercises in this section develop another algo rithm for generating the permutations of 1 2 3 n this algorithm is based on cantor expansions of integers every nonnegative integer less than n has a unique cantor expan sion an 1 n 1 where ai is a nonnegative integer not exceeding i for i 1 2 n 1 the integers a2 an 1 are called the cantor digits of this integer given a permutation of 1 2 n let ak 1 k 2 3 n be the number of integers less than k that fol low k in the permutation for instance in the permutation is the number of integers less than 2 that fol low 2 so 1 similarly for this example a2 2 3 and 0 consider the function from the set of permu tations of 1 2 3 n to the set of nonnegative integers less than n that sends a permutation to the integer that has a2 an 1 defined in this way as its cantor digits 14 find the cantor digits a2 an 1 that correspond to these permutations a b c 15 show that the correspondence described in the pream ble is a bijection between the set of permutations of 1 2 3 n and the nonnegative integers less than n 16 find the permutations of 1 2 3 4 5 that correspond to these integers with respect to the correspondence be tween cantor expansions and permutations as described in the preamble to exercise 14 a 3 b c 111 17 develop an algorithm for producing all permutations of a set of n elements based on the correspondence described in the preamble to exercise 14 key terms and results terms combinatorics the study of arrangements of objects enumeration the counting of arrangements of objects tree diagram a diagram made up of a root branches leaving the root and other branches leaving some of the endpoints of branches permutation an ordered arrangement of the elements of a set r permutation an ordered arrangement of r elements of a set p n r the number of r permutations of a set with n elements r combination an unordered selection of r elements of a set c n r the number of r combinations of a set with n elements binomial coefficient n also the number of r combinations r of a set with n elements combinatorial proof a proof that uses counting arguments rather than algebraic manipulation to prove a result pascal s triangle a representation of the binomial coeffi cients where the ith row of the triangle contains i for s n j the stirling number of the second kind denoting the number of ways to distribute n distinguishable objects into j indistinguishable boxes so that no box is empty results product rule for counting the number of ways to do a proce dure that consists of two tasks is the product of the number of ways to do the first task and the number of ways to do subtraction rule for counting or inclusion exclusion for sets if a task can be done in either ways or n2 ways then the number of ways to do the task is n2 minus the number of ways to do the task that are common to the two different ways subtraction rule or inclusion exclusion for sets the number of elements in the union of two sets is the sum of the number of elements in these sets minus the number of elements in their intersection division rule for counting there are n d ways to do a task if it can be done using a procedure that can be carried out in n ways and for every way w exactly d of the n ways correspond to way w division rule for sets suppose that a finite set a is the union of n disjoint subsets each with d elements then n a d the pigeonhole principle when more than k objects are placed in k boxes there must be a box containing more than one object the generalized pigeonhole principle when n objects are placed in k boxes there must be a box containing at least in k objects p n r n n r c n r n n r r n r pascal s identity n 1 n n the second task after the first task has been done k k n n n k k sian product of finite sets is the product of the number of elements in each set sum rule for counting the number of ways to do a task in one of two ways is the sum of the number of ways to do these tasks if they cannot be done simultaneously sum rule for sets the number of elements in the union of pairwise disjoint finite sets is the sum of the numbers of elements in these sets there are nr r permutations of a set with n elements when repetition is allowed there are c n r 1 r r combinations of a set with n ele ments when repetition is allowed there are n n2 nk permutations of n objects of k types where there are ni indistinguishable objects of type i for i 1 2 3 k the algorithm for generating the permutations of the set 1 2 n review questions 1 explain how the sum and product rules can be used to find the number of bit strings with a length not exceeding 10 2 explain how to find the number of bit strings of length not exceeding 10 that have at least one 0 bit 3 a how can the product rule be used to find the number of functions from a set with m elements to a set with n elements b how many functions are there from a set with five elements to a set with 10 elements c how can the product rule be used to find the number of one to one functions from a set with m elements to a set with n elements d how many one to one functions are there from a set with five elements to a set with 10 elements e how many onto functions are there from a set with five elements to a set with 10 elements 4 how can you find the number of possible outcomes of a playoff between two teams where the first team that wins four games wins the playoff 5 how can you find the number of bit strings of length ten that either begin with or end with 6 a state the pigeonhole principle b explain how the pigeonhole principle can be used to show that among any 11 integers at least two must have the same last digit 7 a state the generalized pigeonhole principle b explain how the generalized pigeonhole principle can be used to show that among any integers there are at least ten that end with the same digit 8 a what is the difference between an r combination and an r permutation of a set with n elements b derive an equation that relates the number of r com binations and the number of r permutations of a set with n elements c how many ways are there to select six students from a class of 25 to serve on a committee d how many ways are there to select six students from a class of 25 to hold six different executive positions on a committee 9 a what is pascal s triangle b how can a row of pascal s triangle be produced from the one above it 10 what is meant by a combinatorial proof of an identity how is such a proof different from an algebraic one 11 explain how to prove pascal s identity using a combina torial argument a state the binomial theorem b explain how to prove the binomial theorem using a combinatorial argument c find the coefficient of in the expansion of 2x 5y supplementary exercises 1 how many ways are there to choose 6 items from 10 dis tinct items when a the items in the choices are ordered and repetition is not allowed b the items in the choices are ordered and repetition is allowed c the items in the choices are unordered and repetition is not allowed d the items in the choices are unordered and repetition is allowed 13 a explain how to find a formula for the number of ways to select r objects from n objects when repetition is allowed and order does not matter b how many ways are there to select a dozen objects from among objects of five different types if objects of the same type are indistinguishable c how many ways are there to select a dozen objects from these five different types if there must be at least three objects of the first type d how many ways are there to select a dozen objects from these five different types if there cannot be more than four objects of the first type e how many ways are there to select a dozen objects from these five different types if there must be at least two objects of the first type but no more than three objects of the second type 14 a let n and r be positive integers explain why the number of solutions of the equation x2 xn r where xi is a nonnegative integer for i 1 2 3 n equals the number of r combinations of a set with n elements b how many solutions in nonnegative integers are there to the equation x2 x4 17 how many solutions in positive integers are there to the equation in part b 15 a derive a formula for the number of permutations of n objects of k different types where there are indis tinguishable objects of type one n2 indistinguishable objects of type two and nk indistinguishable ob jects of type k b how many ways are there to order the letters of the word indiscreetness 16 describe an algorithm for generating all the permutations of the set of the n smallest positive integers 17 a how many ways are there to deal hands of five cards to six players from a standard card deck b how many ways are there to distribute n distinguish able objects into k distinguishable boxes so that ni objects are placed in box i 18 describe an algorithm for generating all the combinations of the set of the n smallest positive integers 2 how many ways are there to choose 10 items from 6 dis tinct items when a the items in the choices are ordered and repetition is not allowed b the items in the choices are ordered and repetition is allowed c the items in the choices are unordered and repetition is not allowed d the items in the choices are unordered and repetition is allowed 3 a test contains true false questions how many dif ferent ways can a student answer the questions on the test if answers may be left blank 4 how many strings of length 10 either start with 000 or end with 1111 5 how many bit strings of length 10 over the alphabet a b c have either exactly three as or exactly four bs 6 the internal telephone numbers in the phone system on a campus consist of five digits with the first digit not equal to zero how many different numbers can be assigned in this system 7 an ice cream parlor has different flavors 8 different kinds of sauce and toppings a in how many different ways can a dish of three scoops of ice cream be made where each flavor can be used more than once and the order of the scoops does not matter b how many different kinds of small sundaes are there if a small sundae contains one scoop of ice cream a sauce and a topping c how many different kinds of large sundaes are there if a large sundae contains three scoops of ice cream where each flavor can be used more than once and the order of the scoops does not matter two kinds of sauce where each sauce can be used only once and the order of the sauces does not matter and three top pings where each topping can be used only once and the order of the toppings does not matter 8 how many positive integers less than a have exactly three decimal digits b have an odd number of decimal digits c have at least one decimal digit equal to 9 d have no odd decimal digits e have two consecutive decimal digits equal to 5 f are palindromes that is read the same forward and backward 9 when the numbers from 1 to are written out in deci mal notation how many of each of these digits are used a 0 b 1 c 2 d 9 10 there are signs of the zodiac how many people are needed to guarantee that at least six of these people have the same sign 11 a fortune cookie company makes different fortunes a student eats at a restaurant that uses fortunes from this company and gives each customer one fortune cookie at the end of a meal what is the largest possible number of times that the student can eat at the restaurant without getting the same fortune four times how many people are needed to guarantee that at least two were born on the same day of the week and in the same month perhaps in different years 13 show that given any set of 10 positive integers not ex ceeding there exist at least two different five element subsets of this set that have the same sum 14 a package of baseball cards contains cards how many packages must be purchased to ensure that two cards in these packages are identical if there are a total of different cards 15 a how many cards must be chosen from a standard deck of cards to guarantee that at least two of the four aces are chosen b how many cards must be chosen from a standard deck of cards to guarantee that at least two of the four aces and at least two of the 13 kinds are chosen c how many cards must be chosen from a standard deck of cards to guarantee that there are at least two cards of the same kind d how many cards must be chosen from a standard deck of cards to guarantee that there are at least two cards of each of two different kinds 16 show that in any set of n 1 positive integers not exceed ing 2n there must be two that are relatively prime 17 show that in a sequence of m integers there exists one or more consecutive terms with a sum divisible by m 18 show that if five points are picked in the interior of a square with a side length of 2 then at least two of these points are no farther than 2 apart 19 show that the decimal expansion of a rational number must repeat itself from some point onward once a computer worm infects a personal computer via an infected e mail message it sends a copy of itself to e mail addresses it finds in the electronic message mailbox on this personal computer what is the maximum number of different computers this one computer can infect in the time it takes for the infected message to be forwarded five times 21 how many ways are there to choose a dozen donuts from varieties a if there are no two donuts of the same variety b if all donuts are of the same variety c if there are no restrictions d if there are at least two varieties among the dozen donuts chosen e if there must be at least six blueberry filled donuts f if there can be no more than six blueberry filled donuts 22 find n if a p n 2 110 b p n n c p n 4 12p n 2 23 find n if a c n 2 45 b c n 3 p n 2 c c n 5 c n 2 24 show that if n and r are nonnegative integers and n r then p n 1 r p n r n 1 n 1 r 25 suppose that s is a set with n elements how many ordered pairs a b are there such that a and b are subsets of s with a b hint show that each element of s belongs to a b a or s b 26 give a combinatorial proof of corollary 2 of section 6 4 by setting up a correspondence between the subsets of a set with an even number of elements and the subsets of this set with an odd number of elements hint take an el ement a in the set set up the correspondence by putting a in the subset if it is not already in it and taking it out if it is in the subset 27 let n and r be integers with 1 r n show that c n r 1 c n 2 r 1 2c n 1 r 1 c n r 1 28 prove using mathematical induction that n c j 2 in every subset ai there are elements that have been assigned each color let m d be the largest integer such that every collection of fewer than m d sets each con taining d elements is 2 colorable a show that the collection of all subsets with d elements of a set s with 1 elements is not 2 colorable b show that m 2 3 c show that m 3 7 hint show that the collec tion 1 3 5 1 2 6 1 4 7 2 3 4 2 5 7 3 6 7 4 5 6 is not 2 colorable then show that all collections of six sets with three elements each are 2 colorable 35 a professor writes 20 multiple choice questions each with the possible answer a b c or d for a discrete mathematics test if the number of questions with a b c and d as their answer is 8 3 4 and 5 respectively how many different answer keys are possible if the questions can be placed in any order 36 how many different arrangements are there of eight peo ple seated at a round table where two arrangements are 29 show that if n is an integer then how many ways are there to assign 24 students to five k 0 3k n 4n faculty advisors 38 how many ways are there to choose a dozen apples from a bushel containing 20 indistinguishable delicious apples 30 show that n 1 n 1 n if n is an integer with 20 indistinguishable macintosh apples and 20 indistin show that n 2 n 1 n 1 n if n is an in kind must be chosen 3 x 17 where x x and x are nonnegative integers 32 in this exercise we will derive a formula for the sum of 3 with 1 2 3 the squares of the n smallest positive integers we will count the number of triples i j k where i j and k are integers such that 0 i k 0 j k and 1 k n in two ways a show that there are such triples with a fixed k de duce that there are n k2 such triples b show that the number of such triples with 0 i j k and the number of such triples with 0 j i k both equal c n 1 3 c show that the number of such triples with 0 i j k equals c n 1 2 d combining part a with parts b and c conclude that n k2 2c n 1 3 c n 1 2 k 1 n n n how many bit strings of length n where n 4 contain exactly two occurrences of 34 let s be a set we say that a collection of sub sets a2 an each containing d elements where d 2 is 2 colorable if it is possible to assign to each element of s one of two different colors so that a 1 x2 2 and 3 b 6 and x3 5 c 4 x2 3 and x3 5 a how many different strings can be made from the word peppercorn when all the letters are used b how many of these strings start and end with the letter p c in how many of these strings are the three letter p s consecutive how many subsets of a set with ten elements a have fewer than five elements b have more than seven elements c have an odd number of elements 42 a witness to a hit and run accident tells the police that the license plate of the car in the accident which contains three letters followed by three digits starts with the let ters as and contains both the digits 1 and 2 how many different license plates can fit this description 43 how many ways are there to put n identical objects into m distinct containers so that no container is empty 44 how many ways are there to seat six boys and eight girls in a row of chairs so that no two boys are seated next to each other 45 how many ways are there to distribute six objects to five boxes if a both the objects and boxes are labeled b the objects are labeled but the boxes are unlabeled c the objects are unlabeled but the boxes are labeled d both the objects and the boxes are unlabeled 46 how many ways are there to distribute five objects into six boxes if a both the objects and boxes are labeled b the objects are labeled but the boxes are unlabeled c the objects are unlabeled but the boxes are labeled d both the objects and the boxes are unlabeled the signless stirling number of the first kind c n k where k and n are integers with 1 k n equals the number of ways to arrange n people around k circular tables with at least one person seated at each table where two seatings of m people around a circular table are considered the same if everyone has the same left neighbor and the same right neigh bor 47 find these signless stirling numbers of the first kind a c 3 2 b c 4 2 c c 4 3 d c 5 4 how many 11 element rna sequences consist of 4 as and and end with caa exercises and are based on a discussion in a method used in the for sequencing rna chains used enzymes to break chains after certain links some enzymes break rna chains after each g link while others break them after each c or u link using these enzymes it is sometimes possible to correctly sequence all the bases in an rna chain 53 suppose that when an enzyme that breaks rna chains af ter each g link is applied to a link chain the fragments obtained are g ccg aaag and uccg and when an enzyme that breaks rna chains after each c or u link is applied the fragments obtained are c c c c ggu and gaaag can you determine the entire 12 link rna chain from these two sets of fragments if so what is this rna chain 54 suppose that when an enzyme that breaks rna chains af ter each g link is applied to a 12 link chain the fragments obtained are ac ug and acg and when an enzyme that breaks rna chains after each c or u link is applied the fragments obtained are u gac and gac can you determine the entire rna chain from these two sets of fragments if so what is this rna chain show that if n is a positive integer then n c n j 49 show that if n is a positive integer with n 3 then c n n 2 3n 1 c n 3 4 show that if n and k are integers with 1 k n then c n 1 k c n k 1 nc n k 51 give a combinatorial proof that 2n divides n whenever n is an even positive integer hint use theorem 3 in sec tion 6 5 to count the number of permutations of 2n objects where there are two indistinguishable objects of n differ ent types of a finite set when repetition is allowed 56 devise an algorithm for generating all the r combinations of a finite set when repetition is allowed 57 show that if m and n are integers with m 3 and n 3 then r m n r m n 1 r m 1 n show that r 3 4 7 by showing that in a group of six people where any two people are friends or enemies there are not necessarily three mutual friends or four mutual en emies computer projects write programs with these input and output 1 given a positive integer n and a nonnegative integer not exceeding n find the number of r permutations and r combinations of a set with n elements 2 given positive integers n and r find the number of r permutations when repetition is allowed and r com binations when repetition is allowed of a set with n el ements 3 given a sequence of positive integers find the longest in creasing and the longest decreasing subsequence of the sequence 4 given an equation x2 xn c where c is a constant and x2 xn are nonnegative integers list all the solutions 5 given a positive integer n list all the permutations of the set 1 2 3 n in lexicographic order 6 given a positive integer n and a nonnegative integer r not exceeding n list all the r combinations of the set 1 2 3 n in lexicographic order 7 given a positive integer n and a nonnegative integer r not exceeding n list all the r permutations of the set 1 2 3 n in lexicographic order 8 given a positive integer n list all the combinations of the set 1 2 3 n 9 given positive integers n and r list all the r permutations with repetition allowed of the set 1 2 3 n 10 given positive integers n and r list all the r combinations with repetition allowed of the set 1 2 3 n computations and explorations use a computational program or programs you have written to do these exercises 1 find the number of possible outcomes in a two team play off when the winner is the first team to win 5 out of 9 6 out of 11 7 out of 13 and 8 out of 15 2 which binomial coefficients are odd can you formulate a conjecture based on numerical evidence 3 verify that c 2n n is divisible by the square of a prime when n 1 2 or 4 for as many positive integers n as you can the theorem that tells that c 2n n is divisible by the square of a prime with n 1 2 or 4 was proved in by andrew granville and olivier ramaré their proof settled a conjecture made in by paul erdo s and ron graham 4 find as many odd integers n less than as you can for which c n n 2 is not divisible by the square of a prime formulate a conjecture based on your evidence 5 for each integer less than 100 determine whether c 2n n is divisible by 3 can you formulate a conjecture that tells us for which integers n the binomial coefficient c 2n n is divisible by 3 based on the digits in the base three ex pansion of n 6 generate all the permutations of a set with eight elements 7 generate all the 6 permutations of a set with nine elements 8 generate all combinations of a set with eight elements 9 generate all 5 combinations with repetition allowed of a set with seven elements writing projects respond to these with essays using outside sources 1 describe some of the earliest uses of the pigeonhole prin ciple by dirichlet and other mathematicians 2 discuss ways in which the current telephone numbering plan can be extended to accommodate the rapid demand for more telephone numbers see if you can find some of the proposals coming from the telecommunications in dustry for each new numbering plan you discuss show how to find the number of different telephone numbers it supports 3 discuss the importance of combinatorial reasoning in gene sequencing and related problems involving genomes 4 many combinatorial identities are described in this book find some sources of such identities and describe important combinatorial identities besides those already introduced in this book give some representative proofs including combinatorial ones of some of these identities 5 describe the different models used to model the dis tribution of particles in statistical mechanics including maxwell boltzmann bose einstein and fermi dirac statistics in each case describe the counting techniques used in the model 6 define the stirling numbers of the first kind and describe some of their properties and the identities they satisfy 7 describe some of the properties and the identities that stir ling numbers of the second kind satisfy including the con nection between stirling numbers of the first and second kinds 8 describe the latest discoveries of values and bounds for ramsey numbers 9 describe additional ways to generate all the permutations of a set with n elements besides those found in section 6 6 compare these algorithms and the algorithms described in the text and exercises of section 6 6 in terms of their computational complexity 10 describe at least one way to generate all the partitions of a positive integer n see exercise 47 in section 5 3 7 1 an introduction to discrete probability 7 2 probability theory 7 3 bayes theorem 7 4 expected value and variance ombinatorics and probability theory share common origins the theory of probability was first developed more than years ago when certain gambling games were analyzed although probability theory was originally invented to study gambling it now plays an essential role in a wide variety of disciplines for example probability theory is extensively applied in the study of genetics where it can be used to help understand the inheritance of traits of course probability still remains an extremely popular part of mathematics because of its applicability to gambling which continues to be an extremely popular human endeavor in computer science probability theory plays an important role in the study of the com plexity of algorithms in particular ideas and techniques from probability theory are used to determine the average case complexity of algorithms probabilistic algorithms can be used to solve many problems that cannot be easily or practically solved by deterministic algorithms in a probabilistic algorithm instead of always following the same steps when given the same input as a deterministic algorithm does the algorithm makes one or more random choices which may lead to different output in combinatorics probability theory can even be used to show that objects with certain properties exist the probabilistic method a technique in com binatorics introduced by paul erdo s and alfréd rényi shows that an object with a specified property exists by showing that there is a positive probability that a randomly constructed object has this property probability theory can help us answer questions that involve uncertainty such as determining whether we should reject an incoming mail message as spam based on the words that appear in the message 7 1 an introduction to discrete probability introduction probability theory dates back to when the italian mathematician physician and gambler girolamo cardano wrote the first known systematic treatment of the subject in his book liber de ludo aleae book on games of chance this book was not published until which may have held back the development of probability theory in the seventeenth century the french mathematician blaise pascal determined the odds of winning some popular bets based on the outcome when a pair of dice is repeatedly rolled in the eighteenth century the french mathematician laplace who also studied gambling defined the probability of an event as the number of successful outcomes divided by the number of possible outcomes for instance the probability that a die comes up an odd number when it is rolled is the number of successful outcomes namely the number of ways it can come up odd divided by the number of possible outcomes namely the number of different ways the die can come up there are a total of six possible outcomes namely 1 2 3 4 5 and 6 and exactly three of these are successful outcomes namely 1 3 and 5 hence the probability that the die comes up an odd number is 3 6 1 2 note that it has been assumed that all possible outcomes are equally likely or in other words that the die is fair in this section we will restrict ourselves to experiments that have finitely many equally likely outcomes this permits us to use laplace s definition of the probability of an event we will continue our study of probability in section 7 2 where we will study experiments with finitely many outcomes that are not necessarily equally likely in section 7 2 we will also introduce some key concepts in probability theory including conditional probability independence of events and random variables in section 7 4 we will introduce the concepts of the expectation and variance of a random variable finite probability an experiment is a procedure that yields one of a given set of possible outcomes the sample space of the experiment is the set of possible outcomes an event is a subset of the sample space laplace s definition of the probability of an event with finitely many possible outcomes will now be stated definition 1 the probability of an event can never be negative or more than one according to laplace s definition the probability of an event is between 0 and 1 to see this note that if e is an event from a finite sample space s then 0 e s because e s thus 0 p e e s 1 examples 1 7 illustrate how the probability of an event is found example 1 an urn contains four blue balls and five red balls what is the probability that a ball chosen at random from the urn is blue solution to calculate the probability note that there are nine possible outcomes and four of these possible outcomes produce a blue ball hence the probability that a blue ball is chosen is 4 9 example 2 what is the probability that when two dice are rolled the sum of the numbers on the two dice is 7 solution there are a total of 36 equally likely possible outcomes when two dice are rolled the product rule can be used to see this because each die has six possible outcomes the total girolamo cardano cardano born in pavia italy was the illegitimate child of fazio cardano a lawyer mathematician and friend of leonardo da vinci and chiara micheria a young widow in spite of illness and poverty cardano was able to study at the universities of pavia and padua from where he received his medical degree cardano was not accepted into milan s college of physicians because of his illegitimate birth as well as his eccentricity and confrontational style nevertheless his medical skills were highly regarded one of his main accomplishments as a physician is the first description of typhoid fever cardano published more than 100 books on a diverse range of subjects including medicine the natural sciences mathematics gambling physical inventions and experiments and astrology he also wrote a fascinating autobiography in mathematics cardano s book ars magna published in established the foundations of abstract algebra this was the most comprehensive book on abstract algebra for more than a century it presents many novel ideas of cardano and of others including methods for solving cubic and quartic equations from their coefficients cardano also made several important contributions to cryptography cardano was an advocate of education for the deaf believing unlike his contemporaries that deaf people could learn to read and write before learning to speak and could use their minds just as well as hearing people cardano was often short of money however he kept himself solvent through gambling and winning money by beating others at chess his book about games of chance liber de ludo aleae written in but published in offers the first systematic treatment of probability it also describes effective ways to cheat cardano was considered to be a man of dubious moral character he was often described as a liar gambler lecher and heretic number of outcomes when two dice are rolled is 62 36 there are six successful outcomes namely 1 6 2 5 3 4 4 3 5 2 and 6 1 where the values of the first and second dice are represented by an ordered pair hence the probability that a seven comes up when two fair dice are rolled is 6 36 1 6 lotteries are extremely popular throughout the world we can easily compute the odds of winning different types of lotteries as illustrated in examples 3 and 4 the odd of winning the popular mega millions and powerball lotteries are studied in the supplementary exercises example 3 in a lottery players win a large prize when they pick four digits that match in the correct order four digits selected by a random mechanical process a smaller prize is won if only three digits are matched what is the probability that a player wins the large prize what is the probability that a player wins the small prize solution there is only one way to choose all four digits correctly by the product rule there are 10 000 ways to choose four digits hence the probability that a player wins the large prize is 1 10 000 0 0001 players win the smaller prize when they correctly choose exactly three of the four digits exactly one digit must be wrong to get three digits correct but not all four correct by the sum rule to find the number of ways to choose exactly three digits correctly we add the number of ways to choose four digits matching the digits picked in all but the ith position for i 1 2 3 4 to count the number of successes with the first digit incorrect note that there are nine possible choices for the first digit all but the one correct digit and one choice for each of the other digits namely the correct digits for these slots hence there are nine ways to choose four digits where the first digit is incorrect but the last three are correct similarly there are nine ways to choose four digits where the second digit is incorrect nine with the third digit incorrect and nine with the fourth digit incorrect hence there is a total of 36 ways to choose four digits with exactly three of the four digits correct thus the probability that a player wins the smaller prize is 36 10 000 9 0 example 4 there are many lotteries now that award enormous prizes to people who correctly choose a set of six numbers out of the first n positive integers where n is usually between 30 and 60 what is the probability that a person picks the correct six numbers out of solution there is only one winning combination the total number of ways to choose six numbers out of is c 6 34 6 we can now estimate the computational complexity of dijkstra s algorithm in terms of additions and comparisons the algorithm uses no more than n 1 iterations where n is the number of vertices in the graph because one vertex is added to the distinguished set at each iteration we are done if we can estimate the number of operations used for each iteration we can identify the vertex not in sk with the smallest label using no more than n 1 comparisons then we use an addition and a comparison to update the label of each vertex not in sk it follows that no more than 2 n 1 operations are used at each iteration because there are no more than n 1 labels to update at each iteration because we use no more than n 1 iterations each using no more than 2 n 1 operations we have theorem 2 the traveling salesperson problem we now discuss an important problem involving weighted graphs consider the following prob lem a traveling salesperson wants to visit each of n cities exactly once and return to his starting point for example suppose that the salesperson wants to visit detroit toledo saginaw grand rapids and kalamazoo see figure 5 in which order should he visit these cities to travel the minimum total distance to solve this problem we can assume the salesperson starts in detroit because this must be part of the circuit and examine all possible ways for him to visit the other four cities and then return to detroit starting elsewhere will produce the same circuits there are a total of 24 such circuits but because we travel the same distance when we travel a circuit in reverse order we need only consider 12 different circuits to find the minimum total distance he must travel we list these 12 different circuits and the total distance traveled for each circuit as can be seen from the list the minimum total distance of miles is traveled using the circuit detroit toledo kalamazoo grand rapids saginaw detroit or its reverse route total distance miles detroit toledo grand rapids saginaw kalamazoo detroit detroit toledo grand rapids kalamazoo saginaw detroit detroit toledo kalamazoo saginaw grand rapids detroit detroit toledo kalamazoo grand rapids saginaw detroit detroit toledo saginaw kalamazoo grand rapids detroit detroit toledo saginaw grand rapids kalamazoo detroit detroit saginaw toledo grand rapids kalamazoo detroit 598 detroit saginaw toledo kalamazoo grand rapids detroit detroit saginaw kalamazoo toledo grand rapids detroit detroit saginaw grand rapids toledo kalamazoo detroit detroit grand rapids saginaw toledo kalamazoo detroit detroit grand rapids toledo saginaw kalamazoo detroit an handbook der handlungsreisende the traveling salesman mentions the traveling salesman problem with sample tours through germany and switzerland we just described an instance of the traveling salesperson problem the traveling sales person problem asks for the circuit of minimum total weight in a weighted complete undirected graph that visits each vertex exactly once and returns to its starting point this is equivalent to asking for a hamilton circuit with minimum total weight in the complete graph because each vertex is visited exactly once in the circuit the most straightforward way to solve an instance of the traveling salesperson problem is to examine all possible hamilton circuits and select one of minimum total length how many circuits do we have to examine to solve the problem if there are n vertices in the graph once a starting point is chosen there are n 1 different hamilton circuits to examine because there are n 1 choices for the second vertex n 2 choices for the third vertex and so on because a hamilton circuit can be traveled in reverse order we need only examine n 1 2 circuits to find our answer note that n 1 2 grows extremely rapidly trying to solve a traveling salesperson problem in this way when there are only a few dozen vertices is impractical for example with 25 vertices a total of 24 2 approximately 3 1 different hamilton circuits would have to be considered if it took just one nanosecond 10 9 second to examine each hamilton circuit a total of approximately ten million years would be required to find a minimum length hamilton circuit in this graph by exhaustive search techniques because the traveling salesperson problem has both practical and theoretical importance a great deal of effort has been devoted to devising efficient algorithms that solve it however no algorithm with polynomial worst case time complexity is known for solving this problem furthermore if a polynomial worst case time complexity algorithm were discovered for the traveling salesperson problem many other difficult problems would also be solvable using polynomial worst case time complexity algorithms such as determining whether a proposition in n variables is a tautology discussed in chapter 1 this follows from the theory of np completeness for more information about this consult a practical approach to the traveling salesperson problem when there are many vertices to visit is to use an approximation algorithm these are algorithms that do not necessarily produce the exact solution to the problem but instead are guaranteed to produce a solution that is close to an exact solution also see the preamble to exercise 46 in the supplmentary exercises of chapter 3 that is they may produce a hamilton circuit with total weight w such that w w cw where w is the total length of an exact solution and c is a constant for example there is an algorithm with polynomial worst case time complexity that works if the weighted graph satisfies the triangle inequality such that c 3 2 for general weighted graphs for every positive real number k no algorithm is known that will always produce a solution at most k times a best solution if such an algorithm existed this would show that the class p would be the same as the class np perhaps the most famous open question about the complexity of algorithms see section 3 3 in practice algorithms have been developed that can solve traveling salesperson problems with as many as vertices within 2 of an exact solution using only a few minutes of computer time for more information about the traveling salesperson problem including his tory applications and algorithms see the chapter on this topic in applications of discrete mathematics also available on the website for this book a set not recognized by a finite state automaton we have seen that a set is recognized by a finite state automaton if and only if it is regular we will now show that there are sets that are not regular by describing one such set the technique used to show that this set is not regular illustrates an important method for showing that certain sets are not regular example 6 show that the set n 0 1 2 made up of all strings consisting of a block of followed by a block of an equal number of is not regular solution suppose that this set were regular then there would be a nondeterministic finite state automaton m s i f f recognizing it let n be the number of states in this machine that is n s because m recognizes all strings made up of a number of followed by an equal number of m must recognize let s2 be the sequence of states that is obtained starting at and using the symbols of as input so that f 0 s2 f 0 sn f sn 1 0 sn 1 f sn 1 f 1 1 note that is a final state because there are only n states the pigeonhole principle shows that at least two of the first n 1 of the states which are sn must be the same say that si and sj are two such identical states with 0 i j n this means that f si sj where t j i it follows that there is a loop leading from si back to itself obtained using the input 0 a total of t times in the state diagram shown in figure 6 now consider the input string t there are t more consecutive at the start of this block than there are consecutive that follow it because this string is not of the form because it has more than it is not recognized by m consequently f t cannot be a final state however when we use the string t as input we end up in the same state as before namely the reason for this is that the extra t in this string take us around the loop from si back to itself an extra time as shown in figure 6 then the rest of the string leads us to exactly the same state as before this contradiction shows that n 0 1 2 is not regular more powerful types of machines finite state automata are unable to carry out many computations the main limitation of these machines is their finite amount of memory this prevents them from recognizing languages that are not regular such as n 0 1 2 because a set is regular if and only if it is the language generated by a regular grammar example 6 shows that there is no regular grammar alan turing invented turning machines before modern computers existed that generates the set n 0 1 2 however there is a context free grammar that generates this set such a grammar was given in example 5 in section 13 1 because of the limitations of finite state machines it is necessary to use other more pow erful models of computation one such model is the pushdown automaton a pushdown automaton includes everything in a finite state automaton as well as a stack which provides unlimited memory symbols can be placed on the top or taken off the top of the stack a set is recognized in one of two ways by a pushdown automaton first a set is recognized if the set consists of all the strings that produce an empty stack when they are used as input second a set is recognized if it consists of all the strings that lead to a final state when used as input it can be shown that a set is recognized by a pushdown automaton if and only if it is the language generated by a context free grammar however there are sets that cannot be expressed as the language generated by a context free grammar one such set is n 0 1 2 we will indicate why this set cannot be recognized by a pushdown automaton but we will not give a proof because we have not developed the machinery needed however one method of proof is given in exercise 28 of the supplementary exercises at the end of this chapter the stack can be used to show that a string begins with a sequence of followed by an equal number of by placing a symbol on the stack for each 0 as long as only are read and removing one of these symbols for each 1 as long as only following the are read but once this is done the stack is empty and there is no way to determine that there are the same number of in the string as there are other machines called linear bounded automata more powerful than push down automata that can recognize sets such as n 0 1 2 in particular linear bounded automata can recognize context sensitive languages however these machines cannot recognize all the languages generated by phrase structure grammars to avoid the limitations of special types of machines the model known as a turing machine named after the british mathematician alan turing is used a turing machine is made up of everything included in a finite state machine together with a tape which is infinite in both directions a turing machine has read and write capabilities on the tape and it can move back and forth along this tape tur ing machines can recognize all languages generated by phrase structure grammars in addition turing machines can model all the computations that can be performed on a computing ma chine because of their power turing machines are extensively studied in theoretical computer science we will briefly study them in section 13 5 alan mathison turing alan turing was born in london although he was conceived in india where his father was employed in the indian civil service as a boy he was fascinated by chemistry performing a wide variety of experiments and by machinery turing attended sherborne an english boarding school in he won a scholarship to king s college cambridge after completing his dissertation which included a rediscovery of the central limit theorem a famous theorem in statistics he was elected a fellow of his college in turing became fascinated with the decision problem a problem posed by the great german mathematician hilbert which asked whether there is a general method that can be applied to any assertion to determine whether the assertion is true turing enjoyed running later in life running as a serious amateur in competitions and one day while resting after a run he discovered the key ideas needed to solve the decision problem in his solution he invented what is now called a turing machine as the most general model of a computing machine using these machines he found a problem involving what he called computable numbers that could not be decided using a general method from to turing visited princeton university to work with alonzo church who had also solved hilbert s decision problem in turing returned to king s college however at the outbreak of world war ii he joined the foreign office performing cryptanalysis of german ciphers his contribution to the breaking of the code of the enigma a mechanical german cipher machine played an important role in winning the war after the war turing worked on the development of early computers he was interested in the ability of machines to think proposing that if a computer could not be distinguished from a person based on written replies to questions it should be considered to be thinking he was also interested in biology having written on morphogenesis the development of form in organisms in turing committed suicide by taking cyanide without leaving a clear explanation legal troubles related to a homosexual relationship and hormonal treatments mandated by the court to lessen his sex drive may have been factors in his decision to end his life social advertising uses information about consumers peers including peer affiliations with a brand product organization etc to target ads and contextualize their display this approach can increase ad efficacy for two main reasons peers affiliations reflect unobserved consumer characteristics which are correlated along the social network and the inclusion of social cues i e peers association with a brand alongside ads affect responses via social influence processes for these reasons responses may be increased when multiple social signals are presented with ads and when ads are affiliated with peers who are strong rather than weak ties we conduct two very large field experiments that identify the effect of social cues on consumer responses to ads measured in terms of ad clicks and the formation of connections with the advertised entity in the first experiment we randomize the number of social cues present in word of mouth advertising and measure how responses increase as a function of the number of cues the second experiment examines the effect of augmenting traditional ad units with a minimal social cue i e displaying a peer affiliation below an ad in light grey text on average this cue causes significant increases in ad performance using a measurement of tie strength based on the total amount of communication between subjects and their peers we show that these influence effects are greatest for strong ties our work has implications for ad optimization user interface design and central questions in social science research categories and subject descriptors j social and behavioral sciences sociology j social and behavioral sciences economics h models and principles user machine systems general terms measurement experimentation human factors additional key words and phrases online advertising peer effects social networks introduction social media activity now constitutes a substantial fraction of time spent on the web goel et al users of social networking technologies create explicit representations of their relationships with other users their peers boyd and ellison and use those connections as channels for information dissemination they also establish connections with other entities in order to express their identities and subscribe to content sun et al the widespread adoption of such technologies has led to advertising approaches that differ from existing approaches such as search based advertising for example rather than inferring consumer intent via search terms social advertising systems can match ads to consumers who have peers that are affiliated with the brand product or organization being advertised hill et al tucker authors contributed equally to this work correspondence may be sent to e b at ebakshy fb com or d e at deaneckles fb com permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation copyrights for components of this work owned by others than acm must be honored abstracting with credit is permitted to copy otherwise to republish to post on servers to redistribute to lists or to use any component of this work in other works requires prior specific permission and or a fee permissions may be requested from publications dept acm inc penn plaza suite new york ny usa fax or permissions acm org ec june valencia spain copyright acm 146 social advertising systems can also display social context about peers who are affiliated with the entity being advertised these social cues create a channel for consumers to influence one another like word of mouth wom and viral marketing approaches advertisers employ social ads with the goal of spreading attitudes and behaviors through consumers social networks thus many of the central research questions in wom research apply to social advertising and research on social advertising can contribute to the broader understanding of social influence in the behavioral and economic sciences in particular studies of social advertising can be used to learn about how consumer responses depend on a the number of social signals that consumers receive from their peers b the characteristics of the relationship between the consumer and their peers the present research addresses both of these topics despite the similarities between wom marketing and social advertising there are a number of important qualities that distinguish the two first a minimal lightweight consumer behavior e g creating a connection with an entity is sufficient to make that consumer a source of peer influence second social influence via these systems is passive and automatically targeted cf aral and walker that is once an individual creates a persistent connection with an advertised entity social influence can occur continually without additional actions such as sharing messages with others finally social advertising allows stakeholders to play a more active role in creating and funding ad campaigns with specific text images or video we regard social advertising as any advertising method that uses information about consumers social networks to target ads and or provide personalized social signals thus there are two ways in which the use of social information in advertising can affect consumer responses social networks encode unobserved consumer characteristics which allow advertisers to target likely adopters and the inclusion of social cues creates a new channel for social influence recent work on social advertising e g tucker has recognized these mechanisms but has been largely unable to identify the the extent in which social influence actually plays a role as far as we know the present research is the first to identify the effect of social signals from peers on consumer responses to advertising we use field experiments to make this identification possible overview we investigate the effects of social cues in online advertising using two very large field experiments that randomize the number of social cues present in ad units on facebook experiment examines the marginal effect of referring to additional peers in word of mouth type ad units we identify the average dose response function more specifically the cue response function for one to three peers whose shape and slope differs substantially from na ıve estimates derived from observational data we find that consistent with the expectation that social cues are a channel for positive peer effects showing more peers affiliated with the advertised entity can increase positive consumer responses having established the presence of substantial social influence effects we extend this analysis to minimal social cues about a single peer experiment manipulates whether ads include a reference to a single peer or a visually commensurate control message even this minimal cue mentioning a single peer can cause substantial increases in clicks and the creation of connections with the advertised entity finally we examine how consumer responses vary with the strength of the relationship between the consumer and affiliated peer as measured by past communication behavior our results show that those with stronger ties who are affiliated with the advertised entity are more likely to respond to an ad even if no cue is given furthermore the presence of the cue for strong ties can generate larger influence effects compared to those with weak ties a b xi yia ui yja xj uj dija c fig causal relationships in consumer responses to advertising solid lines indicate cause and effect relationships dashed lines indicate that variables are correlated in some possibly unknown way a responses are caused by observed and unobserved individual characteristics b responses may be correlated with peers responses even when there is no social influence c responses can be explained both by social influence and correlation among peer characteristics here one mechanism for social influence among other possible mechanisms is the inclusion of social cues dija in the ad before reporting on these experiments we describe the causal relationships that we aim to distinguish and estimate and we review some additional related work on online advertising and social influence causal relationships in social advertising at a basic level an advertisement is a stimulus designed to encourage an individual to engage with an entity such as a brand product movie musical artist organization etc the efficacy of an advertisement typically given in terms of a click through rates or conversion rates can depend critically upon the relationship between the entity and the characteristics of the consumer which may include both stable traits and transient states e g demographic characteristics and recent user activity the causal of this scenario illustrated in figure is relatively simple a user i has some set of known features xi which affect the yia to the advertisement a errors in predicting the response can be regarded as resulting from unobserved characteristics ui of the user predictive aspects of social information many characteristics attitudes and behaviors are clustered in social networks mcpherson et al that is birds of a feather flock together this clustering has multiple causes including individuals preferring to associate with similar others preference homophily opportunities for forming and maintaining relationships that are biased towards similar others structural homophily kossinets and watts currarini et al external causes whose effects are localized in the social network and prior peer influence by which peers become more similar over time present causal models in this section by drawing graphs consistent with the causal framework in pearl our discussion of confounding of homophily and influence follows a more general treatment by shalizi and thomas is the response to a specific ad a including the ad creative this allows for the sake of simplicity not including variables related to the ad creative or advertised entity lewis et al for simplicity we subsequently use homophily to refer to all observed prior clustering of consumer features the presence of homophily in social networks suggests that characteristics of an individual can be predicted via characteristics of her peers figure illustrates how a peer response to an ad is predictive of a consumer response even if information about one peer behavior is not observed by and cannot affect the consumer i e in the absence of peer influence here xi and xj are observed characteristics of the consumer and her peer and ui and uj are unobserved characteristics homophily implies that these variables will be correlated in some way therefore a peer j behavior at a previous time yja is informative about the consumer i subsequent behavior yia furthermore individuals who interact frequently exhibit greater correlation among characteristics compared to those who do not mcpherson et al this suggests that those who have more opportunity to influence one another may also be expected to respond to an ad in the same way even in the absence of social influence peer influence and confounding peer behaviors can also influence consumer responses to ads at least since early psychological experiments on conformity to group behavior sherif asch and observational studies of opinion leadership in mass communication katz and lazarsfeld these peer effects have been a central subject for the social and economic sciences in the presence of peer effects an individual response to an ad will be associated with their peers responses not only because of homophily but also because peer behavior causes individual behavior in particular if consumers can observe or infer the responses of their peers then they are expected to use this information even automatically and outside of conscious awareness tanner et al to determine their response ads that include such information about peer behaviors i e as social cues are expected to affect consumer responses via these social influence processes the presence of peer effects including those via social cues in ad interfaces is illustrated by figure the peer response to the ad yja can affect the consumer response to the ad yia via multiple mechanisms of principal interest here is social influence via social cues yja dija yia although it is important to note that previous interactions between i and j given by yia yia may also cause influence effects in the absence of cues this picture highlights the difficulty in determining whether a change in responses to ads is due to the presence of social cues other forms of social influence stemming from prior interaction or the correlation in behaviors induced by homophily for example even if one were to control for all observed characteristics xi and xj it is expected that there are unobserved characteristics ui and uj that make the responses of i and j dependent identification via experiments randomized experiments are the gold standard for causal inference rubin and the identification of peer effects is no exception when they are possible field experiments combine this internal validity with external validity shadish and cook conventional experimental methodology for online advertising randomly assigns users that even if observed characteristics are incomplete or noisy peer characteristics can still be predictive for example backstrom et al improve estimates of the location of individuals by using the location of their facebook friends sometimes refer to peer effects as social interactions durlauf and ioannides manski moffitt to a treatment group which sees an ad and a control or holdout group that does not see an ad the experimenter then compares the outcome variable of interest such as purchases for these two groups lewis and reiley in the context of measuring peer effects in social advertising experimenters can randomly assign user ad pairs to receive varying numbers of social cues for example when a user views an ad which can display a social cue whether that social cue is actually displayed would be determined by random assignment in the causal model of figure this amounts to random assignment of the values of dija experimenters can then compare response rates of user ad pairs assigned to different social cue conditions the remainder of this paper focuses on experimental comparisons of this kind related work online networks are focused on sharing information and as such have been studied extensively in the context of information diffusion large scale observational studies explore a variety of diffusion like phenomena in contexts including the apparent spread of links on blogs adar and adamic and twitter bakshy et al the joining of groups backstrom et al product recommendations leskovec et al and the adoption of user contributed content in virtual economies bakshy et al data from these studies are highly suggestive of social influence the probability of adopting a behavior increases with the number of adopting peers however as noted by anagnostopoulos et al and aral et al such studies can easily overestimate the role of influence in online behavior because of homophily shalizi and thomas go further to illustrate that statistical methods cannot adequately control for confounding factors in observational studies without the use of very strong assumptions some recent work has used field experiments to examine effects of social signals in online advertising and similar settings tucker estimates combined effects of social targeting and social cues in ads and highlights the value of distinguishing them two other recent experiments are similar to the present work in that they manipulate particular mechanisms of social influence aral and walker randomly assign individuals to versions of an application that included or lacked viral marketing features their passive broadcast feature has similarities to social advertising in that it is visible to peers and includes a social cue however their experiment manipulated whether an individual adoption of the application would notify their peers rather than whether individuals with adopter peers were exposed to the broadcasted message bakshy et al randomize exposure to particular links shared by peers on facebook and found that individuals were more likely to share the same links as their friends even if they did not see the links on the site this tendency was stronger for users who had multiple sharing friends or a single friend who was a strong tie since this experiment either completely allowed exposure or prevented it for each individual link pair it did not identify the effect of influence via social cues setting and data analysis procedures we conducted two large field experiments on facebook during a short period in this section introduces the relevant aspects of facebook and some general characteristics of the data common to both experiments subject experience a primary mode of interaction on facebook occurs through news feed where users share with and consume content from their network of peers activity along these channels are called stories and include short messages links photos and other content ties between users are established by one user requesting to become friends on facebook and the other user accepting the request in this paper we treat a user friends as constituting their set of peers users can also establish connections with other entities by liking particular pages which correspond to businesses organizations products movies musical artists celebrities etc the liking affiliation is generally visible to the user peers when they visit the user profile and in peers news feeds this latter case includes connection stories that indicate a user has liked a particular page additionally content shared by a page appears in news feed for users who like that page several different types of ad units may be present in the right hand column of the site many ad units are associated with particular pages these units can be targeted toward users with peers who like the page i e social targeting and display these peers alongside the ad unit i e social cues for a given user ad pair the affiliated peers are all peers who like the page and are eligible to be mentioned e g the peers personal settings allow displaying this information to the user experiments and use two different types of ad units described in section and section in both cases clicking on the ad unit takes the user to the advertised page clicking on a link labeled like either in the ad unit itself or on the linked page creates a new connection between the user and the page we study effects of social cues on two responses clicks on the linked content and liking the advertised page assessing variation in response rates the observed outcomes responses to ad impressions are not independent and identically distributed iid users and ads vary in their response rates and users only occur in combination with a limited number of ads that is the responses arise from a data generating process with unbalanced crossed random effects of users and ads statistical methods that assume iid data neglect this dependence structure and are expected to be anti conservative i e produce confidence intervals that are too narrow for example if one observed impressions for users and ads methods that treat as the relevant n will generally substantially overstate confidence about response rates to address this issue all statistical inference in this paper employs a bootstrap strategy for data with this crossed structure brennan et al owen we now briefly describe our use of this strategy so that readers can appropriately interpret our results and apply the method to similar problems in the standard iid bootstrap the analyst constructs r bootstrap replicates by sampling n observations with replacement from a full dataset of size n for each of these r replicates one computes the statistic of interest e g a ratio of proportions a simple method for computing a confidence interval for that statistic is to use the and percentiles of the resulting r statistics this is the bootstrap percentile confidence interval this standard bootstrap procedure is not appropriate for dependent data such as in our experiment where users and ads are dependent instead analysts should resample both users and ads independently owen we use a variation of this strategy that is suitable for online and distributed computation owen and eckles rather than resampling n observations the data is re weighted according the the following procedure for the rth replicate each user is assigned a poisson draw and each ad is assigned a poisson draw each user ad pair is then assigned the product of the corresponding draws as its weight this strategy is known to be conservative when estimating the variance of means i e it produces confidence intervals that in implementing the multiway bootstrap in r is available at https github com deaneckles the results in this paper were produced by similar software for apache hive a b c fig experimental treatment for sponsored story ad units in experiment figure illustrates the three possible treatment conditions for users with three peers zia who are affiliated with the sponsored page a dia b dia c dia clude the true mean more than the time throughout we report bootstrap percentile confidence intervals using r experiment influence of multiple peers theory suggests that individuals are more likely to adopt actions previously taken by their peers when multiple social signals are present schelling granovetter centola and macy these models are central to our understanding of information diffusion processes and form the basis for the formal analysis of viral marketing watts and strogatz kempe et al watts and dodds however as explained earlier homophily and other factors obscure the true shape and magnitude of such dose response functions in real world data we use sponsored story ad units figure to understand the marginal effect of social signals on consumer behavior above and beyond what one might expect due to homophily and other sources of heterogeneity sponsored story ad units resemble organic stories that appear in the news feed when a peer likes a page similar to conventional wom approaches the story does not include an advertiser generated message and must be associated with at least one peer the main treatment unit is therefore the number of peers shown since the ad units are essentially sponsored versions of organic news feed stories they follow the same visual constraints imposed by the news feed they must feature at least one affiliated peer and a small version of the first peer profile is displayed in the leftmost part of the unit the first peer name may be followed by the names of up to two other affiliated peers a small thumbnail version of the page profile image is also displayed sampling and assignment procedure the first experiment applies to a simple random sample of all facebook users all sponsored stories displayed to users in this sample were subject to the following assignment procedure to determine the number of peers to be mentioned user ad pairs i a are randomly assigned to a number of peers dia to be mentioned in the sponsored story since the maximum number of peers shown is limited by the number of affiliated peers zia i e peers who have liked the advertised page and are eligible to be mentioned user ad pairs are assigned with equal probability to all of the possible values of dia min zia we limit our analysis to user ad pairs with one two or three available peers zia experimental assignment is a deterministic function of user ad pairs so all impressions for a user ad pair have the same number and order of peers mentioned in the sponsored story in total experiment includes users ad ids and distinct user ad pairs select their own profile photo for the thumbnail this is used throughout the site accompanying the user stories and comments number of peers shown d normalized click rate z z z a number of peers shown d normalized like rate z z z b fig the average cue response function is the relationship between the number of peers shown d and a click through rate and b like rate within panel differences are caused by the treatment number of cues and are significant with p error bars are bootstrapped confidence intervals note that because some of the same users and ad ids contribute to each response rate overlapping intervals do not indicate non significance of associated differences and ratios panels represent different populations of user ad pairs where the user has z or affiliated peers all probability scales are normalized by the smallest point estimate in the plot average cue response function we first examine the average cue response function which shows how response rates vary as a function of the number of peers shown this relationship conditional on the number of affiliated peers is shown in figure panels with z and z illustrate the causal effect of showing multiple peers on click and like rates for user ad pairs with two affiliated peers displaying a second peer caused a ci relative increase in click rate and a ci relative increase in like rate when there were three affiliated peers this increase was slightly weaker the click rate increased by ci and like rate by ci the cue response function can also be used to examine whether influence appears to follow simple contagion where each additional cue is expected to cause a constant or slightly sub linear increase or complex contagion i e schelling granovetter centola and macy whereby additional social signals result in a super linear increase in the response we find that there is no statistically significant difference between the probability increase from d to d and d to d p for clicks p for likes this is consistent with simple contagion although since we only measure average effects it is difficult to entirely rule out complex contagion it is also apparent from figure that response rates increase with the number of affiliated peers even when users only see a single cue this trend illustrates how in many observational settings the relationship between an individual behavior with the number of peers who also exhibit that behavior may not reflect increased levels of influence while homophily is an obvious cause of the increase in response rates the difference should not be attributed to homophily alone for example consider variation in the network wide popularity of pages sponsored stories associated with pages with more connected users will be observed more frequently with a larger number of affiliated peers if these more popular pages have sponsored stories with higher response rates then this will contribute to the between panel differences in figure a b fig the two treatment conditions for social ads in experiment subjects who are to be exposed to ads with at least one affiliated peer are randomly assigned to see either a general information about the total number of affiliated individuals dia or b a minimal social cue featuring one affiliated peer dia likewise users with more peers will be observed more frequently with a larger number of affiliated peers these users might be expected to differ in their response rates finally the selection of which stories to display to a user is endogenous an individual characteristics are used to predict click through rates which cause the story to be displayed but these characteristics may also be correlated with the number of affiliated peers thus unlike comparisons given by the average cue response function within panel comparisons variation between users with different numbers of affiliated peers cannot be given a straightforward interpretation two limitations of this experiment concern the nature of the particular ad unit used first since this unit consists of reporting the connection of one or more peers with the entity it was not possible to examine the effect of social cues relative to a baseline without any cues second as the number of peers shown increases so does the height of the ad unit and number of non white pixels in the ad unit while this change is very small relative to the overall size of the unit this is a potential confound with the effects of increasing the number of peers referred to in the social cue both of these limitations motivate the design of experiment in which we examine how a more minimal social cue affects consumer responses experiment influence of minimal social cues the next experiment examines the presence of minimal social cues with an ad in particular we measure the effect of adding light grey text with the name of a single peer associated with the entity being advertised thus the first goal of this experiment is to identify the effect of having a cue alongside an ad we test whether the social influence effects in experiment extend to the presence or absence of any social cues when these cues are visually commensurate with one another and subordinate to an advertiser created message the second experiment also enables us to examine how the effect of social cues in advertising varies with the strength of the relationship between a consumer and affiliated peer previous work motivates the hypothesis that both homophily hill et al kossinets andwatts and social influence hovland andweiss rogers and bhowmik goethals and nelson should cause peers with stronger relationships to have responses that are more correlated we expect response rates to be higher for user ad pairs where an affiliated peer is a strong tie rather than a weak tie this effect should exist even in the absence of a social cue but we also expect that the effect of a social cue will be larger for strong ties than weak ties experiment manipulates social cues alongside a social ad unit which includes advertiser specified creative consisting of a title image and caption figure beneath the custom creative is a link that says like and gives information about people who like this page in small gray text if a user has any affiliated peers the name of one of those peers can be displayed if a user has no affiliated peers the total number of users who have liked the page is displayed sampling and assignment procedure the procedures for experiment are the same as experiment the experiment differs in that it involves random assignment to the presence or absence of a social cue figure rather the number of peers referred to in an always present cue that is user ad pairs i a are randomly assigned to the presence of a social cue mentioning a single peer dia or to the absence of a social cue dia in the latter case the total number of users who like the page is displayed in the same location and typeface as the peer name as before the selection of the treatment and peer is deterministic so users assigned to the dia condition will always see the same peer in total experiment includes users ads ids and user ad pairs average effect of a social cue we begin by estimating average effects of the minimal social cue figure displays response rates with and without the minimal social cue for different numbers of affiliated peers similar to the previous experiment comparisons across different numbers of affiliated peers i e between panels of figure show that response rates increase when more peers are available regardless of how many were shown as before it is assumed that this increase results from homophily and heterogeneity in ad user pairs that are associated with the number of affiliated peers available to a user for a given ad see section above average peer effects resulting from the minimal social cue are identified by comparing responses with and without the social cue for each number of affiliated peers we find that depending on the number of affiliated peers the cue increases click rates by to and like rates by to figure for example for users with a single affiliated peer referring to that peer increases the click rate by ci and the like rate by ci this provides evidence that even a minimal social cue can substantially affect a consumer response to an ad tie strength to what extent do consumer responses to an ad depend on the nature of the relationship between the user and her affiliated peer the preceding analysis treats relationships among consumers as binary where a tie is either present or absent we now examine the strength of a consumer relationship with her affiliated peers on a manyvalued scale to do this we consider cases in which users have exactly one affiliated peer who was either shown or not shown i e those user ad pairs featured in the first panel of figure measure of tie strength we use a measure of tie strength based on the frequency in which two users communicate with each other via facebook the total number of comments and messages created by a user during this day period is their total communication count ci we measure the strength of the directed tie between user i and user j as wij cij ci we define tie strength as the fraction of user i communications that are directed at user j or on posts by user j similar measures are good predictors of standard measures of interpersonal trust burke et al and dia stimuli can also be regarded as including a social cue though not a personalized social cue about peers compared with non informative text in this location we might expect that ad efficacy will be higher with this general prevalence information in this case experiment underestimates the effects of having a minimal social cue number of peers shown d normalized click rate z z z a number of peers shown d normalized like rate z z z b fig average cue response function for subjects shown no peers d or one peer d when subjects have or peers that can be associated with the advertisement within panel differences reflect differences in the treatment condition and are significant with p for both click and like rates error bars are bootstrapped confidence intervals note that because some of the same users and ad ids contribute to each response rate overlapping intervals do not indicate non significance of associated differences and ratios number of affiliated peers z click risk ratio a number of affiliated peers z like risk ratio b fig relative increase in responses from including the minimal social cue error bars are bootstrapped confidence intervals selection of peers as top friends kahanda and neville recent studies of information consumption backstrom et al and diffusion have also used this measure bakshy et al because the validity of our tie strength measure requires that the user communicates via facebook during the day period prior to the experiment we restrict our analysis of tie strength to users who fall in the middle of the distribution of total communication count ci in particular all users in this section fall within the and percentiles of the count distribution in addition we use a percentile transformed total communication count q ci as a covariate that measures how much user i uses facebook messages and comments to communicate model in order to pool information across similar values of tie strength and to facilitate statistical inference we model responses to ads using logistic regression with natural splines we fit a model in which ad clicks are predicted by the presence of a minimal social cue dia for user i and ad a measured tie strength wij for user i with the affiliated peer j and the user percentile transformed total communication count q ci this model includes interactions of the minimal social cue with tie strength and with total communication count in particular the model is specified as yija α δdia τf wij ηdia f wij γq ci λq ci f wij where f is a natural spline basis expansion for measured tie strength with knots at the second and third quartiles of measured tie strength over all impressions we fit the same model to the data for the response of liking the page results response rates increase with tie strength both in the presence and absence of social cues figure shows predicted response rates for user ad pairs as a function of tie strength with solid and without dashed the minimal cue since response rates vary with total communication count we plot the results for users at the median of the total communication count ci the increase of response rates in both conditions is consistent with the expectation that social influence homophily and other sources of heterogeneity should all produce increased correlation in responses among strong tie peers compared to weak tie peers if influence is greater for strong ties then we should expect the increase in response rates that results from the display of social cues to be larger for strong ties than weak ties we find that this is the case both in terms of risk ratio and risk difference i e average treatment effect on the treated figure displays the click and like risk ratios for different values of measured tie strength these risk ratios increase with tie strength for example consider ad user pairs with wij and wij the percentile the stronger tie ad user pairs have a larger click risk ratio risk ratio difference of ci 160 and a larger like risk ratio risk ratio difference of ci 280 that is the relative increase from the social cue is larger for stronger ties even though the denominator of this ratio the response probability without the social cue is also larger for stronger ties conclusion we summarize the primary contributions of this work as follows first we rigorously measure social influence via social cues on an economically relevant form of user behavior we construct the average cue response function which gives the relationship between the number of social signals received by the individual and average rates of response the shape and slope of the cue response function differ dramatically from the na ıve observational estimates of social influence effects obtained through simple conditioning on the number of peers this difference illustrates the utility of experimentation in estimating peer effects second we demonstrate the substantial consequences of including minimal social cues in advertising this highlights that the most related model specifications yield qualitatively similar results specifically models also including a three way product interaction of dia q ci and f wij resulted in very similar fits models with only a linear term for tie strength wij rather than a natural spline produced have a statistically significant interaction between tie strength and the cue but suffered from model bias since the true relationship is non linear on the logistic scale both responses we fit the model separately to each of the r bootstrap replicates confidence intervals are computed as the and percentiles of the bootstrap distribution of predicted response rates tie strength normalized click rate a tie strength normalized like rate b fig estimated average response as a function of tie strength between the user and the single affiliated peer action rates increase with tie strength both in the presence d solid and absence d dashed of the minimal social cue featuring the affiliated peer each plot shows model fits via equation for users at the median total communication count i e q ci ranging from zero to the percentile of tie strength shaded regions are bootstrapped confidence intervals of the predicted response rate which are generated by fitting the model to r bootstrap replicates of the data tie strength click risk ratio 02 04 05 a tie strength like risk ratio 02 04 05 b fig estimated risk ratio of clicking and liking as a function of measured tie strength from the model in equation individuals with an affiliated peer with whom they communicate with more often have a higher relative increase in the probability of each response shaded regions are bootstrapped confidence intervals of the ratio between predicted responses with and without the cue subtle of forms of personalized social signals can play an important role in determining consumer responses to advertising above and beyond correlations due to homophily third we measure the positive relationship between a consumer response and the strength of their connection with an affiliated peer this relationship exists even when social cues are absent from advertisements furthermore cues have a stronger effect for stronger ties these results suggest that social advertising systems can benefit from incorporating tie strength measures into the selection of ads and social cues finally we hope that the explicit analysis of causal relationships in social advertising will aid researchers and decision makers to understand recent developments in marketing the present work has important limitations that suggest directions for future research first tie strength as a theoretical construct has been given multiple definitions and is operationalized in many ways we used transactional data about communication behavior as a measure of tie strength our choice of measure provides a readily interpretable and applicable measure but limited some analyses to consumers with sufficient levels of communication activity and future work could attempt to replicate our results for other measurements of tie strength including trust and intimacy second we could not randomize the tie strength of the peers who were affiliated with and referred to in the social cues in particular the population of ad user pairs is different for strong and weak ties so the analysis of experiment did not directly allow for causal inference about the effects of changes to tie strength of peers in social cues third we have only examined two methods for presenting social cues with online advertising the minimal social cue we examined in experiment is appealing in that it allows us to attribute its affects to social influence processes rather than simply increasing the size or general visual characteristics of the ad however many other ways of presenting social information to consumers are possible for example a more prominent display of peers association or activity around an entity combined with an advertiser generated creative may have a stronger response than any one of the two units on their own in addition future work may examine the effectiveness of such ad displays as well the impact of social signals on other outcomes such as ad recall and attitudes towards brands gibs and bruich finally it is important to note that we only estimate average effects over the population of facebook users that naturally see ads with social context individual responses to cues may vary substantially from user to user and further work is needed to understand how factors such as age gender number of friends or activity on site relate to the effect of social cues as one of next generation wireless communication systems third generation partnership project long term evolution lte is committed to provide technologies for high data rates and system capacity further lteadvanced lte a was defined to support new components manuscript received march revised july c xu l song x cheng and b jiao are with the state key laboratory of advanced optical communication systems and networks school of electronics engineering and computer science peking university beijing china e mail chen xu pku edu cn lingyang song pku edu cn xiangcheng pku edu cn jiaobl pku edu cn z han is with the electrical and computer engineering department university of houston houston tx usa e mail uh edu q zhao and x wang are with the docomo beijing communication lab beijing china e mail zhaoq docomolabs beijing com cn wangxl docomolabs beijing com cn this work was partially supported by the national project under grant by the national natural science foundation of china under grants and by the specialized research fund for the doctoral program of higher education of china and by the qatar national research fund us nsf cns eccs cns and cns digital object identifier jsac sup for lte to meet higher communication demands local area services are considered as popular issues to be improved and by reusing spectrum resources local data rates have been increased dramatically however the unlicensed spectrum reuse may bring inconvenience for local service providers to guarantee a stable controlled environment e g ad hoc network which is not in the control of the base station bs or other central nodes hence accessing to the licensed spectrum has attracted much attention device to device communication is a technology component for lte a the existing researches allow as an underlay to the cellular network to increase the spectral efficiency in communication user equipments ues transmit data signals to each other over a direct link using the cellular resources instead of through the bs which differs from femtocell where users communicate with the help of small low power cellular base stations users communicate directly while remaining controlled under the bs therefore the potential of improving spectral utilization has promoted much work in recent years which shows that can improve system performances by reusing cellular resources as a result is expected to be a key feature supported by next generation cellular networks although communication brings improvement in spectral efficiency and system capacity it also causes interference to the cellular network as a result of spectrum sharing thus an efficient interference coordination must be formulated to guarantee a target performance level of the cellular communication there exists several work about the power control of ues for restricting co channel interference the authors in utilized mimo transmission schemes to avoid interference from cellular downlink to receivers sharing the same resources interference management both from cellular to communication and from to cellular networks are considered in in order to further improve the gain from intra cell spectrum reuse properly pairing the cellular and users for sharing the same resources has been studied the authors in proposed an alternative greedy heuristic algorithm to lessen interference to the primary cellular networks using channel state information csi the scheme is easy operated but cannot prevent signaling overhead in the resource allocation scheme avoids 00 c ieee supplement emerging technologies in communications part the harmful interference by tracking the near far interference and identifies the interfering cellular users in the authors provided analysis on optimum resource allocation and power control between the cellular and connections that share the same resources for different resource sharing modes then the schemes are to further optimize the resource usage among users sharing the same resources based on the aforementioned work it indicates that by proper resource management communication can effectively improve the system throughput with the interference between cellular networks and transmissions being restricted however the problem of allocating cellular resources to transmissions is of great complexity our works differ from all mentioned above in that we consider a scheme to maximize the system sum rate by allowing multiple pairs share one cellular user spectrum resource game theory offers a set of mathematical tools to study the complex interactions among interdependent rational players and to predict their choices of strategies in the present researches game theory including a large number of different game methods are used to analyze resource allocation problems such as power and wireless spectrum allocations in communication networks resource management in grids and distributed resource coordination in mega scale container terminal a combinatorial auction model for resource management was introduced in the combinatorial auction based resource allocation mechanism allows an agent bidder to place bids on combinations of resources called packages rather than just individual resource unit actually the combinatorial auctions cas have been employed in a variety of industries for e g truckload transportation airport arrival and departure slots as well as wireless communication services the benchmark environment of auction theory is the private value model introduced by vickrey in which one bidder has a value for each package of items and the value is not related to the private information of other bidders much of work has not recognized that bidders care in complex ways about the items they compete the cas motivate bidders to fully express their preferences which is an advantage in improving system efficiency and auction revenues up to that point our interest is to apply the ca game in solving arbitrary links reusing the same cellular frequency bands with the purpose of optimizing the system capacity however it exists a series of problems and challenges in cas such as pricing and bidding rules the winner determination problem wdp which as mentioned in the literature leads to the np hard allocation problem therefore we focus on the evolution mechanisms named iterative combinatorial auctions i cas in i cas the bidders submit multiple bids iteratively and the auctioneer computes provisional allocations and ask prices in each auction round in this paper we study an effective spectrum resource allocation for communication as an underlay to further improve system efficiency based on the i ca the whole system consists of the bs multiple cellular users that receive signals from the bs and multiple pairs that communicate with respective receivers using licensed spectrum resources considering that interference minimization is a key point and multiple pairs sharing the same resources can bring large benefits on system capacity we formulate the problem as a fig system model of communication underlaying cellular networks with downlink resource sharing reverse i ca game that means the resources as the bidders compete to obtain business while links as the goods or services wait to be sold by this way the packages of pairs are auctioned off in each auction round furthermore we investigate some important properties of the proposed resource allocation mechanism such as cheat proof convergence and price monotonicity part of our work has been published in which introduces a sequential second price auction as the allocation mechanism for underlay communication and explains the detailed algorithm using an n ary tree in this work we further reduce the computational complexity and apply our scheme to winner ii channel models which contain a well known indoor scenario the simulation results show that the auction algorithm leads to a good performance on the system sum rate and provides high system efficiency while has lower complexity than the exhaustive search allocation the rest of the paper is organized as follows in section ii we describe the system model of the underlay cellular network and give the explanation and expression of the system sum rate the primary problem is formulated in section iii in section iv the resource allocation algorithm based on a reverse i ca is proposed in section v the main properties of the proposed algorithm are investigated in section vi we present the numerical simulation results and relevant analysis on the system sum rate algorithm efficiency and properties finally we draw the conclusions in section vii ii system model in this section we introduce the system model for underlay communication the scenario of multiple and cellular users is first described and then the expression of system sum rate is given a scenario description a model of a single cell with multiple users is considered as shown in fig ues with data signals between each other ieee journal on selected areas in communications supplement vol no september are in the communication mode while ues that transmit data signals with the bs keep in the traditional cellular mode each user is equipped with a single omnidirectional antenna the locations of cellular users and pairs are randomly set and traversing the whole cell without loss of generality we employ the uniform distribution to describe the user locations which is proposed for system simulation in notice that from stochastic geometry with for poisson distributions the users are uniformly located as well if the number of users is known for simplicity and clarity we illustrate co channel interference scenario involving three ues uec ued and ued and omit the interference and control signal signs among others uec is a traditional cellular user that is distributed uniformly in the cell ued and ued are close enough to satisfy the distance constraints of communication and at the same time they also have communicating demands one member of the pair ued is distributed uniformly in the cell and the position of the other member ued follows a uniform distribution inside a region at most l from ued the existing researches confirm that with power control or resource scheduling mechanism the inter cell interference can be managed efficiently therefore we place an emphasis on the intra cell interference which is due to resource sharing of and cellular communication generally speaking the session setup of communication requires the following steps a request of communicating is initiated by one ue pair the system detects traffic originating from and destined to the ue in the same subnet if the traffic fulfills a certain criterion e g data rate the system considers the traffic as potential traffic the bs checks if communication offers higher throughput if both ues are capable and mode offers higher throughput the bs may set up a bearer the cross layer processes of resource control can be contained in the above steps and be generally summarized as the transmitters both cellular and users send detecting signals then csi would be obtained by corresponding receivers and be feedback to the control center e g the bs the power control and spectrum allocation are conducted based on certain principles finally the bs sends control signals to users according to allocation results even if the connection setup is successful the bs still maintain detecting if ue should be back to the cellular communication mode furthermore the bs maintains the radio resource control for both cellular and communication based on these communication features our work mainly focuses on assigning cellular resources to communication in this paper we consider a scenario of sharing downlink dl resource of the cellular network as shown in fig we assume ued is the transmitter of the pair sharing the same sub channel with the bs and thus ued as the receiver receives interference from the bs also the cellular receiver uec is exposed to interference from ued in addition the users feed back the csi to the bs whereas the bs transmits control signals to the pair in the way that the system achieves power control and resource allocation during the dl period of the cellular system both cellular and users receive interference as they share the same subchannels here we assume that any cellular user resource blocks rbs can be shared with multiple pairs and each pair can use more than one user rbs for transmitting we assume the numbers of cellular users and pairs in the model are c and d respectively during the dl period the bs transmits signal xc to the c th c c cellular user and the d th d d pair uses the same spectrum resources transmitting signal xd the received signals at ue c and receiver d are written as yc pbhbcxc d βcd pdhdcxd nc yd pdhddxd pbhbdxc d βdd pd hd dxd nd where pb pd and pd are the transmit power of bs transmitter d d respectively hij is the channel response of the i j link that is from equipments i to j nc and nd are the additive white gaussian noise awgn at the receivers with one sided power spectral density psd βcd represents the presence of interference satisfying βcd when rbs of ue c are assigned to ue d otherwise βcd as a cellular user can share resources with multiple pairs it also satisfies d βcd d similarly βdd represents the presence of interference between pairs d and d in this paper the channel is modeled as the rayleigh fading channel and thus the channel response follows the independent identical complex gaussian distribution in addition the free space propagation path loss model p d α is used where and p represent signal power measured at and d away from the transmitter respectively α is the pathloss exponent hence the received power of each link can be expressed as pr ij pi ij pi dij α where pr ij and dij are the received power and the distance of the i j link respectively pi represents the transmit power of equipment i and is the complex gaussian channel coefficient that obeys the distribution cn and we simplify the received power at equals the transmit power b system sum rate for the purpose of maximizing the network capacity the signal to interference plus noise ratio sinr should be considered as an important indicator the sinr of user j is γj ij pint j where pint j denotes the interference signal power received by user j and accounts for the terminal noise at the receiver determined by the shannon capacity formula we can calculate the channel rate corresponding to the sinr of cellular and users as cellular users suffer interference from communicating that sharing the same spectrum resource the interference power of cellular user c is supplement emerging technologies in communications part pint c d dc while the interference of receiver d is from both bs and users that are assigned the same resources to the interference power of user d can be expressed as pint d bd d βdd pd d d based on and we can obtain the channel rate of cellular user c and receiver d as rc bc d dc rd dd bd d βdd pd d d respectively here d d so d βdd pd d d represents the interference from the other pairs that share spectrum resources with pair d the dl system sum rate can be defined as c c rc d d βcdrd in the next section we formulate the problem of designing βcd for each pair as an optimization issue of maximizing iii problem formulation in this section we introduce two concepts valuation model and utility function which are bases of the auction mechanism also some definitions are given a valuation model as communication shares the same spectrum resources with cellular communication at the same time slot the cochannel interference should be limited as much as possible to optimize the system performance the radio signals experience different degrees of fading and thus the amount of interference depends on transmit power and spatial distances accordingly we focus on assigning appropriate resource blocks rbs occupied by cellular users to pairs in order to minimize interference to achieve a higher system sum rate next we formulate the relation between the allocation result and the rate of the shared channel the relation can be defined as a value function whose target value is the channel rate we define d as a package of variables representing the index of pairs that share the same resources we assume the total pairs can form n such packages thus if the members of the k th k n user package share resources with cellular user c the channel rates of ue c and pair d d dk can be written as rk c bc d dk dc rk d dd bd d dk d pd d d respectively the rate of the operating channel shared by ue c and pairs d dk is rck rk c d dk rk d according to when assigning resources of ue c to the k th package of pairs the channel rate is given by vc k bc d dk dc d dk dd bd d dk d pd d d in the proposed reverse i ca mechanism we consider spectrum resources occupied by cellular user c as one of the bidders who submit bids to compete for the packages of pairs in order to maximize the channel rate it is obvious that there would be a gain of channel rate owing to communicating as long as the contribution to data signals from is larger than that to interference signals considering the constraint of a positive value we define the performance gain as vc k max vc k vc which is the private valuation of bidder c for the package of pairs dk here vc denotes the channel rate of ue c without co channel interference and is obtained by vc bc thus we have the following definition definition a valuation model v vc k is a set of the private valuations of all bidders c c for all packages dk d k n b utility function in the auction the cellular resource denoted by c obtains a gain by getting a package of communications however there exists some cost such as control signals transmission and information feedback during the access process we define the cost as a pay price definition the price to be payed by the bidder c for the package dk is called pay price denoted by pc k the unit price of item d k d dk can be denoted by pc d here we consider linear anonymous prices which means the prices are linear if the price of a package is equal to the sum of the prices of its items and the prices are anonymous if the prices of the same package for different bidders are equal thus we have pc k d dk pc d d dk p d c c ieee journal on selected areas in communications supplement vol no september therefore the payment of a bidder is determined by the unit price p d and the size of bidding package dk definition bidder utility or named bidder payoff uc k expresses satisfaction of bidder c getting package dk the bidder utility can be defined as uc k vc k pc k based on vc k in and vc in we can obtain the utility of bidder c as uc k bc d dk dc d dk dd bd d dk d pd d d bc d dk p d in order to describe the allocation outcome intuitively we give the definition below definition the result of the auction is a spectrum allocation denoted by x xc which allocates a corresponding package to each bidder and the allocated packages may not intersect i j xi xj we consider a set of binary variables xc k to redefine the allocation as xc k if xc dk otherwise according to the literature two most popular bidding languages are exclusive or xor which allows a bidder to submit multiple bids but at most one of the bids can win and additive or or which allows one to submit multiple bids and any non intersecting combination of the bids can win we consider the xor bidding language in this paper thus satisfies n k xc k and n k xc k xc for c c if given an allocation x the total bidder utility of all bidders can be denoted as u all x c c n k xc k uc k furthermore the auctioneer revenue is denoted as a x c c n k xc k pc k which is usually considered to be the auctioneer gain iv resource allocation algorithm based on reverse iterative combinatorial auction in this section we formulate the resource allocation for communication as a reverse i ca game first we introduce some concepts of the i ca games then we investigate details of the allocation process a reverse iterative combinatorial auction game as mentioned before we assume the total spectrum resources are divided into c units with each one already providing communication service to one cellular user by the auction game the spectrum units are assigned to n user packages dn with each package consisting of at least one pair in other words the spectrum units compete to obtain communication for improving the channel rate during an i ca game the auctioneer announces an initial price for each item and then the bidders submit to the auctioneer their bids at the current price as long as the demand exceeds the supply or on the contrary that the supply exceeds the demand the auctioneer updates raises or reduces the corresponding price and the auction goes to the next round obviously it can be shown that the overall gain which includes the total gain of the auctioneer and all bidders does not depend on the pay price but equals to the sum of the allocated packages valuations i e a x uall x c c n k xc k pc k c c n k xc k uc k c c n k xc k pc k c c n k xc k vc k pc k c c n k xc k vc k as our original intention we employ the i ca to obtain an efficient allocation for spectrum resources definition an efficient allocation denoted by x x xc xc k is an allocation that maximizes the overall gain given the private bidder valuations for all possible packages in an efficient allocation can be obtained by solving the combinatorial allocation problem cap definition the combinatorial allocation problem cap also sometimes referred as winner determination problem wdp leads to an efficient allocation by maximizing the overall gain max dk xc x x c c vc k where x denotes the set of all possible allocations an integer linear program using the binary decision variables xc k is formulated for the cap as max c c n k xc k vc k t n k xc k c c dk d dk c c xc k d d the objective function maximizes the overall gain and the constraints guarantee at most one package can be allocated to each bidder each item cannot be sold more than once in fact there might be multiple optimal solutions of the cap with the same objective function from the auctioneer point of view tie breaking rules are needed to determine which of the optimal solutions is selected in a real auction the auctioneer does not know the private valuations of the bidders neither can it solve the np hard problem to solve the cap the auctioneer selects the winners on the basis of the submitted bids in each round therefore in case of the xor bidding language the wdp formulation is similar to the cap and the supplement emerging technologies in communications part only difference is the objective function max c c n k xc k pt c k where pt c k represents the pay price of bidder c for package dk in round t based on definition the overcome of a ca is not always efficient here we employ allocating efficiency as a primary measure to benchmark auctions definition allocating efficiency in cas can be expressed as the ratio of the overall gain of the final allocation to that of an efficient allocation e x a x uall x a x uall x which has e x b algorithm for resource allocation in this subsection the details of the allocation scheme based on reverse i ca are introduced we has modeled the resource allocation problem as a reverse i ca game and gave the valuation model utility function and other important concepts many i ca designs especially for the centralized i ca design are based on ask prices the pricebased i ca designs differ by the pricing scheme and price update rules in the proposed algorithm linear prices are used as mentioned in subsection iii b for they are easy to understand for bidders and convenient to communicate in each auction round because of the interference from links cellular channels should guarantee the performance of cellular system before allowing the access hence we consider a descending price criterion in the algorithm prices update by a greedy mode that once a bidder submits a bid for items or packages the corresponding prices are fixed otherwise the prices are decreased at the beginning of the allocation the bs collects the location information of all the pairs in addition the round index t the initial ask price d for each item pair d and the fixed price reduction δ are set up when the initial prices are announced to all the bidders i e spectrum resources occupied by cellular ues each bidder submits bids which consist of its desired packages and the corresponding pay prices jump bidding where bidders are allowed to bid higher than the prices is not allowed in our scheme thus bidders always bid at the current prices according to the cap proposed in definition and the analysis about the wdp we simplify the problem of maximizing the overall gain as a process of collecting the highest pay price as a result bidder c bids for package dk as long as uc k combining and we have vc k pt c k d dk pt d where the round index t in this case let bt c k dk pt c k denote the submitted bid at the end of round t and bt bt c k denotes all the bids when is not satisfied bid bt c k if d dk satisfies bt c k bt dk bt c k it reveals that the supply exceeds table i the resource allocation algorithm initial state the bs collects the location information of all pairs the valuation of the c th resource unit for package k is vc k c c k n which is given by the round index t and the initial price d the fixed price reduction δ are set up resource allocation algorithm bidder c submits bids dk pc k depending on its utility bidder c bids for package dk as long as uc k which is represented by if uc k bidder c submits if d dk satisfies bt c k bt dk bt c k the bs sets t t pt d pt d δ where d is the over supplied item and the auction moves on to the next round return to step the bs detects the bids of all the bidders it exists bt k bt k k n it exists bt pt bt pt c satisfying if neither of the conditions in step is satisfied go to step otherwise the overall demand exceeds supply for at least one item the bs sets pt d pt d δ and δ can be set by δ δ i where i is an integer factor return to step the allocation can be determined by repeating the above steps the auction continues until all links are auctioned off or every cellular channel wins a package the demand then the bs sets t t pt d pt d δ where d is the over supplied item and the auction moves on to the next round in a normal case as long as the price of a package decreases below a bidder valuation for that package the bidder submits a bid for it the bs allocates the package to the bidder and fixes the corresponding prices of items at the same time constrained by the xor bidding language the bidder is not allowed to participate the following auction rounds as the asking prices decrease discretely every round it may exist a situation that more than one bidders bid for packages containing the same items simultaneously the bs detects the bids of all the bidders it exists bt k bt k k n it exists bt pt k1 bt pt k2 k1 k2 c2 c satisfying dk2 if either of the above conditions is satisfied the overall demand exceeds supply for at least one item then the bs sets a fine tuning pt d pt d δ where d is the temporary over demanded item and δ can be set by δ δ i where i is an integer factor that affects the convergence rate the allocation can be determined by multiple iterations the auction continues until all the links are auctioned off or every channel wins a package our algorithm is detailed in table i v analysis of the proposed resource allocation algorithm in this section we investigate the important properties of the proposed auction based resource allocation mechanism a cheat proof as the general definition cheat proof means that reporting the true demand in each auction round is a best response for each bidder ieee journal on selected areas in communications supplement vol no september proposition the resource allocation algorithm based on the reverse i ca is cheat proof proof from we can get that the utility of bidder uc k depends on the valuation of the package it bids and unit prices of the items in details it is the interference between cellular and communications that mainly affects the utility as the expression is extremely complex to resolve we consider the case that only one item constitutes the package without loss of generality the utility of bidder c can be rewritten as uc d bc dc dd bd bc pt d and the differential expressions of the utility with respect to hdc and hbd are uc d hdc bc ln dc bc dc uc d hbd dd ln bd dd pbh2 bd respectively accordingly utility uc d is a monotonically decreasing function with respect to both hdc and hbd thus the optimal strategy is to bid the link that has a lower channel gain with the cellular transmitter and receiver in a descending price auction items are always too expensive to afford at the beginning with the number of iterations t increasing the prices of items drop off given a package dk in round t bidder c has the right to submit bid dk pt c k or given that all the other bidders submit their true demands according to we consider the strategy of bidder c in two cases if c bids when its true valuation for dk satisfies uc k it will quit this round and lose the package which maximizes its channel rate if c bids dk pt c k when its true valuation for dk satisfies uc k and finally wins the package it will obviously get a negative surplus that is unwanted from the above analysis we can conclude that the optimal strategy for cellular channel c is to submit its true demand in each round or it will get a loss in its utility as a result of any deceiving that is the proposed resource allocation algorithm is cheat proof b convergence in this subsection we prove that the proposed algorithm has the convergence property proposition the resource allocation algorithm based on the reverse i ca has the convergence property that the number of the iterations is finite proof according to theorem all the bidders submit their true demands in each auction round in order to obtain the utility from winning from we can derive ut c ut c δ where ut c denotes the utility of bidder c in round t according to the algorithm we have that bidder c will get zero utility with no bid if ut c and have an opportunity to win a positive utility with bid dk pt c k if ut c therefore in the beginning bidder c plays a waiting game and once ut c k it will bid for dk as long as it is the only one that submits a bid it will get the package with a sufficiently large t and δ we can finally get xc k similarly if more than one bidders bid for the same item we can have an allocation by ascending price process with the step δ δ subjected to dk d dk c c xc k in the package can not be sold once more thus for a finite number of packages n the number of iterations is finite that is the proposed scheme would reach convergence in addition the value of the price step δ has a direct impact on the speed of convergence of the proposed scheme the scheme converges fast when δ is large while it converges slowly when δ is small the fine tuning δ also has the same nature but less impact on convergence c price monotonicity in an i ca game the price updates through several ways i e monotonically increasing monotonically decreasing and non monotonic modes here we focus on the price nonmonotonicity in the proposed reverse i ca algorithm proposition in the proposed descending price auction the raising item prices in a round may be necessary to reflect the competitive situation moreover it brings efficiency improvement proof from the algorithm proposed in table i there exists a situation that more than one bidders submit bids for the same package or different packages with intersection when prices are reduced to some certain values but auctions do not allow one item being obtained by multiple bidders as the second constraint in shows in this situation raising the corresponding prices by a fine tuning δ δ i makes bidders to reinspect their utility functions once a bidder finds its utility less than zero it quits from the competition by a finite number of iterations the winner converges to one bidder since the ascending price process maximizes the auctioneer revenue as shown in the allocation has higher efficiency than a random allocation in that situation d complexity as mentioned before a traditional cap in fact is an nphard problem the normal solution of which is the centralized exhaustive search we set that the number of items to be allocated is m and the number of bidders is n for an exhaustive optimal algorithm an item can be allocated with n possible results thus all the m items are allocated with nm possible results the complexity of the algorithm can be denoted by o nm in the proposed reverse i ca scheme bidders reveal their entire utility function i e they calculate valuations for all possible packages the number of which is cm m if the total number of iterations is t the complexity of the auction based scheme is o n t from the proposed algorithm we have pt d d δ t the fine tuning has a small impact on the result and can be omitted here so the worst case is t d δ it is obvious that for sufficient large values of m and n general values of d and δ a much supplement emerging technologies in communications part table ii main simulation parameters parameter value cellular layout isolated cell sector system area the radius of the cell is m noise spectral density dbm hz sub carrier bandwidth khz noise figure db at device antenna gains bs dbi device dbi the maximum distance of m transmit power bs dbm device dbm lower complexity is obtained by using the proposed reverse i ca scheme that is o nm o n d δ if we constrain the number of pairs sharing the same channel to one the complexity would be further reduced to o n m d δ and the performance of this reduced scheme is included in the simulation in subsection vi a e overhead in underlay system the bs is still the control center of resource allocation and the global csi should indeed be available at the bs for the proposed scheme in addition to the csi detection feedback and the control signaling transmission the reverse i ca scheme does not need additional signaling overhead compared to existing resource scheduling schemes such as maximum carrier to interference max c i and proportional fair pf which also need the global csi to optimize the system performance the difference is that the reverse i ca scheme requires more complicated csi due to the interference between and cellular network at the beginning of the allocation the transmitters need to send some packets containing detection signals then the obtained csi at each terminal or cellular receiver would be feedback to the bs after that iteration process would be conducted at the bs and no signaling needs to be exchanged among the network nodes until the control signals forwarding methods such as csi feedback compression and signal flooding would help reduce the overhead in addition the future work on communication could consider some mechanism that limit the number of pairs sharing the same channel by e g distance constraint which would obviously help reduce the overhead but for this paper the target is to obtain the nearest optimal solution wherefore we do not consider the simplification vi simulation results and discussions in this section we provide the simulation results to illustrate the performances of the proposed reverse i ca algorithm besides we give the necessary analysis for the results the main simulation parameters are listed in table ii as shown in fig simulations are carried out in a single cell both pathloss model and shadow fading are considered for cellular and links the wireless propagation is modeled according to winner ii channel models and channel is based on office indoor scenario while cellular channel is based on the urban microcell scenario 115 number of pairs sum rate bit hz 136 exhaustive optimal r i ca reduced r i ca random allocation fig system sum rate for different allocation algorithms in the case of resource units a system sum rate the system sum rate with different numbers of pairs and different numbers of resource units using the proposed auction algorithm is illustrated in fig fig the sum rate can be obtained from from fig and fig we can see that the system sum rate goes up with both the number of pairs and the number of resource units increasing on one side when the amount of resources is fixed more users contribute to a higher system sum rate on the other side as the amount of resource increases the probability of resources with less interference to links being assigned to them enhances which can lead to the increased sum rate this phenomenon is similar to the effect of multiuser diversity definitely cellular users also contributes to the performance from another perspective fig fig shows the system sum rate for different allocation algorithms the curve marked exhaustive optimal is simulated by the exhaustive search way which guarantees a top bound of the system sum rate the curve marked reduced r i ca is the result of a reduced reverse i ca scheme in which the number of pairs sharing the same cellular resources is constrained to one the curve marked r i ca represents the performance of the proposed reverse i ca algorithm and the last one is the simulation result using random allocation of spectrum resources firstly we can see that the proposed auction algorithm is relatively much superior to the random allocation secondly the optimal allocation results in the highest system sum rate but the superiority compared to r i ca is quite small especially when the number of cellular resource units increases as fig shows moreover we find that the performance of reduced r i ca approximates to that of r i ca scheme in case of resource units but differs obviously in case of resource units shown in fig the reason for this phenomenon is that the constraint of the reduced r i ca limits pairs accessing to the network when the number of resources units is less than that of pairs thus a large capacity loss products ieee journal on selected areas in communications supplement vol no september 55 number of pairs sum rate bit hz exhaustive optimal r i ca reduced r i ca random allocation fig system sum rate for different allocation algorithms in the case of resource units number of cellular resource units sum rate bit hz exhaustive optimal r i ca reduced r i ca random allocation fig system sum rate for different allocation algorithms in the case of pairs b system efficiency we define the system efficiency as η opt where opt represents the exhaustive optimal sum rate fig shows the system efficiency with different numbers of pairs and different numbers of resource units the simulation result indicates that the proposed algorithm provides high the lowest value of η is around system efficiency moreover the efficiency is stable over different parameters of users and resources as to the point of efficiency value being about the number of resource units and the number of pairs are both small the linear price rule limits bidders to bid the maximal valuation packages but to bid the packages having maximal average unit valuation for this reason the efficiency decreases slightly as to other points the efficiency is stable above which reflects a small performance gap between the proposed algorithm and the exhaustive search scheme in fact the descending price rule determines the bidder that has the highest bid on current items would win the corresponding package which maximizes the current overall gain however number of resource units c number of pairs d efficiency fig system efficiency η with different numbers of pairs and different numbers of resource units 25 iteration times unit price d2d4 fig price monotonicity an example of price non monotonicity in the reverse i ca scheme the gap cannot be avoided as the algorithm essentially follows a local or an approximate global optimum principle c price monotonicity fig shows an example of the price non monotonicity in the reverse i ca scheme the four curves represent unit price of four pairs as the enlarged detail shows the unit price of pair has an ascending process during the auction as the step δ is much less than descending step δ the phenomenon of ascending price is hard to pick out when the items have been sold out their prices are fixed to the selling value and from the figure we can find that the pair is the last one to be sold vii conclusions in this paper we have investigated how to reduce the effects of interference between and cellular users in order to improve the system sum rate for a underlay network we have proposed the reverse iterative combinatorial auction as the mechanism to allocate the spectrum resources for supplement emerging technologies in communications part communications with multiple user pairs we have formulated the valuation of each pair for each resource unit and then explained a detailed auction algorithm depending on the utility function a non monotonic descending price iteration process has been modeled and analyzed to be cheat proof converge in a finite number of rounds and has low complexity the simulation results show that the system sum rate goes up with both the number of pairs and the number of resource units increasing the proposed auction algorithm is much superior to the random allocation and provides high system efficiency which is stable over different parameters of users and resources models of networked diffusion that are motivated by analogy with the spread of infectious disease have been applied to a wide range of social and economic adoption processes including those related to new products ideas norms and behaviors however it is unknown how accurately these models account for the empirical structure of diffusion over networks here we describe the diffusion patterns arising from seven online domains ranging from communications platforms to networked games to microblogging services each involving distinct types of content and modes of sharing we find strikingly similar patterns across all domains in particular the vast majority of cascades are small and are described by a handful of simple tree structures that terminate within one degree of an initial adopting seed in addition we find that structures other than these account for only a tiny fraction of total adoptions that is adoptions resulting from chains of referrals are extremely rare finally even for the largest cascades that we observe we find that the bulk of adoptions often takes place within one degree of a few dominant individuals together these observations suggest new directions for modeling of online adoption processes categories and subject descriptors j social and behavioral sciences sociology general terms economics measurement additional key words and phrases computational social science contagion diffusion social networks introduction a longstanding hypothesis in diffusion research is that adoption of products and ideas spreads through interpersonal networks of influence analogous to the manner in which an infectious disease spreads through a susceptible population anderson and may accordingly theoretical models of diffusion have generally imitated disease models in the sense that popular products are assumed to diffuse multiple steps from their origin in the manner of epidemics infecting large numbers of people in the process watts leskovec et al although this assumption is entirely plausible empirical diffusion research has historically relied on aggregate data such as cumulative adoption curves coleman et al bass young iyengar et al which reveal only the total number of adopters at any given time while these curves are consistent with the hypothesis of viral disease like diffusion they are also consistent with other mechanisms such as marketing or mass media van den bulte and lilien as a consequence the extent to which adoption processes are driven by these different mechanisms and therefore the extent to which prevailing models accurately describe online diffusion is unknown in recent years the increased availability of online social interaction data has offered new opportunities to map out the network structure of diffusion processes adar author addresses microeconomics and social systems yahoo research west street new york ny correspondence may be sent to s g at goel yahoo inc com permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation copyrights for components of this work owned by others than acm must be honored abstracting with credit is permitted to copy otherwise to republish to post on servers to redistribute to lists or to use any component of this work in other works requires prior specific permission and or a fee permissions may be requested from publications dept acm inc penn plaza suite new york ny usa fax 0481 or permissions acm org ec june valencia spain copyright acm 00 623 and adamic leskovec et al bakshy et al sun et al bakshy et al here we leverage online diffusion data to address the question of how much total adoption derives from viral spreading versus some other process as well as the implications of this empirical result for theoretical models of diffusion in particular online diffusion in order to identify generic features of online diffusion structure we study seven diverse examples comprising millions of individual adopters as opposed to biological contagion our domain of interest comprises the diffusion of adoptions where adoption implies a deliberate action on the part of the adopting individual in particular we do not consider mere exposure to an idea or product to constitute adoption contagious processes such as email viruses which benefit from accidental or unintentional transmission are therefore excluded from consideration although restricted in this manner the range of applications that we consider is broad the seven studies described below draw on different sources of data were recorded using different technical mechanisms over different timescales and varied widely in terms of the costliness of an adoption this variety is important to our conclusions as while each individual study no doubt suffers from systematic biases arising from the particular choice of data and methods collectively they are unlikely to all exhibit the same systematic biases to the extent that we observe consistent patterns across all examples we expect that our findings should be broadly applicable to other examples of online and possibly offline diffusion as well the remainder of this paper proceeds as follows after reviewing the diffusion literature in section in section we describe in detail the seven domains we investigate we present our main results in section showing that not only are most cascades small and shallow but also that most adoptions lie in such cascades in particular it is rare for adoptions to result from chains of referrals finally in section we discuss the implications of these results for diffusion models as well as the apparent discord between our results and the prevalence of popular products such as facebook and gmail whose success is often attributed to viral propagation related work the adoption of new products or behaviors has been described rhetorically in the language of contagion for centuries mackay le bon by the late the conceptual link between adoption and diffusion had been greatly reinforced by their enshrinement in simple mathematical models which were derived by analogy either from mass action laws of chemical reactions coleman et al or from classical models of mathematical epidemiology bass these various models however all embodied the core substantive assumption that new adopters are influenced by the proportion of the population that has adopted previously the main empirical prediction of these models was that viewed over time the cumulative number of adopters in a population ought to be described by an s shaped curve a prediction that was at least broadly consistent with a large number of empirical studies rogers bass subsequently a large literature has developed in marketing and related fields that has explored the mathematical properties of disease like models in addition related but distinct families of models have emerged that begin from different assumptions about the psychology of the adoption process granovetter and others lopez pintado and watts for example have argued that many adoption decisions especially costly ones are more appropriately modeled as a nonlinear threshold whereupon adoption takes place only after some critical number or fraction of some sample population had adopted dodds and watts have proposed a model of generalized contagion that contained both disease like and threshold models as spe cial cases while young has proposed a model of observational learning that also exhibits threshold like behavior although they differ in their psychological motivation and formal properties a common feature to all these models is that a small seed set of initial adopters possibly just one can under the right circumstances trigger a much larger cascade of subsequent adoptions that builds upon itself over multiple possibly many generations the dynamics and precursors of large cascades or social epidemics as they are also known gladwell have also been studied extensively in networked models of adoption where the classical uniform mixing assumption namely that the probability of adoption depends only on the global ratios of adopters and non adopters anderson and may is replaced with the assumption that individuals are influenced only by some relatively small number of network neighbors moore and newman watts an important contribution of these network diffusion models is that unlike in the case of uniform mixing for which all seeds are effectively interchangeable both the local and global structural properties of networks can greatly influence the size and likelihood of a cascade that is triggered by any given seed motivated by this observation a growing number of papers have addressed the natural question of how seeds can be selected so as to maximize the total amount of influence that some exogenous agent can hope to exert over a network with some given structure assuming a simple variant of a network influence model for example domingos and richardson estimated the viral lift that could be attained by incentivizing a single consumer to adopt subsequently kempe et al found approximately optimal algorithms for selecting a maximally influential set of seeds for a variety of simple models and recently kitsak et al showed that for simple disease like models of adoption diffusion was maximized when initiated by individuals in the dense core of the network whereas much of the earlier literature focuses on the conditions required for a large cascade to occur at all the influence maximization literature focuses on the operational question of how to efficiently trade off between the cost of seeding and the size of the cascade generated nevertheless both literatures start from the assumption that a relatively small number of seeds can trigger a relatively large number of adoptions via some usually multistep diffusion process in their numerical experiments for example kempe et al find that depending on the assumed infection probability optimal targeting can generate cascades from several times to several hundred times the size of the seed set domingos et al report an even more extreme result that the viral lift from a single targeted consumer can be as high as others finally many models of adoption bass granovetter watts find that under the right circumstances cascades triggered by a single seed can encompass the entire population regardless of size whether it is stated explicitly or not in other words the assumption that a relatively small number of seeds can trigger a relatively large number of adoptions via some usually multistep diffusion process is central to many of the interesting questions posed by the modeling literature but this assumption provokes two related empirical questions how frequently do such large cascades occur in real diffusion processes and how much total adoption do they account for the central contribution of this paper is to directly address these two questions in the context of online diffusion answering these questions has been difficult for offline adoption processes for which the data have historically suffered from two limitations first most empirical diffusion studies have relied on aggregated data reflecting the total number of adoptions in a population as has been pointed out elsewhere van den bulte and lilien although these adoption curves are consistent with diffusion processes they are also consistent with alternative explanations such as marketing or advertising efforts or simply heterogeneity in adoption propensity to address this problem individual level diffusion data are required meaning that one can observe precisely who influences whom to adopt which product at a given time in an offline context however data of this kind are extremely difficult to collect especially at the scale required to study large cascades spreading over potentially many generations of adopters a second problem with empirical diffusion studies has been that they are highly subject to selection bias as examples of successful diffusion are much easier to observe than examples of unsuccessful ones liben nowell and kleinberg more generally without a representative sample of diffusion attempts it is difficult to quantify the overall rate at which diffusion takes place or how much total adoption it accounts for a major advantage of studying diffusion in an online context therefore is that it is increasingly possible to overcome these limitations allowing researchers first to obtain very large samples of potential online diffusion for example all news stories published on twitter all videos posted to youtube or all third party applications launched on facebook regardless of whether they actually diffuse or not and second to map out the precise network structure of the corresponding diffusion processes as they propagate from individual to individual in recent years a number of studies adar and adamic bakshy et al sun et al bakshy et al have made use of online data to study various diffusion processes as we discuss later the results of these studies are consistent with ours however they have not directly addressed the central question of how much total adoption derives from viral spreading versus some other process of particular relevance is leskovec et al who analyzed product recommendations in a network of users of an e commerce website with the primary objective of enumerating and counting the types of diffusion cascades that arose as we do they find that most cascades are small however they do not consider the subsequent and from our perspective key question of how much adoption is accounted for by the minority of large cascades in addition to this different objective we note that leskovec et al investigate one domain with a high barrier to adoption users must receive a recommendation and purchase a product to be counted as adopters in contrast we compare seven domains in which the type and the cost of adoption ranges greatly from retweeting on twitter to sending an email to purchasing furthermore our analysis covers domains in which there is ambiguity as to which is the parent node in the cascade as in the recommendations domain of leskovec et al but also domains in which parents are uniquely identified through tracking urls the larger point is that because each prior study of online diffusion focused on a single web platform such as facebook sun et al twitter bakshy et al second life bakshy et al or blog networks adar and adamic and also invoked different metrics it has previously been difficult to draw conclusions about online diffusion processes in general data and methods for each domain we define a diffusion event or cascade watts kempe et al as comprising a seed individual who takes the relevant adoption action independently of any other individual in our dataset followed by other non seed individuals who are influenced either directly or indirectly by the seed to take the same action from this definition it follows that every individual in a diffusion event can be modeling efforts have tended to focus on the mechanics of successful diffusion rather than on its likelihood for example the bass model deterministically predicts universal diffusion and is hence unable to account for attempts at diffusion that do not succeed connected by some unbroken path of adoptions back to the original seed hence each diffusion event can be represented as a single connected graph moreover in instances where an adopter may have been influenced by more than one previous adopter we ascribe influence exclusively to the earliest such parent node thus in contrast with related work leskovec et al where multiple parents are allowed we map all diffusion events to tree structures that originate with a single seed and terminate at one or more leaf nodes finally having reconstructed all distinct diffusion events we calculate two key quantities first the frequency of distinct diffusion structures trees that we observe and second the proportion of total adoptions that are accounted for by each of these structures given the scale of the data calculations are carried out with the mapreduce parallel computation framework dean and ghemawat before describing the data in detail we note that although the definitions just outlined are consistent across all seven domains the manner by which we infer influence differs across them in three of the applications we directly observe interpersonal diffusion influence whereas in the remaining four we infer influence from the underlying network of interpersonal connections and the temporal sequence of adoptions although in the latter case it is possible that apparent influence can be accounted for simply by homophily aral et al shalizi and thomas our inferred diffusion trees nevertheless provide an effective upper bound on the actual diffusion taking place via the respective underlying networks moreover while it is possible that some amount of diffusion goes undetected e g if it occurs over a channel such as literal word of mouth that we cannot track given the relative ease with which one can share content via twitter facebook email and other modes of electronic communication we suspect this unobserved adoption accounts for only a small fraction of total adoptions of the online products investigated here observed diffusion domains yahoo kindness over a one month period in yahoo philanthropic arm launched a website kindness yahoo com that asked users to create status updates describing kind acts they had performed after which these updates were propagated via yahoo facebook twitter and other means in order to attract new users to visit the site and post updates of their own because all users were logged in and because each user received a unique coded url when arriving at the site e g kindness yahoo com that was used to bring others to the site it was possible to trace the chain of adopters through which each new user arrived because this tracking method works regardless of how people chose to share links e g by email blog instant message status update forum etc we could reconstruct the diffusion of the new site across the internet during the course of the experiment approximately users adopted the campaign meaning that they visited the site and also posted at least one status update zync liu et al shamma and liu is a plug in for yahoo messenger an instant messaging im application that allows pairs of users to watch videos synchronously while sending instant messages to one another individuals initiate a session by sending a video url to another user through the yahoo instant messaging client the invited user must accept the invitation before the video commences thus in contrast with our other examples a single use of zync requires two users since zync has been activated by approximately million users however to avoid counting spurious dyads we define adoption in this instance the structural simplicity of the vast majority of cascades we find detailed in section our results are qualitatively the same regardless of which particular cascade representation is chosen as having initiated a session not merely having accepted an invitation yielding adopters the secretary game is an online variant of the secretary problem ferguson a sequential search game in which players attempt to find an optimal stopping rule players are encouraged to share the game url with at least three other people with an explanation that the game designers are seeking the world best players as with yahoo kindness user specific urls tracked player to player diffusion between and the game was played over times by nearly adopters inferred diffusion domains in the following cases we observe time stamped adoptions occurring over a known network here we did not directly observe interpersonal transmission of information but rather inferred that the parent of an adopting node if one existed was its first network contact to adopt before it did twitter news stories we tracked the diffusion of news stories posted on the microblogging service twitter during november where the original article was distributed by one of five popular news sites the new york times cnn msnbc yahoo news and the huffington post individuals were said to have adopted an article if they posted i e tweeted a link to the story in total we observed adoption events to mitigate left censoring we only counted urls that had not appeared for two weeks prior to the first adoption we observed i e any previous diffusion must have occurred at least two weeks prior and then completely disappeared only to restart because the timescale of diffusion on twitter lasts only a few days in the vast majority of cases this approach avoids almost all possible left censoring correspondingly we addressed the possibility of rightcensoring by considering only diffusion events that had been initiated at least two weeks prior to the end of the observation period all cascades where thus observed for at least two weeks twitter videos analogous to the news articles we tracked youtube videos posted on twitter during november where users were again said to have adopted a particular video if they tweeted a link to it million adoption events were observed friend sense was a third party facebook application that queried respondents about their political views as well as their beliefs about their friends political views goel et al because the analysis required knowing the friends actual views various sharing features were built into the application to encourage viral growth over the course of a four month period in close to individuals used the application providing over answers to more than questions yahoo voice is a paid service launched in that allows users to make voiceover ip calls to phones through yahoo messenger between and million users purchased voice credits all of whom we define to be adopters diffusion in this case is considered to occur over the yahoo messenger im network which comprises over million users with a median of network neighbors each where two individuals are connected if they list each other as a buddy i e social contact tree canonicalization in computing the frequency of cascade structures across the diffusion datasets care needs to be taken to aggregate all variations of the same fundamental structure for x x x x x x x fig canonical names for some example trees example the two trees below while superficially different should clearly be taken to represent the same cascade structure for the purposes of our analysis in particular we would like to aggregate all isomorphic trees i e trees that are identical under a relabeling of the vertices definition tree isomorphism consider two rooted trees and r2 with vertices vi edges ei vi vi and roots ri vi then and are isomorphic if there exists a bijection φ v2 such that φ r2 and φ φ e2 determining whether two arbitrary graphs are isomorphic is in general a difficult computational problem no polynomial time algorithm has been found and somewhat surprisingly it is not even know whether the problem lies in either p or np complete in the case of trees however there are standard and efficient ways for determining isomorphism one of which we detail below definition the canonical name c t of a rooted tree t is a string defined inductively on the height of the tree by the following two rules basis the canonical name for the one node tree is x induction if t has more than one node let tk denote the subtrees of the root indexed such that c c c tk under the lexicographic order then the canonical name for t is c c tk figure shows the canonical names for a few example tree structures it is clear that two trees have the same canonical name if and only if they are isomorphic moreover as shown in aho et al c t can be computed in time linear in the number of nodes these canonical tree names thus allow us to efficiently aggregate all isomorphic variations of each cascade structure results before presenting our results we note that in addition to variations in the method of data collection our examples also varied widely with respect to a number of dimensions thought to be important to adoption decisions such as the costliness of the adoption the nature of the network over which the adoptions are diffusing and the timescale on which the diffusion process proceeded for example whereas yahoo voice represents an adoption decision that costs money adoptions in the gaming domains are costly only in time and twitter retweets are next to costless whereas the im network represents a network of reciprocated private communication ties the majority of facebook edges do not involve active communication and edges on twitter are largely density 03 all else tree size ccdf 30 tree depth y kindness zync secretary game twitter news twitter videos friendsense y voice a b c fig the distribution of diffusion cascade structures unreciprocated and public whereas zync friend sense and yahoo voice clearly exhibit positive externalities in the sense that the utility of the products in question increases with the number of adopting neighbors such network effects are less likely in the remaining domains and whereas the diffusion cascades on twitter generally terminated within a day or two the secretary game and friend sense spread actively for several weeks while cascades on yahoo voice extended over several years given the heterogeneity in data collection timescales ranging from days to years and the nature of adoptions described above the distribution of diffusion structures across all seven cases is striking in its similarity fig shows the frequency of cascades accounted for by the most commonly occurring tree structures across the seven domains we study the vast majority of instances ranging from to across domains show no diffusion at all i e the tree consists only of the seed while the next most frequent outcome is in all cases a single additional adopter in fact the same seven simple tree structures account for upwards of of cascades in each domain figs and complement this result showing that the distributions of tree size and depth respectively are likewise extremely skewed in all domains less than of cascades consist of more than seven nodes and less than extend further than one degree from the seed node although the similarity across domains is striking our finding that most cascades are small and shallow is not on its own surprising a number of recent empirical studies of online diffusion adar and adamic leskovec et al bakshy et al sun et al bakshy et al have also observed that the size distribution of diffusion events is right skewed and heavy tailed which necessarily implies that most events are small indeed leskovec et al leskovec et al even identify many of the same motifs the usual intuition regarding heavy tailed distributions however is that large events although rare are sufficiently large to dominate certain key proper density adoptions 03 30 all else tree size ccdf adoptions 30 300 tree depth y kindness zync secretary game twitter news twitter videos friendsense y voice a b c fig the distribution of adoptions across cascade structures i e the fraction of adopters residing in each tree type table i summary diffusion statistics domain of one node cascades mean cascade size mean cascade depth of adoptions within one degree of a seed yahoo kindness yahoo zync the secretary game twitter news stories 98 twitter videos 96 friend sense yahoo voice ties of the corresponding system to illustrate it is likely that in the course of human history avian influenza has jumped from birds to humans hundreds or even thousands of times and that the vast majority of such events have led to only one or at most a few infections much as we see here nevertheless the vast majority of infections belong to a handful of very large cascades which are the epidemics of historical record arguably the majority of all humans who have ever been infected with avian influenza were infected during a single event namely the pandemic during which more than million individuals are thought to have been infected taubenberger and morens in our domain therefore even if it is the case that of cascades are small if it is also the case that the remaining are extremely large epidemic like trees the large cascades could still account for the bulk of all adoption activity what is surprising therefore is that we find no evidence of such epidemics in our data on the contrary fig shows that the seven tree structures from fig also adoption depth ccdf 01 y kindness zync secretary game twitter news twitter videos friendsense y voice fig the distribution of adoptions by depth which indicates that the vast majority of adoptions occur within generation of a seed percent of adoptions beyond one generation of a seed ccdf 01 20 40 80 fig for the approximately 000 videos and news stories posted on twitter independently by at least ten users the distribution of the proportion of adoptions occurring beyond one generation of a seed in particular less than of these products have the majority of their adoptions occurring beyond one generation of a seed account for around of all adoptions and thus only a minority of adoptions are found within large cascades fig in fact shows that in each of the seven domains less than of adoptions occur in cascades consisting of more than nodes and similarly fig shows that less than occur in trees that extend more than two generations from the seed finally fig demonstrates that very few adoptions across domains take place more than one degree from a seed node regardless of the size or depth of the tree in which they occur in other words in contrast with the intuition of viral spread leading to rare but large multi step epidemics we find that the vast majority of adoptions occur either without peer to peer influence or within one step of such an independent adopter large cascades that is are not only rare but are also insufficiently large to appreciably alter average cascade size which varies within a remarkably narrow range of across domains table although we have considered a diverse set of examples of online diffusion where in all cases the platform designers or message initiators in the case of twitter had the intention of promoting the diffusion of their products it is possible that we have simply not studied enough distinct instances to witness a case of truly viral spread in largely invariant with respect to adoption threshold suggesting that our conclusions are also robust with respect to particular definitions of adoption discussion our observation that multi step diffusion is not only rare in these online domains but that the vast majority of adoptions occurs within one degree of a seed node has several implications for theories of diffusion as they apply to online content and possibly for adoption processes more generally first although the motifs cataloged in figs and are quite likely consistent with those predicted by standard epidemiological models for sufficiently low rates of infectiousness we would argue that the academic literature on diffusion has paid little attention to this low infectivity parameter range focusing instead on the so called supercritical parameter regime which yields large epidemiclike events that propagate for many generations and infect large populations bass moore and newman watts kempe et al liben nowell and kleinberg as we have noted the implicit justification for this focus is that even if epidemic like events are rare they are occasionally so large that understanding and predicting them is of both theoretical and practical importance our results challenge this conventional wisdom we find not only that multi step diffusion occurs rarely on these platforms but that even when it does it accounts for only a small percentage of total adoptions to the extent that such theoretical models of adoption are intended to explain typical diffusion events we thus advocate more emphasis on subcritical processes a second related point is that even if one could explicitly encode low transmission rates into diffusion models we argue that doing so would ignore a key scientific goal namely identifying the underlying causes of these observed rates elaborating on this point consider a hypothetical model that incorporates just two effects the propensity for individuals to encounter products outside their immediate social network e g via mass media and the degree of similarity in product taste between social contacts the dynamics of such a model would therefore be governed by two countervailing forces on the one hand as individuals are exposed to more products through advertising and mass media adoptions due to social referrals and hence viral cascades become less prevalent on the other hand as the preferences of network neighbors become more similar social referrals exhibit more value increasing the frequency of viral adoptions although a formal analysis of a model of this type is left for future work and although there are no doubt many other factors that one might think to include in such a model our general claim is that by explicitly modeling the psychological antecedents of adoption even a relatively simple model could lend insight to our empirical observations beyond simply asserting that they correspond to low infectivity examples finally our observation that the vast majority of adoptions we study do not result from multi step diffusion naturally raises the question of what then does account for truly large adoption events that do occur such as online videos that generate many millions of views in a short period of time or products like facebook gmail or hotmail the sudden popularity of which is often attributed to viral diffusion over networks of individuals one possibility consistent with our results is that events such as these are not strictly viral at all in the sense implied by infectious disease models but rather obtain the bulk of their attention either from traditional advertising or from other coverage by the mass media media efforts that is might generate a large wave of adopters without requiring any viral peer to peer growth of the sort implied by epidemic models indeed it is precisely a version of this broadcast diffusion that we observe for the largest cascades in our data as shown in fig the four largest cascades all from twitter are relatively shallow spreading only a few steps from the fig structure of the four largest cascades observed all of which occurred on twitter colors indicate node depth with the large green points corresponding to seed nodes seed node thus not only are large cascades infrequent in these online domains but even when present they do not resemble the multi step epidemics that arise in prevalent theoretical models of diffusion watts kempe et al leskovec et al in which cascades become large by spreading for many generations referring again to the influenza epidemic although it infected more than million people individuals typically infected no more than a handful of others mills et al and hence its scale was determined almost entirely by the large number of generations over which it spread by contrast fig shows that the largest adoption cascades we find spread for at most a few generations before dying out in two of the top four cases in fact almost all adoptions are directly attributable to a single user with a very high follower count moreover the first and fourth largest cascades correspond to independent introductions of the same newly released music video by a popular artist justin bieber and thus benefitted from intense media attention marketing and advertising in short the largest adoption events that we observe occur through mechanisms qualitatively distinct from those in strictly peer to peer models of contagion in turn this observation may motivate a more general class of diffusion models that follow classical influence studies katz and lazarsfeld in explicitly differentiating media actors from ordinary individuals an alternative explanation for the occurrence of such popular products is that truly viral processes exhibit certain critical features that are unlikely to arise in the kind of social media domains that we consider here for example email viruses such as the melissa bug and code red smith spread over many generations to infect millions of computers on a global scale like biological epidemics email viruses are capable of propagating themselves automatically and without consent from the infected in all the domains we investigate however both the adoption and transmission of products are deliberate decisions suggesting that willful adoption and transmission may be moderating factors of epidemic spread yet another possible explanation is suggested by the example of respondent driven sampling rds salganik and heckathorn goel and salganik a type of snowball sampling in which survey respondents enlist the next wave of participants where recruitment chains routinely propagate for many generations and usually terminate only as a result of deliberate action by the survey organizers the success of this sampling methodology however depends critically on the provision of substantial financial incentives for recruiting malekinejad et al thus as with email viruses the feature that drives the viral nature of rds is unlikely to be present in typical online adoption processes which lack direct financial incentives these alternative mechanisms notwithstanding the possibility remains that viral spread among networks of individuals is occasionally responsible for large online adoption events with potentially substantial consequences and that we have simply not witnessed any such events in our data for example liben nowell and kleinberg study the propagation of an internet chain letter that does in fact appear to have spread virally for hundreds of generations over more than ten years while understanding the process underlying this seemingly rare phenomenon is certainly an important goal that such viral diffusion may occur in some instances should be weighed against the very large number and diversity of observations that fail to exhibit any of the requisite properties we conclude by noting that in addition to their scientific and theoretical implications our findings have practical relevance specifically although the idea that social media can spread in the manner of biological epidemics is provocative our findings indicate that strategies based on triggering social epidemics are likely unrealistic rather we believe that marketers and others interested in efficiently diffusing information should work to harness and enhance the potentially valuable gains from each seed where even incremental improvements in pass along rates can lead to substantial returns on investment watts and peretti by reducing sharing costs and including various calls to action in word of mouth campaigns reliable improvements in performance are likely possible aral and walker even if truly viral diffusion of social media remains improbable information aggregation we develop a dynamic model of opinion formation in social networks when the information required for learning a parameter may not be at the disposal of any single agent individuals engage in communication with their neighbors in order to learn from their experiences however instead of incorporating the views of their neighbors in a fully bayesian manner agents use a simple updating rule which linearly combines their personal experience and the views of their neighbors we show that as long as individuals take their personal signals into account in a bayesian way repeated interactions lead them to successfully aggregate information and learn the true parameter this result holds in spite of the apparent naïveté of agents updating rule the agents need for information from sources the existence of which they may not be aware of worst prior views and the assumption that no agent can tell whether her own views or those of her neighbors are more accurate elsevier inc all rights reserved introduction in everyday life people form opinions over various economic political and social issues such as how to educate their children or whether to invest in a certain asset which do not have an obvious solution these issues allow for a great variety of opinions because even if a satisfactory solution exists it is not easily recognizable in addition the relevant information for such problems is often not concentrated in any source or body of sufficient knowledge instead the data are dispersed throughout a vast network where each individual observes only a small fraction consisting of his her personal experience this motivates an individual to engage in communication with others in order to learn from other people experiences for example hagerstrand and rogers document such a phenomenon in the choice of new agricultural techniques by various farmers while kotler shows the importance of learning from others in the purchase of consumer products in many scenarios however the information available to an individual is not directly observable by others at most each individual only knows the opinions of few individuals such as colleagues family members and maybe a few news organizations will never know the opinions of everyone in the society and might not even know the full personal experience of anyone but herself this limited observability coupled with the complex interactions of opinions arising from dispersed we thank the editors vincent crawford and matthew jackson and two anonymous referees for very helpful remarks and suggestions we also thank ilan lobel and numerous seminar and conference participants for useful feedback and comments jadbabaie molavi and tahbaz salehi gratefully acknowledge financial support from the air force office of scientific research complex networks program and the office of naval research sandroni gratefully acknowledges the financial support of the national science foundation grants ses and ses corresponding author e mail address sandroni kellogg northwestern edu a sandroni see front matter elsevier inc all rights reserved http dx doi org 1016 j geb 06 001 a jadbabaie et al games and economic behavior information over the network makes it highly impractical for agents to incorporate other people views in a bayesian fashion the difficulties with bayesian updating are further intensified if agents do not have complete information about the structure of the social network or the probability distribution of signals observed by other individuals such incomplete information means that they would need to form and update opinions not only on the states of the world but also on the network topology as well as other individuals signal structures this significantly complicates the required calculations for bayesian updating of beliefs well beyond agents regular computational capabilities nevertheless the complications with bayesian learning persist even when individuals have complete information about the network structure as they still need to perform deductions about the information of every other individual in the network while only observing the evolution of opinions of their neighbors the necessary information and the computational burden of these calculations are simply prohibitive for adopting bayesian learning even in relatively simple networks in this paper we study the evolution of opinions in a society where agents instead of performing bayesian updates apply a simple learning rule to incorporate the views of individuals in their social clique we assume that at every time period each individual receives a private signal and observes the opinions i e the beliefs held by her neighbors at the previous period the individual updates her belief as a convex combination of i the bayesian posterior belief conditioned on her private signal and ii the opinions of her neighbors the weight an individual assigns to the opinion of a neighbor represents the influence or persuasion power of that neighbor on her at the end of the period agents report their opinions truthfully to their neighbors the influence that agents exert on one another can be large or small and may depend on each pair of agents moreover this persuasion power may be independent of the informativeness of their signals in particular more persuasive agents may not be better informed or hold more accurate views in such cases in initial periods agents views may move towards the views of the most persuasive agents and hence away from the data generating process we analyze the flow of opinions as new observations accumulate first we show that agents eventually make correct forecasts provided that the social network is strongly connected that is there exists either a direct or an indirect information path between any two agents hence the seemingly naïve updating rule will eventually transform the existing data into a near perfect guide for the future even though the truth is not recognizable agents do not know if their views are more or less accurate than the views of their neighbors and the most persuasive agents may have the least accurate views by the means of an example we show that the assumption of strong connectivity cannot be disposed of we further show that in strongly connected networks the non bayesian learning rule also enables agents to successfully aggregate dispersed information each agent eventually learns the truth even though no agent and her neighbors by themselves may have enough information to infer the underlying parameter eventually each agent learns as if she were completely informed of all observations of all agents and updated her beliefs according to bayes rule this aggregation of information is achieved while agents avoid the computational complexity involved in bayesian updating thus with a constant flow of new information a sufficient condition for social learning in strongly connected networks is that individuals simply take their personal signals into account in a bayesian manner if such a condition is satisfied then repeated interactions over the social network guarantee that the viewpoints of different individuals will eventually coincide leading to complete aggregation of information our results also highlight the role of social networks in information propagation and aggregation an agent can learn from individuals with whom she is not in direct contact and even from the ones of whose existence she is unaware in other words the indirect communication path in the social network guarantees that she will eventually incorporate the information initially revealed to agents in distant corners of the network into her beliefs thus agents can learn the true state of the world even if they all face an identification problem our basic learning results hold in a wide spectrum of networks and under conditions that are seemingly not conducive to learning for example assume that one agent receives uninformative signals and has strong persuasive powers over all agents including the only individual with informative signals but who may not know that her signals are more informative than the signals of others the agent with informative signals cannot directly influence the more persuasive agents and only has a small direct persuasive power over a few other agents we show that all agents views will eventually be as if they were based on informative signals despite the fact that most agents will never see these informative signals and will not know where they come from thus the paper also establishes that whenever agents take their own information into account in a bayesian way neither the fine details of the network structure beyond strong connectivity nor the prior beliefs can prevent them from learning as the effects of both are eventually washed away by the constant flow of new information gale and kariv illustrate the complications that can arise due to repeated bayesian deductions in a simple network also as demarzo et al point out in order to disentangle old information from new a bayesian agent needs to recall the information she received from her neighbors in the previous communication rounds and therefore w ith multiple communication rounds such calculations would become quite laborious even if the agent knew the entire social network an exception as shown recently by mossel and tamuz is the case in which agents signal structures their prior beliefs and the social network are common knowledge and all signals and priors are normally distributed it is important that not all agents face the same identification problem we formalize this statement in the following sections a jadbabaie et al games and economic behavior 225 the paper is organized as follows the next section discusses the related literature section contains our model our main results are presented in section in section we show that under suitable conditions bayesian and non bayesian learning procedures asymptotically coincide section concludes all proofs can be found in appendix a related literature there exists a large body of works on learning over social networks both boundedly and fully rational the bayesian social learning literature focuses on formulating the problem as a dynamic game with incomplete information and characterizing its equilibria however since characterizing the equilibria in complex networks is generally intractable the literature for the most part studies relatively simple and stylized environments more specifically rather than considering repeated interactions over the network it focuses on models where agents interact sequentially and communicate with their neighbors only once examples include banerjee bikhchandani et al smith and sørensen banerjee and fudenberg and more recently acemoglu et al an exception is rosenberg et al who study a model of dynamic games with purely informational externalities and provide conditions under which players eventually reach a consensus our work is also related to the social learning literature that focuses on non bayesian learning models such as ellison and fudenberg and bala and goyal in which agents use simple rule of thumb methods to update their beliefs in the same spirit are demarzo et al golub and jackson and acemoglu et al which are based on the opinion formation model of degroot in degroot style models each individual initially receives one signal about the state of the world and the focus is on conditions under which individuals in the connected components of the social network converge to similar opinions golub and jackson further show that if the size of the network grows unboundedly this asymptotic consensus opinion converges to the true state of the world provided that there are not overly influential agents in the society a feature that distinguishes our model from the works that are based on degroot model such as golub and jackson is the constant arrival of new information over time whereas in degroot model each agent has only a single observation the individuals in our model receive information in small bits over time this feature of our model can potentially lead to learning in finite networks a feature absent in other degroot style models where learning can only occur when the number of agents increases unboundedly thus as long as there is a constant flow of new information and agents take their personal signals into account in a bayesian manner the resulting learning process asymptotically coincides with bayesian learning despite the fact that agents use a degroot style update to incorporate the views of their neighbors the crucial difference in results between golub and jackson and acemoglu et al on the one hand and our model on the other is the role played by the social network in successful information aggregation these papers show that presence of influential individuals those who are connected to a large number of people or affect their opinions disproportionally may lead to disagreements or spread of misinformation in contrast in our environment strong connectivity is the only requirement on the network for successful learning and neither the network topology nor the influence level of different agents can prevent learning in fact social learning is achieved even if the most influential agents both in terms of their persuasion power and in terms of their location in the network are the ones with the least informative signals finally our work is also related to epstein et al who provide choice theoretic foundations for non bayesian opinion formation dynamics of a single agent however the focus of our analysis is on the process of information aggregation over a network comprising of many agents the model agents and observations let θ denote a finite set of possible states of the world we consider a set n n of agents interacting over a social network each agent i starts with a prior belief μi θ which is a probability distribution over the set θ more generally we denote the opinion of agent i at time period t by μi t θ conditional on the state of the world θ at each time period t an observation profile ωt t ωn t sn s is generated by the likelihood function θ we let ωi t si denote the signal privately observed by agent i at period t and si denote agent i signal space which we assume to be finite the privately observed signals are independent over time but might be correlated among agents at the same time period we assume that θ for all θ s θ and use i θ to denote the i th marginal of θ we further assume that every agent i knows the conditional likelihood function i θ known as her signal structure we do not require the observations to be informative about the state in fact each agent may face an identification problem in the sense that she might not be able to distinguish between two states we say two states are observationally equivalent from the point of view of an agent if the conditional distributions of her signals under the two states coincide more specifically the elements of the set θθ i θ θ i si θ i si θ for all si si are observationally equivalent to state θ from the point of view of agent i a jadbabaie et al games and economic behavior 225 finally for a fixed θ θ we define a probability triple ω f pθ where ω is the space containing sequences of realizations of the signals ωt s over time f is the σ field generated by the sequence of signal profiles and pθ is the probability measure induced over sample paths in ω in other words pθ t θ we use eθ to denote the expectation operator associated with measure pθ define fi t as the σ field generated by the past history of agent i observations up to time period t and let ft be the smallest σ field containing all fi t for i n social structure we assume that when updating their views about the underlying state of the world agents communicate their beliefs with individuals in their social clique an advantage of communicating beliefs over signals is that all agents share the same space of beliefs whereas their signal spaces may differ making it difficult for them to interpret the signals observed by others moreover in many scenarios private signals of an individual are in the form of personal experiences which may not be easily communicable to other agents we capture the social interaction structure between agents by a directed graph g v e where each vertex in v corresponds to an agent and an edge connecting vertex i to vertex j denoted by the ordered pair i j e captures the fact that agent j has access to the opinion held by agent i note that opinion of agent i might be accessible to agent j but not the other way around for each agent i define ni j v j i e called the set of neighbors of agent i the elements of this set are agents whose opinions are available to agent i at each time period we assume that individuals report their opinions truthfully to their neighbors a directed path in g v e from vertex i to vertex j is a sequence of vertices starting with i and ending with j such that each vertex is a neighbor of the next vertex in the sequence we say the social network is strongly connected if there exists a directed path from each vertex to any other vertex belief updates before the beginning of each period agents observe the opinions of their neighbors at the beginning of period t signal profile ωt t ωn t is realized and signal ωi t is privately observed by agent i following the realization of the private signals each agent computes her bayesian posterior belief conditional on the signal observed and then sets her final belief to be a linear combination of the bayesian posterior and the opinions of her neighbors observed right before the beginning of the period at the end of the period agents report their opinions to their neighbors more precisely if we denote the belief that agent i assigns to state θ θ at time period t by μi t θ then μi t aii bu μi t ωi t j ni ai jμj t where ai j captures the weight that agent i assigns to the opinion of agent j in her neighborhood bu μi t ωi t is the bayesian update of μi t when signal ωi t is observed and aii is the weight that the agent assigns to her bayesian posterior conditional on her private signal which we refer to as the measure of self reliance of agent i note that weights ai j must satisfy j ni i ai j in order for the period t beliefs to form a well defined probability distribution even though agents incorporate their private signals into their beliefs using bayes rule their belief updating is non bayesian rather than conditioning their beliefs on all the information available to them agents treat the beliefs generated through linear interactions with their neighbors as bayesian priors when incorporating their private signals the beliefs of agent i on θ at any time period induce forecasts about the future events we define the k step ahead forecasts of agent i at a given time period as the probability measure induced by her beliefs over the realizations of her private signals in the next k consecutive time periods more specifically we denote the period t belief of agent i that signals si si k si will be realized in time periods t through t k respectively by m k i t si si k thus the k step ahead forecasts of agent i at time t are given by m k i t si si k θ i si θ i si θ i si k θ dμi t θ and therefore the law of motion for the beliefs about the parameters can be written as μi t θ aiiμi t θ i ωi t θ mi t ωi t j ni ai jμj t θ one can generalize this belief update model and assume that agent i belief depends linearly on her own beliefs at the previous time period as well such an assumption is equivalent to adding a prior bias to the model as stated in epstein et al since this added generality does not change the results we assume that agents have no prior bias a jadbabaie et al games and economic behavior 225 for all θ θ the dynamics for belief updating in our model are local in the sense that each individual only uses the beliefs of her immediate neighbors to form her opinions ignores the structure of the network and does not make any inferences about the beliefs of other individuals the above dynamics for opinion formation compared to the bayesian case impose a significantly smaller computational burden on the individuals moreover individuals do not need to keep track of the identities of their neighbors and the exact information provided by them they only need to know the average belief held in their neighborhood given by the term j ni ai jμj t in the special case that the signals observed by an agent are uninformative or equivalently there are no signals after time t eq reduces to the belief update model of degroot used by golub and jackson when analyzing the asymptotic behavior of the beliefs sometimes it is more convenient to use matrix notation define a to be a real n n matrix which captures the social interaction of the agents as well as the weight that each agent assigns to her neighbors more specifically we let the i j element of the matrix a be ai j when agent j is a neighbor of agent i and zero otherwise thus eq can be rewritten as μt θ aμt θ diag t θ t t ann n ωn t θ mn t ωn t μt θ where μt t μn t and diag of a vector is a diagonal matrix which has the entries of the vector as its diagonal in the special case that a is the identity matrix our model reduces to the standard bayesian case in which the society consists of n bayesian agents who do not have access to the beliefs of other members of the society and only observe their own private signals social learning given the model described above we are interested in the evolution of opinions in the network and whether this evolution can lead to learning in the long run learning may either signify uncovering the true parameter or learning to forecast future outcomes these two notions of learning are distinct and may not occur simultaneously we start this section by specifying the exact meaning of both types of learning throughout we denote the true parameter by θ θ and the probability law generating signals ω by p t θ definition the k step ahead forecasts of agent i are eventually correct on a path ω if along that path m k i t si si si k i si θ i si θ i si k θ as t for all si si k ski moreover we say the beliefs of agent i weakly merge to the truth on some path if along that path her k step ahead forecasts are eventually correct for all natural numbers k this notion of learning captures the ability of agents to correctly forecast events in the near future it is well known that under suitable assumptions repeated applications of bayes rule lead to eventually correct forecasts with probability one the key condition is absolute continuity of the true measure with respect to initial beliefs in the presence of absolute continuity the mere repetition of bayes rule eventually transforms the historical record into a near perfect guide for the future however predicting events in near future accurately is not the same as learning the underlying state of the world in fact depending on the signal structure of each agent there might be an identification problem which can potentially prevent the agent from learning the true parameter θ we define an alternative notion of asymptotic learning according to which agents uncover the underlying parameter definition agent i n asymptotically learns the true parameter θ on a path ω if along that path μi t θ ast asymptotic learning occurs when the agent assigns probability one to the true parameter as mentioned earlier making correct forecasts about future events does not necessarily guarantee learning the true state in general the converse is not true either however it is straightforward to show that in our model asymptotically learning θ implies eventually correct forecasts to simplify notation and where no confusion arises we denote the one step ahead forecasts of agent i by mi t rather than m i t note that our notion of weak merging is weaker than what kalai and lehrer refer to as weak merging of opinions whereas definition only requires that k step ahead forecasts of agent i are eventually correct for all k kalai and lehrer notion of weak merging of opinions requires that she assign asymptotically correct probabilities to all events that are measurable with respect to fi t for all t for an example see lehrer and smorodinsky a jadbabaie et al games and economic behavior 225 correct one step ahead forecasts we now turn to the main question of this paper under what circumstances does learning occur over the social network our first result shows that under mild assumptions in spite of local interactions limited observability and the non bayesian belief update agents will eventually have correct one step ahead forecasts the proof is provided in appendix a proposition suppose that the social network is strongly connected all agents have strictly positive self reliances and there exists an agent with positive prior belief on the true parameter θ then the one step ahead forecasts of all agents are eventually correct with p probability one this proposition states that when agents use non bayesian updating rule to form and update their opinions they will eventually make accurate predictions about the realization of their private signals in the next period note that as long as the social network remains strongly connected neither the topology of the network nor the influence levels of different individuals prevent agents from making correct predictions one of the features of proposition is the absence of absolute continuity of the true measure with respect to the prior beliefs of all agents in the society as long as some agent assigns a positive prior belief to the true parameter θ all agents will eventually be able to correctly predict the next period realizations of their private signals in the sense of definition in fact eventually correct one step ahead forecasts arise even if the only agent for whom absolute continuity holds is located at the fringe of the society has very small persuasive power over her neighbors and almost everyone in the network is unaware of her existence the main reason for this phenomenon is that due to the naïve part of the updating there is a contagion of beliefs to all agents which eventually leads to absolute continuity of their beliefs with respect to the truth besides the existence of an agent with a positive prior belief on the true state the above proposition requires the existence of positive self reliances to guarantee correct forecasts this requirement is intuitive it prohibits agents from completely discarding information provided to them through their observations clearly if all agents discard their private signals no new information is incorporated into their opinions and simply turns into a diffusion of prior beliefs the final requirement for accurate predictions is strong connectivity of the social network the following example illustrates that this assumption cannot be disposed of example consider a society consisting of two agents n and assume that θ with the true state being θ both agents have non degenerate prior beliefs over θ assume that signals observed by the agents are conditionally independent and belong to the set h t we further assume that agent signals are non informative while agent observations are perfectly informative about the state that is h t and for h t as for the social structure we assume that agent has access to the opinion of agent while agent cannot observe the opinion of agent clearly the social network is not strongly connected we let the social interaction matrix be a α α where α is the weight that agent assigns to the opinion of agent when updating her beliefs using eq since the private signals observed by the latter are non informative her beliefs at all times remain equal to her prior clearly she makes correct forecasts at all times agent forecasts on the other hand will always remain incorrect notice that since her signals are perfectly informative agent one step ahead predictions are eventually correct if and only if she eventually assigns probability to the true state however the belief she assigns to follows the law of motion t α t t t t t which cannot converge to zero as t is strictly positive the intuition for failure of learning in this example is simple given the same observations the two agents make different interpretations about the state even if they have equal prior beliefs moreover agent follows the beliefs of the less informed agent but is unable to influence her back this one way persuasion and non identical interpretations of signals due to non identical signal structures result in incorrect one step ahead forecasts on the part of agent finally note that had agent discarded the opinions of agent and updated her beliefs according to bayes rule she would have learned the truth after a single observation weak merging to the truth the key implication of proposition is that as long as the social network is strongly connected the one step ahead forecasts of all agents will eventually be correct our next result establishes that not only agents make accurate predictions about their private observations in the next period but also under the same set of assumptions make correct predictions about any finite time horizon in the future a jadbabaie et al games and economic behavior 225 proposition suppose that the social network is strongly connected all agents have strictly positive self reliances and there exists an agent with positive prior belief on the true parameter θ then the beliefs of all agents weakly merge to the truth with p probability one the above proposition states that in strongly connected societies having eventually correct one step ahead forecasts is equivalent to weak merging of agents opinions to the truth this equivalence has already been established by kalai and lehrer for bayesian agents note that in the purely bayesian case an agent k step ahead forecasts are simply products of her one step ahead forecasts making the equivalence between one step ahead correct forecasts and weak merging of opinions immediate however due to the non bayesian updating of the beliefs the k step ahead forecasts of agents in our model do not have such a multiplicative decomposition making this implication significantly less straightforward social learning proposition shows that in strongly connected social networks the predictions of all individuals about the realizations of their signals in any finite time horizon will eventually be correct implying that their asymptotic opinions cannot be arbitrary the following proposition which is our main result establishes that strong connectivity of the social network not only leads to correct forecasts but also guarantees successful aggregation of information all individuals eventually learn the true state proposition suppose that a the social network is strongly connected b all agents have strictly positive self reliances c there exists an agent with positive prior belief on the true parameter θ d there is no state θ θ that is observationally equivalent to θ from the point of view of all agents in the network then all agents in the social network learn the true state of the world p almost surely that is μi t θ with p probability one for all i n as t proposition states that under regularity assumptions on the social network topology and the individuals signal structures all agents will eventually learn the true underlying state of the world notice that agents only interact with their neighbors and perform no deductions about the world beyond their immediate neighbors nonetheless the non bayesian updating rule eventually enables them to obtain relevant information from others without exactly knowing where it comes from in fact they can be completely oblivious to important features of the social network such as the number of individuals in the society the topology of the network other people signal structures the existence of some agent who considers the truth plausible or the influence level of any agent in the network and still learn the parameter moreover all these results are achieved with a significantly smaller computational burden than what is required for bayesian updating the other significant feature of proposition is that neither the topology of the network the signal structures nor the influence levels of different agents prevent learning for instance even if the agents with the least informative signals are the most persuasive ones and are located at the bottlenecks of the network everyone will eventually learn the true state social learning is achieved despite the fact that the truth is not recognizable to any individual and she would not have learned it by herself in isolation the intuition behind proposition is simple recall that proposition implies that the vector of beliefs of individual i i e μi t cannot vary arbitrarily forever and instead will eventually be restricted to the subspace that guarantees correct forecasts asymptotic convergence to such a subspace requires that she assigns an asymptotic belief of zero to any state θ which is not observationally equivalent to the truth from her point of view otherwise she would not be able to form correct forecasts about future realizations of her signals however due to the non bayesian part of the update corresponding to the social interactions of agent i with her neighbors an asymptotic belief of zero is possible only if all her neighbors also consider θ asymptotically unlikely this means that the information available to agent i must be eventually incorporated into every other individuals beliefs the role of the assumptions of proposition can be summarized as follows the strong connectivity assumption creates the possibility of information flow between any pair of agents in the social network the assumption on positive selfreliances guarantees that agents do not discard the information provided to them through their private observations the third assumption states that some agent assigns a positive prior belief to the truth this agent may be at the fringe of the society may have a very small influence on her neighbors and almost no one may be aware of her existence hence the ultimate source of learning may remain unknown to almost everyone clearly if the prior beliefs of all agents assigned to the truth is equal to zero then they will never learn as we show in the next section the asymptotic beliefs of all individuals do indeed coincide with those of a set of agents who make full bayesian deductions a jadbabaie et al games and economic behavior 225 fig a strongly connected social network of agents the last assumption indicates that the collection of observations of all agents is informative enough about the true state that is θ θ n θ where θ i is the set of states that are observationally equivalent to θ from the point of view of agent i this assumption guarantees that it is possible to learn the truth if one has access to the observations of all agents in the absence of this assumption even highly sophisticated bayesian agents with access to all relevant information such as the topology of the network and the signal structures would not be able to completely learn the state due to an identification problem finally note that when agents have identical signal structures and therefore θ i θ j for all i and j they do not benefit from the information provided by their neighbors as they would be able to asymptotically learn just as much through their private observations the next examples show the power and limitations of proposition example consider the collection of agents n who are located in a social network as depicted in fig at every time period agent i can observe the opinion of agent i and agent has access to the opinion held by agent clearly this is a strongly connected social network assume that the set of possible states of the world is given by θ θ where θ is the true underlying state of the world we also assume that the signals observed by the agents belong to the set si h t for all i are conditionally independent and have conditional distributions given by i h θ i i if θ θi i otherwise for all i n the signal structures are such that each agent suffers from some identification problem i e the information in the observations of any agent is not sufficient for learning the true state of the world in isolation more precisely θ i θ θi for all i which means that from the point of view of agent i all states except for θi are observationally equivalent to the true state θ nevertheless for any given state θ θ there exists an agent whose signals are informative enough to distinguish the two that is i θ therefore proposition implies that as long as one agent assigns a positive prior belief on the true state θ and all agents have strictly positive self reliances then μi t θ as t for all agents i with p probability one in other words all agents will asymptotically learn the true underlying state of the world clearly if agents discard the information provided to them by their neighbors they have no means of learning the true state example consider a strongly connected social network consisting of two individuals n assume that θ and h t also assume that the distribution function describing the random private observations of the agents conditional on the underlying state of the world is given by the following tables h t h t h t h t in other words under state the private observations of the two agents are perfectly correlated while when the underlying state of the world is their observations are perfectly negatively correlated even though the joint distributions of the signals generated by and are different we have i h i h for i i e the local signal structure of each agent is the same under either state as a result despite the fact that agents will eventually agree on their opinions and make correct forecasts they do not uncover the underlying parameter as and θ2 are observationally equivalent from the point of view of both agents that is in this example assumption d of proposition does not hold this is a stronger restriction than requiring θ θ for all θ θ see also example a jadbabaie et al games and economic behavior 225 finally we remark that proposition is silent on the rate at which information is aggregated in particular the rate of learning may depend on the fine details of the structure of the social network as well as agents signal structures in addition while non bayesian agents learn the truth as if they have access to all relevant signals and update their beliefs according to bayes rule this aggregation of information may not happen at the same rate that bayesian agents learn bayesian learning in the introduction we claimed that with a constant flow of new information bayesian and non bayesian learning rules asymptotically coincide in this section we make this claim precise in particular we consider the problem of aggregation of information on a strongly connected social network when individuals incorporate the views of their neighbors in a fully bayesian manner and show that under assumptions similar to those of proposition bayesian agents learn the true parameter in the remainder of the paper we assume that individuals have full information about the topology of the social network and the signal structures of all other agents our results remain valid even if agents have prior beliefs about the network structure and the distributions from which others signals are drawn suppose that agents have a common prior belief over the finite set of states of the world θ let x b q be a probability space where x θ ω is the space containing the realizations of the underlying state of the world and the sequence of signal profiles over time b f is the product σ algebra and q is the probability measure defined as q θ ω θ pθ ω we use e to denote the expectation operator associated with measure q proposition suppose that all agents incorporate their private signals and views of their neighbors in a fully bayesian manner also suppose that a the social network is strongly connected b the agents common prior has full support over θ c there are no two states that are observationally equivalent from the point of view of all agents then all agents in the social network learn the realized state of the world with q probability one thus bayesian agents will eventually learn the true parameter as long as the network is strongly connected and no two states are observationally equivalent from the point of view of all agents however in contrast to proposition this result requires all agents to assign a non zero prior belief on all states clearly if a bayesian agent considers a state θ to be impossible then no new information would convince her that θ is the underlying state of the world so apart from the case where agents assign zero prior probability to the realized state of the world our non bayesian learning procedure asymptotically coincides with bayesian learning thus even if agents incorporate the views of their neighbors using a degroot style update their beliefs are asymptotically as if they performed full bayesian deductions this result holds provided that agents take their private signals into account in a bayesian manner and that new information continuously arrives over time conclusions in this paper we develop a model of dynamic opinion formation in social networks bridging the gap between bayesian and degroot style non bayesian models of social learning agents fail to incorporate the views of their neighbors in a fully bayesian manner and instead use a local updating rule more specifically at every time period the belief of each individual is a convex combination of her bayesian posterior belief and her neighbors expressed beliefs our results show that agents eventually make correct forecasts as long as the social network is strongly connected in addition agents successfully aggregate all information over the entire social network they eventually learn the true underlying state of the world as if they were completely informed of all signals and updated their beliefs according to bayes rule the main insight suggested by our results is that with a constant flow of new information the key condition for social learning is that individuals take their personal signals into account in a bayesian way repeated interactions over the social network guarantee that the differences of opinions eventually vanish and learning is obtained the aggregation of information is achieved even if individuals are unaware of important features of the environment in particular agents do not need to have any information or form beliefs about the structure of the social network nor the views or characteristics of most agents as they only update their opinions locally and do not make any deductions about the world beyond their immediate neighbors moreover the individuals do not need to know the signal structure of any other agent in the network besides their own thus individuals eventually achieve full learning through a simple local updating rule and avoid the highly complex computations that are essential for full bayesian updating over the network similar results are shown by rosenberg et al and mueller frank forthcoming albeit under different sets of assumptions a jadbabaie et al games and economic behavior 225 even though our results establish that asymptotic learning is achieved in all strongly connected social networks the rate at which information is aggregated depends on the topology of the network as well as agents signal structures relatedly the fine details of the social network structure would also affect asymptotic learning if agents influences vary over time in particular if the influences of individuals on their neighbors vanish over time disagreements may persist even if the social network is strongly connected another feature of the model studied in this paper is the assumption that agents can communicate their beliefs with their neighbors a potentially unrealistic assumption when the size of the state space is large this leads to the open questions of whether there are more efficient modes of communication and whether asymptotic social learning can be sustained when agents communicate some sufficient statistics of their beliefs with one another we intend to investigate these questions in future work appendix a proofs a two auxiliary lemmas before presenting the proofs of the results in the paper we state and prove two lemmas both of which are consequences of the martingale convergence theorem lemma let a denote the matrix of social interactions the sequence ni viμi t θ converges p almost surely as t where v is any non negative left eigenvector of a corresponding to its unit eigenvalue proof first note that since a is stochastic it always has at least one eigenvalue equal to moreover there exists a non negative left eigenvector corresponding to this eigenvalue we denote such a vector by v evaluate eq at the true parameter θ and multiply both sides by v from the left v μt θ v aμt θ n i viμi t θ aii i ωi t θ mi t ωi t thus e n i viμi t θ ft n i viμi t θ n i viaiiμi t θ e i ωi t θ mi t ωi t ft where e denotes the expectation operator associated with measure p since f x x is a convex function jensen inequality implies that e i ωi t θ mi t ωi t ft e mi t ωi t i ωi t θ ft and therefore e n i viμi t θ ft n i viμi t θ the last inequality is due to the fact that v is element wise non negative as a result ni viμi t θ is a submartingale with respect to the filtration ft which is also bounded above by v hence it converges p almost surely lemma suppose that there exists an agent i such that μi θ also suppose that the social network is strongly connected then the sequence ni vi logμi t θ converges p almost surely as t where v is any non negative left eigenvector of a corresponding to its unit eigenvalue proof similar to the proof of the previous lemma we show that ni vi logμi t θ is a bounded submartingale and invoke the martingale convergence theorem to obtain almost sure convergence by evaluating the law of motion at θ taking log from both sides and using the fact that the row sums of a are equal to one we obtain logμi t θ aii logμi t θ aii log i ωi t θ mi t ωi t j ni ai j logμj t θ a matrix is said to be stochastic if it is entry wise non negative and all its row sums are equal to one this is a consequence of the perron frobenius theorem for more on the properties of non negative and stochastic matrices see berman and plemmons a jadbabaie et al games and economic behavior 225 where we have used the concavity of the logarithm function note that since the social network is strongly connected the existence of one agent with a positive prior on θ guarantees that after at most n periods all agents assign a strictly positive probability to the true parameter which means that logμi t θ is well defined for large enough t and all i our next step is to show that e log i ωi t θ mi t ωi t ft to obtain this note that e log i ωi t θ mi t ωi t ft e log mi t ωi t i ωi t θ ft log e mi t ωi t i ωi t θ ft thus e logμi t θ ft aii logμi t θ j ni ai j logμj t θ which can be rewritten in matrix form as e logμt θ ft a logμt θ where by the logarithm of a vector we mean its entry wise logarithm multiplying both sides by a non negative left eigenvector v leads to e n i vi logμi t θ ft n i vi logμi t θ thus the non positive sequence ni vi logμi t θ is a submartingale with respect to filtration ft and therefore converges with p probability one with these lemmas in hand we can prove proposition a proof of proposition first note that since the social network is strongly connected the social interaction matrix a is an irreducible stochastic matrix and therefore its left eigenvector corresponding to the unit eigenvalue is strictly positive according to lemma ni viμi t θ converges with p probability one where v is the positive left eigenvector of a corresponding to its unit eigenvalue thus evaluating eq at θ and multiplying both sides by v from the left imply n i viaiiμi t θ i ωi t θ mi t ωi t p a and therefore by the dominated convergence theorem for conditional expectations n i viaiiμi t θ e i ωi t θ mi t ωi t ft p a since the term viaiiμi t θ e i ωi t θ mi t ωi t ft is non negative for all i each such term converges to zero with p probability one moreover the assumptions that all diagonal entries of a are strictly positive and that of its irreducibility which means that v is entry wise positive lead to μi t θ e i ωi t θ mi t ωi t ft for all i p a furthermore lemma guarantees that ni vi logμi t θ converges almost surely implying that μi t θ converges to a strictly positive number with probability one for all i note that once again we are using the fact that v is a strictly positive vector hence e i ωi t θ mi t ωi t ft almost surely thus an n n matrix a is said to be reducible if for some permutation matrix p the matrix p ap is block upper triangular if a square matrix is not reducible it is said to be irreducible for more on this see e g berman and plemmons for the statement and a proof of the theorem see page of durrett a jadbabaie et al games and economic behavior 225 e i ωi t θ mi t ωi t ft si si i si θ i si θ mi t si si si i si θ i si θ mi t si mi t si mi t si i si θ si si i si θ mi t si mi t si p a where the second equality is due to the fact that both i θ and mi t are probability measures on si and therefore si si i si θ si si mi t si in the last expression the term in the brackets and the denominator are always non negative and therefore mi t si i si θ p a for all si si and all i n a proof of proposition we first present and prove a simple lemma which is later used in the proof of the proposition lemma suppose that the social network is strongly connected all agents have strictly positive self reliances and there exists an agent i such that μi t θ then for all θ θ e μt θ ft aμt θ with p probability one as t proof taking conditional expectations from both sides of eq implies e μt θ ft aμt θ diag t θ t t ft anne n ωn t θ mn t ωn t ft μt θ on the other hand we have e i ωi t θ mi t ωi t ft si si i si θ i si θ mi t si si si i si θ p a w here the convergence is a consequence of proposition the fact that i θ is a probability measure on si implies si si i si θ completing the proof we now present the proof of proposition proof of proposition 15 we prove this proposition by induction note that by definition agent i beliefs weakly merge to the truth if her k step ahead forecasts are eventually correct for all natural numbers k in proposition we established that the claim is true for k for the rest of the proof we assume that the claim is true for k and show that m k i t si si k converges to kr i si r θ for any arbitrary sequence of signals si si k ski first note that lemma and eq imply that for all θ θ e μi t θ ft μi t θ aii i ωi t θ mi t ωi t μi t θ p a multiplying both sides by kr i si r θ for an arbitrary sequence of signals si si k sk i and summing up over all θ θ lead to θ θ k r i si r θ e μi t θ ft μi t θ aii θ θ k r i si r θ i ωi t θ mi t ωi t μi t θ 15 we would like to thank a referee for bringing a technical mistake in an earlier draft of the paper to our attention recall that we use si si to denote a generic element of the signal space of agent i whereas ωi t denotes the random variable corresponding to i observation at period t a jadbabaie et al games and economic behavior 225 with p probability one on the other hand θ θ k r i si r θ e μi t θ ft μi t θ e m k i t si si k ft m k i t si si k where we have used the definition of k step ahead forecasts of agent i the induction hypothesis and the dominated convergence theorem for conditional expectations imply that the right hand side of the above equation converges to zero with p probability one therefore θ θ k r i si r θ i ωi t θ mi t ωi t μi t θ p a for any arbitrary sequence of signals si si k sk i which is equivalent to mi t ωi t m k i t ωi t si si k m k i t si si k p a thus once again by the induction hypothesis m k i t ωi t si si k mi t ωi t k r i si r θ with p probability one the dominated convergence theorem for conditional expectations implies e m k i t ωi t si si k mi t ωi t k r i si r θ ft p a rewriting the conditional expectation operator as a sum over all possible realizations of ωi t leads to i si i i θ m k i t i si si k mi t si k r i si r θ p almost surely and therefore guaranteeing m k i t si si si k mi t si k r i si r θ p a for all si si finally the fact that mi t si i si θ with p probability one proposition completes the proof a proof of proposition we first show that for any agent i there exists a finite sequence of private signals that is more likely to realize under the true state θ than any other state θ unless θ is observationally equivalent to θ from the point of view of agent i lemma for any agent i there exists a positive integer ˆki a sequence of signals ˆsi ˆs i ˆk i si ˆk i and constant δi such that ˆk i r i ˆs i r θ i ˆsi r θ δi θ θ i where θ i θ θ i si θ i si θ for all si si proof by definition for any θ θ i the probability measures i θ and i θ are distinct therefore by the kullback leibler inequality there exists some constant i such that si si i si θ log i si θ i si θ i recall that by assumption i si θ for all si θ si θ and all i a jadbabaie et al games and economic behavior 225 for all θ θ i which then implies si si i si θ i si θ i si θ δ i for δ i exp i on the other hand given the fact that rational numbers are dense on the real line there exist strictly positive rational numbers q si si si with q si chosen arbitrarily close to i si θ satisfying si si q si such si si i si θ i si θ q si δ i θ θ i therefore the above inequality can be rewritten as si si i si θ i si θ k si δ i ˆki θ θ i for some positive integers k si and ˆki satisfying ˆki si si k si picking the sequence of signals of length ˆki ˆsi ˆs i ˆk i such that si appears k si many times in the sequence and setting δi δ i ˆk i proves the lemma the above lemma shows that the sequence of private signals in which any signal si si appears with a frequency close enough to i si θ is more likely under the truth θ than any other state θ which is distinguishable from θ we now proceed to the proof of proposition proof of proposition first we prove that agent i assigns an asymptotic belief of zero to states that are not observationally equivalent to θ from her point of view recall that according to proposition the k step ahead forecasts of agent i are eventually correct for all positive integers k guaranteeing that m k i t si si k kr i si k θ with p probability one for any sequence of signals si si k in particular the claim is true for the integer ˆki and the sequence of signals ˆsi ˆs i ˆk i satisfying in lemma θ θ μi t θ ˆk i r i ˆsi r θ i ˆsi r θ p a therefore θ θ i μi t θ ˆk i r i ˆs i r θ i ˆsi r θ θ θ i μi t θ p a leading to θ θ i μi t θ ˆk i r i ˆs i r θ i ˆsi r θ with p probability one the fact that ˆki and ˆsi ˆs i ˆk i were chosen to satisfy implies that ˆk i r i ˆsi r θ i ˆsi r θ δi θ θ i and as a consequence it must be the case that μi t θ as t for any θ θ i therefore with p probability one agent i assigns an asymptotic belief of zero to any state θ that is not observationally equivalent to θ from her point of view the fact that the rationals form a dense subset of the reals means that there are rational numbers q si si si arbitrarily close to i si θ si si setting q si q si i si q si guarantees that one can always find strictly positive rational numbers q si si si adding up to one while at the same time is satisfied a jadbabaie et al games and economic behavior 225 now consider the belief update rule for agent i given by eq evaluated at some state θ θ i μi t θ aiiμi t θ i ωi t θ mi t ωi t j ni ai jμj t θ we have already shown that μi t θ p almost surely however this is not possible unless j ni ai jμj t θ converges to zero as well which implies that μj t θ with p probability one for all j ni note that this happens even if θ is observationally equivalent to θ from the point of view of agent j that is even if θ θ j as a result all neighbors of agent i will assign an asymptotic belief of zero to parameter θ regardless of their signal structure we can extend the same argument to the neighbors of neighbors of agent i and by induction since the social network is strongly connected to all agents in the network thus with p probability one μi t θ i n θ θ θ n implying that all agents assign an asymptotic belief of zero to states that are not observationally equivalent to θ from the point of view of all individuals in the society therefore statement d in the assumptions of proposition implies that μi t θ for all θ θ with p probability one guaranteeing complete learning by all agents a proof of proposition we define pi t as the σ field generated by the private signals observed by agent i up to period t pi t σ ωi ωi t and pi as the smallest σ field containing pi t for all t we also define the posterior belief of agent i on θ and the σ field generated by all her observations up to a given time period which consists of her private signals and the sequence of beliefs of her neighbors recursively as follows ii t σ ωi ωi t νj νj t j ni νi t θ q θ ii t with νi we denote the smallest σ field that contains ii t for all t by ii finally we define hi t as the σ field generated by the sequence of beliefs of agent i up to period t hi t σ νi νi t and hi as the smallest σ field containing hi t for all t by observing the sequence of her private signals agent i can asymptotically distinguish between the states that are not observationally equivalent from her point of that is for any θ θ e i pi t i q a where i is the indicator function of the event that the realized state is observationally equivalent to θ from the point of view of agent i on the other hand by the dominated convergence theorem for conditional expectations e i pi t converges to e i pi with q probability one consequently e i pi i q a which means that i is measurable with respect to pi the fact that pi ii guarantees that it is also measurable with respect to ii thus for any θ θ νi t θθ i e i ii t i q a in other words agent i asymptotically rules out states that are not observationally equivalent to the underlying state of the world from her point of view next note that for any θ θ with q probability one therefore not only agent j would be able to eventually distinguish between any two states that are not observationally equivalent from her own point of view but also between those that are not observationally equivalent from the points of view of any of her neighbors finally the fact that the social network is strongly connected guarantees that every individual would be able to eventually distinguish between any two states that are observationally equivalent from the point of view of some other agent this observation coupled with the assumption that no two states are observationally equivalent from the point of view of all agents implies νi t θ θ q a completing the proof log linear learning is a learning algorithm that provides guarantees on the percentage of time that the action profile will be at a potential maximizer in potential games the traditional analysis of log linear learning focuses on explicitly computing the stationary distribution and hence requires a highly structured environment since the appeal of loglinear learning is not solely the explicit form of the stationary distribution we seek to address to what degree one can relax the structural assumptions while maintaining that only potential function maximizers are stochastically stable in this paper we introduce slight variants of log linear learning that provide the desired asymptotic guarantees while relaxing the structural assumptions to include synchronous updates time varying action sets and limitations in information available to the players the motivation for these relaxations stems from the applicability of log linear learning to the control of multiagent systems where these structural assumptions are unrealistic from an implementation perspective elsevier inc all rights reserved introduction the theory of learning in games has sought to understand how and why equilibria emerge in non cooperative games traditionally social science literature develops descriptive game theoretic models for players analyzes the limiting behavior and generalizes the results for larger classes of games recently there has been a significant amount of research seeking to understand these behavioral models not from a descriptive point of view but rather from a prescriptive point of view arslan et al mannor and shamma marden et al shoham et al the goal is to use these behavioral models as a prescriptive control approach in distributed multi agent systems where the guaranteed limiting behavior would represent a desirable operating condition a game theoretic approach to distributed control of multi agent systems involves designing the interactions of the agents within a non cooperative game framework it turns out that a design of such distributed systems is strongly related to a class of non cooperative games known as potential games or more generally weakly acyclic games marden et al the reason for this stems from the fact that in distributed engineering systems each agent utility function should be appropriately aligned to the global objective and this class of games captures this notion of alignment this connection is important for two main reasons first potential games provide a paradigm for designing analyzing and controlling multi agent systems in fact there are existing methodologies for designing local agent utility research supported by the social and information sciences laboratory at the california institute of technology afosr grants 0538 0375 and 05 0321 and nsf grant ecs corresponding author e mail addresses jason marden colorado edu j r marden shamma gatech edu j s shamma see front matter elsevier inc all rights reserved http dx doi org 1016 j geb 03 j r marden j s shamma games and economic behavior functions that guarantee that the resulting game is a potential game marden and wierman submitted for publication wolpert and tumor furthermore these methodologies also guarantee that the action profiles that maximize the global objective of the multi agent system coincide with the potential function maximizers in the resulting potential game see section second potential games have been studied extensively in the game theory literature and several established learning algorithms with guaranteed asymptotic results could be implemented to control these multi agent systems most of the learning algorithms for potential games guarantee convergence to a nash equilibrium a representative sampling of these algorithms includes fictitious play monderer and shapley joint strategy fictitious play marden et al adaptive play young and many others young marden et al however in potential games a pure nash equilibrium may be inefficient with regards to the global objective in certain settings such as a congestion game with linear cost functions it may be possible to bound the efficiency but in general such a bound need not be possible roughgarden vetta therefore from a design perspective having a learning algorithm that guarantees convergence to the most efficient nash equilibrium is desirable this is especially true when utility functions have been designed to ensure that the action profiles that maximize the global objective of the multi agent system coincide with the potential function maximizers log linear learning originally introduced in blume is one of the few learning dynamics that embodies this notion of equilibrium selection in potential games log linear learning guarantees that only the joint action profiles that maximize the potential function are stochastically stable the enabler for these results is the introduction of noise into the decision making process young blume in log linear learning this noise permits players to occasionally make mistakes where mistakes represent the selection of suboptimal actions the structure of the noise in log linear learning is such that the probability of selecting a suboptimal action is associated with the magnitude of the payoff difference associated with a best response and the suboptimal action as the noise vanishes the probability that a player selects a suboptimal action goes to zero the traditional analysis of log linear learning has centered around explicitly computing the stationary distribution this analysis relies on a highly structured setting i players utility functions constitute a potential game ii players update their strategies one at a time which we refer to as asynchrony iii at any stage a player can select any action in the action set which we refer to as completeness iv each player is endowed with the ability to assess the utility he would have received for any alternative action provided that the actions of all other players remain fixed nonetheless log linear learning has received significant research attention young blume marden et al arslan et al benaim and sandholm alos ferrer and netzer these results range from analyzing convergence rates benaim and sandholm shah and shin to the necessity of the structural requirements alos ferrer and netzer in particular alos ferrer and netzer demonstrate that if the structural requirements of i and ii are relaxed arbitrarily then the equilibrium selection properties of log linear learning are no longer guaranteed since the appeal of log linear learning is not solely the explicit form of the stationary distribution we seek to address to what degree one can relax the structural assumptions while maintaining that only potential function maximizers are stochastically stable one motivation for this relaxation is that the structured setting of log linear learning may inhibit its applicability as a distributed control mechanism in many distributed systems however these results are of broader interest in the setting of game theoretic learning our main contribution in this paper is demonstrating that the structural assumption of log linear learning can be relaxed while maintaining that only potential function maximizers are stochastically stable we introduce slight variants of log linear learning to include both synchronous updates and incomplete action sets in both settings we prove that only potential function maximizers are stochastically stable furthermore we introduce a payoff based version of log linear learning in which players are only aware of the utility they received and the action that they played note that log linear learning in its original form is not a payoff based learning algorithm in the payoff based log linear learning we also prove that only potential maximizers are stochastically stable this result follows a string of research analyzing payoff based dynamics also referred to as completely uncoupled dynamics in games babichenko young marden et al foster and young germano and lugosi hart and mas colell blum et al in general the existing literature focuses on convergence to nash equilibrium or nash equilibrium in different classes of games using only payoff based information in contrast to these results our proposed algorithm provides convergence to the best nash equilibrium i e the potential function maximizer using only payoff based information the key enabler for these results is to change the focus of the analysis away from deriving the explicit form of the stationary distribution of the learning process towards characterizing the stochastically stable states the resulting analysis uses the theory of resistance trees for regular perturbed markov decision processes young one important issue that is not addressed in this paper is convergence rates for the proposed algorithms recent results have proposed slight it is worth noting that the usage of asynchrony in this paper is inconsistent with the standard usage of the term in computer science where the term references each agent common knowledge of the clock j r marden j s shamma games and economic behavior variations of log linear learning that result in desirable convergence rates for a class of congestion games shah and shin asadpour and saberi and social networks montanari and saberi it remains an open and important research question to characterize convergence rates for variations of log linear learning that are more suitable for distributed control the outline of the paper is as follows in section we review the game theoretic concepts that we will use in this paper in section we prove that log linear learning in its original form is a regular perturbed markov process utilizing this connection we reprove the equilibrium selection properties of log linear learning using the theory of resistance trees for regular perturbed markov processes furthermore we analyze the convergence properties of log linear learning outside the realm of potential games in section we analyze a variant of log linear learning that includes synchronous updates in section we show that if action sets are not complete then potential function maximizers may not be stochastically stable in potential games furthermore we derive a variant of the traditional log linear learning that rectifies this problem in section we introduce a payoff based version of log linear learning that maintains the equilibrium selection properties of log linear learning in section we illustrate the applicability of the payoff based version of log linear learning on a coordination game lastly in section we provide some concluding remarks setup we consider a finite strategic form game with n player set i n where each player i has a finite action set ai and a utility function ui a r where a an for an action profile a an a let a i denote the profile of player actions other than player i i e a i ai ai an with this notation we will sometimes write a profile a of actions as ai a i similarly we may write ui a as ui ai a i let a i j i aj denote the set of possible collective actions of all players other than player i we define player i best response set for an action profile a i a i as bi a i a i ai ui a i a i max ai ai ui ai a i classes of games in this paper we focus on three classes of games identical interest potential and weakly acyclic in each of these classes of games a pure nash equilibrium is guaranteed to exist each class of games imposes a restriction on the admissible utility functions identical interest games the most restrictive class of games that we consider is identical interest games in such a game the players utility functions ui ni are the same that is for some function φ a r ui a φ a for every player i i and for every a a it is easy to verify that all identical interest games have at least one pure nash equilibrium namely any action profile a that maximizes φ a potential games a significant generalization of an identical interest game is a potential game monderer and shapley in a potential game the change in a player utility that results from a unilateral change in strategy equals the change in the global utility specifically there is a function φ a r such that for every player i i for every a i a i and for every a i a i ai ui a i a i ui a i a i φ a i a i φ a i a i when this condition is satisfied the game is called an exact potential game with the potential function φ in potential games any action profile maximizing the potential function is a pure nash equilibrium hence every potential game possesses at least one such equilibrium we will also consider a more general class of potential games known as ordinal potential games in ordinal potential games there is a global function φ a r such that for every player i i for every a i a i and for every a i a i ai ui a i a i ui a i a i φ a i a i φ a i a i j r marden j s shamma games and economic behavior weakly acyclic games consider any game with a set a of action profiles a better reply path is a sequence of action profiles al such that for every l there is exactly one player i such that i a i a i ii a i a i and iii ui a ui a in other words one player moves at a time and that player increases his own utility a best reply path is a better reply path with the additional requirement that each unilateral deviation is the result of a best response more specifically a best reply path is a sequence of action profiles al such that for every l there is exactly one player i such that i a i a i ii a i a i and iii a i bi a i consider any potential game with potential function φ starting from an arbitrary action profile a a construct a better reply path a al until it can no longer be extended note first that such a path cannot cycle back on itself because φ is strictly increasing along the path since a is finite the path cannot be extended indefinitely hence the last element in a maximal better reply path from any joint action a must be a nash equilibrium this idea may be generalized as follows a game is weakly acyclic if for any a a there exists a better reply path starting at a and ending at some pure nash equilibrium young a game is weakly acyclic under best replies if for any a a there exists a best reply path starting at a and ending at some pure nash equilibrium young potential games are special cases of weakly acyclic games an equivalent definition of weakly acyclic games utilizing potential functions is given in marden et al a game is weakly acyclic if and only if there exists a potential function φ a r such that for any action a a that is not a nash equilibrium there exists a player i i with an action a i ai such that ui a i a i ui ai a i and φ a i a i φ ai a i utility design utilizing game theoretic tools for distributed control of multi agent systems requires defining a local utility function for each agent designing these utility functions is nontrivial as there are several pertinent issues that need to be considered including scalability locality tractability and efficiency of the resulting stable solutions marden and wierman submitted for publication the starting point of utility design for multi agent systems is a global objective function of the form g a r which captures the behavior that the system designer would like to achieve the utility design question is how to distribute this global objective function to meet a variety of design objectives one approach is the wonderful life utility wolpert and tumor or marginal contribution utility which takes on the form ui ai a i g ai a i g a i where ai is fixed and referred to as the null action of player i it is straightforward to verify that the resulting game is a potential game with potential function g therefore the potential function maximizers of the resulting game coincide with the optimal system behavior the classes of games considered in section provide a paradigm for designing these local utility functions where weakly acyclic games provide the most flexibility with respect to utility design research in utility design for multi agent systems has sought to identify how to utilize this flexibility to meet various design objectives log linear learning in a repeated game at each time t each player i i simultaneously chooses an action ai t ai and receives the utility ui a t where a t t an t the action of player i is chosen at time t according to a probability distribution pi t ai where ai denotes the set of probability distributions over the finite set ai let pai i t denote the probability that player i will select action ai we refer to pi t as the strategy of player i at time t the following learning algorithm is known as log linear learning blume at each time t one player i i is randomly chosen and allowed to alter his current action all other players must repeat their current action at the ensuing time step i e a i t a i t at time t player i employs the strategy pi t ai where pai i t e τ ui ai a i t a i ai e τ ui a i a i t for any action ai ai and temperature τ the temperature τ determines how likely player i is to select a suboptimal action as τ player i will select any action ai ai with equal probability as τ player i will select a best response to the action profile a i t i e ai t bi a i t with arbitrarily high probability in the case of a non unique best response player i will select a best response at random uniformly the results in this paper focus on all three classes of games as they are all relevant for multi agent systems for example in marden et al the authors showed that for the problem of rendezvous it is impossible to design utility functions within the framework of potential games such that all resulting nash equilibria satisfy a given coupled constraint performance criterion however using the extra flexibility of weakly acyclic games utility functions could be designed to meet the desired performance criterions j r marden j s shamma games and economic behavior 808 consider any potential game with potential function φ a r in the repeated potential game in which all players adhere to log linear learning the stationary distribution of the joint action profiles is μ a where blume μ a e τ φ a a a e τ φ a one can interpret the stationary distribution μ as follows for sufficiently large times t μ a equals the probability that a t a as one decreases the temperature τ all the weight of the stationary distribution μ is on the joint actions that maximize the potential function the above analysis characterizes the precise stationary distribution as a function of the temperature τ the importance of the result is not solely the explicit form of the stationary distribution but rather the recognition that as τ the only stochastically stable states of the process are the joint actions that maximize the potential function from this point on rather than looking for the explicit stationary distribution we focus on analyzing the stochastically stable states of the process using the theory of resistance trees for regular perturbed markov decision processes this relaxation will allow us to modify the traditional log linear learning to include synchronous updates incomplete action sets and a payoff based implementation background on resistance trees the following is a very brief summary of the detailed review presented in young let denote the probability transition matrix for a finite state markov chain over the state space z we refer to as the unperturbed process let z denote the number of states consider a perturbed process such that the size of the perturbations can be indexed by a scalar and let p be the associated transition probability matrix the process p is called a regular perturbed markov process if p is ergodic for all sufficiently small and p approaches at an exponentially smooth rate young specifically the latter condition means that z z z lim p z z z z and p z z for some lim p z z r z z for some nonnegative real number r z z which is called the resistance of the transition z z note in particular that if z z then r z z construct a complete directed graph with z vertices one for each state the vertex corresponding to state z j will be called j the weight on the directed edge i j is denoted as ρi j r zi z j a tree t rooted at vertex j or j tree is a set of z directed edges such that from every vertex different from j there is a unique directed path in the tree to j the resistance of a rooted tree t is the sum of the resistances ρi j on the z edges that compose it the stochastic potential γj of state z j is defined to be the minimum resistance over all trees rooted at j the following theorem gives a simple criterion for determining the stochastically stable states young lemma theorem let p be a regular perturbed markov process and for each let μ be the unique stationary distribution of p then lim μ exists and the limiting distribution is a stationary distribution of the stochastically stable states i e the support of are precisely those states with minimum stochastic potential furthermore if a state is stochastically stable then the state must be in a recurrent class of the unperturbed process proof for log linear learning using theory of resistance trees before discussing variants of log linear learning we will reprove the original result using the theory of resistance trees the proof approach presented in this section can also be found in beggs blume we present the proof in detail for two reasons i the proof approach is similar to the forthcoming arguments for the variations of log linear learning presented in the subsequent sections and ii the structure of the proof can be exploited to analyze the limiting behavior of log linear learning in games outside the realm of potential games before proving that the only stochastically stable states are the potential function maximizers we will prove two lemmas the first lemma establishes that log linear learning induces a regular perturbed markov decision process lemma log linear learning induces a regular perturbed markov processwhere the unperturbed markov process is an asynchronous best reply process and the resistance of any feasible transition i is r max a i ai ui a i i ui j r marden j s shamma games and economic behavior 808 proof the unperturbed process is the following at each time t one player i i is randomly chosen and allowed to alter his current action all other players must repeat their current action at the ensuing time step i e a i t a i t at time t player i selects a best response to the action profile a i t i e ai t bi a i t in the case of multiple best responses player i selects one at random uniformly we refer to these dynamics as an asynchronous best reply process log linear learning induces a finite aperiodic irreducible process over the state space a we will analyze this process with respect to e τ rather than the temperature τ let p denote the associated transition probability matrix the probability of transitioning from to i is p n ui i ai ai ui ai i we assume for convenience that the updating player is selected randomly according to a uniform distribution define the maximum utility of player i for any action profile a i a i as vi a i max ai ai ui ai a i multiplying the numerator and denominator of by vi i we obtain p n vi i ui i ai ai vi i ui ai i accordingly lim p vi i ui i n bi i where bi i denotes the size or number of actions in player i best response set this implies that the process p is a regular perturbed markov process where the resistance of the transition is r vi i ui i max a i ai ui a i i ui i notice that r before stating the second lemma we introduce some notation a feasible action path p is a sequence of joint actions p am that are the result of unilateral deviations i e for each k m ak ai ak i for some player i and action ai ai the resistance of a path p is the sum of the resistance of each edge r p m k r ak ak lemma consider any finite n player potential game with potential function φ a r where all players adhere to log linear learning for any feasible action path p am and reverse path pr am am the difference in the total resistance across the paths is r p r pr φ φ am the player selection process could be relaxed to any probability distribution where the probability of selecting a given player is bounded away from j r marden j s shamma games and economic behavior 808 proof we will start by analyzing any edge in the feasible path ak ak suppose that ak a i ak i for some player i the resistance across this edge is r ak ak vi ak i ui ak the resistance across the reverse edge ak ak is r ak ak vi ak i ui ak vi ak i ui ak where the second equality is because ak i ak i the difference in the resistances across this edge is r ak ak r ak ak ui ak ui ak φ ak φ ak using the above equality the difference in the resistances across the two paths is r p r pr m k r ak ak r ak ak m k φ ak φ ak φ φ am before stating the following theorem we introduce the notation of a resistance tree in the context of log linear learning a tree t rooted at an action profile a is a set of a directed edges such that from every action profile a there is a unique directed path in the tree to a the resistance of a rooted tree t is the sum of the resistances on the edges r t a a t r a a let t a be defined as the set of trees rooted at the action profile a the stochastic potential of the action profile a is defined as γ a min t t a r t we refer to a minimum resistance tree as any tree that has minimum stochastic potential that is any tree t that satisfies r t min a a γ a proposition consider any finite n player potential game with potential function φ a r where all players adhere to log linear learning the stochastically stable states are the set of potential maximizers i e a a φ a maxa a φ a proof as mentioned earlier the proposition follows from the known form of the stationary distribution we now present a proof based on minimum resistance trees in preparation for the forthcoming analysis on variations of log linear learning by lemma we know that log linear learning induces a regular perturbed markov process therefore an action profile a a is stochastically stable if and only if there exists a minimum resistance tree rooted at a suppose that a minimum resistance tree t is rooted at an action profile a that does not maximize the potential function let a be any action profile that maximizes the potential function since t is a rooted tree there exists a path p from a to a of the form p a am a notice that p is a feasible action path consider the reverse path pr that goes from a to a pr a am a construct a new tree t rooted at a by adding the edges of pr to t and removing the redundant edges p the new tree will have the following resistance r t r t r pr r p j r marden j s shamma games and economic behavior 808 by lemma we know that r t r t φ a φ a r t we constructed a new tree t rooted at a with strictly less resistance than t therefore t cannot be a minimum resistance tree the above analysis can be repeated to show that all action profiles that maximize the potential function have the same stochastic potential hence all potential function maximizers are stochastically stable additional comments for log linear learning the insight that log linear learning is a regular perturbed markov decision process enables us to analyze the stochastically stable states for any game structure i e not only potential games using lemma and theorem in young corollary consider any finite n player game where all players adhere to log linear learning if an action profile is stochastically stable then the action profile must be contained in a best reply cycle i e a best reply path of the form a am a proof in any regular perturbed markov process the stochastically stable states must be contained in the recurrent classes of the unperturbed process in the case of log linear learning the unperturbed process is an asynchronous best reply process therefore regardless of the game structure if an action profile is stochastically stable it must be contained in a best reply cycle consider any game that is i weakly acyclic under best replies and ii all nash equilibria are strict a strict nash equilibrium is a recurrent class of the unperturbed process furthermore since there exists a best reply path from any action profile to a pure nash equilibrium we know that the recurrent classes of the unperturbed process are the set of strict nash equilibria therefore we have the following sharper characterization of the limiting behavior corollary consider any finite n player weakly acyclic game under best replies where all players adhere to log linear learning if all nash equilibria are strict then the set of stochastically stable states must be contained in the set of nash equilibrium in the case of non strict nash equilibrium we fall back on corollary which gives us that the stochastically stable states must be contained in a best reply cycle the above analysis also holds true for games that are close to being potential games as stated in the following corollary corollary consider any finite n player game with potential function φ a r where all players adhere to log linear learning suppose for any player i i actionsa i a i ai and joint action a i a i players utility satisfies ui a i a i ui a i a i φ a i a i φ a i a i δ for some δ if δ is sufficiently small then the stochastically stable states are contained in the set of potential maximizers we omit the proof of corollary as it is identical to the resistance tree proof for log linear learning in the case of an exact potential function the only difference is the implication of lemma where equality is replaced by a bound on the difference of the resistances across two paths this situation could arise when utility functions are corrupted with a slight degree of noise the main point of this section is that utilizing the theory of resistance trees to calculate the limiting behavior allows for a degree of relaxation in the learning process without significantly impacting the limiting behavior or for that matter requiring new proofs the price that we pay for this relaxation is that we forego characterizing the precise stationary distribution in favor of characterizing the support of the limiting distribution revisiting asynchrony in this section we explore whether asynchrony is necessary to guarantee the optimality of the stochastically stable states for log linear learning in potential games do the properties of the stochastically stable states change if a limited number of players are allowed to update each period or are we bound by asynchrony a nash equilibrium a is strict if for all i i and actions ai a i we have ui a ui ai a i j r marden j s shamma games and economic behavior 808 suppose at each time t a group of players g i is randomly chosen according to a fixed probability distribution q where denotes the set of subsets of i and qg denotes the probability that group g will be chosen we will refer to q as the revision process at time t each player i g plays a strategy pi t ai where pai i t e τ ui ai a i t a i ai e τ ui ai a i t for any action ai ai all players not in g must repeat their previous action i e a j t a j t for all j g we will refer to this learning algorithm as synchronous log linear learning with revision process q this setting is the focus of alos ferrer and netzer a counterexample synchronous log linear learning not only alters the resulting stationary distribution but also affects the set of stochastically stable states the following example illustrates this phenomenon consider a three player identical interest game with the following payoffs in this example there are several nash equilibria namely all joint action profiles that yield a utility of in addition to the action profile that yields a utility of suppose the revision process q has full support on the player sets an example of such a revision process is q we will see that all action profiles are stochastically stable before proceeding with the discussion we first introduce the following notation for any action profiles a a a the set of groups where the transition a a is feasible given the revision process q is defined as g a a q g i i i ai a i g qg synchronous log linear learning with revision process q induces a regular perturbed markov process with the unperturbed process being synchronous learning under best replies in synchronous log linear learning a transition is feasible if and only if g a a q it is straightforward to show that the resistance of a feasible transition is r min g g q i g max a i ai ui a i i ui i since synchronous log linear learning is a regular perturbed markov process we know that a minimum resistance tree must be rooted at any stochastically stable state it is straightforward to show that any action profile that yields a utility of has a rooted tree of resistance hence all such action profiles are stochastically stable furthermore there also exists a resistance tree rooted at of the form j r marden j s shamma games and economic behavior 808 r r r r r r r therefore is also stochastically stable building off the same tree it is straightforward to construct trees rooted at and that also have resistance e g a3 r r r r r r r therefore all action profiles are stochastically stable regular revision processes characterizing the stochastically stable states of a synchronous log linear learning process is highly dependent on the structure of the revision process q in addition to the structure of the game we start by defining a regular revision process introduced in alos ferrer and netzer definition regular revision process a regular revision process is a probability distribution q where for each player i i the probability assigned to the group consisting of only player i is positive i e qi the previous example demonstrates that any action profile may be stochastically stable even if we restrict our attention to regular revision processes and identical interest games however one can easily characterize the stochastically stable states if the game embodies a special structure this leads to the following theorem which extends theorem in alos ferrer and netzer to a larger class of games see alos ferrer and netzer for alternative examples of games and revision processes outside the realm of regular revision processes that exhibit similar behavior j r marden j s shamma games and economic behavior 808 theorem consider any finite n player weakly acyclic game under best replies where all nash equilibria are strict if all players adhere to synchronous log linear learning with regular revision process q then the stochastically stable states are contained in the set of nash equilibria proof synchronous log linear learning with regular revision process q is a regular perturbed markov process therefore the stochastically stable states must be contained in the recurrent class of the unperturbed process the unperturbed process is as follows at each time t a group of players g i is randomly chosen according to q at time t each player i g selects an action from his best reply set i e ai t bi a i t all players not in g must repeat their previous action i e a j t a j t for all j g for any game that is weakly acyclic game under best replies if all nash equilibria are strict then the recurrent classes of the unperturbed process are precisely the set of nash equilibria 3 independent revision process in general guaranteeing optimality of the stochastically stable states in potential games for an arbitrary revision process and utility interdependence structure is unattainable in this section we focus on particular class of revision processes that we refer to as independent revision processes in such setting each player independently decides whether to update his strategy by the log linear learning rule with some probability ω more precisely at each time t each player i i simultaneously plays the following strategy i with probability ω player i selects his previous action i e ai t ai t or ii with probability ω player i plays a strategy pi t ai where for any action ai ai pai i t e τ ui ai a i t a i ai e τ ui ai a i t theorem 2 consider any finite n player potential game with potential function φ a r where all players adhere to synchronous log linear learning with an independent revision process with update parameter ω set ω e τ m for sufficiently large m the stochastically stable states are the set of potential maximizers note that theorem 2 considers stochastic stability in terms of the combined perturbation of log linear vs maximizing action selection through τ and the group selection through ω the perturbations are related by ω e τ m with τ as usual proof of theorem 2 the probability of transitioning from a a is s i g s m s m i s i s ui a i a i a i ai ui a i a i where g i i ai a i and e τ it is straightforward to verify that synchronous log linear learning with an independent revision process is a regular perturbed markov process where the resistance of any transition a a is r a a m g i g max a i ai ui a i a i ui a i a i the unperturbed process corresponds to players never experimenting i e ω without loss of generality we will assume that φ a is bounded between and for all a a utilizing this normalization the resistance of the transition a a satisfies the following inequality m g n r a a m g let t be a minimum resistance tree rooted at a if for each edge a a t there is a single deviator i e i i ai ai then arguments from the proof of proposition 3 establish that a must be a potential function maximizer suppose there exists an edge a a t with multiple deviators i e i i ai ai 2 let g i i ai a i the resistance of this transition is at least r a a m g any best response potential game is weakly acyclic game under best replies voorneveld however the converse is not true the strictness condition could be relaxed as stated in the footnote of corollary 3 2 j r marden j s shamma games and economic behavior 808 consider any path p a a g a where each transition ak ak k g reflects a unilateral deviation by some player i g according to the resistance of each edge along this path is at most r ak ak m n therefore the resistance of the path p is at most r p g m n m g construct a new tree t rooted at a by adding the edges of p to t and removing the redundant edges pr the redundant edges are the set of edges leaving the action profiles a g in the original tree t the redundant edges pr include the edge a a in addition to g other edges each of which has resistance at least m the total resistance of the redundant edges is at least r pr m g m g the new tree t has resistance r t r t r p r pr r t m g m g m g r t m g since g 2 if m then r t r t this implies that any edge in a minimum resistance tree consists of only a single deviator which in turn implies that only potential function maximizers are stochastically stable games played on graphs the independent revision process discussed in the previous section guarantees that the only stochastically stable states are potential function maximizers irrespective of the structure of the utility interdependence in this section we focus on a particular structure of utility interdependence by considering games played on graphs where there are potentially a large number of players and each player utility is only effected by the actions of a subset of other players in such settings we demonstrate that one can exploit this structure to expand the class of revision processes that guarantee optimality of the stochastically stable states in games played on graphs each player s utility is influenced by a subset of other players let ni i represent the set of players that impact player i s utility we will refer to ni as the neighbors of player i in such a setting player i s utility is of the form ui ani r where ani j n j aj to avoid unnecessary notation we will still express the utility of player i given the action profile a as ui a if player i s utility depends on the complete action profile then ni i we make the following assumption on the revision process q assumption for any player i i there exists a group g i such that i g and qg proposition consider any finite n player potential game with potential function φ a r let all players adhere to synchronous log linear learning with a revision process satisfying assumption assume further that the revision process is such that any group of players g with qg is conflict free i e i j g i j i n j then the stochastically stable states are the set of potential maximizers proof for a given revision process q a transition is possible if g q according to it is straightforward to exploit the conflict free assumption to verify that r r φ φ the remainder of the proof is identical to the proof for log linear learning in section 3 2 we note that this result also could have been proved by analyzing the precise stationary distribution using the detailed balance condition as set forth in young this result is unsatisfying in that it requires a special structure on the revision process consider the following relaxation of the revision process roughly speaking suppose that the revision process q usually selects conflict free groups but occasionally selects conflicted groups to formalize this intuition we make the following assumption on the revision process j r marden j s shamma games and economic behavior 808 assumption 2 let the revision process q α be continuously parameterized by α where q α satisfies assumption for all α for any g qg α for some α implies that there exists a ρ such that lim α qg α αρ for any conflicted i e not conflict free group c such that qc α and any player i c there exists a conflict free group f i such that i f qf qf α κ α qc α for some κ independent of f and c assumption 2 is of the presented form to ensure that the resulting process is a regular perturbed process i e transitional probabilities decay at an exponentially smooth rate one could write a simpler representation of this assumption but the resulting analysis would require relaxing the conditions for regular perturbed processes and the results in young such developments are unnecessary for this paper theorem 3 consider any finite n player potential game with potential function φ a r where all players adhere to synchronous log linear learning with a revision process q α that satisfies assumptions 2 set α e τ m for sufficiently large m the stochastically stable states are the set of potential function maximizers as with theorem 2 theorem 3 considers stochastic stability in terms of the combined perturbation of log linear vs maximizing action selection through τ and conflicted vs conflict free group selection through α the perturbations are related by α e τ m with τ as usual proof of theorem 3 without loss of generality we assume that φ a is bounded between and for all a a synchronous log linear learning with a revision process q α induces a regular perturbed markov decision process the unperturbed process consists of conflict free group selection with unilateral best replies let t be a minimum resistance tree rooted at a if for each edge a a t there exists a conflict free group g such that qg and i i ai ai g then arguments from the proof of proposition establish that a must be a potential function maximizer suppose there exists an edge a a where there does not exist a conflict free group g such that qg and i i ai ai g from assumption 2 the probability of this transition is at most order mρ for some ρ accordingly the associated resistance satisfies r a a mρ by assumption 2 there exists a path p a al a of at most length i i ai a i where each transition ak ak k l reflects a unilateral deviation that can be accomplished by a conflict free group assumption 2 further implies that the resistance of each transition ak ak along the path p is at most r ak ak n the main idea behind the above inequality is as follows resistance computation is based on the product of the probability of a group being selected and the probability of an action being selected there exists a conflict free group with nonvanishing probability that can accomplish the desired transition the selection of this group contributes zero to the resistance value the action selection contributes to the resistance by at most n cf eq and the assumed normalized bounds of the potential function the resistance along the path p is bounded by r p i i ai ai n we can conclude that for mρ r p r a a construct a new tree t still rooted at a by adding the edges of p to t and removing the redundant edges the new tree t has strictly less resistance than t contradicting the assumption that t was a minimum resistance tree hence all minimum resistance trees must consist of only conflict free transitions this implies that only potential function maximizers are stochastically stable j r marden j s shamma games and economic behavior 808 revisiting completeness one role for learning algorithms in distributed control is to guide the decisions of players in real time that is the iterations of a learning algorithm correspond to the sequential decisions of players in such a setting players may not have the ability to select any particular action in their action set at any given time one example is multi vehicle motion control where an agent s action set represents discrete spatial locations mobility limitations restrict the ability to traverse from one location to another in a given time period standard log linear learning assumes an agent can access any action at each iteration which we refer to as completeness furthermore in order to implement log linear learning agents must have access to information i e hypothetical payoffs for all possible actions even in non control theoretic applications having this degree of information when each player s action set is quite large is demanding rather having an algorithm that allows each agent to select the next action using only local information is desirable these considerations motivate the introduction of constraints between iterations of a learning algorithm let a t be the joint action at time t with constrained action sets the set of actions available to player i at time t is a function of his action at time t and will be denoted as ci ai t ai we will adopt the convention that ai ci ai for any action ai ai i e a player is always allowed to stay with his previous action we will say that a player s action set is complete if ci ai ai for all actions ai ai we make the following two assumptions on constrained action sets assumption for any player i i and any action pair ami ai there exists a sequence of actions ami satisfying aki ci ak i for all k 2 m assumption 2 for any player i i and any action pair ai ci ci a counterexample log linear learning in its original form requires completeness of players action sets suppose log linear learning is employed in a situation with constrained action sets at time t player i plays a strategy pi t ai where pai i t e τ ui ai a i t a i ci ai e τ ui a i a i t for any action ai ci ai for any action ai ci ai it is easy to see that log linear learning in this setting induces a regular perturbed markov process the resistance of a feasible transition i e is of the form i for some player i and action ci is now r max a i ci ui a i i ui the following example demonstrates that log linear learning applied to constrained action sets need not result in stochastically stable joint actions that are potential function maximizer consider a two player identical interest game with payoffs player player 2 in this example there are two nash equilibria and but only one potential maximizer let the constrained action sets satisfy one could view constrained action sets as either a constraint on available moves for distributed engineering systems or a constraint on information in game theoretic learning we note that this scenario could have been formulated as a stochastic game shapley where the state is defined as the previous action profile and the state dependent action sets are defined according to the constrained action sets we avoid formally defining the game as a stochastic game in favor of a more direct treatment j r marden j s shamma games and economic behavior 808 b1 b1 c2 a state is stochastically stable if and only if the state has minimum stochastic potential the stochastic potential of state b1 is highlighted by the following resistance tree b1 b2 b1 b2 r r r r r the stochastic potential of state b3 is highlighted by the following resistance tree b1 b2 b3 b1 b2 10 a2 b3 10 r 10 r r r r hence a2 b1 is not stochastically stable in fact one can construct an alternative example which illustrates that the stochastically stable states need not be contained in the set of nash equilibria for potential games when action sets are incomplete the key insight for this example lies in the best reply graph in log linear learning with complete action sets the recurrent classes of the unperturbed process are the nash equilibria constraining action sets changes the structure of the best reply graph in particular these changes may induce new local nash equilibria i e action profiles where no player can unilaterally improve his utility by selecting an action in his constrained action set these local nash equilibria are now recurrent classes of the unperturbed process and are candidates for the stochastically stable states of the perturbed process furthermore this example illustrates that lemma 3 2 no longer holds in the constrained setting because the difference in the resistances across two paths is no longer a function of the endpoints 2 binary log linear learning in this subsection we introduce a variant of log linear learning that will rectify the problems caused by constrained action sets consider the following learning algorithm which we refer to as binary log linear learning originally proposed in arslan et al marden et al at each time t one player i i is randomly chosen uniformly and allowed to alter his current action all other players must repeat their current action at the ensuing time step i e a i t a i t at time t player i selects one trial action ˆai uniformly from his constrained action set ci ai t ai player i plays a strategy pi t ai where pai t i t e τ ui a t e τ ui a t e τ ui ˆai a i t p ˆa i i t e τ ui ˆai a i t e τ ui a t e τ ui ˆai a i t j r marden j s shamma games and economic behavior 808 pai i t ai ai t ˆai if ai t ˆai then pai t i marden et al analyze the stationary distribution of binary log linear learning when action sets are constrained in particular if the probability of selecting a trial action satisfies a certain specified condition then the stationary distribution of 3 is preserved this condition involves accommodating for the time varying cardinality ci ai t i we now show that this condition can be relaxed by changing the focus of the analysis from the stationary distribution to the stochastically stable states theorem consider any finite n player potential game with potential function φ a r and constrained action sets satisfying assumptions and 2 if all players adhere to binary log linear learning then the stochastically stable states are the set of potential maximizers before proving that the only stochastically stable states are the potential maximizers we introduce two lemmas the first lemma establishes that binary log linear learning is a regular perturbed markov decision process lemma binary log linear learning induces a regular perturbed markov process where the resistance of any feasible transition i where ci is r max a i ui a i i ui proof the unperturbed process employs a maximizing strategy as opposed to log linear strategy binary log linear learning with constrained action sets induces a finite aperiodic irreducible process over the state space a irreducibility is a consequence of assumption let e τ let p denote the associated probability transition matrix the probability of transitioning from to i where ci is now of the form p n ci ui ui ui 11 redefine the functions vi max ui ui bi a ui a vi accordingly lim p vi ui n ci bi this implies that the process p is a regular perturbed markov process where the resistance of the transition is r vi ui we state the following lemma without proof as it is almost identical to the proof of lemma 3 2 lemma 2 consider any finite n player potential game with potential function φ a r where all players adhere to binary loglinear learning for any feasible action path p am and reverse path pr am am the difference in the total resistance across the paths is r p r pr φ φ am the results presented in this section still hold even if the probability distributions are not uniform in the case of the player selection process any probability distribution with full support on the player set 2 n would work the probability of selecting a trial action can be relaxed in the same fashion j r marden j s shamma games and economic behavior 808 proof of theorem by lemma we know that binary log linear learning induces a regular perturbed markov process therefore an action profile a a is stochastically stable if and only if there exists a minimum resistance tree rooted at a suppose that a minimum resistance tree t was rooted at an action profile a that does not maximize the potential function let a be any action profile that maximizes the potential function since t is a rooted tree there exists a path p from a to a of the form p a am a notice that p is a feasible action path consider the reverse path pr that goes from a to a pr a am a such a path exists because of assumption 2 construct a new tree t rooted at a by adding the edges of pr to t and removing the redundant edges p the new tree will have the following resistance r t r t r pr r p by lemma 3 2 we know that r t r t φ a φ a r t therefore we constructed a new tree t rooted at a with strictly less resistance than t accordingly t cannot be a minimum resistance tree the above analysis can be repeated to show that all action profiles that maximize the potential function have the same stochastic potential hence all potential function maximizers are stochastically stable when revisiting the example in section the stochastic potential of state a2 b1 is now highlighted by the following resistance tree b1 b2 b3 a2 b1 10 a2 b2 10 a2 b3 10 r r r r r 9 the potential maximizer a2 b1 is the only stochastically stable state payoff based implementation in any of the previous versions of log linear learning each player needs to be endowed with the ability to assess the utility that the player would have received had the player selected an alternative action that is given any action profile a each player i is able to compute ui a i a i for all a i ai in this section we seek to understand what happens if players do not have access to this information rather players have access to only i the action they played and ii the utility they received payoff based learning algorithms have received significant attention recently babichenko young marden et al foster and young germano and lugosi while most of this research focusing on introducing dynamics that show that pure nash equilibria are stochastically stable in this section we introduce a payoff based learning algorithm such that only potential function maximizers are stochastically stable in fact we combine two of the algorithms previously studied synchronous log linear learning with an independent revision process in section 3 and binary log linear learning in section 2 to develop a payoff based version of log linear learning with the desired properties we introduce the following learning algorithm called payoff based log linear learning for any time t 2 let a t and a t be the action profiles at time t and t respectively define xi t to be a binary flag that indicates whether player i experimented in time t at time t each player i simultaneously selects an action ai ai according to the following rule if player i did not experiment in period t i e xi t then j r marden j s shamma games and economic behavior 808 with probability ω ai t is chosen randomly uniformly over ai xi t with probability ω ai t ai t xi t where ω is the exploration rate if player i experimented in period t i e xi t then ai t ai t with probability e τ ui a t e τ ui a t e τ ui a t ai t with probability e τ ui a t e τ ui a t e τ ui a t 12 and xi t these dynamics can be described as follows occasionally players experiment with a new action in the event that a player does experiment then the player switches to the new action with a log linear strategy over the utility received in the previous two time periods the above dynamics require that a player knows only his own utility received and action employed in the last two time steps and whether he experimented or not in the previous time step these dynamics do not require players to have any knowledge regarding the actions strategies or utilities of the other players theorem consider any finite n player potential game with potential function φ a r where all players adhere to payoff based log linear learning set ω e τ m for sufficiently large m the stochastically stable states are the set of potential maximizers in a similar manner to theorem 4 3 theorem considers stochastic stability in terms of the combined perturbation of log linear action selection through τ and experimentation through ω the perturbations are related by ω e 1 τ m the remainder of this subsection is devoted to the proof of theorem 1 without loss of generality we will assume that φ a is bounded between and 1 for all a a first define the state of the dynamics at time t to be the tuple z t a t 1 a t x t a a 1 n claim 1 payoff based log linear learning induces a regular perturbed markov decision process furthermore the resistance of a transition from state to state a2 is r ms i 1 vi ui i 1 vi ui where vi max ui ui s x i xi proof the unperturbed process corresponds to players never experimenting i e ω the recurrent classes of the unperturbed process are states of the form a a where boldface denotes a zero vector of appropriate dimension valid transitions of the perturbed process are of the following form x1 a2 where or 1 ai 1 a1i let x1 a2 x2 and e 1 τ the probability of the transition from is j r marden j s shamma games and economic behavior 808 p i 1 ω i 1 ω ai i 1 ui ui ui i 1 a2i a1i ui ui ui it is straightforward to verify that lim p r with r as stated in the claim note that the resistance of a particular transition is expressed as two terms the first term in eq denoted captures the number of synchronous deviators the second term denoted captures the log linear response of players partition the state space into the following five sets d e and e is the set of states a a x such that xi 1 for at least one player i is the set of states a a 0 such that a a d is the set of states a a 0 such that a is not a pure nash equilibrium e is the set of states a a 0 such that a is a pure nash equilibrium but is not a maximizer of φ e is the set of states a a 0 such that a is a pure nash equilibrium and is a maximizer of φ these account for all possible states the only candidates for the stochastically stable states are the recurrent classes of the unperturbed process this eliminates the possibility that states in and are stochastically stable therefore from now on we restrict our attention only to trees rooted at states of d e and e for simplicity we use shorthand notation to write a a 0 as simply a claim 2 all edges have a resistance r a1 m proof any transition requires at least one player to experiment which happens with at most probability on the order of m note that a single edge between states in d e or e will involve transitions between multiple states accordingly the expression r a1 refers to the resistance of the path between and a1 claim 3 all edges a1 in a minimum resistance tree must consist of a unilateral deviator i e i i a1i 1 proof we will construct resistance trees over vertices in d e e let t be a minimum resistance tree suppose there exists an edge a1 where multiple players switched actions i e there exists a group of players g i g 1 such that a0i a1i if and only if i g we can express the resistance of this transition as r a1 m g r a1 where r a a captures the effect of in eq because of the assumed normalized potential function 0 r a1 g construct a new path p of unilateral deviations p a 1 a g a1 where for all k 0 1 g 1 ak 1 a1i ak i for some player i g from the resistance of the path p satisfies m g r p m g g construct a new tree t by adding the edges of p to t and removing the redundant edges the removed resistance of the edge emanating from the originating node is at least m g furthermore the removed resistance from each of the remaining edges is at least m by claim 2 therefore the overall removed resistance is at least m g m g 1 the added path p has resistance of at most m 1 g accordingly the new tree has strictly less resistance as long as g 2 this contradicts the original t being a minimum resistance tree j r marden j s shamma games and economic behavior 808 fig 1 simulation of payoff based log linear learning on proposed coordination game algorithm achieves convergence to potential function maximizer relatively quickly claim 4 a minimum resistance tree must be rooted at an action profile that maximizes the potential function proof let t be a minimum resistance tree rooted at d there exists a better reply path p from to a nash equilibrium a p a1 ak a since p is a better reply path the resistance of the path is precisely r p km construct a new tree t rooted at am by adding the edges of p to t and removing the redundant edges this results in r t r t because the removed edge exiting a has resistance r a a m which is a contradiction accordingly a minimum resistance tree cannot be rooted at a state in d suppose that a minimum resistance tree t was rooted at a nash equilibrium that did not maximize the potential function i e e let a be any action profile that maximizes the potential function since t is a rooted tree there exists a path p from a to of the form p a ak ak 1 where each edge al 1 al is the result of unilateral deviation consider the reverse path pr pr a1 ak a exploiting the similarities between payoff based log linear learning with unilateral updates and binary log linear learning we can use lemma 2 to show that the difference in the total resistance across the paths is r p r pr φ a0 φ a construct a new tree t rooted at a by adding the edges of pr to t and removing the redundant edges p the new tree will have resistance r t r t r pr r p r t φ a0 φ a r t since the new tree t rooted at a has strictly less resistance than t t cannot be a minimum resistance tree the above analysis can be repeated to show that all action profiles that maximize the potential function have the same stochastic potential hence all potential function maximizers are stochastically stable illustration in this section we simulate the learning algorithm payoff based log linear learning on a simple coordination game with player set i 1 2 10 action sets ai x y for each player i n and payoffs 808 j r marden j s shamma games and economic behavior 2012 808 ui a a x if ai x 0 5 a y if ai y where a x denotes the number of players that selected x in the action profile a i e a x i n ai x the above game is an instance of a congestion game with two pure strategy nash equilibria x x x and y y y it is straightforward to show that the potential function satisfies φ x φ y fig 1 illustrates the outcome of a simulation of payoff based log linear learning on the given coordination game with the following specifications 0 1 m 2 and a 0 y y this example highlights that it is not necessary to utilize limiting cases of perturbation terms i e 0 and m sufficiently large to achieve the desired equilibrium selection the reason for this is that having multiple deviators is not problematic with respect to the asymptotic guarantees in many settings conclusion the central issue explored in this paper is whether the structural requirements of log linear learning are necessary to guarantee convergence to the potential function maximizer in potential games in particular we have shown that the requirements of asynchrony and completeness are not necessary the key enabler for these results is to change the focus of the analysis away from deriving the explicit form of the stationary distribution of the learning process towards characterizing the stochastically stable states establishing that log linear learning constitutes a regular perturbed markov process allows the utilization of theory of resistance trees as in young to analyze the limiting properties of variations on log linear learning broadly deﬁned social dilemmas involve a conﬂict between immediate self interest and longer term col lective interests these are challenging situations because acting in one immediate self interest is tempting to everyone involved even though everybody beneﬁts from acting in the longer term collective interest as such greater knowledge of social dilemmas should help us understand not only the theoret ical puzzles of why people cooperate or not but also the ways in which cooperation in groups and orga nizations can be maintained or promoted this article reviews different types of social dilemmas highlights recent developments in the ﬁeld especially within psychology and suggests some new ave nues for future research we illustrate that the ﬁeld of social dilemma is growing and ﬂourishing in terms of theory interdisciplinary collaboration and applicability producing insights that are novel replicable and applicable to many social situations where short term self interest is at odds with the long term interests of teams organizations or nations elsevier inc all rights reserved introduction many of the world most pressing problems represent social dilemmas broadly deﬁned as situations in which short term self interest is at odds with longer term collective interests some of the most widely recognized social dilemmas challenge society well being in the environmental domain including overharvesting of ﬁsh overgrazing of common property overpopulation destruc tion of the brazilian rainforest and buildup of greenhouse gasses due to overreliance on cars the lure of short term self interest can also discourage people from contributing time money or ef fort toward the provision of collectively beneﬁcial goods for example people may listen to national public radio without con tributing toward its operations community members may enjoy a public ﬁreworks display without helping to fund it employees may elect to never go above and beyond the call of duty choosing instead to engage solely in activities proscribed by their formally deﬁned job description and citizens may decide to not exert the effort to vote leaving the functioning of their democracy to their compatriots as the preceding examples illustrate social dilemmas apply to a wide range of real world problems they exist within dyads small groups and society at large and they deal with issues relevant to a large number of disciplines including anthropology biology eco corresponding author e mail address pam van lange psy vu nl p a m van lange nomics mathematics psychology political science and sociology given their scope implications and interdisciplinary nature social dilemmas have motivated huge literatures in each of these disci plines several excellent reviews of this literature exist but many are dated or are narrowly focused on a speciﬁc variable that inﬂu ences cooperation in social dilemmas in the present paper we build on past reviews by outlining key principles relevant to the deﬁnition of social dilemmas summarizing past reviews discuss ing recent developments in the ﬁeld and identifying future re search directions with the potential to shed additional light on this important and ever developing ﬁeld social dilemmas beyond the prisoner dilemma and immediate consequences social dilemmas come in many ﬂavors sometimes cooperation means giving or contributing to the collective sometimes it means not taking or consuming from a resource shared by a collective sometimes the time horizon is short even as short as a single interaction sometimes it is long lasting almost without an end as in ongoing relationships there are social dilemmas involving two persons and social dilemmas involving all people living in a country continent or even world not surprisingly the diversity in social dilemma settings has led researchers to offer a range of different deﬁnitions for the concept in his annual review of psy chology article dawes was one of the ﬁrst who formally coined the term social dilemma which he deﬁned as a situation see front matter elsevier inc all rights reserved http dx doi org j obhdp in which a each decision maker has a dominating strategy dictat ing non cooperation i e an option that produces the highest out come regardless of others choices and b if all choose this dominating strategy all end up worse off than if all had cooperated i e a deﬁcient equilibrium but as we will see while focusing on the crux of the dilemma this deﬁnition does not do justice to some other outcome structures or more precisely interdependence structures that also captures the conﬂict between self interest and collective interest which include not only the prisoner di lemma but also the chicken dilemma and the assurance dilemma or trust dilemma this deﬁnition also does not include the tempo ral or time dimension e g messick brewer van lange joireman because consequences can be immediate short term or delayed long term such a more inclusive conceptualiza tion allows us to include social traps social fences public good dilemmas and resource dilemmas see table we brieﬂy discuss both features in turn prisoner chicken and assurance dilemmas the well known prisoner dilemma has often been used as the basis for deﬁning social dilemmas which is also evident in dawes deﬁnition we suggest that two other outcome interdependence structures can also be viewed as social dilemmas if one relaxes the requirements for a dominating strategy and a single equilib rium these structures include the chicken and the assurance or trust dilemma in both dilemmas the individual vs collective conﬂict essential to social dilemmas is retained there is a non cooperative course of action that is at times tempting for each individual and if all pursue this non cooperative course of action all end up worse off than if all had cooperated in the chicken dilemma each person is tempted to behave non cooperatively by driving straight toward one opponent in an effort to win the game but if neither player cooperates swerves both parties experience the worst outcome possible death clearly chicken does not involve a dominating strategy as the best decision for an individually rational decision maker depends on what he or she believes the other will do if one believes the other will cooperate swerve the best course of action is to behave non cooperatively and continue driving ahead however if one is con vinced that the other will not cooperate will not swerve one best course of action is to cooperate swerve because it is better to lose the game than to die there are interesting parallels be tween chicken and situations in which people are faced with the dilemma whether to maintain honor or status at nearly any risk see kelley et al the assurance trust dilemma also lacks a dominating strategy and is unique in that the highest collective and individual out comes occur when both partners choose to cooperate this corre spondence of joint and own outcomes might suggest that the solution is simple and there is no dilemma however if one party considers beating the other party to be more important than obtaining high outcomes for the self and others or is convinced the other will behave competitively the best course of action is to not cooperate thus like the chicken dilemma the assurance dilemma is a situation in which there is a non cooperative course of action that can at times be tempting for each individual and if all pursue this non cooperative course of action all are worse off than if all had cooperated the temporal dimension we often see that the consequences for self can be immediate or delayed just as the consequences for the collective can be immedi ate or delayed this temporal dimension is exempliﬁed in social traps or situations in which a course of action that offers positive outcomes for the self leads to negative outcomes for the collective examples of delayed social traps include the buildup of pollution due to overreliance on cars and the eventual collapse of a common ﬁshing ground as a result of sustained overharvesting given their emphasis on consuming or taking a positive outcome for the self social traps are often called take some dilemmas a classic example of which is the commons or resource dilemma these social trap situations may be contrasted with social fences or situations in which an action that results in negative conse quences for the self would if performed by enough people lead to positive consequences for the collective examples of delayed social fences include the eventual deterioration of a company po sitive culture due to employees unwillingness to engage in extra role or organizational citizenship behaviors such as being a good sport and helping new employees adjust and the gradual deterio ration of an education system due to taxpayers unwillingness to fund school levies given their emphasis on giving something of the self such as time money or effort social fences are often called give some dilemmas a classic example of which is the public goods dilemma which have been extensively studied by economists in particular deﬁnition and history we deﬁne social dilemmas as situations in which a non cooper ative course of action is at times tempting for each individual in that it yields superior often short term outcomes for self and if all pursue this non cooperative course of action all are often in the longer term worse off than if all had cooperated this deﬁni tion is inclusive of the well known prisoner dilemma as well as the chicken dilemma and the assurance or trust dilemma and it includes the correlation with time such that consequences for self are often immediate or short term while the consequences for the collective often unfold over longer periods of time we sug gest that this provides a fairly comprehensive deﬁnition of social dilemmas at the same time we acknowledge that other important distinctions are not included one such distinction is the difference between ﬁrst order dilemma which represents the initial dilemma table classiﬁcation of social dilemmas after messick and brewer collective consequences immediate delayed social traps take some dilemmas commuting by car vs public transportation or carpooling leads to daily trafﬁc congestion and stress harvesting as many ﬁsh as one can from a common resource eventually leads to the collapse of the resource commons resource dilemmas social fences give some dilemmas electing to not contribute to a community funded ﬁreworks show results in cancellation of the show choosing to not engage in extra role behaviors that beneﬁt one company eventually leads to a deterioration of the company positive culture public goods dilemmas and a second order dilemma which represents the dilemma that one might face when deciding whether to contribute to a costly system that might promote cooperation in the ﬁrst order dilemma e g a system that sanctions free riders yamagishi coop eration in the ﬁrst order dilemma is known as elementary coopera tion while cooperation in the second order dilemma is known as instrumental cooperation as we will see in this article a good deal of contemporary research on social dilemmas has been devoted to this very problem providing strong evidence that many but not all people are quite willing to engage in costly behavior to reward other group members who have cooperated and punish those who have not cooperated e g fehr gächter it is interesting to note that the deﬁnitions of social dilemmas are marked by several important conceptual reviews of social dilemmas in one of the earliest reviews pruitt and kimmel summarized years of research on experimental games concluding that cooperation requires both the goal of cooperating and the expectation that others will cooperate the well known goal expectation theory three years later dawes published his review of research on the n person prisoner dilemma in which he introduced among others the terms give some and take some games building on hardin analysis of the tragedy of the commons messick and brewer subse quently discussed the notion of social traps and fences and identi ﬁed two categories of solutions to social dilemmas including individual solutions and structural solutions in more recent reviews komorita and parks and kollock reiterated many of the same themes and discussed how re ciprocal strategies e g tit for tat and sanctions encourage coop eration which was inspired by yamagishi earlier work on ﬁrst order and second order social dilemmas or elementary cooperation and instrumental cooperation and then over the past decade fehr and gächter conceptualized and studied the potential for reward and punishment generally showing pro nounced increases in cooperation in situations in which partici pants were able vs were not able to punish or reward one another a recent meta analysis provides strong support for the power of reward and punishment and also suggests that they may be even more effective when administered by fellow members facing the social dilemma rather than authorities for a recent review see balliet mulder van lange interestingly some of these in sights were already recognized by elinor ostrom in nobel prize laureate in who suggested that institutes could play a very important role in regulating the management of natural re sources and avoiding ecosystem collapses she emphasized the importance of sanctioning and reward preferably at local levels and the use of local monitoring and conﬂict resolution that are inexpensive and of easy access she was a strong believer in local arrangements by self determination of the community by higher level authorities and she believed in internal mechanisms such as effective communication internal trust and reciprocity among the people who literally face the social dilemma looking back these historical developments reveal several noteworthy trends first various scientiﬁc disciplines clearly have grown toward each other such that there is much greater ex change of knowledge and tools such as research paradigms that are very important to further progress in the science of human cooperation second we witness that theory or science and real ity or application go hand in hand these are issues that are immediately apparent in several edited volumes e g foddy smithson schneider hogg schroeder suleiman budescu fischer messick to recent overviews e g van lange balliet parks van vugt and to meta analytic re views on basic issues such as trust balliet van lange in press a indeed in the past several years a plea for interdisciplinary re search gintis translation from basic theory to societal application e g parks joireman van lange in press and issues of generalization to different samples and societies herrmann et al see also balliet van lange in press b underscore ex actly the point we are trying make and the further link with neu roscience genetics and culture makes it all the more interesting glimcher camerer fehr poldrack henrich et al indeed these are truly exciting times for research and theorists of human cooperation as such it makes sense to focus our atten tion to recent developments while acknowledging the classics which is what follows next recent developments our review is organized around calls in the literature for the development of theory more interdisciplinary and applied re search and three broad categories of factors that inﬂuence cooper ation in social dilemmas structural psychological and dynamic inﬂuences our review focuses largely on developments in the so cial psychological literature though we also address growing liter atures in a number of related ﬁelds of study moreover in light of space our goal is not to exhaustively catalogue the many factors that drive choice behavior in social dilemmas but rather to high light several important and exciting developments in the ﬁeld ultimately our goal is to use this discussion of recent develop ments as a bridge between classic research on social dilemmas and future directions with the potential to contribute new insights to this important and growing ﬁeld in particular we will discuss broad developments in a theoretical frameworks in psychology b interdisciplinary approaches to social dilemmas and c ecolog ical validity or trends from games to real life theoretical frameworks despite the wealth of empirical studies on social dilemmas the ﬁeld has often been criticized for lacking a coherent macro level theoretical framework this is not to say that dilemma research has been atheoretical but rather that the theories and hypotheses offered have tended to focus more narrowly on a speciﬁc set of variables and or processes for a review see parks et al in press several theories however have been advanced with the potential to bring order to the ﬁeld including classic and extended versions of interdependence theory kelley thibaut kelley et al van lange rusbult the appropriateness framework weber kopelman messick and evolutionary theorizing such as reciprocal altruism indirect reciprocity and costly signaling interdependence theory one theory that has served as an integrative framework for sev eral social interaction situations and interpersonal relations is interdependence theory kelley thibaut kelley et al van lange rusbult with its roots in game theory interdependence theory assumes that interdependent interactions are a combined function of an interdependence structure e g the prisoner dilemma the interacting partners e g partner a and b and interaction dynamics e g the use of a tit for tat strategy or sabi structure partners a and b interaction within this framework interdependence theory also assumes that decision makers transform a given structure or matrix of objective out comes into an effective matrix of subjective outcomes that is more closely linked to behavior the given matrix represents short term self interested preferences determined by the situation in combi nation with each individual needs skills etc while the effective matrix emerges once decision makers take into account broader so cial and temporal concerns including concern with others out comes and or concern with the long term consequences of one actions and or cognitive and affective states such as recently primed schemas and mood as shown in fig while the notion of transformations has been recognized for some time there have been at least two signiﬁcant advances with respect to this important concept first in understanding social preferences or social utilities there have been increasing at tempts to summarize the major preferences building on previous models e g messick mcclintock some integrative mod els suggest that transformations can be understood in terms of the weights that people assign to outcomes for self outcomes for other and equality in outcomes e g van lange apart from self interest such theorizing identiﬁes altruism collectivism and egalitarianism as important motives that might underlie coopera tion van lange de cremer van dijk van vugt and provides a broader interdependence based framework for under standing various programs of research focusing on particular mo tives e g batson moreover whereas the majority of past theory and research has emphasized motivational transforma tions a good deal of recent theory and research is now focusing on the role of other cognitive and affective transformations e g empathy van lange second whereas early theorizing in social dilemmas based on interdependence theory typically stressed the importance of so cial transformations e g as a result of prosocial vs proself value orientations more recent theory and research have been devoting increasing attention to the role of temporal transformations e g as a result of future time orientation or a concern with future consequences joireman kelley et al van lange joireman these transformations are essential to under standing behavior in many social dilemmas that involve both a so cial conﬂict individual vs collective interests and a temporal conﬂict short term vs long term interests indeed an important challenge in those social dilemmas is the willingness and ability for self control which is often deﬁned in terms of choosing to max imize the long term vs short term consequences of one actions e g joireman balliet sprott spangenberg schultz with in this framework features of the person or situation that promote self control and or a concern with future consequences have the potential to encourage cooperation e g insko et al joir eman et al van lange klapwijk van munster or promote positive responses to non cooperation such as forgiveness balliet li joireman appropriateness framework another theoretical advance in the ﬁeld of social dilemmas is weber et al appropriateness framework see also dawes messick the appropriateness framework assumes that decisions are driven by three basic factors including one deﬁni tion or recognition of a situation e g is this a cooperative task or not one identity e g do i strongly identify with my group and the application of decision rules or heuristics e g do unto others as you would have them do unto you these three factors are thought to inﬂuence how decision makers answer the fundamental question what does a person like me do in a situation like this as shown in fig weber and colleagues framework sug gests that features of the objective situation impact the decision maker identity and how the situation is perceived the model also assumes that identity is driven by a decision maker personal his tory e g individual differences learning the decision maker identity then inﬂuences how he or she interprets the situation and how perception of the situation impacts his or her choice of decision rules which ultimately leads to one ﬁnal decision like kelley and thibaut interdependence theory weber and col leagues model stresses decision makers construal of the situation moreover complementing interdependence theory the appropri ateness framework clearly recognizes the impact of personal iden tity and decision heuristics both of which have featured prominently in recent work on social dilemmas evolutionary theory needless to say scholars have also increasingly drawn on broad theoretical frameworks from evolutionary theory to account for altruism and cooperation in social dilemmas in particular four theories advanced to understand altruism include kin selection re fig interdependence theory structure partners a and b and interaction sabi model illustrating transformation from given to effective matrix van lange rusbult fig appropriateness framework weber et al ciprocal altruism or direct reciprocity indirect reciprocity and costly signaling kin selection suggests that people are more likely to help those with whom they share a genetic link hamilton in support of this theory people are likely to help kin less likely to harm kin and are more likely to tolerate injustices from kin also people are more likely to help close kin over distal kin especially in important life death decisions whereas for relatively mundane issues kinship matter somewhat less e g burnstein crandall kitayama reciprocal altruism trivers is the evolutionary theoreti cal concept that most game theorists and psychologists would call reciprocity or direct reciprocity there is indeed considerable evidence for the idea that people cooperative behavior is strongly inﬂuenced by the other persons behavior in fact there is even evidence that of the participants make a choice in a social di lemma that can be characterized as reciprocal behaving exactly as cooperatively as they expected the other person to cooperate van lange study we return to the topic of reciprocity later in this article indirect reciprocity theory assumes that people respond to infor mation relevant to other reputation as being cooperative or non cooperative by behaving cooperatively or non cooperatively moreover it assumes that people favor a cooperative reputation over a non cooperative reputation a mechanism which may ac count for the evolution of cooperation among strangers with whom one is not genetically related and with whom one does not expect future interaction and did not have interactions in the past there is indeed evidence revealing that in the absence of possibilities of direct reciprocity people respond to reputational information by giving more to others who had been cooperative in the past and this explains how cooperation can develop and sus tain when updated reputational information is available wede kind milinski costly signaling theory is closely linked to indirect reciprocity theory and assumes that humans and other species might en gage in costly activities to signal traits that often are desired se lected for by other people e g gintis smith bowles for example donating large donations when communicated to others might signal not only generosity but also other desirable traits that might explain why a person is able to do so for example generos ity may signal abundance of resources and high status such signals may provide desirable opportunities when selected as interaction partner including extending lucrative business or if one is so in clined mating opportunities an excellent example of costly sig naling is the escalation of cooperative behavior in a process known as runaway social selection nesse or competitive altruism barclay hardy van vugt in summary we suggest interdependence theory as broad the oretical framework that helps understand what a situation is about interdependence structure and what people might make of it transformations the appropriateness framework comple ments this approach by emphasizing the role of norms identity concerns and heuristics that people might use to make decisions in social dilemmas and of course the evolutionary theories place cooperation in a broader context of adaptation thereby emphasiz ing the functional value of direct and indirect forms of reciprocity and the role of reputation and signaling the latter framework has made considerable progress over the past decade and may serve as a grand theory for many more speciﬁc theories by delineating the ultimate causes of human cognition and behavior in social dilemmas interdisciplinary perspectives beyond increased development of theory we have witnessed increased attention to interdisciplinary research indeed it has been a bit of a paradox that social dilemma researchers working in different disciplines did not cooperate as much as they could or perhaps should on the one hand of course the various disci plines make their own unique contributions for example evolu tionary biology focuses on ultimate distal mechanisms that support evolutionary outcomes ﬁtness or reproductive success experimental economists frequently develop experimental games to study issues related to cooperation mathematicians and game theorists provide a logical analysis of rational choice behavior in such experimental games and social psychologists explore the proximate person and situation factors that impact choice behav ior in such games thus from one perspective it is understandable that many researchers have chosen to tackle the issue of coopera tion in social dilemmas from their own disciplinary perspective at the same time over the past years we have seen that anthropologists evolutionary biologists economists neuroscien tists political scientists psychologists and sociologists increas ingly work together to address fundamental questions about human cooperation for example books and papers have appeared that outline the beneﬁts of bridging various disciplines for the study of human cooperation and we see increasing evidence of cross referencing among various disciplines e g gintis so cial dilemma researchers are also increasingly applying social di lemma analyses to understand a range of real world problems including commuting decisions joireman van lange van vugt van vugt meertens van lange and organizational citizenship behaviors e g joireman kamdar daniels duell see also balliet ferris also social psychologists are increasingly drawing on theories and methods in neighboring disciplines to gain a more complete picture of cooperation in social dilemmas as an example research ers are now using evolutionary theory to understand self presen tation motives for cooperating in social dilemmas e g hardy van vugt see also griskevicius tybur van den bergh neuroimaging techniques to better understand altruistic punishment in social dilemmas e g de quervain et al singer et al and insights from ﬁeld studies to raise impor tant basic questions about the evolution of institutions and sanc tioning systems that can promote cooperation e g kollock yamagishi indeed as we will see there has been an explosion of research on the effectiveness of reward and pun ishment in promoting cooperation much of it revolving around the evolution of reward and punishment the automaticity of pun ishment the framing of reward and punishment the notion of antisocial punishment i e punishing cooperators rather than non cooperators the impact of moral appeal vs sanctioning the role of social norms and the role of culture in shaping reward and punish ment e g chen pillutla yao egas riedl gächter herrmann in sum researchers are beginning to seriously address the call for more interdisciplinary collaboration on social dilemmas ecologically valid research in the longstanding history of social dilemmas researchers the orists and critics alike have often provided the suggestion to en hance the ecological validity of their paradigms see also komorita parks as we will see researchers have heeded the call by developing novel social dilemma paradigms that help mirror many of the core features of real world social dilemmas and conducting more research in the ﬁeld expanded paradigms first complementing the classic prisoner dilemma commons dilemma and public goods dilemma researchers have begun to use a greater variety of games to provide insight into social dilem mas such as the ultimatum bargaining game the dictator game the trust game and decomposed games while not social dilemmas per se these simple yet elegant games get to the heart of many is sues central to decision making in social dilemmas including fair ness ultimatum bargaining game giving and altruism dictator game willingness to trust trust game and various social value orientations decomposed game beyond these games recent research has also begun to explore alternative but related paradigms like the give or take some di lemma mccarter et al and the anti commons dilemma e g vanneste van hiel parisi depoorter the give or take some dilemma represents a hybrid between the public good dilemma and the resource dilemma modeling a situation where group members ﬁrst provide the public good from which they sub sequently harvest the anti commons dilemma by comparison is the mirror image of the resource dilemma and depicts situations where the problem lies in the underutilization of resources when individuals can exclude others from using a resource see also hel ler heller eisenberg research on these new para digms is still scarce but the ﬁrst ﬁndings suggest that they may evoke different behaviors the anti commons dilemma for exam ple appears to evoke much lower levels of cooperation than its mirror image the resource dilemma vanneste et al another topic receiving increased attention is cooperation in multiple group settings because in reality people often face dilemmas in which they may belong to different groups for exam ple a soldier at war is a member of his or her country while ﬁghting members of another group here in group cooperation may be quite high ﬁghting hard due to a strong orientation to ward one own group for example patriotism which in turn leads to greater intergroup hostility and warfare as another exam ple in the context of environmental dilemmas older generations are often asked to sacriﬁce for younger generations who will even tually inherit the planet such settings evoke speciﬁc dynamics which have led researchers to design and study new interdepen dence structures like the intergroup dilemma e g or team games bornstein bornstein ben yossef halevy bornstein sagiv the nested dilemma wit kerr and the intergenerational dilemma wade benzoni hernandez medvec messick wade benzoni tenbrunsel bazerman the intergroup dilemma models situations in which two groups are in conﬂict individuals of each group can contribute to win from the other group but at the same time face a social dilemma in their own group making it more advantageous for them not to contrib ute in the nested dilemma individuals are members of subgroups which in turn are part of a superordinate group similar to employ ees being members of departments within a large organization in the intergenerational dilemma individuals are member of a group in which they can harvest from a scarce resource knowing that at a later point in time their group will be succeeded by a next gener ation here the issue is about the willingness to share with future generations the introduction of multiple groups not only increases the external validity of social dilemmas it also generates new in sights for example by showing that within group cooperation may instigate intergroup conﬂict in the intergroup dilemma that categorization at the subgroup level may be detrimental for the collective in a nested dilemma and that intergenerational deci sions may be subject to egocentrism applied research complementing the development of new experimental para digms an increasing number of scientists is also examining social dilemmas as they occur in everyday life for example as already mentioned ostrom and her colleagues have published numerous books and articles on how real groups of constituents arrange sys tems for successfully managing common pool resources research ers have also been actively applying social dilemma insights to understand cooperative behavior in organizations e g organiza tional citizenship behaviors joireman et al consumer behavior e g sen gurhan canli morwitz collective ac tion voting and political behavior e g klandermans van lange bekkers chirumbolo leone proenvironmental behavior e g joireman kortenkamp moore and commuting decisions joireman et al van vugt et al applied research has also recently explored structural solu tions to real world dilemmas such as the impact of private meter ing on water conservation van vugt samuelson reactance against the ﬁrst carpool lane in europe van vugt van lange meertens joireman and support for the develop ment and implementation of public transportation systems e g joireman lasane bennett richards solaimani in summary over the past two decades we have seen a strong growth in the development of new game situations that do more justice to some basic features of social dilemmas such as the com mons dilemma the anti commons dilemma and dilemmas which specify interdependence among different groups of collectives such as team games these situations are of great theoretical interest and also enhance opportunities for addressing key issues in society such as the underuse of resources sometimes resulting in waste of resources or patterns of intergroup conﬂict the growth in ﬁeld studies might reinforce some of the lab based con clusions and often serve as powerful demonstrations of what might happen or as reminders of hardin tragedy of the com mons and these studies serve a heuristic function for theoretical ideas or practical obstacles that might go otherwise unnoticed or less noticed developments in structural psychological and dynamic inﬂuences as outlined earlier interdependence theory assumes that choice behavior in interdependent settings is a combined function of structural inﬂuences e g features of the decision and or social sit uation psychological inﬂuences e g internal motives framing re cently primed schemas or affect and dynamic interaction processes e g how certain individuals respond to a tit for tat strategy or whether forgiveness or retaliation will predominate when others do not cooperate we adopt this framework for discussing some recent programs of research on social dilemmas we ﬁrst discuss structural inﬂuences by reviewing research on rewards and pun ishments asymmetries between decision makers and uncertainty over various aspects of the social dilemma decision in subsequent sections we review recent research on psychological inﬂuences e g individual differences and dynamic interaction processes e g reciprocal strategies structural inﬂuences rewards punishment and the social death penalty it has long been known that the objective payoffs facing deci sion makers i e the given payoff structure can have a large im pact on cooperation in social dilemmas e g komorita parks rapoport those payoffs in turn may be determined by an experimenter e g by presenting relatively low or high levels of fear and greed or by the actual outcomes afforded by the situ ation e g the cost of contributing to a public good vs the value of consuming the good in terms of the situation another factor that has a large impact on the actual or anticipated payoffs in a social dilemma is the presence of rewards for cooperation and punish ment for non cooperation indeed a recent meta analysis showed that rewards and punishments both have moderate positive effects on cooperation in social dilemmas balliet li et al balliet li macfarlan et al balliet mulder et al administer ing rewards and punishments is costly however and may thereby create a second order public good for example sanctions may be good for the collective but individuals may decide not to contrib ute money or effort for this purpose in his classic work yamagishi showed that people are willing to make such contributions if they share the goal of cooperation but do not trust others to voluntarily cooperate more recently fehr and gächter showed that people are also often willing to engage in costly punishment and may even prefer institutions that pro vide the possibility of such sanctions perhaps in part because the possibility of costly punishment can help to install a norm of cooperation gürerk irlenbusch rockenbach one of the most dramatic forms of punishment currently receiv ing attention is ostracism or social exclusion research on ostracism and social exclusion reveals that even the possibility of social exclusion is a powerful tool to increase cooperation and that this threat might be more effective in small as opposed to large groups e g cinyabuguma page putterman kerr et al ouwerkerk kerr gallucci van lange moreover it ap pears that most people realize that harmful pursuit of self interest can lead to social punishments see gächter herrmann thöni as noted by kerr et al in everyday life small groups may not often go as far as to socially exclude people but the threat is often there especially in the form of social marginalization by paying less attention to non cooperative members or involving them in somewhat less important group decisions although punishments can be effective in promoting coopera tion some adverse effects have been documented in recent re search for example several studies have shown that sanctions can decrease rather than increase cooperation especially if the sanctions are relatively low e g gneezy rustichini mul der van dijk de cremer wilke tenbrunsel messick one explanation for these adverse effects is that punish ments may undermine people internal motivation to cooperate cf deci koestner ryan according to tenbrunsel and messick sanctions can also lead people to interpret the so cial dilemma as a business decision as opposed to an ethical deci sion thus reducing cooperation researchers are now also documenting that groups may at times punish cooperators a somewhat counterintuitive phenom enon known as antisocial punishment gächter herrmann herrmann thöni gächter in one of the most recent pa pers on this topic parks and stone found across several studies that group members indicated a strong desire to expel an other group member who contributed a large amount to the provi sion of a public good and later consumed little of the good i e an unselﬁsh member last but not least there is also growing evi dence suggesting that punishment might be most effective when it is administered in a decentralized manner by fellow members rather than in a centralized manner by an authority for some tentative evidence see balliet li et al balliet li macfarlan et al balliet mulder et al nosenzo sefton asymmetries in resources beneﬁts and roles another popular topic in social dilemmas is the role of asym metries in most early social dilemma studies group members were symmetric in that they each possessed an equal number of endowments that they could contribute to a public good and or could each beneﬁt equally from public goods and collective re sources moreover group members typically made their decisions simultaneously rather than sequentially and frequently made their decision without reference to speciﬁc roles in a group such as whether one is a leader or a follower while such symmetries help simplify the dilemma in real life various types of asymmetry are more prevalent recognizing this researchers are now explor ing how such asymmetries impact choice behavior in social dilemmas for example research has shown that those who are wealthier and those who beneﬁt more from a well functioning public good behave more cooperatively e g marwell ames van dijk wilke but see rapoport these differences partly reﬂect differences in the relative costs of contributing e g contributing a certain amount of money may be less risky for the less wealthy but they may also connect to feelings of fairness e g people consider it fair if the wealthy contribute more than the less fortunate moreover in step level situations asymmetries are often used as a tacit coordination device e g by deciding to contribute in proportion to the number of endowments one pos sess yet this only works if people tacitly agree on which tacit coordination rule to apply van dijk de kwaadsteniet de cre mer and of course group members do not always agree indeed in some cases people may have self serving ideas on what would be fair or reasonable especially when people face mul tiple types of asymmetry messick sentis wade benzoni et al in short resource asymmetries can have a large im pact on cooperation in social dilemmas another asymmetry that can impact cooperation in social dilemmas revolves around the role one assumes within the group for example de cremer and colleagues have shown that leaders take more of a common resource than followers in large part be cause leaders feel more entitled to behave selﬁshly de cremer van dijk interestingly the tendency for leaders to take more than followers is stronger when the leader has a proself value orientation de cremer van dijk and when there is a high degree of variability among group members harvests stouten de cremer van dijk uncertainty in most social dilemma experiments the characteristics of the dilemma have been known with certainty to all group members for example in resource dilemmas participants are usually in formed about the exact size of the resource the exact replenish ment rate and the number of participants similarly in public goods dilemmas participants are often aware of the exact thresh old required to provide the public good or the function linking contributions to beneﬁts in a continuous public good in real life however such deﬁning characteristics are not always clear as peo ple often face various types of environmental uncertainty messick allison samuelson suleiman rapoport this uncertainty in turn has been shown to reduce willingness to coop erate in various social dilemmas e g budescu rapoport sulei man gustafsson biel gärling and several explanations have been offered to account for the detrimental ef fects of uncertainty for example uncertainty may undermine efﬁ cient coordination de kwaadsteniet van dijk wit de cremer van dijk et al lead people to be overly optimistic regarding the size of a resource gustafsson et al and or provide a justiﬁcation for non cooperative behavior for a review see van dijk wit wilke budescu also uncertainty undermines cooperation when people believe their behavior is quite critical for the realization of public goods but when critical ity is low uncertainty matters less or may even slightly promote cooperation chen au komorita future research may well identify other crucial moderators of this uncertainty effect noise one ﬁnal structural factor that has received attention in recent years is the concept of noise in many experimental social dilem mas there is a clear connection between one intended level of cooperation and the actual level of cooperation communicated to one partner e g if partner a decides to give partner b coins partner b learns that partner a gave coins however in the real world it is not uncommon for a decision maker actual level coop eration to be positively or negatively impacted by factors outside of his or her control i e noise while positive noise is possible i e cooperation is higher than intended the majority of research has focused on the detrimental effects of negative noise i e when cooperation is lower than intended this research clearly has shown that negative noise reduces cooperation in give some games van lange ouwerkerk tazelaar and willingness to man age a common resource responsibly especially among prosocials faced with a diminishing resource brucks van lange moreover the adverse consequences of negative noise can spill over into subsequent dilemmas that contain no noise brucks van lange while noise can clearly undermine cooperation several studies also suggest it can be overcome for example if the partner pursues a strategy that is slightly more generous than a strict tit for tat strategy e g tit for tat klapwijk van lange van lange et al when people are given an opportu nity to communicate tazelaar van lange ouwerkerk and when people are encouraged to be empathetic rumble van lange parks in summary structural inﬂuences center on key differences in the interdependence structure of the social dilemma such that outcomes linked to cooperation can be improved through reward and outcomes linked to non cooperation through punishment with exclusion representing a strong form of punishment the ef fects of structural differences often go beyond material outcomes and elicit a rich psychology involving neuroscientiﬁc cognitive and emotional processes asymmetries and roles are important determinants of behavior in social dilemma yet understudied especially when looking at social dilemmas in everyday life where asymmetries and roles seem the rule and not the exception uncer tainty and noise are also omnipresent in everyday life and they may shape the psychology in many ways in that they may chal lenge trust feelings of control and perhaps sometimes give rise to judgments and heuristics that are predictably inaccurate such as unrealistic optimism regarding the state of affairs such as size of the pool or unrealistic pessimism regarding other willingness to cooperate psychological inﬂuences advances have also been made in understanding how a variety of psychological variables impact cooperation in social dilemmas in this section we focus on four categories of psychological vari ables including individual differences decision framing priming and affect social value orientation a long history of social dilemma research makes clear that peo ple differ in fundamental ways in how they approach and interact in social dilemmas the personality variable that has received the lion share of the attention is social value orientation messick mcclintock van lange although svo has long been recognized as a predictor of social dilemma cognition and behavior e g kelley stahelski kuhlman marshello researchers continue to gain deeper insights into its origin e g van lange otten de bruin joireman measurement e g eek garling murphy ackerman handgraaf and inﬂuence on cognition and behavior in lab and ﬁeld studies as noted earlier several excellent reviews of the svo liter ature have recently been published e g au kwong balli et parks joireman bogaert boone declerck van lange et al nevertheless a number of recent advances are worth noting first whereas researchers have often deﬁned a prosocial value orientation in terms of a desire to maximize joint outcomes it is becoming increasingly clear that prosocials are also very concerned with maximizing equality for example in his integrative model of social value orientation van lange suggests that the desire to maximize joint gain and equality are positively correlated and that prosocials pursue both goals cf de cremer van lange while individualists and competitors pursue neither more recent evidence supports the claim that equality in outcomes is the primary concern among prosocials eek garling con sistent with the argument that prosocials consider equality an important principle research shows that prosocials are more likely than individualists and competitors to a use an equal split is fair rule in negotiation settings de dreu boles b re spond with a high degree of anger to violations of equality regard less of how such violations impact their own outcomes whereas individualists and competitors only respond to violations of equal ity when such violations harm their own outcomes stouten et al and c show a high degree of activity in the amygdala when evaluating unequal distributions of outcomes haruno frith taken together these ﬁndings suggest that a concern with equality is very strongly linked to how prosocials approach social dilemmas how they respond to others who might violate equality and what makes them distinctively different from individualists and competitors it is also plausible that because of their concern with equality prosocials might feel strongly about restoring justice in the world e g joireman duell and gravitate to political parties that emphasize not only solidarity but also egalitarianism e g van lange et al second researchers continue to ﬁnd evidence for the ecological validity of svo as an example research has shown that relative to individualists and competitors prosocials are more willing to do nate to help the ill and the poor but not the local sports club and volunteer as participants in psychology experiments e g mcclintock allison van lange schippers balliet exhibit citizenship behavior in organizations nauta de dreu van der vaart engage in proenvironmental behavior cameron brown chapman joireman et al ex press stronger preferences for public transportation van vugt et al coordinate i e sync their behavior with an interac tion partner lumsden miles richardson smith macrae and be perceived as cooperative based on their non verbal behav ior shelley page rives yeagley kuhlman in short since the publication of komorita and parks book an impressive number of studies have been published supporting the real world impact of svo trust another variable closely linked to cooperation is trust accord ing to one of the most accepted deﬁnitions trust is a psychologi cal state comprising the intention to accept vulnerability based upon the positive expectations of the intentions or behavior of an other rousseau sitkin burt camerer p as such trust involves vulnerability that is the uncertainty and risk that comes with the control another person has over one outcomes and positive expectations which often imply a set of beliefs in the cooperative intentions or behavior of another person or people in general rotter see also evans krueger early work on trust in social dilemmas showed that those high in dispo sitional trust were more likely than those low in trust to increase cooperation in response to a partner stated intention to cooperate parks henager scamahorn reduce consumption of a depleting common messick et al and contribute to public goods parks yamagishi since these initial studies a number of important insights regarding trust and cooperation have emerged first research sug gests that people who are not very trusting of others are not nec essarily noncooperative in a motivational sense rather they are simply prone to believe that others will not cooperate and that fear undermines their own elementary cooperation however when given the chance to contribute to a sanctioning system that punishes noncooperators low trusters are actually quite cooper ative in other words they appear quite willing to engage in instrumental cooperation by contributing to an outcome structure that makes it for everybody including those with selﬁsh motives attractive to cooperate or unattractive to not cooperate yamagi shi for earlier evidence see yamagishi second trust matters more when people lack information about other people intentions or behavior or when they are faced with considerable uncertainty see yamagish an interesting case in point is provided by tazelaar et al who as mentioned earlier found that levels of cooperation are much lower when peo ple face a social dilemma with noise more interesting they found that this detrimental effect of noise was more pronounced for peo ple with low trust than for people with high trust tazelaar et al study third based on a recent meta analysis it is clear that trust mat ters most when there is a high degree of conﬂict between one own and others outcomes balliet van lange in press a cf parks hulbert this ﬁnding makes sense as these are the situa tions involving the greatest degree of vulnerability as trusting oth ers to act in the collective interest can be quite costly in such situations indeed as noted earlier trust is in many ways about the intention to accept vulnerability based upon positive expecta tions of the intentions or behavior of another rousseau et al see also evans krueger consideration of future consequences a ﬁnal trait relevant to cooperation in social dilemmas is the consideration of future consequences deﬁned as the extent to which people consider the potential distant outcomes of their cur rent behaviors and the extent to which they are inﬂuenced by these potential outcomes strathman gleicher boninger ed wards p cf joireman shaffer balliet strathman in press several studies have shown that individuals high in cfc are more likely than those low in cfc to cooperate in experimen tally created social dilemmas e g joireman posey barnes true love parks kortenkamp moore and real world dilemmas for example by engaging in proenvironmental behavior e g joireman et al strathman et al commuting by public transportation e g joireman et al and supporting structural solutions to transportation problems if the solution will reduce pollution joireman et al other individual differences a number of additional individual differences have received attention in recent dilemmas research this research has shown for example that cooperation in social dilemmas is higher among those low in narcissism campbell bush brunell low in dispositional envy parks rumble posey low in extraver sion and high in agreeableness koole jager van den berg vlek hofstee high in intrinsic orientation sheldon mcgregor high in sensation seeking and self monitoring boone brab ander van witteloostuijn and high in the need to belong assuming the group is large de cremer leonardelli decision framing the psychological framing of social dilemmas has also re ceived a fair amount of recent attention for example in general emphasizing the acquisitive aspect of the dilemma you can gain something from the task leads people to be less cooperative than emphasizing the supportive aspect of the dilemma you can con tribute toward a common good kramer brewer simi larly cooperation is lower when decision makers view the social dilemma as a business decision rather than an ethical decision tenbrunsel messick or a social decision liberman sam uels ross pillutla chen framing the dilemma as a public goods vs a commons can also impact cooperation but as de dreu and mccusker show the direction of such framing effects seems to depend on the instructions given and the decision maker svo to summarize cooperation rates are lower in give some than in take some dilemmas when instructions to the dilem ma emphasize individual gain or decision makers have an individ ualistic value orientation whereas cooperation is higher in give some than in take some games when instructions emphasize col lective outcomes or decision makers have a prosocial value orien tation in general group members are more concerned to distribute outcomes equally over group members in the take some dilemma than in the give some dilemma van dijk wilke fi nally research has also shown that cooperation decreases if people come to believe they have been doing better than expected and in creases if people believe they have been doing worse than ex pected parks sanna posey priming another question that has received some attention is whether it is possible to induce cooperation through subtle cues and sugges tions the answer is generally yes though the dynamics of prim ing cooperation are surprisingly complex and it is not clear whether they exert very strong effects but some effects are worth mentioning for example priming an interdependent mindset effectively promotes cooperation utz but if the person has a prosocial orientation it is better to prime a self mindset which can activate their existing prosocial values utz similarly prosocials show increased cooperation when encouraged to think about smart behavior whereas such smart primes will just make proselfs more selﬁsh utz ouwerkerk van lange heuristics like priming the application of decision heuristics to social di lemma choice has received relatively little attention yet the work on heuristics that has been done is quite revealing a small amount of this work has looked at the value of heuristics for directing behavior in large scale social dilemmas messick liebrand parks komorita the primary focus however has been on an equality heuristic or norm under which people choose with an eye toward making sure everyone has the same experience in resource consumption type tasks the equality heu ristic is oriented around everyone receiving the same amount of the resource people tend to anchor on it and then adjust their choices in a self serving direction allison mcqueen schaerﬂ allison messick roch lane samuelson allison dent when the dilemma involves contribution equality is oriented around everyone giving the same amount though the motivator of this heuristic is not constant sometimes equality is used to emphasize fairness in that all should give but at other times it is used to emphasize efﬁciency in that everybody giving the same amount is the easiest way to achieve the goal stouten de cremer van dijk stouten et al further along this line some theorists have argued that in mixed motive situations most decision heuristics are employed in order to max imize the likelihood of engaging in fair behavior on the assump tion that coming across as fair conveys to others that one is trustworthy lind affect the inﬂuence of affect on decision making is another topic of current prominence within the ﬁeld of social dilemmas here re search has focused on both general mood states and speciﬁc emo tions regarding mood a clear pattern that emerges is that a positive mood is not necessarily beneﬁcial for encouraging cooper ation for example a positive mood can lead people to infer that they have been sufﬁciently supportive of the group and they are now at liberty to choose however they wish e g hertel fiedler it may also be that a positive mood leads people to focus more on internal states which would heighten selﬁshness while negative moods lead to an external focus which would heighten cooperation tan forgas these ﬁndings are consistent with the emerging notion that happiness is not always a useful mood state to induce gruber mauss tamir and raises the interesting notion that it could be beneﬁcial to make social di lemma participants feel bad in some way about the situation along these lines it has been shown that those who feel badly about their choices in a social dilemma will become more cooper ative in subsequent dilemmas even if there is a considerable time lag between the initial and subsequent dilemmas ketelaar au this immediately raises the question of whether it would mat ter which speciﬁc negative emotion was induced for example would it be irrelevant whether a person felt mad or sad so long as the feeling was negative for that matter might there be other speciﬁc emotions that come into play when choosing in a social di lemma in fact there is evidence that cooperation is connected with a range of negative emotions including envy parks et al guilt e g nelissen et al shame e g de hooge breugelmans zeelenberg regret martinez zeelenberg rijsman anger and disappointment e g wubben de cre mer van dijk with most acting as stimulators of cooperation on a related note a more recent line of research has focused on how cooperation is impacted when one partner communicates certain emotions for example research shows that when one partner is not really in a position to retaliate people are more cooperative when their partner appears happy but if one partner can retaliate people are more cooperative when their partner ex presses anger van dijk van kleef steinel van beest such research shows that communicated emotions are often inter preted as a signal that informs us how another person might re spond to our non cooperative and cooperative behavior e g van kleef de dreu manstead indeed research also shows that cooperators are more likely than individualists and competi tors to smile when discussing even mundane aspects of their day and that cooperators individualists and competitors can be identiﬁed simply on the basis of their non verbal behavior shelley et al in summary personality differences in social values trust con sideration of future consequences framing priming heuristics and affect represent a long list of variables that are important to understanding the psychological processes that are activated in so cial dilemmas presumably personality inﬂuences might be more stable over time and generalizable across situations than some other more subtle inﬂuences such as framing priming and affect the stable and subtle inﬂuences are both important as they provide the bigger picture of what the social dilemmas might chal lenge in people in different people and how some of these chal lenges might be inﬂuenced in implicit ways the effect sizes of framing and especially priming may sometimes be somewhat modest yet the effects tend to be fairly robust and therefore they help us understand how cooperation could perhaps be promoted in cost effective ways such as by just activating a particular psycho logical state or mindset in the ways social dilemmas are communi cated and presented dynamic interaction processes in the preceding sections we focused mainly on how features of the decision situation and person inﬂuence the decision to coop erate at a given point in time while some of these variables could be viewed as having a dynamic component e g the impact of rewards and punishments on cooperation most of the variables were static in the sense that they did not typically concern how a decision maker faced with a social dilemma actively responds to changes in his or her environment over time sometimes this means that personality differences are expressed in how people re spond to others over time e g how an individualist might respond to a tit for tat strategy kuhlman marshello or that per sonality differences become weaker and that most people respond strongly to information about others behavior in a group as it un folds over time e g the number of noncooperators in a group chen bachrach in the present section we consider several promising lines of research addressing on going interaction pro cesses within the context of social dilemmas by examining what happens after group members have made their choices learned of others choices and must make a subsequent choice speciﬁcally we consider recent work on reciprocal strategies generosity in the context of misunderstandings or noise locomotion and support for structural solutions to social dilemmas direct reciprocity there is a long tradition of research on how different reciprocal strategies e g unconditionally cooperative unconditionally non cooperative or conditionally cooperative impact cooperation in social dilemmas e g komorita parks hulbert the well established ﬁnding is that the tit for tat tft strategy start cooperative and then respond in kind to the partner actions is the most effective strategy if one is motivated pursue joint welfare as well as own welfare axelrod the effectiveness of the other strategy however has been shown to depend on an indi vidual social value orientation for example in their classic work kuhlman and marshello had cooperators individualists and competitors play trials of a person prisoner dilemma game against one of three pre programmed strategies cooperative tft non cooperative kuhlman and marshello found that cooperators showed high levels of cooperation unless their partner always chose to behave non cooperatively competitors showed low levels of cooperation regardless of their partner strategy and individualists showed high levels of cooperation only when paired with a partner pursuing a tft strategy for many years these ﬁndings led to the conclusion that a tft was always the best strategy for eliciting cooperation b that an unconditionally cooperative strategy was sure to be exploited and c that individ ualists but not competitors could be taught to cooperate when they came to understand it was in their own best interest recent research however has called into question each of these conclusions for example van lange et al have shown that in situations involving negative noise i e when one cooperation level is not as high as it was intended tft is actually less effective at eliciting cooperation than a more generous strategy in which one responds in a slightly more cooperative manner than one partner did on the previous trial e g tft one explanation for this ﬁnding is that when one partner adopts a generous reci procal strategy it encourages one to maintain the impression that one partner has benign intentions and can be trusted see also klapwijk van lange second arguing against the inevita ble exploitation of unconditional cooperators weber and murni ghan showed that consistent cooperators can effectively encourage cooperation in social dilemmas often ultimately pro moting their own long term best interests third whereas it was long assumed that competitors could not learn to cooperate shel don showed that when given enough time competitors in crease their level of cooperation in response to a tit for tat strategy finally parks and rumble showed that the timing of rewards and punishments matters whereas prosocials are most likely to cooperate when their cooperation is immediately recipro cated competitors are most likely to cooperate when punishment for non cooperation is delayed in sum recent research has shed new light on how reciprocal strategies can promote cooperation indirect reciprocity recent research has also explored how indirect reciprocity can encourage cooperation whereas the effects of direct reciprocity are observed in repeated encounters between two individuals cooperation in larger settings may be promoted by indirect reci procity according to this view cooperation may be advantageous because we tend to help people who have helped others in the past as noted earlier and brieﬂy illustrated by the experiment of wedekind and milinski indirect reciprocity models build on reputation effects by assuming that people may gain a positive reputation if they cooperate and a negative reputation if they do not indeed people are more likely to cooperate with others who donated to a charity fund like unicef milinski semmann krambeck notably people also seem to be well aware of these positive effects as they are more willing to donate and coop erate if they feel their reputation will be known by others than if they feel others are not aware of their contributions e g griskev icius et al there is even evidence indicating that subtle cues of being watched by means of an image of pair of eyes can enhance donations bateson nettle roberts which suggest the subtle power of reputational mechanisms locomotion typically experimental research on multi trial social dilemmas has explored how people respond to a given partner or group however in the real world one is not inevitably stuck with cer tain partners one can exit relationships and groups and enter oth ers recognizing exit and selection and exclusion of new partners as viable options in social dilemmas a number of recent studies have begun to study locomotion and changes in group composition in social dilemmas for example van lange and visser showed that people minimize interdependence with others who have exploited them and that competitors minimize interdepen dence with others who pursue tft which is understandable as competitors cannot effectively achieve greater relative outcomes with a partner pursuing tft similarly it is clear that conﬂict with in a group may induce people to leave their group eventually lead ing to group ﬁssions hart van vugt the conﬂict may come from failure to establish cooperation in the group or a decline in cooperation as cooperative members exit yamagishi van lange visser see also de cremer van dijk or from dissatisfaction with autocratic leadership van vugt jep son hart de cremer conversely prospects of coopera tion may encourage individuals to enter groups for example when sanctions of non cooperation promote the expectation of cooperation see gürerk et al communication frequently communication is conceptualized as a psychological variable after all communication is often conceptualized in terms of verbal or non verbal messages that are characterized by a fair amount of interpretation and subjectivity in the social dilemma literature various forms of communication have been compared classic research on social dilemma has shown that communication can effectively promote cooperation see komorita parks but it is not just cheap talk that explains why communication might promote cooperation even though face to face interaction by itself may be helpful to simply to talk about issues that are not in any way relevant to the social dilemma does not seem to promote cooperation dawes mctavish shaklee some researchers have suggested and found that at least in single trial social dilemmas promising to make a cooperative choice may be quite effective but only if all group members make such a promise orbell et al subsequent research supported this line of reasoning in that communication with pledge promotes cooperation because it promotes a sense of group identity and a belief that one choice matters i e that one choice is believed to be critical chen these ﬁndings are important not only because they inform us about the psychology of decision making in social dilemmas but also how they might help us explain the dynamics of cooperation moreover in real life social dilemmas group members may actually decide whether they favor a struc ture in which they openly communicate their intended choices for example as noted by chen in work groups managers could ask to make a pledge of time and effort and then propose several binding pledge systems especially those that are group based such that they create a common fate whereby they serve as normative standards for everybody involved communication may strengthen a sense of identity but it also promotes a norm of generalized reciprocity which is why it might speak to similar mechanisms as those that dynamically underlie the effects of di rect and indirect reciprocity support for structural solutions one ﬁnal issue being addressed concerns structural solutions to social dilemmas which involve changing the decision making authority e g by electing a leader rules for accessing the com mon resource or the incentive structure facing decision makers e g by making the cooperative response more attractive in the lab the most heavily studied structural solution has been the elec tion of a leader many early studies showed that people were more likely to elect a leader when the group had failed to achieve opti mal outcomes in a social dilemma e g underprovided a public good or overused a common resource messick et al van vugt de cremer additional research shows that after a group has failed willingness to elect a leader tends to be lower in commons dilemmas as opposed to public goods dilemmas e g van dijk wilke wit when collective failure is be lieved to be the result of task difﬁculty as opposed to greed sam uelson and among those with a prosocial vs a proself orientation de cremer samuelson research com paring different leadership alternatives shows that group members are more likely to support democratic vs autocratic leaders and to stay in groups led by democratic vs autocratic leaders van vugt et al finally a new and promising line of research on leadership and cooperation introduces evolutionary concepts this research for example has revealed that competition within groups may increase the preference for female leadership whereas intergroup competition may increase preferences for male leader ship van vugt spisak beyond the lab a number of ﬁeld studies have also explored sup port for structural solutions many building off of samuelson multiattribute evaluation model samuelson proposed that decision makers evaluate structural solutions in terms of efﬁciency self interest fairness and freedom and that the importance of the four dimensions varying as a function of individual differences e g in social value orientation or consideration of future consequences samuelson model has received support in several ﬁeld studies exploring support for improvements in public transportation e g joireman van lange et al field research on structural solu tions has also explored the impact of private metering during a water shortage van vugt samuelson and reaction against the ﬁrst carpool lane in europe van vugt et al finally as noted earlier research on structural solutions to social dilemmas has been greatly advanced by ostrom and her colleagues who have studied the development of institutions designed to manage com mon pool resources e g ostrom ostrom gardner walker the broad conclusion reach by ostrom and colleagues is that local management of small communities and the enhancement and maintenance of trust in these communities is essential for both the communities and the broader collective or as ostrom and ahn stated the very condition for a successful market economy and democracy is that a vast number of people relate in a trustwor thy manner when dealing with others to achieve collective actions of various scales p in summary it is one thing to predict and explain how people might behave in relatively static situations such as social dilem mas without repeated interaction it is quite another thing to pre dict and explain dynamic interaction patterns while classic research has emphasized reciprocity such as tit for tat as a func tional strategy promoting cooperative interaction more recent re search suggests that it is functional to add a bit of generosity one reason is that generosity helps to maintain or promote trust which in turn is a key ingredient to cooperation further when social dilemmas do not elicit sufﬁcient cooperation we see that people exhibit a greater willingness to support several solutions including the option of communication with binding elements such as pledges and the structural solution of electing a leader in doing so they tend to support democratic leadership over autocratic leadership together feelings of trust criticality and we ness such as the feeling we are in this together seem essential for small communities to productively approach and resolve social dilemmas they may not only underlie cooperation but also why participants contribute to dynamic interaction patterns and struc tural changes in social dilemmas and why such instrumental con tributions are effective in promoting cooperation prospects for the future of social dilemmas looking back researchers have made signiﬁcant progress in theory development applied and interdisciplinary research and in understanding the impact of structural psychological and dynamic factors on cooperation moreover on the whole we see increased attention to paradigms and issues more closely approx imating real world dilemmas e g paradigms that recognize asymmetries noise structural solutions in sum the ﬁeld has made signiﬁcant and exciting advances over the past years yielding valuable insights into the dynamics of cooperation in a variety of social dilemmas we should admit that our review has not been comprehensive in that important literatures on social dilemmas and human cooperation have not been addressed we are thinking of seminal papers by anthropologists evolutionary scientists experimental economists mathematicians political sci entists and theoretical biologists the most important reason for this is limitations in terms of space and admittedly time but the important point to be made is that by focusing on the psychol ogy of social dilemmas we are underestimating the diversity in conceptual approach interdisciplinary research and methodologi cal paradigms and all signs suggest that this diversity will con tinue and expand in the next decades looking ahead we see several promising directions for future research at the broadest level we believe the ﬁeld would beneﬁt from continued attention to developing an overarching theoretical framework earlier we reviewed interdependence theory and evo lutionary theory as relatively broad theoretical frameworks these frameworks share a number of meaningful connections broadly conceived by its focus on the analysis of situational structure interdependence theory is an ideal position to start our conceptual analysis the same could be argued for game theory but interde pendence theory has the advantage of providing a relatively coher ent framework in which the conceptual links among situations are delineated by providing a taxonomy of dimensions including situ ational dimensions such as degree of dependence degree of con ﬂicting interest information availability and time horizon as key dimensions e g kelley et al van lange rusbult this taxonomy helps us understand the game read situation peo ple are facing and the problems or opportunities that the game again read situation affords this interdependence based analy sis not only provides key insights into the structure of situation what is the situation about it also suggests the broad relevance of our own interaction goals are we cooperative or not and those we attribute to others in a global or concrete manner are other people cooperative or not the latter attributions or beliefs are of course closely linked to trust evolutionary theory provides a broad framework for under standing the ultimate mechanisms relevant to trust and cooper ation and psychological theory including the appropriateness framework should help us understand the proximal mechanisms relevant to trust and cooperation to illustrate interdependence theory and game theory suggests the importance of incomplete information in social dilemmas deﬁned by a conﬂict of self inter est and collective interest incomplete information begs trust did the other intentionally help or harm the collective interest evo lutionary this is important because it challenges the ways in which cooperation may be evolved for example it may help us under stand why giving strangers the beneﬁt of doubt has functional and survival value even more it may help us understand the roots of generosity nowak sigmund proximally giving others the beneﬁt of doubt especially when accompanied by the communication of generosity will enhance trust the other has in your intentions which in turn is crucial for coping with uncer tainty and incomplete information van lange et al we are truly looking forward to a fruitful and comprehensive integra tion of adaption to structure the game we play the psychological and interpersonal processes involved what we make of the game and the ultimate functions it serves in terms of psychological eco nomic and evolutionary outcomes such integrative theorizing has clear potential in understanding empirical and interdisciplinary research on uncertainty noise social exclusion and sanctions we also believe the ﬁeld would beneﬁt by devoting increased attention to structural solutions to social dilemmas as these solutions seem to hold the greatest po tential for encouraging cooperation in the many wide scale dilem mas we face arguably one of the most important dilemmas we face is the problem of global warming unfortunately international attempts to raise support for a structural solution to this dilemma have encountered challenges given its complexity solving the di lemma of global warming will inevitably require teams of scien tists who bring strong theory valid methods and a willingness to approach the problem from an interdisciplinary perspective from our perspective social dilemma researchers are clearly poised to contribute to that effort science is about ﬁnding the truth general knowledge progress and innovation and applicable knowledge van lange this is what makes science so exciting the science of social dilemmas makes it even more exciting because it addresses the basic ques tion of human nature the selﬁsh and prosocial aspects of human kind and because we often face a reality in which we experience social dilemmas on a weekly or even daily basis imaginary or real people often ﬁnd themselves in situations that have much in com mon with social dilemmas with strangers with colleagues with friends with close partners these social interactions can be quite challenging and sometimes even puzzling why did she do that to me how do we deal with strangers do we trust them does our image or reputation matter and on a larger scale newspapers are often addressing issues of scarcity e g the risk of depleting speciﬁc ﬁsh species greed the excessive pursuit of self interest e g incentives for the executive ofﬁcers in the ﬁnancial sector or difﬁculties in establishing contractual agreements among coun tries for maintaining a healthy levels of environmental quality we acknowledged already the relevance of applicable knowl edge one broad lesson that one might infer from the social dilem ma literature is that often it is the combination of measures rather than their isolated effects that effectively promote cooper ation for example authorities are often associated with structural solutions such as sanctioning free riding and rewarding coopera tive action and trust is often associated with interpersonal rela tions at least in psychology but like horizontal trust among people vertical trust between people and institutes institutional trust is crucial for the acceptance of rewards and punishment above and beyond outcomes in a narrow sense people want to be treated fairly and respectfully for example a local govern ment who listens to the concerns that people may have and pro vides accurate information in a transparent manner might often not only enhance vertical trust but also a stronger commitment and willingness among people to make a positive contribution to urgent social dilemmas a case in point is tyler and degoey research on the water shortage in california which demonstrated that people exercised more constraint on their water consumption it they felt treated more fairly by the authorities likewise it is often true that relatively small groups in large societies such as local communities have enormous potential to organize and manage themselves in ways that promote coopera tion and prevent them from depleting natural resources in small groups people are able to develop rules that match the local cir cumstances they are able to monitor one another behavior and punish free riding and reward generosity quite effectively people care very strongly about their image or reputation in their local community and so if the norms favouring cooperation are well speciﬁed then often the mere presence of others makes a big dif ference these are important virtues of a local organization formal or informal relative to a more global authority it is crucial that members of small communities trust each other so that monitoring and norm enforcement can take place is a cost effective informal manner there is a recent meta analytic study involving societies that provides evidence that trust and social norm enforcement may reinforce each other in securing and pro moting cooperation in large scale societies in societies where trust is low such as greece or south africa punishment was hardly effective in promoting cooperation but in high trust societies such as denmark or china possibilities for punishment in public goods dilemmas promoted cooperation very effectively the broad conclusion is that the effectiveness of punishment in promoting cooperation in a public goods experiment is greater in societies with high trust rather than low trust balliet van lange in press a another important result of this meta analysis is that societies with stronger democracies demonstrate a greater ability to secure and promote contributions towards public goods by the use of peer punishment these ﬁndings paint a picture in which the ways in which individuals relate to each other in small groups and local communities is important to the overall functioning of society and this suggests the strong positive reinforcement among structural solutions third party intervention and psycho logical solutions as noted earlier many of the insights described above were al ready recognized by the late elinor ostrom who passed away in at the age of more than years ago she suggested that institutes could play a very important role in regulating the local management to preserve natural resources and avoid ecosystem collapses ostrom in retrospect her insights in many ways reinforce conclusions that are now supported by a meta analytic study in particular among smaller units such as dyads and small groups it is trust and reciprocity that matters and we would add generosity and forgiveness along with effective communication within a frame of sufﬁcient vertical trust people will adopt an accepting attitude to governmental interventions such as the pro vision of rewards and punishment and some constraint on their autonomy these are also analyses of social dilemmas where the various scientiﬁc ﬁelds and disciplines should inform one another to effectively understand how small groups might help effectively manage and resolve ongoing social dilemmas looking back and ahead we cannot help but conclude that the study of social dilemmas is alive and kicking over the years the ﬁeld has produced numerous replicable ﬁndings advanced our theoretical understanding of human cooperation fostered com munication among scientiﬁc disciplines and has at least made a beginning of applying such knowledge to social dilemmas as we face them in everyday life being dedicated social dilemmas researchers ourselves our observations may be a bit colored but we think that the research that has accumulated has resulted in a sea of knowledge that should be exceptional useful in facing the numerous challenges theoretical empirical methodological and societal that the ﬁeld will encounter in the future examples of some key challenges are understanding the how and why of re wards and punishment the strength of fairness and perhaps altru ism as social preferences and the power of beliefs about humankind as individuals and groups and how these might im pact our behavior also the ﬁeld has just started to explore the role of emotions construal processes facial information intergroup is sues reputation gossip and many more issues that are relevant to how people approach others in social dilemmas we could go on but simply thinking about these intriguing issues makes us look for ward to the next several decades of research on social dilemmas abstract device to device communications was initially proposed in cellular networks as a new paradigm for enhancing network performance the emergence of new applications such as content distribution and location aware advertisement introduced new user cases for communications in cellular networks the initial studies showed that communications has advantages such as increased spectral efficiency and reduced communication delay however this communication mode introduces complications in terms of interference control overhead and protocols that are still open research problems the feasibility of communications in long term evolution advanced is being studied by academia industry and standardization bodies to date there are more than papers available on communications in cellular networks but there is no survey on this field in this paper we provide a taxonomy based on the communicating spectrum and review the available literature extensively under the proposed taxonomy moreover we provide new insights into the over explored and under explored areas that lead us to identify open research problems of communications in cellular networks index terms device to device communications cellular networks lte lte a i introduction as telecom operators are struggling to accommodate the existing demand of mobile users new data intensive applications are emerging in the daily routines of mobile users e g proximity aware services moreover cellular technologies wimax and lte a which have extremely efficient physical and mac layer performance are still lagging behind mobile users booming data demand therefore researchers are seeking for new paradigms to revolutionize the traditional communication methods of cellular networks device to device communication is one of such paradigms that appears to be a promising component in next generation cellular technologies communication in cellular networks is defined as direct communication between two mobile users without traversing the base station bs or core network communication is generally non transparent to the cellular network and it can manuscript received october revised january accepted march date of publication april date of current version november this work was supported in part by the european union seventh framework programme under grant crowd by the comunidad de madrid medianet project under grant tic and by micinn under grant the authors are with the imdea networks institute madrid spain and also with the university carlos iii of madrid madrid spain e mail arash asadi imdea org qing wang imdea org vincenzo mancuso imdea org color versions of one or more of the figures in this paper are available online at http ieeexplore ieee org digital object identifier comst fig representative use cases of communications in cellular networks occur on the cellular spectrum i e inband or unlicensed spectrum i e outband in a traditional cellular network all communications must go through the bs even if both communicating parties are in range for communication this architecture suits the conventional low data rate mobile services such as voice call and text message in which users are not usually close enough to have direct communication however mobile users in today cellular networks use high data rate services e g video sharing gaming proximity aware social networking in which they could potentially be in range for direct communications i e hence communications in such scenarios can highly increase the spectral efficiency of the network nevertheless the advantages of communications are not only limited to enhanced spectral efficiency in addition to improving spectral efficiency communications can potentially improve throughput energy efficiency delay and fairness in academia communication was first proposed in to enable multihop relays in cellular networks later the works in investigated the potential of communications for improving spectral efficiency of cellular networks soon after other potential use cases were introduced in the literature such as multicasting peer to peer communication video dissemination machine to machine communication cellular offloading and so on the most popular use cases of communications are shown in fig the first attempt to implement communication in a cellular network was made by qualcomm flashlinq which is a phy mac network architecture for communications underlaying cellular networks flashlinq takes advantage of ofdm ofdma technologies and distributed ieee personal use is permitted but republication redistribution requires ieee permission see http www ieee org publications rights index html for more information ieee communication surveys tutorials vol no fourth quarter fig schematic representation of overlay inband underlay inband and outband scheduling to create an efficient method for timing synchronization peer discovery and link management in enabled cellular networks in addition to academia and telecommunication companies is also investigating communications as proximity services prose in particular the feasibility of prose and its use cases in lte are studied in and the required architectural enhancements to accommodate such usecases are investigated in currently prose is supposed to be included in release as a public safety network feature with focus on one to many communications a brief overview of standardization activities and the fundamentals of prose can be found in the majority of the literature on communications proposes to use the cellular spectrum for both and cellular communications i e underlay inband these works usually study the problem of interference mitigation between and cellular communication in order to avoid the aforementioned interference issue some propose to dedicate part of the cellular resources only to communications i e overlay inband here resource allocation gains utmost importance so that dedicated cellular resources be not wasted other researchers propose to adopt outband rather than inband communications in cellular networks so that the precious cellular spectrum be not affected by communications in outband communications the coordination between radio interfaces is either controlled by the bs i e controlled or the users themselves i e autonomous outband communication faces a few challenges in coordinating the communication over two different bands because usually communication happens on a second radio interface e g wifi direct and bluetooth the studies on outband investigate issues such as power consumption and inter technology architectural design fig graphically depicts the difference among underlay inband overlay inband and outband communications a related topics since communication is a new trending topic in cellular networks there is no survey available on the topic however from an architectural perspective communications may look similar to mobile ad hoc networks manet and cognitive radio networks crn however there are some key differences among these architectures that cannot be ignored although there is no standard for communications communications in cellular network are expected to be overseen controlled by a central entity e g evolved node b enb users may act autonomously only when the cellular infrastructure is unavailable the involvement of the cellular network in the control plane is the key difference between and manet and crn the availability of a supervising managing central entity in communications resolves many existing challenges of manet and crn such as white space detection collision avoidance and synchronization moreover communication is mainly used for single hop communications thus it does not inherit the multihop routing problem of the manet an extensive survey on spectrum sensing algorithms for cognitive radio applications and routing protocols for manet can be found in and respectively communication is another architecture that might benefit from like schemes is the data communication between machines that does not necessarily need human interaction although similarly to focuses on data exchange between numerous nodes or between nodes and infrastructure it does not have any requirements on the distances between the nodes so is application oriented and technology independent while aims at proximity connectivity services and it is technology dependent b contributions and organization of the survey in this paper we provide an extensive review of available literature on communications which is the first of its kind moreover we provide new insights to the existing works which lead us to the under explored open issues in section ii we categorize the available literature based on our proposed taxonomy in sections iii and iv we review the works using inband the papers proposing to use outband are surveyed in section v after reviewing the available literature we discuss the state of the art protocol proposals and provide an overview of prose services in section vi in section vii we provide a discussion on the common assumptions of the surveyed literature the advantages and disadvantages of different approaches the maturity of the field and its emergence into the real world systems in addition this section sheds light on the open issues and potential research directions in communications finally we conclude the paper in section viii ii taxonomy in this section we categorize the available literature on communication in cellular networks based on the spectrum in which communication occurs in the following subsection we provide a formal definition for each category and subcategory next we provide a quick overview of the advantages and disadvantages of each method inband the literature under this category which contains the majority of the available work proposes to use the cellular spectrum for both and cellular links the motivation for choosing inband communication is usually the high control over cellular i e licensed spectrum some researchers see e g consider that the interference in the unlicensed asadi et al survey on device to device communication in cellular networks fig device to device communication classification spectrum is uncontrollable which imposes constraints for qos provisioning inband communication can be further divided into underlay and overlay categories in underlay communication cellular and communications share the same radio resources in contrast links in overlay communication are given dedicated cellular resources inband can improve the spectrum efficiency of cellular networks by reusing spectrum resources i e underlay or allocating dedicated cellular resources to users that accommodates direct connection between the transmitter and the receiver i e overlay the key disadvantage of inband is the interference caused by users to cellular communications and vice versa this interference can be mitigated by introducing high complexity resource allocation methods which increase the computational overhead of the bs or users outband here the links exploit unlicensed spectrum the motivation behind using outband communication is to eliminate the interference issue between and cellular link using unlicensed spectrum requires an extra interface and usually adopts other wireless technologies such as wifi direct zigbee or bluetooth some of the work on outband see e g suggest to give the control of the second interface technology to the cellular network i e controlled in contrast others see e g propose to keep cellular communications controlled and leave the communications to the users i e autonomous outband uses unlicensed spectrum which makes the interference issue between and cellular users irrelevant on the other hand outband may suffer from the uncontrolled nature of unlicensed spectrum it should be noted that only cellular devices with two wireless interfaces e g lte and wifi can use outband and thus users can have simultaneous and cellular communications fig illustrates the taxonomy introduced for communications in cellular networks in the following sections we review the related literature based on this taxonomy iii underlaying inband early works on in cellular networks propose to reuse cellular spectrum for communications to date the majority of available literature is also dedicated to inband especially communications underlaying cellular networks in this section we review the papers that employ underlaying to improve the performance of cellular networks in terms of spectrum efficiency energy efficiency cellular coverage and other performance targets a spectrum efficiency by exploiting the spatial diversity underlaying inband is able to increase the cellular spectrum efficiency this can be done by proper interference management mode selection resource allocation and by using network coding interference between the cellular and communications is the most important issue in underlaying communications good interference management algorithms can increase the system capacity and have attracted a lot of attention the authors of propose to use cellular uplink resources for communications since reusing uplink resources for users can cause interference to cellular uplink transmissions at the bs users monitor the received power of downlink control signals to estimate the pathloss between transmitter and the bs this helps the users to maintain the transmission power below a threshold to avoid high interference to cellular users if the required transmission power for a link is higher than the minimal interference threshold the transmission is not allowed the authors also propose to use dynamic source routing algorithm for routing among users in case of multi hop communications the simulations show that probability of having links increases with stronger pathloss component this is because the stronger the pathloss the weaker the interference caused by transmission at the bs in the authors also study the uplink interference between and cellular users and propose two mechanisms to avoid interference from cellular users to users and vice versa in order to reduce the interference from cellular users to communications users read the resource block allocation information from the control channel therefore they can avoid using resource blocks that are used by the cellular users in the proximity the authors propose to broadcast the expected interference from communication on a cellular resource block to all users hence the users can adjust their transmission power and resource block selection in a manner that the interference from communication to uplink transmission is below the tolerable threshold the authors show via simulation that the proposed mechanisms improve the system throughput by zhang et al propose a graph based resource allocation method for cellular networks with underlay communications they mathematically formulate the optimal resource allocation as a nonlinear problem which is np hard the authors propose a suboptimal graph based approach which accounts for interference and capacity of the network in their proposed graph each vertex represents a link or cellular and each edge connecting two vertices shows the potential interference between the two links the simulation results show that the general mode selection involves choosing between cellular mode i e the bs is used as a relay and mode i e the traffic is directly transmitted to the receiver that the numerical performance gains reported in this article may have been obtained under different simulation experiment settings which are specific to the cited work ieee communication surveys tutorials vol no fourth quarter graph based approach performs close to the throughput optimal resource allocation in a new interference cancellation scheme is designed based on the location of users the authors propose to allocate a dedicated control channel for users cellular users listen to this channel and measure the sinr if the sinr is higher than a pre defined threshold a report is sent to the enb accordingly the enb stops scheduling cellular users on the resource blocks that are currently occupied by users the enb also sends broadcast information regarding the location of the users and their allocated resource blocks hence users can avoid using resource blocks which interfere with cellular users simulation results show that the interference cancellation scheme can increase the average system throughput up to in comparison to the scenario with no interference cancellation janis et al address a similar solution in where the users also measure the signal power of cellular users and inform the bs of these values the bs then avoids allocating the same frequency time slot to the cellular and users which have strong interference with each other which is different from the proposed scheme of minimizes the maximum received power at pairs from cellular users the authors first show via numerical results that communications with random resource allocation can increase the mean cell capacity over a conventional cellular system by next they show that their proposed interference aware resource allocation scheme achieves higher capacity gain than the random resource allocation strategy the work in proposes a new interference management in which the interference is not controlled by limiting transmission power as in the conventional interference management mechanisms the proposed scheme defines an interference limited area in which no cellular users can occupy the same resources as the pair therefore the interference between the pair and cellular users is avoided the disadvantage of this approach is reducing multi user diversity because the physical separation limits the scheduling alternatives for the bs however numerical simulations prove that the capacity loss due to multi user diversity reduction is negligible compared to the gain achieved by their proposal in fact this proposal provides a gain of over conventional interference management schemes a similar method is also considered in where interference limited areas are formed according to the amount of tolerable interference and minimum sinr requirements for successful transmission the proposed scheme consists in i defining interference limited areas where cellular and users cannot use the same resource and ii allocating the resources in a manner that and cellular users within the same interference area use different resources the simulation results show that the proposed scheme performs almost as good as max rate and better than conventional schemes yu et al propose to use han kobayashi rate splitting techniques to improve the throughput of communications in rate splitting the message is divided into two parts namely private and public the private part as the name suggests can be decoded only by the intended receiver and the public part can be decoded by any receiver this technique helps interference victims to cancel the interference from the public part of the message by running a best effort successive interference cancellation algorithm the authors also analytically solve the rate splitting problem in a scenario with two interfering links finally they show via numerical simulations that their rate splitting proposal increases the cell throughput up to higher when the pair is placed far from the bs and close to each other doppler et al study different aspects of communications in cellular networks in they study the session and interference management in communications as an underlay to lte a networks in in this paper they mainly discuss the concepts of and provide a first order protocol for the necessary functionality and signaling they use numerical simulations to show that enabled cellular networks can achieve up to higher throughput than conventional cellular networks in they study the problem of mode selection i e cellular or in lte a cellular networks they propose to estimate the achievable transmission rate in each mode by utilizing the channel measurements performed by users after the rate estimation each user chooses the mode which results in a higher transmission rate at each scheduling epoch the simulations show that their proposal has gain on system throughput over the conventional cellular communications to improve the capacity of cellular networks the authors of propose a joint communication and network coding scheme they consider cooperative networks where communication is used to exchange uplink messages among cellular users before the messages are transmitted to the bs for example cellular users a and b exchange their uplink data over link then each user sends the coded data containing the original data from both users to the bs here the interference is controlled using the interference aware algorithm proposed in they show that random selection of cooperative users is not efficient because the combination of users channel qualities may not be suitable for network coding to overcome this inefficiency they propose to group the users with complementary characteristics to enhance the performance of network coding using numerical simulation they show that their proposal increases the capacity by and in comparison to random selection and decode andforward relaying schemes respectively moreover they show that multi antenna capability reduces the impact of interference from the bs and increases the number of users by the authors of consider a single cell scenario including a cellular user cua and a pair dub and duc dub and duc communicate with each other over the link and cua communicates with the bs by using dub as a relay see fig the relay i e dub can communicate bi directionally with the other user duc as well as assisting the transmission between the bs and the cellular user cua the time is divided into two different periods i during the first period duc and either the bs or cua send data to dub concurrently and ii during the second period dub sends data to duc and either the bs or cua the authors investigate the achievable capacity region of the and the cellular link simulation results show that by adjusting the power of bs and cellular device the area asadi et al survey on device to device communication in cellular networks fig evaluation scenario of the cell includes a pair i e dub and duc and one cellular user i e cua dub also acts as a relay between the bs and cua of capacity region of the link and bs device link can be enlarged by up to xu et al in consider the sum rate optimization in a single cell scenario with underlayed communications using underlay communication the network can suffer from intra cell interference they adopt the iterative combinatorial auction game in their proposed spectrum resource allocation mechanism in this game spectrum resources are considered to be bidders that compete to obtain business and links are considered as goods or services that are waiting to be sold the authors formulate the valuation of each resource unit for groups of links based on this they propose a non monotonic descending price auction algorithm and show that the proposed algorithm can converge in a finite number of iterations moreover the complexity of their proposal is lower than traditional combinatorial allocation schemes in the simulation the authors use winner ii channel models and the simulation results show that the proposed scheme can improve the sum rate up to which varies with the number of spectrum resource units summary the surveyed literature in this subsection showed that communication can improve the spectrum efficiency greatly this improvement can be achieved by exploiting techniques such as interference reduction among cellular and users or interference aware avoidance among these papers and adopt more advanced mathematical techniques than the others the proposed methods in these papers can be either self organized or network controlled the self organized methods proposed in introduce less overhead and are more efficient in comparison to network controlled methods it should also be noted that using advanced mathematical techniques such as nonlinear programming and game theory can result in higher gain than simpler interference reduction avoidance methods based on heuristics however they also introduce higher computational overhead which should be taken into account when comparing the performances of the proposals b power efficiency power efficiency enhancement techniques for enabled cellular networks is also a very interesting research topic xiao et al propose a heuristic algorithm for power allocation in ofdma based cellular networks they propose a heuristic that performs power allocation and mode selection using the existing subcarrier and bit allocation algorithms in and the heuristic first allocates the resources for the cellular users and then performs resource allocation and mode selection for users if the required power level of transmission is higher than a certain threshold the pair communicates through the bs via simulations they show that the integration of their proposed heuristic with the existing algorithms in improves the downlink power consumption of the network around in comparison to the traditional ofdma system without the authors of propose an algorithm for power allocation and mode selection in communication underlaying cellular networks the algorithmmeasures the power efficiency which is a function of transmission rate and power consumption of the users in different modes cellular and after computing the power efficiency each device uses the mode in which it achieves higher power efficiency the drawback of this algorithm is that the controller should perform an exhaustive search for all possible combinations of modes for all devices the authors benchmark their algorithm against the scheme of in which two users communicate over link only if their pathloss is lower than the pathlosses between each user and the bs the simulation results indicate that their algorithm achieves up to gain over the scheme proposed in the authors of aim to minimize the overall transmission power in a multi cell ofdm cellular network they assume a multi cell scenario in which the bs serves a fixed number of cellular and users the authors formulate the problem of joint mode selection resource allocation and power allocation through linear programming which is proven to be np hard in a strong sense due to the complexity of linear programming the authors decide to consider the power allocation in a single cell and propose a heuristic algorithm to solve it they use a distributed sub optimal heuristic which performs mode selection and resource allocation in a single cell scenario the performance of the heuristic is compared with other two schemes i cellular mode in which transmission should go through the bs and ii mode in which all users can only communicate directly and passing through the bs is not allowed the authors provide simulation results showing that the gain of power efficiency of the proposed method over conventional cellular networks is significant up to when the distance between users is less than summary it was observed in this subsection that communication can result in increased power efficiency of the network a common technique to achieve this is to dynamically switch between cellular and modes the authors in and propose heuristic algorithms to solve the mode selection problem while employs the brute force technique the performance of the method in is thus better than those in the other two but it also requires much more computation c performance with qos power constraints there are many works which focus on the improving the system performance while maintaining certain qos power constraints the authors of propose a resource allocationmethod for communication underlaying cellular network which guarantees qos requirements for both ieee communication surveys tutorials vol no fourth quarter and cellular users they mathematically formulate the resource allocation problem which is a nonlinear constraint optimization problem they divide the problem into three subproblems first the bs checks the feasibility of the connection based on the sinr requirements admission control next they formulate the optimal power control for the pair finally a maximum weight bipartite matching based scheme is used for resource allocation for cellular and users the authors benchmark their proposed algorithm against the works in via numerical simulations the results show that their approach provides up to throughput gain over the algorithms proposed in the authors of consider the mode selection and resource allocation in communications underlay cellular networks where several pairs of links co exist with several cellular users they formulate the problem of maximizing the system throughput with minimum data rate requirements and use the particle swarm optimization method to obtain the solutions the simulation results show that the proposed method has throughput gain over the orthogonal resource sharing scheme i e overlay which will be explained later where the achievable gain varies with the distance of users simulation results also show that this method can improve the system performance under the constraint of minimum data rate of users the authors of consider the scheduling and mode selection problem for in ofdma networks they assume that the system time is slotted and each channel is divided into subchannels they formulate the problem of maximizing the mean sum rate of the system with qos satisfaction as a stochastic optimization problem and use the stochastic sub gradient algorithm to solve it from the solution they design a sub channel opportunistic scheduling algorithm that takes into account the csi of and cellular links as well as the qos requirement of each user the numerical results show that the mean sumrate can be improved by up to this gain increases when the average pair distance reduces moreover with the communication the fairness among users can be achieved with the qos requirement specified for each user in a two phase resource allocation scheme for cellular network with underlaying communications is proposed the authors first formulate the optimal resource allocation policy as an integer programming problem which is np hard hence they propose a two phase low complexity suboptimal solution instead of the np hard problem in the first phase they extend the technique used in to perform optimal resource allocation for cellular users in the second phase they use a heuristic subchannel allocation scheme for flows which initiates the resource allocation from the flow with minimum rate requirements the heuristic also accounts for a power budget i e a transmission power that does not impact the transmission rate of cellular flows in the subchannel allocation for flow the authors of and consider a single cell scenario where a cellular user and two users share the same radio resources they assume that the bs is aware of the instantaneous channel state information csi of all the links and it controls the transmit power and the radio resources of the links the objective is to optimize the sum rate with energy power constraint under three different link sharing strategies i e non orthogonal sharing mode orthogonal sharing mode and cellular mode the authors show analytically that an optimal solution can be given either in closed form or can be chosen from a set the numerical simulation for single cell and and multi cell scenarios illustrates that under their proposed algorithm the transmission will not bring much interference to the cellular transmission moreover the interference aware resource allocation increases the system sum rate by up to similar scenario and objective are considered in the difference among and is that in the bs is only aware of the average csi of the links whereas in and it is aware of the instantaneous csi of links summary improving the performance of enabled cellular systems with qos power constraints usually requires advanced techniques such as stochastic optimization nonlinear programming and integer optimization as expected the solution of these approaches and their derived sub optimal heuristic can indeed improve the system performance with qos power constraints however they do not seem to be a good candidate for time stringent application with limited computational capacity nonetheless the authors of and derived the closed form of the optimal solution that in fact reduces the computational complexity it should be noted that their considered scenario only consist of a cellular user and a pair which is not practical in reality d miscellaneous in addition to spectrum efficiency power efficiency and system performance with different constraints there are some other interesting works aiming to enhance the fairness spectrum utility cellular coverage and reliability the authors of aim to improve the user spectrum utility through mode selection and power allocation where the spectrum utility is defined as the combination of users data rates power expenditure and bandwidth as for the mode selection the users can choose to transmit in bs or modes in bsmode transmitter and receiver communicate through the bs as in the conventional cellular system in mode transmitter directly communicates with the receiver using the cellular resources as in underlay communication the authors first derive the optimal transmission power for the above mentioned modes and then use evolutionary game to obtain the model selection each user performs mode selection individually and independently the bs collects users mode selection decisions and broadcasts this information to all users to help them for future mode selections numerical results show that via the proposed method the spectrum utility can be improved by up to and when compared to solely bs mode and mode respectively xu et al in propose a resource allocation method based on sequential second price auction for communications underlaying cellular networks in a second price auction the winner pays as much as the second highest bid in the proposed auction each resource block is put on auction and pairs asadi et al survey on device to device communication in cellular networks should bid for the resource blocks that they want to occupy therefore each pair makes a bidding for every resource block and the bidding values are a function of achievable throughput of the bidding pair on the auctioned resource block simulation results show that the achievable throughput of their proposal is at least of the optimal resource allocation strategy the results also illustrate that the proposal achieves a fairness index around and system sum rate efficiency higher than communication is also a promising way to enlarge the cellular coverage and improve the performance of cell edge users e g the authors of propose a method to use nodes as virtual infrastructure to improve system capacity and system coverage a node within the bs service range can be assigned a relay node depending on the network conditions and traffic requirements nodes close to each other are separated into different groups and the bs serves the groups using the round robin scheduling policy to mitigate interference through monte carlo simulation techniques for both uplink and downlink the authors show that the throughput of cell edge users can be improved from to the cell coverage can also be enlarged with significant data rates yang et al in propose an architecture to setup links for lte a based system which is seldom considered by other researchers this architecture includes a reference point between the enabled users to support proximity measurement channel state measurements and data transmission a bearer that offloads traffic from the evolved packet system eps bearer is also included to provide the direct traffic path between users the authors propose to include a function in the packet data gateway p gw for proximity services in addition a protocol architecture is proposed to manage the bearer and support enhancement through an example the authors present the detailed procedure to offload data from cellular user links to links min et al in try to improve the reliability of communications through receive mode selection they consider three receive modes in cellular networks with underlay communications i the receiver decodes the desired signal directly while treating other signals as noise ii the receiver conceals other signals first and then decodes the desired signal and iii the bs retransmits the interference from cellular communication to the receiver the last mode is proposed by the authors to improve the reliability of the link the paper investigates the outage probability under these three receive modes and provides closed form results for computing outage probability each user can separately calculate the outage probability for each receive mode using the closed from formulas provided from the analysis at each time instant the users dynamically choose the best mode i e the mode with the lowest outage probability in order to reduce the energy consumption of the mobile device the bs performs the outage probability calculations and sends the results to each user however this approach increases the computational overhead of the bs numerical results show that the outage probability can be improved by up to under the proposed receive method which increases the reliability of communications to ensure the reliability of cellular users the authors of propose a scheme that does not cause outage for cellular users they state that assuming users have knowledge of the location and channel state of cellular users is not feasible in a real system therefore they design a distributed power control scheme that leverages a predefined interference margin of cellular users then users adjust their power level in such a way that their transmission does not exceed the interference margin of cellular users power adjustment can be done if the interference margin and estimating the channel gain between user and the bs are known the authors also propose to use distributed source routing algorithm to perform multi hop communication in the network simulation results indicate that the outage probability of links reduces as the pathloss component of the link increases han et al in consider the uplink channel reuse in a single cell network the aim is to maximize the number of admitted links while minimizing the average interference caused by links the authors formulate the problem as a nonlinear programming and design a heuristic algorithm based on the hungarian algorithm their simulation results show that the performance of the proposed heuristic algorithm can be as good as the optimal solution however from the results we can see that the number of admitted links under the proposed algorithm does not increase greatly as compared to a random link allocation maximum one link which is less than the authors of propose to use communication to accommodate communications in cellular networks they state that communications usually need low data rate but they are massive in numbers which leads to highly increased control overhead moreover communication is usually handled by a random medium access technique which is susceptible to congestion and limited by number of contending users therefore the authors in propose to use network assisted communication among several machines and a cellular device next the cellular device is used to relay traffic to the bs this approach can significantly reduce the overhead for the bs the authors show via numerical simulations that their approach achieves gain over the scheme that does not make use of communications the work in proposes a hybrid automatic repeat request harq for multicast in enabled cellular networks the idea is to divide users into clusters and have the bs broadcast packets to all devices within a cluster in each cluster there is a cluster header ch a non ch user that fails to receive the broadcast packet reports nack to the ch via the link the ch can report the status of the broadcast transmission to the bs via a message stating one of the following states i an message that represents all the users within the cluster have successfully received the broadcast packet ii an message representing that all the users within the cluster have failed to receive the broadcast packet so that the bs has to re broadcast the packet iii a message representing that the ch has successfully received the packet but at least another user has failed to receive it the ch then transmits the packet to those that have failed to receive it and iv a message representing that the ch ieee communication surveys tutorials vol no fourth quarter has failed to receive the frame but at least another user has successfully received it then the ch will choose a user that has received the packet to transmit the packet to those that have failed to receive it this method highly reduces the frame loss ratio of the feedback nack ack from devices compared to the method where each device sends an ack nack to the bs therefore the performance of multicast is improved in communication is used for content distribution in cellular networks the authors propose a location aware scheme which keeps track of the location of users and their requests for example if the bs receives a request from user a for a content which is available in the cache of a nearby user b it instructs user b to send the content via link to user a using this method the bs does not require to re transmit a content which has been already transmitted the amount of bandwidth saved with communications can be used for future or pending transmissions using this approach we can potentially reduce the transmission delay and increase the capacity of the network it should be noted that keeping track of users location and their cached traffic can lead to high control overhead moreover the location tracking method should be optimized so that the battery of cellular device is not drained by the gps summary in this subsection the surveyed literature focused on various metrics and use cases some employed advanced mathematical techniques such as game theory to improve system performance in terms of fairness or spectrum utilization using advanced mathematical techniques such as stochastic lyapunov optimization and dynamic programming might lead to increased complexity but they are indeed effective enhancement approaches which give the researchers insight to evaluation of other metrics such as queue stability and packet transfer time the authors of provide the first protocol for signaling and other functionality in enabled networks this helps greatly the researchers and engineers who plan to implement in the real world nevertheless the evaluation scenario in can be enhanced to a more realistic setup other papers focus on exploiting new use cases of communication such as multicast and content distribution although most of these papers have not used advanced mathematical tools their proposals lead to high performance gains moreover communication appears to be a viable candidate for applications such as proximity peer to peer gaming and social networking finally a summary of the works on underlay communication in cellular networks is provided in table i in terms of metrics use cases analytical tools evaluation method scope and achieved performances iv overlaying inband different from the works reviewed in the previous subsection the authors of propose to allocate dedicated resources for communications this approach eliminates the concerns for interference from communications on cellular transmissions but reduces the amount of achievable resources for cellular communications in fodor et al elaborate on the challenges of communications in cellular networks and suggest to control communications from the cellular network they claim that network assistance can solve the inefficiencies of communications in terms of service and peer discovery mode selection channel quality estimation and power control in a conventional peer and service discovery method users should send beacons in short intervals and monitor multiple channels which is very energy consuming however this process can become more energy efficient if the bs regulates the beaconing channel and assists users so that they do not have to follow the power consuming random sensing procedure bs assistance also improves the scheduling and power control which reduces the interference the authors use simple monte carlo simulation to evaluate the performance of communications the results show that can increase the energy efficiency from bps hz mw to bps hz mw in the best case scenario where the distance between users is m the authors of propose the incremental relay mode for communication in cellular networks in the incremental relay scheme transmitters multicast to both the receiver and bs in case the transmission fails the bs retransmits the multicast message to the receiver the authors claim that the incremental relay scheme improves the system throughput because the bs receives a copy of the message which is retransmitted in case of failure therefore this scheme reduces the outage probability of transmissions although the incremental relay mode consumes part of the downlink resources for retransmission the numerical simulation results show that this scheme still improves the cell throughput by in comparison to underlay mode in communication is used to improve the performance of multicast transmission in cellular networks due to wireless channel diversity some of the multicast group members i e cluster may not receive the data correctly the authors propose to use communications inside the clusters to enhance themulticast performance specifically after everymulticast transmission some of thememberswhich manage to decode the message will retransmit it to those which could not decode the message unlike the prior work in and where there is only one predefined retransmitter the number of retransmitters in changes dynamically to maximize the spectral efficiency the authors show via numerical simulations that their proposed algorithm consumes less spectrum resources in comparison to the scenario with only one retransmitter summary in this subsection we surveyed the works which proposed to use dedicated resources for communications a bs assisted scheduling and power control was proposed in in order to reduce interference differently the authors of and focus on relaying use case of specifically proposes to use the bs as a relay backup retransmitter for the transmission and uses multiple users as relays re transmitters for multicasting both methods proposed in and have low complexity which makes them practical for real world scenarios the algorithm proposed in is much more complex and it exhibits very high performance when the maximal distance between users is short asadi et al survey on device to device communication in cellular networks table i summary of the literature proposing underlaying inband table ii summary of the literature proposing overlaying inband a summary of the works on overlay communication in cellular networks is provided in table ii v outband in this section we review the papers in which communications occur on a frequency band that is not overlapping with the cellular spectrum outband is advantageous because there is no interference issue between and cellular communications outband communication can be managed by the cellular network i e controlled or it can operate on its own i e autonomous a controlled in works that fall under this category the authors propose to use the cellular network advanced management features to control communication to improve the efficiency and ieee communication surveys tutorials vol no fourth quarter fig data flow between users and the enb i e bs reliability of communications they aim to improve system performance in terms of throughput power efficiency multicast and so on the authors of propose to use ism band for communications in lte they state that simultaneous channel contention from both and wlan users can dramatically reduce the network performance therefore they propose to group users based on their qos requirement and allow only one user per group to contend for the wifi channel the channel sensing between groups is also managed in a way that the groups do not sense the same channel at the same time they show via simulation that their approach increases the throughput up to in comparison to the scenario in which users contend for the channel individually the authors of propose to use communications for increasing the throughput and energy efficiency of cellular networks the authors propose to form clusters among cellular users who are in range for wifi communication after the cluster is formed only the cluster member with the highest cellular channel quality i e cluster head communicates with the bs the cluster head is also responsible to forward the cellular traffic of its clients i e other users who belong to the same cluster to the bs the authors provide an analytical model to compute the throughput and power consumption for the proposed scheme the advantages of this scheme are threefold i the spectral efficiency increases because the cluster head has the highest channel quality among the cluster members which corresponds to transmissions with high modulation and coding schemes mcs ii the energy efficiency is increased because the cluster clients can go to cellular power saving mode iii the fairness can be increased because the cellular resources is distributed among cluster members in a way that users with poor channel quality are not starved the authors show via numerical simulation that communications improve throughput and energy efficiency with respect to classical round robin schedulers by and respectively the results also show that the proposed scheme can achieve almost perfect fairness furthermore the work in provides a protocol for the communication scheme proposed in the authors first elaborate on the required modification for messaging and signaling procedures of lte and wifi direct technologies next they define a protocol stack to connect the two technologies as illustrated in fig the protocol stack will be further elaborated in section vi this paper sheds light on different aspects of integrating lte and wifi direct such as channel quality feedback scheduling security etc via a home grown lte simulator the authors show that the proposed scheme in can improve the packet delay of a round robin scheduler by up to and it can guarantee delays less than ms with probability golrezaei et al point out the similarities among video content requests of cellular users they propose to cache the popular video files i e viral videos on smartphones and exploit communications for viral video transmissions in cellular networks they partition each cell into clusters smaller cells and cache the non overlapping contents within the same cluster when a user sends a request to the bs for a certain content the bs checks the availability of the file in the cluster if the content is not cached in the cluster the user receives the content directly from the bs if the content is locally available the user receives the file from its neighbor in the cluster over the unlicensed band e g via wifi the authors claim that their proposal improves the video throughput by one or two orders of magnitude the authors of propose a method to improve video transmission in cellular networks using communications this method exploits the property of asynchronous content reuse by combining communication and video caching on mobile devices their objective is to maximize per user throughput constrained to the outage probability i e the probability that a user demand is unserved they assume devices communicate with each other with a fixed data rate and there is no power control over the link through simulations the authors show that their proposed method outperforms the schemes with conventional unicast video transmission as well as the coded broadcasting the results show that their proposed method can achieve at least and throughput gain over the conventional and coded broadcasting methods respectively when the outage probability is less than wang et al propose a bs driven traffic spreading bits algorithm to exploit both the cellular and links bits leverages devices instantaneous channel conditions and queue backlogs to maximize the bs scheduling options and asadi et al survey on device to device communication in cellular networks hence increases the opportunistic gain the authors model the bits policy with the objective to maximize delay sensitive utility under an energy constraint they develop an online scheduling algorithm using stochastic lyapunov optimization and study its properties through simulations they show that under bits the utility can be improved greatly and the average packet transfer delay can be reduced by up to the authors also evaluate bits using realistic video traces the results show that bits can improve the average peak signal to noise ratio psnr of the received video by up to db and the frame loss ratio can be reduced by up to cai et al propose a scheduling algorithm to exploit both time varying channel and users random mobility in cellular networks they consider a scenario where the bs broadcasts deadline based content to different group of users users move randomly within the cell and users of the same group are assumed to be able to communicate directly at a high rate when they are close to each other therefore users can exchange all the content within their current lists during a contact period during each slot the bs dynamically selects a group of users to broadcast content to at a chosen service rate based on the scheduling algorithm employed if the service rate is too high for some users to successfully receive the content these users will exploit the communication to fetch content from nearby users in the near future the authors formulate the scheduling problem with the objective to maximize the group utility function next they solve the maximization problem under the assumption of statistically homogeneous user mobility and then extend it to the heterogeneous scenarios simulation results show that the proposed scheduling algorithm can improve the system throughput from to compared to the scheduling algorithm without communications summary the works addressed in this subsection focus on various use cases of communication the authors in and use clustering and game theory to boost the throughput performance as well as energy efficiency and fairness for the first time they designed a detailed protocol for outband communications in the work in and aim to improve the performance of content distribution the methods proposed in and are simple while that of is more complex the performance of bothmethods is evaluated to be good in addition to content distribution the authors in also consider user mobility and deadline based content for which they provide comprehensive evaluations under a realistic simulation setup including real time video transmission b autonomous autonomous communication is usually motivated by reducing the overhead of cellular networks it does not require any changes at the bs and can be deployed easily currently there are very few works in this category wang et al propose a downlink bs transparent dispatching policy where users spread traffic requests among each other to balance their backlogs at the bs as shown in fig they assume that users traffic is dynamic i e the bs does not always have traffic to send to all the users at any time they illustrate the dispatching policy by considering a scenario with two users and fig example of the bs transparent traffic spreading a no traffic spreading b traffic spreading from to being served by the bs the queues and depict the numbers of files at user bs queues in fig a since the queues at the bs are balanced the dispatchers at each user would detect that traffic spreading is not beneficial thus users send their new requests to the bs directly in fig b there are more files in than the dispatcher of would detect that traffic spreading is beneficial because in the near future would be empty and thus the opportunistic scheduling gain is lost therefore asks to forward its new file requests to the bs after receiving the corresponding files from the bs forwards them to this dispatching policy is user initiated i e it does not require any changes at the bs and works on a per file basis this policy exploits both the time varying wireless channel and users queueing dynamics at the bs in order to reduce average file transfer delays seen by the users the users perceive their channel conditions to the bs i e cellular channel conditions and share them among each other the authors formulate the problem of determining the optimal file dispatching policy under a specified tradeoff between delay performance and energy consumption as a markov decision problem next they study the properties of the corresponding optimal policy in a two user scenario a heuristic algorithm is proposed which reduces the complexity in large systems by aggregating the users the simulation results demonstrate that the file transfer delays can be reduced by up to using the proposed methodology in addition their proposal consumes less power than performance centric algorithms while achieving significant gains up to a summary of the works on outband communication in cellular networks is provided in table iii vi proposed protocols the majority of researchers have addressed issues such as interference resource allocation power allocation and so on only a few researchers propose protocols for communications in particular the authors of and propose a protocol stack for inband and outband communication respectively in the authors describe the required architectural and protocol modification in the current cellular standards to adapt inband communication the main architectural modification consists in adding a server inside or outside the core network in case the server is placed outside the core network it should have interfaces with mobility management entity mme policy and charging rules function pcrf ieee communication surveys tutorials vol no fourth quarter table iii summary of the literature proposing outband fig illustration of proposed architecture in peer servers and application servers the server is expected to handle functionalities such as device identifier allocation call establishment ue capability tracking service support and mobility tracking fig illustrates the architecture which was described above the authors also propose a protocol stack in which pairs have extra phy mac radio link control rlc and packet data convergence protocol pdcp layer for direct communication this means that ues retain their cellular connectivity while communicating over link the authors of elaborate on the feasibility of outband communications in lte based systems as mentioned before the target of their proposal is opportunistic packet relaying to this aim the authors provide a protocol stack which connects lte andwifi direct protocols see fig the authors propose to encapsulate lte pdcp packet data units pdus into wifi packet and transmit it them over wifi to the receiver if the receiver needs to relay the packet to the enb it simply extracts the lte pdcp pdu from wifi and processes it through rlc mac and phy layers as shown in fig in addition to providing a protocol stack important procedures such as device discovery registration connection establishment default dedicated bear setup mobility management csi reporting scheduling security are also addressed in this paper while addressing all these issues the authors try to minimize the modification to the existing protocols for example the discovery phase and connection establishment are asadi et al survey on device to device communication in cellular networks fig device discovery and connection establishment procedure very similar to wifi direct standard defined procedure the main difference is the addition of an extra phase i e specific messages see fig in order to exchange lte ids between users for a detailed description of the protocol refer to none of the proposed protocols for communication comment on when how to activate mode thus this remains an open research problem to be solved in future as mentioned earlier two working groups are also investigating the prose use cases in lte and required protocol architecture enhancements to accommodate such use cases prose communication supports two types of data path direct mode and locally routed in direct mode two ues exchange data directly with each other in locally routed data between ues is routed locally via the enb both of these are different from the data path specified in current lte standard where the serving gateway packet data network gateway sg pgw is involved besides control path in prose communication has more choices if two ues using the prose communication are served by the same enb the system can decide to perform control information between ue enb and epc the ues can also exchange control signaling directly with each other to minimize signaling modification if two ues involved in the prose communication are served by different enbs the system can decide to perform control information between ue enb and epc in additional the enbs can coordinate with each other directly for radio resource management and the ues can communicate directly to exchange control signaling the also defines other aspects of prose communication such as prose direct discovery roaming support for public safety service and support for wlan direct communication etc details of these aspects can be found in based on the above mentioned schemes the proposes tens of use cases such as prose enabled ues discover other prose enabled ues which can be used for social networking supports large number of ues in a dense environment which can be used for city parking service establishes prose assisted wlan direct communications which can be used for cellular traffic offloading and so on vii discussions and future work so far we have reviewed the available literature on communications in cellular networks in this section we will shed light on some important factors such as common assumptions scope of the works and common techniques a common assumptions most of the papers in the literature assume the bs is aware of the instantaneous csi of cellular and or links e g this assumption is essential because their proposed solutions need the bs participation to make scheduling decisions for cellular and users alternatively when the users decide on the their transmission slots the common assumption is that users are aware of the cellular and links on the other hand there are also papers such as and that assume the bs or users are only aware of the statistical csi of the links with this assumption the large overhead for reporting instantaneous csi can be avoided to mitigate possible interference from transmission to cellular transmission assumes that users are aware of minimum interference threshold of cellular users with the latter assumptions the users can opportunistically choose the transmission slots in which they do not interfere with the cellular users the proposals which involve in clustering users commonly assume that the cluster are far enough so that there is no or negligible interference among different clusters e g this assumption may not hold in populated areas or dense deployments a very interesting observation from the reviewed literature is that the majority of papers assume that the bs or users always have traffic to send therefore they use throughput as a common metric however the authors of consider a scenario with dynamic traffic load and evaluate the average file transfer delay and delay sensitive utility under their proposed traffic spreading mechanism respectively since the latter assumption is more realistic it would be interesting to see the performance of the aforementioned works under dynamic traffic flows b inband or outband majority of the papers propose to reuse the cellular resources for communications i e inband however outband communication is attracting more and more attention in the past few years before comparing the two approaches we summarize the advantages and disadvantages of each approach inband inband is advantageous in the sense that i underlay increases the spectral efficiency of cellular spectrum by exploiting the spatial diversity ii any cellular ieee communication surveys tutorials vol no fourth quarter table iv advantages and disadvantages of different types of communications device is capable of using inband communication the cellular interface usually does not support outband frequencies and iii qos management is easy because the cellular spectrum can be fully controlled by the bs the disadvantages of inband communications are i cellular resources might be wasted in overlay ii the interference management among and cellular transmission in underlay is very challenging iii power control and interference management solutions usually resort to high complexity resource allocation methods and iv a user cannot have simultaneous cellular and transmissions it appears that underlay communication is more popular than overlay the authors who propose to use overlay usually try to avoid the interference issue of underlay however allocating dedicated spectrum resources to users is not as efficient as underlay in terms of spectral efficiency we believe that the popularity of underlay is due to its higher spectral efficiency outband this type of communications has merits such as i there is no interference between cellular and users ii there is no need for dedicating cellular resources to spectrum like overlay inband iii the resource allocation becomes easier because the scheduler does not require to take the frequency time and location of the users into account and iv simultaneous and cellular communication is feasible nevertheless outband has some disadvantages which are i the interference in unlicensed spectrum is not in the control of the bs ii only cellular devices with two radio interfaces e g lte and wifi can use outband communications iii the efficient power management between two wireless interfaces is crucial otherwise the power consumption of the device can increase and iv packets at least the headers need to be decoded and encoded because the protocols employed by different radio interfaces are not the same although the literature on inband is wider than that of outband it seems that researchers have started to explore the advantages of outband and they are considering it as a viable alternative to inband we believe that with the evolutionary integration of smartphones in phone market the majority of mobile devices will be equipped with more than one wireless interface which makes it possible to implement outband schemes moreover the standards such as are looking into handover to and from different platforms e g wimax and lte which could significantly reduce the complexity of coordination between different wireless interfaces in outband table iv summarizes the above mentioned merits and disadvantages table v analytical tools used in the literature c maturity of in cellular networks we believe communication in cellular networks is a relatively young topic and there is a lot to be done explored in this field we support this belief by looking into the analytical techniques and evaluation methods which are used in the available literature analytical techniques in comparison to other fields such as opportunistic scheduling the number of techniques used in the literature and their popularity is very low the majority of the literature only proposes ideas architectures or simple heuristic algorithms some of the papers formulate their objectives as optimization problems but leave them unsolved due to np hardness therefore we believe there is room for investigating optimal solutions for interference coordination power management and mode selection table v summarizes the mathematical techniques used in the related literature evaluation method another metric for maturity of a field is the evaluation method the more realistic the evaluation asadi et al survey on device to device communication in cellular networks table vi evaluation methods in the literature method the more mature the study of that field table vi shows different evaluation methods used in the literature as we can see majority of the papers use numerical evaluation and some use simple home grown simulators there is no paper using experimental evaluation this is mainly due to the fact that experimental testbeds for cellular network are extremely costly and do not have support for yet the literature rarely uses popular network simulators such as opnet omnet in turn currently available network simulators do not support communications d how far is from a real world implementation although communication is not mature yet it is already being studied in the standardization body recently decided that the focus of in lte would be on public safety networks moreover qualcomm has shown interest in this technology and they also built a prototype for communications in cellular network which can be used in different scenarios such as social networking content sharing and so on this confirms that communication is not only a new research topic in academia but also that there is interest in such a technology in the industry there are various obstacles to implement in cellular networks for example the operators are used to having control of their spectrum and the way it is used as a result a successful implementation should allow communications in a manner that operators are not stripped off their power to control their network moreover there are physical challenges such as suitable modulation format and csi acquisition which should be addressed efficiently therefore we believe that communications will become an essential part of cellular communications in the next few years e implementation challenges in real world although communication triggered a lot of attention and interest in academia industry and standardization bodies it is not going to be integrated into the current communication infrastructure until the implementation challenges are resolved here we explain some of the major challenges faced by communications interference management under inband communication ues can reuse uplink downlink resources in the same cell therefore it is important to design the mechanism in a manner that users do not disrupt the cellular services interference management is usually addressed by power and resource allocation schemes although the characteristics of interference are not well understood yet power allocation in inband the transmission power should be properly regulated so that the transmitter does not interfere with the cellular ue communication while maintaining the minimum sinr requirement of the receiver in outband the interference between and cellular user is not of concern therefore power allocation may seem irrelevant in outband however with increased occupancy of ism bands efficient power allocation becomes crucial for avoiding congestion collision issues and inter system interference resource allocation this is another important aspect of communication specially for inband interference can be efficiently managed if the users communicate over resource blocks that are not used by the nearby interfering cellular ues resource allocation for outband simply consists in avoiding ism bands which are currently used by other users wifi hotspots etc modulation format this is one of the challenges which is rarely addressed by researchers the existing lte ues use an ofdma receiver in downlink and a sc fdma for uplink transmission thus for using downlink resp uplink resources the ue should be equipped with ofdma transmitter resp sc fdma receiver channel measurement accurate channel information is indispensable to perform efficient interference management power allocation and resource allocation conventional cellular systems only need the downlink channel information from ues and the uplink channel information is readily computed at the base station unfortunately communication requires information on the channel gain between pairs the channel gain between transmitter and cellular ue and the channel gain between cellular transmitter and receiver the exchange of such extra channel information can become an intolerable overhead to the system if the system needs instantaneous csi feedback the trade off between accuracy of csi and its resulting overhead is to be further investigated energy consumption communication can potentially improve the energy efficiency of the ue however this highly depends on the protocol designed for device discovery and communication for example if the protocol forces the ue to wake up very often to listen for pairing requests or to transmit the discovery messages frequently the battery life of the ue may significantly reduces the trade off between ue power consumption and discovery speed of the ues should be better studied harq considering the complexity of interference management in communication harq appears to be a viable technique to increase the robustness harq can be sent either directly i e from the receiver to the transmitter or indirectly i e from the receiver to the enb and from the enb to the transmitter the direct mode poses less overhead to the enb in comparison to indirect mode moreover benefits from the ack nack messages arrive to the transmitter with shorter delay f potential future work here we elaborate on some of the possible research directions and open problems in communications in cellular ieee communication surveys tutorials vol no fourth quarter networks in the following we list some open problems based on different research methodologies theoretical work as we mentioned earlier the use of mathematical tools and optimization techniques in the stateof the art are very limited the current literature definitely lacks optimal mode selection techniques and interference and power control mechanisms the queue stability analysis using techniques such as stochastic lyapunov optimization can be also an interesting issue to tackle this can be further extended to provide throughput based utility throughput power tradeoff delay bounds and delay analysis of communications in cellular networks architecture there is very little work explaining the required architecture in order to support communications in cellular networks it is interesting to further investigate on the capability of the current centralized cellular architecture to handle procedures such as device discovery connection setup cellular network registration process interference control resource allocation security and so on similarly software defined networking oriented architectures soon will have to include in the equation indeed needs to be studied in the more complex context of hetnets due to growing market interest for availability of multiple radio technologies deployed on mobile devices application a decade ago was first proposed for relaying purposes in cellular networks to date researchers proposed new use cases for communications in cellular networks such as multicasting peer to peer communication video dissemination communication and cellular offloading we believe communication can have more applications in the telecommunication world for example it would be interesting to see the application of communication in social networking location aware services vehicular networks smart grids etc performance analysis as seen in table vi the majority of the available literature is based on numerical or home grown simulations although these types of evaluation method are suitable for studying the potential gains they are still far from reality due to simplified assumptions we believe a performance evaluation using the existing network simulators such as opnet omnet or an experimental evaluation can help in revealing both real performance and new challenges of communications in cellular networks viii conclusion in this paper we provided an extensive survey on the available literature on communications in cellular networks we categorized the available literature based on the communication spectrum of transmission into two major groups namely inband and outband the works under inband were further divided into underlay and overlay outband related literature was also sub categorized as controlled and autonomous the major issue faced in underlay communication is the power control and interference management between and cellular users overlay communication does not have the interference issue because and cellular resources do not overlap however this approach allocates dedicated cellular resources to users and has a lower spectral efficiency than underlay in outband there is no interference and power control issue between and cellular users nevertheless the interference level of the unlicensed spectrum is uncontrollable hence qos guaranteeing in highly saturated wireless areas is a challenging task we also discussed the weaknesses and strength of the existing literature we pointed out the shortcomings of current works and proposed potential future research directions our survey showed that communication in cellular networks is immature and there are still numerous open issues such as interference management power control etc we also shed light on some possible research directions needed to improve the understanding of potentialities for real world applications are men more willing to take financial risks than women the answer to this question has immediate relevance for many economic issues we assemble the data from sets of experiments with one simple underlying investment game most of these experiments were not designed to investigate gender differences and were conducted by different researchers in different countries with different instructions durations payments subject pools etc the fact that all data come from the same basic investment game allows us to test the robustness of the findings we find a very consistent result that women invest less and thus appear to be more financially risk averse than men elsevier b v all rights reserved introduction many economic interactions involve some form of risk thus it is not surprising that a substantial body of research in social science has tried to understand how decision makers incorporate risk in their choices expected utility the dominant theory of decision under risk makes some testable empirical predictions however under expected utility the actual level of risk taking behavior by the agent is left as a free parameter allowing for individual differences this is also true for more behaviorally driven theories such as prospect theory in this article we study one important systematic difference in risk taking between groups in particular we study the interaction of risk taking with the gender of the decision maker the common stereotype is that women are more risk averse than men this stereotype is important since it can potentially explain important economic phenomena empirical investigation of gender differences in risk taking do point in the direction of less risk taking by women than by men see the surveys in eckel and grossman and croson and gneezy a major problem with the empirical investigation of individual differences in risk taking is the variation in the methods used to study the phenomenon considering only the experimental work done mostly by psychologists each experiment corresponding author e mail addresses charness econ ucsb edu g charness ugneezy ucsd edu u gneezy some might argue that this difference across gender seems grounded in evolutionary psychology given our role differentiated hunter gatherer roots for a background to the evolutionary psychology literature see for example tooby and cosmides cosmides et al however it is not completely clear how the hunter gatherer origins map into contemporary financial behavior connecting competitive behavior with risk taking dekel and scotchmer developed an evolutionary model of preference formation to investigate to what extent evolution leads to risk taking in winner take all environments like reproduction they show that winner take all games are related to the survival of risk takers and the extinction of risk averters since in many species a winner take all game determines the males right to reproduce the argument suggests that males will evolve to be risk takers see front matter elsevier b v all rights reserved doi j jebo g charness u gneezy journal of economic behavior organization uses a different decision problem which makes it hard to compare results in addition some of these articles found gender differences without looking for them and others were specifically designed to test for these differences the problem with the former approach is that these articles report the results of one experiment and we do not know how many experiments have looked for a difference and did not find one we are left with a selection bias of articles with a positive finding the problem with the latter approach is that when the goal is to find or not to find a gender difference the design of the experiment may be based on a small set of incidents in which the researchers expect to find the results they are after here again finding no difference can lead to the researcher shelving the project it is easier to publish an article that reports a gender difference in risk taking than a study that reports no difference for example when a study finds that women are more risk taking than men man bites dog this study has a much higher chance of being published than a study that finds no difference in risk taking this bias in publication also creates incentives for researchers to design studies that will generate such a difference the novelty of our article is in using existing empirical results that were collected in a systematic way using thousands of observations by different researchers in a variety of setups but based on one simple investment game most of the data were collected not in order to study gender differences but rather to study other hypotheses regarding investment behavior and they vary with respect to the subject pools country age different incentives and probabilities repeated versus one shot interaction laboratory versus internet known probabilities versus uncertainty and framing moreover since the original data were not collected in order to facilitate comparisons there was no effort made by the researchers to have a uniform design e g use the same instructions this variety allows us to test the robustness of the hypothesis the data are based on an investment decision that was introduced by gneezy and potters in this choice the decision maker receives x and is asked to choose how much of it x she wishes to invest in a risky option and how much to keep the amount invested yields a dividend of kx k with probability p and is lost with probability p the money not invested x x is kept by the investor the payoffs are then x x kx with probability p and x x with p in all cases p and k are chosen so that p k making the expected value of investing higher that the expected value of not investing thus a risk neutral or risk seeking person should invest x while a risk averse person may invest less the choice of x is the only decision the participants make in the experiment we report data from all studies of which we are aware using this method for testing risk aversion the striking and consistent result is that despite the large environmental differences among the sets of experiments a consistent gender difference is reported men choose a higher x than women do data dreber et al this study was conducted in the field with highly skilled people who consider probabilities and risk quite frequently tournament bridge players the financial stakes were also rather large participants at the fall north american bridge championship in boston were recruited for the study almost all of the participants were serious tournament bridge players who play many dozens of session per year the primary focus of the study was with how risk taking behavior correlates with different versions of a dopamine receptor gene a prior study was concerned with risk taking by these individuals in their bridge decisions this study examines financial risk taking the results are interesting even though there were no significant gender differences in risk taking in tournament bridge decisions which are not financial in nature there was a very large gender difference in financial risk taking design participants at the national bridge tournament first completed an incentivized bridge quiz and then took part in the investment task each of participants was endowed with from which they could choose to invest as much as they wished in the risky asset if successful this asset paid times the amount invested i e k the chance of success was p each participant kept whatever amount he or she chose not to invest results the aggregate amounts invested by males and females are highly different on average males invested standard deviation or of the endowment in contrast females invested only standard deviation or of the endowment the difference in investment rates is strongly significant the wilcoxon ranksum test gives z p while a t test gives t p this simple risk elicitation mechanism provides strong evidence that in the field females are substantially more financially risk averse than males dreber and hoffman this study considered the relationship between financial risk taking and the ratio between the length of the index finger and the ring finger this is a biological measure thought to positively correlate with prenatal estrogen and negatively correlate with prenatal testosterone see coates et al for other evidence on the digit ratio and financial risk taking in fact such a relationship was found with a strong correlation between the digit ration and the level of g charness u gneezy journal of economic behavior organization investment in the risky asset these results suggest that prenatal hormones influence risk preferences more than years later the measure of financial risk taking was the investment decision we have been discussing design each of students males and females at the stockholm school of economics was endowed with sek about from which he or she could choose to invest as much as they wished in the risky asset if successful this asset paid times the amount invested i e k the chance of success was p each participant kept whatever amount he or she chose not to invest results overall the average investment was sek with a standard error of males invested sek with a standard error of females invested with a standard error of the percentage point difference is highly significant with a test statistic on investment levels of z ranksum test or t t test both of these tests give significance at p thus females are seen to be definitely more risk averse than males in this study charness and gneezy in charness and gneezy we consider whether psychological biases such as ambiguity aversion and the illusion of control affect the rate of investment in a risky asset design data were collected from large classes at ucsb in which each student who was willing to participate in fact all students was randomly assigned to one of our treatments a total of people participated males and females eight separate sets of instructions ambiguity aversion conditions and four illusion of control conditions were passed out with each person randomly assigned one of these each participant was endowed with units which could be kept or invested in a risky asset if successful this asset paid times the amount invested i e k the chance of success was p in the first ambiguity condition participants were told that there was one container with red balls and black balls and another container with an unknown composition of red or black balls people then chose how much to invest the success color that would pay if drawn and the container from which to draw the ball in the second third condition only the container with the known unknown distribution was mentioned and people chose how much to invest and the success color in the final condition the participant faced the same choice of the containers as in the first condition but was required to pay units out of the endowment if she or he wished to draw from the container with the known distribution in the first illusion condition participants were told that a sided die would be rolled to determine success each person then chose how much to invest success numbers that would pay if rolled and who would roll the die the experimenter or the investor in the second third condition only the investor experimenter could roll the die in the final condition the participant chose as in the first condition but was required to pay units out of the endowment if she or he wished to personally roll the die one of every participants was randomly chosen to actually receive payment at the rate of for every unit each person who was selected then met individually with the experimenter and was paid privately after the realization of the risky asset results once again we see a strong difference in investment behavior across gender in all independent comparisons in table males invested more than females again we can apply the binomial test comparing the average investment for males and females in each of the eight conditions the likelihood that either gender would invest more than the other in all eight table investment with ambiguity aversion and illusion of control treatment avg male investment n avg female investment n illusion free choice illusion investor rolls illusion experimenter rolls illusion costly choice all illusion choices ambiguity free choice ambiguity known only ambiguity unknown only ambiguity costly choice all ambiguity choices all choices g charness u gneezy journal of economic behavior organization treatments is p indicating a significant difference the overall average male investment in the ambiguity illusion treatments was higher than the average female investment charness and gneezy in charness and gneezy we consider whether framing differences affect the choice of investment in a risky asset to the extent that we find a framing effect we examine whether this effect is stronger for men or for women some experiments do not find gender differences in differently framed gambles for example in eckel and grossman participants chose among gambles that differed in expected return and variance and were presented either as a loss or as no loss eckel and grossman found that women are more risk averse across all frames another example is presented in eckel and grossman in press who studied gamble and investment frames with the possibility of losses and a gamble frame with no losses again women were more risk averse than men in all three framings see also powell and ansic on the other hand several studies do find gender differences by frame in an abstract lottery choice schubert et al frame choices as either potential gains or as potential losses they find that women were more risk averse than men in the gain domain frame consistent with the evidence presented earlier for the loss domain gambles however this result is reversed men are more risk averse than women in contextual environment gambles e g investment and insurance schubert et al subjects exhibited no evidence of systematic gender differences in risk attitudes design we conducted experiments at both the university of chicago and ucsb participants at ucsb were recruited by sending out an e mail message to a list of people who had signed up to be contacted about experiments these people were drawn from the general student population the chicago participants were recruited using campus ads people came to the lab individually the instructions were presented and the experimenter answered any questions each person then made an investment choice for the first period and observed the results of the roll of an sided die this continued for periods in each period each participant was endowed with units which could be kept or invested in a risky asset if successful this asset paid six times the amount invested i e k the chance of success was p we paid each person for every units they had aggregated over the course of the periods each individual session lasted min there were separate treatments differing only in some phrasing in the instructions which are shown in appendix a total of people participated in the natural condition males and females and in the frame condition males and females in the natural condition the second sentence of the instructions read that in each period you will receive points while in the frame condition this read in each period you will receive points to invest in a risky asset two sentences followed immediately in the natural condition mentioning that the participant could choose which portion of the points to invest and that the points not invested would accumulate in the total balance in the frame condition these sentences came after intervening paragraphs results the results are shown in figs and and are summarized in table first we see that investment rates differ considerably across treatments the average investment is in the frame condition compared to in the natural condition a wilcoxon mann whitney ranksum test see siegel and castellan using each individual average investment rate indicates that this difference is highly significant z p the small difference in the language used in the instructions makes a big difference in investment behavior suggesting that financial decisions may be easily influenced by the manner of presentation second we see that males consistently invest at a higher rate in each period in both the natural and frame treatments so the average investment for males is always higher than that for females tobit regressions reflecting the censoring at no one invested with investment as the dependent variable find that the t p one tailed test for the gender dummy in the natural condition and t p one tailed test for the gender dummy in the frame condition the gender difference is more significant if we pool the data across treatments finally regarding the question of whether the frame has differential effects on males and females we see that the investment rate increases by points for males and by points for females the difference between these differences is not large only points on the other hand if we instead look at the percentage change in the investment rates we see that the average investment nearly doubles up for females but increases by only just over half for males nevertheless although while it may seem that the framing has a somewhat larger effect on females we do not identify a if we presume a directional hypothesis the likelihood that males invest more than females in all treatments is p we also have data from a natural treatment in which a decision was only made for one period with points worth the average investment for males was and the average investment for females was as shall be seen this differs little from the period natural results g charness u gneezy journal of economic behavior organization natural period fig framing period fig table investment with different framing conditions treatment avg male investment n avg female investment n difference natural frame all choices significant effect thus while our simple framing device has a major effect on the investment rate our evidence that women are more susceptible to this framing is inconclusive yu yu collected data both online and in the laboratory to study the separate effects of alternative feedback information rules and the freedom to change the investment in a repeated environment this approach attempts to decompose into components the effects observed in gneezy and potters a wilcoxon ranksum test on framing differences using each individual overall investment rate gives z while the corresponding test using percentage changes gives z at best the latter test statistic indicates significance at p one tailed test to conduct these tests we ordered the investment rates in each condition and made comparisons directly across the conditions so that the highest investment rates were compared etc we have nm and nf for these comparisons g charness u gneezy journal of economic behavior organization table yu treatment avg male investment n avg female investment n laboratory daily intermediate weekly all laboratory choices 46 internet daily intermediate 58 weekly all internet choices all choices 26 design the participants in the online experiment were recruited using an email list of students who register in order to be invited to participate in experiments plus mba students who took a decision making class students replied to this message the participants were divided randomly into three equal sized groups and they then received the instructions for the experiment about of the respondents actually participated in the experiment after receiving the instructions the participants in the laboratory experiment were recruited using ads posted on campus participants were given instructions that were very similar to those of the online study each investment day took min in all treatments participants were told that the experiment consists of successive investment days each investment day lasted from a m until midnight in each investment day they received points and were asked to choose the portion of this amount between points and points inclusive they wish to invest in a risky option the rest of the points those not invested were accumulated in the participant total balance participants were told that at the end of the three weeks one participant would be chosen at random for actual payment calculated as the sum of the earnings in each of the investment days this person was paid for each points accumulated investing in the risky option meant that in any particular investment day there is a p probability that the investment will succeed and a p probability that the investment will fail if the investment failed the participant lost the entire amount she or he invested if the investment was successful she or he received times the amount invested i e k the computer randomly chose whether the participant won or lost in any given day to make their investment decisions participant had to log on to our webpage using the username and password assigned to each of them personally in an email there were three treatments in the online experiment in the daily condition people could change their investment on every investment day in the intermediate condition people only made investment choices on the first day of each investment week days and the weekly condition was the same as the intermediate condition except that participants only received information about the realization of each day once per investment week results as can be seen from the results presented in fig and table there is a substantial and consistent gender difference in investment choices the average investment for males in each condition is between and higher than the average investment for females half again as much 51 overall this difference is substantially larger in the on line treatments than in the lab condition using an ols regression yu finds that the gender dummy is highly significant and the binomial test gives p two tailed test regression results treating these data as panel data also provide strong support for the hypothesis that men invest more in the risky option than women in each of the treatments other studies in this section we consider all the other studies of which we are aware that gather gender data on risk and investment using this methodology these studies are langer and weber haigh and list fellner and sutter charness and genicot bellemare et al dreber and hoffman and gneezy et al ertac and gurdal and gong and yang these articles examine various aspects of risk taking in multi period designs using the same type of investment choice as described above it is interesting that the subject pools vary widely across these experimental studies some of the studies that used this game did not record the gender of participants e g gneezy and potters and the student data in haigh and list and are hence excluded from our analysis g charness u gneezy journal of economic behavior organization 58 dail y lab conditio n inves tme nt d ay amount in risky option men women dail y on li ne condition inves tme nt d ay amount in risky option men women intermediate lab conditio n inves tme nt d ay amount in risky option men women interm ediate on li ne condition inves tme nt d ay amount in risky option men women weekly lab co nditio n inves tme nt d ay amount in risky option men women week ly on li ne conditio n 13 15 inves tme nt d ay amount in risky option men women fig the language of the instructions the parameters used and the length of the experiment all vary between the experiments we present these data in table we aggregate across multiple treatments in a study as the number of female observations would otherwise sometimes be very small in some case in of the experiments in table men invested more than females only in one experiment in reported in the table there is no gender difference in risk taking note that gong and yang find that males are less risk averse than females in both matrilineal and patrilineal societies we can apply the simple binomial test where the null hypothesis is that there is no difference between the number of studies finding that males are more financially risk averse than females and the number of studies that show the opposite the likelihood that either gender would invest more than the other in all nine studies where there is a difference is p while the likelihood that males invest more than females in every case is p these data support the consistent pattern of greater risk taking by males in investment choices in combination with the data from sections the pattern seems compelling even if we count the data from gneezy et al as indicating greater risk taking by females we have p on a two tailed test or p on a one tailed test g charness u gneezy journal of economic behavior organization 58 table investment choices in other studies study participants periods avg male investment n avg female investment n langer and weber finance students mannheim 58 haigh and list professional traders cbot 9 58 fellner and sutter undergrads jena bellemare et al undergrads tilburg 9 45 charness and genicot undergrads ucla dreber and hoffman students stockholm gneezy et al villagers in tanzania and india 50 ertac and gurdala undergrads turkey gong and yang matrilineal villagers in china 9 gong and yang patrilineal villagers in china a we include only the individual risk decisions where there is a positive expected return from investing in the risky asset a similar gender difference applies in the other cases conclusion the results reported in this article are obtained by using data from previous studies based on one similar design in which the data of interest was recorded independently of the goal of the study the field of experimental economics is growing quite rapidly with experiments being relatively easy to run compared with e g analyzing real world data there is a natural tendency to continue to collect new evidence without fully considering what we might learn from the data that are already available we suspect that as the field of experimental economics matures the approach taken in the current article will become more common and will help to provide answers to important economic questions the answer to the question we posed at the beginning of this article is clear women make smaller investments in the risky asset than do men and so appear to be financially more risk averse we believe that this very clear and consistent result answers an open question in the literature we do not argue that women are always more risk averse than men and clearly research into the boundaries of these findings should be encouraged however one should be careful not to base counter arguments regarding no difference or difference in the other direction on one or two studies rather a more comprehensive investigation of the boundary conditions based a substantial number of experiments and robustness checks should be encouraged as a first step in this direction researchers running experiments should be encouraged to record as much background information about the participants as possible the economic implications of our results are important as investment behavior by men and women an inherently high payoff decision appears to differ a few articles e g sundén and surette finucane et al jianakoplos and bernasek hinz et al bajtelsmit and van derhei investigate allocation of portfolio assets and find that gender is significantly related to asset allocation women portfolios are less risky than men however studying investments in field data has certain limitations for example it is hard to know how investment decisions are reached in households with married couples bernasek and shwiff we believe that the convergence of the laboratory findings such as we report in this article with empirical findings from investment decisions is an important step in understanding the important features of gender differences in risk taking five experiments demonstrate that experiencing power leads to overconfident decision making using multiple instantiations of power including an episodic recall task experiments a measure of work related power experiment and assignment to high and low power roles experiment power produced overconfident decisions that generated monetary losses for the powerful the current findings through both mediation and moderation also highlight the central role that the sense of power plays in producing these decision making tendencies first sense of power but not mood mediated the link between power and overconfidence experiment second the link between power and overconfidence was severed when access to power was not salient to the powerful experiment and when the powerful were made to feel personally incompetent in their domain of power experiment these findings indicate that only when objective power leads people to feel subjectively powerful does it produce overconfident decision making elsevier inc all rights reserved introduction in british petroleum bp executives confidently downplayed potential risks associated with their oil well located in the gulf of mexico assuring regulators that it was virtually impossible for a major accident to occur achenbach months later an oil rig exploded killing workers and resulting in a massive oil leak that spanned more than a mile underwater the executives who have since been accused of an ongoing pattern of overlooking safety precautions could have prevented the accident with greater attention to and preparation for potential hazards burdeau mohr in aol ceo steve case orchestrated a billion deal with time warner the largest merger in history at that time in spite of his assuredness that the new arrangement would lead to sustained profit and growth the deal squandered billion in shareholder value in the first quarter alone and led to his demise as ceo as was the case in these two stories the decisions made by power holders across a multitude of arenas including businesses government religious institutions and nonprofit organizations are often marred with overconfidence e g see hayward hambrick hribar yang li tang malmenier tate furthermore when powerful leaders are plagued with overconfidence the consequences for performance can be detrimental making important decisions in the absence of adequate information hinders not only one own performance and ability to maintain power but often hurts companies stockholders and the general public too as the aol and bp cases highlight in spite of the attention given to the occurrence of overconfidence among the powerful little is known about the social and psychological factors that are responsible for overconfidence among power holders in the current research we attempt to understand the precise nature of this relationship exploring not only what causes it but also when it is most likely to occur we also examine alternative explanations for the presence of overconfidence among the powerful on the one hand it could be that overconfident individuals are drawn to power and or are more likely to obtain high power positions e g see anderson brion if so the implication would be that carefully promoting only those who do not have pre existing tendencies toward overconfidence would help to address the problem alternatively the experience of having power itself may create or produce overconfidence above and beyond one pre existing tendencies making taming the relationship between power and overconfidence a considerably more complicated task in the present research we examine precisely this possibility exploring whether when and why the experience of power may facilitate overconfident decision making see front matter elsevier inc all rights reserved doi j obhdp corresponding author address university of southern california marshall school of business bri exposition blvd los angeles ca united states fax 3582 e mail address nathanaf usc edu n j fast organizational behavior and human decision processes 260 contents lists available at sciverse sciencedirect organizational behavior and human decision processes journal homepage www elsevier com locate obhdp power as a determinant of overconfidence building on recent advances in the power literature we propose that the experience of power exacerbates overconfidence consistent with many others in the field we define power as asymmetric control over valued outcomes e g emerson keltner gruenfeld anderson magee galinsky keltner et al approach inhibition theory of power asserts that power increases the activation of the behavioral approach system bas leading individuals to be especially sensitive to rewarding possibilities a number of recent studies have provided support for this theory after experiencing power individuals pay more attention to positive and rewarding information anderson berdahl anderson galinsky express themselves more freely galinsky magee gruenfeld whitson liljenquist and adopt an orientation toward action anderson berdahl fast gruenfeld sivanathan galinsky galinsky gruenfeld magee magee galinsky gruenfeld furthermore recent evidence shows that experiencing an elevated sense of power defined as the subjective sense that one is powerful and influential regardless of whether this is actually the case anderson john keltner coincides with confidence inducing states such as optimism anderson galinsky risk taking jordan sivanathan galinsky in press and exaggerated perceptions of control over outcomes fast et al building on these ideas we predict that power will via an elevated subjective sense of power lead to an overestimation of one accuracy in decisionmaking domains moreover we predict that this relationship will hold even when such overconfidence is maladaptive these predictions are consistent with the notion that behavioral approach activation is associated with a greater degree of positive and actionfacilitating cognitions and a reduced tendency to deliberate on information that might hinder the active pursuit of one goals keltner et al overconfidence and related constructs the concept of overconfidence is situated in a larger body of evidence in cognitive psychology demonstrating the widespread prevalence of positive illusions and self enhancement biases this literature has repeatedly demonstrated that people tend to view themselves the world and the future more positively than is objectively warranted e g but see klayman soll gonzalez vallejo barlas dunning griffin milojkovic ross dunning heath suls moore cain there are three commonly observed strands of these effects each distinct from the others first overconfidence refers to an inflated sense of confidence in the accuracy of one knowledge and or cognitive estimates for example people are often overly confident about the precision of their answers to various questions posed to them e g russo schoemaker soll klayman because sound decision making typically requires accuracy regarding what one does and does not know overprecision in one knowledge is especially important in organizational settings in the present paper we will focus primarily on explaining the emergence of this distinct type of overconfidence among the powerful overconfidence as defined here is distinct from a second common self enhancement bias known as the better than average effect the latter refers to the tendency to believe that one is above average on desirable categories alicke klotz breitenbecher yurak vredenburg klar medding sarel weinstein for instance an overwhelming majority of people judge themselves to be in the top of drivers in the us svenson a statistical impossibility similar perceptions have been documented in arenas of health taylor brown risk taking fischhoff slovic lichtenstein and negotiations neale bazerman in contrast to overconfidence in one knowledge the better than average effect emphasizes one abilities or probabilities relative to those of other people a third strand illusory control refers to an overestimation of one personal control over outcomes that are either chance based or fall outside the control of the individual langer thompson armstrong thomas illusory control which has been linked to power fast et al is distinct from overconfidence in one knowledge moore healy as the former relates to the perception that through one abilities and or actions she is more capable of influencing outcomes than she is in reality one might for example believe that one can win more money using a slot machine with a lever one can physically manipulate relative to a slot machine with no lever as the latter offers no perceived opportunity for control not only is overconfidence in one accuracy theoretically and empirically distinct from illusory control moore healy but at a basic theoretical level the two constructs speak to different styles of control that humans pursue illusory control maps onto the concept of primary control characterized as attempts to satisfy one need for control by directly changing one environment rothbaum weisz snyder in contrast overconfidence maps onto the concept of secondary control which refers to the goal of gaining control by accurately predicting and understanding one environment rothbaum et al see moore and healy for a more in depth and highly insightful discussion on the similarities and differences between overconfidence in one accuracy the better than average effect and illusory control finally it is worth noting that our current emphasis on overconfidence is also distinct from prior work exploring how power affects judgments and evaluations for example inesi found that power reduces loss aversion by increasing the anticipated value of gains and decreasing the anticipated negative value of losses in contrast our work explores whether power leads people to overestimate the accuracy of their own knowledge thus the two ideas capture different effects associated with the experience of power additionally the present work is theoretically different from georgeson and harris meta analysis that summarized evidence indicating that as people power increases they derogate less powerful others by rating their performances negatively rather than studying derogation in the context of performance evaluations we make a distinct contribution by exploring overconfidence in one own knowledge and its effects on decision making theoretical and empirical contributions in the present research we aim to build on and extend extant theory by making three primary contributions first we seek to show that power leads to overconfidence in the accuracy of one knowledge or overprecision see moore healy overconfidence matters a great deal especially among power holders within organizations as many of their high impact decisions are based on perceived precision of relevant knowledge for example decisions to acquire a company enter a new market or execute a joint venture are all based on the perceived accuracy of one knowledge about the value of the acquired company market forecast and production capacities of the potential partner although the distinction between overprecision and overassessment of one abilities is critical moore healy it is often ignored in the literature the present research examines the potential psychological connection between power and overconfidence above and beyond exaggerated beliefs in one personal abilities or one capacity to n j fast et al organizational behavior and human decision processes 260 control outcomes fast et al specifically we predict that power produces greater overconfidence in the accuracy of one knowledge than those who are less powerful a second advance is to help uncover why power may lead to overconfidence there are at least two plausible mechanisms first it could be that power leads to overconfidence as a result of activating a subjective sense of power leading in turn to greater attention to positive and action facilitating cognitions keltner et al such as overconfidence alternatively the observed relationship might instead be a result of role expectations experienced by the power holder for example it might be the case that the powerful demonstrate extreme confidence in their knowledge in order to display a trait they believe is expected of those in highpower roles this idea is consistent with research showing in a variety of contexts that inhabiting work roles incites pressures to conform to specific standards of behavior for reviews see biddle stryker statham one way to empirically separate these two mechanisms is to hold objective power i e asymmetric control over resources constant while manipulating whether or not one experiences the subjective sense that one is powerful therefore in addition to testing sense of power as a mediator in two of our experiments we block subjective feelings of power but leave in place the expectations that accompany high power roles thus the present research seeks to push the field forward by examining the central role that sense of power has on the effects of power through both mediation and moderation we investigate whether a sense of power mediates the effects of power on overconfidence as well as whether b the effect of sense of power on overconfidence remains when the subjective sense of power is blocked finally we extend the literature by examining the effect of power on decision making performance whereas the illusion of personal control can be adaptive e g taylor brown overconfidence in one accuracy can be problematic especially when making decisions that hinge on precision in such cases power may prevent one from arriving at accurate assessments by generating overconfidence in one knowledge thus we advance prior research by examining whether power induced overconfidence will produce decisions that lead the powerful to lose real money specifically we predict that the experience of elevated power will cause people to underperform on decision making tasks that require accuracy of knowledge relative to those who are less powerful even when they are financially motivated to be accurate overview of the present research we conducted five experiments to test our core hypotheses that power produces overconfidence hypothesis and that the subjective sense of power is the causal mechanism hypothesis wealso hypothesized that the relationship between power and overconfidence would hold even when expressions of overconfidence mean financial losses hypothesis finally we predicted that making power holders feel incompetent in their roles would eliminate the relationship between power and overconfidence hypothesis in experiments we manipulated power by using an episodic recall task and explored whether feeling powerful leads to overconfidence and underperformance in an information task experiment examines this relationship further by assessing mood and subjective sense of power as potential mediators in experiment we assessed power within a work context to examine if this relationship holds even when individuals stand to incur financial losses furthermore in order to help show that any observed effects were caused by the sense of power we made the workplace salient for some and not others predicting that the power overconfidence link would emerge only when participants powerful positions in the workplace were made salient finally in experiment we examined whether lacking a sense of power eliminates the overconfidence displayed by high power actors by manipulating perceived aptitude for a highpower role experiment in experiment we investigated the effect of power on confidence and performance by having participants answer general knowledge questions russo schoemaker we manipulated power by having participants recall a time in which they had power although power is conceived as a structural variable its psychological properties can be activated by recalling past experiences with power galinsky gruenfeld magee and activating power in this manner produces the same subsequent effects as those obtained using structural and role based manipulations of power anderson galinsky fast et al galinsky et al method participants were undergraduates women from a large midwestern who participated in exchange for a payment the experiment involved two conditions a high power condition n and a low power condition n participants came in groups of to the lab and were brought into separate rooms to participate in the experiment in these rooms participants were given a high or low power experiential prime power participants were instructed to recall and write about a personal incident in which they had or lacked power over another individual or individuals galinsky et al in the high power condition participants wrote about a situation in which they controlled the ability of another person or persons to get something they wanted or were in a position to evaluate those individuals in the low power condition participants wrote about a personal incident in which someone else had power over them overconfidence after completing the power manipulation participants were asked to provide answers to a series of six factual questions adapted from russo and schomaker they were required to provide the correct answer by estimating a confidence interval around each of their answers for example one question martin luther king age at time of death required the participant to provide a range in which the correct answer lied with confidence e g years results and discussion we found no effects of sex on confidence levels given the different units involved with each answer i e age distance weight we converted the responses to standardized scores z score we used the mean z score for the six items for all of our analyses as predicted high power participants m sd assigned narrower answer ranges for their confidence intervals than did low power participants m sd t p d these findings support our prediction that power leads individuals to overestimate the accuracy of their knowledge we also computed a mean accuracy measure by averaging the number of times the correct answer fell between the confidence intervals provided by the participant analyses of accuracy age was not recorded n j fast et al organizational behavior and human decision processes 260 revealed that confidence in one answers was inversely correlated with accuracy r p and as a result high power participants m sd were marginally less likely to find the correct answer within their confidence boundaries than were lowpower participants m sd t p d consistent with our hypothesis high power individuals set narrower confidence margins for their answers than did low power participants furthermore this overconfidence led the powerful to be less accurate than low power individuals experiment experiment sought to strengthen the findings from experiment in a number of ways first we included a baseline condition in order to ensure that the high power condition rather than the low power condition was driving the effects second we assessed overconfidence in a work relevant domain testing participants confidence in their estimates of the future performance of potential employees third we sought to more clearly get at overconfidence and underperformance by using a domain where predictions both mattered and had objective answers we chose the domain of professional hockey because it places participants in a role that managers face routinely i e rating and selecting potential employees and additionally because there are objective metrics that are observable and painstakingly recorded in hockey games allowing for a precise measurement of player performance given the objective data on past and future performance we could compare participants predictions to actual performance as in experiment we manipulated power with a recall prime galinsky et al after manipulating power we asked participants to take on the role of an organizational representative making hiring assessments and recommendations for a hockey team and select the player they believed would perform the best for their team we then measured confidence in their estimates for all players because we chose actual statistics from eventual nhl players we were able to assess the accuracy of participants judgments method participants were students women men at a large western university and a european business school who participated either for course credit or payment the two samples produced the same patterns of results so we combined them for further analyses the experiment involved three conditions highpower n low power n and neutral power n after the manipulation participants completed a hiring task which assessed overconfidence power manipulation as in experiment participants were instructed to recall and write about a personal incident in which they had or lacked power over another individual or individuals or recall and write about their previous day galinsky et al in the neutral power condition participants wrote about their experiences during the previous day this ensured that all participants completed the same task recalling and writing about past events while being primed with different levels of power galinsky et al overconfidence after completing the power manipulation participants were asked to assess and draft five athletes i e potential employees for a hockey team they were asked to select the player among five minor league hockey players who they thought would perform the best for their nhl team the following year they were provided with each player minor league statistics see appendix a and were asked to estimate how many points i e goals scored and assists tallied each would accrue in his first year in the nhl and to estimate confidence intervals around each of their answers in other words if they predicted that player a would score goals they would then estimate a number below and above such that they were certain the player actual performance would fall within that range e g and accuracy we were interested not only in participants confidence levels but also in whether or not they were correct in their predictions of how many goals their top player would score to assess whether or not power hindered accuracy we measured the difference between the player actual performance i e how many total points goals plus assists accrued by the player in his first year in the nhl and the closest confidence interval provided by the participant to illustrate if a participant selected confidence intervals of and but the player total points was the participant score would be i e 90 minus correct predictions i e points that fell within the confidence intervals received a score of because there was no distance between the player actual score and the range provided by participants results and discussion six participants reported familiarity with the hockey players and thus were removed from analyses there was a main effect for sex men in the sample had larger confidence intervals m sd than women m sd p and were also more accurate m sd versus m sd p respectively we repeated all analyses with sex as a covariate and found the same patterns of results and significance levels thus we do not discuss this variable further overconfidence consistent with our predictions a contrast comparing highpower participants with participants in the other two conditions produced a significant effect such that high power participants m assigned narrower confidence intervals for their top choice than did low power participants m and neutralpower participants m t p d see table this pattern extended beyond the top selected player the same general pattern was present across all five players t p d additional tests showed that highpower participants had narrower confidence intervals for their top player than low power participants t p d the difference between high power and neutral power participants was also significant t p d there was no difference between low power and neutral power participants accuracy as predicted a contrast comparing high power participants with participants in the other two conditions produced a significant effect such that the distance between the player actual performance and high power participants confidence intervals m was greater than that of low power participants m and neutral power participants m t p d see table this tendency extended beyond the top selected player the same general pattern was marginally present across all five players t p d n j fast et al organizational behavior and human decision processes 260 additional tests showed that high power participants were less accurate than neutral power participants t p d the difference between high power and low power participants approached significance t p d there was no difference between low power and neutral power participants consistent with our predictions power led individuals to overestimate the accuracy of their knowledge high power individuals set narrower confidence margins for their picks than did lowpower participants furthermore because of their overconfidence high power individuals had significantly lower accuracy in their predictions than low power and neutral power participants specifically those in the high power condition set confidence intervals for player performance that were significantly further away from the player actual performance experiment experiment explored the psychological processes that produce the causal link from power to overconfidence shown in experiments and we have proposed that a sense of power i e the subjective sense that one is powerful is the driving force behind this increased overconfidence however it is also possible that having power and or recalling a time when one feels powerful could induce positive affect which in turn might lead to overconfidence although several studies have found either that mood is not affected by power or that it does not mediate power effect on other variables e g anderson berdahl study fast et al galinsky et al smith trope we wanted to explicitly rule out this alternative explanation for the relationship between power and overconfidence in order to do this we assessed the effects of manipulated power on confidence sense of power and mood method participants were adults women m age sd age recruited from amazon mechanical turk see buhrmester kwang gosling for a payment of 75 the experiment involved three randomly assigned conditions high power n 49 neutral n and low power n they then responded to several items measuring sense of power confidence and positive and negative mood power manipulation participants completed the same power manipulations highpower neutral power and low power used in experiment confidence confidence was assessed with four items assessing the degree to which participants felt confident in their own thoughts and knowledge i feel confident in my thoughts i feel confident in my beliefs i am certain of my knowledge and i m very sure about what i know items were rated on a point likert scale anchored by strongly disagree and strongly agree the scale was reliable a m sd sense of power we used the eight item sense of power scale anderson galinsky anderson oliver keltner in press to assess how powerful people felt sample items include if i want to i get to make the decisions i can get people to listen to what i say i think i have a great deal of power even if i voice them my views have little sway reverse scored all items were rated on a point likert scale anchored by strongly disagree and strongly agree the scale was reliable a m sd positive and negative affect we measured affect with the positive and negative affect schedule panas watson clark tellegen participants rated how they presently felt by rating positive e g interested excited proud alert and negative upset nervous jittery afraid items all items were rated on a point likert scale anchored by strongly disagree and strongly agree the items were reliable for both positive affect a 92 m sd and negative affect a m 51 sd results and discussion there was no effect of sex on confidence sense of power or affect age had a marginal positive effect on confidence r 15 p 06 and positively correlated with positive affect r p so we repeated all analyses with age as a covariate and found the same patterns of results and levels of significance thus we do not discuss this variable further consistent with experiments and a contrast comparing high power participants with participants in the other two conditions produced a significant effect high power participants m sd reported more confidence than participants in the neutral m sd and low power m sd conditions t p d additional tests showed that high power participants were more confident than low power participants t p d the difference between high power and neutral power participants revealed the same pattern but did not reach significance t p d there was no difference between low power and neutral power participants we next tested whether the power manipulation led to a greater sense of power as predicted a contrast comparing high power participants with participants in the other two conditions revealed that high power participants m sd felt more powerful than participants in the neutral m sd and low power m sd conditions t p d additional tests showed that high power participants had a greater sense of power than low power participants t p d the difference between high power and neutralpower participants was also significant t p d there was no difference between low and neutral power participants we also assessed the degree to which the power manipulation led to positive affect a contrast comparing high power participants with participants in the other two conditions indicated that high power participants m sd felt marginally more table effects of power on confidence i e confidence intervals and accuracy i e distance from correct answer related to performance predictions for selected player experiment low power baseline high power m sd m sd m sd confidence intervals distance from correct answer 34 75 25 note smaller confidence intervals represent higher levels of confidence larger accuracy distance scores represent lower accuracy n j fast et al organizational behavior and human decision processes 260 positive than participants in the neutral m sd and low power m sd conditions t p d this was driven primarily by a difference between the high power and neutral power conditions t 50 p d neutral power participants also reported marginally more positive mood than low power participants t p d there was no difference between high power and lowpower participants for negative affect a contrast comparing high power participants with participants in the other two conditions indicated that high power participants m sd did not report less negative affect than participants in the neutral m sd and low power m sd conditions t p 13 d 25 no contrasts were significant finally we examined subjective sense of power as a possible mediator of the effect of the power manipulation on confidence because there was a marginal effect of power on positive affect we also examined positive affect as a possible mediator we used preacher and hayes bootstrapping procedure to test the simultaneous mediation of each variable for the effect of the high power manipulation on confidence low power and neutral conditions were combined as they produced the same patterns of effects for both confidence and sense of power as shown in fig sense of power but not positive mood mediated the effect of power on confidence positive mood did not mediate the effect of power on confidence as the bias corrected bootstrapped confidence intervals for positive mood included zero indicating that mediation through this path was not significant ci 00 in contrast sense of power mediated the link between power and confidence as the bias corrected bootstrapped confidence intervals for sense of power did not include zero demonstrating that mediation through this path was significant ci 38 we ran additional analyses using a level coding scheme for power and found the same patterns and levels of significance conceptual replication we conducted a conceptual replication of this study using a shortened version of each of the scales and replicated all the effects power led to higher confidence higher sense of power but not more positive affect than low power and neutral conditions as predicted positive sense of power did mediate the effect of power the confidence did not include zero ci but positive mood was not a significant mediator confidence intervals for positive mood included zero ci to replicating experiments and power led to greater confidence in one own thoughts and knowledge furthermore this effect was mediated by the decision maker subjective sense of power providing important insight into the mechanism in short it is when manipulations of power produce a subjective experience of power that they lead to overconfidence experiment in the next experiment we sought to extend the effects observed in the previous three experiments first we examined whether we would observe the same power overconfidence relationship in the field we did so by assessing whether holding a position of power in one place of work leads to overconfidence to help rule out alternative accounts namely the possibility that overconfidence leads to power rather than our hypothesized direction we manipulated the salience of the high power role for half of the participants we made their positions at work salient just before they responded to our measure of overconfidence i e we measured work positions and level of power before assessing overconfidence for the remaining half positions were not made salient i e we measured work positions and level of power after assessing overconfidence if the link between power and low power high power low competence feedback high competence feedback fig amount of won lost as a function of power and competence feedback experiment p p p power manipulation 64 sense of power positive affect confidence in precision fig subjective sense of power but not positive affect mediates the effect of power manipulation on confidence study 254 n j fast et al organizational behavior and human decision processes 260 overconfidence is due to the tendency for a disposition based overconfidence leading to positions of power we would expect to see a positive correlation between power and overconfidence regardless of power salience however if the psychological experience of power leads to overconfidence we would expect a two way interaction such that the link between power and confidence will be more pronounced when one power is made salient than when it is not thus we hypothesized that the positive link between power and overconfidence would emerge only among participants with high power positions and whose work roles were made salient this interaction would help establish that the psychological state of feeling powerful drives the effects of power on overconfidence in order to create a conservative test of our hypotheses we motivated accuracy by providing the chance to earn additional money we accomplished this by informing participants that they could place bets on difficult trivia questions and then assessed their performance method eighty adults 57 women from various professions m age 35 sd age 04 were recruited from a national database maintained by a university and paid the participants occupied a variety of roles varying in levels of power across diverse professional domains power participants rated the degree to which they had formal power at work by completing a item scale see fast chen e g to what extent do you have a position of power at your place of work a m sd role salience we manipulated whether power role was salient by either measuring work related power before assessing overconfidence highsalience condition or after assessing overconfidence low salience condition role salience condition was dummy coded low role salience high role salience overconfidence participants were given the opportunity to bet on five individual questions before viewing the actual items the questions were as follows what is the most popular first name in the world what is the capital of azerbaijan how many countries were members of the european union as of june which us state instituted the nation first mandatory seat belt law in and which north american city has the following subway stops kendall square central square and porter square to motivate participants we reminded them that they would receive for the study but could also purchase or bet on up to five trivia questions for each correct answers would have a payoff of resulting in a net gain of and incorrect answers would pay nothing resulting in a net loss of importantly we explicitly told participants that the questions were extremely difficult stating note that each of the questions is designed to be fairly difficult on average people tend to get them correct around of the time given this information a rational decision maker would elect not to place any bets instead electing to keep the per question a chance of being correct leads to an expected value calculation of a loss of per bet i e expected value calculation on winning is for each bet after indicating whether or not they wanted to bet on the questions they answered the actual questions because participants were informed of the extremely low odds of winning and made their bets before they had a chance to see the questions the choice to bet on a question represents overconfidence in one general knowledge we measured the number of bets made which ranged from to m sd additionally we measured the average amount of money lost scores ranged from a loss of to a gain of m loss sd 41 results and discussion there was no effect of sex on overconfidence age was negatively correlated with overconfidence so we controlled for it we also controlled for income in order to rule out the possibility that personal wealth rather than power explains people willingness to make bets we tested our main prediction using multiple regression analysis power was mean centered and treated as a continuous variable the number of bets placed was then regressed onto power role salience and the two way power x role salience interaction term there were no main effects but the predicted interaction between power and role salience emerged b 45 t p as hypothesized among participants whose roles were salient power was associated with greater overconfidence b 37 t 35 p in contrast when confidence was measured before the work position and level of power had been made salient there was no relationship between power and overconfidence b t 38 p the interaction on money won was marginally significant b t p as hypothesized among those whose work role was salient greater power was associated with a significantly greater loss of money b t p and among those whose work role was not salient power was unrelated to financial loss b t 31 p taken together these findings support our predictions when work roles were salient participants in high power positions at work displayed overconfidence in their knowledge betting more often on trivia questions even when the odds of winning were glaringly low in addition strengthening the pattern observed in experiments and high power participants underperformed such that they lost more money than did their lower power counterparts importantly this heightened overconfidence and underperformance was present only among high power participants whose power was made salient this suggests that the overconfidence was driven by a subjective sense of power rather than being simply the result of overconfident individuals achieving high power roles experiment taken together the results of the previous experiments indicated that power increases confidence in one knowledge by producing a subjective sense of power sense of power mediated the effects of power on confidence in experiment and salience of power moderated overconfidence and the resulting loss of money in experiment the next experiment further explored moderation by actively hindering the sense of power recent findings indicate that power fails to produce its typical effects e g self enhancement when the power holder is made to feel incompetent fast gruenfeld or illegitimate lammers galinsky gordijn otten thus we predicted that the effect of power on confidence observed in our previous experiments would be eliminated if the power holder was made to feel personally incompetent to test this possibility we conducted a laboratory experiment in which we assigned people to high or low power roles and then provided either false negative or positive feedback about their aptitude to perform well in a high power role we then gave n j fast et al organizational behavior and human decision processes 260 participants the chance to bet money on a series of trivia questions consistent with the idea that overconfidence is driven through a sense of power we hypothesized that the effect would disappear among those who had received negative feedback about their personal competence in addition we also used a more conservative test of overconfidence than the one used in experiment instead of receiving information about the difficulty of the questions and betting before viewing the actual questions participants in the present study had the opportunity to view and answer the question before betting on the likelihood of a correct answer method forty three students from a large western were paid 8 for their participation they were randomly assigned to play high or low power roles participants were directed to one of two rooms and seated at partitioned cubicles so that they could not see any other participants power manipulation participants were instructed that the purpose of the study was to examine social interactions in a work based environment they were then assigned to play the formal role of either supervisor high power condition n or production worker low power condition n the instructions stated that each participant had a partner in an adjacent room and that the supervisor would be guiding the production worker on a task and then evaluating and deciding whether to reward the production worker performance these instructions were corroborated with a clipboard next to each of the supervisor computers the top page was entitled supervisor and had categories in which to evaluate the production worker at the bottom of the page were two checkboxes indicating that my production worker deserves a bonus based on performance and my production worker does not deserve a bonus based on performance as such the high power participants were aware that they occupied a high power role that afforded the ability to evaluate and reward their partners power pilot test to ensure that the power manipulation was effective we conducted a pilot test with students participants using a point scale reported greater perceived access to power in the supervisor role m 82 sd 77 than in the production worker role m 46 sd t p thus our power manipulation was effective personal competence manipulation all participants were asked to complete an assessment of their leadership aptitude entitled the leadership aptitude scale the scale consisted of 15 items designed to have high face validity but to be generic enough to have an ambiguous right answer in addition participant received the following instructions there are no right or wrong answers to the following questions only personal preferences please indicate your answer and answer each question honestly the scale consisted of yes no items e g have you ever held a leadership position at a place of employment do you feel you have strong verbal communication skills and 11 likert scale items e g when participating in a group project to what extent do you tend to dominate the group upon completing the task they were randomly assigned their scores via the computer five categories of scores were given poor fair average good and excellent and participants saw that their scores either fell in the range of fair low competence condition n 21 or excellent high competence condition n all participants also received the following statement the leadership aptitude scale las identifies the extent to which people are a good fit for positions of leadership and authority people who score high on the scale are described as assertive influential and a strong fit for leadership roles they tend to have the ability to skillfully define problems identify the highest priority issues and understand multiple perspectives finally high las individuals tend to accurately identify personal mistakes encourage and seek out constructive criticism and adjust their behavior accordingly in other words leadership comes naturally to them low scores do not necessarily mean that a person would be a bad leader but rather that the person is not a natural fit and may not be as competent as others competence pilot test to ensure that the competence manipulation was effective we assessed perceived competence in the pilot test participants using a point scale reported greater perceived competence associated with the high competence manipulation m sd 58 than the low competence manipulation m sd t 12 p thus our competence manipulation was effective overconfidence to assess overconfidence participants could bet on six trivia questions from their allotment of 8 in contrast to experiment participants did not receive information implying that betting would be irrational rather each of the six items had only two answers to chose from e g question according to the census which state has the higher population answer nevada or virginia indicating no less than a 50 chance of being correct importantly participants read and answered each question before confidence was assessed after reading each question and selecting an answer participants indicated how confident they were in their answer by choosing whether to bet money on it confidence in this study then was how much money participants won lost overall ranging from a loss of to a gain of m sd 94 results and discussion one participant was excluded for correctly guessing the purpose of the experiment there were no main effects of power or competence on overconfidence but the predicted power x competence interaction emerged f 41 p as hypothesized among high power participants those with positive feedback regarding leadership aptitude lost more money m 82 sd than did those who received negative feedback m 00 sd 56 t 14 p d among lowpower participants the reverse pattern occurred though it was not significant t 70 p 11 d in this case participants who received positive feedback won money m sd 76 whereas those who received negative feedback lost money m sd 78 overall high power high competent age and sex were not recorded because the dv money won ranges from to it was in effect a 13 point scale the mean was close to zero i e m which could be likened to 19 on a 13 point scale this puts the high standard deviations in context in order to examine our effects in different ways we also added to the scores making all scores positive and assessed results for the square root of the scores both by testing for an interaction and by comparing the high power low competence condition to the other three we still obtained the same patterns of results thus strengthening our findings n j fast et al organizational behavior and human decision processes 260 participants lost more money than participants in the other conditions t 38 2 03 p d 66 see fig these findings offer yet further support for the notion that power can lead to overconfidence and underperformance in decision making tasks as in experiment powerful participants lost more money but importantly this effect was eliminated when the powerful were made to feel incompetent along with the results of experiments and this supports the notion that overconfidence by the powerful is driven by a subjective sense of power general discussion five experiments demonstrated that the psychological experience of power can lead to overconfidence in decision making tasks we used multiple instantiations of power and overconfidence including an episodic recall task experiments a measure of power in the workplace experiment and random assignment to high and low power roles experiment across the experiments we consistently found support for the prediction that power increases overconfidence in the accuracy of one thoughts and beliefs furthermore the effect persisted even when participants were financially motivated to be accurate experiments and the current research established the critical role that the sense of power plays in the power overconfidence link first sense of power but not mood mediated the link between power and confidence in experiment second two variables linked to a sense of power moderated the effect of power on overconfidence and money lost the link between power and overconfidence was broken when one access to power at work was not made salient to participants experiment and when power holders were made to feel inadequate in the domain of power experiment thus power appears to lead to overconfidence by activating a subjective feeling of power in contrast to the present research overconfidence is often treated as an individual difference variable e g parker fischhoff accordingly it could simply be the case that overconfident individuals are simply more likely to obtain power in line with this notion recent evidence indicates that overconfident individuals are indeed more likely than their less confident peers to obtain power anderson brion importantly by taking an experimental approach the present research shows a causal relationship between power and overconfidence specifically we find that the former increases the latter experiments 2 and demonstrated that random assignment to high power treatments elevated expressions of overconfidence among participants except when they received feedback indicating a lack of competence experiment furthermore experiment demonstrated a link between power at work and overconfidence only when the power was made salient to participants thus we are able to rule out individual differences in overconfidence as the driver for our effects the present research is not without limitations our experiments were conducted in both laboratory and field settings but it would be interesting to assess the presence of overconfidence more systematically with field studies conducted in the workplace including the use of independent and dependent variables with greater face and construct validity some of our studies were conducted with small sample sizes so replicating the effects observed here with larger sample sizes would be beneficial our results indicate that lacking perceived competence reduces overconfidence among the powerful but other moderators would be interesting to investigate as well another notable limitation in this and related work on the psychology of power is the lack of attention to power longitudinal effects we are unable to answer the question as to whether the effects observed here will increase or instead decrease over time contributions the present findings offer several key contributions to both the power and decision making literatures first they move beyond existing work on power to show that power produces a form of overconfidence referred to as overprecision i e overconfidence in the accuracy of one judgment see moore healy not only is this form of overconfidence documented in the present studies distinct from the better than average effect and the illusion of control but it is also more proximal to many of the overconfident decisions committed by organizational actors e g decision making in the context of mergers and acquisitions thus it is both theoretically and practically relevant to organizational science second thesefindings contribute toa small but growing literature on the effects of power on performance although power improves performanceoncertain tasks e g seeoverbeck park our results indicate thatpower can harmperformance on tasks that require careful deliberation and accuracy a common task for many leaders if the sole task facing bp executives had been to speak passionately persuasively and with charisma about the chances of success in their oil wells the effects of a high sense of power would likely have improved their effectiveness however overprecision is not nearly as adaptive when accuracy based decisions have consequences for both organizations and society the present findings indicate that the effects ofpower left unchecked may serve to hinder their performance on such tasks also see see morrison rothman soll althoughpower is often self reinforcing which leads to stable hierarchies magee galinsky the current findings suggest one potential path where power leads to its own demise if the powerful bet their resourcesonmisguidedprecision theymayfindthemselves stripped of their power we add however that there may be some contexts in which overconfidence in one knowledge could be adaptive for example entrepreneurs often work in domains marked by limited knowledge and high risk thus feeling overconfident in one knowledge may help in that it could provide the confidence needed to keep moving forward without giving up third the present research advances our theoretical understanding of power psychological and behavioral effects by showing that the link between power and overconfidence is driven by a sense of power the link was mediated by sense of power experiment and moderated both by the salience of power experiment and by the power holder faith in his or her capacity to effectively fulfill the high power role experiment the implication of this effect is that in order for a position of power to produce overconfidence and perhaps other forms of positive illusions one rank must be made salient and accompanied by a sense of power to the degree that an actor lacks perceived competence in the domain of power he or she will be less likely to display overconfidence and other approach related tendencies our results offer important implications about which power holders are especially likely to demonstrate overconfidence and how it might be reduced it could be useful for future work to examine additional factors that moderate the sense of power among the powerful including personality traits type of power possessed e g french raven and the nature of one power motive e g prosocial versus antisocial magee langner the present findings also advance the decision making literature despite the critical importance of decisions made by powerful individuals in organizations and society surprisingly little research has been done to specifically examine the effects of power on decision processes the present work indicates that power may harm decision making particularly when accuracy is critical it is our hope that future work can build on these findings by identifying cognitive repairs heath larrick klayman to alleviate overconfidence among high powered individuals a fait accompli for organizational actors for example powerful decision makers n j fast et al organizational behavior and human decision processes 260 table players and their minor league statistics experiment 2 player a games played goals scored assists total points goals assists points per game height 50 1000 weight shoots right handed birthdate january 22 birthplace sweden 43 43 86 37 player b games played goals scored assists total points goals assists points per game height weight shoots right handed birthdate mar birthplace united states 21 28 41 player c games played goals scored assists total points goals assists points per game height weight shoots right handed birthdate february 1990 birthplace canada 58 105 72 player d games played goals scored assists total points goals assists points per game height 100 weight shoots left handed birthdate october 2 birthplace canada 29 67 96 43 player e games played goals scored assists total points goals assists points per game height 100 weight shoots right handed birthdate october birthplace canada 8 29 37 n j fast et al organizational behavior and human decision processes 260 may overcome their overconfidence not only via commonly recommended routes such as assigning a devil advocate or surrounding oneself with diverse opinions but also by exposing themselves to situations that place checks and balances on their sense of power alternatively perhaps merely reminding oneself of past mistakes in judgment could be enough to break through the high power bubble in which the powerful often reside finally the results in our experiments highlight an interesting parallel between power and narcissism narcissistic individuals tend to display hubristic overconfidence in addition to engaging in risk taking behaviors chatterjee hambrick other work indicates that narcissists seek out and are selected into positions of power brunell et al our findings uncover another piece of the puzzle by demonstrating that obtaining power can lead to some of the same tendencies observed in narcissists future research exploring this possibility as well as the potential moderating effects of narcissism on responses to power would be fruitful conclusion the results of five experiments indicate that feeling powerful exacerbates overconfidence in so doing they highlight the presence of an interesting yet disturbing cyclical pattern not only do overconfident people tend to acquire roles that afford power anderson brion but the subjective sense of power brought on by these roles causes people to become further overconfident in their own knowledge the present findings offer a compelling explanation for why the powerful often underperform and or lose their power prematurely overconfident decision making for example bp is projected to lose a staggering amount of money some estimates are as high as 100 billion as a result of the decisions that led to the oil spill described previously tony hayward the ceo of bp lost his high power role during the disaster steve case ceo of aol also lost his position of power after the merger with time warner failed to produce positive results similar stories can be told of leaders who make ill informed decisions to wage unwinnable wars launch new products that are destined to fail or restructure their organizations in ways that neglect key competencies finding practical ways to soften and or hold in check the causal relationship between power and overconfidence represents an important endeavor for future research helping the powerful safely escape this perilous aspect of power is not only in the interest of power holders but is also in the interest of all who are daily impacted by their decisions in this article we explore the issues that surround within subject and between subject designs we describe experiments in economics and in psychology that make comparisons using either of these designs or both that sometimes yield the same results and sometimes do not the overall goal is to establish a framework for understanding which critical questions need to be asked about such experimental studies what authors of such studies can do to ameliorate fears of confoundedness and which scenarios are particularly susceptible to divergent results from the two approaches overall we find that both designs have their merits and the choice of designs should be carefully considered in the context of the question being studied and in terms of the practical implementation of the research study elsevier b v all rights reserved introduction a fundamental characteristic of experimental approaches to economic studies is that researchers can observe behavior in an abstract environment that they control ideally by exposing participants to different treatments one is able to achieve identification of causality there are two primary ways in which experimenters can construct these environments in a within subject designed experiment each individual is exposed to more than one of the treatments being tested whether it be playing a game with two different parameter values being treated and untreated answering multiple questions or performing tasks under more than one external stimulus with such designs as long as there is independence of the multiple exposures causal estimates can be obtained by examining how individual behavior changed when the circumstances of the experiment changed in a between subject designed experiment each individual is exposed to only one treatment with these types of designs as long as group assignment is random causal estimates are obtained by comparing the behavior of those in one experimental condition with the behavior of those in another in this article we explore the issues that surround each of the two design approaches we describe experiments in economics and in psychology that make comparisons using either within or between designs or both that sometimes yield the same results and sometimes do not the overall goal is to establish a framework for understanding which critical questions need to be asked about such experimental studies what authors of such studies can do to ameliorate fears of confoundedness and which scenarios are particularly susceptible to divergent results from the two approaches corresponding author e mail addresses charness econ ucsb edu g charness ugneezy ucsd edu u gneezy see front matter elsevier b v all rights reserved doi 1016 j jebo 2 g charness et al journal of economic behavior organization 8 overall we find that both designs have their merits and the choice of designs should be carefully considered in the context of the question being studied and in terms of the practical implementation of the research study in our view between subjects designs are more conservative and one should be cautious about carry over and demand effects in within subjects designs however within designs lend themselves to more powerful econometric techniques and in many cases are a closer match to a theoretical perspective we discuss how one might ameliorate the issues of concern regarding within designs in the remainder of this article we provide an overview in section 2 and some simple examples in section we discuss experiments where the two different methods led to different results in section and to similar results in section we describe some econometric issues in section and conclude in section 2 overview both within and between designs we will henceforth use between and within have their proponents and yet it seems clear there are advantages and disadvantages to each approach so the issue is more nuanced within designs may lead to spurious effects through respondents expecting to act in accord with some pattern or attempting to provide answers to satisfy their perceptions of the experimenter expectations this is known as a demand effect according to which participants in experiments interpret the experimenter intentions and change their behavior accordingly either consciously or not rosenthal white demand effects are likely to be stronger in a within design within analyses have three main advantages with respect to between analyses first their internal validity does not depend on random assignment second in many frameworks they offer a substantial boost in statistical power finally they are more naturally aligned with most theoretical mindsets a theorist is likely to imagine an agent in a market reacting to a price change not two agents in separate markets with different prices however in environments where an individual is likely to only face a single decision a between design might have more external validity the disadvantages to within analyses are essentially a slew of confounds to identification that may be introduced because of the necessity of exposing each subject to multiple treatments one has to worry about the order of exposure affecting the reference and framing of treatments we emphasize that this is a gross simplification of the distinction between the two approaches and that what constitutes exposure is critically dependent on the research question for example if the researcher question is how exposure to x affects reactions to price changes a between subjects design involves observing two group of individuals react to price changes one group in the presence of x and the other group outside of it in such a between design the fears that we have highlighted so far in the context of within designs are now present in both treatments if exposure to x affects the formation of biases within subjects then the between difference in behavior will not be the causal effect of x between designs typically have no natural anchor thus results inherently have substantial noise and may miss important and real patterns real world problems about whether to make a particular decision are often posed as between subjects choices about which decision to make may be considered to be within subjects between analyses are statistically simple to perform as long as random assignment is achieved across groups little sophistication is required even when the games are extended beyond one round if two groups play rounds and one group is treated while the other is not we can compare between the two groups the problem here is that statistical power is hard to come by because in a strict sense each group can only provide one independent data point this is exacerbated by the fact that the nature of laboratory and field experiments generally lends itself to considerably smaller samples than is typically available with field observational data and that between analyses have severe limitations in relation to testing a large parameter set if we are interested in behavior under several variants of a game then we have a tradeoff between statistical power and the number of variants that we can test choosing a design means weighing concerns over obtaining potentially spurious effects against using less powerful tests opinions on this issue vary across the experimental community we ourselves tend to prefer between designs whenever these are practical as we believe these represent more conservative tests and we would rather err on the side of caution nevertheless one must consider the context when making this design choice a large field within experimental economics deals with the evaluation of utility theories these theories are formulated to describe individual responses to different choices given that we might be quick to decide that a between designed experiment that evaluates a theory about utility is unnatural if individual a is risk averse over gains while individual b is risk seeking over losses could we really conclude that individuals in general have mirrored preferences kahneman and tversky demonstrate multiple failures of expected utility theory from questionnaire data using a between design they expose some to gambles over gains and other to gambles over losses they observe that risk averse preferences on positive prospects are mirrored by risk seeking preferences on negative prospects they call this the reflection effect soon after this hershey and schoemaker criticized the between results of kahneman and tversky on the basis that a between analysis does not accurately represent a test of expected utility theory because no individual preference reversals are occurring they use the results from a within designed experiment to claim that the preferences demonstrated are not consistent with reflectivity and thus prospect theory however they make multiple serious mistakes with the design in two out of three treatments all of the loss questions were presented before all of the gain questions while in the third treatment they were presented side by side to emphasize the experiment focus on reflectivity furthermore the order g charness et al journal of economic behavior organization 8 of their questions within each section was never varied nor was the order of the options always presented clearly there is ample room for the biases discussed earlier to influence results here budescu and weiss use a within analysis of the same issue but take into account all of the factors ignored by hershey and schoemaker they randomize the order and presentation of their gambles they also use irrelevant gambles within lab sessions to try and minimize salience of earlier choices these irrelevant gambles were also varied across treatments to ensure that they were not an additional source of bias their results support the original kahneman and tversky finding the main lesson here is that achieving proper identification can often be more important than providing an exact test of theory while between analyses can be theoretically less palatable we should remember that random assignment is a powerful tool that we may need to trust to produce useable results furthermore this issue again demonstrates the importance of addressing potential sources of bias introduced by a within design that said budescu and weiss affirm that with careful and clever design one can access their statistical and theoretical advantages simple examples wtp elicitations in terms of examples one basic experimental setting in which the issue of choosing a within or a between design arises is a willingness to pay wtp elicitation a researcher may ask her participants how much they would be willing to pay for a sandwich in their neighborhood bakery and then how much they would be willing to pay for the same sandwich in the airport instead the researcher could ask half of the participants how much they would pay at the bakery and the other half how much they would pay at the airport just laying out this simple experiment makes clear an immediate attraction of a within subject design here the experimenter gets twice as much data with the same number of individuals also immediately apparent is the fact that the experimental environments of the two methods are fundamentally different because regardless of the order that the questions are asked in the within analysis subjects have a reference or comparison point when responding to the second question since an experimenter cannot un ask it in order to reset the individual to a resting state unwanted psychological sources of variation are introduced once any question is asked an early argument in this spirit is made in grice who criticizes the common use of surveys and within experiments in psychological studies for non independence of questions and tasks poulton specifically criticizes within studies for ignoring what he calls range effects this refers to the fact that exposure to a range of values in the lab affects subjects by lending contextual comparison to all scenarios other than the first in a methodological paper greenwald criticizes within designs based on the effects of practice sensitization and carry over that confound causality he outlines when within designs are problematic mainly as a function of the type of question being asked by the researcher all these papers argue that one should avoid these designs when the experimenter is interested in behavior in the absence of practice when exposure to multiple treatments makes the individual overly sensitive to variations between the treatments and when treatments have persistent effects if we wish to use a within design we need to understand that exposure to multiple scenarios has psychological consequences however the fixes may not always be obvious sticking with the example of wtp elicitation imagine that in the bakery airport experiment described above we vary the order of the scenarios presented to each individual in the within study but their elicited value under the second scenario is always biased by their exposure to the first pooling across individuals exposed to the bakery first we have a good measure of the bakery value and a bad measure of the airport value the opposite is true for individuals exposed to the airport first the natural thing to do would be to throw away the dirty measures and just compare the clean ones but now we are back to square one with a between analysis if we average the dirty and clean measures and looked at the difference between the two elicited values we would need to maintain the assumption that the biases are of the same size and direction i e the bias is independent of the scenario kahneman and ritov goal was establishing wtp among other things for an assortment of public goods they use a survey that presents individuals with a headline and then asks for a response individuals repeat this process for a number of headlines worried that carry over and range effects confound their within analysis they analyze the correlation between question number and response within each individual they find no substantial correlation and thus conclude independence of responses this is a good simple example of problem recognition and response but this is not a perfect fix to the problem for example frederick and fischhoff measured wtp for a variety of goods with within and between designs the quantities of each good they were rating were varied in both designs individuals got the direction of the difference in wtp correct between the low and high quantities but it was much bigger by a factor of 2 in the within design the authors suggest that in the within design subjects feel more compelled to differentiate their answers by observing both scenarios at once and having to contrast them a good example of why these biases exist comes from the literature on evaluability when we consider how much we value one product in isolation we think about the amount of enjoyment we will receive when we consume it when we consider whether or not we value one product more or less than another we need to compare the products directly the literature on evaluability focuses on the fact that when we are forced to make direct comparisons between products there may be features of the products that are very easy to compare think speaker wattage or thread count of sheets and features that are not think aesthetic design of speakers or color matching with sheets if we overweight easily evaluable characteristics when we have to compare two products decision under joint evaluation of products could diverge from decisions under separate evaluation the application of this idea to the within versus between paradigm is straightforward because within g charness et al journal of economic behavior organization 8 designs necessitate exposure to more than one product in the case of wtp elicitation individuals could be using different criteria to supply their wtps in within designs than in between designs where they evaluate only one product in isolation this line of reasoning can be extended to countless scenarios other than the wtp that we deal with in the laboratory these points are made convincingly in hsee who lays out the differences between joint and separate evaluation modes for consumption further support for this is presented in hsee and leclerc who use a between designed experiment in which subjects are exposed to one product another product or both they demonstrate that joint evaluation of products leads to different valuations than separate evaluations this is a clear demonstration of one of the major concerns about within analyses see also hsee and zhang the main lesson from this set of studies is that failing to take into account the complexities of a within analysis can make or break the validity of a result if the order of asking a question matters significantly then something other than experimental environment is contributing to the variation across groups this variation could be from contextual referencing learning sensitization to changes carry over effects or other psychological factors if this bias is specific to one order but not the other or specific to the orders in different way then just varying the order of the questions might not be enough to remedy the problem considering that one of the three main advantages behind a within design is economizing on subjects this is worth bearing in mind at the planning stage other tasks could be used to help reset an individual time could elapse between elicitations or the experiment could be conducted in a segmented way the goal should be to achieve an independent evaluation of each scenario by participants and if a strong presence of the biases discussed is likely a between approach may be preferable another potential source of difficulty in wtp elicitation is what environmental economists have termed scope insensitivity kahneman and knetsch and desvousges et al popularized the notion that when the contingent valuations of public goods are obtained between individuals altering the quantity provided has little effect on wtp directly contradicting basic economic theory within elicitations have not found this result e g brookshire et al this would seem to indicate that we have to weigh the potential difficulty that individuals have in accurately perceiving the scope of goods in a between design and forcing individuals to perceive scope differences in a within design for farther discussion and some limits of scope neglect see carson et al we believe that the important lesson to be learned from the environmental literature is that while it is easy for a researcher to look at two versions of a survey and see how they differ respondents in a between framework see their information in a vacuum especially considering that the effort that individuals exert in responding can be difficult and costly to control detecting responses to relatively minor changes can be difficult to impossible in a between design in these circumstances we would expect different results from the different design types and which result is more meaningful likely depends on the context different methods different results experimentalists have recognized for a while that the framing of decisions can influence choices tversky and kahneman andreoni epley and gneezy etc framing decisions in the lab as contextual comparisons as in within designs or judgments made in isolation as in between designs can produce different results carry over between scenarios can create patterns that would not exist in an isolated situation or over sensitivity to changes in parameters can develop that leads to observed differences where they would not otherwise exist the carry over context and sensitization effects mentioned earlier do not have natural tendencies to produce specific behavioral responses their effects are functions of the circumstances experimenter demand effects however may well have the tendency to magnify differences between evaluations the act of moving a participant from scenario a b makes them explicitly aware of the change to their environment often in modern experiments the context in scenarios a and b are identical except for one parameter to which the participant naturally pays attention in an otherwise sterile and unchanging laboratory environment participants might ask themselves how they should change their behavior in response rather than first asking should they change their behavior to the participant it may seem as if the experimental variation is prompting a behavioral change hence the label experimenter demand as discussed above this concern was raised in the psychological literature over thirty years ago in relation to the problems associated with individuals hypothesizing about experimenter intentions this argument states that the kinds of variation we need to perform in experiments may result in decisions that do not represent natural preferences because the manipulation itself is unnatural with the goal of studying the power of the availability heuristic in determining probabilistic judgments milburn had participants fill out surveys estimating the likelihood of future events two of three groups are asked to estimate the probability of a series of future events while each member of the third group estimates the probability of a single event in surveys that elicit multiple probabilities clear order response patterns emerge the probability of positive events occurring steadily increased over the time horizon while the probability of negative events steadily decreased the between results were different however the probability of positive events occurring decreased with time in concurrence with the availability heuristic hypothesis it seems likely in this case that contextual comparisons carry over and range effects in the within surveys were influencing the results fox and tverksy studied ambiguity aversion one group of gambles was clear with respect to the odds of winning while the other was vague mindful of within between differences they gave some individuals the clear gamble some the g charness et al journal of economic behavior organization 1 8 vague gamble and some got both the within analysis indicated ambiguity averse behavior while the between did not the authors suggest that the comparative context in a within design is likely the cause for this result arguing that individual analyses of gambles could be different when participants are considering two and asked to choose one than just being asked whether or not to take a gamble when the gambles are presented together one easy comparison that the individual can make between the two is that they know the odds for one but not the other in gneezy participants were asked to evaluate the behavior of a car salesman who lies about the condition of the vehicle the cost of his lie repair costs he does not tell the buyer about is either low or high some individuals are exposed to both conditions while other only see one of them the within and between results both indicate that individuals consider the behavior less fair when the cost of the lie is increased but individuals who were exposed to both scenarios changed their opinions drastically in the between design 36 percent of subjects called the behavior very unfair in the low cost scenario while percent of subjects called it very unfair in the high cost scenario in the within design percent of subjects called it very unfair under low cost but percent called it very unfair under high cost the percentage point difference in rates is almost double with the within design gneezy postulates that the comparative context of the within analysis and possibly also an experimenter demand effect induces this difference the main lesson from this is that in a within analysis with a series of questions we can analyze order response correlations to get an idea of whether questions were answered independently 1 this can be done if the order of the questions is varied or if the order and questions are designed very carefully to avoid response trends otherwise we cannot distinguish between carry over bias and changing preferences perhaps a more serious problem with a within design is experimenter demand it seems advisable to strive to change the scenario in a way that does not trigger change for change sake this could be expressed as an independence condition between the set of possible behaviors in each scenario and the set of scenarios to which an individual is exposed 5 different methods similar results an example of the within and between methods agreeing with one another is the experimental evaluation of eyewitness accuracy and confidence the examples here represent the simplest of within and between designs but this lends itself to an intuitive understanding of the procedures all subjects participate in many rounds of judgments but by limiting analysis to one question at a time experimenters can still look at between differences despite the structure there are potential order issues with such a design for an optimal between analysis of specific questions individuals should be presented with the questions in the same order if two individuals see the same question at a different point in their sessions directly comparing them is problematic unless we have a strong belief in independence however for an optimal within analysis the question order needs to be varied to help minimize the effect of carry over and sensitization biases in the context of skill based rather than preference based experiments these concerns may not present such substantial obstacles subjects are aware of the fact that experimenters are interested in their ability but this should not change the fact that subjects should be motivated to perform as well as possible a systematic bias in these cases seems unlikely a paper demonstrating this is deffenbacher et al participants were tested on their memory for faces and the confidence with which they recognized them all subjects were shown a series of 50 faces and then asked to recall them a week later to compute the correlation between accurate recognition and eyewitness confidence the authors took two approaches they first calculated the correlation coefficients for each question separately between individuals before averaging across questions and second for each individual separately between questions before averaging across individuals the between method results in a coefficient of while the within method gives 31 smith et al 1989 applied similar methods and their results yield a between confidence accuracy correlation of 14 and a within confidence accuracy correlation of 17 these results demonstrate that it is possible to design an experiment with both within and between tests but once we switch to a multi period format there is a tradeoff we need to think about regarding question order the particular experiments mentioned above appear to be good examples of lab tasks that are naturally closer to independent across periods than some of the examples used earlier there are certainly stories that could be told about why this is not the case but we see very similar results from both methods especially in the second paper the skill based nature of the tasks would seem to make potential experimenter demand effects less likely econometric considerations when considering a design for experimental research it seems prudent to consider econometric concerns when making choices in this section we discuss some general issues in this regard as well as some specific examples as an example consider for a moment an experiment with multiple periods where individuals can answer a series of questions repeat tasks or play games with one or more other participants the within between distinction becomes murkier when we talk about longer horizons the tasks can change over time or remain static individuals can play with anonymous or known opponents and opponents can be selected randomly or deterministically 1 we wish to emphasize that this is more of a check on the results rather than a fix of the problem g charness et al journal of economic behavior organization 2012 1 8 more concretely suppose that an experimenter is interested in performance with respect to a task individuals perform the task in either low or high stakes conditions the nature of the research question requires multiple periods e g because of learning one option would be to split the sample into a high treatment and a low treatment and run both for a number of periods comparing the two treatments gives a between estimate of the behavioral difference an alternative approach would be to have two treatments play for both high and low stakes but to reverse the order of exposure across treatments treatment a might play low stakes for five periods and then high stakes for five while treatment b would do the opposite by analyzing how individuals change their behavior in response to a change in stakes we can obtain a within estimate of the treatment effect see isaac and walker 2 the accuracy of this approach depends on whether any order biases cancel one another out across the two orders as mentioned in the wtp section we have discussed the issues surrounding these approaches already the face recognition experiments from the psychology literature are examples of identical individual tasks with repetitions however simple correlation tests are inconsistent in the within design case because of the standard omitted individual heterogeneity issue when we talk about panel data in fact whenever we switch to a within design in a multi period experiment there are more serious econometric concerns that necessitate attention the resulting panel data is not simply separable along treatment lines by using a random effects framework we can at least in principle achieve more efficient results by using a fixed effects framework we can obtain potentially efficient and consistent within estimates in cases where the right side variable is not randomly assigned across individuals confidence or accuracy in the face recognition case fixed effects must be used to achieve consistency 3 it is important to remember though that for ease and simplicity there should always be a way to design an experiment to obtain a consistent between estimate there are issues surrounding the move to a full fledged effects model when analyzing experimental data critics argue that the correlation structure can never be fully untangled in cases of multi player multi period games however we argue that while not ideal these approaches may be the best option in certain scenarios consider the case where the parameter set to be tested is too large for a between design to be feasible we must then use a within design in which there is individual specific variation in the parameter value akin to varying the order of questions or treatments one can parameterize each game using a random draw from the parameter set randomizing here alleviates concerns with order effects from a more structured design opponents can be randomly and anonymously re matched after each round we can consistently estimate the effect of the parameter on game behavior in this case with a random effects model subject to a couple of caveats the re matching itself should not present a substantial issue unless the group size is very small playing the same opponent in adjacent rounds is unlikely and is often explicitly prohibited and combined with anonymity it should reduce an individual consideration of the possibility to a minor probabilistic contingency a problem with this approach is that when an individual is exposed to the play of others she learns about the laboratory population if this does not occur in a standard way across individuals a time period dummy in the effects model will not fully account for these effects similarly if exposure to certain types of play creates emotional responses in certain individuals that can carry over into future periods even including individual partner and time effects would not account for these individual cross time effects an example of a paper that demonstrates this is fehr and gächter who experimentally test punishment and cooperation in public goods games individuals played in groups of four for rounds either with or without punishment given the random and anonymous re matching in the stranger condition the authors calculate their probability of having an identical group twice in a row as slightly less than five percent and run a comparison stranger treatment where this probability was forced to be zero their results did not differ significantly lending support to the random procedure fehr and gächter perform their analysis in a number of ways first they take a between approach they find contribution is much higher in the treatment with punishment and a downward trend of contribution exists without punishment but does not exist with punishment they find that free riding emerges as a focal point without punishment but that it does not with punishment notice that they use a between approach to observe time specific trends as well to figure out why punishment increases contributions they take an effects model approach they model the data as a panel including timeperiod effects and group effects in the stranger treatment their dependent variable is whether or not an individual was punished and they include other average contribution own negative deviation and own positive deviation on the right hand side since the variables are definitely correlated with group effects they take a fixed effects approach they find a substantial correlation between own negative deviation and a punishment outcome 2 note there are two within options in this case one could use individual differences between the two phases of the game or average across individuals and use the overall group difference between the two phases both are within estimates one is at the individual level and the other at the group level with respect to individuals the within group approach combines within and between variation 3 the different types of effects models have intuitive interpretations that run parallel to the study of within versus between experiment designs in a panel model the between estimate uses data with individual specific variation averaged out within estimates come from data with individual fixed characteristics differenced out with any between individual variation removed the estimators are in many ways analogous to enforcing a between or within design ex post on the data a random effects estimate is a linear combination of both the between and within estimate a fixed effects estimate is just the within estimate in fact the tradition among experimental economists has long favored non parametric statistical analysis with the most conservative wing considering that each session in which there is interaction among the participants constitutes only one independent observation g charness et al journal of economic behavior organization 2012 1 8 andreoni and samuelson study the effect of a parameter in determining behavior in a twice repeated prisoners dilemma they have 11 values to test so a between approach is out of the question they have individuals play for rounds randomly and anonymously matched with one other person each round before each game one of the eleven parameter values is randomly chosen by the computer the authors take a panel approach using time and individual fixed effects 5 in this case the panel approach gave them the statistical power to test a hypothesis that would have been essentially impossible otherwise conclusion the methodological issue of within versus between designs is ubiquitous in experimental work between designs are more conservative in nature but have limitations in some cases while within designs have more power but potentially suffer from confounds it is important to point out that researchers can combine the two designs in simple ways to access the advantages of both methods a population of individuals can be split into two treatments a and b for observations in each cell of a between subjects design however if group a is asked question a and then question b and vice versa for group b now the researcher has observations in a within design with order effects controlled for and two between comparisons with observation in each cell whether or not all of the data can be used depends on the results but this design provides double the amount of information given the sample size hedged against a guarantee that at least one key comparison will be valid we have attempted to clearly delineate the issues surrounding this choice of methods we provide example of experiments that use within between or both designs sometimes these approaches yield the same results and sometimes they do not the issue is not a simple one and the choice depends on a number of factors we hope to further the discussion and the development of a framework informing experimental researchers about the benefits and drawbacks of each approach and have also discussed how one might minimize concerns of confounds such as providing spatial or temporal distance between or among within decisions in addition we have discussed which scenarios are particularly susceptible to different behavioral patterns across the two approaches in general we prefer between designs but recognize the limitations involved there may be cases where a within design is the only practical way to go however we believe that although within designs look attractive the researchers need to make the case that the confounds discussed above do not pose a challenge for the results of course this article is not a final word on this topic but there has been little explicit discussion of these issues within the community of experimental economics and it seems time to begin to remedy this omission in this paper we analyze several new methods for solving optimization problems with the objective function formed as a sum of two terms one is smooth and given by a black box oracle and another is a simple general convex function with known structure despite the absence of good properties of the sum such problems both in convex and nonconvex cases can be solved with efficiency typical for the first part of the objective for convex problems of the above structure we consider primal and dual variants of the gradient method with convergence rate o and an accelerated multistep version with convergence rate o where k is the iteration counter for nonconvex problems with this structure we prove convergence to a point from which there is no descent direction in contrast we show that for general nonsmooth nonconvex problems even resolving the question of whether a descent direction exists from a point is np hard for all methods we suggest some efficient line search procedures and show that the additional computational work necessary for estimating the unknown problem class parameters can only multiply the complexity of each iteration by a small constant factor we present also the results of preliminary computational experiments which confirm the superiority of the accelerated scheme dedicated to claude lemaréchal on the occasion of his birthday the author acknowledges the support from office of naval research grant efficiently computable compressed sensing yu nesterov b center for operations research and econometrics core catholic university of louvain ucl voie du roman pays louvain la neuve belgium e mail nesterov core ucl ac be yu nesterov keywords local optimization convex optimization nonsmooth optimization complexity theory black box model optimal methods structural optimization regularization mathematics subject classification introduction motivation in recent years several advances in convex optimization have been based on development of different models for optimization problems starting from the theory of self concordant functions it was becoming more and more clear that the proper use of the problem structure can lead to very efficient optimization methods which significantly overcome the limitations of black box complexity theory see section in for discussion for more recent examples we can mention the development of smoothing technique or the special methods for minimizing convex objective function up to certain relative accuracy in both cases the proposed optimization schemes strongly exploit the particular structure of corresponding optimization problem in this paper we develop new optimization methods for approximating global and local minima of composite convex objective functions φ x namely we assume that φ x f x x where f x is a differentiable function defined by a black box oracle and x is a general closed convex function however we assume that function x is simple this means that we are able to find a closed form solution for minimizing the sum of with some simple auxiliary functions let us give several examples constrained minimization let q be a closed convex set define as an indicator function of the set q x if x q otherwise then the unconstrained minimization of composite function is equivalent to minimizing the function f over the set q we will see that our assumption on simplicity of the function reduces to the ability of finding a closed form euclidean projection of an arbitrary point onto the set q barrier representation of feasible set assume that the objective function of the convex constrained minimization problem find f min f x x q is given by a black box oracle but the feasible set q is described by a ν selfconcordant barrier f x define x ν f x φ x f x x and gradient methods for minimizing composite functions x arg min x q f x then for arbitrary x int q by general properties of self concordant barriers we get f x f x φ x x x f x x x ν f φ x x x thus a point x with small norm of the gradient of function φ approximates well the solution of the constrained minimization problem note that the objective function φ does not belong to any standard class of convex problems formed by functions with bounded derivatives of certain degree sparse least squares in many applications it is necessary to minimize the following objective φ x ax b x def f x x where a is a matrix of corresponding dimension and k denotes the standard lk norm the presence of additive term very often increases the sparsity of the optimal solution see this feature was observed a long time ago see for example recently this technique became popular in signal processing and statistics from the formal point of view the objective φ x in is a nonsmooth convex function hence the standard black box gradient schemes need o iterations for generating its solution the structural methods based on the smoothing technique need o iterations however we will see that the same problem can be solved in o iterations of a special gradient type scheme contents in sect we study the problem of finding a local minimum of a nonsmooth nonconvex function first we show that for general nonsmooth nonconvex functions even resolving the question of whether there exists a descent direction from a point is np hard however for the special form of the objective function we can introduce the composite gradient mapping which makes the above mentioned problem solvable the objective function of the auxiliary problem is formed as a sum of the objective of the usual gradient mapping and the general nonsmooth convex term for the particular case this construction was proposed in we present different properties of this object which are important for complexity analysis of optimization methods in sect we study the behavior of the simplest gradient scheme based on the composite gradient mapping we prove that in convex and nonconvex cases we have exactly the same complexity results as in the usual smooth situation an interested reader can find a good survey of the literature existing minimization techniques and new methods in and however this idea has much longer history to the best of our knowledge for the general framework this technique was originally developed in yu nesterov for example for nonconvex f the maximal negative slope of φ along the minimiza tion sequence with monotonically decreasing function values increases as o k where k is the iteration counter thus the limiting points have no decent directions see theorem if f is convex and has lipschitz continuous gradient then the gradient method converges as o it is important that our version of the gradient method has an adjustable stepsize strategy which needs on average one additional computation of the function value per iteration in sect we introduce a machinery of estimate sequences and apply it first for justifying the rate of convergence of the dual variant of the gradient method afterwards we present an accelerated version which converges as o as compared with the previous variants of accelerated schemes e g our new scheme can efficiently adjust the initial estimate of the unknown lipschitz constant in sect we give examples of applications of the accelerated scheme we show how to minimize functions with known strong convexity parameter sect how to find a point with a small residual in the system of the first order optimality conditions sect and how to approximate unknown parameters of strong convexity sect in sect we present the results of preliminary testing of the proposed optimization methods an earlier version of the results was published in the research report notation in what follows e denotes a finite dimensional real vector space and e the dual space which is formed by all linear functions on e the value of function e at x e is denoted by x by fixing a positive definite self adjoint operator b e e we can define the following euclidean norms h bh h h e b e in the particular case of coordinate vector space e r n we have e e then usually b is taken as a unit matrix and x denotes the standard coordinate wise inner product further for function f x x e we denote by f x its gradient at x f x h f x f x h o h h e clearly f x e for a convex function we denote by x its subdifferential at x finally the directional derivative of function φ is defined in the usual way dφ y u lim α φ y αu φ y α finally we use notation a b for indicating that the inequality a b is an immediate consequence of the displayed relation in this paper for the sake of simplicity we restrict ourselves to euclidean norms only the extension onto the general case can be done in a standard way using bregman distances e g gradient methods for minimizing composite functions composite gradient mapping the problem of finding a descent direction for a nonsmooth nonconvex function or proving that this is not possible is one of the most difficult problems in numerical n and denote analysis in order to see this let us fix an arbitrary integer vector c z n i γ i c consider the function φ x γ max x i min x i c x i n i n clearly φ is a piece wise linear function with φ nevertheless we have the following discouraging result lemma it is np hard to decide if there exists x r n with φ x proof assume that some vector σ r n with coefficients satisfies equation c σ then φ σ let us assume now that φ x for some x r n we can always choose x with i n x i denote δ c x in view of our assumption we have x i δ i n γ denoting σ i sign x i i n we can rewrite this inequality as σ i x i δ therefore σ i x i σ i x i δ and we conclude that c σ c x c σ x δ γ max σ i x i γ δ i n since c has integer coefficients this is possible if and only if c σ it is well known that verification of solvability of the latter equality in boolean variables is np hard this is the standard boolean knapsack problem thus we have shown that finding a descent direction of function φ is np hard considering now the function max φ x we can see that finding a local minimum of a unimodular nonsmooth nonconvex function is also np hard thus in a sharp contrast to smooth minimization for nonsmooth functions even the local improvement of the objective is difficult therefore in this paper we restrict ourselves to objective functions of very special structure namely we consider the problem of minimizing def φ x f x x over a convex set q where function f is differentiable and function is closed and convex on q for characterizing a solution to our problem define the cone of feasible directions and the corresponding dual cone which is called normal f y u τ x y x q τ e yu nesterov n y x y x q e y q then the first order necessary optimality conditions at the point of local minimum x can be written as follows φ f x ξ n x def where ξ x in other words φ u u f x since is convex the latter condition is equivalent to the following dφ x u u f x note that in the case of convex f any of the conditions is sufficient for point x to be a point of global minimum of function φ over q the last variant of the first order optimality conditions is convenient for defining an approximate solution to our problem definition the point x q satisfies the first order optimality conditions of local minimum of function φ over the set q with accuracy if dφ x u u f x u note that in the case f x e with f x x this condition reduces to the following inequality min dφ x u min u min max f x ξ u u ξ x max f x ξ u u ξ x min ξ x f x ξ max min f x ξ u ξ x u for finding a point x satisfying condition we will use the composite gradient mapping namely at any y q define m l y x f y f y x y tl y arg min m l y x l x y x q x where l is a positive constant recall that in the usual gradient mapping we have our modification is inspired by then we can define a constrained analogue of the gradient direction for a smooth function the vector g l y l b y tl y e gradient methods for minimizing composite functions where the operator b defines the norm in case of ambiguity of the objective function we use notation g l y φ it is easy to see that for q e and we get g l y φ y f x for any l our assumption on simplicity of function means exactly the feasibility of operation let us mention the main properties of the composite gradient mapping almost all of them follow from the first order optimality condition for problem f y l b tl y y ξ l y x tl y x q where ξ l y tl y in what follows we denote φ tl y f tl y ξ l y φ tl y we are going to show that the above subgradient inherits all important properties of the gradient of a smooth convex function from now on we assume that the first part of the objective function has lipschitz continuous gradient f x f y l f x y x y q from and convexity of q one can easily derive the following useful inequality see for example f x f y f y x y lf x y x y q first of all let us estimate a local variation of function φ denote sl y f tl y f y tl y y lf theorem at any y q l f g l y l lf φ tl y y tl y g l y φ y φ tl y moreover for any x q we have φ tl y x tl y sl y g l y tl y x l lf g l y tl y x l yu nesterov proof for the sake of notation denote t tl y and ξ ξ l y then φ t x y lf t y t lf t y t f y l b t y ξ y t l f f y t y t ξ y t l f φ y t y f y f y t y taking into account the definition we get further f t ξ y t x y f y ξ y t f t f y t y l b y t y t f t f y t y l l f l l f t y g l y thus we get finally f t ξ t x f t t x f y l b t y x t f t f y t x g l y x t sl y g l y t x l and follows corollary for any y q and any u f tl y u we have lf dφ tl y u l g l y in this respect it is interesting to investigate the dependence of g l y lemma the norm of the gradient direction g l y norm of the step tl y y is decreasing in l in l is increasing in l and the proof indeed consider the function ω τ min f y f y x y x q x y x the objective function of this minimization problem is jointly convex in x and τ therefore ω τ is convex in τ since the minimum of this problem is attained at a single point ω τ is differentiable and gradient methods for minimizing composite functions ω τ τ y y τ τ y since ω is convex ω τ is an increasing function of τ hence τ y decreasing function of τ the second statement follows from the concavity of the function ω l min f y f y x y x q l x y is a x now let us look at the output of the composite gradient mapping from a global perspective theorem for any y q we have g l y l lf x y m l y tl y min φ x x q m l y tl y φ y if function f is convex then m l y tl y min φ x x q l x y proof note that function m l y x is strongly convex in x with convexity parameter l hence φ y m l y tl y m l y y m l y tl y l y tl y g l y l x y x further if f is convex then m l y tl y min f y f y x y x q min f x x x q min φ x x q l x y l x y for nonconvex f we can plug into the same reasoning the following consequence of f y f y x y f x lf x y yu nesterov remark in view of for l l f we have φ tl y m l y tl y hence in this case inequality guarantees φ tl y min φ x x q l x y finally let us prove a useful inequality for strongly convex φ lemma let function φ be strongly convex with convexity parameter μφ then for any y q we have tl y x sl y g l y μφ l lf μφ l g l y where x is a unique minimizer of φ on q proof indeed in view of inequality we have lf l g l y t y x sl y g l y l l tl y x φ tl y tl y x μφ tl y x and follows now we are ready to analyze different optimization schemes based on the composite gradient mapping in the next section we describe the simplest one gradient method define first the gradient iteration with the simplest backtracking strategy for the line search parameter we call its termination condition the full relaxation gradient methods for minimizing composite functions gradient iteration g x m set l m repeat t tl x if φ t m l x t then l l γu φ t m l x t until output g x m t t g x m l l g x m s sl x if there is an ambiguity in the objective function we use notation gφ x m for running the gradient scheme we need to choose an initial optimistic estimate l for the lipschitz constant l f l f and two adjustment parameters γu and γd let q be our starting point for k consider the following iterative process gradient method gm l yk g yk l k t mk g yk l k l l k max l mk γd thus yk tmk yk since function f satisfies inequality in the loop the value l can keep increasing only if l l f taking into account condition we obtain the following bounds l l k m k γu l f l k l f k moreover if γd γu then yu nesterov note that in there is no explicit bound on the number of repetition of the loop however it is easy to see that the total amount of calls of oracle nk after k iterations of cannot be too big lemma in the method for any k we have γu l f ln γd k ln nk ln γu ln γu γd l proof denote by n i the number of calls of the oracle at iteration i then l i l i γun i γd thus ni ln γd l i ln ln γu ln γu li hence we can estimate k nk ni i ln γd l k ln k ln γu ln γu in remains to note that l k max l γγdu l f a reasonable choice of the adjustment parameters is as follows γu γd nk k lf lk l f thus the performance of the gradient method is well described by the estimates for the iteration counter therefore in the rest of this section we will focus on estimating the rate of convergence of this method in different situations let us start from the general nonconvex case denote g mi yi i k g mi yi i k arg min i k δk min theorem let function φ be bounded below on q by some constant φ then δk φ φ k gradient methods for minimizing composite functions moreover for any u f yik with u we have dφ yik u γu l f φ φ k proof indeed in view of the termination criterion in we have φ yi φ yi φ yi m mi yi tmi yi g mi yi summing up these inequalities for i k we obtain denote jk i k since yik tm jk y jk for any u f yik with u we have lf dφ yik u m jk m jk l f m jk g m jk y jk lf m jk φ φ γu l f k jk δk φ φ k let us describe now the behavior of the gradient method in the convex case theorem let function f be convex on q assume that it attains a minimum on q at point x and that the level sets of φ are bounded y x r y q φ y φ if φ φ x γu l f r then φ φ x we have φ yk φ x γu l f r otherwise for any k l f r k moreover for any u f yik with u we have γu l f r dφ yik u k k γu lf proof since φ yk φ yk for all k we have the bound yk x r valid for all generated points consider yk α αx α yk q α yu nesterov then φ yk m mk yk tmk yk y yk α min φ y y q φ αx α yk min φ yk α φ yk φ x α mk y yk mk α yk x min α γu l f r if φ φ x γu l f r then the optimal solution of the latter optimization problem is α and we get φ φ x γu l f r otherwise the optimal solution is α φ yk φ x φ φ x γu l f r γu l f r and we obtain φ yk φ yk from this inequality denoting λk λk λk φ yk φ x l f r φ yk φ x we get λk λk γu l f r l f r hence for k we have λk k k φ φ x l f r l f r further let us fix an integer m m k since φ yi φ yi g mi yi i k we have k k m δk i m g mi yi φ ym φ x φ ym φ yk l f r m gradient methods for minimizing composite functions denote jk i k then for any u f yik with u we have lf dφ yik u m jk g m jk y jk m jk l f lf m jk jk δk γu l f r m k m γu l f γu l f r l m k m m jk choosing m we get m k m k k theorem let function φ be strongly convex on q with convexity parameter μφ if μφ l f then for any k we have φ yk φ x γu l f μφ k φ φ x φ φ x otherwise φ yk φ x μφ l f k φ φ x proof since φ is strongly convex for any k we have φ yk φ x μφ yk x denote yk α αx α yk q α then φ yk min α min α min α mk α yk x γu l f φ yk α φ yk φ x α yk x γu l f φ yk α α φ yk φ x μφ φ αx α yk μ the minimum of the last expression is achieved for α min φl f hence if μφ l f then α and we get φ yk φ x γu l f φ yk φ y φ yk φ y μφ if yu nesterov μφ l f then α μφ l f and φ yk φ x φ yk φ y μφ l f l remark in theorem the condition number μφf can be smaller than one for strongly convex φ the bounds on the directional derivatives can be obtained by combining the inequalities with the estimate φ yk φ x l l f f g l f yk and inequality thus inequality results in the bound dφ yk u γu l f μφ k f φ φ and inequality leads to the bound dφ yk u μφ l f k f φ φ which are valid for all u f yk with u the estimate may create an impression that a large value of γu can reduce the total number of calls of the oracle this is not true since γu enters also into the estimate of the rate of convergence of the methods e g therefore reasonable values of this parameter lie in the interval accelerated scheme in the previous section we have seen that for convex f the gradient method converges as o however it is well known that on convex problems the usual gradient scheme can be accelerated e g chapter in let us show that the same acceleration can be achieved for composite objective functions consider the problem min φ x f x x x e where function f is convex and satisfies and function is closed and strongly convex on e with convexity parameter μ we assume this parameter to be known the case μ corresponds to convex denote by x the optimal solution to gradient methods for minimizing composite functions in problem we allow dom e therefore the formulation covers also constrained problem instances note that for the first order optimality condition defining the composite gradient mapping can be written in a simpler form tl y dom f y ξ l y l b y tl y g l y where ξ l y tl y for justifying the rate of convergence of different schemes as applied to we will use the machinery of estimate functions in its newer variant taking into account the special form of the objective in we update recursively the following sequences a minimizing sequence xk k a sequence of increasing scaling coefficients ak k def ak ak ak k a sequence of estimate functions ψk x lk x ak x x k where dom is our starting point and lk x are linear functions in x e however as compared with we will add a possibility to update the estimates for lipschitz constant l f using the initial guess l satisfying and two adjustment parameters γu and γd for the above objects we maintain recursively the following relations ak φ xk ψk min x ψk x ψk x ak φ x x x e k these relations clearly justify the following rate of convergence of the minimizing sequence φ xk φ x x ak k denote vk arg min x e ψk x since μψk for any x e we have ak φ xk x vk ψk x vk ψk x ak φ x x hence taking x x we get two useful consequences of x vk x vk x k yu nesterov note that the relations can be used for justifying the rate of convergence of a dual variant of the gradient method indeed for dom define x x and choose l satisfying condition dual gradient method dg l k yk g vk l k t mk g vk l k l l k max l mk γd ak ψk x ψk x mk f vk mk f vk x vk x in this scheme the operation g is defined by since is simple the points vk are easily computable note that the relations and k are trivial relations can be justified by induction define φk i k φ yi and xk φ xk φk for k then min ψk x f vk f vk x vk x ψk x mk x vk ak φk min f vk f vk x vk x x mk ak φk ak m mk vk yk ak φk ak φ yk ak φk thus relations are valid for all k since the values mk satisfy bounds for method we obtain the following rate of convergence φ xk φ x γu l f x k note that the constant in the right hand side of this inequality is four times smaller than the constant in however each iteration in the dual method is two times more expensive as compared to the primal version however the method does not implement the best way of using the machinery of estimate functions let us look at the accelerated version of as parameters it has the starting point dom the lower estimate l for the lipschitz constant l f and a lower estimate μ μ for the convexity parameter of function gradient methods for minimizing composite functions accelerated method a l μ initial settings x x iteration k set l l k repeat find a from quadratic equation set y ak xk avk ak a ak a k μa l and compute tl y if φ tl y y tl y until φ tl y y tl y l l φ tl y then l l γu φ tl y define yk y mk l ak a l k mk γd xk tmk yk ψk x ψk x ak f xk f xk x xk x as compared with gradient iteration we use a damped relaxation condition as a stopping criterion of the internal cycle of lemma condition in is satisfied for any l l f proof denote t tl y multiplying the representation φ t f t ξ l y l b y t f t f y by the vector y t we obtain φ t y t l y t f y f t y t φ t f y f t y t f y f t l f y f t y t φ t f y f t y t f y f t l l hence for l l f condition is satisfied thus we can always guarantee l k m k γu l f if γd γu then the upper bound remains valid yu nesterov let us establish a relation between the total number of calls of oracle nk after k iterations and the value of the iteration counter lemma in the method for any k we have nk γu l f ln γd k ln ln γu ln γu γd l proof denote by n i the number of calls of the oracle at iteration i at each cycle of the internal loop we call the oracle twice for computing f y and f tl y therefore l i l i i γd thus ln γd l i ln ln γu ln γu li ni hence we can compute k nk ni i γ u γd in remains to note that l k ln γd l k k ln ln γu ln γu lf thus each iteration of needs approximately two times more calls of the oracle than one iteration of the gradient method lf lk l f γu γd nk k however we will see that the rate of convergence of is much higher let us start from two auxiliary statements lemma assume μ μ then the sequences xk ak and ψk generated by the method a l μ satisfy relations for all k proof indeed in view of initial settings of and hence for k both relations are trivial assume now that relations are valid for some k in view of for any x e we have ψk x ak φ x x ak ak φ x ak f xk f xk x xk x x and this is let us show that the relation is also valid gradient methods for minimizing composite functions indeed in view of function ψk x is strongly convex with convexity parameter μak hence in view of for any x e we have ψk x ψk μak x vk ak φ xk μak x vk therefore min ψk x ak f xk f xk x xk x ψk x e μak x vk ak φ xk φ xk x xk min ak φ xk x e min ak ak φ xk ak φ xk xk xk x e μak x vk ak φ xk x xk min ak φ xk φ xk ak yk ak vk ak xk x e μak x vk ak φ xk x xk min ak φ xk ak φ xk yk xk x e μak x vk ak φ xk x vk the minimum of the later problem is attained at x vk we have proved inequality ψk ak φ xk ak φ xk yk xk ak μak b φ xk thus ak μak φ xk on the other hand by the termination criterion in we have φ xk yk xk φ xk mk it remains to note that in we choose ak from the quadratic equation ak ak ak thus is valid mk ak μak yu nesterov thus in order to use inequality for deriving the rate of convergence of method a l μ we need to estimate the rate of growth of the scaling coefficients ak k lemma for any μ the scaling coefficients grow as follows ak k l f for μ the rate of growth is linear ak γu l f μ l f k k proof indeed in view of equation in we have mk mk ak ak ak ak ak ak μak ak ak ak mk ak ak ak γu l f ak ak thus for any k we get ak k l f if μ then by the same reasoning as above we obtain μak ak ak μak ak γu l f ak ak hence ak ak f since γu l f we come to now we can summarize all our observations theorem let the gradient of function f be lipschitz continuous with constant l f also let the parameter l satisfy condition then the rate of convergence of the method a l as applied to the problem can be estimated as follows φ xk φ x γu l f x x k if in addition the function is strongly convex then the sequence xk k generated by a l μ satisfies both and the following inequality φ xk φ x γu l f x μ l f k k in the next section we will show how to apply this result in order to achieve some specific goals for different optimization problems gradient methods for minimizing composite functions different minimization strategies strongly convex objective with known parameter consider the following convex constrained minimization problem min fˆ x x q where q is a closed convex set and fˆ is a strongly convex function with lipschitz continuous gradient assume the convexity parameter μ fˆ to be known denote by σ q x an indicator function of set q σ q x x q otherwise we can solve the problem by two different techniques reallocating the prox term in the objective for μ μ fˆ define μ μ f x fˆ x x x σ q x x note that function f in is convex and its gradient is lipschitz continuous with l f l fˆ μ moreover the function x is strongly convex with convexity parameter μ on the other hand φ x f x x fˆ x σ q x thus the corresponding unconstrained minimization problem coincides with constrained problem since all conditions of theorem are satisfied the method a l μ has the following performance guarantees fˆ xk fˆ x γu l fˆ μ x μ l fˆ μ k k this means that an solution of problem can be obtained by this technique in o ln μ l fˆ yu nesterov iterations note that the same problem can be solved also by the gradient method however in accordance to its performance guarantee is much worse it needs o l fˆ ln μ iterations restart for problem define the following components of composite objective function in f x fˆ x x σ q x let us fix an upper bound n for the number of iterations in a consider the following two level process choose u q compute u k as a result of n iterations of a u k l k in view of definition we have fˆ u k fˆ x thus taking n γu l fˆ μ fˆ γu l fˆ x u k l fˆ fˆ u k fˆ x μ fˆ n we obtain fˆ u k fˆ x fˆ u k fˆ x hence the performance guarantees of this technique are of the same order as approximating the first order optimality conditions in some applications we are interested in finding a point with small residual of the system of the first order optimality conditions since lf dφ tl x u l u f tl x u g l x l l f φ x φ x l f the upper bounds on this residual can be obtained from the estimates on the rate of convergence of method in the form or however in this case the first inequality does not give a satisfactory result indeed it can guarantee that gradient methods for minimizing composite functions the right hand side of inequality vanishes as o this rate is typical for the gradient method see and from accelerated version we can expect much more let us show how we can achieve a better result consider the following constrained optimization problem min f x x q where q is a closed convex set and f is a convex function with lipschitz continuous gradient let us fix a tolerance parameter δ and a starting point q define x σ q x δ x consider now the unconstrained minimization problem with composite objective function φ x f x x note that function is strongly convex with parameter μ δ hence in view of theorem the method a l δ converges as follows γu l f φ xk φ x x δ l f k for simplicity we can choose γu γd in order to have l k l f for all k let us compute now tk g xk l k t and mk g xk l k l then φ xk φ x φ xk φ tk g mk xk l m k γu l f and we obtain the following estimate g mk xk k γu l f δ x l f in our case the first order optimality conditions for computing tmk xk can be written as follows f xk δ b tk ξk g mk xk where ξk σ q tk note that for any y q we have σ q y σ q tk ξk y tk ξk y tk yu nesterov hence for any direction u f tk with u we obtain f tk u f xk u lf g mk xk mk lf g mk xk δ b tk ξk u g mk xk mk lf g mk xk δ tk mk assume now that the size of the set q does not exceed r and δ l let us choose the number of iterations k from inequality 2γu l f k then the residual of the first order optimality conditions satisfies the following inequality γu l f lf f tk u r l l0 u f tk u for that the required number of iterations k is at most of the order o ln unknown parameter of strongly convex objective in sect we have discussed two efficient strategies for minimizing a strongly convex function with known estimate of convexity parameter μ fˆ however usually this information is not available we can easily get only an upper estimate for this value for example by inequality μ fˆ sl x l fˆ x q let us show that such a bound can be also used for designing an efficient optimization strategy for strongly convex functions for problem assume that we have some guess μ for the parameter μ fˆ and a starting point u q denote x fˆ x σ q x let us choose u l t u l l u l s and minimize the composite objective by method a μ using the following stopping criterion gradient methods for minimizing composite functions com pute vk xk l k t stop the stage if a g mk xk or b mk ak mk xk l k l g u if the stage was terminated by condition a then we call it successful in this case we run the next stage taking vk as a new starting point and keeping the estimate μ of the convexity parameter unchanged suppose that the stage was terminated by condition b that is an unsuccessful stage if μ were a correct lower bound for the convexity parameter μ fˆ then g mk xk 2mk fˆ xk fˆ x x ak g u ak μ hence in view of condition b in this case the stage must be terminated by condition a since this did not happen we conclude that μ μ fˆ therefore we redefine μ μ and run again the stage keeping the old starting point we are not going to present all details of the complexity analysis of the above strategy it can be shown that for generating an solution of problem with strongly convex objective it needs o κ ˆ ln κ fˆ f κ fˆ o κ ˆ ln κ fˆ ln f def κ fˆ l fˆ μ fˆ calls of the oracle the first term in this bound corresponds to the total amount of calls of the oracle at all unsuccessful stages the factor κ ˆ ln κ fˆ represents an upper f bound on the length of any stage independently on the variant of its termination computational experiments we tested the algorithms described above on a set of randomly generated sparse least squares problems of the form find φ minn φ x def x r ax b x where a an is an m n dense matrix with m n all problems were generated with known optimal solutions which can be obtained from the dual representation of the initial primal problem minn x r yu nesterov ax b x minn maxm u b ax x r u r maxm minn b u u r x r maxm b u u r u u u x a t u x x at u thus the problem dual to consists in finding a euclidean projection of the vector b r m onto the dual polytope d y rm at y this interpretation explains the changing sparsity of the optimal solution x τ to the following parametric version of problem def minn φτ x x r ax b τ x indeed for τ we have φτ x τ x b a τ τ x τ hence in the dual problem we project vector τb onto the polytope d the nonzero components of x τ correspond to the active facets of d thus for τ big enough we have τb int d which means x τ when τ decreases we get x τ more and more dense finally if all facets of d are in general position we get in x τ exactly m nonzero components as τ in our computational experiments we compare three minimization methods two of them maintain recursively relations this feature allows us to classify them as primal dual methods indeed denote φ u u b u as we have seen in φ x φ u x r n u d moreover the lower bound is achieved only at the optimal solutions of the primal and and a starting point z dom relations dual problems for some sequence z i i ensure gradient methods for minimizing composite functions k ak φ xk minn x r i ai f z i f z i x z i ak x x z in our situation f x ax b x x and we choose z denote u i b az i then f z i a t u i and therefore ui f z i f z i z i a t u i z i b u i ui φ u i denoting u k ak k ai u i i we obtain k ak φ xk φ u k ak φ xk ai φ u i i k minn x r i ai f z i x ak x x x in view of u k cannot be feasible φ u k φ xk φ min φ u u d let us measure the level of infeasibility of these points note that the minimum of optimization problem in is achieved at x vk hence the corresponding first order optimality conditions ensure therefore ai u k is diagonal k ai a u i bvk i n def ρ u k i di ak i ak bvk b i j then ai u k ak t i n assume that the matrix b in di i j otherwise vk i and ai u k di vk ak x ak yu nesterov where α max α thus we can use function ρ as a dual infeasibility measure in view of it is a reasonable stopping criterion for our primal dual methods for generating the random test problems we apply the following strategy choose m m the number of nonzero components of the optimal solution x of problem and parameter ρ responsible for the size of x generate randomly a matrix b r m n with elements uniformly distributed in the interval generate randomly a vector v r m with elements uniformly distributed in define y v v sort the entries of vector b t y in the decreasing order of their absolute values for the sake of notation assume that it coincides with a natural order for i n define ai αi bi with αi chosen accordingly to the following rule bi y for i m if bi y and i m αi ξ i bi y otherwise where ξi are uniformly distributed in for i n generate the components of the primal solution x i ξi sign ai y for i m otherwise where ξi are uniformly distributed in ρm define b y ax thus the optimal value of the randomly generated problem can be computed as φ y x in the first series of tests we use this value in the termination criterion let us look first at the results of minimization of two typical random problem instances the first problem is relatively easy in this table the column gap shows the relative decrease of the initial residual in the rest of the table we can see the computational results for three methods primal gradient method abbreviated as pg dual version of the gradient method abbreviated as dg accelerated gradient method abbreviated as ac in all methods we use the following values of the parameters γu γd let us explain the remaining columns of this table for each method the column shows the number of iterations necessary for reaching the corresponding reduction of the initial gap in the function value column ax shows the necessary number of matrix vector multiplications note that for computing the value f x we need one multiplication if in addition we need to compute the gradient we need one more multiplication for example in accordance to the estimate each iteration of needs four computations of the pair function gradient hence in this method we can expect eight matrix vector multiplications per iteration for the gradient method we need in average two calls of the oracle however one of them is done in the line search procedure and it requires only the function value hence in this case we expect to have three matrix vector multiplications per iteration in the table above we can observe the remarkable accuracy of our predictions finally the column speedup represents the absolute accuracy of current approximate solution as a percentage of the worst case estimate given by the corresponding rate of convergence since the exact l f is unknown we use l instead k yu nesterov we can see that all methods usually significantly outperform the theoretically predicted rate of convergence however for all of them there are some parts of the trajectory where the worst case predictions are quite accurate this is even more evident from our second table which corresponds to a more difficult problem instance in this table we can see that the primal gradient method still significantly outperforms the theoretical predictions this is not too surprising since it can for example automatically accelerate on strongly convex functions see theorem all other methods require in this case some explicit changes in their schemes however despite all these discrepancies the main conclusion of our theoretical analysis seems to be confirmed the accelerated scheme significantly outperforms the primal and dual variants of the gradient method in the second series of tests we studied the abilities of the primal dual schemes and in decreasing the infeasibility measure ρ see this problem gradient methods for minimizing composite functions at least for the dual gradient method appears to be much harder than the primal minimization problem let us look at the following results problem n m m ρ gap in this table we can see the computational cost for decreasing the initial value of ρ in times note that both methods require more iterations than for problem which was solved up to accuracy in the objective function of the order moreover for reaching the required level of ρ method has to decrease the residual in the objective up to machine precision and the norm of gradient mapping up to the accelerated scheme is more balanced the final residual in φ is of the order and the norm of the gradient mapping was decreased only up to let us look at a bigger problem as compared with problem in problem the sizes are doubled this makes almost no difference for the accelerated scheme but for the dual gradient method the computational expenses grow substantially the further increase of dimension makes the latter scheme impractical let us look at how these methods work on problem with ρ being a termination criterion the reason of the failure of the dual gradient method is quite interesting in the end it generates the points with very small residual in the value of the objective function therefore the termination criterion in the gradient iteration cannot work properly due to the rounding errors in the accelerated scheme this does not happen since the decrease of the objective function and the dual infeasibility measure is much more balanced in some sense this situation is natural we have seen that on the current test problems all methods converge faster at the end on the other hand the rate of thus in this metric we have very good lower and upper bounds for the lipschitz constant l f let us look at the corresponding computational results we solve the problem with n m and ρ up to accuracy gap for different sizes m of the support of the optimal vector which are gradually increased from to problem recall that the first line of this table corresponds to the previously discussed version of problem for the reader convenience in the next table we repeat the final results on the latter problem adding the computational results for m both with no diagonal scaling thus for m the diagonal scaling makes problem very easy for easy problems the simple and cheap methods have definite advantage with respect to more complicated strategies when m increases the scaled problems become more and more difficult finally we can see again the superiority of the accelerated scheme needless to say at this moment of time we have no plausible explanation for this phenomenon our last computational results clearly show that an appropriate complexity analysis of the sparse least squares problem remains a challenging topic for future research to cite this article mohamad y jaber christoph h glock ahmed m a el saadany supply chain coordination with emissions reduction incentives international journal of production research doi to link to this article https doi org published online feb submit your article to this journal article views citing articles view citing articles full terms conditions of access and use can be found at http www tandfonline com action journalinformation journalcode international journal of production research vol no january 82 supply chain coordination with emissions reduction incentives mohamad y jabera christoph h glockb and ahmed m a el saadanya a department of mechanical and industrial engineering ryerson university toronto ontario canada chair of business management and industrial management university of wuerzburg wuerzburg germany b received march final version received december the european union emissions trading system eu ets is considered one of the main legislative systems that are set up to reduce emissions and protect the environment most of the works in the literature approach this system from a legislation and or global point of view little has been done to examine this system from the perspective of the user this work is believed to be the first to consider the eu ets system in a supply chain and operations management context a two level vendor buyer supply chain model with a coordination mechanism is presented while accounting for greenhouse gas ghg emissions from manufacturing processes different emissions trading schemes are considered and possible combinations between these schemes are presented the developed model could be found useful by mangers who wish to jointly minimise the inventory related and ghg emissions costs of their supply chains when penalties for exceeding emissions limits are considered numerical examples are presented and results are discussed keywords supply chain coordination emissions trading system inventory environment introduction actions on climate change have been topping priority lists in many countries especially with increasing pressures from the public carbon dioxide emissions were estimated at billion tons in and are expected to reach billion tons by enkvist et al the european union emissions trading system eu ets which was initiated in as a method to reduce greenhouse gas emissions has been viewed by the european commission for climate change as the first and biggest international scheme for the trading of greenhouse gas emission allowances the eu ets involves more than electricity power generation stations and industrial plants in about countries this system is expected to result in lower emissions than the levels by the eu ets depends on the cap and trade concept where companies have a limit on greenhouse gas emissions and anything above and beyond this limit is charged however the cost to limit these emissions is less than the charge kruger and pizer reported that penalties for emissions in excess of surrendered allowances are per ton some countries e g germany impose a one time fine that is up to european environmental agency http www eea europa eu in case the emissions are less than the specified limit companies are allowed to trade or keep unused allowances the implementation of the eu ets had some good results with some significant changes and improvements including auctioning of allowances expected by a similar program to eu ets was implemented in los angeles california which has the worst pollution level among the major cities in the usa to cut the emissions of oxides of nitrogen nox and oxides of sulphur sox although the average prices for emissions are expensive ranging between and per ton and the average trade size is high there seems to be a constant growth in the size and number of trades with trade prices being associated with the location of the emitting facility and the type of pollutants it emits gangadharan dobos examined how much pollution rights are there to trade and what are the effects of trading on the production inventory policies he found that introducing emissions trading while assuming that the pollution may be described by a non decreasing and convex function of the production rate results in smoothing the optimal production inventory strategy but with higher costs soleille emphasised the importance of accessibility i e affordable transaction costs easier compliance verification penalty application in case of non compliance etc market activity to maintain the fluidity of demand and supply in order to decrease price volatility and the presence corresponding author email mjaber ryerson ca issn print issn online ß taylor francis http dx doi org http www tandfonline com m y jaber et al of strong governmental legislations to guarantee the success of emissions trading schemes the importance of integrating environmental issues into logistics and inventory systems decision making process was strongly emphasised in bonney and bonney and jaber of course not every emissions tax system is welcomed and appreciated mayor and tol noted that the amount of emissions produced by the uk aviation industry increased by between and and is expected to continue growing they examined the combined effect of applying four different taxes of carbon dioxide emissions and found such a policy to be less costly their rationale to explain this finding is that some taxes are considered as a revenue raising tax reform promoted under the guise of climate policy this finding was echoed in the way us carbon tax was presented carbon dioxide emissions are estimated to be responsible for half of the man made greenhouse effect the united states has considered limiting the us carbon emissions to complement the eu ets through a revenueneutral carbon tax which is easier to model than the cap and trade model where the tax is applied for every ton of emissions metcalf in his paper metcalf stated that the carbon tax rate should reflect the social cost of carbon emissions he identified this cost as the social marginal damages of emissions which has various estimates ranging from to per ton metcalf also surveyed the literature and found that the average recommended carbon tax for emissions is in the range of per ton he argued that the tax is more efficient than the trading system as the later promotes trading with price volatility while the former is a more disciplined stream floros and vlachou studied the impact of a carbon tax on emissions and found that considering a carbon tax as a policy instrument in the greek industry which was estimated at per ton enticed industries to reduce direct and indirect emissions tomlinson et al considered the renewal or refurbishment of electrical and mechanical equipment in industrial facilities and showed how to quantify the environmental cost of refurbishment with respect to the associated carbon emissions by considering the whole life of equipment the authors also considered the design the raw materials used the manufacturing method the transportation mode used and the disposal technique of an industrial unit to calculate the associated emissions and recommended using their method for a better estimation of investment appraisal and procurement strategies although there are several works in the literature on greenhouse gas emissions that consider policy design and the effect of introducing a new legislation on the environment our review of the literature did not come across a study that considers the same issue from a user or a manufacturer point of view that is how should one deal with new policies and which factors should be considered in running day to day operations in the next section a model that optimises the joint production inventory policy for a two level vendor buyer supply chain with greenhouse gas emissions cost penalties for exceeding the emissions limit and inventory related costs is developed numerical examples that address six research questions are solved and the results are discussed in section the paper is summarised and concluded in section model this section develops a mathematical programming problem for a two level supply chain vendor buyer when emissions trading is considered it is assumed that greenhouse gas emissions are a function of the vendor production rate the objective of the developed model is to determine the optimal production rate subsequently the joint lot sizing policy that minimises the total supply chain cost taking into consideration emissions certification limit penalties for exceeding the emissions quota and the capital invested to increase the emissions limit by purchasing new certificates next we list the notations and decision variables make assumptions where necessary and develop the mathematical model notations d hm hr sm sr a b demand rate unit year manufacturer holding cost unit year retailer holding cost unit year manufacturer setup cost retailer ordering cost emissions function parameter ton emissions function parameter ton year international journal of production research c cec cep i e eli n pmax yi peli po pmin emissions function parameter ton unit emissions tax ton emissions penalty year for exceeding emissions limit i greenhouse gas emissions ton unit emissions limit i ton year minimum production demand ratio where number of emissions limits maximum attainable production rate unit year emissions limit variable for emissions limit i which is if the emissions exceed the allowable limit i and otherwise minimum production rate that satisfies emissions limit i unit year production rate that minimises greenhouse gas emissions per unit produced unit year minimum production rate unit year pmin d where em c edcec þ n x yi cep i decision variables p manufacturer production rate p d unit year vendor buyer coordination multiplier buyer inventory level retailer vendor inventory level manufacturer supply chain coordination is achieved when the operations of the manufacturer and the retailer are optimised collectively instead of independently e g jaber and zolfaghari glock a simplified two level vendor buyer supply chain model is presented with being the manufacturer lot size multiplier of the retailer order quantity and the value of being reflected in the manufacturer cycle inventory as shown in figure e g goyal et al el saadany and jaber as depicted in figure the vendor manufacturer delivers its order in positive integer shipments to the buyer retailer to satisfy the supply chain demand at a rate d over time t the supply chain cost sc is the sum of the manufacturer setup and holding costs per unit of time and the retailer ordering and holding costs per unit of time as a function of and p and it is computed as sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ d hr ssc sc þ sr þ hm þ þ p t time time figure example of a two level supply chain with m y jaber et al the derivation of equation is provided in appendix the relationship between the production rate of a process and the rate of generating greenhouse gas emissions ton per unit is given from bogaschewsky as e bp þ c the case where emissions can be expressed as a convex function of the production rate or equipment speed as described in equation has been empirically validated for car engines tu v rheinland furthermore it has also been shown that the energy consumption for various production processes follows a similar pattern to that described above e g ga lweiler pack fandel since the consumption of energy is usually associated with the generation of greenhouse gas emissions it is therefore reasonable to infer to a function of the form given in equation an emissions cost edcec is added to the total cost function to account for every ton of greenhouse gas emissions released into the environment as in the us carbon emissions tax system in addition as in the case of eu ets we assume that a supply chain is penalised by paying cep i once its emissions exceed a given limit eli per year therefore the total cost per unit of time is the sum of the supply chain inventory related and greenhouse gas emissions costs per unit of time including applicable penalties which is given from equations and as minimise sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n x d hr yi cep i tc ðp þ þ sr þ hm þ þ d þ edcec þ p subject to yi if ed eli else where i n d p pmax integer value the total cost function in equation is composed of three components which are the supply chain cost the cost of emissions and the penalty cost for exceeding the allowed limit if only the supply chain costs as given in equation are considered then the locally optimal production rate pmin d balances the inventory holding setup and ordering costs glock considering only the emissions cost in contrast results in a locally optimal production rate po which minimises the emissions cost function in equation at po b any deviation from po by either increasing or decreasing the production rate results in higher emissions cost penalties depend on the value of emissions and accordingly on the value of po the fact that varying the production rate may be associated with an additional production cost is not considered in this model for reasons of simplicity as it will only add another effect to the model which enforces or dampens a deviation from pmin the reader is referred to glock for a discussion of this aspect condition ensures that yi takes on the value when the corresponding emissions limit eli is exceeded and condition restricts the production rate to vary between a minimum and a maximum value which may be due to technical reasons therefore the optimal solution p is equal to pmin or po or lies between these two values which brings us to the following lemma lemma the optimum production rate can be determined by the following two conditions if po pmin d then d p b if po pmin d then p d accounting for carbon emissions taxes and emissions allowances in the total cost function permits studying the impact of the eu and us greenhouse gas emissions reduction programmes on the total cost of operating a supply chain and on the managerial decisions taken by the supply chain partners further by combining both approaches we are able to study the impact of a third greenhouse gas emissions reduction system that combines emissions allowances and emissions taxes and that to the authors knowledge has not yet been studied in the literature international journal of production research or been applied in practice in the following section we provide numerical examples to illustrate the behaviour of the supply chain for alternative greenhouse gas emissions reduction schemes the per unit of time supply chain inventory related cost ssc is the only cost term dependent on to simplify and accelerate the search for an optimal solution i e p we determine upper and lower bound values for to do so assume that holds a real number and that ssc is differentiable over where ssc is convex since ssc by setting the first partial derivative of ssc equal to zero i e ssc and solving for to get sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ sm ðhm þ hr þ ðpþ sr hm d pþ pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ where ðpþ holds paﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ minimum value of ðpmin d þ sm ðhm þ hr þ sr hm þ and a maximum value of ðpmax b sm ðhm þ hr þ sr hm bþ if for example for a given value of p then if ðp ðp and otherwise so substituting ðpþ in equation reduces equation to a function of a single decision variable p for which constraint will no longer be needed the mathematical problem described above can be simply solved using mathematical software such as maple mathcad or mathematica or by using excel however it may be useful to provide the reader with a step by step solution procedure which is given below step set yi for i n and k b tc set tc p step compute po if po pmin d then p d if po pmin d then conduct a full search over the interval d b to find p that minimises equation step find from equation that minimises equation if set and go to step p and go to step if cep i for i n set p step compute e as in if ed elk set yk if k n set k k þ and repeat step e p and compute tc p e according to equation set p step b max b set p rﬃﬃﬃﬃﬃﬃﬃ elk cd ad þ ad b d set yk and compute tc p b according to equation and as shown in steps and if p b tc p e p p b step if tc p if k set k k and go to step e and compute tc p according to equation and as shown in steps and step set p p step initialises the algorithm first emissions penalties are neglected by setting all yi equal to zero steps and compute the optimal values of p and for the solution found in steps and step computes the number of emissions limits that are violated and adds the corresponding penalty costs to the total costs in steps and the break points in the emissions penalty function are tested for optimality the best solution found in this procedure is then selected as the final solution in step numerical examples in the previous section a mathematical model was proposed that considers greenhouse gas emissions as well as emissions penalties and emissions taxes in a supply chain model in this section we intend to answer the following research questions which production rate should a manufacturer choose if it faces a carbon tax which production rate should a manufacturer choose if it faces a carbon emissions penalty which production rate should a manufacturer choose if it faces a combination of a carbon tax and emissions penalty m y jaber et al can it be beneficial for a manufacturer not to fulfil the entire annual demand if it faces emissions penalty and a capacity constraint which production rate should a manufacturer choose if it can buy additional emissions allowances on the market and how much emissions allowances should be bought how does cooperation between a manufacturer and a retailer impact the generation of greenhouse gas emissions in a supply chain example impact of carbon taxes in this example we address the first research question and examine the case of a carbon tax system with no emissions penalty let d units year hm unit year hr unit year sm sr a ton b ton year c ton unit cec ton and cep year no emissions penalty the mathematical programming problem described in equations generates an optimal solution at p units year and where the total cost is tc year when the system operates at its minimum production capacity pmin units year the minimum total cost tc year occurs at on the other hand when the system operates at the production rate that minimises the emissions function described in equation the minimum total cost tc year occurs at the model was investigated for varying values of p while minimising the total cost the behaviour of emc ssc and tc ssc þ emc are shown in figure the results indicate that the optimal emissions rate does not necessarily result in a minimum total cost tc since p po this suggests that considering a non optimal production rate that corresponds to minimum emissions results in an unnecessary increase in the total cost tc this also shows the importance of considering both supply chain inventory related and emissions costs when defining an optimal production rate further it is clear that when a carbon tax is imposed on the system we have p pmin po i e example impact of emissions penalty in this example we address the second research question and consider the case of a penalty that is imposed if emissions exceed a specified limit in this example a carbon tax is not applied cec let cep year cost tc scc emc production rate figure behaviour of emissions cost emc supply chain cost scc and total cost tc for varying values of p where cep and cec international journal of production research and eil emax tons year while the remaining input parameters are the same as those of example the total cost function tc is at its minimum when p pmin units year and where the total cost is year when the system operates at po b units year the best solution occurs at where the minimum cost is year the model was investigated for varying values of p for cep year and cep year while minimising the total cost the behaviour of emc ssc and tc are shown in figure as shown in figure considering a penalty for emissions with no carbon tax imposed on the system or with no additional costs for emissions beyond the allowable limit penalty results in an optimal solution that occurs i at a cost tc scc emc 2600 2200 1600 production rate b cost tc scc emc 2600 2200 1600 production rate figure behaviour of emissions cost emc supply chain cost scc and total cost tc for varying values of p when a cep and b cep year and cec m y jaber et al pmin units year and where the total cost is year case cep year or ii at p units year and where the total cost is year case cep year case produces a lower total cost than case because the production and demand rates are almost synchronised where the savings in holding costs are more than the additional emissions tax resulting from operating at pmin rather than p case suggests that imposing a large penalty for exceeding the permissible emissions limit entices the manufacturer to operate at a production rate p where p upper and lower bounds of p are computed from equation that minimises the emissions function given in equation in contrast to the scenario studied in the first example it is clear here that in the case when an emissions penalty is imposed on the system the manufacturer optimal production rate is either pmin or the smallest production rate that keeps its emissions below the specified limits to avoid paying penalties for exceeding them this production rate can be calculated by setting equation equal to eli and solving for p to get pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ b eli dþ peli where peli min by substituting the values of the parameters a b c and d in equation thus we conclude that p pmin peli when an emissions penalty is imposed on the system example impact of carbon taxes and emissions penalty this example addresses research question and considers the case when a penalty is charged for exceeding the emissions limit in addition to a carbon tax let cep year and eli emax tons year where i while keeping the values of the remaining input parameters as specified in example the mathematical programming problem described in equations generates an optimal solution at p units year and where the total cost is year for pmin units year and the total cost is year when the system operates at the production rate that minimises the emissions function in equation po b units year the optimal solution occurs at where the total cost is year the model was investigated for varying values of p while minimising the total cost the behaviour of emc ssc and tc are shown in figure cost tc scc emc 2800 2600 2400 2200 1900 1600 1400 production rate figure behaviour of emissions cost emc supply chain cost scc and total cost tc for varying values of p when cep year and cec ton international journal of production research as shown in figure when a penalty is charged for exceeding the permissible limit of emissions and when a carbon tax is imposed on the system a manager may decide to operate the system at pmin pmin with minimum inventory cost ssc but with an emissions penalty and an emissions tax emc or operate the system at p p which minimises the emissions function emc and avoids paying the penalty where ssc is not minimum and an emissions tax still has to be paid in case a p value between peli and po can in principle be optimal i e p peli po assuming a smaller penalty instead of results in enticing the manufacturer to increase its production rate to the point that reduces the emissions its system generates the effect of the emissions cost would be more noticeable for values of the production rate that deviate from the optimal one thus for such a system a legislator may impose a reasonable penalty that does not heavily burden the manufacturer while ensuring that environmentally viable results are attained in addition a low emissions window ed gives the decision maker the flexibility to adjust its production rate p for less than a increase in the manufacturer total cost from to example impact of demand reductions and capacity constraints in this example we answer research question and examine the case where the vendor manufacturer faces a capacity constraint and has the option to fulfil only a fraction of its customer demand this example adopts the input parameters from example except for but under the assumption that the manufacturer lacks the flexibility to vary the production rate over a range as in the previous examples the maximum attainable production rate owing to limited resources or capacity constraints is assumed to be units year the minimum of the total cost function when no capacity constraint is imposed was found to occur at p units year and where the total cost is year however this production rate is not attainable in the present example this suggests that it is beneficial to reduce the production rate to p and deliver buyer cycles in a vendor cycle i e where the minimum cost is year another practical option can be reached by not fulfilling a fraction of the annual demand reducing d from to units year for example results in an optimal solution with a total cost of year attained when p units year and the model was also investigated for varying values of p while minimising the total cost the behaviour of figure behaviour of emissions cost emc supply chain cost scc and total cost tc for varying values of p when cep cec and pmax m y jaber et al table emissions penalty schedule i emissions limit i penalty charged cep i ed 220 ed ed ed ed ed emc ssc and tc are shown in figure note that the dashed lines in figure indicate the infeasible region i e p figure illustrates that if the profit margin is narrow e g as in the steel industry it could be reasonable not to fulfil a portion of the annual demand in order to avoid paying a hefty penalty in this case switching from a minimum production rate with nominal demand and excessive emissions tc year and ed tons year to a lower demand level and minimum emissions results in savings of per year these savings can be favourable even with a loss of few units of sales however if there is a cost associated with each unit of demand lost cl then the savings in total cost should be weighed against the total cost of lost units cl that is if cl then not fulfilling a portion of demand may be a good policy to avoid paying a penalty example impact of emissions allowance trading in this example we address research question and consider the case of multiple penalties imposed at different emissions limits or intervals combined with an emissions penalty see the schedule in table with and without carbon tax i e cec and cec multiple penalties correspond to the cap and trade system where companies that exceed their emissions allowances have to buy additional emissions certificates on the market using alternative values for cep i with cep i cep takes account of the fact that buying certificates may be associated with increasing costs as the demand for emissions certificates increases using the input parameters from example except for the model was investigated for varying values of p 1200 while minimising the total cost note that to find the optimal solution the break points of the emissions cost function have to be tested for optimality using equation as carried out in steps and of the solution procedure the behaviour of emc ssc and tc are shown in figure figure suggests that there are three main alternatives p to consider a minimum production rate p 1200 with a double emissions penalty and totalling yi cep i with a total cost of year rate p that results in less emissions and one penalty and a production totalling y c with a total cost of year and a higher production i ep i rate p that results in less emissions and no penalty and with a total cost of 329 year in this case going for minimum emissions and no penalties is not an optimal solution and going for maximum emissions is also not an optimal solution on the contrary a mixture of emissions penalties and savings from supply chain costs is optimal figure was reproduced for the case when an emissions tax and an emissions penalty are imposed the optimal policy was found to occur at p where yi with a total cost of 54 year the behaviour of emc ssc and tc are shown in figure example manufacturer retailer coordination in this example the importance of coordination between the vendor manufacturer and the buyer retailer is examined the input parameters from examples to are used in this example in the no coordinated case the sum of equation shown in the appendix and the total emissions and penalty costs i e nc ð pþ nc sc þ em c are optimised for and p the optimised values of the coordinated case were determined earlier in the corresponding examples the results are summarised in table international journal of production research cost tc scc emc 1200 1500 1800 2100 2400 2700 3000 production rate figure behaviour of emissions cost emc supply chain cost scc and total cost tc for varying values of p for different cep i values and cec cost 000 tc 000 scc emc 000 1200 1500 1800 2100 2400 2700 3000 production rate figure behaviour of emissions cost emc supply chain cost scc and total cost tc for varying values of p for different cep i values and cec m y jaber et al table optimal production inventory policies for coordinated and no coordinated cases example coordination p tc emc no yes no yes no yes no yes no yes no yes 1100 1100 1200 1200 1676 1742 387 notes the p tc and emc values are rounded to the nearest integer a pmax d bpmax d the results show that for the data of example cec cep coordination reduced the supply chain cost by about from to 289 and increased emissions and penalty costs by about from to respectively the results also show that for examples cec cep cec cepi cec cep pmax 4b cec cep pmax d and cec cep1 coordination reduces the supply chain inventory related cost and the sum of the emissions and penalty costs by about from 417 to and from to about from to and from to about from to 387 and from to 4000 about from 266 to 977 with no emissions and penalty costs and about from 605 to 290 and from to respectively summary and conclusion the paper investigated the european union emissions trading system eu ets in a two level vendor buyer supply chain context this work is believed to be the first to examine such a system from a user or an operations management point of view the supply chain model considered in the paper is investigated for a coordination mechanism while accounting for greenhouse gas emissions e g generated from the manufacturer processes a mathematical model was modified to account for emissions tax and penalty paid for exceeding emissions limit the numerical examples considered in the paper illustrate the behaviour of the supply chain cost function for several possible scenarios that may describe different legislative systems it was found that a policy that considers a penalty for emissions only results in more than one option for the supply chain decision maker which might result in an optimal solution that recommends producing excessive amounts of greenhouse gas emissions a policy that considers a combination of carbon tax and emissions penalty was found to be the most effective one as the optimal solution generated was usually associated with low emissions supply chain coordination was also found to minimise the total system cost when emissions and penalty costs were considered however the reduction was in inventory related costs with no change in the sum of the emissions and penalty costs perhaps the effect of coordination on emissions reduction would be different if a more complex supply chain structure jaber and goyal was considered rather than the simple vendor buyer one of the paper this will be dealt with in a follow up paper the model developed in the paper may be found to be a useful tool for decision makers who wish to optimise the performance of their supply chains in conjunction with the financial obligations that emitting greenhouse gases brings future work could concentrate on studying alternative types of pollutants such as the production of scrap in manufacturing processes for example and investigating workable incentive schemes that legislators may offer to companies to entice them to reduce the negative impact that their manufacturing processes and operations have on the environment further it would be interesting to investigate how other relationships than that considered in the paper which assumes that the emissions generated by a facility is a convex function of its output rate affect the international journal of production research performance of logistics and inventory systems finally introducing stochastic variables into the model developed in the paper seems to be promising one example is to treat the amount of emissions caused by the production process at a certain production rate as a random variable another example is to address the question of whether a company that exceeds its emissions allowance is fined or not this could be done by subjecting the fine to a random variable as well if exceeding the emissions allowance leads to a fine with a probability of less than it would then be interesting to study the behaviour of supply chains so as to give legislators some guidelines as to how fines should be structured so that companies adhere to regulations many design problems in engineering are typically multiobjective under complex nonlinear constraints the algorithms needed to solve multiobjective problems can be signiﬁcantly different from the methods for single objective optimization computing effort and the number of function evaluations may often increase signiﬁcantly for multiobjective problems metaheuristic algorithms start to show their advantages in dealing with multiobjective optimization in this paper we formulate a new cuckoo search for multiobjective optimization we validate it against a set of multiobjective test functions and then apply it to solve structural design problems such as beam design and disc brake design in addition we also analyze the main characteristics of the algorithm and their implications crown copyright published by elsevier ltd all rights reserved keywords cuckoo search metaheuristic multiobjective optimization introduction engineering design often concerns multiple design objectives under complex highly nonlinear constraints in reality different objectives often conﬂict each other and sometimes truly optimal solutions may not exist at all and some compromise and approximations are often needed in addition to these challenges and complexity a design problem is subjected to various design constraints limited by design codes or standards material properties and choice of available resources and costs even for global optimization problems with a single objective if the design functions are highly nonlinear global optimality is not easy to reach furthermore many real world problems are often np hard which means there is no known efﬁcient algorithm which can be used for a given problem therefore for a given problem heuristic choices of algorithms and techniques are usually used in practice in combination with the problem speciﬁc knowledge as some guidance on the other hand metaheuristic algorithms are very powerful in dealing with this kind of optimization and there are many review articles and excellent textbooks in contrast with single objective optimization multiobjective problems are typically much difﬁcult and complex in a single objective we have to ﬁnd the optimal solution which is often a single point in the solution space except the case where there are multiple equally optimal points for a multiobjective optimization problem there are multiple optimal solutions which n corresponding author present address mathematics and scientiﬁc computing national physical laboratory teddington uk e mail addresses xin she yang npl co uk cam ac uk x s yang form the so called pareto front in order to get the sense of the unknown pareto front we have to generate many different solution points and therefore computational effort will increase depending on the number of approximate points complexity of the problem and the way of handling solution diversity ideally the solutions obtained on the pareto front should distribute relatively uniformly and un biased however there is no technique to ensure that this can be achieved in practice from the implementation point of view algorithms work well for single objective optimization usually cannot directly work for multiobjective problems unless under special circumstances such as combining multiobjectives into a single objective using some weighted sum method substantial modiﬁcations are needed to make an algorithm work in addition to these difﬁculties a further challenge is how to generate solutions with enough diversity so that new solutions can sample the search space efﬁciently furthermore real world optimization problems always involve some degree of uncertainty or noise for example materials properties for a design product may vary signiﬁcantly an optimal design should be robust enough to allow such inhomogeneity and also provides good choice for decision makers or designers despite these challenges multiobjective optimization has many powerful algorithms with many successful applications in addition metaheuristic algorithms start to emerge as a major player for multiobjective global optimization they often mimic the successful characteristics in nature especially biological systems many new algorithms are emerging with many important applications recently a new metaheuristic search algorithm called cuckoo search cs has been developed by yang and deb preliminary studies show that it is very promising and could outperform existing algorithms such as pso in this paper we will see front matter crown copyright published by elsevier ltd all rights reserved doi j cor x s yang s deb computers operations research 1624 extend cs to solve multiobjective problems and formulate a multiobjective cuckoo search mocs algorithm we will ﬁrst validate it against a subset of multiobjective test functions then we will apply it to solve design optimization problems in engineering including bi objective beam design and a design of a disc brake meanwhile we will also discuss the unique features of the proposed algorithm as well as topics for further studies here m is a minimum step and g is a scale parameter clearly as we have rﬃﬃﬃﬃﬃﬃ g lðs g mþ this is a special case of the generalized le vy distribution in general le vy distribution should be deﬁned in terms of fourier transform b fðkþ multiobjective cuckoo search in order to extend the cuckoo search for single optimization to solve multiobjective problems let us brieﬂy review the interesting breed behavior of certain cuckoo species then we will outline the basic ideas and steps of the proposed algorithm cuckoo behavior cuckoo are fascinating birds not only because of the beautiful sounds they can make but also because of their aggressive reproduction strategy some species such as the ani and guira cuckoos lay their eggs in communal nests though they may remove others eggs to increase the hatching probability of their own eggs quite a number of species engage the obligate brood parasitism by laying their eggs in the nests of other host birds often other species there are three basic types of brood parasitism intraspeciﬁc brood parasitism cooperative breeding and nest takeover some host birds can engage direct conﬂict with the intruding cuckoos if a host bird discovers the eggs are not its owns it will either throw these alien eggs away or simply abandons its nest and builds a new nest elsewhere some cuckoo species such as the new world broodparasitic tapera have evolved in such a way that female parasitic cuckoos are often very specialized in the mimicry in color and pattern of the eggs of a few chosen host species this reduces the probability of their eggs being abandoned and thus increases their reproductivity efﬁciency of le vy ﬂights in nature animals search for food in a random or quasirandom manner in general the foraging path of an animal is effectively a random walk because the next move is based on the current location state and the transition probability to the next location which direction it chooses depends implicitly on a probability which can be modeled mathematically for example various studies have shown that the ﬂight behavior of many animals and insects has demonstrated the typical characteristics of le vy ﬂights a recent study by reynolds and frye shows that fruit ﬂies or drosophila melanogaster explore their landscape using a series of straight ﬂight paths punctuated by a sudden turn leading to a le vy ﬂight style intermittent scale free search pattern even light can be related to le vy ﬂights subsequently such behavior has been applied to optimization and optimal search and preliminary results show its promising capability broadly speaking le vy ﬂights are a random walk whose step length is drawn from the le vy distribution often in terms of a b simple power law formula lðsþ where o b r is an index mathematically speaking a simple version of le vy distribution can be deﬁned as rﬃﬃﬃﬃﬃﬃ g g if o m o exp 2ðs mþ ðs lðs g mþ if r o b r where a is a scale parameter the inverse of this integral is not easy as it does not have analytical form except for a few special cases for the case of b we have fðkþ ak whose inverse fourier transform corresponds to a gaussian distribution another special case is b and we have fðkþ ð5þ which corresponds to a cauchy distribution pðx g mþ g p þðx where m is the location parameter while g controls the scale of this distribution for the general case the inverse integral z b lðsþ dk p can be estimated only when is large we have lðsþ abgðbþsinðpb þ b here gðzþ is the gamma function z gðzþ t z e t dt in the case when n is an integer we have gðnþ ðn fig shows an example of drawing steps which obey a le vy distribution while fig shows their path during a le vy ﬂight it is worth pointing out that le vy ﬂights are more efﬁcient than brownian random walks in exploring unknown large scale search space there are many reasons to explain this efﬁciency and one of them is due to the fact that the variance of le vy ﬂights ðtþ t b b r increases much faster than the linear relationship i e ðtþ t of brownian random walks fig distribution of consecutive steps x s yang s deb computers operations research 1624 similarity of solutions to the other solution this implies that the mutation is a vectorized operator via the combination of le vy ﬂights and differential quality of the solutions these unique features work in a combination can ensure the efﬁciency of the proposed algorithm based on these three rules the basic steps of the multiobjective cuckoo search mocs can be summarized as the pseudo code shown in fig when generating new solutions xðt þ for say cuckoo i a le vy ﬂight is performed xðti þ xðtþ þ a levyð bþ i where a is the step size which should be related to the scales of the problem of interest in most cases we can use a in order to accommodate the difference between solution quality we can also use a ðxðtþ xðtþ þ j i fig le vy ﬂights in steps multiobjective cuckoo search algorithm in the original cuckoo search for single objective optimization by yang and deb the following three idealized rules are used each cuckoo lays one egg at a time and dumps it in a randomly chosen nest the best nests with high quality of eggs solutions will carry over to the next generations the number of available host nests is ﬁxed and a host can discover an alien egg with a probability pa a in this case the host bird can either throw the egg away or abandon the nest so as to build a completely new nest in a new location for multiobjective optimization problems with k different objectives then we modify the ﬁrst and last rules to incorporate multiobjective needs each cuckoo lays k eggs at a time and dumps them in a randomly chosen nest egg k corresponds to the solution to the kth objective each nest will be abandoned with a probability pa and a new nest with k eggs will be built according to the similarities differences of the eggs some random mixing can be used to generate diversity for simplicity this last assumption can be approximated by a fraction pa of the n nests being replaced by new nests with new random solutions at new locations for the maximization of objectives the quality or ﬁtness of a solution can simply be proportional to each objective function and a non dominated solution should be sought mathematically speaking the ﬁrst rule can be converted into a randomization process so that a new solution can be randomly generated either by a random walk or by le vy ﬂight at the same time a localized random permutation is carried out over solutions which can be considered as a form of crossover for each nest there can be k solutions which are generated in the same way as in essence the second rule corresponds to elitism so that the best solutions are passed onto the next generation and such selection of the best helps to ensure the algorithm converge properly in addition the third rule can be considered as the mutation so that the worst solutions are discarded with a probability and new solutions are generated according to the where is a constant while the term in the bracket corresponds to the difference of two randomly solutions this mimics that fact that similar eggs are less likely to be discovered and thus new solutions are generated by the proportionality of their difference the product means entry wise multiplications le vy ﬂights essentially provide a random walk while their random steps are drawn from a le vy distribution for large steps u t b levy o b which has an inﬁnite variance with an inﬁnite mean here the consecutive jumps steps of a cuckoo essentially form a random walk process which obeys a power law step length distribution with a heavy tail in addition a fraction pa of the worst nests can be abandoned so that new nests can be built at new locations by random walks and mixing the mixing of the eggs solutions can be performed by random permutation according to the similarity difference to the host eggs obviously the generation of step size samples is not trivial using le vy ﬂights a simple scheme discussed in detail by yang can be summarized as ðtþ ðxðtþ j xi þ levyðbþ u b ðtþ ðxðtþ j xi þ where u and v are drawn from normal distributions that is u þ v þ fig multiobjective cuckoo search mocs x s yang s deb computers operations research 1624 with su þ bþsinðpb þ bþ b sv here g is the standard gamma function it is worth pointing out that the unique features of the mocs algorithm are exploration by le vy ﬂight mutation by a combination of le vy ﬂights and vectorized solution difference crossover by selective random permutation and elitism cuckoo search essentially uses a good combination of all these basic components and thus it is potentially more powerful than algorithms using one or some of these components for example particle swarm optimization use an update of velocity vector which consists a term of eðxi g n þ which is the difference of the current solution xi and the current global best g n this is the main way for randomization which limits the steps that are proportional to the solution difference in cuckoo search however the randomization can be more efﬁcient as the steps obey a le vy distribution which can be approximated by a power law therefore the steps consist of many small steps and occasionally large step long distance jumps comparing with pso this long jumps may increase the search efﬁciency of cuckoo search signiﬁcantly in some cases especial for multimodal nonlinear problems pareto front a solution vector u un þt a f is said to dominate another vector v vn þt if and only if ui r vi for ng and i a ng ui o vi in other words no component of u is larger than the corresponding component of v and at least one component is smaller similarly we can deﬁne another dominance relationship by u v u v pf fsa a s sg the stopping criterion can be deﬁned in many ways we can either use a given tolerance or a ﬁxed number of iterations from the implementation point of view a ﬁxed number of iterations is not only easy to implement but also suitable to compare the closeness of pareto front of different functions so we have set the ﬁxed number iterations as which is sufﬁcient for most problems if necessary we can also increase it to a larger number in order to generate more optimal points on the pareto front we can do it in two ways increase the population size n or run the program a few more times through simulations we found that to increase of n typically leads to a longer computing time than to re run the program a few times this may be due to the fact that manipulations of large matrices or longer vectors usually take longer so to generate points using a population size requires to run the program four times which is easily done within a few minutes therefore in all our simulations we will use the ﬁxed parameters b and pa multiobjective test functions there are many different test functions for multiobjective optimization but a subset of a few widely used functions provides a wide range of diverse properties in terms pareto front and pareto optimal set to validate the proposed mocs we have selected a subset of these functions with convex non convex and discontinuous pareto fronts we also include functions with more complex pareto sets to be more speciﬁc in this paper we have tested the following ﬁve functions schaffer min min sch test function with convex pareto front it is worth pointing out that for maximization problems the dominance can be deﬁned by replacing with therefore a point xn a f is called a non dominated solution if no solution can be found that dominates it the pareto front pf of a multiobjective can be deﬁned as the set of non dominated solutions so that f ðxþ f ðxþ g t where f ðf f k þ to obtain a good approximation to pareto front a diverse range of solutions should be generated using efﬁcient techniques in the present approach le vy ﬂights ensure the good diversity of the solutions as we can see from later simulations xi d a i where d is the number of dimensions the pareto optimality is reached when g function with a non convex front f f ðxþ f ðxþ g g function with a discontinuous front sﬃﬃﬃﬃﬃ þ g g f ðxþ f ðxþ g numerical results parametric studies the proposed multiobjective cuckoo search is implemented in matlab and computing time is within a few seconds to less than a minute depending on the problem of interest we have tested it using a different range of parameters such as population size n le vy exponent b and discovery probability pa by varying n to b and pa we found that the best parameters for most applications are to pa to and b or in addition the parameter can be in the range of and the value works well for most applications rx r qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ f ðxþ f g þ pd f ðxþ ðx function with a convex front or in term of the pareto optimal set in the search space pf n fx a f a f f þ f ðxþg where g in functions and is the same as in function in the function varies from to and from to lz function x jp f þ xj sin þ d j a j pﬃﬃﬃﬃﬃ x jp f þ þ xj sin þ d j a j where is odd and r j rdg and j is and peven ﬃﬃﬃﬃﬃ j r dg this function has a pareto front f f with a x s yang s deb computers operations research 1624 pareto set jp xj sin þ d j d x1 a after generating pareto points by mocs the pareto front pﬃﬃﬃﬃﬃ generated by mocs is compared with the true front f f of see fig let us deﬁne the distance or error between the estimate pareto front pfe to its correspond true front pft as ef jpf e pf t n x ðpf ej pf t where n is the number of points the convergence property can be viewed by following the iterations fig shows the exponential like decrease of ef as the iterations proceed left and the logarithmic scale right we can see clearly that our mocs algorithm indeed converges almost exponentially the results for all the functions are summarized in table and the estimated pareto fronts and true fronts of other functions are shown in figs and in order to compare the performance of the proposed mocs with other established multiobjective algorithms we have carefully selected a few algorithms with available results from the literature in case of the results are not available we have tried to implement the algorithms using well documented studies and then generated new results using these algorithms in particular we have used other methods for comparison including vector evaluated genetic algorithm vega nsga ii multiobjective differential evolution mode differential evolution for multiobjective optimization demo multiobjective bees algorithms bees and strength pareto evolutionary algorithm spea the performance measures in terms of generalized distance dg are summarized in table for all the above major methods design optimization design optimization especially design of structures has many applications in engineering and industry as a result there are many different benchmarks with detailed studies in the literature among the well known benchmarks are the welded beam design and disc brake design in the rest of this paper we will solve these two design benchmarks using mocs true pareto front mocs design of a welded beam multiobjective design of a welded beam is a classical benchmark which has been solved by many researchers the problem has four design variables the width w and length l of the welded area the depth d and thickness h of the main beam the objective is minimize both the overall fabrication cost and the end deﬂection d f table summary of results fig pareto front of a comparison of the front found by mocs and the true front x functions errors iterations errors iterations sch lz 3e 3000 4000 fig convergence of the proposed mocs the least square distance from the estimated front to the true front of for the ﬁrst iterations left and the logarithmic scale for iterations right x s yang s deb computers operations research 1624 true pareto front mocs true pareto front mocs f f fig pareto fronts of test functions sch and true pareto front mocs true pareto front mocs f f f f fig pareto fronts of test functions and lz x table comparison of dg for n and t iterations zdt2 sch lz vega nsga ii mode demo bees spea mocs 80e 34e 29e 91e 73e 79e 04 17e 07 02 03 03 88e 02 03 19e f methods the problem can be written as minimise f ðxþ l þ lþ minimize f d subject to g ðxþ w hr g ðxþ dðxþ f1 fig pareto front for the bi objective beam design g ðxþ tðxþ r g ðxþ sðxþ 000 g ðxþ þ lþ g ðxþ w g ðxþ pðxþ r where sðxþ 504 000 hd l q 6000 þ x s yang s deb computers operations research 1624 dg g ðxþ mocs bees demo mode nsga ii spea vega g ðxþ f 14ðr2 r þ r þ r g ðxþ r r þ r þ the simple limits are rr r iterations fig convergence comparison for the beam design pﬃﬃﬃ ðwþ þ j qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ þ ðwþ r r rf r 3000 rs r the pareto front of solution points after iterations obtained by mocs is shown in fig where we can see that the results are smooth and are the same or better than the results obtained in the comparison of the convergence rates is plotted in the logarithmic scales in fig we can see from these two ﬁgures that the convergence of mocs are of the highest and exponential in both cases this again suggests that mocs provides better solutions in a more efﬁcient way 000hd 6000 a pﬃﬃﬃ qd j p rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ abl þb tðxþ a2 þ d dh pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ d f2 856 the simple limits or bounds are rl d r and r w hr by using the mocs we have solved this design problem the approximate pareto front generated by the non dominated solutions after iterations is shown in fig this is consistent with the results obtained by others in addition the results are more smooth with fewer iterations in order to see how the proposed mocs performance for the real world design problems we also solved the same problems using other available multiobjective algorithms the comparison of the convergence rates is plotted in the logarithmic scales in fig by comparison the convergence rates of mocs are of the highest in an exponentially decreasing way f1 fig pareto front for the disc brake design design of a disc brake r þðs minimize f ðxþ f ðxþ dg design of a multiple disc brake is another benchmark for multiobjective optimization the objectives are to minimize the overall mass and the braking time by choosing optimal design variables the inner radius r outer radius r of the discs the engaging force f and the number of the friction surface s this is under the design constraints such as the torque pressure temperature and length of the brake this bi objective design problem can be written as mocs bees demo mode nsga ii spea vega 82 r þ r þ subject to g ðxþ ðr rþ g ðxþ 200 400 800 iterations fig convergence comparison for the disc brake design 1000 x s yang s deb computers operations research 1624 the simulations for these benchmarks and test functions suggest that mocs is a very efﬁcient algorithm for multiobjective optimization it can deal with highly nonlinear problems with complex constraints and diverse pareto optimal sets conclusions multiobjective optimization problems are typically very difﬁcult to solve in this paper we have successfully formulated a new algorithm for multiobjective optimization namely multiobjective cuckoo search based on the recently developed cuckoo search algorithm the proposed mocs has been tested against a subset of well chosen test functions and then been applied to solve design optimization benchmarks in structural engineering results suggest that mocs is an efﬁcient multiobjective optimizer in comparison with other algorithms cuckoo search performs well for almost all these test problems this superiority can be attributed to the fact that cuckoo search uses a combination of vectorized mutation crossover by permutation and le vy ﬂights and selective elitism among the best solutions in addition the not so good solutions can be replaced systematically by new solutions and new solutions are often generated by preferring quality solutions in the solution sets thus the mechanism of the overall search moves is more subtle and balanced compared with the simple mechanism used in particle swarm optimization obviously a more detailed validation study over a wide range of test functions will be carried out in the follow up work in the near future additional test and comparison of the proposed are highly needed in the future work we will focus on the parametric studies for a wider range of test problems including discrete and mixed type of optimization problems we will try to test the diversity of the pareto front it can generate so as to identify the way to improve this algorithm to suit for a diverse range of problems there are a few efﬁcient techniques to generate diverse pareto fronts and some combination with these techniques may improve mocs even further further research can also emphasize the performance comparison of this algorithm with other popular methods for multiobjective optimization in addition hybridization with other algorithms may also prove to be fruitful abstract in this paper we develop a randomized block coordinate descent method for minimizing the sum of a smooth and a simple nonsmooth block separable convex function and prove that it obtains an ε accurate solution with probability at least ρ in at most o n ε log ρ iterations where n is the number of blocks this extends recent results of nesterov siam j optim 362 which cover the smooth case to composite minimization while at the same time improving the complexity by the factor of and removing ε from the logarithmic term more importantly in contrast with the aforementioned work in which the author achieves the results by applying the method to a regularized version of the objective function with an unknown scaling factor we show that this is not necessary thus achieving first true iteration complexity bounds for strongly convex functions the method converges linearly in the smooth case we also allow for arbitrary probability vectors and noneuclidean norms finally we demonstrate numerically that the algorithm is able to solve huge scale regularized least squares problems with a billion variables keywords block coordinate descent huge scale optimization composite minimization iteration complexity convex optimization lasso sparse regression gradient descent coordinate relaxation gauss seidel method an extended abstract of a preliminary version of this paper appeared in the work of the first author was supported in part by epsrc grant ep mathematics for vast digital resources the second author was supported in part by the centre for numerical algorithms and intelligent software funded by epsrc grant ep and the scottish funding council p richtárik b m takác school of mathematics university of edinburgh edinburgh uk e mail peter richtarik ed ac uk m takác e mail takac mt gmail com mathematics subject classification p richtárik m takác 90c25 introduction the goal of this paper in the broadest sense is to develop efficient methods for solving structured convex optimization problems with some or all of these not necessarily distinct properties size of data the size of the problem measured as the dimension of the variable of interest is so large that the computation of a single function value or gradient is prohibitive there are several situations in which this is the case let us mention two of them memory if the dimension of the space of variables is larger than the available memory the task of forming a gradient or even of evaluating the function value may be impossible to execute and hence the usual gradient methods will not work patience even if the memory does not preclude the possibility of taking a gradient step for large enough problems this step will take considerable time and in some applications such as image processing users might prefer to see have some intermediary results before a single iteration is over nature of data the nature and structure of data describing the problem may be an obstacle in using current methods for various reasons including the following completeness if the data describing the problem is not immediately available in its entirety but instead arrives incomplete in pieces and blocks over time with each block corresponding to one variable it may not be realistic for various reasons such as memory and patience described above to wait for the entire data set to arrive before the optimization process is started source if the data is distributed on a network not all nodes of which are equally responsive or functioning it may be necessary to work with whatever data is available at a given time it appears that a very reasonable approach to solving some problems characterized above is to use block coordinate descent methods cd in the remainder of this section we mix arguments in support of this claim with a brief review of the relevant literature and an outline of our contributions block coordinate descent methods the basic algorithmic strategy of cd methods is known in the literature under various names such as alternating minimization coordinate relaxation linear and non linear gauss seidel methods subspace correction and domain decomposition as working with all the variables of an optimization problem at each iteration may be inconvenient difficult or impossible for any or all of the reasons mentioned above the variables are partitioned into manageable blocks with each iteration focused on updating a single block only the remaining blocks being fixed both for their conceptual and algorithmic simplicity cd methods were among the first optimization approaches proposed and randomized block coordinate descent methods studied in the literature see and the references therein for a survey of block cd methods in semidefinite programming we refer the reader to while they seem to have never belonged to the mainstream focus of the optimization community a renewed interest in cd methods was sparked recently by their successful application in several areas training support vector machines in machine learning optimization compressed sensing regression protein loop closure and truss topology design partly due to a change in the size and nature of data described above order of coordinates efficiency of a cd method will necessarily depend on the balance between time spent on choosing the block to be updated in the current iteration and the quality of this choice in terms of function value decrease one extreme possibility is a greedy strategy in which the block with the largest descent or guaranteed descent is chosen in our setup such a strategy is prohibitive as i it would require all data to be available and ii the work involved would be excessive due to the size of the problem even if one is able to compute all partial derivatives it seems better to then take a full gradient step instead of a coordinate one and avoid throwing almost all of the computed information away on the other end of the spectrum are two very cheap strategies for choosing the incumbent coordinate cyclic and random surprisingly it appears that complexity analysis of a cyclic cd method in satisfying generality has not yet been done the only attempt known to us is the work of saha and tewari the authors consider the case of minimizing a smooth convex function and proceed by establishing a sequence of comparison theorems between the iterates of their method and the iterates of a simple gradient method their result requires an isotonicity assumption note that a cyclic strategy assumes that the data describing the next block is available when needed which may not always be realistic the situation with a random strategy seems better here are some of the reasons i recent efforts suggest that complexity results are perhaps more readily obtained for randomized methods and that randomization can actually improve the convergence rate ii choosing all blocks with equal probabilities should intuitively lead to similar results as is the case with a cyclic strategy in fact a randomized strategy is able to avoid worst case order of coordinates and hence might be preferable iii randomized choice seems more suitable in cases when not all data is available at all times iv one may study the possibility of choosing blocks with different probabilities we do this in sect the goal of such a strategy may be either to improve the speed of the method in sect we introduce a speedup heuristic based on adaptively changing the probabilities or a more realistic modeling of the availability frequencies of the data defining each block step size once a coordinate or a block of coordinates is chosen to be updated in the current iteration partial derivative can be used to drive the steplength in the same way as it is done in the usual gradient methods as it is sometimes the case that the a function f r n r is isotone if x y implies f x f y p richtárik m takác computation of a partial derivative is much cheaper and less memory demanding than the computation of the entire gradient cd methods seem to be promising candidates for problems described above it is important that line search if any is implemented is very efficient the entire data set is either huge or not available and hence it is not reasonable to use function values at any point in the algorithm including the line search instead cheap partial derivative and other information derived from the problem structure should be used to drive such a method problem description and our contribution in this paper we study the iteration complexity of simple randomized block coordinate descent methods applied to the problem of minimizing a composite objective function i e a function formed as the sum of a smooth convex and a simple nonsmooth convex term def min f x f x ψ x x r n we assume that this problem has a minimum f f has block coordinate lipschitz gradient and ψ is a block separable proper closed convex extended real valued function block separability will be defined precisely in sect possible choices of ψ include i ψ this covers the case of smooth minimization and was considered in ii ψ is the indicator function of a block separable convex set a box i e def ψ x i sn x if x i si i otherwise where x i is block i of x r n to be defined precisely in sect and sn are closed convex sets this choice of ψ models problems with smooth objective and convex constraints on blocks of variables indeed takes on the form min f x subject to x i si i n iteration complexity results in this case were given in iii ψ x λ x for λ in this case we decompose r n into n blocks each corresponding to one coordinate of x increasing λ encourages the solution of to be sparser applications abound in for instance machine learning statistics and signal processing the first iteration complexity results for the case with a single block were given in iv there are many more choices such as the elastic net group lasso and sparse group lasso one may combine indicator functions with other block separable functions such as ψ x λ x i sn x si li u i where the sets introduce lower and upper bounds on the coordinates of x randomized block coordinate descent methods iteration complexity results strohmer and vershynin have recently proposed a randomized karczmarz method for solving overdetermined consistent systems of linear equations and proved that the method enjoys global linear convergence whose rate can be expressed in terms of the condition number of the underlying matrix the authors claim that for certain problems their approach can be more efficient than the conjugate gradient method motivated by these results leventhal and lewis studied the problem of solving a system of linear equations and inequalities and in the process gave iteration complexity bounds for a randomized cd method applied to the problem of minimizing a convex quadratic function in their method the probability of choice of each coordinate is proportional to the corresponding diagonal element of the underlying positive semidefinite matrix defining the objective function these diagonal elements can be interpreted as lipschitz constants of the derivative of a restriction of the quadratic objective onto one dimensional lines parallel to the coordinate axes in the general as opposed to quadratic case considered in this paper these lipschitz constants will play an important role as well lin et al derived iteration complexity results for several smooth objective functions appearing in machine learning shalevschwarz and tewari proposed a randomized coordinate descent method with uniform probabilities for minimizing regularized smooth convex problems they first transform the problem into a box constrained smooth problem by doubling the dimension and then apply a coordinate gradient descent method in which each coordinate is chosen with equal probability nesterov has recently analyzed randomized coordinate descent methods in the smooth unconstrained and box constrained setting in effect extending and improving upon some of the results in in several ways while the asymptotic convergence rates of some variants of cd methods are well understood iteration complexity results are very rare to the best of our knowledge randomized cd algorithms for minimizing a composite function have been proposed and analyzed in the iteration complexity sense in a few special cases only a the unconstrained convex quadratic case b the smooth unconstrained ψ and the smooth block constrained case ψ is the indicator function of a direct sum of boxes and c the regularized case as the approach in is to rewrite the problem into a smooth box constrained format first the results of can be viewed as a major generalization and improvement of those in the results were obtained independently our contribution in this paper we further improve upon and extend and simplify the iteration complexity results of nesterov treating the problem of minimizing the sum of a smooth convex and a simple nonsmooth convex block separable function we focus exclusively on simple as opposed to accelerated methods the reason for this is that the per iteration work of the accelerated algorithm in on huge scale instances of problems with sparse data such as the google problem where sparsity corresponds to each website linking only to a few other websites or the sparse problems we consider in sect is excessive in fact even the author does not recommend using the accelerated method for solving such problems the simple methods seem to be more efficient each algorithm of this paper is supported by a high probability iteration complexity result that is for any given confidence level ρ and error tolerance ε p richtárik m takác table summary of complexity results obtained in this paper algorithm objective complexity algorithm ucdc theorem convex composite algorithm ucdc theorem strongly convex composite algorithm rcds theorem convex smooth algorithm rcds theorem strongly convex smooth max f f log ε f f log ε ερ μ l ψ n μ l μ log ψ l f f f ερ lp log ε log μ f l p f f ερ we give an explicit expression for the number of iterations k which guarantee that the method produces a random iterate xk for which p f xk f ε ρ table summarizes the main complexity results of this paper algorithm uniform block coordinate descent for composite functions ucdc is a method where at each iteration the block of coordinates to be updated out of a total of n n blocks is chosen uniformly at random algorithm randomized block coordinate descent for smooth functions rcds is a method where at each iteration block i n is chosen with probability pi both of these methods are special cases of the generic algorithm sect randomized block coordinate descent for composite functions rcdc x and μ w appearing in table will be defined prethe symbols p l rw φ cisely in sect for now it suffices to say that l is a diagonal matrix encoding the block coordinate lipschitz constants of the gradient of f p is a diagonal matrix x is a measure of the squared distance of the encoding the probabilities pi rw initial iterate from the set of minimizers of problem in a norm defined by a diagonal matrix w and μφ w is the strong convexity parameter of function φ with respect to that norm let us now briefly outline the main similarities and differences between our results and those in a more detailed and expanded discussion can be found in sect composite setting we consider the composite whereas covers the unconstrained and constrained smooth setting only no need for regularization nesterov high probability results in the case of minimizing a function which is not strongly convex are based on regularizing note that in nesterov considered the composite setting and developed standard and accelerated gradient methods with iteration complexity guarantees for minimizing composite objective functions these can be viewed as block coordinate descent methods with a single block randomized block coordinate descent methods the objective to make it strongly convex and then running the method on the regularized function the regularizing term depends on the distance of the initial iterate to the optimal point and hence is unknown which means that the analysis in does not lead to true iteration complexity results our contribution here is that we show that no regularization is needed by doing a more detailed analysis using a thresholding argument theorem better complexity our complexity results are better by the constant factor of also we have removed ε from the logarithmic term general probabilities nesterov considers probabilities pi proportional to l iα where α is a parameter high probability results are proved in for α only our results in the smooth case hold for an arbitrary probability vector p general norms nesterov expectation results theorems and are proved for general norms however his high probability results are proved for euclidean norms only in our approach all results in the smooth case hold for general norms simplification our analysis is shorter in the numerical experiments section we focus on sparse regression for these problems we introduce a powerful speedup heuristic based on adaptively changing the probability vector throughout the iterations contents this paper is organized as follows we start in sect by defining basic notation describing the block structure of the problem stating assumptions and describing the generic randomized block coordinate descent algorithm rcdc in sect we study the performance of a uniform variant ucdc of rcdc as applied to a composite objective function and in sect we analyze a smooth variant rcds of rcdc that is we study the performance of rcdc on a smooth objective function in sect we compare known complexity results for cd methods with the ones established in this paper finally in sect we demonstrate the efficiency of the method on regularized least squares and linear support vector machine problems preliminaries in sect we describe the setting basic assumptions and notation sect describes the algorithm and in sect we present the key technical tool of our complexity analysis assumptions and notation block structure we model the block structure of the problem by decomposing the space r n into n subspaces as follows let u r n n be a column permutation of the n n identity matrix and further let u un be a decomposition of u into n submatrices with ui being of size n ni where i ni n p richtárik m takác clearly any vector x r n can be written uniquely as x x i uit x ri r ni also note that uit u j i ui x i where ni ni identity matrix if i j otherwise ni n j zero matrix for simplicity we will write x x x n t we equip ri with a pair of conjugate euclidean norms t i bi t t t i bi t t t ri where bi r ni ni is a positive definite matrix and is the standard euclidean inner product example let n n ni for all i and u en be the n n identity matrix then ui ei is the ith unit vector and x i eit x ri r is the ith coordinate of x also x i ei x i if we let bi for all i then t i t i t for all t r smoothness of f we assume throughout the paper that the gradient of f is block coordinate wise lipschitz uniformly in x with positive constants l l n i e that for all x r n i n and t ri we have i f x ui t i f x i li t i i f x f x i uit f x ri where def an important consequence of is the following standard inequality f x ui t f x i f x t li t i separability of ψ we assume that ψ r n r is block separable i e that it can be decomposed as follows n ψ x ψi x i i where the functions ψi ri r are convex and closed randomized block coordinate descent methods global structure for fixed positive scalars wn let w diag wn and define a pair of conjugate norms in r n by def n x w wi x i i def y w max y x x w i n wi y i i i we write tr w i wi in the the subsequent analysis we will use w l sect and w l p sect where l diag l l n and p diag pn level set radius the set of optimal solutions of is denoted by x and x is any element of that set define y x rw x max max def y x x w f y f x which is a measure of the size of the level set of f given by x in some of the results in this paper we will need to assume that rw is finite for the initial iterate and w l or w l p strong convexity of f in some of our results we assume and we always explicitly mention this if we do that f is strongly convex with respect to the norm w for some w we use w l in the nonsmooth case and w l p in the smooth case with strong convexity parameter μ f w a function φ r n r is strongly convex with respect to the norm w with convexity parameter μφ w if for all x y domφ φ y φ x φ x y x μφ w y x w where φ x is any subgradient of φ at x the case with μφ w reduces to convexity strong convexity of f may come from f or ψ or both we will write μ f w resp μψ w for the strong convexity parameter of f resp ψ it follows from that μ f w μ f w μψ w the following characterization of strong convexity will also be useful for all x y domφ and α φ αx α y αφ x α φ y μφ w α α x y w from the first order optimality conditions for we obtain f x x x for all x domf which combined with used with y x and x x yields f x f μ f w x x w x domf also it can be shown using and that μ f l p richtárik m takác norm scaling note that since μφ t w μφ w t the size of the strong convexity parameter depends inversely on the size of w hence if we want to compare convexity parameters for different choices of w we need to normalize w first a natural way of normalizing w is to require tr w tr i n if we now define def w n tr w w we have tr w n and μφ w tr w n μφ w the algorithm notice that an upper bound on f x ui t viewed as a function of t ri is readily available f x ui t f x ui t ψ x ui t f x vi x t ci x where def vi x t i f x t li t i ψi x i t and ψ j x j def ci x j i we are now ready to describe the generic randomized block coordinate descent method for solving given iterate xk algorithm picks block i k i n with probability pi and then updates the ith block of xk so as to minimize exactly in t the upper bound on f xk ui t note that in certain cases it is possible to minimize f xk ui t directly perhaps in a closed form this is the case for example when f is a convex quadratic randomized block coordinate descent methods the iterates xk are random vectors and the values f xk are random variables clearly xk depends on xk only as our analysis will be based on the expected periteration decrease of the objective function our results hold if we replace vi xk t by f xk ui t in algorithm key technical tool here we present the main technical tool which is used at the end of of our iteration complexity proofs theorem fix r n and let xk k be a sequence of random vectors in r n with xk depending on xk only let φ r n r be a nonnegative function and define ξk φ xk lastly choose accuracy level ε confidence level ρ and assume that the sequence of random variables ξk k is nonincreasing and has one of the following properties i e ξk xk ξk for all k where is a constant ii e ξk xk ξk for all k such that ξk ε where is a constant if property i holds and we choose ε and k ε log or if property ii holds and we choose k log ερ p ξ k ε ρ then proof first notice that the sequence ξkε k defined by ξkε ξk if ξk ε otherwise satisfies ξkε ε ξk ε therefore by markov inequality p ξk ε p ξkε ε e ξkε ε and hence it suffices to show that θ k ερ def where θk e ξkε assume now that property i holds we first claim that then ε ε ξ ε e ξk xk ξkε e ξk xk ε ξkε k p richtárik m takác consider two cases assuming that ξk ε from we see that ξkε ξk this ε ξk and property i gives combined with the simple fact that ξk ε e ξk xk e ξk xk ξk ξkε ξkε assuming that ξk ε we get ξkε and from monotonicity assumption ξk ε ε ξk ε hence ξk putting these together we get e ξk xk ε ε ξk ξk which establishes the first inequality in the second inequality in follows from the first by again analyzing the two cases ξk ε and ξk ε now by taking expectations in and using convexity of t t in the first case we obtain respectively θk θk k θk θk k notice that 25 is better than precisely when θk ε since θk we have θk finally letting ε θ k θk θk θk θk θk θk θk 25 ck1 therefore if we let log follows from ε ε ε log ρ ε we obtain ε log ρ ε e ε ερ now assume that property ii holds using similar arguments as those leading to ε ξkε ξkε for all k which implies we get e ξk k k θ k θ0 again establishing log ερ e ξ log ξ ερ the above theorem will be used with xk k corresponding to the iterates of algorithm and φ x f x f restarting note that similar albeit slightly weaker high probability results can be achieved by restarting as follows we run the random process ξk repeatedly r log times always starting from each time for the same number of iterations for which p ε it then follows that the probability that all r values will be larger than ε is at most r ρ note that the restarting technique randomized block coordinate descent methods demands that we perform r evaluations of the objective function this is not needed in the one shot approach covered by the theorem it remains to estimate in the two cases of theorem we argue that in case i indeed using similar arguments as in theorem this we can choose ε e ε leads to e e which by markov inequality implies that in a single run of the process we have p ε therefore k ε e ε ε e ε log iterations suffice in case i a similar restarting technique can be applied in case ii tightness it can be shown on simple examples that the bounds in the above result are tight coordinate descent for composite functions in this section we study the performance of algorithm in the special case when all probabilities are chosen to be the same i e pi for all i for easier future reference we set this method apart and give it a name algorithm the following function plays a central role in our analysis def h x t f x f x t t l ψ x t comparing with using and we get n h x t f x vi x t i i therefore the vector t x t x t n x with the components t i x defined in algorithm is the minimizer of h x t x arg min h x t t r n let us start by establishing two auxiliary results which will be used repeatedly 14 p richtárik m takác lemma let xk k be the random iterates generated by ucdc then e f xk f xk h xk t xk f n n n f xk f 30 proof e f xk xk n nf i n 16 i xk ui t i xk f xk vi xk t i xk ci xk n h xk t xk n n n h xk t xk n n n f xk ci xk n i n f xk n j ψ j xk i j i n n h x k t x k n f x k lemma for all x domf we have h x t x min y r n f y y x μ f l proof h x t x min h x t min h x y x t r n y r n min f x f x y x ψ y y x y r n min f y y r n μ f l y x l ψ y y x l l convex objective in order for lemma to be useful we need to estimate h xk t xk f from above in terms of f xk f lemma fix x x x domψ and let r x x l then f x f f x f if f x f r h x t x f r f x f otherwise proof since we do not assume strong convexity μ f w and hence randomized block coordinate descent methods h x t x lemma min f y y x min f αx α x x x α y r n min f x α f x f r α minimizing the last expression gives α min follows f x f the result we are now ready to estimate the number of iterations needed to push the objective value within ε of the optimal value with high probability note that since ρ appears in the logarithm it is easy to attain high confidence theorem choose initial point and target confidence ρ further let the target accuracy ε and iteration counter k be chosen in any of the following two ways i ε f f and k max r f f ε log max r f f f f ii ε min r f f and k r ε log f f ερ if xk is the random point generated by ucdc as applied to the convex function f then p f xk f ε ρ proof since f xk f for all k we have xk x l r l for all x x plugging the inequality lemma into 30 lemma then gives that the following holds for all k f x f n f x f e f xk f xk max f xk f k k n xk x f x f max f xk f k xk x l f x f max f xk f k r l let ξk f xk f and consider case i if we let max r f f then from we obtain e ξk xk ξk ξk ξk k 16 p richtárik m takác moreover ε ξ0 the result then follows by applying theorem consider now case ii letting that e ξk r ε notice that if ξk ε inequality implies xk max ε r ξk ξk again the result follows from theorem strongly convex objective the following lemma will be useful in proving linear convergence of the expected value of the objective function to the minimum lemma if μ f l μψ l then for all x domf we have h x t x f μ f l μψ l f x f proof letting μ f μ f l μψ μψ l and α μ f μψ μψ we have h x t x lemma min f y y r n μ f y x l min f αx α x α min α f α f x α μ f α x x l μ f μψ α α μ f α x x l f x α f x f the last inequality follows from the identity μ f μψ α μ f α a modification of the above lemma and of the subsequent results using it is possible where the assumption μ f l μψ l replaced by the slightly weaker assumption μ f l indeed in the third inequality in the proof one can replace μ f μψ by μ f the estimate gets improved a bit however we prefer the current version for reasons of simplicity of exposition we now show that the expected value of f xk converges to f linearly theorem assume μ f l μψ l and choose initial point if xk is the random point generated ucdc then e f xk f k μ f l μψ l n μψ l proof follows from lemmas and f f randomized block coordinate descent methods the following is an analogue of theorem in the case of a strongly convex objective note that both the accuracy and confidence parameters appear in the logarithm theorem assume μ f l μψ l choose initial point target accuracy level ε f f target confidence level ρ and ψ l k n μ f μ l μψ l log f f ερ if xk is the random point generated by ucdc then p f xk f ε ρ proof using markov inequality and theorem we obtain p f xk f ε e f xk f μ l μψ l k f f ρ n1 f μψ l let us rewrite the condition number appearing in the complexity bound in a more natural form l 15 tr l n μψ μψ l μ f l μψ l μ f l μψ l tr l n l μψ l μf hence it is up to the constant equal to the ratio of the average of the lipschitz constants l i and the strong convexity parameter of the objective function f with respect to the normalized norm l a regularization technique in this section we investigate an alternative approach to establishing an iteration complexity result in the case of an objective function that is not strongly convex the strategy is very simple we first regularize the objective function by adding a small quadratic term to it thus making it strongly convex and then argue that when algorithm is applied to the regularized objective we can recover an approximate solution of the original non regularized problem this approach was used in to obtain iteration complexity results for a randomized block coordinate descent method applied to a smooth function here we use the same idea outlined above with the following differences i our proof is different ii we get a better complexity result and iii our approach works also in the composite setting fix and ε and consider a regularized version of the objective function defined by p richtárik m takác def fμ x f x μ x l μ ε x l clearly fμ is strongly convex with respect to the norm l with convexity parameter μ fμ l μ in the rest of this subsection we show that if we apply ucdc to fμ with target accuracy then with high probability we recover an εapproximate solution of note that μ is not known in advance since x is not known this means that any iteration complexity result obtained by applying our algorithm to the objective fμ will not lead to a true valid iteration complexity bound unless a bound on x l is available we first need to establish that an approximate minimizer of fμ must be an approximate minimizer of f lemma if x satisfies fμ x min x r n fμ x then f x f ε proof clearly f x fμ x x rn def if we let xμ arg min x r n fμ x then by assumption fμ x fμ xμ fμ xμ min f x x x r n l f x x l f x putting all these observations together we get f x f x fμ x f x fμ xμ ε 42 f x ε the following theorem is an analogue of theorem the result we obtain in this way is slightly different to the one given in theorem in that ε is replaced by n x ε in some situations x can be significantly smaller than r theorem choose initial point target accuracy level ε f f target confidence level ρ and k n x ε l log f f ερ if xk is the random point generated by ucdc as applied to fμ then p f xk f ε ρ randomized block coordinate descent methods proof let us apply theorem to the problem of minimizing fμ composed as f ψμ with ψμ x ψ x x with accuracy level note that μψμ l μ 40 fμ fμ xμ f fμ xμ f f xμ f f x n n μ ε comparing and in view of and theorem implies that p fμ xk fμ xμ ρ it now suffices to apply lemma coordinate descent for smooth functions in this section we give a much simplified and improved treatment of the smooth case ψ as compared to the analysis in sects and of as alluded to in the above we will develop the analysis in the smooth case for arbitrary possibly non euclidean norms i i n let be an arbitrary norm in rl then its dual is defined in the usual way max t t the following lemma is a simple result which is used in without being fully articulated nor proved as it constitutes a straightforward extension of a fact that is trivial in the euclidean setting to the case of general norms since we will also need to use it and because we think it is perhaps not standard we believe it deserves to be spelled out explicitly note that the main problem which needs to be solved at each iteration of algorithm in the smooth case is of the form with i f xk and i lemma if by we denote an optimal solution of the problem def min u t t t then u αs α α r proof for α the last statement is trivial if we fix α then clearly u αs min min αs βt t β βt p richtárik m takác for fixed t the solution of the inner problem is β αs t whence u αs αs t min t α max t t αs proving the first claim next note that optimal t t in 49 maximizes t over t therefore t which implies that αs β αs t α t α αs giving the second claim finally since t depends on only we have αs β t αs t t and in particular t t therefore αs α we can use lemma to rewrite the main step of algorithm in the smooth case into the more explicit form 17 t i x arg min vi x t arg min i f x t t ri t ri i lf i x li t i i f x leading to algorithm the main utility of lemma for the purpose of the subsequent complexity analysis comes from the fact that it enables us to give an explicit bound on the decrease in the objective function during one iteration of the method in the same form as in the euclidean case f x f x ui t i x i f x t i x l i u i lf i x l i i f x i li li t i x i 2l i i f x i convex objective we are now ready to state the main result of this section theorem choose initial point target accuracy ε min f f p target confidence ρ and k l p ε log f xl f randomized block coordinate descent methods or k l p ε log if xk is the random point generated by rcds p as applied to convex f then p f xk f ε ρ proof let us first estimate the expected decrease of the objective function during one iteration of the method n f xk e f xk xk pi f xk f xk ui t i xk i n i pi i f xk i f xk w where w l p since f xk f for all k and because f is convex we get f xk f max x x f xk xk x f xk w rw whence f xk e f xk xk f xk f rw by rearranging the terms we obtain f x f e f xk f xk f xk f x w x we obtain the if we now use theorem with ξk f xk f and 2rw c1 result for k given by we now claim that ξ0 from which it follows that the result holds for k given by indeed first notice that this inequality is equivalent to f f rw now a straightforward extension of lemma in to general weights states that f is lipschitz with respect to the norm v with the constant tr l v this in turn implies the inequality f x f tr l v x x from which follows by setting v w and x v p richtárik m takác strongly convex objective assume now that f is strongly convex with respect to the norm l p see definition with convexity parameter μ f l p using with x x and y xk we obtain f f xk f xk h μ f l p μ μ f l p h f l p f xk h l p h l p where h x xk applying lemma to estimate the right hand side of the above inequality from below we obtain f f xk f l p f xk l p 54 let us now write down an efficiency estimate for the case of a strongly convex objective theorem let f be strongly convex with respect to l p with convexity parameter μ f l p choose initial point target accuracy ε f f target confidence ρ and k μ f l p log f f ερ if xk is the random point generated by rcds p as applied to f then p f xk f ε ρ proof the expected decrease of the objective function during one iteration of the method can be estimated as follows n f xk e f xk xk pi f xk f xk ui t i xk i n i pi i f xk i f xk l p 54 μ f l p f xk f after rearranging the terms we obtain e f xk f xk μ f l p e f xk f it now remains to use part ii of theorem with ξk f xk f and μ f randomized block coordinate descent methods the leading factor μ in the complexity bound can in special cases be f written in a more natural form we now give two examples uniform probabilities if pi μ f l p n for all i then 38 tr l n μ f n l μ f l μ f l n tr l n μf l probabilities proportional to the lipschitz constants if pi μ f l p in both cases μ f l p tr l μ f tr l i μ f i li tr l for all i then n tr l n μ f i is equal to n multiplied by a condition number of the form tr l n μ f w where the numerator is the average of the lipschitz constants l l n w is a diagonal matrix of weights summing up to n and μ f w is the strong convexity parameter of f with respect to w comparison of cd methods with complexity guarantees in this section we compare the results obtained in this paper with existing cd methods endowed with iteration complexity bounds smooth case ψ in table we look at the results for unconstrained smooth minimization of nesterov and contrast these with our approach for brevity we only include results for the non strongly convex case we will now comment on the contents of table in detail uniform probabilities note that in the uniform case pi n for all i we have r p nr and hence the leading term ignoring the logarithmic factor in the complexity estimate of theorem line of table coincides with the leading term in the complexity estimate of theorem line of table the second result in both cases it is r ε note that the leading term of the complexity estimate given in theorem of line of table which covers the uniform case is worse by a factor of the complexity is for achieving p f xk f ε ρ probabilities proportional to lipschitz constants if we set pi then li tr l for all i r p tr l r in this case theorem in line of table gives the complexity bound l r ignoring the logarithmic factor whereas we obtain the bound ε l r line of table an improvement by a factor of note that there ε n is a further additive decrease by the constant and the additional constant lp f f if we look at the sharper bound general probabilities note that unlike the results in which cover the choice of two probability vectors only lines and of table uniform and proportional to l i our result line of table covers the case of arbitrary probability vector p this opens the possibility for fine tuning the choice of p in certain situations so as to minimize r p logarithmic factor note that in our results we have managed to push ε out of the logarithm norms our results hold for general norms no need for regularization our results hold for applying the algorithms to f directly i e there is no need to first regularize the function by adding a small quadratic term to it in a similar fashion as we have done it in sect this is an essential feature as the regularization constants are not known and hence the complexity results obtained that way are not true valid complexity results the complexity in the case of the randomized methods gives iteration counter k for which e f xk f ε nonsmooth case ψ in table we summarize the main characteristics of known complexity results for coordinate or block coordinate descent methods for minimizing composite functions note that the methods of saha tewari and shalev shwartz tewari cover the regularized case only whereas the other methods cover the general block separable case however while the greedy approach of yun and tseng requires per iteration work which grows with increasing problem dimension our randomized strategy can be implemented cheaply this gives an important advantage to randomized methods for problems of large enough size the methods of yun tseng and saha tewari use one lipschitz constant only the lipschitz constant l f of the gradient of f with respect to the standard euclidean norm note that maxi l i l f i l i if n is large this constant is typically much larger than the block coordinate constants l i shalev shwartz and tewari use coordinate lipschitz constants but assume that all of them are the same this is suboptimal as in many applications the constants l i will have a large variation and hence if one chooses β maxi l i for the common lipschitz constant steplengths will necessarily be small see fig in sect let us now compare the impact of the lipschitz constants on the complexity estimates for simplicity assume n n and let u x the estimates are listed in table it is clear from the last column that the the approach with individual constants l i for each coordinate gives the best complexity 26 p richtárik m takác numerical experiments in this section we study the numerical behavior of rcdc on synthetic and real problem instances of two problem classes sparse regression lasso sect and linear support vector machines sect as an important concern in sect is to demonstrate that our methods scale well with size our algorithms were written in c and all experiments were run on a pc with gb ram sparse regression lasso consider the problem min x rn ax b λ x where a an rm n b rm and λ the parameter λ is used to induce sparsity in the resulting solution note that is of the form with f x ax b and ψ x λ x moreover if we let n n and ui ei for all i then the lipschitz constants l i can be computed explicitly l i ai computation of t t i x reduces to the soft thresholding operator in some of the experiments in this section we will allow the probability vector p to change throughout the iterations even though we do not give a theoretical justification for this with this modification a direct specialization of rcdc to takes the form of algorithm if uniform probabilities are used throughout we refer to the method as ucdc instance generator in order to be able to test algorithm under controlled conditions we use a variant of the instance generator proposed in sect of the generator was presented for λ but can be easily extended to any λ in it one chooses the sparsity level randomized block coordinate descent methods table the time it takes for ucdc to complete a block of n iterations increases linearly with a and does not depend on of a and the optimal solution x after that a b x and f f x are generated for details we refer the reader to the aforementioned paper in what follows we use the notation a and x to denote the number of nonzero elements of matrix a and of vector x respectively speed versus sparsity in the first experiment we investigate on problems of size m and n the dependence of the time it takes for ucdc to complete a block of n iterations the measurements were done by running the method for n iterations and then dividing by on the sparsity levels of a and x looking at table we see that the speed of ucdc depends roughly linearly on the sparsity level of a and does not depend on x at all indeed as a increases from through to the time it takes for the method to complete n iterations increases from about through to about this is to be expected since the amount of work per iteration of the method in which coordinate i is chosen is proportional to ai computation of α ai 22 and gk efficiency on huge scale problems tables and present typical results of the performance of ucdc started from on synthetic sparse regression instances of big huge size the instance in the first table is of size m 2 and n with a having 107 nonzeros and the support of x being of size 000 in both tables the first column corresponds to the full pass iteration counter k n that is after k n coordinate iterations the value of this counter is reflecting a single pass through the coordinates the remaining columns correspond to respectively the size of the current residual f xk f relative to the initial residual f x0 f size xk of the support of the current iterate xk and time in seconds a row is added whenever the residual initial residual is decreased by an additional factor of let us first look at the smaller of the two problems table after n coordinate iterations ucdc decreases the initial residual by a factor of and this takes about a minute and a half note that the number of nonzeros of xk has stabilized at this point at 000 the support size of the optima solution the method has managed to identify the support after s the residual is decreased by a factor of this surprising convergence speed and ability to find solutions of high accuracy can in part be explained by the fact that for random instances with m n f will typically be strongly convex in which case ucdc converges linearly theorem it should also be noted that decrease factors this high would rarely be needed in practice however it is nevertheless interesting to see that a simple algorithm can achieve such high levels of accuracy on certain problem instances in huge dimensions ucdc has a very similar behavior on the larger problem as well table 7 note that a has billion nonzeros in n iterations the initial residual is decreased by a factor of and this takes less than an hour and a half after less than a day the residual is decreased by a factor of 000 note that it is very unusual for convex optimization randomized block coordinate descent methods table 7 performance of ucdc on a sparse regression instance with a billion variables and billion nonzeros in matrix a table ucdc needs many more iterations when m n but local convergence is still fast methods equipped with iteration complexity guarantees to be able to solve problems of these sizes performance on fat matrices m n when m n then f is not strongly convex and ucdc has the complexity o nε log theorem in table we illustrate the behavior of the method on such an instance we have chosen m n a 107 and x note that after the first 010 n iterations ucdc decreases the residual by a factor of only this takes less than 19 min however the decrease from to is done in 15 n iterations and takes s only suggesting very fast local convergence comparing different probability vectors nesterov considers only probabilities proportional to a power of the lipschitz constants pi lα n i i l iα α in fig we compare the behavior of rcdc with the probability vector chosen according to the power law for three different values of α and fig development of f xk f for sparse regression problem with λ left and λ right all variants of rcdc were compared on a single instance with m 000 n 2 000 and x different instances produced by the generator yield similar results and with λ the plot on the left corresponds to λ the plot on the right to λ note that in both cases the choice α is the best in other words coordinates with large l i have a tendency to decrease the objective function the most however looking at the λ case we see that the method with α stalls after about 000 iterations the reason for this is that now the coordinates with small l i should be chosen to further decrease the objective value however they are chosen with very small probability and hence the slowdown a solution to this could be to start the method with α and then switch to α later on on the problem with λ this effect is less pronounced this is to be expected as now the objective function is a combination of f and ψ with ψ exerting its influence and mitigating the effect of the lipschitz constants coordinate descent versus a full gradient method in fig 2 we compare the performance of rcdc with the full gradient fg algorithm with the lipschitz constant l f g λmax a t a for four different distributions of the lipschitz constants l i note that maxi l i l f g i l i since the work performed during one iteration of fg is comparable with the work performed by ucdc during n coordinate iterations for fg we multiply the iteration count by n in all four tests we solve instances with a 000 000 this will not be the case for certain types of matrices such as those arising from wavelet bases or fft of higher accuracy it is recommended to switch to α in fact it turns out that we can do much better than this using a shrinking heuristic 7 speedup by adaptive change of probability vectors it is well known that increasing values of λ encourage increased sparsity in the solution of 56 in the experimental setup of this section we observe that from certain iteration onwards the sparsity pattern of the iterates of rcdc is a very good predictor of the sparsity pattern of the optimal solution x the iterates converge to more specifically we often observe in numerical experiments that for large enough k the following holds xk i l k xl i x i in words for large enough k zeros in xk typically stay zeros in all subsequent and correspond to zeros in x note that rcdc is not able to take advantage of this indeed rcdc as presented in the theoretical sections of this paper uses the fixed probability vector p to randomly pick a single coordinate i to be updated in each iteration hence eventually i x i pi proportion of time will be spent on vacuous k updates looking at the data in table one can see that after approximately n iterations xk has the same number of non zeros as x 000 what is not visible in the table is that in fact the relation holds for this instance much sooner in fig we illustrate this phenomenon in more detail on an instance with m n 000 and x first note that the number of nonzeros solid blue line in the current iterate i i xk is first growing from zero since we start with x0 to just there are various theoretical results on the identification of active manifolds explaining numerical obser vations of this type see 7 and the references therein see also randomized block coordinate descent methods 1000 shrinking shrinking shrinking nonzeros for shrinking nonzeros for shrinking nonzeros for shrinking 800 count k f x f 400 200 2 2 x k k x fig comparison of different shrinking strategies below n in about 104 iterations this value than starts to decrease starting from about k and reaches the optimal number of nonzeros at iteration k and stays there afterwards note that the number of correct nonzeros i cn k i xk x i is increasing for this particular instance and reaches the optimal level x very quickly at around k an alternative and perhaps a more natural way to look at the same thing is via the number of incorrect zeros i z k i xk i x i indeed we have cn k i z k x note that for our problem i z k for k the above discussion suggests that an iterate dependent policy for updating of the probability vectors pk in algorithm might help to accelerate the method let us now introduce a simple q shrinking strategy for adaptively changing the probabilities as follows at iteration k where is large enough set i pk i p k q def q n q n i if xk q xk otherwise this is equivalent to choosing i k uniformly from the set 2 n with probability q and uniformly from the support set of xk with probability q clearly different variants of this can be implemented such as fixing a new probability vector for k as opposed to changing it for every k and some may be more effective and or efficient than others in a particular context in fig we illustrate the effectiveness of q shrinking on an instance of size m 500 n 000 with x 50 we apply to this problem a modified version of rcdc started from the origin x0 p richtárik m takác table 9 a list of a few popular loss functions l w xi yi name property svm loss svm c continuous max y j w t x j 2 svm loss svm c continuous max y j w t x j t log e y j w x j logistic loss lg c 2 continuous in which uniform probabilities are used in iterations and q shrinking is introduced as of iteration pk i n p k i q for for k k we have used n notice that as the number of nonzero elements of xk decreases the time savings from q shrinking grow indeed 9 shrinking introduces a saving of nearly when compared to shrinking to obtain xk satisfying f xk f 14 we have repeated this experiment with two modifications a a random point was used as the initial iterate scaled so that x0 n and b the corresponding plots are very similar to fig with the exception that the lines in the second plot start from x0 n 2 linear support vector machines consider the problem of training a linear classifier with training examples x1 y1 xm ym where xi are the feature vectors and yi the corresponding labels classes this problem is usually cast as an optimization problem of the form min f w f w ψ w w rn where m f w γ l w xi yi i l is a nonnegative convex loss function and ψ for regularized and ψ 2 for regularized linear classifier some popular loss functions are listed in table 9 for more details we refer the reader to and the references therein for a survey of recent advances in large scale linear classification see because our setup requires f to be at least c continuous we will consider the svm and lg loss functions only in the experiments below we consider the regularized setup 123 randomized block coordinate descent methods table lipschitz constants and coordinate derivatives for svm loss function i f w li m svm i 2 lg γ yj xj y j wt x j j y j w t x j m y j w t x j i e γ yj xj t e y j w x j j 1 j 1 m i yj xj i 2 yj xj j 1 2 1 a few implementation remarks the lipschitz constants and coordinate derivatives of f for the svm and lg loss functions are listed in table for an efficient implementation of ucdc we need to be able to cheaply update the partial derivatives after each step of the method if at step k coordinate i gets updated j def via wk 1 wk tei and we let rk y j w t x j for j 1 m then j j i rk 1 rk t y j x j j 1 m let oi be the number of observations feature i appears in i e oi j x i j then the update and consequently the update of the partial derivative see table requires o oi operations in particular in feature sparse problems where 1 n oi m an average iteration of ucdc will be very cheap i 1 n 2 2 small scale test we perform only preliminary results on the dataset binary this dataset has 236 features and 242 training and testing instances we train the classifier on of training instances 18 217 the rest we used for cross validation for the selection of the parameter γ in table we list cross validation accuracy cv a for various choices of γ and testing accuracy ta on instances the best constant γ is 1 for both loss functions in cross validation in fig we present dependence of ta on the number of iterations we run ucdc for we measure this number in multiples of n as you can observe ucdc finds good solution after n iterations which for this data means less then half a second let us remark that we did not include bias term or any scaling of the data 6 2 3 large scale test we have used the dataset bridge to algebra 6 which has features and 19 264 training n iterations fig dependence of tested accuracy ta on the number of full passes through the coordinates the entire training set required approximately s in the case of svm loss and s in the case of lg loss we have run ucdc for n iterations environmental pressures have caused green supply chain management gscm to emerge as an important corporate environmental strategy for manufacturing enterprises for manufacturers to fully realise the performance potentials of gscm they need to integrate internal gscm practices emphasising functional coordination with external gscm practices such as cooperation with suppliers and customers in the implementation using coordination theory this article examines three models used to evaluate the mediation relationships between the external and internal practices of gscm with respect to environmental economic and operational performance we posit that the strategic stance of manufacturing enterprises in improving their overall performance and competitive position requires a joint coordination of internal and external gscm practices survey data collected from chinese manufacturing enterprises are used to validate our arguments by testing the mediation effects of two categories of gscm practices our empirical results show support for the mediation effects which indicates the importance for manufacturers to coordinate between the internal and external aspects of implementing gscm practices to reap the performance benefits coordinating internal and external gscm practices to seek performance improvements is an important aspect of the manufacturing operations strategy the dynamics of implementing gscm practices and the performance contingencies are worthwhile topics to pursue in future research keywords environmental management supply chain management coordination theory mediation organisational performance operations strategy 1 introduction increasing institutional and technical pressures have caused enterprises to expand their focus into greening their organisations zhu and sarkis lai et al organisational greening efforts need to be balanced with organisational economic performance boiral yang et al simultaneously individual enterprises have become members of a large network of enterprises evolving from independent operations to integrated supply chain strategies vachon and mao yang et al these organisational relationships have also seen a gradual shift to improved collaboration and integration with supply chain partners frohlich and westbrook gunasekaran et al over the past couple of decades sarkis these supply chain and environmental concerns within green supply chain management gscm have evolved as an important strategy for manufacturing enterprises and their supply chains to improve their overall performance and competitive stance yet the adoption of gscm has significant barriers to overcome for example multiple complexities and uncertainties are important hurdles when enterprises seek to undertake gscm practices in their operations inter organisational and cross functional integration of environmental production engineering marketing and logistics personnel and their concerns exemplify the characteristics of effective gscm that contribute to these complexity and uncertainty considerations sarkis for a manufacturing enterprise to reap effective performance gains from gscm practice adoption coordination and integration of both internal e g management support and external gscm practices such as cooperation with suppliers and customers is required chen et al lee and klassen yang et al enterprise performance gains from gscm practices adoption include both financial and non financial dimensions geffen and rothenberg sarkis vachon seuring and mueller the variety of performance outcomes adds to the complexity of the gscm adoption corresponding author email zhuqh dlut edu cn issn print issn online ß taylor francis http dx doi org 10 1080 http www tandfonline com q zhu et al situation due to difficulties in fully realising and evaluating the multiple performance dimensions tsai and hung bai and sarkis the basic thesis of this study is that coordination of internal and external gscm is necessary to achieve multiple performance benefits including environmental economic and business performance dimensions we specifically posit that gscm coordination is based on the adoption of practices in a specific sequence to most effectively achieve the various performance benefits that is in some situations the initial adoption of internal gscm practices is necessary for the effective adoption of external gscm practices to realise performance gains this is the first study investigating the role of the sequential adoption of gscm practices on performance impacts examining the mediations between gscm practices due to sequential adoption and their performance effects can provide further insights into the better management of gscm complexities on implementation to understand the sequential adoption and coordination situations for implementing gscm we examine the various mediation effects of internal and external gscm practice adoption and various dimensions of organisational performance improvements a mediation model helps to identify whether a relation between an independent factor e g a specific internal gscm practice and a dependent factor e g organisational performance is affected by the existence of a mediating factor e g a specific external gscm practice if the mediation effect is significant it means that the mediating factor needs to be adopted for the relationship between the independent and dependent factor to be significant thus studying the sequence of adoption and coordination situations for environmental management practices such as gscm gains managerial importance our research advances gscm theory development by incorporating a coordination theoretic perspective in gscm research coordination theory argues that the coordination of organisational activities such as internal and external dimensions of management practices will lead to better performance outcomes in the supply chain lai et al practical contributions from our study include managerial insights into these mediating effects for manufacturing enterprises to plan and develop specific programs to better implement gscm and other environmental management practices we initially introduce some background and theoretical development in section 2 section 3 describes the methodology including the survey questionnaire development the data collection and factor analyses the mediation analyses and results to evaluate our hypotheses developed in section 2 are presented in section section concludes the paper with a summary of research and practical implications and identifies the potentials for future research in gscm 2 background and theoretical development in this section we provide some general background on gscm introduce a theoretical framework for our study and then derive various hypotheses within this framework that will be evaluated 2 1 green supply chain management some background gscm integrates environmental concerns into supply chain management the supply chain includes activities associated with the transformation and flow of goods or services from materials sources to the end consumers including the integration of those activities internal and external to the firm bowersox and closs similarly gscm can be viewed at multiple levels including external and internal gscm perspectives we consider environmental management practices that include transactions with suppliers and customers as external supply chain activities those activities without direct supplier or customer involvement such as eco design environmental management and financial policies within a manufacturer s direct control are considered as internal activities zhu et al these practices and categories are further described and operationalised in the methodology section many studies show that gscm practices can improve environmental performance but the linkage also depends on organisational capacity judge and elenkov the relationships between gscm and other corporate environmental practices and economic performance have been studied but the results are conflicting sarkis and cordeiro wagner et al limited work has examined the relationship between gscm and operational performance vachon and klassen this lack of a clear relationship between gscm adoption and the resulting improved performance whether it is environmental economic or operational has become a barrier for manufacturing enterprises that seek to justify gscm implementation international journal of production research previous studies have shown that the direct effects between gscm and performance improvement are significant but performance improvements are not always obvious zhu et al these studies also found that internal gscm practices such as internal environmental management and eco design have been adopted and implemented on a greater scale than external gscm practices such as cooperation with suppliers and customers this imbalance between the adoption of internal and external gscm practices may explain why improvements in operational environmental and economic performance do not always occur these complexities and uncertainties have to be more carefully investigated we now introduce a theoretical framework that may help explain why differences in performance outcomes in the complex gscm environment may occur we especially focus on coordination theory and the possible mediating interrelationships between internal and external gscm practices with respect to manufacturing enterprise performance outcomes which will help us determine how the coordination sequence should occur 2 2 theoretical framework 2 2 1 three possible models previous studies have found that environmental management is a mediator a necessary practice for a manufacturing enterprise to gain cost and service competitiveness through scm related practices such as supplier management e g yang et al concerns also arise relating to the adoption order of internal versus external gscm practices where research attention on this important implementation issue is scant for example there are situations when external practices require the adoption of internal practices for performance improvements to occur alternatively the adoption of internal practices alone may not fully contribute to performance enhancement if the adoption mediation of external practices is not present our goal in this paper is to determine whether mediation and what types of mediation exist between the adoption of internal and external gscm practices and organisational performance we investigate three possible theoretical models as shown in figure 1 studies that have investigated various aspects of the first model a the independent effects of the internal and external categories of gscm practice adoption on performance improvement have arrived at varying results zhu et al these relationship investigations have shown strongly positive weak or no relationships additional investigation of the remaining mediating frameworks will help us uncover insights into this inconsistency of performance outcomes associated with gscm implementation two possible alternative mediation models may exist to explain the implementation of gscm model b argues that external gscm practices mediate the relationship between internal gscm practices and performance model c argues that internal gscm practices mediate the relationship between external gscm practices and performance therefore we develop a series of hypotheses to determine which model most effectively characterises the relationship between implementing gscm practices and performance in the discussion below we will provide the theoretical reasoning for our belief that models b and c are possible alternatives to explain these relationships 2 2 2 coordination theory and green supply chain management the theoretical underpinnings for our study are grounded in the coordination theoretic perspective in supply chain management research has separately investigated internal and external characteristics when investigating the a b external gscm external gscm performance internal gscm performance internal gscm c external gscm performance internal gscm figure 1 the three possible models of gscm practices on performance q zhu et al supply chain and inter organisational performance wong et al in a survey of 100 randomly selected supply chain management research articles it was found that inter organisational construct investigations were well represented in the literature but studies of intra organisational construct relationships and supply chain management were the subject of very few studies burgess et al singh et al only one publication was found that investigated both inter and intra organisational constructs mcadam and brown coordination theory argues that enterprises should integrate activities along their supply chain malone and crowston coordination theory states that dependencies exist among activities and need to be managed properly the theory has been used to analyse inter organisational dependencies gosain et al coordination of product information in the supply chain legner and schemm and bundling of digitised logistics activities lai et al organisational practices such as gscm are coordinated through the networks of communications and relationships that exist among organisational actors and the strength of those networks predicts superior performance shah et al in the strategic supply chain management grew out of the recognition that increased reliance on improved relationships collaborations and information exchange with supply chain partners gunasekaran et al was needed with the aim to operate as cooperative value chains rather than as independent organisations seeking individualised pecuniary goals koufteros et al both internal and external organisational changes are required for successful supply chain management lai and cheng greater cooperation and coordination across the supply chain both intra and inter organisational through long term and strategic relationships have led to improved financial and organisational performance da silveira and arkader lai et al investigation of internal and external coordinating mechanisms collectively amongst organisational and interorganisational networks has rarely been studied external cooperation amongst organisations may not provide significant performance improvements nor be successful without proper internal cooperation it has been found that manufacturers with well developed internal and external interfaces perform better than their counterparts only with sound internal interfaces koufteros et al innovation gscm practices can be viewed as environmental organisational innovations is typically an outcome of the interaction between a firm and various outside entities yeung et al supplier and customer involvement integration and alliances are important routes to innovation generation and performance improvements in organisations koufteros et al both internal and external issues and relationships come into play in this type of innovation development yeung et al although both intra and inter organisational coordination are researched in the management literature they are typically addressed using distinct analytical frameworks organisation design extended from internal units to external organisations can strengthen inter organisational coordination networks and align them with those for intra organisational coordination mota and de castro failure to carefully coordinate between the inter and intra organisational levels can cause poor performance as well as high coordination costs e g delay re handling organisational change without considering the dependency among the coordination elements can lead to performance inferior to the expected outcomes the impact of external relationships with suppliers and customers on performance is often mediated by internal coordination wong et al this one way relationship was extended and investigated as a possible two way mediation between customer coordination investments supplier coordination investments and delivery performance da silveira and arkader such a two way mediation theoretical supposition argues that the relationship between customer coordination investments and delivery performance is mediated by supplier coordination investments and that the relationship between supplier coordination investments and delivery performance is mediated by customer coordination investments these supply chain mediation investigations focused on external investments and coordination none considered the joint internal coordination mechanisms and their relationships to external mechanisms thus the investigation of mediating effects between the intra and inter organisational coordination of external and internal gscm practices as mediating factors to each other and their effect on performance can provide insights into gscm coordination theory and supply chain management research the internal and external aspects of gscm reflect the coordination structure for the allocation of organisational resources to handle complex tasks of activity coordination to enhance efficiency and environmental performance gains within coordination theory the supply chain parties the actors or agents perform the tasks that require organisational and partner inputs resources to achieve a set of eco efficiency goals implementation of gscm and the associated internal and external practices serve as the coordination mechanism to manage the task dependency between supply chain processes extending their activities across intra and inter organisational boundaries to improve and balance economic and environmental performance international journal of production research there are three possible kinds of relationships among external gscm internal gscm and the performance of manufacturers as described in figure 1 the two groups of practices can act independently of each other each affecting performance directly a second possibility is one way mediation that is the external gscm practices affect internal gscm practices which in turn influence manufacturer performance similarly the third is that the internal gscm practices affect external gscm practices which in turn influence manufacturer performance in summary coordination theory suggests that for manufacturing enterprises to perform well across the supply chain internal and external practices need to be coordinated and aligned much of the literature in this area has focused on conceptual arguments and piecemeal case studies further empirical investigations within an emergent field such as gscm is helpful in extending coordination theory beyond technological and product innovation concerns to a new domain within supply chain management the following section provides the theoretical arguments for our hypotheses on the various relationships of internal and external practices with performance outcomes 2 3 hypotheses 2 3 1 gscm practices and environmental performance the literature has offered insights into the potential patterns of internal and external supply chain based relations for improving environmental performance geffen and rothenberg vachon seuring and mueller a positive relationship between supply chain management and environmental performance is increasingly evidenced in the literature previous studies show that external gscm practices such as supplier and customer collaboration will facilitate the adoption of internal gscm practices with the explicit purpose of improving environmental performance in the supply chain wide context vachon and klassen vachon also developing collaborative relationships with suppliers is favourable for the adoption and development of internal innovative environmental technologies geffen and rothenberg based on coordination theory from the previous section and empirical evidence presented by geffen and rothenberg we argue that a lack of internal gscm development and coordination with external practices will weaken environmental performance improvements among manufacturing enterprises similarly externally focused gscm practices e g green design of the process with suppliers for minimising wastes and customer cooperation for eco design of the product need internal coordination mechanisms e g specialised staff training on environmental management issues and cross functional cooperation to cascade the task requirements through the organisational hierarchy for these external practices to be effectively carried out thus we posit the first set of alternate hypotheses hypothesis external gscm practices mediate the relationships between internal gscm practices and environmental performance hypothesis internal gscm practices mediate the relationships between external gscm practices and environmental performance 2 3 2 gscm practice and economic performance whether gscm and corporate socially responsible practices can improve economic performance is still an open question seuring and muller some have shown that environmental management and gscm have a positive relationship with an organisation s economic performance rao and holt in general inter firm relations provide formal and informal mechanisms that promote trust reduce risk and in turn increase cooperation commitment and hence profitability lai others have suggested that economic performance is not being reaped in short term profitability and sales performance when gscm practices are implemented bowen et al the literature over the past 15 years seems to have divided views on whether there are joint gains win win s or tradeoffs that must be managed for environmental and economic performance in sustainable supply chains seuring and muller among the barriers to the implementation of environmental management practices the most critical aspect appears to be economic reasons and issues related to costs ambec and lanoie restrictions to firms behaviour may arise from the enactment of internal procedures as well as from conformity with extant environmental regulations eco control such as compliance with internal and external procedures posits considerable restrictions to the opportunistic behaviour of firms as well as increased operational costs and this may not benefit the economic performance of the participating firms henri and journeault these mixed q zhu et al results for the relationships between economic and environmental performance lead to the possibility of more complex mediations occurring between internal and external practices for example the lack of external gscm programs may actually weaken the long term viability of operational advantages for internal profitability plambeck and denand it is possible for well executed external gscm relationships to fail to bring economic performance the reason behind this can be a lack of internal managerial support and resources to take financial advantage of these relationships it remains unclear whether either or both external or internal gscm practices are mediators of this relationship we posit the existence of two alternate mediation effects between external and internal gscm practices and the improvement of organisational economic performance hypothesis external gscm practices mediate the relationships between internal gscm practices and economic performance hypothesis internal gscm practices mediate the relationships between external gscm practices and economic performance 2 3 3 gscm practice and operational performance research indicates a positive relationship between external gscm practices and operational performance through interaction with suppliers and customers manufacturers can improve their operational performance ellram et al yeung et al research has also shown that internal gscm practices such as integrated environmental management systems and staff involvement can improve operational performance hanna et al it has been argued that producing an environmentally friendly product may create a final product that is safer and less costly and which has higher more consistent quality and greater scrap value porter and van der linde sarkis the lean and green literature has also argued that the level of customers involvement in improvements to the lean performance of a supplier firm is positively related to the environmental management practices simpson et al yet coordination between internal and external practices has seen no conclusive findings using the argument that both internal and external gscm practices have influenced operational performance we use coordination theory to posit two alternate mediation effects thus similar to economic performance we put forward two alternate hypotheses hypothesis external gscm practices mediate the relationships between internal gscm practices and operational performance hypothesis internal gscm practices mediate the relationships between external gscm practices and operational performance 3 methodology 3 1 survey design external gscm practices include operations based relationships between suppliers and customers and thus we introduce measurement items for evaluating green purchasing and customer cooperation with environmental concerns reverse logistics has gained recent attention due to growing public pressure for environmental protection and declining resource availability chung and wee in line with this trend we extend reverse logistics to the supply chain wide context to include reuse recycling throughout the lifecycle of products and services for closing the loop kainuma and tawara zhu et al incorporating investment recovery as a major dimension of gscm using previous work zhu and sarkis 21 measurement items for evaluating the three external gscm practice factors are introduced see details in table 1 nine items for green purchasing factor 2 seven for customer cooperation with environmental concerns factor and five for investment recovery factor 3 represent the external gscm practices we have a total of three internal gscm practice factors two of these factors relating to internal gscm practices eco design factor 6 with four items and internal environmental management factor 1 with items were introduced in previous work zhu and sarkis eco design is an important dimension of gscm since this practice can be helpful in reducing energy material consumption and costs throughout the supply chain at the earliest inception of the product or process design chung and wee gabbar zhu and liu an additional factor pertaining to internal financial and incentive policies linking environmental performance that comprises four new measurement items factor was also developed for this study and is based on recent work international journal of production research table 1 rotated factor matrixa on the measurement items for evaluating gscm practices factor survey item providing design specification to suppliers that include environmental requirements for purchased items cooperation with suppliers for environmental objectives environmental audit for suppliers internal management suppliers certification second tier supplier environmentally friendly practice evaluation adopting just in time logistics system for supplier cooperation suppliers are selected using environmental criteria cooperating with suppliers to reduce packaging require suppliers to use environmental packaging degradable and non hazardous cooperation with customers for eco design cooperation with customers for cleaner production cooperation with customers for green packaging cooperation with customers for using less energy during product transportation adopting third party logistics for customer cooperation cooperation with customers for product take back cooperation with customers for reverse logistics relationships investment recovery sale of excess inventories materials sale of scrap and used materials sale of excess capital equipment collecting and recycling end of life products and materials establishing a recycling system for used and defective products design of products for reduced consumption of material energy design of products for reuse recycle recovery of material component parts design of products to avoid or reduce use of hazardous products and or their manufacturing process design of processes for minimisation of waste commitment of gscm from senior managers support for gscm from mid level managers cross functional cooperation for environmental improvements special training for workers on environmental issues total quality environmental management environmental compliance and auditing programs iso certification environmental management systems exist eco labelling of products existence of pollution prevention programs the internal performance evaluation system incorporates environmental factors generate environmental reports for internal evaluation compensation is linked to performance on environmental factors for senior executives compensation is linked to performance on environmental factors for managers compensation is linked to performance on environmental factors for employees accounting practices incorporate social costs extraction method principal component analysis rotation method varimax with kaiser normalisation a rotation converged in eight iterations with compensation incentive and reward systems for organisational environmental initiatives russo and harrison thus we have a total of items for evaluating internal gscm practices the measurement items organised in a survey questionnaire for measuring both external and internal gscm practices were designed for response using a five point scale i e 1 ¼ not considering it 2 ¼ planning to consider it q zhu et al table 2 rotated factor matrixa on the measurement items for evaluating gscm performance factor survey item reduction of air emission reduction of waste water reduction of solid wastes decrease of consumption of hazardous harmful toxic materials decrease of frequency of environmental accidents improvement of an enterprise s environmental situation decrease of cost for materials purchasing decrease of cost for energy consumption decrease of fee for waste treatment decrease of fee for waste discharge decrease of fine for environmental accidents increase in amount of goods delivered on time decrease of inventory levels decrease of scrap rate promotion of products quality increased in product line improved in capacity utilisation note extraction method principal component analysis rotation method varimax with kaiser normalisation a rotation converged in four iterations which means in the early phases of discussion and consideration and may not be considered for final implementation 3 ¼ considering it currently which means that the gscm practice has been planned for and is on the way to being implemented but not yet carried out ¼ initiating implementation and ¼ implementing successfully seventeen measurement items for evaluating gscm performance were adopted from a previous study zhu et al including environmental economic and operational performance see details in table 2 these measurement items were operationalised in the survey questionnaire to evaluate performance improvement as a result of implementing gscm practices using a five point scale 1 ¼ not at all 2 ¼ some but insignificant 3 ¼ some and slightly significant ¼ significant and ¼ highly significant 3 2 data collection given situations of resource consumption waste production and implementation of environmental management practices our survey focused on four traditional polluting industrial sectors chemical petrochemical electronic automobile and mechanical industrial sectors according to the distribution of such industries in china we chose major cities and industrial zones as our targeted research areas in we delivered and acquired survey data mainly in suzhou of jiangsu province in southeast china dalian of liaoning province in northeast china and tianjin in the mid east of china following the rationale of podsakoff et al we considered that common method bias might arise to avoid item characteristic effects due to ambiguous items that might lead to unreliable answers we carried out a pretest for the measurement items in the questionnaire survey by interviewing two enterprises each from the four industrial sectors the pretest aimed to check if the measurement items could be fully understood and if more measurement items should be included each interview lasted about half a working day we made minor modifications mainly on how to better present the items in the questionnaire according to the suggestions of the interviewees to avoid common rater effects due to the respondent s perceived need to provide consistent or socially desirable answers we allowed options that respondents could choose to give personal information or remain anonymous for the identity of both the respondent and the company subsequent to the pretest we administered the survey to the sample enterprises in the four industries international journal of production research out of a total of questionnaires sent by postal mail to all the enterprises and representatives we received a total of unique and usable manufacturing enterprise responses the key informants targeted by this study and responding to the survey had middle or senior management experience surveying the target respondents at this organisational level is supported by other research such as lai et al who argued that mid level managers such as those involved in managing purchasing and supply relationships will have the needed knowledge to respond on supply chain management issues such as external and internal gscm practices which is consistent with our findings from extensive corporate interviews beyond the survey questionnaire similarly bowen et al found that the further the middle managers perceptions of the corporate attitude to environmental issues is in advance of legislation regulation and other firms in the industry the more likely the unit is to implement a gscm practice of the responses there were 3 from the chemical petrochemical industry 17 2 from the electronic industry from the automobile industry 2 18 2 from the mechanical industry and 1 from other industries we targeted manufacturers with different ownership types and with different numbers of employees the respondents include state owned manufacturers 41 9 private chinese manufacturers 8 and foreign manufacturers or joint ventures 3 including 1 small manufacturers with less than employees 34 6 medium sized manufacturers with between 300 and employees and 31 3 large manufacturers with over employees 3 3 factor analysis as we developed additional items to measure the different aspects of implementing gscm practices we conducted an exploratory factor analysis to confirm the groupings of gscm practices and performance from our survey data factors were extracted using the maximum likelihood method followed by a varimax rotation the kaiser criterion eigenvalue 1 was employed in conjunction with an evaluation of the scree plots both the scree test and the initial eigenvalue test suggested six and three meaningful factors for gscm practices and performance respectively these factor analysis results suggested grouping the gscm practice items into six factors as anticipated see table 1 the six gscm practice factors explain 9 of the inherent variation a similar factor analysis of the gscm performance items also resulted in three factors explaining 9 of the inherent variation we identify three external gscm practice factors green purchasing gp customer cooperation on environmental concerns cc and investment recovery ir furthermore we identify three internal gscm practice factors ecodesign eco internal environmental management iem and internal financial policies ifp further analysis using reliability tests confirmed the reliability of these six factors with cronbach s alphas of 91 89 85 90 95 and 94 respectively the three factors on performance were labelled environmental performance economic performance and operational performance the same analysis was applied and the results also confirmed the reliability of these three performance related factors with values of cronbach s alpha amounting to 88 89 and 90 respectively for each factor all cronbach alpha values are well above the threshold value of 70 for item reliability as suggested by nunnally to establish internal consistency and validity of latent constructs results and discussion we applied hierarchical multiple regression analysis to examine the proposed mediating relationships and directions we initially evaluate the potential mediating effects for each of the internal and external gscm practice factors as suggested by baron and kenny testing of the mediating effect involves three steps first the independent variables must relate to affect the mediating variables as shown in table 3 all regression coefficients using internal gscm practice factors as mediators and external gscm practice factors as independent factors are significant p thus it is reasonable to consider internal gscm practices as mediators similarly the results in table provide support to consider external gscm practices as mediators in the second step the independent variable must affect the dependent variable table 3 shows that the regression results between external gscm practice factors as independent factors and performance improvements as dependent factors are significant p similarly table 4 shows that the regression results between internal gscm practice factors as independent factors and performance improvements as dependent factors are significant p thus these results provide evidence to support either internal or external gscm practice factors as mediators in this step q zhu et al table 3 regressions with internal gscm practices as mediators mediator independent factor dependent factor performance eco design eco internal environmental management iem internal financial policies ifp environmental economic operational 680 351 267 612 435 245 494 461 216 293 316 146 194 286 196 280 342 217 green purchasing gp customer cooperation cc investment recovery ir note the regressions are performed separately between one mediator dependent factor and one independent factor p p table 4 regressions with external gscm practices as mediators mediator independent factor eco iem ifp dependent factor performance gp cc ir environmental economic operational 680 612 494 351 435 461 267 245 216 328 388 426 111 187 238 0 284 0 300 0 316 note the regressions are performed separately between one mediator dependent factor and one independent factor p 0 p 0 p 0 the third step requires that the mediator must affect the dependent variable perfect or complete mediation exists when the independent variable has no effect on the dependent variable when the mediating variable is held constant baron and kenny a partial mediation effect exists when the independent variable has a reduced but still significant effect on the dependent variable when the mediating variable is introduced held constant results of mediation effects for environmental economic and operational performance are shown in tables 6 and 7 respectively an example of this methodological step is provided in the investigation of model 1 2 for gp and eco in table in this investigation we first introduce gp the external gscm factor into the regression the results show a significant of 0 085 the next step is to introduce eco an internal gscm practice factor into the hierarchical regression we now see that is significant at a value of 0 119 the change in the value is significant 0 034 similarly we introduce eco first and then add gp as the second step the change in is not significant at a value of 0 008 thus from this result we can argue that the relationship between an external gscm practice factor gp and performance outcome environmental is mediated by an internal gscm practice factor eco this result is shown as supporting a mediating effect 2 in the last column of table for example if an external gscm practice mediates the relationship between an internal gscm factor and a performance factor then we have support for a mediating effect 1 penultimate column in table for example 4 1 gscm mediation effects on environmental performance no model in table shows mediating effects of external gscm practices on internal gscm practices and environmental performance thus hypothesis receives no support six of nine models show complete mediating effects of internal gscm practices on external gscm practices and environmental performance thus hypothesis is supported table shows that three internal gscm practice factors explain the additional variance of environmental performance p 0 when green purchasing is controlled the percentage of variance in environmental performance explained by green purchasing becomes very small and non significant when three internal gscm practice factors are controlled thus support is found that the relationship between green purchasing and international journal of production research table hierarchical multiple regression results for environmental performance complements to enhance the environmental performance of green purchasing practices such as cooperating with suppliers to reduce packaging table shows that three internal gscm practice factors explain the additional variance of environmental performance p 0 when customer cooperation with environmental concerns is controlled at the same time the percentage of variance in environmental performance explained by customer cooperation with environmental concerns remains significant when three internal gscm practice factors are controlled one exception is that the additional variance explained by customer cooperation is less significant p 0 when internal financial policies are controlled thus only a partial mediating effect of internal financial policies on the relationship between customer cooperation and environmental performance is supported such results indicate that customer cooperation can affect environmental performance regardless of which internal gscm practices are implemented or not implemented for compensation systems and environmental accounting practices to improve environmental performance customer cooperation would be a beneficial program to include table shows that three internal gscm practice factors explain the additional variance of environmental performance p 0 when investment recovery is controlled the percentage of variance in environmental performance explained by investment recovery becomes very small and non significant when three internal gscm factors are controlled thus we found empirical support that the relationship between investment recovery and environmental performance is completely mediated by all three internal gscm practice factors hypothesis receives further support compared with developed countries such as the usa and germany investment recovery has received relatively less attention in developing countries such as china zhu and sarkis without internal gscm practices it is less likely that investment recovery practices will improve environmental performance for chinese manufacturers one of the eco design practices is to design products for reuse recycling and recovery of materials and component parts such designs can provide incentives for manufacturers to embark on investment recovery internal environmental management particularly special training for workers on environmental issues is helpful for manufacturers to promote investment recovery practices internal financial policies can provide motivation and support for employees to implement investment recovery thus these examples of the different practices show that their inclusion will benefit the application of investment recovery and environmental performance 4 2 gscm mediation effects on economic performance table 6 shows that hypothesis is supported by several mediation effects green purchasing explains the additional variance of economic performance p 0 when eco design is controlled the percentage of variance international journal of production research in economic performance explained by eco design decreases and becomes non significant when green purchasing is controlled thus we find support that the relationship between eco design and economic performance is completely mediated by green purchasing similar results can be found for the other three models namely models 7 9 and 13 customer cooperation on environmental issues fully or partially mediates the relationships between internal environmental management internal financial policies and eco design each separately with economic performance the implication here is clear that these internal environmental management practices will enhance the economic position of manufacturers with strong customer cooperation and support for environmental management efforts strong coordination between the downstream supply chain and internal activities is an important way for manufacturers to improve economic performance upstream coordination relationships with green purchasing and internal practices have a slightly more complex relationship when seeking economic gains for manufacturing enterprises hypothesis is also supported by one complete and two partial mediation effects the internal financial policies explain the additional variance of economic performance p 0 01 when green purchasing is controlled the percentage of variance of economic performance explained by green purchasing becomes very small and non significant when the factor of internal financial policies is controlled there is support for complete mediation in the relationship between green purchasing and economic performance by internal financial policies internal environmental management and internal financial policies are both partial mediators for the relationship between investment recovery and economic performance thus having these practices in place will promote investment recovery and hence economic performance incorporating organisational activities such as environmental management systems and improving compensation reward systems for workers on environmental performance seem to enhance the economic gains from investment recovery initiatives overall the relationship between eco design and economic performance is mediated by all three external gscm practice factors there is clear evidence that coordination in design activities and external organisational relationships is needed it is also critical that no matter where in the product lifecycle the product lies most of the impact is locked into the product at the design stage when materials are selected and product performance is largely determined lenvis and gretsakis thus strong evidence exists for including customers and suppliers as partners in environmental design to bring economically beneficial results for manufacturers one of the difficulties with this integration is that there may be a long time gap in economic performance from eco design and such benefits can be achieved more readily with related external cooperation with suppliers and customers 4 3 gscm mediation effects on operational performance hypothesis is supported by one complete mediation effect model 1 and one partial mediation effect model 11 green purchasing completely mediates the relationship between eco design and operational performance this result suggests an interesting coordination effect for managing their dependencies it seems that for eco design to result in better operational performance with measures such as lowered inventory levels greater product flexibility increased product lines and improved quality green purchasing practices are required these upstream environmental collaborations seem to greatly influence the way eco design contributes to operational performance we also find support for the relationship between internal financial policies and operational performance being partly mediated by customer cooperation hypothesis has support from three models with partial mediation effects internal environmental management explains the additional variance of economic performance p 0 01 when green purchasing is controlled the percentage of variance of operational performance explained by green purchasing becomes less significant p 5 0 when internal environmental management is controlled the results show support that the relationship between green purchasing and operational performance is partly mediated by internal environmental management internal financial policies also partly mediate relationships between green purchasing and operational performance internal financial policies can be effective practices to motivate employees to find better options in cooperating with suppliers and thus improve operational performance similarly internal environmental management partly mediates the relationships between investment recovery and operational performance with internal environmental management such as certification and management support investment recovery can be better implemented and operational performance can be improved through results such as decreased scrap rates q zhu et al 5 conclusions in this study we posit that the introduction of gscm practices may require an appropriate sequential implementation for a manufacturer to accrue the various performance benefits to address this specific concern we evaluate the mediating influences of internal and external gscm practices on each other and their subsequent effects on economic environmental and operational performance measures we find a number of significant results that support the existence of mediations between gscm practices on performance outcomes these results provide us with managerial implications and directions for future research which we present below 5 1 summary of findings there are several research implications with respect to the mediation effects of internal and external gscm practices for environmental economic and operational performance improvement our empirical evidence showing the existence of mediation effects indicates the need for manufacturers to coordinate between internal and external gscm practices to realise their performance potential to the fullest internal gscm practices mediate the relationships between external gscm practices and environmental performance three internal gscm practices completely mediate the relationships of the two external gscm practices i e green purchasing and investment recovery with environmental performance only the internal financial policy partly mediates the relationship between customer cooperation and environmental performance from the coordination theoretic viewpoint the role of internal gscm practices in improving environmental performance through external gscm practices should not be neglected we found several complete and partial mediating effects of various internal and external gscm practices on economic performance improvements green purchasing completely mediates the relationship between eco design and economic performance internal financial policy completely mediates the relationship between green purchasing and economic performance customer cooperation mediates relationships between all three internal gscm practices and economic performance investment recovery completely mediates the relationship between eco design and economic performance thus manufacturing enterprises should pay attention to the complexity of coordinating internal and external gscm practices if they seek economic gain from practice implementation only a few mediating effects were found for the operational performance outcome factor green purchasing completely mediates the relationship between eco design and operational performance customer cooperation partly mediates the relationship between internal financial policy and operational performance for eco design to benefit operational performance green purchasing initiatives should not be neglected environmentally friendly inputs through green purchasing are required before eco design of products can deliver operational benefits examining the role of green purchasing as a critical organisational complement eco design for performance benefits is a fertile line of gscm research not investigated in the literature according to our findings further investigation into customer participation in environmental management initiatives of manufactures such as product return and recycling policy and incentive schemes to achieve financial objectives are promising areas for further gscm research 5 2 managerial implications uncertain and incomplete information are two key issues for manufacturing enterprises when implementing gscm practices results of the mediation effects show that appropriate sequential implementation of internal and external gscm practices may allow manufacturers to accrue environmental economic and operational performance benefits to improve environmental performance by external gscm practices such as green purchasing and investment recovery it is desirable for enterprises to implement all three internal gscm practices when eco design of products is present within the firm the lack of a mechanism to coordinate design requirements with green purchasing can compromise environmental performance outcomes customer cooperation with environmental concerns is directly associated with environmental performance however without the existence of internal financial policies customer cooperation in environmental efforts will not improve organisational environmental performance to improve economic performance through eco design efforts enterprises have to implement green purchasing and strong internal financial incentive policies if chinese manufacturers decide to implement eco design practices without green purchasing and internal financial incentive policies in place the result will be a compromise on international journal of production research economic performance building collaborative relationships with customers is a necessary ingredient for each of the three internal gscm practices to result in improved economic performance investment recovery is desirable for eco design to achieve economic performance moreover the potential capital gains from investment recovery necessitate an internal environmental management and an internal financial policy that is conducive to improving economic performance eco design has the potential to improve operational performance only if green purchasing is in place an internal financial policy may improve operational performance and such an improvement can be more significant if the internal financial policy is implemented with customer cooperation 5 3 limitations and future research there are several other potential research directions some related to limitations on the interpretation of the results of our study first the dynamic evolution of gscm is an important issue for implementing these practices due to changing and increasing institutional pressure from various stakeholders such as government and supply chain partners manufacturing enterprises can better implement their gscm practices with adaptive evolution to the changing market dynamics and requirements a longitudinal study is useful to complement the current study with a focus on examining the contingent situations of gscm pressures practices and performance as well as their structural relationships in the temporal dimension second will other nations that are more or less developed have similar outcomes what other barriers or motivations might exist in terms of resources or capabilities thus motivational reasons for implementation of gscm should be studied considering stakeholders such as governments and the public to understand these mediations another research direction is to identify ways that can help manufacturers to improve performance through gscm implementation this study provides some implications on the mediating effects among the components of gscm practices conducive to performance gains however a broader path analysis or structural equation model between gscm practices may provide additional insights into the complex nature of these various relationships further questions as to the influence of specific performance impacts of gscm implementation on individual organisational functions and the most appropriate performance metrics for evaluating the internal and external aspects of implementing gscm still exist bai and sarkis this study has laid a solid foundation for identifying some of these complexities but as with most sustainability programs additional complexities probably exist acknowledgements we are grateful for the helpful comments of two anonymous reviewers on an earlier version of this paper this work is supported by a grant from the national science fund for distinguished young scholars the national natural science foundation of china project projects and the program for new century excellent talents in university in china ncet 0082 and nsfc jsps lai is fully supported by a grant from the research grants council of the hong kong special administrative region china grf polyu note 1 in our study internal gscm practices include a general grouping of activities such as eco design management practices and internal financial policies in support of environmental programs external gscm practices include green purchasing customer cooperation and investment recovery increase in environmental concerns together with legislations are forcing industries to take a fresh look at the impact of their supply chain operations on the environment this paper introduces a mixed integer linear programming based framework for sustainable supply chain design that considers life cycle assessment lca principles in addition to the traditional material balance constraints at each node in the supply chain indeed the framework distinguishes between solid and liquid wastes as well as gaseous emissions due to various production processes and transportation systems the framework is used to evaluate the tradeoffs between economic and environmental objectives under various cost and operating strategies in the aluminum industry the results suggest that current legislation and emission trading schemes ets must be strengthened and harmonized at the global level in order to drive a meaningful environmental strategy moreover the model demonstrates that efﬁcient carbon management strategies will help decision makers to achieve sustainability objectives in a cost effective manner elsevier b v all rights reserved keywords sustainable supply chain design environment greenhouse gases emissions life cycle assessment reverse logistics emission trading scheme carbon management introduction supply chain network design attempts to deﬁne the best supply chain conﬁguration that enables an organization to maximize its long term economic performance typically the decisions cover two planning levels strategic decisions on sourcing production opening or closing of facilities distribution and sales tactical decisions on supply network planning affecting the ﬂow of goods trough the network flexibility robustness and responsiveness are some of the strategies that have been used to adapt to dynamic changes in the supply chain environment sabri and beamon but unfortunately the pursuit of short term proﬁtability is still recognized as the one of the major drivers for managerial decisions and this among other things has contributed to the slowdown in the current global economy nowadays given the constraints relative to the availability of non renewable resources metal oil etc enterprises are more than ever obliged to rethink their strategies to ensure the sustainability of their operations closed loop supply chains are one of the options that are being considered lieckens and vandaele barker and zabinsky srivastava pochampally et al other avenues being studied include different actions related to one or more phases of the product life cycle such as product design hugo and pistikopoulos production planning and control for remanufacturing jayaraman et al luo et al inventory management ferretti et al product recovery jayaraman n correspondence to notre dame street ouest montreal quebec canada e mail address amin chaabane etsmtl ca a chaabane see front matter elsevier b v all rights reserved doi j ijpe reverse logistics sheu et al sheu and carbon emissions reduction ramudhin et al however these actions may not be enough to guarantee longterm sustainability indeed recovery of used products and re processing remanufacturing recycling disposal incineration etc might not only increase operating costs but also contribute to an increase in greenhouse gases ghg emissions which defeats long term stainability sustainable development recognizes the interdependence between three dimensions the economic the environmental and the social performances of an organization an integrated approach that links supply chain decisions to the three pillars of sustainability is advocated sustainable supply chain design frota neto et al is a new emerging approach that arose in response to this situation and tries to embed economic environmental as well as societal decisions in supply chains at design time the objective of the methodology proposed in this paper is to present a formal decision model that considers the important dimensions of sustainability throughout the supply chain life cycle literature review traditionally the main objective of optimization models used in strategic network design focused on the economic aspect of supply chains goetschalcks and fleischmann however more recently there has been a growing awareness about environmental issues the ﬁrst proposals tried to integrate such considerations at the plant level the main drawback of these approaches is that it may result in solutions that reduce the negative environmental a chaabane et al int j production economics impact somewhere in the supply chain at the expense of increasing it elsewhere life cycle assessment lca methodology has been proposed in response to this situation de benedetto and klemes lca is a process for evaluating the environmental impacts associated with a product process or activity it identiﬁes and quantiﬁes the energy and materials used and the waste released to the environment and evaluates and implements opportunities for environmental improvements the assessment covers the entire life cycle of the product process or activity including extracting and processing raw materials manufacturing transportation and distribution reuse and maintenance recycling and ﬁnal disposal hugo and pistikopoulos present a mathematical programming based methodology with explicit inclusion of life cycle assessment lca criteria as part of the strategic investment decisions related to the design and planning of supply chain networks nagurney et al develop a supply chain model in which the manufacturers can produce homogeneous product in different manufacturing plants with distinct environmental emissions frota neto et al develop a framework for the design and evaluation of sustainable logistic networks where activities affecting the environment and cost efﬁciency in logistic networks are considered guillen gosalbez and grossmann present a supply chain network design model to determine the supply chain conﬁguration along with the planning decisions that maximizes the net present value and minimizes environmental impact the model includes structural and planning decisions suitable performance measures in evaluating the supply chain are important and directly affect their applicability gunasekaran et al various types of performance measures have been used to evaluate sustainable supply chain most frequently they combine the economic performance with the environmental performance in order to ﬁnd the trade off between the two performances nagurney and toyasaki pistikopoulos and hugo sheu et al lu et al frota neto et al guillen gosalbez and grossmann the economic dimension represents the cost or the proﬁt in net present value pistikopoulos and hugo various performance metrics have been developed to evaluate quantitatively the environmental impact of products processes and activities such as the emissions of ghg cfc nox y luo et al waste generation liquid or solid energy use and material recovery in recent years different comprehensive environmental performance metrics has been proposed such as the eco indicator brentrup et al eco indicator contreras et al ecological footprints and ecopro luo et al these metrics are based on different methodological structures and weighting techniques where assumptions are different the applicability of different supply chain models have been tested in real industrial cases and in different ﬁelds petrochemical production zhou et al aluminum industry ferretti et al personal computer min and melachrinoudis dotoli et al and the pulp and paper industry frota neto et al it shows particularly that numerous initiatives have provided incentives for organizations to become more sustainable some of these regulations are mandatory but increasingly others are just voluntary environmental programs and considered as new alternatives for gaining or maintaining a competitive advantage for instance many industries are engaged in voluntary rl activities like the automotive industry schultmann et al cellular telephones jayaraman et al computers white et al pulp and paper industry frota neto et al because they can achieve additional proﬁt while the lca principle has been successfully applied to design new products and processes that reduce environmental damage global warming ozone depletion acidiﬁcation toxicity etc limited work has been conducted on the development of decision making models that integrate both lca principles and supply chain management principles seuring and muller in addition few studies have addressed the impact of integrating external control mechanisms on sustainable supply chain management practices such as environmental regulations take back legislation ghg emissions and carbon taxes emission trading stranlund and carbon markets johnson and heinen peace and juliani for instance nagurney et al is one of the ﬁrst studies that addresses carbon taxes in the electric power supply chains nagurney et al subramanian et al propose an approach to integrate environmental consideration within a managerial decision making framework a non linear mathematical programming model is introduced that allows the incorporation of traditional operations planning considerations capacity production and inventory with environmental considerations design production and end of life decisions on the number of carbon credits purchased and sold in different periods are added under the limitation of carbon emissions ramudhin et al are the ﬁrst to propose a carbon market sensitive strategic planning model for sustainable supply chain network design they show that considerations of internal and external control mechanisms are of great importance to decision makers when designing sustainable supply chains this paper extends the model presented in ramudhin et al by consideration of the lca methodology to establish successful sustainable supply chains over time the capability of the model is illustrated by an example of strategic planning in the aluminum supply chain problem statement and methodology among the different approaches available to assess the environmental impact of processes and organizations the lca method seems to be the most promising it aggregates the results of different aspects of environmental studies including ghg emissions that are recognized as the most harmful elements to the environment and responsible for climate change ghg emissions are calculated based on emission factors and converted to carbon dioxide equivalent quantity many countries are implementing various mechanisms to reduce ghg emissions including incentives or mandatory targets to reduce carbon footprint carbon taxes and carbon markets emissions trading are recognized as the most cost effective mechanisms labatt and white the basic idea is to put a price tag on carbon emissions and create new investment opportunities to generate a fund for green technology development bayon et al labatt and white there are already a number of active carbon markets for ghg emissions such as the european union emission trading scheme or eu ets in europe the largest multi national ghg emissions trading scheme in the world the new zealand emissions trading scheme nz ets in new zealand the chicago climate exchange in united state johnson and heinen peace and juliani and more recently the montreal climate exchange in canada measuring and assessing carbon emissions becomes then an important step that can be achieved by lca techniques and software rice et al however compliance with the environmental regulation of carbon emissions in a cost effective manner is challenging thus supply chain network design model had been revised to include the additional cost due to ghg emissions at all levels of the supply chain and social variables affecting the quality of life of the community in which the supply chain operates as shown in fig an lca based approach is necessary in order to establish the link between the critical inputs raw material energy human used product etc and the output products ghg emissions waste at each node of the network over its entire life cycle strategic planning of sustainable supply chains should a chaabane et al int j production economics strategic planning of sustainable supply chains lca level inputs ouputs capital sustainable product network design raw materials sustainable purchasing product energy sustainable manufacturing emissions human ressources sustainable storage solid waste sustainable transportation hazardous waste re processed products reverse logistics closed loop supply chain strategic planning supply chain configuration decisions facilities location capacities resources new product design product allocation product selection post use planning decisions product flows suppliers production distribution inventory planning transportation configuration path recovery re processing investments carbon management technology selection technology acquisition facility design transportation modes buy sell carbon credits economic variables cost revenue taxes transfer environmental variables carbon footprint raw material use energy use social variables noise pollution objective economic cost profit net present value customer service quality flexibility responsiveness risk ecologic environmental eco indicator social quality of life fig an lca approach to support sustainable supply chain design include the recovery of products decisions as well as carbon management strategies in order to be in compliance with the different environmental regulations thus the supply chain performance should be evaluated based on the economic cost and proﬁt the environmental carbon emissions recycling performance waste management and energy use and the social performances quality of life noise etc model development this section describes a generic mathematical model to help decision makers in the design and planning of sustainable supply chain based on the lca methodology the model establishes the link with the emission trading scheme to achieve sustainability objectives in a cost effective manner under the different legislations that caps ghg emissions and impose mandatory targets for recycling products at the end of their life although supply chain sustainability recognizes the link between the economic ecological and social performance an examination of social performances labour equity healthcare safety philanthropic commitment shows that they are dependent on the context of operation of the supply chain type of the industry the government policies and cultural norms thus without loss of generality we do not include the social performance in the mathematical formulation assumptions fig shows the structure of the global supply chain the sites are located in different zones zaz a set of potential suppliers nas can supply raw materials papmp to a set of sub contractors and plants naf to manufacture products pappf the latter can be distributed through a set of potential distribution centers nad final products are shipped from the distribution centers to different customers or markets nac also available are different recycling centers nar for the processing of used products that can be returned to different stages in the supply chain let n denotes the set of the different nodes of the supply chain network f d c r at each production center a set of potential technologies hah is available for use each of these technologies needs some inputs energy liquid solid gazes etc iai in addition to materials and generate different outputs liquid solid gazes oao different transportation modes mam are used for the shipment of products between nodes suppliers production units distribution centers and recycling units each transportation mode needs some inputs e g energy and gazes and may generate some wastes output the main objective of the model is to support sustainable supply chain network design over a long term period of time tat before the description of the detailed model some basic elements about modeling techniques are explained generally we deﬁne two types of nodes production unit and recycling center for a production unit one or several technologies will be available for manufacturing activities the production is based on a bill of material that indicates the quantity of raw material or components required to manufacture components or ﬁnal products the potential technologies available differ in terms of acquisition and operation costs as well as inputs consumption and output emissions fig summarize the situation for a recycling center fig we consider that a certain percentage of products are recovered by the company to simplify the calculation we consider that is proportional to the demand at each period thus we assume that for each period there is a quantity of used products collected and delivered to recycling centers using a bill of material of disassembly the product is disassembled good a chaabane et al int j production economics production units distribution centers suppliers customers recycling centers fig closed loop supply chain network structure b decisions related to production units outputs zone z production unit n qnpntz i pntz technology i fop nt p q p hntz fp nn mt technology h technology h inputs fig characteristics of a production unit zone z qrpntz fp n nmt fig characteristics of a recycling center recycled components and raw materials are reintegrated in production units the non useable products components and materials are destroyed thorough the disposal process decision variables to achieve the objective of sustainability and understand the impact of different control parameters on the decision process several decisions touching various aspects of the supply chain must be taken into account they are a decisions related to sites plants and recycling centers location l ynz quantity of product papmp ppf necessary in production unit naf during time period tat p qphntz quantity of product pappf manufactured assembled using technology hah at production unit naf during time period tat at zone zaz fi ipnt the inventory level of input product papmp ppf at unit production naf during time period tat fo ipnt the inventory level of output product pappf at unit production naf during time period tat ypnt binary variable takes a value of if supplier nas is selected for supplying raw material papmp during time period tat otherwise f ynt binary variable takes a value of if production unit naf is operational during time period tat otherwise h yhnt binary variable takes a value of if technology hah is selected at production unit naf during time period tat otherwise p yphnt binary variable takes a value of if product pappf is manufactured assembled using technology hah at production unit naf during time period tat otherwise c decisions related to distribution centers n qpnt technology fi binary variable takes a value of if site naf d r is located at zone zaz otherwise d ynt binary variable takes a value of if distribution center nad is operational during time period tat otherwise d ipnt the inventory level of product pappf at distribution center nad during time period tat d decisions related to recycling centers r ynt a qpntz r qpntz d qpntz ri ipnt binary variable takes a value of if recycling center nar is operational during time period tat otherwise quantity of product pappf refurbished at recycling center nar during time period tat at zone zaz quantity of raw material papmp recycled at recycling center nar during time period tat at zone zaz quantity of product papmp ppf disposed at recycling center nar during time period tat at zone zaz the inventory level of input of product pappf at recycling center nar during time period tat a chaabane et al int j production economics ro ipnt the inventory level of output of recycled material papmp ppf at recycling center nar during time period tat e decisions related to transportation quantity of product papmp ppf processed from node nan to node an using transportation mode mam during time period tat t ymnn binary variable takes a value of if transportation mode mam is selected between node nan and node an during time period tat otherwise f decisions related to carbon management mt variable cost for raw materials acquisition x x x x x af cpnt fpnnumt p a p mp n a snu a f m a mt a t recycled materials acquisition x xx x x ar cpnt fpnnumt p a p mp n a rnu a f m a m t a t production costs denoted pc which are the costs to manufacture products fixed cost for operating production units xx f f ant ynt n a ft a t cctzþ cctz credits purchased during time period tat at zone zaz credits sold during time period tat at zone zaz fixed cost for technology acquisition x xx h ahhnt yhnt h a hn a f t a t decisions on a b c d and e are related to the supply chain network conﬁguration and aggregate planning while decisions on f determine the strategy to put in place for ghg emissions control the problem can be viewed as a multi objective model where the economic objective function denoted by evaluates the total logistic cost and the environmental objective function denoted by evaluates carbon emissions resulting from operation strategies manufacturing and transportation activities using the intergovernmental panel for climate change ipcc guidelines and other environmental data the global warming potential gwp for each activity of the supply chain is expressed in terms of carbon dioxide equivalent quantity emissions are assumed to be linearly proportional to manufacturing transportation and usage fixed cost for production line conﬁguration x x xx p p aphnt yphnt p a p pf h a hn a f t a t variable cost for manufacturing x x x xx p p cphntz qphntz p a p pf h a hn a f t a t z a z inventory cost of materials x x x fi fi cpnt ipnt p a p mp n a f t a t inventory cost of products x x x fo fo cpnt ipnt p a p pf n a f t a t distribution costs denoted dc which are the costs to distribute model formulation this section describes the linear programming model that considers the critical aspects for the design and strategic planning of sustainable supply chains the choice of multi objective linear programming molp as a methodology to investigate this problem is basically because it helps to ﬁnd the different strategic decisions explained in the previous framework see fig of linear objective functions and a single decision maker or a decision making body the focal company that guarantee a trade off with respect to some linear constrains economic objective the strategic sustainable supply chain network design described before has the objective to ﬁnd a trade off solution between the economic and the environmental performance under the different regulations that caps ghg emissions and impose constraints related producer responsibility at the end of the production life cycle the economic objective is evaluated by the total logistic cost the environmental performance is evaluated by the total emissions of ghg the economic dimension includes different costs location cost denoted zc which are the costs to locate production distribution and recycling centers at the different regions x x l alnz ynz n a f d rz a z supply costs denoted sc which are the costs to acquire materials fixed cost to establish contracts with suppliers x xx aspnt ypnt p a pmp n a st a t products fixed cost for operating distribution centers xx d adnt ynt n a dt a t variable cost for material handling products x xx d d cpnt ipnt p a p pf n a dt a t reverse logistics costs denoted rc which are the costs to recycle and dispose products fixed cost for operating recycling centers xx r arnt ynt n a rt a t cost of recovery of used products x xx xx rc cpnt fpnunmt p a p pr n a rnu a c m a mt a t variable cost of recycling used products x x xx r r cpntz qpntz p a p pf n a rt a t z a z variable cost for disposal of used products x x xx dr d cpntz qpntz p a p pf n a rt a t z a z inventory cost for recovered used products x xx ri ri cpnt ipnt p a p pf n a rt a t inventory cost for recycled products x xx ro ro cpnt ipnt p a p mp p pf n a rt a t a chaabane et al int j production economics transportation cost denoted tc which are the costs to move decision on the most cost effective strategy to be in compliance either with environmental regulation or with voluntary targets thus the decision is to determine the number of credits purchased acc tz in period tat and the number of credits sold vtzcc in period tat xx xx cc cctz atz cctzþ vtzcc cc products fixed cost for transportation links between nodes xx xx t atnnumt ynnumt n a nnu a n m a m t a t variable cost for transportation x xx xx t cpnnumt fpnnumt t a tz a z p a pmp p pf n a n nu a nm a mt a t in summary the economic performance in eq is measured by the objective function that should be minimized to ensure economic sustainability lca based cost denoted lc we consider that the company will identify some strategic input costs water oil energy etc that need to be considered in economic objective function also some outputs waste co products etc need further treatment and there are also some related costs let us denote cit the consumption of the input iai during period tat x xx x x xx p p r r a cit cfiph qphntz þ cfipn qpntz a p a ppf h a hn a f z a z þ x xx p a p pf n a rz a z d d a cfipn qpntz p a ppf n a rz a z þ xx x x t cfim fpnnumt a i a t p a pn a n nu a nm a m let us denote eot the emission of the output oao during period tat x xx x x xx p p r r aþ eot ef q ef q a oph opn phntz p a ppf h a hn a f z a z þ x xx environmental objective the second key objective to achieve sustainable supply chains is the evaluation and the optimization of the environmental impact eq the determination of the environmental performance of a supply chain is not easy and might be different from one industry sector to another however the use of an lca approach helps in the evaluation of the environmental performance of product process and service to make it general we aggregate the different impacts in term of ghg emissions objective function which are very important in our case due to the link with ets once again ghg emissions should be minimized to ensure environmental sustainability x x x in eot coout þ c co it o i tat oao iai d d a efopn qpntz xx x x p a pn a n zc þ sc þ pc þdc þ rc þtc þ lc þcc p a p pf n a rz a z p a ppf n a rz a z þ pntz t a tz a z constraints t efom mt a o a t a nm a m suppliers supplier capacity x x mt r lpnt ypnt a p mp a s a t a f m a m thus the cost of using inputs and treating outputs if necessary is xx xx vit cit þ uot eot iai tat o a ot a t k carbon credit component denoted cc for many organizations and industrial sectors the main emissions are greenhouse gases many companies have set voluntary targets in term of ghg emissions attributable to their supply chain or are subject to a new regulation that caps ghg emissions under an emission trading scheme ets carbon dioxide is tradable this system is based on the allocation of units to a company for exceeding its intensity based ghg emissions reduction targets to emit one metric ton of carbon dioxide equivalent at the end of each compliance period the emissions of the company will be veriﬁed each emitter must then offset its ghg emissions against its intensity based ghg emissions reduction target established by the government the discrepancy between the imposed target and the actual emissions may be offset by among other things the purchase of units on the domestic market in addition to internal reductions large emitters will be able to buy units from the carbon market in order to ensure compliance with their ghg emissions reductions obligations on the other hand those companies with emissions less than the cap will have the possibility to sell credits in the carbon market and generate proﬁt thus carbon management consists of taking the if the supplier is selected it will stay operational for the whole planning horizon z ypnðt ypnt a p mp a s a t production units location of production units at zones x xx p l qphntz r mynz a f a z m big number p a p pf h a ht a t raw material products usage x xx p jppu qpuhntz a p mp a f a t n qpnt pu a p pf h a hz a z products usage x xx x x p fpnunmt jppu qpuhntz nu a f rm a m a p pf a f a t pu a p pf h a hz a z capacity of production units x p p qphntz rqpphnt yphnt a p pf a h a f a t zaz logic constraints if a technology is not selected at a production unit there is no need for conﬁguration p h r yhnt yphnt a p pf a f a h a t a chaabane et al int j production economics logic constraints if the production unit is not operational there is no need to implement a technology in this facility f h yhnt r ynt a f a h a t limited number of distribution centers per zone x l ynz o a d zaz inventory of materials at production units fi ipnðt þ xx fpnunmt þ nu a sm a m x x fi n fpnunmt ipnt þ qpnt a p mp a f a t nu a rm a m initial inventory levels for products fi inventory capacity constraints raw material components a p mp pf p a f a t inventory of output products xx p x x x x fo qphntz þ fpnunmt ipnt þ fpnnumt recycling centers location recycling centers at zones xx r d a l ðqpntz þqpntz þqpntz þ r mynt a r a t m big number p a p pf z a z fo ipnðt þ h a a z pf nu a rm a m nu a f dm a m a p a f a t initial inventory levels for products fo a ppf a f ri a f a t h h z yhnðt yhnt a h a f a t limited number of production units per zone x l ynz o a f initial inventory levels for ﬁnal products a p a d a ppf a d a t distribution center capacity x x d fpnunmt þ fpnnumt r wnt ynt x x nu a f m a m ppf a d at re processing of good products x a fppu ypuntz þqpuntz a p mp ppf a r a t a z inventory of output products raw material components from recycling centers x x x ro r ro ipnðt þ qpntz ipnt þ fpnnumt a p mp p pf a r a t zaz nu a f m a m initial inventory level of output products raw material and components from recycling centers a pmp p pf a r inventory capacity of output products raw material and components at recycling centers recycling process capacity x a r qpntz r qrpnt ynt a ppf a r a t a pmp p pf a r a t zaz if a node is operational it is used for the planning horizon r r zynðt ynt a d a t if the distribution center is not located in a speciﬁc region then it is not operational x d l r ynz a d a t ynt zaz a ppf a r a t a z ro riro ipnt pnt nu a c m a m a d a t a ypntz qpntz if the production center is selected it will stay operational for the whole planning horizon d d z ynðt ynt disposal of non valuable products sorting process ro inventory capacity constraints for ﬁnal products at dcs d ridpnt ipnt pu a ppf nu a c m a m pf a ppf a r a t ri riripnt ipnt r qpntz distribution centers dcs inventory constraints at distribution centers x x x x d d ipnðt þ fpnunmt ipnt þ fpnnumt a p pf a d a t d ð35þ zaz nu a f m a m zaz inventory capacity of used products of recycling centers d qpntz zaz nu a c m a m logic constraint for operating production units x f l ynt o ynz a f a t a ppf a r ri þ ipnðt if a technology is acquired it is used for the whole horizon inventory of used products at recycling centers x x x ri a fpnnumt ipnt þ qpntz a p pf a r a t if a production unit is operational it will stay for the whole planning horizon a p pf a t nan initial inventory of products recovered at recycling centers a ppf a f a t f f ynt z ynðt recovery of used products xx x x fpnnumt dpt dpnt n a c nu a rm a m inventory capacity constraints components products fo ipnt rifo pnt nu a dm a m a pmp ppf a f fi rifipnt ipnt customers demand constraint x x fpnunmt dpnt a ppf a c a t transportation transportation capacity x t fpnnumt rcmnnut ymnnut p a p mp p pf a n a n a m a t a chaabane et al int j production economics if the recycling center is not located in a speciﬁc region then it is not operational x r l r ynz a r a t ynt zaz limited number of distribution centers per zone x l ynz o a d zaz carbon management at each period and zone the company must be in compliance with the limitation of carbon emissions thus x x x out x x x p p r r coo efoph qphntz þ efopn qpntz p a ppf h a hn a f oao þ x x p a ppf n a r d d þ efopn qpntz p a p pf n a r þ x x x x xx p p coin cfiph qphntz i iai r r cfipn qpntz þþ p a p pf n a r x x p a ppf h a hn a f d d cfipn qpntz p a ppf n a r e þ cctzþ cctz rltz a a z limit on the number of credits to purchase r lcop cctz tz a a z limit on the number of credits that can be sold cctzþ r lcos tz a a z it is clear from the model formulation in the previous section that some data to use in the model is sometimes difﬁcult to ﬁnd and the company should make an effort to collect a big number of input parameters to use the proposed model however the primary goal in this paper is to demonstrate how the mathematical model can be used in order to evaluate the impact of different legislations on the supply chain strategy and planning in practice the model could be modiﬁed to capture some speciﬁc strategic considerations of a given supply chain and populated with additional data for the aim of the study some parameters are estimated the others ones are collected from the available information on the web for example the model developed in this paper highlights the importance of lca data in order to help and inform managerial decision making numeric estimates of air emissions solid waste generation material consumption are valuable in estimating model parameters thus enabling the bridging of operational and environmental decisions the intergovernmental panel on climate change ipcc is an important source of data and was used to estimate emission factors http www ipcc nggip iges or jp efdb main php experimental evaluation data the mathematical model has been developed validated and was used in a preliminary study of a supply chain from the aluminum industry to illustrate the potential application as a decision making tool for sustainable supply chain planning under different environmental regulations that impose mandatory limits on carbon emissions as well as the obligation of product recovery and recycling at the end of life in this research we consider the aluminum production as an example to study some important research questions and ﬁnd some managerial decisions under environmental regulations in the aluminum industry there are typically two sources of raw materials namely bauxite which is the primary raw material from which aluminum is made and secondary aluminum which is obtained by recycling aluminum products since aluminum is recyclable without any loss of its natural qualities recovery of the metal via recycling has become an important facet of the industry products under consideration can be made either made from primary or secondary aluminum using either one of two potential technologies which have different operating costs and different ghg emissions critical inputs and outputs including liquid solid energy and gaseous wastes are considered we consider that the supply chain have two type of family products two production units are responsible for the production and products are distributed using two distribution centers for each raw material two suppliers are available used products are returned to recycling centers depending on the state of the returned product they are recycled or disposed the recycled products are returned to the production units to be used in the production process an overview of the supply chain is shown in fig some statistics for the model are summarized in table solution method different commercially available optimization software exists today to solve mip optimization problems as discussed in previous section it is currently impossible to use real data to populate the model however to demonstrate the practical solvability of the model we randomly generate some numerical scenarios of parameters and constraints see table and solve the model using the lingo version of lindo systems inc on an amd mhz pc running windows xp the mean and median run times per numerical scenario with the default lingo solver settings were and respectively which is a very practical amount of time although this example is with limited number of products and sites suppliers production units and recycling centers more sophisticated global optimization approaches for large scale optimization problem could be used to solve large scale supply chain network research questions in this section we will present brieﬂy the research questions that we attempt to answer by populating the model with data and solving it to be in compliance with the environmental regulations the company has different options the ﬁrst one is to re locate production units and recycling centers at other regions carbon prices vary from one region to another the company can invest on new production technologies to reduce carbon emissions and energy use thus the ﬁrst research question is given the regulation on carbon emissions what is the nature of compliance of the supply chain given the cost of the various compliance options the cost of green technologies versus carbon emission trading the second research question is given the regulation based on take back legislation that impose collection recovery and recycling targets for products at the end of their life what is the nature of managerial decisions related to the supply chain results and discussion the ﬁrst aspect analyzed is the impact of carbon price variations on the supply chain conﬁguration under two different scenarios in scenario carbon prices are stable in time however in scenario carbon prices increase over time the carbon prices fig and results for scenario are shown in fig here the carbon credit component is positive and represents of the total cost that a chaabane et al int j production economics raw material suppliers customers distribution production system output output output beauxite supplier aluminium technology aluminium distributor customers production plant carbon anode supplier technology input input output aluminium output second generation aluminium recycling plant input deleted products reverse logistic input fig case study supply chain network model components quantity model components quantity periods materials products regions suppliers production units production technologies distribution centers clients recycling unit inputs outputs carbon prices table statistics for the model period fig carbon prices variation for scenario table characteristics of the mip model case study number of variables binary variables continuous variables number of constraints 809 means that the supply chain needs to buy worth of carbon credits during the planning period to be in compliance with the environmental regulation table compares the results obtained for the two scenarios first we observe that the emission cost for scenario is higher but that the total logistic cost remains the same for both scenarios this is because the supply chain conﬁguration combination of sites technology used distribution channels etc is the same in both scenarios here carbon prices only resulted in an increase in total cost with no consequence on the supply chain conﬁguration because the marginal cost for reducing one unit of ghg emissions is greater than the carbon price from the market hence the best a chaabane et al int j production economics decision is to buy credits form the carbon market to be in compliance with the regulation limits on carbon emissions the second aspect analyzed is the impact of recycling strategies on supply chain planning decisions here we assume that legislation forces the company to accept all recycled products ﬁrst the supply chain is solved for different return rates d of aluminum products for the ﬁrst scenario we consider that only of products available in the market are recycled d in the second scenario secondary aluminum may come from other sources including the direct customers and hence a return rate of d table summarizes the results obtained in this case it shows that an increase in recycling of the products increases the total cost by which translates into a increase in logistics cost and a increase in carbon credit cost in this case the legislation on recycling has a negative impact on carbon costs as it forces the supply chain to use technologies that have higher ghg emissions the ﬁnal aspect studied is the impact of limit on emissions we analyze two scenarios where regulations in terms of carbon emissions becomes more stringent versus in this case we suppose that carbon prices will increase fig fig shows that the quantity of recycled product increases as the limit of emissions is more stringent because carbon emissions are reduced due to the use of recycled product when recycling is cheaper and with less ghg emissions product recycling mostly increases and the cost is minimized however in the last period the quantity of recycled product decreases indeed due to the strategy of carbon management fig that consists of buying carbon credits when carbon prices are not expensive helps the company to reduce the cost of fig cost distribution for scenario table comparison of the two scenarios total logistics cost cost of carbon credit total cost scenario stable scenario increase 441 table cost for the different scenarios return rate variation scenario d 320 214 recycled products period fig recycled product under policy stringency tco2e tco2e carbon credits total cost total logistics cost carbon credit scenario d period fig carbon management under policy stringency a chaabane et al int j production economics 49 tco2e lco2 tco2e product flow tones 20000 period fig product ﬂow between recycling centers and production units compliance to the regulation moreover recycled products are less this means that an environmental regulation that impose limits on ghg emission might lead to decrease in recycling activities if recycling costs are not optimized fig shows that the recycling activity is reduced under policy stringency and when carbon prices are very high conclusion in this article we present a generic mathematical model to assist decision makers in designing sustainable supply chains over their entire life cycle first the model has the potential to be a tool that facilitates the understanding of optimal supply chain strategies under different environmental policies recycling and ghg emissions reduction the model shows that the various environmental legislations must be strengthened and harmonized at a global level in order to drive a meaningful long term environmental strategy the explicit consideration of environmental costs within supply chain design is critical under the emergence of emission trading schemes the integration of life cycle analysis principles at the supply chain design phase maximizes the long term sustainability while some speciﬁc values of model parameters would depend upon the application the methodology presented here is general enough and may be applied to other supply chain studies to evaluate their performance in term of cost and carbon emissions finally although only the economic and environmental dimensions of sustainability are considered in the mathematical model the methodology can integrate the social dimension as soon as measures of social stainability are identiﬁed af cpnt ar cpnt afnt ahhnt apphnt p cphntz fi cpnt fo cpnt adnt d cpnt arnt rc cpnt r cpntz dr cpntz appendix different parameters are necessary to describe the model monetary parameters alnz aspnt fixed cost for locating site naf d r at zone zaz fixed cost due to acquisition of raw material papmp from supplier nas during period tat this represents the cost of development of long term partnership with the supplier to guarantee a good service level e g investing on information technology and communication ri cpnt ro cpnt mt t cpnn mt vit uot purchasing cost of one unit of raw material papmp from supplier nas during time period tat purchasing cost of one unit of recycled material papmp ppf from recycling center nar during time period tat fixed cost associated with the operation of production unit naf during time period tat fixed cost associated with the implementation of a new technology hah in production unit naf during time period tat production cost of one unit of product pappf using technology hah in production unit naf during time period tat variable cost associated with the conﬁguration of technology hah used in production unit naf to manufacture assemble product pappf during time period tat at zone zaz inventory carrying cost of input product papmp ppf at production unit naf during time period tat inventory carrying cost of output product pappf at production unit naf during time period tat fixed cost associated with the operation of distribution center nad during time period tat inventory carrying cost of product pappf at distribution center nad during time period tat fixed cost associated with the operation of recycling center nar during time period tat purchasing cost of one unit of used product pappf at recycling center nar during time period tat refurbishing cost of one unit of used product pappf at recycling center nar during time period tat at zone zaz disposal cost of used product pappf at recycling center nar during time period tat at zone zaz inventory carrying cost of product pappf at recycling center nar during time period tat inventory carrying cost of raw material papmp at recycling center nar during time period fixed cost associated with the establishment of a transportation link between node nan and node an using mode mam during time period tat transportation cost of one unit of product papmp ppf between node nan and node an using transportation mode mam during time period tat usage cost of input iai during time period tat emission cost of output oao during time period tat a chaabane et al int j production economics 49 acc tz the market price of buying a credit during time period tat at zone zaz the market price of selling a credit during time period tat at zone zaz vtzcc lcos tz e ltz limit on the number of credits to sold during compliance time period tat at zone zaz aggregated limit in term of carbon dioxide equivalent emissions during compliance time period tat at zone zaz technical parameters p cfiph r cfipn d cfipn t cfim p efoph r efopn d efopn t efom lpnt jpp qpphnt ifipnt ifo pnt idpnt wnt dpnt dpt iripnt ypntz qrpnt mt european journal of operational research 679 contents lists available at sciverse sciencedirect european journal of operational research journal homepage www elsevier com locate ejor operations research for green logistics an overview of aspects issues contributions and challenges rommert dekker a jacqueline bloemhof b ioannis mallidis c a erasmus school of economics erasmus university rotterdam p o box dr rotterdam the netherlands department of logistics decision and information sciences wageningen university hollandseweg kn wageningen the netherlands c department of mechanical engineering aristotle university of thessaloniki p o box thessaloniki greece b a r t i c l e i n f o article history available online november keywords environment logistics supply chain management transportation a b t r a c t the worldwide economic growth of the last century has given rise to a vast consumption of goods while globalization has led to large streams of goods all over the world the production transportation storage and consumption of all these goods however have created large environmental problems today global warming created by large scale emissions of greenhouse gasses is a top environmental concern governments action groups and companies are asking for measures to counter this threat operations research has a long tradition in improving operations and especially in reducing costs in this paper we present a review that highlights the contribution of operations research to green logistics which involves the integration of environmental aspects in logistics we give a sketch of the present and possible developments focussing on design planning and control in a supply chain for transportation inventory of products and facility decisions while doing this we also indicate several areas where environmental aspects could be included in or models for logistics ó elsevier b v all rights reserved introduction operations research or has been described as the science of better the slogan of the informs society as it mainly focuses on minimizing the costs of existing processes yet in today society it is not only proﬁts that are important as many people companies and governments are concerned about the sustainability of our society so can operations research also contribute to a better environment in our opinion the role of or for the environment should get more attention operations research leads to a more efﬁcient use of resources which is not only cost attractive but also tends to create less emissions of greenhouse gases secondly operations research helps to identify the trade offs between environmental aspects and costs very often much reduction in emissions can be achieved with only a marginal increase in costs operations research techniques and especially multi criteria decision analysis is therefore an important method in this respect in this review we will highlight its possible contributions to green logistics which is the study of practices that aim to reduce the environmental externalities mainly related to greenhouse gas emissions noise and accidents of logistics operations and therefore develop a sustainable balance between economic environmental and social objectives http www greenlogistics org last accessed on august we deal with all aspects of logistics such as corresponding author e mail address rdekker few eur nl r dekker see front matter ó elsevier b v all rights reserved doi j ejor transportation warehousing and inventories and address the related environmental aspects such as emissions of greenhouse gases noise and use of scarce resources we will not differentiate between green logistics and green supply chain management while we mainly focus on transportation we take a broader supply chain perspective however we will not address environmentally conscious manufacturing or waste management the purpose of this overview is to give a sketch of the present and possible developments as many papers are presently being written we do not claim to cover all instead we focus on the structure of the ﬁeld and illustrate this with some representative papers the choice of which always remains subjective there are other overviews such as corbett and kleindorfer b kleindorfer et al on sustainable operations management srivastava and sarkis et al on green supply chain management and sbihi and eglese on combinatorial optimization and green logistics but ours is more comprehensive up to date and more detailed with respect to transportation in this sense we ﬁll the gap in industrial ecology as observed by sheu et al on the integration of logistics ﬂows in a green supply chain a recent book by mckinnon et al has some overlap with this review but we take a wider perspective finally we would like to mention that our structuring is also in line with the business perspectives of consultants see palanivelu and dhawan in our review we follow to a large extent the supply chain structure given by chopra and meindl first we discuss the main physical drivers behind a supply chain and examine r dekker et al european journal of operational research 679 fig framework of the paper transportation in section products and inventories in section and facilities in section we investigate the main choices in these drivers which affect environmental performance we consider these options in the three decision phases of a supply chain namely design planning and control while we also discuss reverse supply chains in section we discuss the design of a supply chain and how the combination of the drivers affects the environment section examines the design of reverse and closed loop supply chains section focuses on the three cross functional drivers viz sourcing planning and pricing revenue management and in section we take a closer look at the operational planning of supply chains green supply chain metrics are examined in sections and describes the or methods that helps in making the trade offs in green logistics viz multi criteria decision making fig describes the proposed framework of this paper transportation with respect to the environment transportation is the most visible aspect of supply chains transportation emissions amount to some of total emissions both at global and eu level stern eea transportation is also a main source for nox and pm particulate matter or ﬁne dust emissions mckinnon and woodburn piecyk and mckinnon have done studies of the most relevant factors for emissions in road transport they developed a framework with ﬁve types of factors viz structural factors inﬂuencing modal split commercial factors inﬂuencing load factors operational factors functional factors and ﬁnally external factors inﬂuencing carbon intensity of fuel since we also address other modes of transport in this paper we will change their framework somewhat we examine four choices with respect to transportation which are supported by operations research models namely mode choice or modal split use of intermodal transport equipment choice and fuel choice the commercial factors will be discussed in later sections mode choice one of the main choices in transport is the mode of transportation viz transport by plane ship truck rail barge or pipelines each mode has different characteristics in terms of costs transit time accessibility and also different environmental performance in reality the choices are limited as the transport mode is often determined by the type of product e g liquid bulk or package and the distance to be travelled in case of intercontinental supply chains the main choice is between air and sea for continental chains it is mostly between truck airplane train or short sea ship time sensitive goods are often supplied by air while large volumes of commodities like coal iron ore are transported by rail inland barge or pipeline in case of gas or oils technological innovations such as cooled reefer containers and data loggers for temperature history have enabled a shift from air to slower modes like truck or sea ship there are few or papers that deal with these issues leal and d agosto use the modal choice method to choose alternative ways of transporting bio ethanol using ﬁnancial and socioenvironmental considerations they ﬁnd that the best choice is to use local road transport to feed long distance pipelines which deliver bio ethanol directly to the ports long distance road transport appears to be the worst of the alternatives considered within transportation there is a large stream of papers identifying the shipper preferences with respect to the different transportation characteristics such as cost quality and speed although one has to be cautious with ﬁgures because they depend heavily on the way they are calculated we would like to present in table the following illustrative comparison of emissions between equipment types in several modes the source is the network for transport and the environment see http www ntmcalc se index html last accessed on february teu is the standard measure for containers and one teu is equivalent to a ft container and pm stands for particulate matters also called ﬁne dust we would like to highlight some important general relations which can be observed in this table first of all the bigger the transport unit in the same mode the fewer the emissions per g t km under ﬁxed utilization loads when comparing transport modes we observe that water can easily carry heavy loads hence water transport is efﬁcient that rail is more efﬁcient than trucks and a boeing though being a large plane is not at all efﬁcient when compared to the other modes the modes do not differ much in sox emissions except for the boeing which clearly emits much more ships are responsible for high nox emissions whereas trucks and diesel rail are relatively clean compared to other modes finally the ﬁgures for ﬁne dust pm do not differ much here it depends very much on the particular engine type and whether soot ﬁlters are applied it will be clear that not one mode is the preferred one from an environmental point of view and that or methods are quite useful to identify the trade offs between different mode choices below we sketch some recent contributions bloemhof et al use sustainability radar diagrams to investigate the environmental impacts of inland navigation compared to table energy use and emissions for typical transport units of different modes source ntm energy use emissions g t km ps type container vessel 000 teu s type container vessel teu railrailelectric diesel kw h t km sox nox pm 018 043 n a heavy truck 00005 008 boeing n a r dekker et al european journal of operational research 679 rail and road transport road transport is the largest contributor of emissions although signiﬁcant emission reductions have been achieved in recent years if rail transport and inland navigation continue to refrain from innovations the gap will close even further it appears that only sustainable innovations that also contribute to proﬁtability will succeed the eu has set standards for nox and pm emissions for trucks as a result trucks meeting the highest standard to date euro v are much cleaner than most ships and trains ocean going ships emit huge amounts of nox see cleanairinitiative it is estimated that ship emissions will surpass total emissions generated by all land based mobile stationary and other sources by unless drastic measures are taken intermodal transport closely related to the choice of transport mode is the use of a single transport load unit like a container over multiple transport modes which is called intermodal transport one of the main inefﬁciencies in transport is the handling of the goods at transshipment points the introduction of containers has signiﬁcantly reduced this inefﬁciency many goods shipped through intercontinental chains are shipped nowadays in containers the land part of such a chain occurs by truck rail or inland barge the rise of new inland container terminals to facilitate inland ship rail road combinations can save thousands of truck kilometers in congestionsensitive areas and thus reduce the environmental impact this also pertains for continental chains the downside of intermodal transport is that it requires more coordination than single mode transport some papers can be found on the value of visibility in intermodal transport yet few papers address environmental aspects such as bauer et al goel presents a transportation model combining shipment and route choices to improve on time delivery performance these kinds of models could easily be adapted to include green metrics such as carbon emissions energy used spoilage and losses etc janic assesses environmental effects of transforming a large airport into a real multimodal transport node connecting the airport to the highspeed rail transport network macharis and bontekoning argue that intermodal freight transportation research is an emerging ﬁeld that needs different types of models compared to those applied to unimodal transport equipment choice and efﬁciency once a choice has been made about the transport mode a decision must be made concerning the type and size of the transportation unit this decision affects capacity speed economics and environmental performance as said in section the larger the transportation unit the fewer emissions per kg transported the fewer emissions of other gasses such as nox and pm as shown in fig yet this relation only holds if the utilization or load factor remains and that is an important tactical operational issue new equipment is also more energy efﬁcient we can observe this in airplanes where modern planes like the airbus or boeing are more energy efﬁcient than the older boeing but also in ocean going container ships like the emma maersk which can carry some 000 teu much more than the ps type in table however the problem is that much capital is tied up in equipment which lasts for decades and that changes are therefore costly papers on these subjects include d agosto and ribeiro who focus on road ﬂeet operation taking into account minimizing fuel consumption as well as wider economic and environmental aspects vanek and morlok review efforts to improve the energy efﬁciency of the most frequently used trucks and to shift more freight to energy efﬁcient rail transportation to this end they identify the ratio of production to transportation energy use of major commodity groups in the us through life cycle analysis and spatial analysis of freight patterns in the us they claim that reductions in energy usage could be achieved by redesigning ﬂow patterns and therefore advocate shifting more freight to energy efﬁcient modes fuel choice and carbon intensity a fourth aspect in green transportation is the choice of fuel modern gasoline is cleaner compared to old gasoline in the nineties and the ﬁrst decade of the century reﬁneries focused on removing lead additives from gasoline so that the air quality would be better biofuels based on corn or on organic waste can easily be mixed with standard gasoline however more extensive use requires adapting engines which is quite expensive to date few or papers are devoted to this aspect however or models can play an important role in assessing the green performance of adapting biofuels about of all the us produced corn is fermented into ethanol to fuel cars the total amount of fossil fuels used in the process to produce biofuel is not much less than the total amount of biofuel produced bai et al focus on minimizing costs for reﬁnery investment feed stock product transportation and public travel and uses a lagrangian relaxation based heuristic algorithm to ﬁnd feasible solutions this work could easily be extended by also minimizing the use of fossil fuels electric vehicles are environmentally friendly since their engines have almost no emissions and emissions in electric power stations can be controlled however they have a limited range so for goods transport they require a change in operation with possible intermediate goods transshipment hence they are popular for city transport in combination with a transfer station in or just outside the city this application can be cheaper than truck transport to compensate for the short range a dense power re supply network has to be set up possibly in conjunction with a swap of batteries finally fuel choices are also important for ships as coastal states may impose restrictions on fuel types visiting ports to bunker fuel cheaply may become an incentive in shipping route design products and inventories life cycle a supply chain is also characterized by the products it supplies chopra and meindl consider only the inventory aspects of products but we take a more comprehensive view the point is that some products are friendlier to the environment than others three product aspects are relevant in this respect first the way they have been produced their carbon footprint secondly the way they have been transported and waiting for use inventories and thirdly whether their value can be recovered after their use reverse logistics finally we also consider packaging and returnable transport items the overall idea is that environmental aspects can be taken into account when choosing between different versions of the same product even at the level of the individual customer moreover that by measuring and publishing the environmental performance of a product manufacturers are more likely to make greener products the ﬁrst aspect life cycle impact is important since it indicates the resources needed to make the product this can be energy but also other scarce resources such as water some companies like tesco walmart and carrefour have started to indicate the carbon footprint that is the total amount of carbon dioxide emitted for production and transport of some of their products the effect is that consumers can base their product selection based on this carbon footprint the labeling seems easier than it is the whole chain which leads to the product has to be investigated and r dekker et al european journal of operational research 679 allocation problems have to be tackled if multiple products are made together in one process which carbon emissions have to be allocated to which product see quariguasi et al the storage of products also has an environmental impact although this is less visible than the transportation of products inventory holding costs play a large part in supply chain design and the more storage is centralized the less the storage costs the environmental storage footprint consists of cooled refrigerated storage for food or heated storage in case of some oils and evaporation during storage a clear example of the choices to be made is the supply of tomatoes in the netherlands in winter they may be sourced from a warm country far away but in that case much transportation is needed on the other hand they may also be produced nationally in greenhouses but in that case energy is needed for heating hence it will be clear that complex calculations need to be performed to make out which option is the best from an environmental point of view in the product lifecycle the use of the product next to its fabrication should also be considered comprehensive results over all phases on the environmental impact of products are however scarce in order to improve the overall carbon footprint of a product it is necessary to assign the footprint to material used energy used production use and transportation quariguasi et al for many product categories there are also indicators for the energy use of products e g in case of refrigerators cars etc under standardized conditions yet this is only one aspect in the whole picture when the use of a product has ended the remaining value can be recovered instead of land ﬁlling or incinerating the remains for energy recovery in this way some of the carbon footprint invested in the product can be reused the recovery typically involves reverse logistics and this will be discussed in detail in section we like to note that this aspect also complicates lifecycle assessments finally we would like to mention packaging and returnable transport items like pallets containers and roll cages according to some reports palanivelu and dhawan packaging represents some of all waste weight as all packages have to be transported to the retailer as well as to the waste processor it is clear that reducing packaging saves the environment an alternative to single use packaging is returnable packaging such as reﬁllable bottles beer kegs etc yet these create logistic problems as they have to be transported back to the place of origin and have to be cleaned a major issue of all these re usable items is that they are used in networks and their inventories need to be redistributed this has been a topic of several or models see for example gonzalez torre and adenso diaz facilities warehouses ports and terminals facilities are the third driver in supply chains we will take a somewhat more comprehensive view than chopra and meindl and include transportation facilities as well so next to distribution centers we consider airports railway stations and ports with container terminals in recent years the terms green facility and green buildings have gained much popularity several environmental aspects are important viz a internal transport and emissions b energy use of facilities c emissions of transport units used for transport to or from facilities d congestion around facilities most distribution centers operate electric equipment like fork lift trucks because there are no direct emissions involved mckinnon et al and distances are short yet in container terminals where distances are long and the equipment has to cope with environmentally difﬁcult conditions short trips many starts etc diesel fueled equipment is mostly used internal transport is therefore also an important driver for emissions and it is good for the environment if these can be reduced an example of a study of transport distances in a container terminal and related emissions is given in geerlings and van duin they favor compact facilities to reduce internal transport there has also been much pressure from port authorities to reduce emissions for example the rotterdam port authorities contractually oblige a substantial reduction in emissions when new container terminals are built and operated also manufacturers of container handling equipment are now propagating electrically operated equipment operations research methods for warehouses traditionally focus on the reduction of travel times although primarily to improve efﬁciency however since they reduce the amount of traveling they are also beneﬁcial for the environment see for example de koster et al ðdukic et al the energy use of facilities has been a concern for many companies and not only for cost reasons several zero emission warehouses have been built for example by installing energy saving installations using sophisticated lighting systems and solar cells see for example palanivelu and dhawan yet hardly any or models have been developed for these purposes thirdly there are emissions of transport units in or near facilities typically ships leave their engines on while moored this generates substantial emissions also because the engines are running in less efﬁcient modes ports need to introduce requirements to use electricity supplied from onshore installations wall current or cold ironing finally facilities are the nodal points in transportation networks and hence often a bottleneck the use of wave systems in the arrival and departure of planes even aggravates this in a wave system arrivals and departures are clustered in time such systems are applied to provide short connection times for transshipping goods or passengers yet it creates peaks in arrivals and if there are any disturbances long waiting may be the result in general waiting has negative environmental consequences airplanes which have to wait before landing ﬂy in waiting routes ships that have to wait offshore can anchor and emit much less trucks waiting for container terminals may also keep their engines on engines running in these idle modes typically have much higher emissions than normal several or methods such as queuing theory and simulation are applied to study these phenomena although often with an economic incentive as waiting is also inefﬁcient the issue however is that these real systems have quite a ﬂuctuating demand and short periods of overﬂow aspects which are typically left out of queuing models next there are more and more appointment systems where arrivals are to some extent controlled in order to avoid congestion and hence emissions see dekker et al yet few papers exist on queuing models with appointments supply and transport chain design in this section we show how the drivers speciﬁed in the previous sections viz transportation and facilities can be combined in a supply or transport chain ensuring that customer requirements are fulﬁlled we will consider how the choices of each driver should be made and therefore address the following aspects sourcing where do we get products from which country determination of production and distribution concepts determination of the type number and location of facilities choice of the transportation means choice of the transportation speed choice of the transportation concept sourcing the sourcing of products directly determines the need for transportation the provision of many cheap products from asia r dekker et al european journal of operational research 679 has created huge transportation streams to europe and the us nearby alternatives like mexico for the us and turkey for the eu could also be used differences in wages are the main drivers for the difference in product costs but many other factors also play a role the choice between the so called far shoring and near shoring also involves other issues as shorter transportation times also increase ﬂexibility iakovou et al propose an analytical model that includes closed formed solutions to investigate the cost and environmental effect of near shoring a portion of production processes next to the serving markets production concepts production concepts indicate how production is organized a well known concept is the just in time jit concept which states that inventories should be minimized and that shipments should only cover short term needs this reduces inefﬁciencies in organizations yet it also implies many small rush transports with few options to ﬁnd return trips because of the small amount of time available for planning this is not a big issue if the suppliers are close to the manufacturer as then only small distances need to be travelled however this is not always the case and jit principles have also been applied in cases where suppliers are more than kilometer away the way products are assembled and transported also has environmental aspects for example we can choose to import ﬁnished cars or choose to transport car parts and modules and assemble these locally repackaging is another option for example wine or beer can be transported in large quantities and bottled locally this way of assembling a product reduces transportation other examples are diapers from which the air is sucked away so that more ﬁt in a package finally there are or models that optimize the loading of containers and other bins load and bin planning also improve transport efﬁciency several or models have been published in this respect but we are not aware of scientiﬁc papers addressing the environmental consequences facility location within or there is a whole stream of research on facility location which mainly deals with the number and location of distribution centers dcs as inbound ﬂows to dcs tend to be consolidated in full trucks and outbound ﬂows tend to be in smaller units the number of dcs has a large effect on transport efﬁciency environmental aspects of supply chain design and facility location in particular have recently received considerable attention we would like to mention the following papers li et al propose a bi objective proﬁt maximization and emissions minimization objective mathematical programming methodology to optimize distribution center locations taking into consideration transportation costs and transportation production carbon emissions mallidis et al propose a multi objective mixed integer programming model mip to examine how distribution center locations dcs leasing or outsourcing transportation and warehouse operations decisions affect the system environmental performance in terms of inbound and outbound transportation and pm emissions wang et al suggest a multi objective optimization model for supply chain network design they consider transportation handling and next investment costs on green equipment or technology next they examine the emissions generated by production and distribution facilities diabat and simchi levi also consider a mip model for the design of a supply chain with a cap on the amount of produced emissions come from warehouses plants and transports to customers they show that supply chain costs increase if the cap becomes tighter ramudhin and chaabane are among the ﬁrst to propose a carbon market sensitive strategic planning model for sustainable supply chain network design they show that considerations of internal and external control mechanisms are of great importance to decision makers when designing sustainable supply chains chaabane et al extend this approach and introduce a mixed integer linear programming based framework for sustainable supply chain design that considers life cycle assessment lca principles in addition to the traditional material balance constraints at each node in the supply chain they apply it to the aluminum industry and conclude that the present emission trading scheme must be strengthened harris et al consider both logistics costs and emissions in supply chain optimization they take into account the supply chain structure number of depots and different freight vehicle utilization ratios and they illustrate their approach with a simulation model of the european automotive industry transportation means and route once the sourcing decision has been made the mode and means of transport must be chosen in this there is also a trade off between inventories and transportation there are several options with respect to transport e g direct versus indirect routes and the choice of the port of entry into a continent typically studies have been done with costs as optimization criterion but some recent studies also include environmental aspects for example mallidis et al include a choice of port of entry in their supply chain design model as transport by ship is environmentally friendly it pays off to choose the nearest port for a customer blauwens et al and hoen et al elaborate the inventory aspects in making mode choices with environmental objectives and carbon caps palmer introduces these objectives in vehicle routing models hoen et al study mode choices under several types of emission caps transport speed during the economic crisis there was an oversupply of container ships and one of the simplest actions was to slow down the speed of the ships from to knots per hour which resulted in a different schedule but also in considerable savings in fuel costs and hence also in emissions this was called slow steaming later also super slow steaming was introduced where ships would sail only knots per hour several studies analyze the effect of the choice of speed see for example cariou psaraftis et al and corbett et al they also consider the effect of the need for more equipment as well as the effect of longer transit time on cargo operational speed optimization is applied in planes and is upcoming for large ships it concerns changing the speed to be on time in case of varying conditions like weather sophisticated software looks for wind directions and speeds and selects those routes with the most tail wind it adapts the speed to the desired arrival time thus avoiding overuse of fuel see for example fagerholt et al for an application in tramp shipping transportation concepts an important transportation concept is consolidation especially in the less than truckload ltl sector in this case small shipments are combined with larger ones to achieve efﬁciencies of scale for transport over long distances the disadvantage of such transport is that time is needed to consolidate the cargoes which means that it is more difﬁcult to plan them just in time r dekker et al european journal of operational research 679 apart from equipment choice there are also strategic choices of delivery time this is reﬂected in the various services offered by express carriers such as fedex tnt and ups see for example tnt they provide several options for transporting small and valuable items such as same day next day and deferred delivery the fastest choice is accomplished by having faster transport e g air instead of road and reducing consolidation options i e waiting for other cargo in fact the fastest transport is a single courier accompanying a unique transport but it is also the most costly and hence it is likely to generate many emissions express carriers also specialize in reliable transport times the guaranteed delivery times can only be achieved by running scheduled services according to a ﬁxed timetable large variations in the demand now imply that load factors and utilizations are typically low which is especially visible in express airlines it is also clear in the difference between parcel transporting companies and the large express companies like ups and fedex the former is much more efﬁcient and cheaper in transporting their goods by waiting longer for other cargo within the express business there is a difference between deliveries and deliveries the latter is to individual customers which is inherently more difﬁcult as customers may not be at home and the address may be more difﬁcult to ﬁnd kull and boyer focus on this last mile supply chain the portion of the supply chain delivering products directly to the customer consolidation concepts for this last mile transport are for example delivering packages in city centers postal ofﬁces retail shops where customers can pick up their package using a unique pincode this avoids multiple delivery attempts because people are not at home a third concept worth mentioning is the use of direct trips versus so called milk runs in which a number of customers are served in one route milk runs are typically more efﬁcient in case of small shipments to customers located not far from each other shipments can be combined and larger and therefore more efﬁcient transport units can be used this also creates some inefﬁciency as cargo typically travels longer before reaching its destination this aspect is quite visible in airline networks some use triangular routes on long distances whereas others go for direct routes only often only direct routes are used for short ﬂights next we would like to mention the choice between direct and indirect transport or also called direct versus hub and spoke transportation the latter consolidates transport to hubs in larger amounts and hence in more efﬁcient transport units whereas direct transport uses small transport units to provide faster service planning direct transports is easier than indirect transport as in the latter the schedules need to be coordinated to provide seamless connections transfer of goods from one mode to another is often a delicate business as errors and disturbances typically occur or become visible at the transfer point the handling of most not containerized cargo is quite labor intensive with the result that transfer is relatively costly yet we see intermediate transport concepts coming back in city logistics where they have several advantages first of all cities require different transport means smaller and less polluting e g electric than highways do as large trucks are more and cost efﬁcient accordingly in several city concepts intermediate transfer points are used to transfer goods from large trucks to environmentally friendly small city trucks or even canal boats furthermore consolidation efﬁciency gains can also be achieved by combining transports from different suppliers to neighboring shops a ﬁnal important recent logistic concept is the use of cross chain control towers whereby different supply chains are combined in order to make efﬁciency gains for example sharing transport equipment has a positive environmental impact because less equipment is needed although all these concepts are related to the environment and have deﬁnite or aspects papers incorporating both are lacking product recovery and closed loop supply chains many products are not completely consumed when their use is stopped and almost all have some remaining value reverse logistics consists of all logistic activities necessary to unlock this value through product recovery it consists of collecting discarded goods inspecting and sorting them followed by some recovery action which can either be a simple cleaning or a complex disassembly and ﬁnally a remanufacturing process and a remarketing of the output which recovery action direct re use remanufacturing parts retrieval or recycling is taken depends on the state of the product the associated costs and the demand for the results when reverse logistics forms an integrated part of a supply chain we use the term a closed loop supply chain many papers have been written on reverse logistics because managing returns is complicated by several problems for example collection networks need to be set up and recovery operations need to be planned the environmental effect of reverse logistics consists of offering products or parts made or retrieved from discarded products and recycled materials instead of having to make new ones however it may be difﬁcult to guarantee the quality of recovered goods and the cleaning processes may be polluting moreover much transportation may be needed to generate large enough volumes for recycling or remanufacturing processes yet in general reverse logistics and closed loop supply chains are considered to be environmentally friendly geyer and jackson many surveys have focused on closed loop supply chains linton and klassen srivastava however the papers mentioned in these surveys do not explicitly deal with environmental impacts in a supply chain context but instead for example use units returned as a measure for environmental impact quariguasi et al discuss if and for which cases the assumption of a sustainable closed loop supply chain holds barker and zabinsky propose a multi criteria decision making model using the analytical hierarchy process for designing reverse logistics networks while alshamrani et al propose a heuristic procedure for developing pick up along with delivery root design strategies for returning materials moreover product recovery issues are also tackled finally we would like to mention le blanc et al who describe the possibilities to improve vehicle routing for reverse logistics supply chain planning and control on the tactical side with time horizons of a quarter to a year several key decisions have to be made such as forecasting production capacity planning inventory control and marketing operations including pricing strategies here we focus on the environmental aspects due to pricing supply chain planning and procurement pricing and emission trading one of the most successful operations research applications from the last decade is revenue management while it was originally developed for airlines it is now also used for hotels and high speed railways etc basically it aims to get most out of scarce capacity in the sense that prices are varied according to capacity left under varying demand compared to a time invariant price it allows lower prices at an early stage compensated by a high price later furthermore customers have a monetary incentive to switch to moments where more capacity is available and hence prices are lower the effect has been that capacity is much better used for example airlines nowadays operate with much higher load factors than in the seventies a study by agatz et al is one of the r dekker et al european journal of operational research 679 few examples where revenue management for package delivery is applied to decrease total kilometers driven they demonstrate what e tailers can learn from the airline pricing by stimulating customers to choose delivery windows in which neighboring customers have already placed delivery orders next to revenue management which is in fact a tactical and operational instrument we would like to mention the carbon emissions trading scheme which was one of the main results of the kyoto protocol this scheme has been institutionalized in the european union emission trading system eu ets in of the eu member countries and similar schemes have been adopted in other countries as well in such a scheme companies are given an initial allowance for carbon emissions which is reduced in time companies wanting to emit more have to buy emission rights on the market there have been several problems with the system when a recession came in and it appeared that some countries had been given very large allowances carbon prices dropped to almost zero in as the global economy is recovering carbon prices have gone up again one of the main issues is which industries are to be included in the scheme and how many rights are to be allocated freely to date only the large industries are included as administration efforts would otherwise be prohibitive several researchers have investigated how companies should react to such a carbon cap we would like to mention two papers benjaafar et al investigate these for very simple inventory control models their work has been extended by hua et al hoen et al investigate transport mode choices under carbon cap constraints allocation route navigation and vehicle routing we would like to note that equipment pooling and speed optimization can be done both at a tactical as well as an operational level beltran et al give an example of methods for the allocation of green vehicles to routes route navigation indicates the route between two given points navigation software for cars trucks and planes apply sophisticated shortest path algorithms and reduce the amount of kilometers to be travelled accordingly they also reduce environmental emissions a vehicle routing vrp package applying or methods also reduces the number of kilometers driven the dutch transport economics agency nea now part of ecorys reported that vrp packages typically reduce the cost by some because fewer kilometers need to be driven and load factors are higher nea palmer dissertation presents an integrated routing and emissions model in which speed can be varied one of his results is that savings up to in emissions can be obtained his work was extended by maden et al who also did a case study with delivery vehicles in the uk also ubeda et al present a case study a further step was made by bektasß and laporte who in their pollution routing problem use a comprehensive emission model taking load and speed into account in vehicle routing and their effect on emissions finally we would like to mention equipment pooling pan et al in press describe a case where transportation equipment is pooled between several companies in order to increase load factors which are said to be only they quantify the effect of this pooling on supply chain planning metrics supply chain planning consists of making capacity plans over medium time horizons in this phase transportation warehouse capacity inventories etc are planned several decision support systems using or models exist and these are often referred to as advanced planning systems they all seem to focus on the traditional cost objective good planning however also reduces the need for corrective actions such as emergency shipments these generally have a negative environmental performance since these types of transport are quite inefﬁcient small volumes with fast transport modes so indirectly or also contributes to a better environment although this aspect has not been quantiﬁed an important element in the use of optimization for the environment is the use of metrics through metrics the environmental effect is made clear and different alternatives can be compared aronsson and huge brodin identiﬁed the measurement of emissions as one of the most important ways to estimate environmental impact moreover metrics can also be used in the transfer of goods and services in a supply chain as they indicate the environmental effect of each supply chain party an important example of such a metric is a which determines the amount of emissions for all transports a company has made several are available today both for individual customers and companies they are based on simple formulas and average statistics an attempt to set a worldwide standard to make the calculations comparable among companies is scgreen they take the total number of kilometers driven average load factors average type of equipment average fuel use and derive the total estimated emissions boer et al yet several issues are important in calculating transport emissions in the aforementioned ways first of all to what extent are return trips or backhauls taken into account secondly if multiple cargos are transported in one trip how should the emissions be allocated to them they could be allocated based on volume or weight but this is not likely to give the same answer so a subjective element is involved hervani et al identiﬁes a selected list of other metrics that range from atmospheric emissions to energy recovery they examine measures for on and off site energy recovery recycling and treatment spill and leak prevention and pollution prevention several authors use ced cumulative energy demand as a measure of environmental impact helias and de haes and huijbregts and rombout observe a high correlation between ced and most of the environmental impact categories used for life cycle analysis sundarakani et al also present a methodology to model carbon footprints across the supply chain they use both long range lagrangian and eulerian transport methods procurement a major element in stimulating green behavior is the inclusion of environmental aspects in the procurement or tendering process in this way green behavior pays off for companies however it also makes these processes more complex and in fact multi criteria decision approaches are needed ates et al study the impact of external environmental investments i e investments in collaboration activities with suppliers related to production and logistics such investments may take the form of joint problemsolving sessions information sharing or establishing common goals etc vachon and klassen as the environmental performance of suppliers has a direct impact on the products of the purchasing ﬁrm it is crucial to assure the same level of environmental awareness operational control of supply and transport chains although it may seem that the strategic choices determine most of the environmental impact also in the daily operations there is much scope for environmental improvement especially with or methods we would like to mention choices such as equipment r dekker et al european journal of operational research 679 finally we like to mention mierlo et al who introduce two rating systems assessing the environmental damage caused by vehicles they assess electric hybrid and fuel cell vehicles using a simpliﬁed life cycle analysis lca method speciﬁc or methods multi criteria decision making most applications of or tools focus on efﬁciency of operations this translates directly into fewer activities transports etc and hence also in fewer emissions yet it is not always that easy and trade offs have to be made e g in procurement multi criteria decision making mcdm methods can typically help in such situations some researchers apply multi objective optimization for example mallidis et al presents supply chain designs made under several objectives viz and cost minimization the supply chain designs do not differ much furthermore mcdm methods can help to identify the synergies between cost and environmental objectives yet few applications of mcdm have been reported in this respect we would like to mention macharis et al on the strategic choices for the flemish transportation and logistics sector and sasikumar et al and barker and zabinsky on strategic issues for reverse logistics we have already mentioned the study by wang et al on supply chain design furthermore there is a study by azapagic and clift who discuss multi objective optimization in life cycle assessment lca they state that the value of multi objective optimization in system analysis lies in providing a set of alternative options for system improvements rather than a single prescriptive solution thus enabling the choice of the best practicable environmental option bpeo and best available technique not entailing excessive cost batneec one of the most important questions in green logistics is how to identify preferred solutions balancing environmental and business concerns quariguasi et al improving environmental quality comes at a cost so the question is which trade offs occur between the environmental impacts of an economic activity and its costs and what are best solutions balancing ecological and economic concerns the aim is to determine solutions in which environmental damage can only be decreased if costs are increased these solutions are called eco efﬁcient the idea of exploring best alternatives is based on pareto optimality huppes and ishikawa despite the extensive literature on multi objective programming determining eco efﬁcient frontiers using multi objective mixed integer programming models is quite new conclusions operations research is most often associated with cost minimization yet there is a substantial impact on the environment although this is often not recognized environmental advocates typically plead for a technology change e g going from oil based fuel to electric cars however in all systems the way they are operated is an important determinant in the environmental performance for example container ships lowering their speed from to knots reduce their fuel use by accordingly or has and will bring important contributions to the environment but it is quite often implicit a fact which has become evident from this review it would be better if or emphasized its value for the environment new models will be required to address the multitude of decisions needed to improve the environment acknowledgements the authors are grateful to remy spliet and the referee for useful comments computers operations research 697 contents lists available at sciencedirect computers operations research journal homepage www elsevier com locate caor a modiﬁed artiﬁcial bee colony algorithm wei feng gao san yang liu department of mathematics xidian university xi an shannxi pr china a r t i c l e i n f o abstract available online june artiﬁcial bee colony algorithm abc is a relatively new optimization technique which has been shown to be competitive to other population based algorithms however there is still an insufﬁciency in abc regarding its solution search equation which is good at exploration but poor at exploitation inspired by differential evolution de we propose an improved solution search equation which is based on that the bee searches only around the best solution of the previous iteration to improve the exploitation then in order to make full use of and balance the exploration of the solution search equation of abc and the exploitation of the proposed solution search equation we introduce a selective probability p and get the new search mechanism in addition to enhance the global convergence when producing the initial population both chaotic systems and opposition based learning methods are employed the new search mechanism together with the proposed initialization makes up the modiﬁed abc mabc for short which excludes the probabilistic selection scheme and scout bee phase experiments are conducted on a set of benchmark functions the results demonstrate good performance of mabc in solving complex numerical optimization problems when compared with two abc based algorithms elsevier ltd all rights reserved keywords artiﬁcial bee colony algorithm initial population solution search equation differential evolution introduction learning from life system people have developed many optimization computation methods to solve complicated problems in recent decades such as genetic algorithm ga inspired by the darwinian law of survival of the ﬁttest particle swarm optimization pso inspired by the social behavior of bird ﬂocking or ﬁsh schooling ant colony optimization aco inspired by the foraging behavior of ant colonies and biogeography based optimization bbo inspired by the migration behavior of island species we call this kind of algorithms for scientiﬁc computation as artiﬁcial life computation artiﬁcial bee colony algorithm abc is such a new computation technique developed by karaboga based on simulating the foraging behavior of honey bee swarm numerical comparisons demonstrated that the performance of abc is competitive to other population based algorithms with an advantage of employing fewer control parameters due to its simplicity and ease of implementation abc has captured much attention and has been applied to solve many practical optimization problems since its invention in however similar to other evolutionary algorithms abc also faces up to some challenging problems for example the convergence speed of abc is typically slower than those of representative corresponding author e mail address com w f gao see front matter elsevier ltd all rights reserved doi j cor population based algorithms e g differential evolution de and pso when handling those unimodal problems what is more abc can easily get trapped in the local optima when solving complex multimodal problems the reasons are as follows it is well known that both exploration and exploitation are necessary for a populationbased optimization algorithm in the optimization algorithms the exploration refers to the ability to investigate the various unknown regions in the solution space to discover the global optimum while the exploitation refers to the ability to apply the knowledge of the previous good solutions to ﬁnd better solutions in practice the exploration and exploitation contradicts to each other in order to achieve good performances on problem optimizations the two abilities should be well balanced while the solution search equation of abc which is used to generate new candidate solutions based on the information of previous solutions is good at exploration but poor at exploitation which results in the above two insufﬁciencies therefore accelerating convergence speed and avoiding the local optima have become two most important and appealing goals in abc research a number of variant abc algorithms have hence been proposed to achieve these two goals however so far it is seen to be difﬁcult to simultaneously achieve both goals for example the chaotic abc algorithm cabc in focuses on avoiding the local optima but brings in a more extra function evaluations in chaotic search as a result to achieve the both goals inspired by de we propose an improved solution search equation which is based on that the bee searches only around the best solution of the previous iteration to improve the exploitation then in order to make full use of and w f gao s y liu computers operations research 697 balance the exploration of the solution search equation of abc and the exploitation of the proposed solution search equation we introduce a selective probability p and get the new search mechanism in addition to enhance the global convergence when producing the initial population both chaotic systems and opposition based learning method are employed the new search mechanism together with the proposed initialization makes up the modiﬁed abc mabc for short which excludes the probabilistic selection scheme and scout bee phase the rest of this paper is organized as follows section summarizes abc the improved abc algorithm is presented in section section presents and discusses the experimental results finally the conclusion is drawn in section of their sources with the onlooker bees on the dance area an onlooker bee evaluates the nectar information taken from all employed bees and chooses a food source site with a probability related to its nectar amount this probabilistic selection depends on the ﬁtness values of the solutions in the population a ﬁtnessbased selection scheme might be a roulette wheel ranking based stochastic universal sampling tournament selection or another selection scheme in basic abc roulette wheel selection scheme in which each slice is proportional in size to the ﬁtness value is employed as follows sn x pi fi fj where fi is the ﬁtness value of solution i obviously the higher the fi is the more probability that the ith food source is selected artiﬁcial bee colony algorithm artiﬁcial bee colony algorithm abc proposed by karaboga in for real parameter optimization is a recently introduced optimization algorithm which simulates the foraging behavior of a bee colony abc classiﬁes the foraging artiﬁcial bees into three groups namely employed bees onlooker bees and scout bees half of the colony consists of employed bees and the other half includes onlooker bees employed bees search the food around the food source in their memory meanwhile they pass their food information to onlooker bees onlooker bees tend to select good food sources from those founded by the employed bees then further search the foods around the selected food source scout bees are translated from a few employed bees which abandon their food sources and search new ones similar to the other population based algorithms abc is an iterative process the units of the basic abc can be explained as follows an onlooker bee evaluates the nectar information taken from all the employed bees and selects a food source xi depending on its probability value pi once the onlooker has selected her food source xi she produces a modiﬁcation on xi by using eq as in the case of the employed bees if the modiﬁed food source has a better or equal nectar amount than xi the modiﬁed food source will replace xi and become a new member in the population scout bee phase if a food source xi cannot be further improved through a predetermined number of trials limit the food source is assumed to be abandoned and the corresponding employed bee becomes a scout the scout produces a food source randomly as follows xi j xmin j j xmin j þ initialization of the population the initial population of solutions is ﬁlled with sn number of randomly generated n dimensional real valued vectors i e food sources let xi fxi xi xi n g represent the ith food source in the population and then each food source is generated as follows xi j xmin j þ j xmin j þ onlooker bee phase where i sn j n xmin j and xmax j are the lower and upper bounds for the dimension j respectively these food sources are randomly assigned to sn number of employed bees and their ﬁtnesses are evaluated where j n main steps of the artiﬁcial bee colony algorithm based on the above explanation of initializing the algorithm population employed bee phase probabilistic selection scheme onlooker bee phase and scout bee phase the pseudo code of the abc algorithm is given below algorithm artiﬁcial bee colony algorithm initialization of the bee phase at this stage each employed bee xi generates a new food source vi in the neighborhood of its present position by using solution search equation as follows vi j xi j þ fi j ðxi j xk j þ where k a sng and j a ng are randomly chosen indexes k has to be different from i fi j is a random number in the range once vi is obtained it will be evaluated and compared to xi if the ﬁtness of vi is equal to or better than that of xi vi will replace xi and become a new member of the population otherwise xi is retained in other words a greedy selection mechanism is employed between the old and candidate solutions calculating probability values involved in probabilistic selection after all employed bees complete their searches they share their information related to the nectar amounts and the positions initialize the population of solutions xi j i sn j n triali triali is the non improvement number of the solution xi used for abandonment evaluate the population cycle repeat produce a new food source population for employed bees for i to sn do produce a new food source vi for the employed bee of the food source xi using and evaluate its quality apply a greedy selection process between vi and xi and select the better one if solution xi does not improve triali triali otherwise triali end for calculate the probability values pi by for the solutions using ﬁtness values produce a new food source population for onlooker bees w f gao s y liu computers operations research 697 t i repeat if random opi then produce a new vi food source by for onlooker bee apply a greedy selection process between vi and xi and select the better one if solution xi does not improve triali triali þ otherwise triali t t endif until t determine scout if maxðtriali þ then replace xi with a new randomly produced solution by end if memorize the best solution achieved so far until cycle number modiﬁed artiﬁcial bee colony algorithm initial population population initialization is a crucial task in evolutionary algorithms because it can affect the convergence speed and the quality of the ﬁnal solution if no information about the solution is available then random initialization is the most commonly used method to generate candidate solutions initial population owing to the randomness and sensitivity dependence on the initial conditions of chaotic maps chaotic maps have been used to initialize the population so that the search space information can be extracted to increase the population diversity in at the same time according to replacing the random initialization with the oppositionbased population initialization can get better initial solutions and then accelerate convergence speed so this paper proposes a novel initialization approach which employs opposition based learning method and chaotic systems to generate initial population here sinusoidal iterator is selected and its equation is deﬁned as follows chk þ sinðpchk þ chk a k k where k is the iteration counter and k is the preset maximum number of chaotic iterations the mapped variables in eq can distribute in search space with ergodicity randomness and irregularity based on these operations we propose the following algorithm to generate initial population which can be used instead of a pure random initialization algorithm a novel initialization approach set the maximum number of chaotic iteration k the population size sn and the individual counter i j chaotic systems for to sn do for j to n do randomly initialize variables j a set iteration counter for to k do chk þ j sinðpchk j þ end for xi j xmin j þchk j ðxmax j xmin j þ end for end for opposition based learning method set the individual counter i j for to sn do for j to n do oxi j xmin j þxmax j xi j end for end for selecting sn ﬁttest individuals from set the fxðsnþ oxðsnþg as initial population a modiﬁed search equation differential evolution de has been shown to be a simple yet efﬁcient evolutionary algorithm for many optimization problems in real world applications it follows the general procedure of an evolutionary algorithm after initialization de enters a loop of evolutionary operations mutation crossover and selection there are several variant de algorithms which are different in that their mutation strategies are adopted differently the following is a mutation strategy frequently used in the literature de best vi xbest þ þ where i sng and and are mutually different random integer indices selected from sng f commonly known as scaling factor or ampliﬁcation factor is a positive real table effect of the selective probability p on the performance of mabc algorithm sphere rosebrock griewank rastrigin nc rastrigin ackley mabc p mean sd mabc p mean sd mabc p mean sd mabc p mean sd mabc p mean sd mabc p mean sd mabc p mean sd w f gao s y liu computers operations research 697 number typically less than that controls the rate at which the population evolves the best solutions in the current population are very useful sources that can be used to improve the convergence performance the example is the de best where the best solutions explored in the history are used to direct the movement of the current population based on the variant de algorithm and the property of abc the solution search equation is devised as follows in the population therefore the solution search dominated by eq is random enough for exploration in other words the solution search equation described by eq is good at exploration but poor at exploitation however according to eq abc best can drive the new candidate solution only around the best solution of the previous iteration therefore the proposed solution search equation described by eq can increase the exploitation of abc abc best vi j xbest j þ fi j j j þ the proposed approach where the indices and are mutually exclusive integers randomly chosen from sng and different from the base index i xbest is the best individual vector with the best ﬁtness in the current population and j a ng is randomly chosen indexes fi j is a random number in the range in eq the coefﬁcient fi j is a uniform random number in and xk j is a random individual from the above explanation it is clear that abc best has a good capacity of the exploitation unfortunately abc best can reduce the exploration of abc if all bees produce new food sources using the algorithm can easily get trapped in the local optima when solving complex multimodal problems in other words abc which is good at exploration but poor at table benchmark functions used in experiments function ðxþ pn pn ði ðn xi i þ p ðxþ p ðxþ jxi jði þ p q ðxþ jxi jþ jxi j ðxþ maxi fjxi j r ir ng p ðxþ ðbxi þ p ðxþ ixi p ðxþ ixi þ pn ðxþ i þ þ ðxi ðxþ ðxþ þ þ ðxþ cosð2pyi þ þ jxi jo xi yi þ jxi jz q pn x cos piﬃ þ ðxþ i i i pn pﬃﬃﬃﬃﬃﬃﬃ ðxþ i xi sinð jxi jþ rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pn pn exp þ þ þ e ðxþ exp n i n p ðxþ pn sin þ þ n i ðyi þ sin ðpyi þ þ pn þ ðyn gþ i uðxi m xi kðxi aþ a r xi r a yi þ ðxi þ uxi a k m kð x aþm x o a i ðxþ ðxþ fsin ð pn pn jxi sinðxi þ þ xi j i ðxi min n n n n n n n n n n n n n n n n n n n n n i p þþ n i ðxi þ sin þ þ pn þ ðxn sin þ þ g þ i uðxi ðxþ search range ½1þ sin þ þ þ sin þ þ jxn þ sin þ pd pkmax k pkmax k k i k ðxi þ þ d k ðxþ k a b kmax qp ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n i xi þ ðxþ þ ð p þ p ðxþ þ þ p i ðxþ ni¼ sinðxi þ n p n for n p ðxþ pn z x o ðxþ þ þ z x o qn pn zi ﬃ p ðxþ i zi i cosð iþ þ z x o rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pn pn ðxþ exp exp þ n i n z x o p ðxþ ni¼ jzi sinðzi þþ zi j z x o n n 600 n n n w f gao s y liu computers operations research 697 exploitation results in a slow convergence while abc best which is good at exploitation but poor at exploration cannot avoid premature convergence to address this contradiction we propose the new search mechanism which introduces the selective probability p to balance the exploration of the solution search equation and the exploitation of the modiﬁed solution search equation the new search mechanism with algorithm makes up mabc based on the above explanation the pseudo code of mabc is given below algorithm modiﬁed artiﬁcial bee colony algorithm set the population size sn give the maximum number of function evaluations max fe perform algorithm to create an initial population fxi ji sng calculate the function values of the population ffi ji sng while stopping criterion is not met namely fe o max fe do for i to sn do produce a new food source using the new search mechanism choose randomly from the current population the indices are mutually exclusive integers randomly chosen from the range ½1 sn which are also different from the index i randomly choose j from ng and produce fi j a generate a new food source vi according to vi j xbest j þ fi j j j þ if f ðvi þ of ðxi þ then xi vi else then if o p then randomly choose j from ng ka sng which has to be different from i and produce fi j a generate a new food source vi according to vi j xi j þ fi j ðxi j xk j þ if f ðvi þ of ðxi þ then xi vi endif end if end if end for end while fe max fe table best worst median mean and standard deviation values obtained by abc and mabc through independent runs on function from to fun dim f4 best worst median mean sd abc mabc abc mabc abc mabc abc mabc abc mabc abc mabc abc mabc abc mabc abc mabc abc mabc abc mabc abc mabc 48 59e 69 48 69 48 abc mabc abc mabc abc mabc 59e abc mabc abc mabc abc mabc 07eþ 65e þ01 18eþ þ01 02e þ01 22e þ þ þ þ abc mabc abc mabc abc mabc signiﬁcant þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ na na na w f gao s y liu computers operations research 697 adjusting the selective probability p set at for all test functions above all the proposed approach is able to reach the balance between exploration and exploitation note that the parameter p plays an important role in balancing the exploration and exploitation of the candidate solution search when p takes only eq is at work when p increases from to the exploration of eq will also increase correspondingly however p should not be too large because the large value of p might weaken the exploitation of the algorithm therefore the selective probability parameter p needs to be tuned in this section six different kinds of thirty dimensional d test functions are used to investigate the impact of this parameter they are the sphere rosenbrock griewank rastrigin nc rastrigin and ackley functions as deﬁned in section mabc runs times on each of these functions and the mean and standard deviation values of the ﬁnal results are presented in table as all test functions are minimization problems the smaller the ﬁnal result the better it is from table we can observe that p can inﬂuence the results when p is we obtain a faster convergence velocity and better results on the sphere and ackley functions for the other four test functions better results are obtained when p is around at the same time p has a smaller effect on the sphere and ackley functions than for the other four test functions hence in our experiments the selective probability p is experimental studies on function optimization problems benchmark functions and parameter settings in this section mabc is applied to minimize a set of scalable benchmark functions of dimensions d or and a set of two functions of higher dimension d or as shown in table summarized in table are the scalable benchmark functions and are continuous unimodal functions is a discontinuous step function and is a noisy quartic function is the rosenbrock function which is unimodal for d and but may have multiple minima in high dimension cases are multimodal and the number of their local minima increases exponentially with the problem dimension are shifted functions and o is a randomly generated shift vector located in search range in addition is the only bound constrained function investigated in this paper table best worst median mean and standard deviation values obtained by abc and mabc through independent runs on function from to fun dim f12 f14 best worst median mean sd abc mabc abc mabc abc mabc 69 41e 68 62 65e 28e 62 abc mabc abc mabc abc mabc 65e 67e 14e abc mabc abc mabc abc mabc 59e 23e abc mabc abc mabc abc mabc 28e abc mabc abc mabc abc mabc 00e 00e 02e 47e abc mabc abc mabc abc mabc 74e 99e 29e 99e abc mabc abc mabc abc mabc þ þ þ 23e 86e þ01 29e 62eþ 41eþ signiﬁcant þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ w f gao s y liu computers operations research 697 the set of experiments tested on numerical benchmark function are performed to compare the performance of mabc with that of abc in all simulations as the number of optimization parameters increases we set the maximum number of function evaluations to be 000 300 000 and 000 for each function the population size is namely respectively all results reported in this section are obtained based on independent runs experimental results the performance on the solution accuracy of abc is compared with that of mabc the results are shown in tables in terms of the best worst median mean and standard deviation of the solutions obtained in the independent runs by each algorithm fig graphically presents the comparison in terms of convergence characteristics of the evolutionary processes in solving the eight different problems an interesting result is that the two abc based algorithms have most reliably found the minimum of it is a region rather than a point in that is the optimum hence this problem may relatively be easy to solve with a success rate important observations about the convergence rate and reliability of different algorithms can be made from the results presented in fig and tables these results suggest that the convergence rate of mabc is better than abc on the most test functions in particular mabc can ﬁnd optimal solutions on functions 2f13 and with d mabc offers the higher accuracy on almost all the functions except functions with and with in the case of functions with and with simulation results show that the convergence rate of mabc is worse than abc while as the results obtained by mabc are of the same order of magnitude as the results by abc on these two functions the superiority of abc to mabc is not very obvious in terms of the best worst median mean and standard deviation of the solutions in a word the superiority in terms of search ability and efﬁciency of mabc should be attributed to an appropriate balance between exploration and exploitation in the columns of tables we report the statistical signiﬁcance level of the difference of the means of the two algorithms note that here þ indicates the t value is signiﬁcant at a level of signiﬁcance by two tailed test stands for the difference of means is not statistically signiﬁcant and na means not applicable covering cases for which the two table best worst median mean and standard deviation values obtained by abc and mabc through independent runs on function from to fun dim f17 best worst median mean sd abc mabc abc mabc abc mabc 64e 14e 99e 94e 79e 02e 12e 63e 06 29e abc mabc abc mabc abc mabc 64e 88e 12e 64e 34e abc mabc abc mabc abc mabc 85e 43e 47e 83e 95e 63e 12e 41e 85e 23e 81e 44e abc mabc abc mabc abc mabc 99e 74e 55e 13e 05e 54e 05e 13e 83e 69e abc mabc abc mabc abc mabc 34e 31 00e 31 02e 29 31 80e 56e 11e 34e 31 00e 50e 28 34e 48e 31 63e 49e 29 30e 32 29 abc mabc abc mabc abc 66e 62e 48e 26e 10e 26e 21e 46e 94e 69e 09e 00 þ abc mabc abc mabc abc mabc 996e 998e 998e 01 01 01 997e 01 01 81e 17e 90e 62e signiﬁcant þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ w f gao s y liu computers operations research 697 table best worst median mean and standard deviation values obtained by abc and mabc through independent runs on function to fun dim 300 300 f26 best worst median mean sd abc mabc abc mabc abc mabc 3323 3323 3323 3323 3323 3323 3323 3323 3323 3323 3323 3323 23e 01 93e 01 abc mabc abc mabc abc mabc 8149 3302 6145 8869 8617 7489 0683 2015 5962 6626 0177 4527 42e 01 01 01 01 23e 00 14e 01 abc mabc abc mabc abc mabc 66e 88e 31 63e 28 53e 13e 27e 28 11e 49e 29e 05e 21e 55e 25e 61e 29 63e 44e 54e 54e 29 39e 28 abc mabc abc mabc abc mabc 17e 53e 00 95e 01 01e 00 14e 00 35e 07 95e 01 06e 00 49e 01 00 55e 00 55e 01 64e 01 92e 00 abc mabc abc mabc abc mabc 45e 13e 13e 92e 36e 07 57e 88e 32e 84e 93e 28e 49e 25e 08 08 abc mabc abc mabc abc mabc 81e 35e 21e 60e 20e 13 82e 77e 83e 53e 13 13 82e 35e 75e 10e 13 11e 26e 13 92e 11e 00e 13 24e 13 69e 31e 07e 90e 58e abc mabc abc mabc abc mabc 63e 24e 10e 46e 60e 1 28e 1 1 12e 1 39e 63e 56e 87e 04e 1 28e 89e 1 81e 03 64e 01e 15 22e 04 11e 53e 03 00e 1 39e 15 signiﬁcant þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ þ table comparison between mabc and abc on optimizing six benchmark functions function dim gabc mabc max fe mean sd max fe mean sd schaffer 81e 01 77e 01 12e 02 04e 03 56e 01 68e 01 65e 02 03 rosenbrock 93e 01 1 90e 00 1 36e 00 1 97e 00 1 01 32e 01 1 61e 01 11e 01 sphere 60 17e 1 43e 15 36e 1 1 9 43e 32 03e 29 67e 32 31e 29 griewank 60 17 54e 99e 17 12e 1 5eþ rastrigin 60 0e þ 1 32e 3 13 44e 1 24e 13 1 5eþ 05 3 05 ackley 60 0eþ 05 0eþ 05 3 21e 1 00e 13 3 25e 15 15 0eþ 05 0eþ 05 98e 73e 26e 15 15 algorithms achieve the same accuracy results it also clearly indicates that the proposed mabc is superior to abc on almost all the functions in table mabc is further compared with gbest guided artiﬁcial bee colony algorithm gabc based on its results reported in the literature mabc follows the parameter w f gao s y liu computers operations research 697 3 effects of each modiﬁcation on the performance of mabc settings in the original paper of gabc it is clear that mabc works better in all cases and achieves better performance than gabc summarizing the earlier statements the ability of mabc is that it can prevent bees from falling into the local minimum reduce evolution process signiﬁcantly and more efﬁciently converges faster compute with more efﬁciency and improve bees searching abilities for abc fitness fe 15 16 x function with d 6 fe 14 16 x function with d 60 fitness fitness 1010 3 15 abc mabc abc mabc 6 fe 14 16 x 1 1 3 3 x fe function with d 60 f12 function with d 60 fitness fitness 15 abc mabc f20 function with d abc mabc fitness in order to analyze the modiﬁcations respectively we call the basic abc with the proposed initialization as and the random initialization with the proposed search mechanism i e mabc without the proposed initialization as we compare the convergence performance of the different abcs on the four test functions to see that how much the initialization and function with d abc mabc 15 1 1 5 3 15 3 5 x fe abc mabc 5 1 1 5 2 5 3 3 5 x fe function with d 5 function with d 5 fitness fitness 5 15 15 abc mabc 1 2 abc mabc 25 3 fe 5 6 x 105 1 2 3 fe fig 1 convergence performance of the different abcs on the eight test functions 5 6 x 105 696 w f gao s y liu computers operations research 697 function with d 105 5 5 fitness fitness 15 25 30 abc mabc 2 15 abc mabc 25 6 fe 30 14 16 x function with d 30 105 function with d 30 105 2 6 fe 14 16 x function with d 30 100 10 2 fitness fitness 10 5 abc mabc 10 10 10 15 2 6 10 fe 14 16 x 10 10 6 10 abc mabc 10 10 10 10 14 2 6 10 fe 14 16 x fig 2 convergence performance of the different abcs on the four test functions the search mechanism make contribution to improving the performance of the algorithm respectively the results are presented in fig 2 it can be observed that and are superior to abc which implies that both the initialization and the search mechanism have positive effect on the performance of the algorithm especially greatly outperforms abc on the other hand the performance comparisons of and mabc are not so apparent as those of and mabc which means that the search mechanism plays a pivotal role in the proposed algorithm however though the contribution of the initialization is far less than the search mechanism the comparisons of mabc and abc and show the initialization is at work 5 conclusion in this paper we have developed a novel optimization algorithm called mabc through introducing the modiﬁed solution search equation to abc and proposing a new framework without probabilistic selection scheme and scout bee phase in addition the initial population is generated by combining chaotic systems with opposition based learning method to enhance the global convergence the experimental results tested on 28 benchmark functions show that mabc outperforms abc and mabc as a consequence mabc may be a promising and viable tool to deal with complex numerical optimization problems it is desirable to further apply mabc to solving those more complex real world continuous optimization problems such as clustering data mining design and optimization of communication networks the future work includes the studies on how to extend mabc to handle those combinatorial optimization problems such as ﬂow shop scheduling problem vehicle routing problem and traveling salesman problem acknowledgments this work is supported by national nature science foundation of china no fundamental research funds for the central universities no no and foundation of state key lab of integrated services networks of china european journal of operational research 1 contents lists available at sciverse sciencedirect european journal of operational research journal homepage www elsevier com locate ejor invited review a review of dynamic vehicle routing problems victor pillac a b michel gendreau c d christelle guéret a andrés l medaglia b a lunam université école des mines de nantes irccyn umr nantes france centro para la optimización y probabilidad aplicada copa ceiba departamento de ingenieria industrial universidad de los andes bogotá colombia c département de mathématiques et de génie industriel école polytechnique de montréal c p succ centre ville montréal canada d centre interuniversitaire de recherche sur les reseaux d entreprise la logistique et le transport cirrelt canada b a r t i c l e i n f o article history received september accepted august available online 30 august keywords transportation combinatorial optimization dynamic vehicle routing a b t r a c t a number of technological advances have led to a renewed interest in dynamic vehicle routing problems this survey classiﬁes routing problems from the perspective of information quality and evolution after presenting a general description of dynamic routing we introduce the notion of degree of dynamism and present a comprehensive review of applications and solution methods for dynamic vehicle routing problems ó elsevier b v all rights reserved 1 introduction the vehicle routing problem vrp formulation was ﬁrst introduced by dantzig and ramser 35 as a generalization of the traveling salesman problem tsp presented by flood 49 the vrp is generally deﬁned on a graph g ðv e cþ where v fv v n g is the set of vertices e fðv i v j þjðv i v j þ 2 v 2 i jg is the arc set and c ðcij þðv i v j is a cost matrix deﬁned over e representing distances travel times or travel costs traditionally vertex is called the depot while the remaining vertices in v represent customers or requests that need to be served the vrp consists in ﬁnding a set of routes for k identical vehicles based at the depot such that each of the vertices is visited exactly once while minimizing the overall routing cost beyond this classical formulation a number of variants have been studied among the most common are the capacitated vrp cvrp where each customer has a demand for a good and vehicles have ﬁnite capacity the vrp with time windows vrptw where each customer must be visited during a speciﬁc time frame the vrp with pick up and delivery pdp where goods have to be picked up and delivered in speciﬁc amounts at the vertices and the heterogeneous ﬂeet vrp hvrp where vehicles have different capacities routing problems that involve moving people between locations are referred to as dial a ride problem darp for land transport or dial a flight problem dafp for air transport corresponding author e mail addresses vpillac mines nantes fr v pillac michel gendreau cirrelt ca m gendreau gueret mines nantes fr c guéret amedagli uniandes edu co a l medaglia see front matter ó elsevier b v all rights reserved http dx doi org 10 1016 j ejor 08 in contrast to the classical deﬁnition of the vehicle routing problem real world applications often include two important dimensions evolution and quality of information evolution of information relates to the fact that in some problems the information available to the planner may change during the execution of the routes for example with the arrival of new customer requests quality of information reﬂects possible uncertainty on the available data for instance when the demand of a customer is only known as a range estimate of its real demand in addition depending on the problem and the available technology vehicle routes can either be designed statically a priori or dynamically for instance the vrp with stochastic demands vrpsd can be seen from both perspectives from a static perspective the problem is to design a set of robust routes a priori that will undergo minor changes during their execution 16 from a dynamic perspective the problem consists in designing the vehicle routes in an online fashion communicating to the vehicle which customer to serve next as soon as it becomes idle based on these dimensions table 1 identiﬁes four categories of routing problems in static and deterministic problems all input is known beforehand and vehicle routes do not change once they are in execution this classical problem has been extensively studied in the literature and we refer the interested reader to the recent reviews of exact and approximate methods by baldacci et al cordeau et al laporte and toth and vigo static and stochastic problems are characterized by input partially known as random variables which realizations are only revealed during the execution of the routes additionally it is assumed that routes are designed a priori and only minor changes are allowed afterwards for instance allowable changes include 2 v pillac et al european journal of operational research 1 table 1 taxonomy of vehicle routing problems by information evolution and quality information quality information evolution input known beforehand input changes over time planning a trip back to the depot or skipping a customer applications in this category do not require any technological support uncertainty may affect any of the input data yet the three most studied cases are stochastic customers where a customer needs to be serviced with a given probability 15 147 stochastic times in which either service or travel times are modeled by random variables and lastly stochastic demands 31 37 126 further details on the static stochastic vehicle routing can be found in the reviews by bertsimas and simchi levi 33 cordeau et al 16 and gendreau et al in dynamic and deterministic problems part or all of the input is unknown and revealed dynamically during the design or execution of the routes for these problems vehicle routes are redeﬁned in an ongoing fashion requiring technological support for real time communication between the vehicles and the decision maker e g mobile phones and global positioning systems this class of problems are also referred to as online or real time by some authors similarly dynamic and stochastic problems have part or all of their input unknown and revealed dynamically during the execution of the routes but in contrast with the latter category exploitable stochastic knowledge is available on the dynamically revealed information as before the vehicle routes can be redeﬁned in an ongoing fashion with the help of technological support besides dynamic routing problems where customer visits must be explicitly sequenced along the routes there are other related vehicle dispatching problems such as managing a ﬂeet of emergency vehicles or the so called dynamic allocation problems in the area of long haul truckload trucking 60 109 in this paper we focus solely on dynamic problems with an explicit routing dimension the remainder of this document is organized as follows section 2 presents a general description of dynamic routing problems and introduces the notion of degree of dynamism section 3 reviews different applications in which dynamic routing problems arise while section provides a comprehensive survey of solution approaches finally section 5 concludes this paper and gives directions for further research deterministic input stochastic input static and deterministic dynamic and deterministic static and stochastic dynamic and stochastic able to track and manage their ﬂeet in real time and cost effectively while traditionally a two step process i e plan execute vehicle routing can now be done dynamically introducing greater opportunities to reduce operational costs improve customer service and reduce environmental impact the most common source of dynamism in vehicle routing is the online arrival of customer requests during the operation more speciﬁcally requests can be a demand for goods 2 62 71 99 or services 17 travel time a dynamic component of most real world applications has been recently taken into account 1 6 28 48 64 65 139 while service time has not been explicitly studied but can be added to travel time finally some recent work considers dynamically revealed demands for a set of known customers 105 126 and vehicle availability 93 in which case the source of dynamism is the possible breakdown of vehicles in the following we use the preﬁx d to label problems in which new requests appear dynamically to better understand what we mean by dynamic fig 1 illustrates the route execution of a single vehicle d vrp before the vehicle leaves the depot time an initial route plans to visit the currently known requests a b c d e while the vehicle executes its route two new requests x and y appear at time and the initial route is adjusted to fulﬁll them finally at time tf the executed route is a b c d y e x this example reveals how dynamic routing inherently adjusts routes in an ongoing fashion which requires real time communication between vehicles and the dispatching center fig 2 illustrates this real time communication scheme where the environment refers to the real world while the dispatcher is the agent that gives instructions to the vehicle once the vehicle is ready ﬁrst dotted arrow the dispatcher makes a decision and instructs the vehicle to fulﬁll request a ﬁrst double headed arrow when the vehicle starts second dotted arrow and ends third dotted arrow service at request a it notiﬁes the dispatcher which in turn updates the available information and communicates the vehicle its next request second double headed arrow 2 2 differences with static routing 2 dynamic vehicle routing problems 2 1 a general deﬁnition the ﬁrst reference to a dynamic vehicle routing problem is due to wilson and colvin they studied a single vehicle darp in which customer requests are trips from an origin to a destination that appear dynamically their approach uses insertion heuristics able to perform well with low computational effort later psaraftis introduced the concept of immediate request a customer requesting service always wants to be serviced as early as possible requiring immediate replanning of the current vehicle route a number of technological advances have led to the multiplication of real time routing applications with the introduction of the global positioning system gps in the development and widespread use of mobile and smart phones combined with accurate geographic information systems giss companies are now in contrast to their static counterparts dynamic routing problems involve new elements that increase the complexity of their decisions more degrees of freedom and introduce new challenges while judging the merit of a given route plan in some contexts such as the pick up of express courier the transport company may deny a customer request as a consequence it can reject a request either because it is simply impossible to service it or because the cost of serving it is too high this process of acceptance denial has been used in many approaches 2 and is referred to as service guarantee in dynamic routing the ability to redirect a moving vehicle to a new request nearby allows for additional savings nevertheless it requires real time knowledge of the vehicle position and being able to communicate quickly with drivers to assign them new destinations thus this strategy has received limited interest with the main contributions being the early work by regan et al 122 v pillac et al european journal of operational research 1 3 fig 1 example of dynamic vehicle routing horizon r the set of requests and ti the disclosure time of request i 2 r assuming that requests known beforehand have a disclosure time equal to de can be expressed as de fig 2 timeline of events for the dynamic routing of a single vehicle the study of diversion issues by ichoua et al and the work by branchini et al dynamic routing also frequently differs in the objective function in particular while a common objective in the static context is the minimization of the routing cost dynamic routing may introduce other notions such as service level throughput number of serviced requests or revenue maximization having to answer to dynamic customer requests also introduces the notion of response time a customer might request to be serviced as soon as possible in which case the main objective may become to minimize the delay between the arrival of a request and its service dynamic routing problems require making decisions in an online manner which often compromises reactiveness with decision quality in other words the time invested searching for better decisions comes at the price of a lower reactiveness to input changes this aspect is of particular importance in contexts where customers call for a service and a good decision must be made as fast as possible 2 3 measuring dynamism different problems or instances of a same problem can have different levels of dynamism which can be characterized according to two dimensions the frequency of changes and the urgency of requests the former is the rate at which new information becomes available while the latter is the time gap between the disclosure of a new request and its expected service time from this observation three metrics have been proposed to measure the dynamism of a problem or instance lund et al deﬁned the degree of dynamism d as the ratio between the number of dynamic requests nd and the total number of requests ntot as follows nd ntot based on the fact that the disclosure time of requests is also important larsen proposed the effective degree of dynamism de this metric can be interpreted as the normalized average of the disclosure times let t be the length of the planning 1 x ti ntot t larsen also extended the effective degree of dynamism to problems with time windows to reﬂect the level of urgency of requests he deﬁnes the reaction time as the difference between the disclosure time ti and the end of the corresponding time window li highlighting that longer reaction times mean more ﬂexibility to insert the request into the current routes thus the effective degree of dynamism measure is extended as follows detw 1 x li t i 1 ntot t it is worth noting that these three metrics only take values in the interval 1 and all increase with the level of dynamism of a problem larsen et al use the effective degree of dynamism to deﬁne a framework classifying d vrps among weakly moderately and strongly dynamic problems with values of de being respectively lower than 3 comprised between 3 and and higher than although the effective degree of dynamism and its variations have proven to capture well the time related aspects of dynamism it could be argued that they do not take into account other possible sources of dynamism in particular the geographical distribution of requests or the traveling times between requests are also of great importance in applications aiming at the minimization of response time although not considered the frequency of updates in problem information has a dramatical impact on the time available for optimization 3 a review of applications recent advances in technology have allowed the emergence of a wide new range of applications for vehicle routing in particular the last decade has seen the development of intelligent transport systems itss which are based on a combination of geolocation technologies with precise geographic information systems and increasingly efﬁcient hardware and software for data processing and operations planning we refer the interested reader to the study by crainic et al for more details on its and the contributions of operations research to this relatively new domain among itss the advanced fleet management systems afmss are speciﬁcally designed for managing a corporate vehicle ﬂeet the core problem is generally to deliver pick up goods or persons to from locations distributed in a given area while customer requests can either be known in advance or appear dynamically during the day vehicles are dispatched and routed in real time potentially by taking into account changing trafﬁc conditions 4 v pillac et al european journal of operational research 1 uncertain demands or varying service times a key technological feature of afmss is the optimization component traditionally vehicle routing relies on teams of human dispatchers meaning a critical operational process is bound to the competence and experience of dispatchers as well as the management costs that are directly linked to the size of the ﬂeet 1 advances in computer science have allowed a technological transfer from operational research to afmss as presented in the studies by attanasio et al 1 du et al godfrey and powell 60 powell and topaloglu roy simao et al and slater the remainder of this section presents applications where dynamic routing has been or can be implemented the interested reader is also referred to the work by gendreau and potvin and ichoua et al for complementary reviews 3 1 services in this category of applications a service request is deﬁned by a customer location and a possible time window while vehicle routes just fulﬁll service requests without considering side constraints such as capacity perhaps the simplest yet most illustrative case in this category is the dynamic traveling salesman problem a common application of dynamic routing can be found in the area of maintenance operations maintenance companies are often committed to their customers by means of a contract which speciﬁes periodical or planned visits to perform preventive maintenance and may also request corrective maintenance on short notice therefore each technician is ﬁrst given a route with known requests at the beginning of the day while new urgent requests are inserted dynamically throughout the day an interesting feature of this problem is the possible mix of skills tools and spare part requirements which have to be matched in order to service the request this problem has been studied by borenstein et al with an application to british telecom another application of dynamic routing arises in the context of the french non proﬁt organization sos médecins this organization operates with a crew of physicians who are called on duty via a call center coordinated with other emergency services when a patient calls the severity of the case is evaluated and a visit by a practitioner is planned accordingly as in other emergency services having an efﬁcient dispatching system reduces the response time thus improving service level for the society on the other hand it is important to decide in real time whether or not to send a physician so that it is possible to ensure a proper service level in areas where emergencies are likely to appear dynamic aspects can also appear on arc routing problems this is for instance the case in the study by tagmouti et al on the operation of a ﬂeet of vehicles for winter gritting applications their work consider a network of streets or road segments that need to be gritted when affected by a moving storm depending on the movements of the storm new segments may have to be gritted and the routing of vehicles has to be updated accordingly 3 2 transport of goods due to the fact that urban areas are often characterized by highly variable traveling times transport of goods in such areas has led to the deﬁnition of a speciﬁc category of applications known as city logistics city logistics can be deﬁned as an integrated vision of transport activities in urban areas taking into account factors such as trafﬁc and competition or cooperation between transport companies barcelo et al 6 developed a general framework for city logistics applications they describe the different modules ranging from modeling the city road network and acquiring real time trafﬁc data to the dynamic routing of a ﬂeet of vehicles zeimpekis et al proposed a decision support system dss for city logistics which takes into account dynamic travel and service times a typical application in city logistics is the courier service present in most urban areas couriers are dispatched to customer locations to collect packages and either deliver them to their destination short haul or to a unique depot long haul depending on the level of service paid by the customer couriers may consolidate pick ups from various customers or provide an expedited service companies offering courier services often have an heterogeneous ﬂeet composed of bicycles motorbikes cars and small vans the problem is then to dynamically route couriers taking into account not only the known requests their type pick up and delivery locations and time windows but also considering trafﬁc conditions and varying travel times a case study by attanasio et al 1 outlines the beneﬁts of using an optimization enabled afms at ecourier ltd a london based company offering courier services the authors illustrate that aside from the improvements in service quality response time and courier efﬁciency the use of an automated system allows decoupling the ﬂeet size from the need for more dispatchers further results motivated by a similar application can be found in gendreau et al and ghiani et al the delivery of newspapers and magazines is a domain in which customer satisfaction is of ﬁrst importance when a magazine or newspaper is not delivered a subscriber contacts a call center and is offered to choose between a voucher or a future delivery in the latter case the request is then forwarded to the delivery company which assigns it to a driver that will do a priority delivery traditionally this process relies on an exchange of phone calls faxes and printed documents that ultimately communicates the pending delivery to the driver once he she comes back to the depot as an alternative bieding et al propose a centralized application that makes use of mobile phones to communicate with drivers and intelligently perform the routing in real time reducing costs and improving customer satisfaction more recently ferrucci et al developed an approach that makes use historical data to anticipate future requests another application in which customer requests need to be answered with short delays can be found in companies with a direct service model such as grocery delivery services in general the customer selects products on a website and then chooses a time frame for the delivery at his home traditionally the vendor deﬁnes an arbitrary number of customers that can be serviced within a time window and the time window is made unavailable to customers as soon as the capacity is reached campbell and savelsbergh 24 deﬁned the home delivery problem in which the goal is to maximize the total expected revenue by dynamically deciding whether or not to accept a customer request within a speciﬁc time window in comparison with the traditional approach this means that the time windows available for a customer are dynamically deﬁned taking into consideration the possible future requests the authors propose a greedy randomized adaptive search procedure grasp and compare different cost functions to capture the problem uncertainty later azi et al 3 proposed an adaptive large neighborhood search alns that takes into account uncertainty by generating scenarios containing possible demand realizations apart from classical routing problems related operational problems also arise in many organizations the review by stahlbock and voss on operations research applications in container terminals describes the dynamic stacker crane problem 5 14 which considers the routing of container carriers loading and unloading ships in a terminal other applications include transport of goods inside warehouses factories and hospitals where v pillac et al european journal of operational research 1 5 documents or expensive medical instruments must be transferred efﬁciently between services larsen et al and zeimpekis et al to complement our review 3 3 transport of persons 4 1 dynamic and deterministic routing problems the transport of persons is in general and by many aspects similar to the transport of goods yet it is characterized by additional constraints such as regulation on waiting travel and service times taxis are arguably the most common on demand individual transport systems requests are composed of a pick up location and time possibly coupled with a destination they can be either known in advance for instance when a customer books a cab for the next day or they can arrive dynamically in which case a taxi must be dispatched in the shortest time when customers cannot share a vehicle the closest free taxi is generally the one which takes the ride leaving limited space for optimization the study by caramia et al 25 generalized by fabri and recht focuses on a multi cab metropolitan transportation system where a taxi can transport more than one passenger at the same time in this case the online algorithms minimize the total traveled distance while assigning requests to vehicles and computing the taxi routes this multi cab transportation system can be generalized as an ondemand or door to door transport service many applications involve the transport of children the elderly disabled people or patients from their home to schools place of work or medical centers xiang et al studied a darp with changing travel speeds vehicle breakdowns and trafﬁc congestion while dial followed by horn 67 69 studied demandresponsive transport systems an extensive review of this class of problems can be found in the studies by cordeau et al 32 and berbeglia et al 14 a singular application of on demand transportation systems can be found in major hospitals with services possibly spread across various buildings on several branches depending on the medical procedure or facility capacity a patient may need to be transferred on short notice from one service to another possibly requiring trained staff or speciﬁc equipment for his her care this application has been studied by beaudry et al kergosien et al and melachrinoudis et al air taxis developed as a ﬂexible response to the limitations of traditional airlines air taxis offer passengers the opportunity to travel through smaller airports avoiding waiting lines at check in and security checks air taxi companies offer an on demand service customers book a ﬂight a few days in advance specifying whether they are willing to share the aircraft stop at an intermediate airport or have ﬂexible traveling hours then the company accommodates these requests trying to consolidate ﬂights whenever possible the underlying optimization problems have not been subject to much attention except in the studies by cordeau et al 32 espinoza et al fagerholt et al 43 and yao et al similar problems arises in helicopter transportation systems typically used by oil and gas companies to transport personnel between offshore petroleum platforms this section presents approaches that have been successfully applied to dynamic routing in the absence of stochastic information in this context critical information is revealed over time meaning that the complete instance is only known at the end of the planning horizon as a consequence exact methods only provide an optimal solution for the current state but do not guarantee that the solution will remain optimal once new data becomes available therefore most dynamic approaches rely on heuristics that quickly compute a solution to the current state of the problem approaches for dynamic and deterministic vehicle routing problems can be divided into two categories those based on periodic reoptimization and those based on continuous reoptimization 4 solution methods few research was conducted on dynamic routing between the work of psaraftis in and the late however the last decade has seen a renewed interest for this class of problems with solution techniques ranging from linear programming to metaheuristics this section presents the major contributions in this ﬁeld and the reader is referred to the reviews books and special issues by gendreau and potvin ghiani et al goel ichoua ichoua et al jaillet and wagner 4 1 1 periodic reoptimization to the best of our knowledge the ﬁrst periodic reoptimization approach is due to psaraftis with the development of a dynamic programming approach his research focuses on the darp and consists in ﬁnding the optimal route each time a new request is known the main drawback of dynamic programming is the well known curse of dimensionality chap 1 which prevents its application to large instances more generally periodic reoptimization approaches start at the beginning of the day with a ﬁrst optimization that produces an initial set of routes then an optimization procedure periodically solves a static problem corresponding to the current state either whenever the available data changes or at ﬁxed intervals of time referred to as decision epochs 29 or time slices the advantage of periodic reoptimization is that it can be based on algorithms developed for static routing for which extensive research has been carried out the main drawback is that all the optimization needs to be performed before updating the routing plan thus increasing delays for the dispatcher yang et al addressed the real time truckload pdp in which a ﬂeet of trucks has to service point to point transport requests arriving dynamically important assumptions are that all trucks can only handle one request at a time with no possible preemption and they travel at the same constant speed the authors propose myopt a rolling horizon approach based on a linear program lp that is solved whenever a new request arrives along the same line of linear programming chen and xu 29 designed a dynamic column generation algorithm dycol for the d vrptw the authors propose the concept of decision epochs over the planning horizon which are the dates when the optimization process runs the novelty of their approach relies on dynamically generating columns for a set partitioning model using columns from the previous decision epoch the authors compared dycol to a traditional column generation with no time limit col computational results based on the solomon benchmark demonstrate that dycol yields comparable results in terms of objective function but with running times limited to 10 seconds opposed to the various hours consumed by col montemanni et al developed an ant colony system acs to solve the d vrp similar to kilby et al their approach uses time slices that is they divide the day in periods of equal duration a request arriving during a time slice is not handled until the end of the time bucket thus the problem solved during a time slice only considers the requests known at its beginning hence the optimization is run statically and independently during each time slice the main advantage of this time partition is that similar computational effort is allowed for each time slice this discretization is also possible by the nature of the requests which are never 6 v pillac et al european journal of operational research 1 urgent and can be postponed an interesting feature of their approach is the use of the pheromone trace to transfer characteristics of a good solution to the next time slice a similar approach was also used by gambardella et al 50 and rizzoli et al 4 1 2 continuous reoptimization continuous reoptimization approaches perform the optimization throughout the day and maintain information on good solutions in an adaptive memory whenever the available data changes a decision procedure aggregates the information from the memory to update the current routing the advantage is that the computational capacity is maximized possibly at the expense of a more complex implementation it is worth noting that because the current routing is subject to change at any time vehicles do not know their next destination until they ﬁnish the service of a request to the best of our knowledge the ﬁrst continuous reoptimization approach is due to gendreau et al with the adaptation of the parallel tabu search ts framework introduced by taillard et al to a d vrptw problem arising in the local operation of long distance express courier services their approach maintains a pool of good routes the adaptive memory which is used to generate initial solutions for a parallel ts the parallelized search is done by partitioning the routes of the current solution and optimizing them in independent threads whenever a new customer request arrives it is checked against all the solutions from the adaptive memory to decide whether it should be accepted or rejected this framework was also implemented for the d vrp 74 while other variations of ts have been applied to the dpdp 6 and the darp 2 bent and van hentenryck 9 introduced the multiple plan approach mpa as a generalization of the ts with adaptive memory the general idea is to populate and maintain a solution pool the routing plans that are used to generate a distinguished solution whenever a new request arrives a procedure is called to check whether it can be serviced or not if it can be serviced then the request is inserted in the solution pool and incompatible solutions are discarded pool updates are performed periodically or whenever a vehicle ﬁnishes servicing a customer this pool update phase is crucial and ensures that all solutions are coherent with the current state of vehicles and customers the pool can be seen as an adaptive memory that maintains a set of alternative solutions in an early work benyahia and potvin 13 studied the d pdp and proposed a genetic algorithm ga that models the decision process of a human dispatcher more recently gas were also used for the same problem 30 65 and for the d vrp genetic algorithms in dynamic contexts are very similar to those designed for static problems although they generally run throughout the planning horizon and solutions are constantly adapting to the changes made to the input 4 2 dynamic and stochastic routing problems dynamic and stochastic routing problems can be seen as an extension of their deterministic counterparts where additional stochastic knowledge is available in the dynamically revealed input approaches for this class of problems can be divided in two categories those based on sampling and those based on stochastic modeling as their name suggests sampling strategies incorporate stochastic knowledge by generating scenarios based on realizations drawn from the random variable distributions each scenario is then optimized by solving the static and deterministic problem they deﬁne on the other hand approaches based on stochastic modeling integrate stochastic knowledge analytically the advantage of sampling is its relative simplicity and ﬂexibility on distributional assumptions while its drawback is the massive generation of scenarios to accurately reﬂect reality alternatively stochastic modeling strategies formally capture the stochastic nature of the problem but they are highly technical in their formulation and require to efﬁciently compute possibly complex expected values examples of these two strategies follow 4 2 1 stochastic modeling powell et al formulated a truckload pdp as a markov decision process mdp later mdps were used by thomas and white and thomas to solve a vrp in which known customers may ask for service with a known probability kim et al also used mdps to tackle the vrp with dynamic travel times unfortunately the curse of dimensionality and the simplifying assumptions make this approach unsuitable in most real world applications nonetheless it allowed new insights in the ﬁeld of dynamic programming to cope with the scalability problems of traditional dynamic programming approximate dynamic programming adp steps forward in time approximates the value function and ultimately avoids the evaluation of all possible states we refer the interested reader to powell 111 for a more detailed description of the adp framework adp has been successfully applied to freight transport and ﬂeet management problems 60 115 in particular novoa and storer propose an adp algorithm to dynamically solve the vrpsd linear programming has also been adapted to the dynamic and stochastic context the optun approach proposed by yang et al as an extension of myopt see section 4 1 1 considers opportunity costs on each arc to reﬂect the expected cost of traveling to isolated areas consequently the optimization tends to reject isolated requests and avoids traversing arcs that are far away from potential requests later yang et al studied the emergency vehicle dispatching and routing and proposed a mathematical formulation that was later used by haghani and yang on a similar problem 4 2 2 sampling sampling approaches rely on the generation of scenarios containing possible realizations of the random variables fig 3 illustrates how scenarios are generated for the d vrp solely based on the current customers fig the optimal tour would be a b e d c which ignores two zones gray areas where customers are likely to appear by sampling the customer spatial distributions fig customers x y and z are generated and the new optimal tour is c x y b a z e d removing the sampled potential customers fig leads to the tour c b a e d which is suboptimal regarding a myopic cost evaluation but leaves room to accommodate new customers at a lower cost the multiple scenario approach msa is a predictive adaptation of the mpa framework discussed in section 4 1 2 the idea behind msa is to take advantage of the time between decisions to continuously improve the current scenario pool during the initialization the algorithm generates a ﬁrst set of scenarios based on the requests known beforehand throughout the day scenarios are then reoptimized and new ones are generated and added to the pool when a decision is required the scenario optimization procedure is suspended and msa uses the scenario pool to select the request to service next msa then discards the scenarios that are incompatible with the current routing and resumes the optimization computational experiments on instances adapted from the solomon benchmark showed that msa outperforms mpa both in terms of serviced customers and traveled distances especially for instances with high degrees of dynamism 9 flatberg et al adapted the spider commercial solver to use multiple scenarios and a consensus algorithm to tackle the d vrp while pillac et al implemented an event driven optimization v pillac et al european journal of operational research 1 11 fig 3 scenario generation in sampling approaches framework based on msa and showed signiﬁcant improvements over state of the art algorithms for the d vrpsd an important component of scenario based approaches such as msa is the decision process which deﬁnes how the information from the scenario pool is used to reach upon a decision regarding the next customer to visit the most common algorithms used to reach a decision in msa are consensus expectation and regret the consensus algorithm 9 10 selects the customer appearing ﬁrst with the highest frequency among scenarios expectation 8 10 consists in evaluating the cost of visiting each customer ﬁrst by forcing its visit in all scenarios and performing a complete optimization finally regret 8 approximates the expectation algorithm and avoids the reoptimization of all scenarios even though these algorithms were initially designed for the routing of a single vehicle they can be extended to the multi vehicle case hvattum et al developed the dynamic sample scenario hedge heuristic dshh an approach similar to the consensus algorithm for d vrp this method divides the planning horizon into time intervals at the beginning of each interval dshh revises the routing by assigning a subset of promising requests to the vehicles depending on the frequency of their assignment over all scenarios dshh later led to the development of the branch and regret heuristic brh where scenarios are merged to build a unique solution various local search approaches have been developed for the stochastic and dynamic problems ghiani et al developed an algorithm for the d pdp that only samples the near future to reduce the computational effort the main difference with msa is that no scenario pool is used and the selection of the distinguished solution is based on the expected penalty of accommodating requests in the near future azi et al 3 developed an adaptive large neighborhood search alns for a dynamic routing problem with multiple delivery routes in which the dynamic decision is the acceptance of a new request the approach maintains a pool of scenarios optimized by an alns that are used to evaluate the opportunity value of an incoming request tabu search has also been adapted to dynamic and stochastic problems ichoua et al and attanasio et al 1 tackled with tabu search the d vrptw and the d pdp respectively 4 2 3 other strategies in addition to the general frameworks described previously the use of stochastic knowledge allows for the design and implementation of other strategies that try to adequately respond to upcoming events the waiting strategy consists in deciding whether a vehicle should wait after servicing a request before heading toward the next customer or planning a waiting period on a strategic location this strategy is particularly important in problems with time windows where time lags appear between requests mitrovic minic 100 proved that in all cases it is better to wait after servicing a customer but a more reﬁned strategy can lead to further improvements the problem is in general to evaluate the likelihood of a new request in the neighborhood of a serviced request and to plan a waiting period accordingly the waiting strategy has been implemented in various frameworks for the d vrp dvrptw 21 d pdp 100 and dynamic and stochastic tsp the strategy has shown good results especially in the case of a limited ﬂeet facing a high request rate aside from the waiting after or before servicing a customer a vehicle can be relocated to a strategic position where new requests are likely to appear this strategy is the keystone of emergency ﬂeet deployment also known as emergency vehicle dispatching or redeployment problem 66 the relocation strategy has also been applied to other vehicle routing problems such as the d vrp d vrptw 21 d tsptw d pdp 59 and the resource allocation problem rap 60 request buffering introduced by pureza and laporte consists in delaying the assignment of some requests to vehicles in a priority buffer so that more urgent requests can be handled ﬁrst 4 3 performance evaluation in contrast to static problems where measuring the performance of an algorithm is straightforward i e running time and solution quality dynamic problems require the introduction of new metrics to assess the performance of a particular method sleator and tarjan 131 introduced the competitive analysis let p be a minimization problem and i the set of all instances of p let z ioff be the optimal cost for the ofﬂine instance ioff corresponding to i 2 i for ofﬂine instance ioff all input data from instance i either static or dynamic is available when building the solution in contrast the data of the online version i is revealed in real time thus an algorithm a has to take into account new information as it is revealed and produce a solution relevant to the current state of knowledge let za ðiþ zðxa ðiþþ be the cost of the ﬁnal solution xa ðiþ found by the online algorithm a on instance i algorithm a is said to be c competitive or equivalently to have a competitive ratio of c if there exists a constant a such that za ðiþ 6 c z ðioff þ þ a 2 i in the case where a the algorithm is said to be strictly c competitive meaning that in all cases the objective value of the solution found by a will be at most of c times the optimal value the competitive ratio metric allows a worst case absolute measure of an algorithm performance in terms of the objective value we refer the reader to borodin and el yaniv for an in depth analysis of 8 v pillac et al european journal of operational research 1 11 this measure and to jaillet and wagner and fink et al for results on various routing problems the main drawback of the competitive analysis is that it requires to prove the previously stated inequality analytically which may be complex for real world applications the value of information proposed by mitrovic minic 100 constitutes a more ﬂexible and practical metric we denote by za ðioff þ the value of the objective function returned by algorithm a for the ofﬂine instance ioff the value of information v a ðiþ for algorithm a on instance i is then deﬁned as v a ðiþ za ðiþ za ðioff þ za ðioff þ the value of information can be interpreted as the gap between the solution returned by an algorithm a on an instance i and the solution returned by the same algorithm when all information from i is known beforehand in contrast with the competitive ratio the value of information gives information on the performance of an algorithm based on empirical results without requiring optimal solutions for the ofﬂine instances it captures the impact of the dynamism on the solution yield by the algorithm under analysis for instance gendreau et al report a value of information between 2 5 and 4 1 for their tabu search algorithm for the dvrptw while tagmouti et al report values between 10 and for a variable neighborhood search descent applied to a dynamic arc routing problem 4 4 benchmarks to date there is no reference benchmark for dynamic routing problems although it is worth noting that various authors based their computational experiments on adaptations of the solomon instances for static routing 8 9 28 29 van hentenryck and bent 145 chap 10 describe how the original benchmark by solomon can be adapted to dynamic problems the interested reader is referred to the website of pankratz and krypczyk for an updated list of publicly available instances sets for dynamic vehicle routing problems 5 conclusions recent technological advances provide companies with the right tools to manage their ﬂeet in real time nonetheless these new technologies also introduce more complexity in ﬂeet management tasks unveiling the need for decision support systems adapted to dynamic contexts consequently during the last decade the research community has shown a growing interest for the underlying optimization problems leading to a new family of approaches speciﬁcally designed to efﬁciently address dynamism and uncertainty by analyzing the current state of the art some directions can be drawn for future research in this relatively new ﬁeld first further work should aim at creating a taxonomy of dynamic vehicle routing problem possibly by extending existing research on static routing this would allow a more precise classiﬁcation of approaches evaluate similarities between problems and foster the development of generic frameworks second there is currently no reference benchmark for dynamic vehicle routing problems therefore there is a strong need for the development of publicly available benchmarks for the most common dynamic vehicle routing problems third with the advent of multi core processors on desktop computers and low cost graphical processing units gpus parallel computing is now readily available for time consuming methods such as those based on sampling although early studies considered distributed optimization most approaches reviewed in this document do not take advantage of parallel architectures the development of parallel algorithms is a challenge that could reduce the time needed for optimization and provide decision makers with highly reactive tools fourth our review of the existing literature revealed that a large fraction of work done in the area of dynamic routing does not consider stochastic aspects we are convinced that developing algorithms that make use of stochastic information will improve the ﬂeet performance and reduce operating costs thus this line of research should become a priority in the near future finally researchers have mainly focused on the routing aspect of the dynamic ﬂeet management however in some applications there is more that can be done to improve performance and service level for instance in equipment maintenance services the call center has a certain degree of freedom in ﬁxing service appointments in other words it means that the customer time windows can be deﬁned or inﬂuenced by the call center operator as a consequence a system in which aside from giving a yes no answer to a customer request suggests convenient times for the company would be highly desirable in such contexts acknowledgements financial support for this work was provided by the cper contrat de projet etat region vallée du libre and the centro de estudios interdisciplinarios básicos y aplicados en complejidad ceiba colombia this support is gratefully acknowledged int j production economics 522 contents lists available at sciverse sciencedirect int j production economics journal homepage www elsevier com locate ijpe supply chain integration and performance the effects of long term relationships information technology and sharing and logistics integration daniel prajogo a n jan olhager b a b department of management monash university po box vic australia department of management and engineering linkoping university sweden a r t i c l e i n f o a b t r a c t article history received 6 july accepted 1 september available online 10 september supply chain integration is widely considered by both practitioners and researchers a vital contributor to supply chain performance the two key ﬂows in such relationships are material and information previous studies have addressed information integration and material logistics integration in separate studies in this paper we investigate the integrations of both information and material ﬂows between supply chain partners and their effect on operational performance speciﬁcally we examine the role of long term supplier relationship as the driver of the integration using data from australian ﬁrms we ﬁnd that logistics integration has a signiﬁcant effect on operations performance information technology capabilities and information sharing both have signiﬁcant effects on logistics integration furthermore long term supplier relationships have both direct and indirect signiﬁcant effects on performance the indirect effect via the effect on information integration and logistics integration elsevier b v all rights reserved keywords supply chain integration logistics integration information integration performance 1 introduction most concepts of supply chain integration explicitly recognize the existence of two ﬂows through the chain there is a ﬂow of goods and an equally important ﬂow of information fisher huang et al pagell power supply chain integration must comprise both information and material and cannot restrict itself to only one higher levels of integration are characterized by increased logistics related communication greater coordination of the ﬁrm logistics activities with those of its suppliers and customers and more blurred organizational distinctions between the logistics activities of the ﬁrm and those of its suppliers and customers stock et al coordination collaboration and cooperation are often used more of less interchangeably for describing integrative efforts among partners to improve the overall efﬁciency of the supply chain holweg et al matopoulos et al singh and power such as collaborative planning forecasting and replenishment cpfr danese logistics integration refers to speciﬁc logistics practices and operational activities that coordinate the ﬂow of materials from suppliers to customers throughout the value stream stock et al logistics provides industrial ﬁrms with time and space utilities by providing the necessary quantity of goods in the right n corresponding author tel 3 2030 fax þ 61 3 e mail addresses daniel prajogo buseco monash edu au d prajogo jan olhager liu se j olhager see front matter elsevier b v all rights reserved doi 10 1016 j ijpe place at the right time la londe caputo and mininno grounded on earlier research the theoretical construct of logistic integration is derived to include the seamless integration of the logistics function of the various supply chain partners stock et al childerhouse and towill information integration refers to the sharing of key information along the supply chain network which is enabled by information technology it one of the main purposes of information integration is to achieve real time transmission and processing of information required for supply chain decision making lee et al show that information sharing can lead to lower cost through reductions in inventories and shortages however in order to realize this value changes in the logistics system are required such as vendor managed inventory vmi programs lead time reductions order quantity reductions and more frequent deliveries see for example selldin and olhager claassen et al in the light of supply chain integration concept logistics and information integration originally reﬂect two interrelated forms of integration which ﬂow in opposite directions i e forward and backward respectively forward integration is concerned with the physical ﬂows of materials from suppliers to manufacturers which we refer to logistics integration on the other hand backward integration is concerned with the coordination of information technologies and the ﬂows of information from manufacturers to suppliers frohlich and westbrook modeled supply chain integration in terms of both information and material using eight items d prajogo j olhager int j production economics 522 concerning it information sharing as well as logistics integration they found that wider scope of integration had a positive association with performance improvement however since the items were combined into a single construct they were unable to identify any relationship between information integration and logistics integration based on a study of ﬁve pairs of suppliers and retailers in taiwan sheu et al proposed a conceptual relationship model including long term relationship supply chain architecture including e g information sharing and it capabilities supplier retailer collaboration and performance they concluded that better it capabilities as well as better communication contribute to a better platform for both parties to engage in supply chain coordination participation and problem solving activities zhou and benton studied the effect of information sharing on supply chain practice the latter captured as a construct including elements of planning production and delivery practice they found that information sharing signiﬁcantly impacts supply chain practice and a signiﬁcant effect of delivery practices on delivery performance the study by li et al found that it capabilities and information sharing had a signiﬁcant effect on supply chain integration of logistics systems and indirectly on performance no direct effect on performance by it implementation was noted in this paper we investigate the relationships among information integration logistics integration long term relationships and the effects on performance we use constructs developed and veriﬁed by chen and paulraj we review the relationships and present the hypotheses then the survey methodology and results are presented finally the implications for managers and researchers are discussed the novel aspect of this research is that the variables have not been studied in this context before to the best of our knowledge this is the ﬁrst study to include all of these variables 2 theoretical background and hypotheses in this section we ﬁrst explain the notion of logistics integration and its impact on competitive performance we then discuss the role of supply chain information integration both it and information sharing in supporting the material ﬂow integration thirdly the long term relationship with suppliers is discussed as an antecedent of supply chain information management and its potential direct impact on performance subsequently we develop our research hypotheses linking long term relationship with suppliers information integration logistics integration and performance 2 1 logistics integration the increasing competition has driven ﬁrms to not only improve their internal operations such as process control and inventory management but also focus on integrating their suppliers and customers into the overall value chain processes the contribution of suppliers in delivering values to customers hence building competitive capabilities quality delivery ﬂexibility and cost has been well recognized the essence of logistics integration is well coordinated ﬂow of materials from suppliers which allow ﬁrms to have a smooth production process frohlich and westbrook such coordination produces a seamless connection between ﬁrms and suppliers in such a way that the boundary of activities between the two parties is getting blurred stock et al it has been well argued that having solid logistics integration will reduce various problems such as the bullwhip effect lee et al geary et al integrated logistics also allow ﬁrms to adopt lean production systems which are characterized by reliable order cycles and inventory reduction cagliano et al schonberger by and large logistics integration allows companies and their supply chain partners to act as a single entity which would result in improved performance throughout the chain tan et al in other words through logistics integration ﬁrms can have the potential beneﬁts of vertical integration quality dependability planning and control and lower costs without having it in the physical sense la londe and masters improved logistics integration between supply chain partners yields a number of operational beneﬁts including reduction in costs nooteboom lead time liu et al and risks clemons et al as well as improvement in sales distribution customer services and service levels seidmann and sundararajan and customer satisfaction kim the majority of empirical surveys on supply chain integration report a positive relationship between integration and performance van der vaart and van donk de toni and nassimbeni found that better performing plants exhibit a higher level of logistical interactions frohlich and westbrook found that the widest arcs of integration had the strongest association with performance improvement sheu et al found that higher levels of collaboration result in operational efﬁciency in the supply chain system and ﬁnally li et al found that supply chain integration is signiﬁcantly related to supply chain performance thus we can formulate our ﬁrst hypotheses logistics integration has a positive relationship with operational performance 2 2 supply chain information integration as frohlich and westbrook suggested the material ﬂow from upstream to the downstream supply chain entities must be supported by the information ﬂow from downstream to upstream based on ﬁve case studies of dyads supplier retailer sheu et al found that better it capabilities as well as better communication contribute to a better platform for both parties to engage in coordination participation and problem solving activities thus both information technology and information sharing can be viewed as antecedents to material ﬂow integration our review of the literature concerning the topics related to supply chain information integration suggest two major aspects the technical aspects information technology connection and the social aspects information sharing and trust for example a number of studies narasimhan and kim frohlich gunasekaran and ngai devaraj et al sanders focused on the importance of adopting e business technologies as a means for information integration the other group of studies yu et al narasimhan and nair carr and kaynak zhou and benton li and zhang sezen focused on the importance of information sharing and communication between ﬁrms and suppliers it is the notion of this paper that both aspects of information integration are important over reliance on technology without willingness to share the critical information pertaining to supply chain will not make the ﬁrms meaningfully connected thus failing to produce logistics integration only ﬁrms that are capable of building both the technical and social aspects of information integration will see the maximum beneﬁts of logistics integration chae et al fiala fawcett et al each of the two aspects of information integration is discussed below 2 2 1 information technology information and communication technology plays a central role in supply chain management in the following aspects first it allows ﬁrms to increase the volume and complexity of information which needs to be communicated with their trading partners second it allows ﬁrms to provide real time supply chain information including d prajogo j olhager int j production economics 522 inventory level delivery status and production planning and scheduling which enables ﬁrms to manage and control its supply chain activities third it also facilitates the alignment of forecasting and scheduling of operations between ﬁrms and suppliers allowing better inter ﬁrms coordination as such the problems in coordinating supply chain activities which often are hindered by time and spatial distance can be reduced paulraj and chen the use of it in supply chain has received considerable attention with various technologies being introduced for business to business communication including web internet private ethernet and epos electronic point of sale studies have shown that effective it connection improves the integration between supply chain partners in terms of material ﬂows soliman and youssef in this regard it supports key processes in supply chains including sourcing procurement and order fulﬁllment kehoe and boughton swaminathan and tayur accordingly we hypothesize the intensity of information technology connection between ﬁrms and their suppliers has a positive relationship with logistics integration 2 2 2 information sharing while the technological aspect of information integration is important it is the frequency the quantity and the quality of information that is shared that really matters according to fawcett et al large investments in it could fail to produce expected beneﬁts if it is not supported by willingness to share needed information information sharing requires ﬁrms to exchange strategic supply chain information and not only transactional data such as materials or product orders the strategic supply chain information provides leverages to the supply chain partner for making strategic decision in their operations li et al for example point of sale history helps suppliers to successfully forecast demand which subsequently improves service level and efﬁciency to their customers similarly real time inventory position helps suppliers to plan their replenishment and delivery schedules thus improving service levels and reducing inventory costs seidmann and sundararajan such level of information sharing requires frequent and intense communication between ﬁrms and suppliers the intensity of communication constitutes high levels of cooperative behavior between supply chain partners which leads to high degree and symmetry of strategic information ﬂows between them klein et al a number of studies have demonstrated various logistics beneﬁts of having information sharing with supply chain partners concerning inventory management cachon and fisher lee et al yu et al zhao et al agility and ﬂexibility swafford et al and the bullwhip effect dejonckheere et al for example vendor managed inventory vmi integration with suppliers has been shown to reduce the bullwhip effect disney and towill as such we hypothesize the intensity of information sharing between ﬁrms and their suppliers has a positive relationship with logistics integration 2 3 long term relationships the ways ﬁrms relate with suppliers have changed considerably given that manufacturing ﬁrms are getting more and more focused on their core competence their reliance on strategic supplier increases prahalad and hamel among the changes three key aspects of supplier relationships are highlighted here first the trend now is to build a long term relationship with suppliers rather than short term contracts helper ogden second in conjunction with the ﬁrst point ﬁrms now use fewer suppliers over a longer period of time rather than keeping a large base of suppliers which allow them to change suppliers for almost every contract the beneﬁts of having low price resulted from creating competition among suppliers are now changed into low total cost of ownership due to long term and large volume of purchases helper third the relationship with suppliers has been enhanced into strategic level where suppliers are now considered as the integral part of the ﬁrm operations choi and hartley kotabe et al chen and paulraj this change has led to various avenues of collaboration including joint improvement program early supplier integration in product design and proﬁt and risk sharing one aspect of strategic supplier relationship is extended longevity long term relationships have several implications and one of them is that ﬁrms may be ready for putting large investments in building the relationship including it and information sharing de toni and nassimbeni klein et al found that the greater the mutual trust the greater the it customization and the greater the strategic information ﬂows sheu et al found that long term orientation affects supply chain architecture which includes it capabilities and information sharing paulraj et al found a signiﬁcant relationship between long term relationship with suppliers and information sharing we can thus formulate the following two related hypotheses long term relationship with suppliers has a positive relationship with information technology connection between ﬁrms and their suppliers long term relationship with suppliers has a positive relationship with information sharing between ﬁrms and their suppliers chen and paulraj modeled a long term relationship as a potential antecedent of buyer performance vickery et al suggested that long term relationships can result in improved ﬁrm performance and de toni and nassimbeni found that better performing plants exhibit a better use of long term supply agreements with sources similarly singh and power found that effective supplier collaboration has a direct effect on ﬁrms competitive performance with this we arrive at our ﬁnal hypothesis the long term relationship with suppliers has a positive relationship with performance 3 research framework the research model is shown in fig 1 the six hypotheses build up the model linking long term relationships information integration logistics integration and performance overall the model captures the three principal elements of an integrated supply chain suggested by handﬁeld and nichols namely information ﬂow product and material ﬂows and long term relationships between supply chain partners our model is different from those in previous studies with several respects sheu et al studied ﬁve pairs of suppliers and retailers in taiwan based on their main ﬁndings they developed a conceptual relationship model the model includes long term relationship supply chain architecture incl e g information sharing and it capabilities supplier retailer collaboration and performance thus their proposed theoretical model contains similar elements as our research model there are two key differences though in that we differentiate between it and information sharing while they consider them jointly to be part of a supply chain architecture construct and we allow for a direct effect of long term supplier relationships on performance different to fawcett et al we propose that it connection and information sharing are antecedents of logistics integration and not directly related to operational performance in order to achieve higher levels d prajogo j olhager int j production economics 522 information integration information technology long term relationship logistics integration information sharing performance fig 1 research framework of performance the information dimension of integration must be transferred to the physical materials integration in other words both information and materials ﬂow must be integrated in order to reach higher levels of performance using data drawn from chinese companies li et al investigated the relationship among it implementation supply chain integration and supply chain performance our study expands their model by incorporating strategic relationships with suppliers as a potential antecedent of it implementation as well as adding the social aspects of information integration i e information sharing to provide a more comprehensive framework in summary our research framework provides contribution to the knowledge in three ways first it tests the link between information ﬂow and material ﬂow in supply chain second it tests the technological and social aspects of information integration third it integrates previous studies on the topic and provides a comprehensive framework on the driver long term relationship and enabler information integration of logistics integration and its effect on performance 4 methods 4 1 sample and procedures the data for this study was drawn from managers of australian manufacturing ﬁrms via a mail survey that was conducted between end of and early the list of the respondents was randomly selected and purchased from a mailing list company in total surveys were mailed out and usable responses were received hence the response rate is 13 1 the data were checked for bias using correlations of responses between early respondents and late respondents based on industry sectors and organizational size the chi square tests on both categories did not indicate any signiﬁcant difference between the two groups of respondents the dataset used in this study has also been used in two other studies olhager and prajogo prajogo et al olhager and prajogo analyse differences between two production systems make to stock versus make to order with regard to the role of supply chain management practices in enhancing business performance prajogo et al examine the unique effects of different supplier management practices on different aspects of operational performance each paper includes logistics integration as a key variable but considers it in a different research framework accompanied by different variables relevant to the speciﬁc purpose of the study in terms of industry sectors 16 of the respondents came from electronic electrical 25 from machinery 8 from automotive 11 from chemical 4 from food processing 7 from construction and 12 from other manufacturing sectors the remaining sectors identiﬁed as others included medical equipment wood printing and paper defense in terms of organizational size based on the number of employees of the respondents came from ﬁrms with less than 100 employees 35 of the ﬁrms have between 100 and employees and the remainder of the respondents came from large manufacturing with over employees we also identiﬁed the process type of the sampled ﬁrms following hayes and wheelwright classiﬁcation 32 of the ﬁrms used assembly line 27 batch 3 cellular 20 job shop and the rest project in terms of the position of the respondents nearly half of the respondents held a position as operations managers 27 supply chain logistics managers procurement purchasing and 3 customer services managers 4 2 measures most items used to build the scales in this study were adapted from the study by chen and paulraj to ensure their content validity a 7 point likert scale was used to provide responses for long term relationship information technology information sharing and logistics integration the scales ranged from 1 strongly disagree to 7 strongly agree we retained the original construct names except for information sharing which chen and paulraj originally labeled as communication we excluded one item from the original scale of logistics integration i e information and materials ﬂow smoothly between our suppliers and us for the reason that the item includes both information and materials ﬂow which contrasts to our framework which not only distinguishes them but also examines their relationships the measure for operational performance comprises four key competitive dimensions in operations namely quality delivery ﬂexibility and cost in this regard the respondents were asked to assess their performance relative to the best competitor in the market with the scale ranging from 1 weakest in the industry to 7 strongest in the industry the items used in this study can be found in table 1 5 data analysis 5 1 scale validity and reliability we used conﬁrmatory factor analysis to simultaneously validate the measures of all variables used in this study the results of the conﬁrmatory factor analysis and the cronbach alpha are presented in table 1 the items loaded signiﬁcantly on their respective constructs the item loadings and the overall model ﬁt results suggest acceptable unidimensionality and convergent validity for the measures carmines and mciver bollen bagozzi et al hoskisson et al cronbach alphas suggest satisfactory reliability of the ﬁve constructs nunnally d prajogo j olhager int j production economics 522 table 1 scale validity and reliability scales items loading paths cronbach alpha long term relationship we expect our relationship with key suppliers to last a long time we collaborate with key suppliers to improve their quality in the long run the suppliers see our relationship as a long term alliance we view our suppliers as an extension of our company 71 79 87 86 88 information technology there are direct computer to computer links with key suppliers inter organizational coordination is achieved using electronic links we use information technology enabled transaction processing we have electronic mailing capabilities with our key suppliers we use electronic transfer of purchase orders invoices and or funds we use advanced information systems to track and or expedite shipments 76 91 85 50 55 0 62 0 87 information sharing communication we share sensitive information ﬁnancial production design research and or competition suppliers are provided with any information that might help them exchange of information takes place frequently informally and or timely we keep each other informed about events or changes that may affect the other party we have frequent face to face planning communication with our suppliers 0 64 0 84 0 75 0 84 0 77 0 60 logistics integration inter organizational logistic activities are closely coordinated our logistics activities are well integrated with suppliers logistics activities we have a seamless integration of logistics activities with our key suppliers our logistics integration is characterized by excellent distribution transportation and or warehousing facilities the inbound and outbound distribution of goods with our suppliers is well integrated 0 76 0 89 0 89 0 85 performance performance of our ﬁnal products speed of deliveries volume or capacity ﬂexibility degree of product variety production costs 0 48 0 52 0 62 0 54 0 47 0 93 0 77 0 64 rmsea 0 nfi 0 0 the slightly lower than standard acceptable value of cronbach alpha for performance 7 is interpreted such that performance is indeed composed of multiple dimensions or elements such disparate measures as quality delivery ﬂexibility and cost performance contributed to this construct and this could suggest that some ﬁrms often specialize or focus to excel in only a subset of these performance dimensions reasonable evidence of discriminant validity of the constructs ahire et al with ﬁve constructs incorporated in this study we conducted ten chi square tests the values of for all tests conﬁrm the discriminant validity of the constructs and lend further evidence towards the lack of common method variance 5 3 structural model 5 2 common method variance and discriminant validity we used harman single factor test to check for common method variance podsakoff and organ this test was conducted using principal component analysis and loading all 26 items on one factor the test checks if one single factor would emerge from factor analysis which would point towards the presence of common method bias the factor analysis indicated that less than 25 variance was extracted and that half of the items suffered from poor factor loadings well below 0 5 these results suggest that common method variance was not a signiﬁcant problem in the data set as an additional check we conducted discriminant validity analysis to examine if the explanatory and the dependent constructs signiﬁcantly overlap each other as suggested by venkatraman discriminant validity was established by conducting conﬁrmatory factor analysis cfa on each pair of the constructs in this study for each pair cfa was conducted twice the ﬁrst cfa allowed the correlation between the two constructs to be freely estimated the chi square value of this model was estimated in the second cfa the correlation between the two constructs was ﬁxed to 1 0 and the chi square value of this model was estimated if the difference between the chi squares obtained from the ﬁrst and second cfa i e is greater than the chi square value at the degree of freedom of 1 and signiﬁcance level of 01 i e 6 64 this provides a we present the results of the structural equation model sem in fig 2 the ratio of 14 to degrees of freedom is less than the recommended value of 3 0 for satisfactory ﬁt of a model to data carmines and mciver bollen hair et al in line with the prescriptions mulaik et al the ﬁt indices and the root mean square error of approximation with conﬁdence interval 0 043 0 060 are deemed acceptable we included organizational size in terms of number of employees and process type ranging from assembly line to project as control variables for operational performance and found no signiﬁcant effect on operational performance 0 00 and 0 08 respectively both with 0 05 the results of the six hypotheses are outlined below logistics integration has a positive relationship with performance this hypothesis is supported as the parameter estimate 0 26 is signiﬁcant the intensity of information technology connection between ﬁrms and their suppliers has a positive relationship with logistics integration this hypothesis is supported as the parameter estimate 0 38 is signiﬁcant the intensity of communication between ﬁrms and their suppliers has a positive relationship with logistics integration d prajogo j olhager int j production economics 522 0 26 information integration 0 25 information technology 0 38 long term relationship logistics integration 0 53 information sharing 0 26 performance 0 39 the path is significant at the 0 01 level χ 2 497 14 df rmsea 0 nfi 0 nnfi 0 963 cfi 0 967 fig 2 results of path analysis this hypothesis is supported as the parameter estimate 0 39 is signiﬁcant long term relationship with suppliers has a positive relationship with information technology connection between ﬁrms and their suppliers this hypothesis is supported as the parameter estimate 0 25 is signiﬁcant long term relationship with suppliers has a positive relationship with communication between ﬁrms and their suppliers this hypothesis is supported as the parameter estimate 0 53 is signiﬁcant long term relationship with suppliers has a positive relationship with performance this hypothesis is supported as the parameter estimate 0 26 is signiﬁcant 5 4 competing structural model for a conﬁrmatory purpose we tested a competing model by adding three paths which were not estimated in the tested model namely long term relationship logistics integration information technology performance and information sharing performance the ﬁtness indices df¼306 923 nnfi¼0 963 968 and the 052 suggests that there is no signiﬁcant improvement on the original model this is indicated by the increasing ratio between the chi square and degree of freedom moreover the three added paths in the competing model were found to be non signiﬁcant at 5 level conﬁrming our a priori model which did not include them 6 discussion of the ﬁndings and their implications this study contributes to the research stream on logistics integration by investigating the relationships between long term relationships information integration logistics integration and competitive performance in general the results of this research provide empirical evidence that effective external logistics integration is engendered by long term relationships and information integration speciﬁcally this study contributes to supply chain studies with the following respects first it demonstrates that the integration of material ﬂow needs to be underpinned by information integration in this way the supply chain material ﬂows from suppliers will be neatly guided by the demand chain information ﬂows from customers this ﬁnding is consistent with previous studies which suggested that the key to building a seamless supply chain is by establishing data and information connection at the supply chain interface by integrating the supply chain information supply chain partners can virtually work as a single entity which will enable them together to respond to market demands and to create best value for customers the importance of having material and information integration has provided both opportunities and challenges for business ﬁrms today the increasing rate of competition customer expectations and market s dynamic has increased the supply chain uncertainty which poses greater risks for companies in addition operational activities have become more fragmented as ﬁrms get more focused on the core competence and relinquish the non core activities to their suppliers whose locations could be geographically dispersed this trend calls for the need for building a solid integration mechanism between business entities of the supply chain members underpinned by accurate and timely information second this study also highlights the need for observing the two aspects of information integration which have been addressed separately in previous studies on supply chain our ﬁndings indicate that both it capabilities and information sharing have a relatively similar effect on logistics integration signifying their equal importance the balance between technical and attitudinal aspects of supply chain information management highlights the integration between hard and soft aspects of management this issue is important since many ﬁrms are primarily attracted by the hard factors which are relatively easy to acquire as long as the ﬁrm has funds and feel conﬁdent that hard technologies will deliver the expected results studies on supply chain integration however have shown that the soft aspects i e building good communication and trust for information sharing of information integration are the areas where many ﬁrms still struggle arshinder and deshmukh further analysis from our data indicate a low correlation 17 at 05 between it and information sharing suggesting that the improvement of one will not automatically and signiﬁcantly improve the other this result suggests that ﬁrms cannot assume that because they are technically connected they are also socially connected management need to build both aspects before they can see the real beneﬁts derived from supply chain information integration third this study demonstrates that long term supply chain relationships help foster collaborative behaviors which are translated into various forms including information integration as mentioned earlier information integration cannot be achieved unless the relationship between supply chain partners goes beyond trading transactional relationships the risk of putting huge investment in it and sharing sensitive information is a serious hindrance which can only be taken when ﬁrms have a strategic and long term relationship at the same time the d prajogo j olhager int j production economics 522 effective use of information integration is also determined by how well ﬁrms understand the exchanged information and this can only be achieved through a learning process over a long period of time cagliano et al found that the level of the use of information technologies and sharing information in supply chains was determined by the level of strategic relationship between supply chain partners this relationship determines whether the information connection e g e business is used simply for tactical contingent or strategic purpose the other issue related to supplier relationship is that establishing longterm partnership indicates that both supply chain parties have developed mutual trust aiming for achieving mutual beneﬁts from their relationships such mutual beneﬁts have been instrumental in determining the effects of information integration as shown in a number of studies see for example ballou et al lambert and cooper fourth the study also demonstrates the direct effect of long term relationships on competitive performance which is not mediated by information integration and logistics integration such practice as joint quality improvement program is one of the examples which contribute to supply chain performance this ﬁnding reinforces the importance of network capability in addition to internal capability of ﬁrms according to the relational view theory dyer and singh ﬁrms need to complement their internal capabilities with other capabilities which they cannot build internally in order to achieve competitive advantage the effective way to access and exploit such capabilities is by building strong and long term relationships with their supply partners who own them this ﬁnding therefore suggests that while supply chain integration in terms of both information and materials are important parts of the overall relationship with suppliers they alone will not maximize the beneﬁts which ﬁrms can derive from the suppliers 7 limitations and further research a number of limitations of the current study can be noted as well as some directions for future research any study is limited to the factors that are included we included factors identiﬁed in the literature on supply chain integration capturing information ﬂow product and material ﬂows and long term relationships between supply chain partners as suggested by handﬁeld and nichols while studying the integration of information and material ﬂows between supply chain partners and noting signiﬁcant effects on performance other factors not included in this study may exist that may have a positive impact on performance we controlled for ﬁrm size and industry which had no signiﬁcant effect in this study however future research may want to control for other effects that may be present in other situations e g between manufacturers of consumer goods and industrial goods in the light of supply chain integration concept logistics and information integration originally reﬂect two interrelated forms of integration which ﬂow in opposite directions i e forward and backward respectively forward integration is concerned with the physical ﬂows of materials from suppliers to manufacturers which we refer to logistics integration on the other hand backward integration is concerned with the coordination of information technologies and the ﬂows of information from manufacturers to suppliers as clariﬁed from the beginning this study assumes that information and material ﬂow in opposite directions within supply chain pipeline with material having forward ﬂow while information having backward ﬂow however today the ﬂows of both information and material have expanded into both directions the development of the concept of reverse logistics denotes that the material ﬂow is now going back backward as used ﬁnished goods are delivered from the point of consumption to the point of origin for possible recycling remanufacturing or disposal sarkis in supporting the reverse logistics the ﬂow of information also needs to go forward as manufacturers need to provide clear product information to make recycling processes easier when the product s life ends ferguson and browne future studies can explore the bi directional ﬂows of material and information which may need a different model of supply chain integration a limitation of this study is the sample population which is restricted to australian ﬁrms although we expect these results to hold for supply chains in general we cannot claim that this is the case therefore future research may extend this study to a broader population of ﬁrms including other countries for generalizability of the results and to detect potential country effects also in this study we rely on the perception of the focal ﬁrm on the relationships it would be beneﬁcial to investigate multiple parties of supply chain relationships to hear both sides of the story concerning issues of reciprocity and mutual effects 8 conclusions this study shows that both information and material ﬂow integration are important for supply chain integration having signiﬁcant effects on performance at the same time supply chain integration is a difﬁcult task as it involves many management aspects in terms of both hard and soft information exchange mechanisms in support of the logistics integration activities concerning the physical material ﬂow between the two parties such complex issues can only be managed where there is a long term relationship between supply chain partners this study therefore reinforces the importance of building long term relationships with suppliers which have been promoted since the emergence of the quality management era all in all competitive performance is positively affected by all constructs included in this study either directly or indirectly this suggests that the integration of supply chain partners is multi faceted and that many competencies act complementary to achieve a higher level of performance anylogic and java nathaniel osgood advantages of anylogic as compared to other agent based modeling software primarily declarative specification less code great flexibility access to java libraries support for multiple modeling types support for mixture of modeling types painful sides of anylogic education advanced export of model results lack of trajectory files lack of a built in debugger need for bits of java code many pieces of system internals of anylogic files xml java code when how much java is a popular cross platform object oriented programming language introduced by sun microsystems anylogic is written in java and turns models into java anylogic offers lots of ways to insert snippets hooks of java code you will need these if you want to e g push anylogic outside the envelop of its typical support e g enabling a network with diverse agent types exchange messages between agents put into place particular initialization mechanisms collect custom statistics over the population stages of the anylogic build inspecting the java code as a step towards creating an executable representation of the code anylogic creates a java representation if you want to see the java code for a model you will need to do a build sometimes it can be helpful to look at this java code to find errors about which anylogic may be complaining advanced to see how things are being accomplished or work requesting viewing of java code examples of where to insert code object properties advanced examples of where to insert code object properties general example of where to insert code presentations properties dynamic properties of presentation elements especially of agents tips to bear in mind while writing code click on the light bulb next to fields to get contextual advice e g on the variables that are available from context while typing code can hold down the control key and press the space key to request autocompletion this can help know what parameters are required for a method etc java is case sensitive can press control j to go to the point in java code associated with the current code snippet can press build button after writing snippet to increase confidence that code is understood example of contextual information file edit view model window help anyl ogic advanced educational use only vt d o t l j q q i d i j tpget suppa rt pro j e ct n l bi pe r so rn el m a el el ll tt el a t o n o o t i e l op filt ha l m ology d ep art ment mainphasel simu lat ion netwo rk seir ma i n parameters l pl ain variable b furnct ions env iro nmen ts e rnv i ro nm ent f b embedded obj ect ana ly si data g datasetab soiute pr eva lence g cou rnt l rnf ect i ou l l pr ese i o per so n l pla in var iabie sta t e ch a rt stat e chart a tated f i pro blem ll i p l on j pe ople statistks de scri ption p eo ple e n v i ro nment g count lnfe ctiou nam c ount l nfe ct i ous t yp e count q sum q av rag e q min q max eact i v e per so iadd st at i st i cs i param ete r flow am var iab le d stock variab le lj event dynamic event plain variab le co ll e ct ion variable fun ct i on ti tabl e fun ct i on i port con rne ct or erntry po int state t rarnsit iorn lrni t ia l state po int er bram ch hist ory state fina l state ernv i ro rnm ernt ac t io rn i ii analysi ii pr e sentat i orn ci c onn ect iv ity ffi ernt erpri se libr ary more libr ari e autocompletion info via control space j anylogic university evaluation use only gj f ile edit view model window help f t p r o je l id did li fftle i ri j iq a get support el palette l el hybridabmnetworkmodeling l j o main j o person color agen tentity entity sta ts eb o simulation main multipleagentcl asseslnnetworkl st at echait suscept ible infected i i gene ral parameter i ev en t dynamic event plain var iable i collection var ia bl e g function cl table function l port connector environmen t i console person active object class genera l x ad v an ced v v el receivemessage lnt boolean stat echart e pr oblems j v el agent receivemessage object boolean sta tech reoeivemessage description location pr evie w description movement parameters ve lo city ro tation on arrival on messagereceiv ed tatechart rece public boolean receiv emessage int ms same as receivemessage object msg but with an integer as message parame ter mg g th e in tege r po sted to the sta techart on before step enterpr i se library pedes tri an library i on step i selection i x y finding the enclosing main class from an embedded agent from within an embedded agent one can find the enclosing main class by calling this will give a reference to the single instance object of the main class in which the agent is embedded an alternative approach is to call main getowner presentation properties both key customizable classes main various agent classes can be associated with presentation elements these elements are assembled during execution into animations presentations of the agents many of these presentation elements have properties that can be set to java expressions enabling programmatic control getting to the anylogic help choose help help contents anylogic help includes many components tutorials user references anylogic library information getting information on the anylogic java libraries the notion of a code library a library lets third parties e g xjtek share compiled code they have developed with others the classes built into our anylogic projects e g agent activeobject networkresourcepool etc are contained in the library the available libraries that come with anylogic java have many additional components that can offer tremendous additional functionality by tapping into this functionality we can avoid having to write code ourselves to use a library you need to know what is in it finding out information interfaces for library elements finding out information interfaces for library elements using libraries there are two major libraries that are built in and can be used without additional reference java libraries anylogic libraries to use an object in the java libraries you will use an import statement using external libraries there are tremendous numbers of party libraries available for java the functionality associated with these libraries is incredibly diverse many of these libraries are available for free others are sold it is very easy to make use of the functionality of party libraries from anylogic in order to do this anylogic needs to know about the external library adding external libraries adding external libraries common contextual variables that are used by code snippets in statistics item indicates current agent in on message received handler for agent msg indicates received message in dynamic properties of an agent replicated line property index indicates current person index in parameters properties of agent populations used to set properties of agents within population index indicates the index of the current agent in the population example code to export dataset fileoutputstream fos new fileoutputstream filename printstream p new printstream fos p println datasetname tostring outputs comma delimited values useful bits of java code gets reference to main object activeobject trace str outputs string to log engine gettime gets the current time agents size gets number of objects in collection agents agents item i gets item i from agent collection uniform generates a random number from useful bits of java code general expressions activeobject traceln stringstr outputs string to log time gets the current internal model time different from the time in the external world members of com xj anylogic engine utilities uniform generates a random number from uniform x gen a random number in range to x lognormal double meannormal double sigmastddevnormal double minnormal draws from a lognormal distribution normal double meannormal double sigmastddevnormal draws from a normal distribution many other probability distributions methods on populations of agents in main class population size gets number of objects in collection population population statname retrieves the current value of the population statistic statname as computed for population population population item int i gets item i from population collection adds agent to that population removes agent from that population useful java code methods to call on or from within using this an agent a getconnectionsnumber returns number of connections between this agent and others gets reference to main object tostring gets string rendition of agent a getconnections gets a collection linked list of agents to which this agent is connected over which we can iterate a connectto agent b connects a to b a disconnectfrom agent b disconnects b from a a disconnectfromall disconnects all agents from a a getconnectedagent int i gets the ith agent connected to a a isconnectedto agent b indicates if a is connected to b methods on statecharts called from within agent code isstateactive intstatename indicates whether agent is in a given state composite or simple getactivesimplestate get number of simple state can then compare to different state names e g in switch statement methods on process flow diagrams source inject int count injects a count of entities into the source object i e into an object of type source gotchas changing rates for leaving a state do not get updated until leave reenter state including by a self transition example use of getactivesimplestate switch tbprogressionstatechart getactivesimplestate case ltbi return color yellow case undiagnosedactivetb return color red case diagnosedactivetb return color orange case tbsusceptible default return color black useful snippets handling messages sending sender deliver msg receiver immediately deliver a message from sender to receiver sender send msg receiver deliver a message from sender to receiver environment delivertorandom msg within main immediately deliver a message to a random agent in the environment send infection within an agent send a message to a randomly selected agent connected to this one where those agents are selected w uniform prob receive message tbprogressionstatechart receivemessage msg to forward message received by agent to statechart useful snippets fields of dynamic properties of line object for agent presentation under dynamic tab of line properties replication getconnectionsnumber dx getconnectedagent index getx getx dy getconnectedagent index gety gety these basically allow for appropriate initiation of visual properties of the inter agent connections in agent on message received handler under agent tab of person statechartname receivemessage msg this forwards a message received by this agent to statechart note that if there are different messages destined for different statecharts they can be dispatched here to different targets tutorial classes objects references nathaniel osgood cmpt recall building the model right some principles of software engineering technical guidelines try to avoid needless complexity use abstraction encapsulation to simplify reasoning development name things carefully design code for transparency modifiability document create self documenting results where possible consider designing for flexibility use defensive programming use type checking to advantage subtyping and sometimes subclassing to capture commonality for unit checking where possible process guidelines use peer reviews to review code design tests perform simple tests to verify functionality keep careful track of experiments use tools for version control documentation referent integrity do regular builds system wide smoke tests integrate with others work frequently in small steps use discovery of bugs to find weaknesses in the q a process recall the challenges of complexity complexity of software development is a major barrier to effective delivery of value complexity leads to systems that are late over budget and of substandard quality complexity has extensive impact in both human technical spheres recall why modularity as a way of managing complexity allows decoupling of pieces of the system separation of concerns in comprehension reasoning example areas of benefit code creation modification testing review staff specialization modularity allows divide and conquer strategies to work as a means to reuse recall abstraction key to modularity abstraction is the process of forgetting certain details in order to treat many particular circumstances as the same we can distinguish two key types of abstraction abstraction by parameterization we seek generality by allowing the same mechanism to be adapted to many different contexts by providing it with information on that context abstraction by specification we ignore the implementation details and agree to treat as acceptable any implementation that adheres to the specification liskov guttag recall a key motivator for abstraction risk of change abstraction by specification helps lessen the work required when we need to modify the program by choosing our abstractions carefully we can gracefully handle anticipated changes e g choose abstracts that will hide the details of things that we anticipate changing frequently when the changes occur we only need to modify the implementations of those abstractions recall defining the interface knowing the signature of something we are using is necessary but grossly insufficient if could count only on the signature of something remaining the same would be in tremendous trouble could do something totally different we want some sort of way of knowing what this thing does we don t want to have to look at the code we are seeking a form of contract we achieve this contact through the use of specifications recall types of abstraction in java functional abstraction action performed on data we use functions in oo methods to provide some functionality while hiding the implementation details we previously talked about this interface class based abstraction state behaviour we create interfaces classes to capture behavioural similarity between sets of objects e g agents the class provides a contract regarding nouns adjectives the characteristics properties of the objects including state that changes over time verbs how the objects do things methods or have things done to them recall functional abstraction functional abstraction provides methods to do some work what while hiding details of how this is done a method might compute a value hiding the algorithm test some condition hiding all the details of exactly what is considered and how e g ask if a person is susceptible perform some update on e g a person e g infect a person simulate the change of state resulting from a complex procedure transmit infection to anther return some representation e g a string of or information about a person in the model encapsulation key to abstraction by specification separation of interface from implementation allowing multiple implementations to satisfy the interface facilitates modularity specifications specify expected behavior of anything providing the interface types of benefits locality separation of implementation ability to build one piece without worrying about or modifying another see earlier examples modifiability ability to change one piece of project without breaking other code some reuse opportunities abstract over mechanisms that differ in their details to only use one mechanism e g shared code using interface based polymorphism two common mechanisms for defining interfaces interface alone explicit java interface constructs interface defines specification of contract interface provides no implementation interface implementation classes using java class construct a class packages together data functionality superclasses provide interface implementations abstract classes as mechanism to specify contract define some implementation but leave much of the implementation unspecified we will focus on this what is a class a class is like a mould in which we can cast particular objects from a single mould we can create many objects these objects may have some variation but all share certain characteristics such as their behaviour this is similar to how objects cast by a mold can differ in many regards but share the shape imposed by the mould in object oriented programming we define a class at development time and then often create multiple objects from it at runtime these objects will differ in lots of parameterized details but will share their fundamental behaviors only the class exists at development time classes define an interface but also provide an implementation of that interface code and data fields that allow them to realized the required behaviour fecall a critical distinction design specification vs execution run times the computational elements of anylogic support both design execution time presence behaviour design time specifying the model execution time runtime simulating the model it is important to be clear on what behavior information is associated with which times generally speaking design time elements e g in the palettes are created to support certain runtime behaviors recall a familiar analogy the distinction between model design time model execution time is like the distinction between time of recipe design here we re deciding what exact set of steps we ll be following picking our ingredients deciding our preparation techniques choosing making our cooking utensils e g a cookie cutter time of cooking when we actually are following the recipe a given element of the recipe may be enacted many times one step may be repeated many times one cookie cutter may make many particular cookies cooking analogy to an agent class a cookie cutter we only need one cookie cutter to bake many cookies by carefully designing the cookie cutter we can shape the character of many particular cookies by describing an agent class at model design time we are defining the cookie cutter we want to use familiar classes in anylogic main class person class simulation class work frequently done with objects reading fields variables within the object setting fields calling methods to compute something a query to perform some task a command creating the objects methods to call on or from within using this an agent a getconnectionsnumber returns number of connections between this agent and others a gets reference to main object a tostring gets string rendition of agent a getconnections gets a collection linked list of agents to which this agent is connected over which we can iterate a connectto agent b connects a to b a disconnectfrom agent b disconnects b from a a disconnectfromall disconnects all agents from a a getconnectedagent int i gets the ith agent connected to a a isconnectedto agent b indicates if a is connected to b composition of methods suppose we have an agent called a a getconnectedagent tostring this will print out the name of the agent to which a is connected a getconnectedagent getconnectionsnum ber this will print out the number of connections possessed by the agent to which a is connected distinction between class and object sometimes we want information or actions that only relates to the class rather than to the objects in the class conceptually these things relate to the mould rather than to the objects produced by the mould for example this information may specify general information that is true regardless of the state of an individual object e g agent we will generally declare such information or actions to be static java variables include parameters arguments to functions local variables within a function fields within a class values references in java variables hold values it is the contents of these variables that is of interest variables themselves just store values there are many types of variables could be parameters to a function local temporary variables within a function variables within a class to be found in every object that is instantiated from that class static variables associated with a class only one variable associated with the class no how many objects of the class are circulating broad types of java values primitive values here the value is directly stored in the variable int double float etc references here the value within the variable actually points to either an object could have many other references to it as well a distinguished value null means doesn t refer to any object objects in java contain data fields property these store information behavior methods functions these allow the object to undertake certain tasks fielda type int fieldb type doube a object can contain references to other objects foo fielda type int fieldb type string fieldc type a fieldw type double fieldy type int finding the enclosing main class from an embedded agent from within an embedded agent one can find the enclosing main class by calling this will give a reference to the single instance object of the main class in which the agent is embedded an alternative approach is to call main getowner reference from agent class to main object sole instance of main class main object age type double sex type sex male a returns this reference assignment consider two variables a and b that hold values consider further the statement a b how this is interpreted depends on the type of b if b is a primitive e g int double here the assignment will make a copy of that value before a b after a b if b holds a reference to an object a will now hold a reference to that same object a after b assignment if the programmer later modifies that object through a that same change will be visible through b as well before a b assignment to a field property of the object through variable a a field after a b references vs values the type of a variable indicates the sort of data to which it can refer looking at a variable type will tell you much about how it can be used whether primitive or reference sort of operations that are possible on the data it holds arrays java supports collections called arrays these store collections of values in an indexed fashion by giving an index we can get back an element these arrays can be of or more dimensions an array of dimension is just a array of references to arrays exam le landsca e information file edit view model window help j l g j vi t l ot i parameters drinl ingperiod smol inglnitiationratebyagean cb plain variables statecharts behavior behavior freewandering gotthirsty gotowater drinkwater initialstate state newdir functions presentation main parameters numberofelephants c j q q j q i fi t i li ii qi l cj j i c get support makeupvegetation placeelephants altitude vegetation mapdrawing altitudesdrawn properties ll console environment gjdisplacementtable qiangletable distrdisplacement distrangle updatevegetation vegetationtocolor altcolor viewvegetation el l palette ll el b model i parameter flow aux variable d stock variable event dynamic event plain variable collection variable g function table function l port connector entry point state transition initial state pointer branch v el q plain variables i problems ll l vegetation plain variable history state final state environment cj description i location general description name ivegetation lia show name d ignore d public d show at runtime access li d static d constant lia save in snapshot type initial value boolean o int o double o string othe r doubl e new douhle action analysis t presentation iui connectivity environment environment selection enterprise library i more libraries dimensional reasoning dimensional consistency testing nathaniel osgood cmpt march talk outline motivations dimensional systems dimensional analysis examples discussion general motivations dimensional analysis da critical historically for scoping models formulating models validating models calibrating models systems modeling community has made important but limited use of da strong advantages from opportunities for improved da use specific performance concerns for public health models dimensions and units dimensions describe semantic category of referent e g length weight pressure acceleration etc describe referent independent of size or existence of measure no conversions typical between dimensions a given quantity has a unique dimension units describe references used in performing a particular measurement e g time seconds weeks centuries this is metadata describes measured value relates to a particular dimension describe measurement of referent dimensional constants apply between units a given quantity can be expressed using many units even dimensionless quantities can have units units dimensions frequency dimension time units year sec etc angle dimension dimensionless unit units radians degrees etc distance dimension length units meters fathoms li parsecs dimensional homogeneity distinctions adding items of different dimensions is semantically incoherent fatally flawed reasoning adding items of different units but the same dimension is semantically sensible but numerically incorrect requires a conversion factor structure of dimensional quantities dimensional quantity can be thought of as a pair value m where value and m d quantity dimension units can be represented as products of powers of reference dimensions units rate of water flow vectors in a d dimensional vector space of ref dimens each index in the vector represents the exponent for that reference dimension unit dimension dictates the value scaling needed for unit conversion a dimensionless quantity holds the same value regardless of measurement system dimensional quantities have operations that are related to but more restricted than for e g a particularly interesting dimensionality unit dimension recall dimensions associated with quantities can be expressed as product of powers we term quantities whose exponents are all as being of unit dimension another term widely used for this is dimensionless this is somewhat of a misnomer in that these quantities do have a dimension just a very special one analogy calling something of length lengthless such quantities are independent of unit choice dimensionality unit choice exponent for dimension dictates the numerical value scaling required by unit conversion consider x ft and y consider converting from feet to meters x ft m y a dimensionless quantity maintains the same numeric value regardless of measurement system cf fraction unit dimension common quantities of unit dimension fractions of some quantity likelihoods probabilities dimensional space quantities in dimension space time length of line is the value of the quantity length treating all quantities as dimensionless loses information projects purely onto the z dimension stock flow dimensional consistency invariant consider a stock and its inflows and outflows for any flow we must have flow stock time this follows because the stock is the integral of the flow computing this integral involves summing up many timesteps in which the value being summed is the flow multiplied by time seeking hints as to the dimension associated w a quantity how is it computed in practice what steps does one go through to calculate this going through those steps with dimensions may yield a dimension for the quantity would its value need to be changed if we were to change diff units e g measure time in days vs years is there another value to which it is converted by some combination with other values if so can leverage knowledge of dimensions of those other quantities computing with dimensional quantities to compute the dimension units associated with a quantity perform same operations as on numeric quantities but using dimensions units we are carrying out the same operations in parallel in the numerics and in the dimensions units with each operation we can perform it twice once on the numerical values once on the associated dimensions dimensional homogeneity there are certain computations that are dimensionally inconsistent are therefore meaningless key principle adding together two quantities whose dimensions differ is dimensionally inhomogeneous inconsistent meaningless by extension ab is only meaningful if b is dimensionless derivation ab a e e b a e beb a e b b the expression on the right is only meaningful if b dimensional notation within this presentation we ll use the notation x d to indicate quantity x is associated with dimension d for example x y person time z 𝑎 𝑏 𝑐 𝑑 example to compute the dimensions we proceed from inside out just as when computing value suppose further that a person b person time c time d b c b c person time time person a b c a b c person person person thus the entire expression has dimension a b c d a b c d d person lotka volterra model variables dimensions fox time hare time time h hf h f hf f cf frequency of oscillations time clearly cannot depend on or because these parameters would introduce other dimensions those dimensions could not be cancelled by any other var the exponent of time in is by symmetry the period must depend on both and which suggests classic sir model s cs i s i r variables dimensions s i r person i cs i i s i r a likelihood c person time person time r i just as could be calculated from data on contacts by n people over some time interval μ time note that the force of infection c s i i rha units time which makes sense firstly multiplying it by s must give rate of flow which is person time secondly the reciprocal of such a transition hazard is just a mean duration in the stock which is a time dimension must be time indicating units associated with a variable in vensim accessing model settings vensim dimensional analysis mdl var file edit view l out i model ro ptio n w in do ws h elp simulate ctrl r total start synthesim ctrl b population reality check sto ll j i j ipn ntacts per year i po dajlas et prev aelnce of from dat format infection birth rate total population l i times new roman choosing model time units file edit view layout model options windows help i current use sketch to set imt al causes c time bounds pswd i sketchi units equiv i xls files i rel modes i o time bounds for model initial time lo final time time step j b p save results every time step or use saveper i units for time j integr ation type runi note quarter month to change later week above parameter day hour minute second ings or edit the equations for the r births birth rate mean time until recovery total population setting unit equivalence requesting a dimensional consistency check confirmation of unit consistency indication of likely dimensional inconsistency vensim interface vensim will perform dimensional simplification via simple algebra on dimensional expressions e g person person is reduced to in some vensim modes when the mouse hovers over a variable vensim will show a pop up tab tip that shows the dimension for that variable vensim can check many aspects of dimensional consistency of a model vensim capabilities associate variables with units define new units beyond built in units e g person deer bird capsule define unit equivalence e g day days tutorial functions and functional abstraction nathaniel osgood cmpt building the model right some principles of software engineering technical guidelines try to avoid needless complexity use abstraction encapsulation to simplify reasoning development name things carefully design code for transparency modifiability document create self documenting results where possible consider designing for flexibility use defensive programming use type checking to advantage subtyping and sometimes subclassing to capture commonality for unit checking where possible process guidelines use peer reviews to review code design tests perform simple tests to verify functionality keep careful track of experiments use tools for version control documentation referent integrity do regular builds system wide smoke tests integrate with others work frequently in small steps use discovery of bugs to find weaknesses in the q a process the challenges of complexity complexity of software development is a major barrier to effective delivery of value complexity leads to systems that are late over budget and of substandard quality complexity has extensive impact in both human technical spheres why modularity as a way of managing complexity allows decoupling of pieces of the system separation of concerns in comprehension reasoning example areas of benefit code creation modification testing review staff specialization modularity allows divide and conquer strategies to work as a means to reuse abstraction key to modularity abstraction is the process of forgetting certain details in order to treat many particular circumstances as the same we can distinguish two key types of abstraction abstraction by parameterization we seek generality by allowing the same mechanism to be adapted to many different contexts by providing it with information on that context abstraction by specification we ignore the implementation details and agree to treat as acceptable any implementation that adheres to the specification liskov guttag a key motivator for abstraction risk of change abstraction by specification helps lessen the work required when we need to modify the program by choosing our abstractions carefully we can gracefully handle anticipated changes e g choose abstracts that will hide the details of things that we anticipate changing frequently when the changes occur we only need to modify the implementations of those abstractions abstraction by parameterization major benefit reuse common needs identified elimination of need to separately develop test review debug diverse forms functions formal parameters generics parameterized types cross cutting aspects parameterized by pointcuts types of abstraction in java functional abstraction action performed on data we use functions in oo methods to provide some functionality while hiding the implementation details we are concentrating on this today interface class based abstraction state behaviour we create interfaces classes to capture behavioural similarity between sets of objects e g agents the class provides a contract regarding nouns adjectives the characteristics properties of the objects including state that changes over time verbs how the objects do things methods or have things done to them functional abstraction functional abstraction provides methods to do some work what while hiding details of how this is done a method might compute a value hiding the algorithm test some condition hiding all the details of exactly what is considered and how e g ask if a person is susceptible perform some update on e g a person e g infect a person simulate the change of state resulting from a complex procedure transmit infection to anther return some representation e g a string of or information about a person in the model why use functional abstraction easier modifiability only one place to update transparency what the code does is clearer reduced clutter throughout code don t have to look at all the gory details every time want to undertake this task can communicate intention from clear name easier later reuse reduced complexity lowers risk of programming error using functional abstraction in anylogic methods methods are functions associated with a class methods can do either or both of computing values performing actions printing items displaying things changing the state of items consist of two pieces header says what types the method expects as arguments and returns as values and exceptions that can be thrown body describes the algorithm code to do the work the implementation method bodies method bodies consist of variable declarations statements statements are commands that do something effect some change for example change the value of a variable or a field return a value from the function call a method perform another set of statements a set of times based on some condition perform one or another set of statements using functional abstraction in anylogic example functions a function definition another example a closer look at the code what is called a function in anylogic is classically called a method parameterization we can parameterize functions so that the values that they yield depends on the values passed to them as arguments by callers this allows flexibly a function can be used somewhat differently in different contexts while parameters may differ the behavior of the function will typically be the same examples of parameterization we may build a function that identifies all people who have been smokers for more than n years n here is a parameter different contexts we might be interested in different n we may wish to count the number of people of a certain sex rather than independently creating separate methods for males and females we may create a method that is called countpopulationofsex that takes a parameter that specifies the sex of interest a hierarchy of functional abstractions we build up higher level functional abstractions out of lower level ones for example the implementation of fractionofcontactsthatsmoke might make use of countsmokingcontacts and countcontacts we might define countmen and countwomen with implementation of both calling countpopulationofsex particularly powerful functional abstractions are those which are parameterized by functions in object oriented programming we generally do this by using polymorphism passing objects that match some interface but whose implementation of that interface can differ more anylogic java events java types and enums nathaniel osgood cmpt march reminder rates events rates and timeouts are associated with types of events in anylogic events can also be declared explicitly from the pallette dynamic events can have multiple instances each instance can be scheduled at the same time the instances disappear after event firing regular static events can be rescheduled enabled disabled but can only have one scheduled firing at a time there are some subtleties with events built in events in addition to handling occurrence of explicit events models automatically support catching certain built in types of events to handle these events code is inserted into certain handler areas for each of different sorts of classes example built in events agent handler code is executed when the specified event e g arrival at a destination message arrival occurs example built in events agent example built in events main types in java types tell you the class of values from which a variable is drawn in java we specify types for parameters variables return values class fields typically we encode information described by elements of one or more different types types legal operations for a given type only certain operators can be used e g e g a double precision value can be divided multiplied turned into a string etc a boolean can be tested for truthhood turned into a string etc a reference to a string can be used to extract prefixes or suffixes find the length concatenated etc an enum values can be turned turned into a string converted to integer etc java primitive types these are built in to the java language primitive types in java are the following boolean double short small integer int char byte long float non primitive types most types we used are not primitive types these are defined either in our code in the standard java libraries why types like specifying dimensions for an object e g l t specifying types lets us know what we re dealing with so we know what to do with it avoid making a silly mistake e g attempting to divide a number by a reference to a person absent types it is likely that these mistakes wouldn t be identified until runtime if we don t happen to test that portion of the program we won t be aware of the error with types we can discover these errors when we are building the program during our build type coercion casting why sometimes we have something that is a member of one type but that can be logically converted to another type examples we have a double precision value and we wish to convert it instead to an integer by dropping fractional component we have an integer or a double char boolean etc and wish to convert it to a string subtyping we have an activeobject that we know is a person and wish to treat it as a person type coercion casting how to cast a value v in one type to another type the following syntax is used targettype v examples traceln string age female item statechart isstateactive pregnant int age parameterized types sometimes a type a is defined in terms of another type b this allows the definition of a to take give back information specific to type b e g methods take an a as a parameter or return a b etc common example collections depending on the type of their content we say that the type a is parameterized by type b we can describe such parameterized types using java generics syntax used a b examples of parameterized type generics a resource pool depending on what resources are included resourcepool myresourceunit an array list like an extensible vector depending on the type of the elements arraylist person hypothetical a pair defined in terms of the first and second element pair string double exam le of a parameterized t file edit view model window help al al mainphasez qi person person java male java al elephant a simulation ophthalmology departmen d mainphasel e rsl fl ports embedded objects lij source i j sink qj networkenter lij networkexit p network ii doctor p procroom qj scope lij procedure p movetoexit release movetoexit rt ll flli v network networkexit sink i t presen aa line j polyline r curve d rectangle round re oval arc qj movetoprocrc presentation d mainphasez d ports embedded objects presentation image li l doctor procroom scope properties l console pixel ala text image group button v el check box f networkgroup d waitinghall d staffroom qj doctor networkresourcepool general edit box radio butt op slider d storagero d procrooml parameters name show name d ignore d public show at runtime create presentation di combo box d procroom t a t i ic p e o r kre c l a st re r c e n t i list box file chooser d exit d r ec tang description package icom xj anylogic libraries enterprise progress bar l problems l i description i resource type capacity defined capacity speed new resource unit moving j directly by home shape by table over time new resourceunit iii cad draw gis map i ii connectivity i o n new unit tio n seize i selection tf enterprise j more libraries enums why often we desire in our models to encode categorical information we can certainly encode such information using integers or shorts etc e g male female province nl nb pei qc etc example using variables int sex int province problem this is fragile we could easily mistake a value as encoding either males or newfoundland labrador e g reversing order of parameters given to a method or entered into a file assigning value for one to another due to a poorly named values e g sex province enums in java enums let us give names to such information refer to the names in our code convert the names where necessary into their associated values compare names define operations on names simplest examples enum sex male female enum province nl nb pei qc on mb sk ab bc variables using enum sex sex province province causes error sex province basics of java expressions statements nathaniel osgood cmpt february java as a formal language java supports many constructs that serve different functions class interface declarations importing references to classes from other code libraries defining methods methods methods are functions associated with a class methods can do either or both of computing values performing actions printing items displaying things changing the state of items consist of two pieces header says what types the method expects as arguments and returns as values and exceptions that can be thrown body describes the algorithm code to do the work the implementation method bodies method bodies consist of variable declarations statements statements are commands that do something effect some change for example change the value of a variable or a field return a value from the function call a method perform another set of statements a set of times based on some condition perform one or another set of statements variable declarations variables in java are associated with types and can contain values the types describe the sort of values that a variable can contain the set of possible values e g double double precision floating point numbers int positive negative integer values within some range string a reference to a text sequence boolean a dichotomous value holding true or false when we declare a variable we indicate its name type and possibly an initial value example variable declarations finding location in continuous space x y in terms of discrete vegetation space c r poor style should be in separate function declares a variable m that initially contains a reference to the main object declares double precision variables x y declares integer values c r and sets equal to the column row for this elephant in the vegetation array common java statements if for while do while try catch finally throw trigger exception an expression typically side effecting assignment call to a function composite statement block multiple statements enclosed in a common java expressions literal my string null causes changes side effecting assignment a b left hand side is some location variable field etc comparison a b a b mathematical operators can be overloaded to mean other things e g as concatenation method call function call this dereferencing looking up field or value b in an object expression a a b ternary operator predicate a b comments comments in java are indicated in two different ways arbitrarily long begun with and ended with these can span many lines within a line after a use comments to describe your intentions boundaries water poor style entire logic conditions checks on boundaries whether water rerouting logic should all be in separate functions from this from each other remove constants for statements for statements iterate repeatedly executing some inner statement many times several variants are available for int i i i statement for int i collection statement heading towards resource determining current position searching for quickest way to find water from that position should be in separate function if statements with an if statement one tests a condition predicate and based on the result either executes one statement or another possibly empty statement if condition if condition true statement or true statement else false statement falls through to later code if condition is false handling of movement logic finding location in continuous space x y in terms of discrete vegetation space c r poor style should be in separate function handling the case of reaching water when thirsty distinguishing the case of many few trees boundaries water poor style entire logic conditions checks on boundaries whether water rerouting logic should all be in separate functions from this from each other remove constants a more complex condition should really place condition in functions that returns a boolean and just call the functions can reuse elsewhere new direction change function info l j i i i j liiu file edit view model window help t e l t l ct c q c j q l if iq qi co i jr get support iiio t i t t project n ia search i el al main lillimlmll e l l palette n i el wandering elephants a elephant f behavior i t model i parameters drinkingperiod i drinkingperiod freewandering parameter smokinginitiationratebyageandsmokingstatus flow aux variable smokinginitiationratebyagean d stock variable plain variables thirsty event statecharts ir behavior headin grandom gotthirsty t drinkwater plain variable freewandering g collection variable gotthirsty headingtowater gotowater ci function gotowater table function drinkwater l port initialstate connector state newdir entry point ij functions i state ii presentation i transition main parameters rt initial state pointer numberofelephants h plain variablesi properties n i console i v el branch history state i ci headingrandom function final state e environment general name iheadingrandom i show name d ignore d public show at runtime ie scri t ion l flo cation code description access i default i d static return type void boolean oint double string oother i void i function arguments action i d analysis i d m i presentation i d ii connectivity i i j i selection i j l lf enterprise library i more libraries new direction change function body setting agent speed set so as to reach target in fixed time until next target shift initiates movement towards randomly chosen destination while do while loop executes a statement as long as some condition is true the classic while loop has the test at the beginning the do while has the test at the end of the loop while loops ii i file edit view model window help j j i p j lli i il qi qi j j j t get support i project l id search i el main elephant l i el ii palette l i el elephant f i mo de l dd para meters b ehavior drinkingperiod para meter smokinglnitiationrateb yagean c drin kingperiod freewandering plain variables smokinglnitiationratebyageandsmokingstatus flow au x variable d stock variable behavior ir behavior dynamic event freewandering gotl hirsty headingrandom gotl hirst y drinkwater plain va ri able f l collection variable gotowater g headingl owater golowater g function drinkwater ci table function initialstate i i port newdir t j j i console i v el connect or functions entry point presentat ion elephant active object class state main parameters numberofelephants general advanced m vegetation c r avoid bounds and water change direction if needed tran siti on initial state pointer plain variables agent if x ii x y y ii m altitude c r branch functions parame ters stop history sta te i i ii problems l i e j el descript ion try new heading until find a valid one final state head ing environment xtry ytry i description i location int coun t o do if count error cound not find way out heading uniform math pi math p i action xtry x cos heading analysis ytry y in heading count i i prese ntation e xtry ii xtry ii ytry ii ytry m altitude int xtry o connectivity and start moving in the new direction to a virtual distant target this will be st moveto x cos head ing y o in heading i enterpr ise libr ary t i i j i i i more libraries j compound statements delineated by innermost is not actually needed because only one statement could remove and the statement inside would still be within the if consequent expression statements assignment expressions as an expression statements including count which is equivalent to count count method call expression as an expression statement delays part equilibrium behaviour higher order delays nathaniel osgood cmpt our route forward common types of delay related dynamics first order delays competing risks delays oscillations first order delays in action simple sit model mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible incidence rate total population prevalence recovery delay initial population t new infections new recovery new illness department of computer science cumulative illnesses newly susceptible immunity loss delay first order delays in action simple sit model recall simple first order decay use initial value pevoiprulelewntith meadnetiamthe until infection dienaftehcstiforonm use formula people with virulent infection mean time until death first order decay variant of last time recall how does this relate to the mean time until death use value use initial value use formula people with virulent infection per month likelihood of death people in stock flow rate of deaths cumulative deaths closeup why this gap per month risk of deaths why this gap use value use value immigrationrate immigration adnenautahl ralipskhao f people x deaths use initial value use formula people x annual risk of death alpha questions what is behaviour of stock x what is the mean time until people die suppose we had a constant inflow what is the behaviour then answers people x behaviour of stock people x baseline mean time until death time year recall that if coefficient of first order delay is then mean time is here years equilibrium value of a first order delay suppose we have flow of rate i into a stock with a first order delay out this could be from just a single flow or many flows the value of the stock will approach an equilibrium where inflow outflow equilibrium value of order delay recall outflow rate for order delay x note that this depends on the value of the stock inflow rate i at equilibrium the level of the stock must be such that inflow outflow for our case we have x i thus x i the lower the chance of leaving per time unit or the longer the delay the larger the equilibrium value of the stock must be to make outflow inflow scenarios for first order delay variation in inflow rates for different immigration inflows what do you expect inflow inflow inflow inflow why do you see this goal seeking pattern what is the goal being sought behaviour of stock for different inflows why do we see this behaviour behaviour of outflow for different inflows why do we see this behaviour imbalance gap causes change to stock rise or fall change to outflow to lower gap until outflow inflow goal seeking behaviour the goal seeking behaviour is associated with a negative feedback loop the larger the population in the stock the more people die per year if we have more people coming in than are going out per year the stock and hence outflow rises until the point where inflow outflows if we have fewer people coming in than are going out per year the stock declines outflow declines until the point where inflow outflows as a causal loop diagram what does this tell us about how the system would respond to a sudden change in immigration response to a change feed in an immigration step function that rises suddenly from to at time set the initial value of stock to how does the stock change over time create a custom graph display it as an input output object editing create input output object for synthesim stock starting empty finlfoloww anrdaotuetflsow idmeamthigsr atsiotenp how would this change with alpha stock starting empty value of stock alpha how would this change with alpha for different values of alpha flow rates outflow rises until inflow this is for the flows what do stocks do for different values of alpha value of stocks why do we see this behaviour a longer time delay or smaller chance of leaving per unit time requires x to be larger to make outflow inflow outflows as delaved version of inputs in flo v and outflo v inflovv and outflovv iq rn time y ecn lq q q lq sq lqq time year illlfilligrn lion step functimu yr d elay dea ths step yr delay mmiigrnrtr ion step yr clejla y eaths step yr d ejla y inflow and outflow rn inflow and outflow q time y eru rn time y ear nrumgratfon step funoliorn q yr delay deaths step functions yt d elay what if stock doesn t start empty decays at first no inflow then output responds with delayed versi people x o i p e e x s unc ions at i i l x alpha people x step functions at initial x alpha deaths step functions at alpha higher order delays aging chains moving beyond the memoryless assumption recall that first order delays assume that the per time unit risk of transitions to the outflow remains equal throughout simulation i e are memoryless problem often we know that transitions are not memoryless e g it may be the transition reflects some physical delays not endogeneously represented e g slow growth of bacterial buildup of damage of high blood sugars glycosylation higher orders of delays we can capture different levels of delay with increasing levels of fidelity using cascaded series of order delays we call the delay resulting from such a series of k order delays a kth order delay e g first order delays in series yield a order delay the behaviour of a kth order delay is a reflection of the behaviour of the order delays out of which it is built to understand the behaviour of kth order delays we will keep constant the mean time taken to transition across the entire set of all delays recall simple order decay meadnetiamthe until pevoiprulelewntith infection dienaftehcstiforonm initial value use formula people with virulent infection mean time until death recall order delay behaviour conditional transition prob for a order delay the per time unit likelihood of leaving given that one has not yet left the stock remains constant unconditional transition prob for a order delay the unconditional per time unit likelihood of leaving declines exponentially i e if were were originally in the stock our chance of having left in the course of a given time unit e g month declines exponentially this reflects the fact that there are fewer people who could still leave during this time unit recall order delay behaviour per month chance of transitioning out during this month likelihood of still being in system order delay use formula mean time to transition across all stages stage count use value of use value of initial value initial value order delay per month chance of transitioning out during this month likelihood of still being in system order delay order delay per month chance of transitioning out during this month likelihood of still being in system through orderdelays per month chance of transitioning out during this month likelihood of still being in system mean times to depart final stage mean time of k stages is just k times mean time of one stage e g if the mean time for leaving stage requires time mean time for k k in our examples as we added stages we reduced the mean time per stage so as to keep the total constant i e if we have k stages the mean time to leave each stage is k times what it would be with just stage infinite order delay as we add more and more stages k the distribution of time to leave the last stage approaches a normal distribution if we reduce the mean time per stage so as to keep the total time constant this will approach an impulse function this indicates an exactly fixed time to transition through all stages distribution of time to depart final stage the distributions for the total time taken to transition out of the last of k stages are members of the erlangdistribution family these are the same as the distribution for the kthinterarrival time of a poisson process k gives exponential distribution first order delay as k approaches normal distribution gaussian pdf from wikipedia notes we do not generally define kthorder delays simply as a means to the end of capturing a certain distribution often representing each stage for its own sake is desirable see examples different causal influences often we represent each such stage as a order delay with that proviso many modeling packages including vensim directly support higher order delays use with caution slides adapted from external source redacted from public pdf for copyright reasons delays competing risks competing risks suppose we have another outflow from the stock how does that change our mean time of proceeding specifically down flow here developing diabetes daianbneutiaclmrioskrtaolifty deapthospoufladtioianbetic dmeveealnoptiemsertod diabettiocsepsrrodgressing basic dynamics dmeveealnoptiemsertod diabettiocsepsrrodgressing daianbneutiaclmrioskrtaolifty deapthospoufladtioianbetic effect of doubling diabetic mortality rate effect on progression rates to esrd do the two scenarios have the same or different mean times to develop esrd if different which scenario is larger why the lower mean time why is the mean time to transition different despite the fact that we didn t change the transition parameter mathematical explanation following slides calculation of mean time varies with mortality rate intuition higher death rate diabetic population will rapidly decrease transitions to esrd will be skewed towards earlier transitions earlier mean time to transition lower death rate diabetic population will decrease less rapidly many will make later transitions to esrd later mean time to transition competing risks stock trajectory solution procedure dx x x dt x suppose we start x at time with initial value x and we want to find the value of x at time t this is just like our previous differential equation except that has been replaced by the solution must therefore be the same as before with the appropriate replacement thus x t x e t mean time to leave competing risks p t dt here is the likelihood of a person leaving via flow e g developing esrd exactly between time t dt t we start the simulation at t so p t for t for t p leaving on flow exactly between time t dt t p leaving on flow exactly between time t t dt still have not left by time t p still have not left by time t for t p still have not left by time t e t for p leaving exactly between time t and t dt still have not left by time t recall for us probability of leaving in a time dt always dt thus p leaving exactly between time t and t dt still have not left by time t dt p t dt p leaving exact b t time t dt t e t dt mean time to transition via flow competing risks by the same procedure as before we have t e p t t te t dt using the formula we derived for the integr al expression we have e p t note that this correctly approaches the single flow case as aging chains including successive order delays competing risks in our model of chronic kidney disease infectious disease model wrapup cmpt february case outbreak people people people people people people people people people people people people people people people sir example time days susceptible population s sir example people infectious population i sir example people recovered population r sir example people case infection declines immediately infectives time month infectives infection extinction recall closed population no birth death infection always dies out in the population some infections will take longer to die out there is a tipping point between two cases of people infected declines out immediately infection causes an outbreak before the infection dies down of people infected rises and then falls recall simple model incorporating population turnover contacts per susceptible infection fractional prevalence mean time with disease susceptible incidence infective mortality recovery recovered recovered mortality mortality mortality rate mortality rate recall our model set c people month chance of transmission per s i contact μ birth and death rate initial infectives other susceptible here the infection can remain endemic blue susceptible red infective green force of infection this is the point where rate of new infections rate of recoveries a person infects on average person before recovering the level of susceptibles is at the lowest level where the infection is sustainable in the short run at this point susceptibles susceptibles at endemic equilibrium why is the of susceptibles rising to well above its sustainable value why is the of susceptibles still declining this fraction of susceptibles at endemic equilibrium is the minimum sustainable value of susceptible i e the value where the properties above hold above this fraction of susceptibles the infected will rise below this fraction of susceptibles the infected will fall the rise is occurring because infectives are so low that so few infections occur that births infections deaths s rises above the sustainable value because infectives are still in decline until that point so infectives remain low for a while the susceptibles are still declining here because the large of infectives still causes enough infections that rate of immigration rate of infections deaths equilibrium behaviour with births deaths the system can approach an endemic equilibrium where the infection stays circulating in the population but in balance the balance is such that simultaneously the rate of new infections the rate of immigration otherwise of susceptibles would be changing the rate of new infections the rate of recovery otherwise of infectives would be changing tipping point now try setting transmission rate β to case infection declines immediately infectives time month infectives infection extinction recall simple model incorporating population turnover contacts per susceptible infection fractional prevalence mean time with disease susceptible incidence infective mortality recovery recovered recovered mortality mortality mortality rate mortality rate recall our model set c people month chance of transmission per s i contact μ birth and death rate initial infectives other susceptible here the infection can remain endemic damped oscillatory behavior modify model to have births and deaths with an annual birth and death rate set model settings final time to long time frame in synthesim running man mode set birth death rates exploring the tipping point now try setting transmission rate β to infection extinction as for the case with a closed population an open population has two cases infection dies out immediately infectives outbreak infection takes off time month here in contrast to the case for a closed population the infection will typically go to an endemic equilibrium table t lnter epi kmic period t of some common infections from anders n and may and theoretical predictions of the period eqn t s inter epidemic period t years infection observed geographical location and time period average age at infection latent plus infectious period d d days inter epidemic period t years calculated measles england and wales i aberdeen scotland l mumps poliomyelitis echovirus type il smallpox chickenpox coxsackie virus type england and wales baltimore usa england and wales england and wa es india l new york city usa glasgow scotland england and wales mrcoplasma pneumoniae england and wales i a u j c la o o r t qt time i yr typically in endemic equilibrium the uninfected fraction of the population s n is the young u l l o c j baste reproductive rate vrg the peak fraction infected y ax and the fraction ever infected plotted a functions of see text and eqns and table estimated values of the basic reproductive rate r for various infections data from anderson anderson and may anderson et al nokes and anderson delays for a while after infectives start declining i e susceptibles are below sustainable endemic value they still deplete susceptibles sufficiently for susceptibles to decline for a while after susceptibles are rising until susceptibles endemic value infectives will still decline for a while after infectives start rising births of infections susceptibles will rise to a peak well above endemic level infection recall for this model a given infective infects c s n others per time unit this goes up as the number of susceptibles rises questions if the mean time a person is infective is μ how many people does that infective infect before recovering with the same assumption how many people would that infective infect if everyone else is susceptible under what conditions would there be more infections after their recovery than before fundamental quantities we have just discovered the values of famous epidemiological quantities for our model effective reproductive number r basic reproductive number effective reproductive number r number of individuals infected by an index infective in the current epidemological context depends on contact rates frequency transmission probability length of time infectives fraction of susceptibles affects whether infection spreads if r of cases will rise if r of cases will fall alternative formulation largest real eigenvalue endemic rate basic reproduction number number of individuals infected by an index infective in an otherwise disease free equilibrium this is just r at disease free equilibrium all other people in the population are susceptible other than the index infective depends on contact number transmission probability length of time infected affects whether infection spreads if epidemic takes off if epidemic dies out alternative formulation largest real eigenvalue initial infection rise exp t d endemic rate basic reproductive number if contact patterns infection duration remain unchanged and if fraction f of the population is susceptible then mean of individuals infected by an infective over the course of their infection is f in endemic equilibrium inflow outflow s n every infective infects a replacement infective to keep equilibrium just enough of the population is susceptible to allow this replacement the higher the the lower the fraction of susceptibles in equilibrium generally some susceptibles remain at some point in epidemic susceptibles will get so low that can t spread open closed population our model set c people month chance of transmission per s i contact μ birth and death rate initial infectives other susceptible what is what should we expect to see thresholds r too low susceptibles r of infectives declining too high susceptibles r of infectives rising infection is introduced from outside will cause outbreak herd immunity infection is introduced from outside will die out may spread to small number before disappearing but in unsustainable way this is what we try to achieve by control programs vaccination etc outflow from susceptibles infections is determined by the of infectives equilibrium behaviour with births deaths the system can approach an endemic equilibrium where the infection stays circulating in the population but in balance the balance is such that simultaneously the rate of new infections the rate of immigration otherwise of susceptibles would be changing the rate of new infections the rate of recovery otherwise of infectives would be changing disease free equilibria no infectives in population entire population is susceptible endemic steady state equilibrium produced by spread of illness assumption is often that children get exposed when young the stability of the these equilibria whether the system departs from them when perturbed depends on the parameter values for the disease free equilibrium on infectious disease models nathaniel osgood cmpt march comments on mathematics dynamic modeling many accomplished well published dynamic modelers have very limited mathematical background can investigate pressing important issues software tools are making this easier over time can gain extra insight flexibility if willing to push to learn some of the associated mathematics achieving highest skill levels in dynamic modeling do require mathematical facility and sophistication to do sophisticated work often those lacking this background or inclination collaborate with someone with background applied math dynamic modeling although you may not use it the dynamic modeling presented rests on the tremendous deep rich foundation of applied mathematics linear algebra calculus differentia integral uni multivariate differential equations numerical analysis including numerical integration parameter estimation control theory real complex analysis for the mathematically inclined the tools of these areas of applied math are available models in mathematical epidemiology of infectious disease long influential modeling tradition beginning with kermack mckendrick models formulated for diverse situations cf anderson may latent incubation period diversity in contact rates heterogeneity preferential mixing vaccinated groups zoonoses variations in clinical manifestations network structure important tradition of closed form analysis mathematical models link together diverse factors typical factors included infection mixing transmission development loss of immunity both individual and collective natural history often multi stage progression recovery birth migration aging mortality intervention impact sometimes included preferential mixing variability in contacts strain competition cross immunity quality of life change health services interaction local perception changes in behavior attitude immune response emergent characteristics of infectious diseases models instability nonlinearity tipping points oscillations multiple fixed points equilibria endemic equilibrium disease free equilibrium oscillations delays the oscillations reflect negative feedback loops with delays these delays reflect stock and flow considerations and specific thresholds dictating whether net flow is positive or negative stock flow stock continues to deplete as long as outflow exceeds inflow rise as inflow outflow the stock may stay reasonably high long after inflow is low key threshold r when of individuals being infected by a single infective this is the threshold at which outflows inflows jan mar may july sept nov jan mar may july sept nov jan mar may july sept nov jan mar may july sept nov jan mar may july sept nov jan mar may july sept nov jan mar may july sept nov jan mar may july sept nov saskatchewan childhood diseases p o p o b b b s o o p i d d d z d 1921 l t i i t i l v j r c n ii it ii i id l j i i l j j t l l slides adapted from external source redacted from public pdf for copyright reasons nonlinearity in state variables effect of multiple policies non additive doubling investment does not yield doubling of results leads to multiple basins of tracking equilibrium multiple equilibria tipping points separate basins of attraction have qualitatively different behaviour oscillations endemic equilibrium disease free equilibrium equilibria disease free no infectives in population entire population is susceptible endemic steady state equilibrium produced by spread of illness assumption is often that children get exposed when young slides adapted from external source redacted from public pdf for copyright reasons tb in sk example stis hc workers hc workers hc workers kendrick mckermack model partitioning the population into broad categories susceptible s infectious i removed r births contacts per susceptible per contact risk of infection force of infection incidence fractional prevalence mean time with disease recovery initial population initial fraction vaccinated mortality rate mortality rate population fractional prevalence shorthand for key quantities for infectious disease models stocks i or y total number of infectives in population this could be just one stock or the sum of many stocks in the model e g the sum of separate stocks for asymptomatic infectives and symptomatic infectives n total size of population this will typically be the sum of all the stocks of people s or x number of susceptible individuals key quantities for infectious disease models parameters contacts per susceptible per unit time c e g contacts per month this is the number of contacts a given susceptible will have with anyone per infective with susceptible contact transmission probability this is the per contact likelihood that the pathogen will be transmitted from an infective to a susceptible with whom they come into a single contact intuition behind common terms i n the fraction of population members or by assumption contacts that are infective important simplest models assume that this is also the fraction of a given susceptible contacts that are infective many sophisticated models relax this assumption c i n number of infectives that come into contact with a susceptible in a given unit time c i n force of infection likelihood a given susceptible will be infected per unit time the idea is that if a given susceptible comes into contact with c i n infectives per unit time and if each such contact gives likelihood of transmission of infection then that susceptible has roughly a total likelihood of c i n of getting infected per unit time e g month key term flow rate of new infections this is the key form of the equation in many infectious disease models total of susceptiblesinfected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n note that this is a term that multiplies both s and i this is much different than the purely linear terms on which we have previously focused likelihood is actually a likelihood density e g can be indicating that mean time to infection is another useful view of this flow recall total of susceptibles infected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n the above can also be phrased as the following s c i n i c s n of infectives average susceptibles infected per unit time by each infective this implies that as of susceptibles falls of susceptibles surrounding each infective falls the rate of new infections falls less fuel for the fire leads to a smaller burning rate basic model structure immigration rate immigration associated feedbacks susceptibles csounstcaecptstibblestwaenedn infectives new infections new recoveries mathematical notation i underlying equations s m c i n i c i i n r i figure from garnett g p an introduction to mathematical models in sexually transmitted disease epidemiology sexually transmitted infections discrete intra agent dynamics statecharts messaging nathaniel osgood february discrete agent dynamics frequently we can represent agent behaviour using as transitioning among a set of states in a state chart for a given simple statechart the agent is in exactly one state at a time fixed transitions between states define possible evolution the transitions between states occur instantaneously based on some condition example state transition diagram discrete agent dynamics transitions many transition conditions are possible timeout spending some period of time in the state fixed rate leave state with some fixed change per unit time this is similar to first order interarrival time and is conceptually linked to the operation of first order delays in stock flow diagrams variable rate if desired we can change the rate over time but anylogic only notices changes when eg agent re enters the state message received we can transition when a message any message or particular type of message is received predicate only transition when condition becomes true these transitions can be conditionally routed via branches conditions can determine to what destination state a particular transition will travel transition type message triggered transition type fixed rate rates flows we ve seen fixed rates before in the form of transition rates in system dynamics models within the system dynamics model a flow out of a stock was commonly set by the multiplication of the stock some rate of transition we use different names for these rates transition rates likelihood of transition per unit time transition e g infection mortality hazard example fixed transition rate hazard example fixed transition rate hazard pevoiprulelewntith meadnetiamthe until the transition rate is the reciprocal of this number i e 𝑀𝑒𝑎𝑛 𝑡𝑖𝑚𝑒 𝑢𝑛𝑡𝑖𝑙 𝐷𝑒𝑎𝑡ℎ infection dienaftehcstiforonm people with virulent infection mean time until death people with virulent infection mean time until death i e people with virulent infection rate first order delays in action simple sit model mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible incidence rate total population prevalence the rates hazards for these flows are just the reciprocal of the corresponding mean time in stock delay recovery delay initial population new infections new recovery new illness department of computer science cumulative illnesses newly susceptible immunity loss delay transition type variable rate example transition rate hazard fixed rates transition hazards with fixed rates we are specifying rates of transitions because we are dealing with the chance that each individual transitions we don t need to multiply by the number of people at risk here there is just person at risk as in sd models these rates can change over time but the statechart needs to be made aware of these changes leave go back into current state circular transition likely trigger change event in agent see manual special elements self transition use if wish to have state register changing out transition rates the self transition will make the state realize that the rate associated with any out transition e g this one has changed transition type fixed residence time timeout example conditional transition the incoming transition into whetherprimaryprogre ssion will be routed to thisoutgoing transitionif this condition is true special elements entry point the associated text is the name of the statechart special elements exit point hands on model use ahead load model alp special elements self transition use if wish to trigger an action w o leaving state the self transition will invoke this action when it occurs tip confirming transition connectivity ensure that both sides of the transition show green circles when connected otherwise may appear connected but will actually be disconnected parallel statecharts by default each statechart evolves independently if coupling is desired can make transitions action dependent on state of other statecharts comparison with aggregate stock flows as for aggregate stocks flow individuals states are discrete unlike aggregate stocks flows one state within a given statechart is active at a time for parallel flows e g comorbidities there is no need for considering all combinations of the possible states we can keep track of how long an individual is in a given state adjust the transition rate accordingly parallel transitions example recording the residence time in a state via a stock with unit inflow i e just accumulates the time present in that state the residence time in the state determines the transition rate out of that state transition rates depending on residence time are generally not possible with aggregate models advanced element hierarchical states predatory prey agent based by xjtek the outermost state captures time since born for natural deaths the middle state captures time since last ate for deaths by hunger eating reenters the inner state transition capture hunting frequency success natural death transition death by hunger note that depends on time in state i e time since last ate eating transition leaves reenters middle state tips on statechart code each state transition has an integer index this by accessed via a static constant holding the name of state within the statechart class statechart statename to determine length of time spent in state statename getlocaltime stateindex to determine current state statechart getactivesimplestate to find out if a state either simple or composite is currently active statechart isstateactive stateindex events in anylogic rates events rates and timeouts are associated with types of events in anylogic events can also be declared explicitly from the pallette dynamic events can have multiple instances each instance can be scheduled at the same time the instances disappear after event firing regular static events can be rescheduled enabled disabled but can only have one scheduled firing at a time there are some subtleties with events event times options for event scheduling manually via restart see following slides when boolean condition changes depends on onchange being called one time can go off at a particular time specified as a calendar time or as a double precision value at some initial time and then cyclically beyond with set timeout period the timeout period is set according to the time unit this goes off after exactly the timeout time at a specified poisson rate interarrival time is exponentially distributed mean time between events is reciprocal of rate i e rate event subtleties be very careful of what you count on for recomputation of rate may think was recomputed but hasn t been event rates and likely event timeout times are only computed occassionally not continuously these are computed when explicitly call event methods start restart onchange when event fires and requires restarting for outgoing transitions when enter a state in a statechart calling reset will disable a rate until re enable e g with call to restart infectious disease models nathaniel osgood cmpt march key quantities for infectious disease models parameters contacts per susceptible per unit time c e g contacts per month this is the number of contacts a given susceptible will have with anyone per infective with susceptible contact transmission probability this is the per contact likelihood that the pathogen will be transmitted from an infective to a susceptible with whom they come into a single contact recall our model set c people month chance of transmission per s i contact μ birth and death rate initial infectives other susceptible slides adapted from external source redacted from public pdf for copyright reasons let adjust several things transmissibility originally to to duration of infection originally to to associated feedbacks susceptibles csounstcaecptstibblestwaenedn infectives new infections new recoveries mathematical notation i people people people example dynamics of sir model no births or deaths sir example people people people people people people people people people people people people time days susceptible population s sir example people infectious population i sir example people recovered population r sir example people shifting feedback dominance susceptibles csounstcaecptstibblestwaenedn infectives new infections new recoveries explaining the stock flow dynamics initially infectives su sceptibles over time more infectives and each infective infects c s n c people on average for each time unit the maximum possible rate the rate of recoveries is in short term infectives grows quickly rate of infection rises quickly positive feedback susceptibles starts to decline but still high enough that each infective is surrounded overwhelmingly by susceptibles so efficient at transmitting fewer susceptibles fewer s around each i rate of infections per i declines many infectives start recovering slower rise to i tipping point of infectives plateaus rate of infections rate of recoveries each infective infects exactly one replacement before recovering in longer term declining of infectives susceptibles lower lower rate of new infections negative feedback change in i dominated by recoveries goal seeking to negative feedback shifting feedback dominance people people people people people people people people people people people people people people people sir example time days susceptible population s sir example people infectious population i sir example people recovered population r sir example people damped oscillatory behavior modify model to have births and deaths with an annual birth and death rate set model settings final time to long time frame in synthesim running man mode set birth death rates slides adapted from external source redacted from public pdf for copyright reasons delays for a while after infectives start declining i e susceptibles are below sustainable endemic value they still deplete susceptibles sufficiently for susceptibles to decline for a while after susceptibles are rising until susceptibles endemic value infectives will still decline for a while after infectives start rising births of infections susceptibles will rise to a peak well above endemic level this is the point where rate of new infections rate of recoveries a person infects on average person before recovering the level of susceptibles is at the lowest level where the infection is sustainable in the short run at this point susceptibles susceptibles at endemic equilibrium why is the of susceptibles rising to well above its sustainable value why is the of susceptibles still declining this fraction of susceptibles at endemic equilibrium is the minimum sustainable value of susceptible i e the value where the properties above hold above this fraction of susceptibles the infected will rise below this fraction of susceptibles the infected will fall the rise is occurring because infectives are so low that so few infections occur that births infections deaths s rises above the sustainable value because infectives are still in decline until that point so infectives remain low for a while the susceptibles are still declining here because the large of infectives still causes enough infections that rate of immigration rate of infections deaths equilibrium behaviour with births deaths the system can approach an endemic equilibrium where the infection stays circulating in the population but in balance the balance is such that simultaneously the rate of new infections the rate of immigration otherwise of susceptibles would be changing the rate of new infections the rate of recovery otherwise of infectives would be changing tipping point now try setting transmission rate β to recall kendrick mckermack model partitioning the population into broad categories susceptible s infectious i removed r births contacts per susceptible per contact risk of infection force of infection incidence fractional prevalence mean time with disease recovery initial population initial fraction vaccinated mortality rate mortality rate population fractional prevalence shorthand for key quantities for infectious disease models stocks i or y total number of infectives in population this could be just one stock or the sum of many stocks in the model e g the sum of separate stocks for asymptomatic infectives and symptomatic infectives n total size of population this will typically be the sum of all the stocks of people s or x number of susceptible individuals intuition behind common terms i n the fraction of population members or by assumption contacts that are infective important simplest models assume that this is also the fraction of a given susceptible contacts that are infective many sophisticated models relax this assumption c i n number of infectives that come into contact with a susceptible in a given unit time c i n force of infection likelihood a given susceptible will be infected per unit time the idea is that if a given susceptible comes into contact with c i n infectives per unit time and if each such contact gives likelihood of transmission of infection then that susceptible has roughly a total likelihood of c i n of getting infected per unit time e g month key term flow rate of new infections this is the key form of the equation in many infectious disease models total of susceptiblesinfected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n note that this is a term that multiplies both s and i this is much different than the purely linear terms on which we have previously focused likelihood is actually a likelihood density e g can be indicating that mean time to infection is another useful view of this flow recall total of susceptibles infected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n the above can also be phrased as the following s c i n i c s n of infectives average susceptibles infected per unit time by each infective this implies that as of susceptibles falls of susceptibles surrounding each infective falls the rate of new infections falls less fuel for the fire leads to a smaller burning rate basic model structure immigration rate immigration underlying equations s m c i n i c i i n r i slides adapted from external source redacted from public pdf for copyright reasons events agent movement over landscapes cmpt february example of processes associated with fixed timeouts aging tightly defined time constants associated with natural history while these may be described as associated with a broad distribution e g with a or order delay much of that variability may be due to heterogeneity for a given person these may be quite specific in duration can capture through a timeout events in anylogic rates events rates and timeouts are associated with types of events in anylogic events can also be declared explicitly from the pallette dynamic events can have multiple instances each instance can be scheduled at the same time the instances disappear after event firing regular static events can be rescheduled enabled disabled but can only have one scheduled firing at a time there are some subtleties with events event times options for event scheduling manually via restart see following slides when boolean condition changes depends on onchange being called one time can go off at a particular time specified as a calendar time or as a double precision value at some initial time and then cyclically beyond with set timeout period the timeout period is set according to the time unit this goes off after exactly the timeout time at a specified rate poisson arrivals interarrival time is exponentially distributed mean time between events is reciprocal of rate i e rate event subtleties be very careful of what you count on for recomputation of rate may think was recomputed but hasn t been event rates and likely event timeout times are only computed occasionally not continuously these are computed when explicitly call event methods start restart onchange when event fires and requires restarting for outgoing transitions when enter a state in a statechart calling reset will disable a rate until re enable e g with call to restart agent movement over landscapes cmpt february landscape movement two options continuous movement e g wandering elephants no physical exclusion agents are assumed to be small compared to landscape scale and can pass without interfering agents move in a direction with some speed discrete cells e g agent based predatory prey divided into columns and rows physical exclusion only one agent in a cell at a time agents move from cell to cell key factor environment the anylogic environment sets the parameters for the nature of the landscape width breadth continuous vs discrete character of discrete neighbourhoods cardinal directions vs euclidian n ne e se s sw w nw environment by comparison discrete environment note extra presence of columns and rows hands on model use ahead load model wandering elephants alp environment landscape information agent movement periodic movement changes new direction change function info new direction change function body setting agent speed set so as to reach target in fixed time until next target shift initiates movement towards randomly chosen destination looking at body of this function method heading towards resource determining current position searching for quickest way to find water from that position should be in separate function initiates movement towards chosen destination resumption of wandering after slaking thirst handling of movement logic finding location in continuous space x y in terms of discrete vegetation space c r poor style should be in separate function handling the case of reaching water when thirsty boundaries water poor style entire logic conditions checks on boundaries whether water rerouting logic should all be in separate functions from this from each other remove constants environment updating vegetation continuous space relevant methods to call on agent already covered moveto x y setvelocity v basic info getx gety setxy x y initial location jumpto x y moves agent to location ismoving gettargetx gettargety where heading to setrotation getrotation environment happens to handle process of maintaining environmental dynamics hands on model use ahead load model schelling segregation alp a model to examine the emergence of segregation a discrete spatial environment with random agent positioning spatial width height width height in discrete cells population dependence on the population slider input sets parameter value threshold parameter default value is that of threshold parameter sets threshold parameter value person is assigned a randomly picked color person visual representation color is set to either red or black with equal likelihood core segregation movement logic person initial location count neighbors sharing same colour should be in diff function only satisfied if fraction of surrounding individuals sharing color exceeds threshold if dissatisfied chance of moving experiment simulation sets parameter assumptions add a parameter to main experiment add a slider setting the slider properties setting value for parameter from slider modify person behavior to depend on new parameter updated code required because new parameter is global and lives in main class rather than in person class movement in discrete space jumptocell int row int column jumps to a particular unoccupied cell precondition destination cell is unoccupied movetonextcell int direction moves agent into neighbouring cell in a given direction directions north south east west northeast northwest southest southwest precondition destination cell is unoccupied jumptorandomemptycell jumps to randomly selected empty cell returning true returns false if no empty cell can be located discovery in discrete space int findrandomemptycell returns row column of an unoccupied cell getting agents in cell or direction getagentatcell int row int column getagentnexttome int direction getneighbors neigbourhood models moore cardinal directions north south east west euclidean north south east west northeast northwest southest southwest important distinction suppose an agent is moving in discrete space and need to be concerned about moving into the same cell as another agent we can readily prevent this agent from moving into another cell currently occupied but canwe prevent this agent from colliding with another agent that wishes to move into the same cell to answer this we need to be clear about the model of time used by agents two key models of time in anylogic synchronous time here agents all change in lockstep separated by fixed time steps when computing agent behavior to determine agent state in the next timestep our enquiries about agent state e g using getagentatcell or getagentnexttome need to ask about the situation in the current timestep we gather needed information regarding current state in onbeforestep and changes are performed in onstep this is similar to what we saw in system dynamics the changes over the next small interval of time δt depend on the current value of the stocks these changes are then applied at once and all stocks are updated enabling synchronous time unless enable the steps the various handlers for synchronized time e g on before step on step on after step etc are executed both environment and agents have on before step and on after step handlers on before step for environments is executed before the corresponding method for agents on after step for environments is executed after the corresponding method for agents synchronous time can be enabled via the environment general page click checkbox enable steps two key models of time in anylogic asynchronous time here every agent is updated at a different time no two agents are typically likely to be updated at exactly the same time so when considering the state of other agents they see the last situation where the other agent has been updated synchronization discrete agent movement in synchronous mode it is difficult to know if two agents will collide using data on the current timestep even if we know where the other object was during the current timestep it possible it will move into the cell we wish to occupy in the next timestep it is simpler to handle this asynchronously here we can have each agent update at slightly different times and observe the location of the other agents at the current time without any significant chance that they will move to the same place at the same time this issue only arises for discrete agent movement as this is the only case where cells only contain agent infectious disease models basic quantities of mathematical infectious disease epidemiology nathaniel osgood cmpt recall closed population no birth death infection always dies out in the population some infections will take longer to die out there is a tipping point between two cases of people infected declines out immediately infection causes an outbreak before the infection dies down of people infected rises and then falls case outbreak people people people people people people people people people people people people people people people sir example time days susceptible population s sir example people infectious population i sir example people recovered population r sir example people case infection declines immediately infectives time month infectives infection extinction recall simple model incorporating population turnover contacts per susceptible infection fractional prevalence mean time with disease susceptible incidence infective mortality recovery recovered recovered mortality mortality mortality rate mortality rate recall our model set c people month chance of transmission per s i contact μ birth and death rate initial infectives other susceptible recall our model set c people month chance of transmission per s i contact μ birth and death rate initial infectives other susceptible here the infection can remain endemic damped oscillatory behavior modify model to have births and deaths with an annual birth and death rate set model settings final time to long time frame in synthesim running man mode set birth death rates equilibrium behaviour with births deaths the system can approach an endemic equilibrium where the infection stays circulating in the population but in balance the balance is such that simultaneously the rate of new infections the rate of immigration otherwise of susceptibles would be changing the rate of new infections the rate of recovery otherwise of infectives would be changing exploring the tipping point now try setting transmission rate β to infection extinction as for the case with a closed population an open population has two cases infection dies out immediately infectives outbreak infection takes off time month here in contrast to the case for a closed population the infection will typically go to an endemic equilibrium slides adapted from external source redacted from public pdf for copyright reasons delays for a while after infectives start declining i e susceptibles are below sustainable endemic value they still deplete susceptibles sufficiently for susceptibles to decline for a while after susceptibles are rising until susceptibles endemic value infectives will still decline for a while after infectives start rising births of infections susceptibles will rise to a peak well above endemic level this is the point where rate of new infections rate of recoveries a person infects on average person before recovering the level of susceptibles is at the lowest level where the infection is sustainable in the short run at this point susceptibles susceptibles at endemic equilibrium why is the of susceptibles rising to well above its sustainable value why is the of susceptibles still declining this fraction of susceptibles at endemic equilibrium is the minimum sustainable value of susceptible i e the value where the properties above hold above this fraction of susceptibles the infected will rise below this fraction of susceptibles the infected will fall the rise is occurring because infectives are so low that so few infections occur that births infections deaths s rises above the sustainable value because infectives are still in decline until that point so infectives remain low for a while the susceptibles are still declining here because the large of infectives still causes enough infections that rate of immigration rate of infections deaths recall kendrick mckermack model partitioning the population into broad categories susceptible s infectious i removed r births contacts per susceptible per contact risk of infection force of infection incidence fractional prevalence mean time with disease recovery initial population initial fraction vaccinated mortality rate mortality rate population fractional prevalence shorthand for key quantities for infectious disease models stocks i or y total number of infectives in population this could be just one stock or the sum of many stocks in the model e g the sum of separate stocks for asymptomatic infectives and symptomatic infectives n total size of population this will typically be the sum of all the stocks of people s or x number of susceptible individuals intuition behind common terms i n the fraction of population members or by assumption contacts that are infective important simplest models assume that this is also the fraction of a given susceptible contacts that are infective many sophisticated models relax this assumption c i n number of infectives that come into contact with a susceptible in a given unit time c i n force of infection likelihood a given susceptible will be infected per unit time the idea is that if a given susceptible comes into contact with c i n infectives per unit time and if each such contact gives likelihood of transmission of infection then that susceptible has roughly a total likelihood of c i n of getting infected per unit time e g month a critical throttle on infection spread fraction susceptible f the fraction susceptible here s n is a key quantity limiting the spread of infection in a population recognizing its importance we give this name f to the fraction of the population that issusceptible key term flow rate of new infections this is the key form of the equation in many infectious disease models total of susceptiblesinfected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n note that this is a term that multiplies both s and i this is much different than the purely linear terms on which we have previously focused likelihood is actually a likelihood density e g can be indicating that mean time to infection is another useful view of this flow recall total of susceptibles infected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n the above can also be phrased as the following s c i n i c s n of infectives average susceptibles infected per unit time by each infective this implies that as of susceptibles falls of susceptibles surrounding each infective falls the rate of new infections falls less fuel for the fire leads to a smaller burning rate recall the importance of susceptible fraction recall total of susceptibles infected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n the above can also be phrased as the following s c i n i c s n of infectives average susceptibles infected per unit time by each infective this implies that as fraction of susceptibles falls fraction of susceptibles surrounding each infective falls the rate of new infections falls less fuel for the fire leads to a smaller burning rate expressing discrete inter agent dynamics messaging events nathaniel osgood discrete agent coupling via messages within anylogic agents can be coupled by either discrete instantaneous and individuated or continuous ongoing and gradual coupling the preferred mechanism for discrete coupling is messages sent between agents many types of messages payloads are possible an agent can send a given message to one or more agents frequently messages are sent locally to neighbors within the environment neighboring nodes on the network nearby neighbors in space messages statecharts messages may be handled in many ways one of the most common ways in which messages are handled is by statecharts a transition can be triggered guarded or gated by a message a transition may be associated with an action that fires off a message to other agents or to other statecharts within the agent receiving a message in this case only messag type exists so the simple fact that a message has been received is sufficient there is no need to inspe message conte sending a message self transition because remains in state message sending messages may be sent to either a particular explicitly specified agent an implicitly specified class of agents neighboring agents in the environment topology random agents all agents any connected agents all connected agents mechanism send message destinationobject send message agentclassid synchronous vs asynchronous delivery messages may be sent in two ways via send asynchronous delivery occurs sometime after call to send via deliver synchronous risks infinite loops in delivery if destination also calls deliver in the reverse direction message payloads sometimes just the fact that a message has been sent provides us with all of the information we need sometimes just distinguishing different message types is sufficient we will sometimes send messages with payloads of data that provide extra information e g the agent that sent the message eg that is infecting us particular parameters can send multiple message types boolean int double string other can specify class type sending a message with a string payload sending a message with object payload receiving a message forwarding messages on to the statechart the action for handling received messages delegates to the statechart object receiving a message building up a simple agent based model the manual technique add a new model project filling in the model project details add an active object class filling in the agent class details the updated project declaring person as an agent check this box updated result note changed icon double click on person scroll until you see the cross hairs this is the grid origin to be centred on their spatial location the person presentation items should be placed here create an oval at the origin cross hairs from the centre of the oval draw a line the on the end of the line should be at the centre of the oval set the replication dynamic property of the line so there is for each connection make sure you have selected the line by clicking on it make sure you have selected the dynamic tab also set the dx and dy properties double click on main class name to view this it should appear on top tab double click here click and drag from person into the space on the right drag from here drag to here set the count of agents in the agent population for clarity rename person to population add an environment drag to here drag from here set the network type to use make the population depend on the environment for placement connections etc try running the model adding color variable this is the name of a java class make sure this is in lower case fill in the type and initial value watch for correct case make oval color property use variable make sure you have selected the dynamic tab make sure you have selected the oval by clicking on it add entry point of state chart add in susceptible state connect with entry point when this really connects the circle should be green see tip at end of presentation fill in code to color green when enter state adding in infective state set to color red when enter state adding transition when this really connects on both sides circles should be green adding infection clearance transition run the model completing set up press this button to start model execution model presentation making infection depend on a message make sure you have selected the transition by clicking on it using a contact event to spread infection add this transition setting person so forwards infection message to statechart make sure the agent tab is selected setting startup code so initially infects a random person so start with infective infection percolation over the network tip beware loose connections corrected network environments in anylogic nathaniel osgood recall spatial types supported continuous no interference between agents continuous movement via velocity only spatial dimensions required discrete space is divided tesselated into cells mutual exclusion of agents from a given cell space information requires dimension rows columns for count of cells in x y location networks spatial layouts distinct node attributes location connections spatial layouts determine where nodes appear in space and often on the screen network type determines who is connected to who for the most part these characteristics are determined independently network topologies connectedness can be defined either alternative to or in addition to spatial layouts agents will have spatial locations in either case hands on model use ahead load model network modification of sir ab network types select environment interaction between network location for one type of networks distanced based whether there is a connection between a and b depends on the distance between a b this sets connectivity based on location considerations distance based layout property for distance based layout distance threshold random connections with random connections scale free network scale free network high degree node low degree node layout types layout type random uniformly distribute x and y position of nodes arranged set node locations in a regular fashion normally in a grid ring set node locations in periodically spaced intervals around a ring shape spring mass adjust node locations such that node locations that are most tightly connected tend to be closer together sets location based on network user defined user can set location e g in initialization code interaction between network location in a spring mass layout the nodes that are highly connected will tend to be clustered here we are determining the location based on the connectivity infectious disease models basic epidemiological quantities and vaccination nathaniel osgood cmpt march births contacts per susceptible per contact risk of infection force of infection incidence fractional prevalence mean time with disease recovery initial population initial fraction vaccinated mortality rate mortality rate population fractional prevalence shorthand for key quantities for infectious disease models stocks i or y total number of infectives in population this could be just one stock or the sum of many stocks in the model e g the sum of separate stocks for asymptomatic infectives and symptomatic infectives n total size of population this will typically be the sum of all the stocks of people s or x number of susceptible individuals intuition behind common terms i n the fraction of population members or by assumption contacts that are infective important simplest models assume that this is also the fraction of a given susceptible contacts that are infective many sophisticated models relax this assumption c i n number of infectives that come into contact with a susceptible in a given unit time c i n force of infection likelihood a given susceptible will be infected per unit time the idea is that if a given susceptible comes into contact with c i n infectives per unit time and if each such contact gives likelihood of transmission of infection then that susceptible has roughly a total likelihood of c i n of getting infected per unit time e g month a critical throttle on infection spread fraction susceptible f the fraction susceptible here s n is a key quantity limiting the spread of infection in a population recognizing its importance we give this name f to the fraction of the population that issusceptible key term flow rate of new infections this is the key form of the equation in many infectious disease models total of susceptibles infected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n note that this is a term that multiplies both s and i this is much different than the purely linear terms on which we have previously focused likelihood is actually a likelihood per unit time e g can be indicating that mean time to infection is another useful view of this flow recall total of susceptibles infected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n the above can also be phrased as the following s c i n i c s n i c f of infectives mean susceptibles infected per unit time by each infective this implies that as of susceptibles falls of susceptibles surrounding each infective falls the rate of new infections falls less fuel for the fire leads to a smaller burning rate recall the importance of susceptible fraction recall total of susceptibles infected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n the above can also be phrased as the following s c i n i c s n of infectives average susceptibles infected per unit time by each infective this implies that as fraction of susceptibles falls fraction of susceptibles surrounding each infective falls the rate of new infections falls less fuel for the fire leads to a smaller burning rate slides adapted from external source redacted from public pdf for copyright reasons critical notions contact rates transmission probabilities equilibria endemic disease free r herd immunity recall flow rate of new infections this is the key form of the equation in many infectious disease models total of susceptiblesinfected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n note that this is a term that multiplies both s and i this is much different than the purely linear terms on which we have previously focused likelihood is actually a likelihood density i e likelihood per unit time e g can be indicating that mean time to infection is recall another useful view of this flow recall total of susceptibles infected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n the above can also be phrased as the following s c i n i c s n of infectives average susceptibles infected per unit time by each infective this implies that as of susceptibles falls of susceptibles surrounding each infective falls the rate of new infections falls less fuel for the fire leads to a smaller burning rate infection recall for this model a given infective infects c s n others per time unit this goes up as the number of susceptibles rises questions if the mean time a person is infective is μ how many people does that infective infect before recovering with the same assumption how many people would that infective infect if everyone else is susceptible under what conditions would there be more infections after their recovery than before fundamental quantities we have just discovered the values of famous epidemiological quantities for our model effective reproductive number r basic reproductive number effective reproductive number r number of individuals infected by an index infective in the current epidemological context depends on contact number transmission probability length of time infected fraction of susceptibles affects whether infection spreads if r of cases will rise if r of cases will fall alternative formulation largest real eigenvalue endemic rate basic reproduction number number of individuals infected by an index infective in an otherwise disease free equilibrium this is just r at disease free equilibrium all other people in the population are susceptible other than the index infective depends on contact number transmission probability length of time infected affects whether infection spreads if epidemic takes off if epidemic dies out alternative formulation largest real eigenvalue initial infection rise exp t d endemic rate basic reproductive number if contact patterns infection duration remain unchanged and if fraction f of the population is susceptible then mean of individuals infected by an infective over the course of their infection is f in endemic equilibrium inflow outflow s n every infective infects a replacement infective to keep equilibrium just enough of the population is susceptible to allow this replacement the higher the the lower the fraction of susceptibles in equilibrium generally some susceptibles remain at some point in epidemic susceptibles will get so low that can t spread our model set c people month chance of transmission per s i contact μ birth and death rate initial infectives other susceptible what is what should we expect to see thresholds r too low susceptibles r of infectives declining too high susceptibles r of infectives rising infection is introduced from outside will cause outbreak herd immunity infection is introduced from outside will die out may spread to small number before disappearing but in unsustainable way this is what we try to achieve by control programs vaccination etc outflow from susceptibles infections is determined by the of infectives equilibrium behaviour with births deaths the system can approach an endemic equilibrium where the infection stays circulating in the population but in balance the balance is such that simultaneously the rate of new infections the rate of immigration otherwise of susceptibles would be changing the rate of new infections the rate of recovery otherwise of infectives would be changing disease free equilibria no infectives in population entire population is susceptible endemic steady state equilibrium produced by spread of illness assumption is often that children get exposed when young the stability of the these equilibria whether the system departs from them when perturbed depends on the parameter values for the disease free equilibrium on vaccination equilibrium behaviour with births deaths the system can approach an endemic equilibrium where the infection stays circulating in the population but in balance the balance is such that simultaneously the rate of new infections the rate of immigration otherwise of susceptibles would be changing the rate of new infections the rate of recovery otherwise of infectives would be changing disease free equilibria no infectives in population entire population is susceptible endemic steady state equilibrium produced by spread of illness assumption is often that children get exposed when young the stability of the these equilibria whether the system departs from them when perturbed depends on the parameter values for the disease free equilibrium on adding vaccination stock add a vaccinated stock a constant called monthly likelihood of vaccination vaccination flow between the susceptible and vaccinated stocks the rate is the stock times the constant above set initial population to be divided between stocks susceptible vaccinated incorporate vaccinated in population calculation additional settings c beta duration of infection birth death rate adding stock experiment with different initial vaccinated fractions fractions infectives time month w ec t i es no inun i gra tion i es t f rac tion i ac cinated w ec t i es no lnun i gra tion i es t fraction vaccinated infecti es no hmnigra tion tes t fraction vaccinated infec t i es no inunigra tion tes t fr ac tion i ac cinated w ec t i es no inun i gra tion i es t f rac tion i ac cinated wec t i es no lnun i gra tion i es t fraction vaccinated infecti es no hmnigra tion tes t fraction vaccinated recall thresholds r too low susceptibles r of infectives declining too high susceptibles r of infectives rising outflow from susceptibles infections is determined by the of infectives delays for a while after infectives start declining they still deplete susceptibles sufficiently for susceptibles to decline for a while after infectives start rising the of infections is insufficient for susceptibles to decline effective reproductive number r number of individuals infected by an index infective in the current epidemiological context depends on contact number transmission probability length of time infected fraction of susceptibles affects whether infection spreads if r of cases will rise if r of cases will fall alternative formulation largest real eigenvalue endemic rate basic reproduction number number of individuals infected by an index infective in an otherwise disease free equilibrium this is just r at disease free equilibrium all other people in the population are susceptible other than the index infective depends on contact number transmission probability length of time infected affects whether infection spreads if epidemic takes off if epidemic dies out alternative formulation largest real eigenvalue initial infection rise exp t d endemic rate recall a critical throttle on infection spread fraction susceptible f the fraction susceptible here s n is a key quantity limiting the spread of infection in a population recognizing its importance we give this name f to the fraction of the population that issusceptible if contact patterns infection duration remain unchanged and then mean of individuals infected by an infective over the course of their infection is f recall endemic equilibrium inflow outflow s n f every infective infects a replacement infective to keep equilibrium just enough of the population is susceptible to allow this replacement the higher the the lower the fraction of susceptibles in equilibrium generally some susceptibles remain at some point in epidemic susceptibles will get so low that can t spread critical immunization threshold consider an index infective arriving in a worst case scenario when noone else in the population is infective or recovered from the illness in this case that infective is most efficient in spreading the goal of vaccination is keep the fraction susceptible low enough that infection cannot establish itself even in this worst case we do this by administering vaccines that makes a person often temporarily immune to infection we say that a population whose f is low enough that it is resistant to establishment of infection exhibits herd immunity critical immunization threshold vaccination seeks to lower f such that f worst case suppose we have a population that is divided into immunized vaccinated and susceptible let qc be the critical fraction immunized to stop infection then f qc f qc qc so if as in our example qc i e of population must be immunized just as we saw infectious disease models vaccination cmpt nathaniel osgood recall thresholds r too low susceptibles r of infectives declining too high susceptibles r of infectives rising outflow from susceptibles infections is determined by the of infectives delays for a while after infectives start declining they still deplete susceptibles sufficiently for susceptibles to decline for a while after infectives start rising the of infections is insufficient for susceptibles to decline effective reproductive number r number of individuals infected by an index infective in the current epidemiological context depends on contact number transmission probability length of time infected fraction of susceptibles affects whether infection spreads if r of cases will rise if r of cases will fall alternative formulation largest real eigenvalue endemic rate basic reproduction number number of individuals infected by an index infective in an otherwise disease free equilibrium this is just r at disease free equilibrium all other people in the population are susceptible other than the index infective depends on contact number transmission probability length of time infected affects whether infection spreads if epidemic takes off if epidemic dies out alternative formulation largest real eigenvalue initial infection rise exp t d endemic rate recall a critical throttle on infection spread fraction susceptible f the fraction susceptible here s n is a key quantity limiting the spread of infection in a population recognizing its importance we give this name f to the fraction of the population that issusceptible if contact patterns infection duration remain unchanged and then mean of individuals infected by an infective over the course of their infection is f recall endemic equilibrium inflow outflow s n f every infective infects a replacement infective to keep equilibrium just enough of the population is susceptible to allow this replacement the higher the the lower the fraction of susceptibles in equilibrium generally some susceptibles remain at some point in epidemic susceptibles will get so low that can t spread equilibrium behaviour with births deaths the system can approach an endemic equilibrium where the infection stays circulating in the population but in balance the balance is such that simultaneously the rate of new infections the rate of immigration otherwise of susceptibles would be changing the rate of new infections the rate of recovery otherwise of infectives would be changing disease free equilibria no infectives in population entire population is susceptible endemic steady state equilibrium produced by spread of illness assumption is often that children get exposed when young the stability of the these equilibria whether the system departs from them when perturbed depends on the parameter values for the disease free equilibrium on adding vaccination stock add a vaccinated stock a constant called monthly likelihood of vaccination vaccination flow between the susceptible and vaccinated stocks the rate is the stock times the constant above set initial population to be divided between stocks susceptible vaccinated incorporate vaccinated in population calculation additional settings c beta duration of infection birth death rate adding stock experiment with different initial vaccinated fractions fractions infectives time month w ec t i es no inun i gra tion i es t f rac tion i ac cinated w ec t i es no lnun i gra tion i es t fraction vaccinated infecti es no hmnigra tion tes t fraction vaccinated infec t i es no inunigra tion tes t fr ac tion i ac cinated w ec t i es no inun i gra tion i es t f rac tion i ac cinated wec t i es no lnun i gra tion i es t fraction vaccinated infecti es no hmnigra tion tes t fraction vaccinated critical immunization threshold consider an index infective arriving in a worst case scenario when noone else in the population is infective or recovered from the illness in this case that infective is most efficient in spreading the goal of vaccination is keep the fraction susceptible low enough that infection cannot establish itself even in this worst case we do this by administering vaccines that makes a person often temporarily immune to infection we say that a population whose f is low enough that it is resistant to establishment of infection exhibits herd immunity critical immunization threshold vaccination seeks to lower f such that f worst case suppose we have a population that is divided into immunized vaccinated and susceptible let qc be the critical fraction immunized to stop infection then f qc f qc qc so if as in our example qc i e of population must be immunized just as we saw scale free networks nathaniel osgood cmpt march recall heterogeneity in contact rates schneeberger et al scale free networks and sexually transmitted diseases a description of observed patterns of sexual contacts in britain and zimbabwe sexually transmitted diseases june volume issue pp associated log log graph schneeberger et al scale free networks and sexually transmitted diseases a description of observed patterns of sexual contacts in britain and zimbabwe sexually transmitted diseases june volume issue pp heterogeneity in contact rates schneeberger et al scale free networks and sexually transmitted diseases a description of observed patterns of sexual contacts in britain and zimbabwe sexually transmitted diseases june volume issue pp intuitive plausibility of importance of heterogeneity someone with high of partners is both more likely to be infected by a partners more likely to pass on the infection to another person via targeted interventions on high contact people may be able to achieve great bang for the buck we may see very different infection rates in high contact rate individuals how to modify classic equations to account for heterogeneity how affects infection spread recall classic infection term y y d xs are susceptibles ys are infectives c is contacts per unit time β is chance a given contact between an infective and a susceptible will transmit infection key step disaggregate by contact rate we break the population up in to groups according to their rate of contacts xi and yi are susceptibles infectives who contact i other people per unit time x is divided into y is divided into this rate of contact used to be a single constant c but first attempt now we ve captured the heterogeneity in rates y j this is the total number of y y i j x i infected people i n i d here we are capturing the higher levels of risk for someone of activity class i as i increases due to higher contact rates problem we are assuming that our i contacts are equally spread among other people in fact they are skewed towards others with a high of contacts people with high of contacts are more likely to be infected this is the total number of revised formulation contacts per unit time made by infectives y y i x j j jn j i d this is the total number of contacts per unit time made by the entire population xi and yi are susceptibles infectives who contact i other people per unit time the fraction indicates fraction of contacts in the population that are with an infective person i times this is the rate of contacts with infectives per unit time experienced by a susceptible in class i force of infection j j jy j jn j will only grow if y grows reformulating state equations in j jy j jn j j jy j jn j y j jy j j j n j d j j n j d j j j jn j j j j n jy j d j j j n jy j j j n j j j j j j jn jn jn d jn jn d j j j j j j j j j j j j n n n j j j j j j jn d jn d jn d j j j j j j n j j n j n n e j j jn d n j d e j d j j j j n e e j d slide nathaniel osgood reformulated equation e e j d this is exactly like the normal sir system with x c e e j is e d e j reformulating in more familiar terms o var j e j e j e e j e e e j e j o c m e j e j m m e o cd d m d e j m rises proportional to the coefficient of variation ratio of the variance to mean scale free networks albert jeong and barabási nature july infectious disease models intervention impact on an open population nathaniel osgood cmpt april recall the importance of susceptible fraction recall total of susceptibles infected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n the above can also be phrased as the following s c i n i c s n of infectives average susceptibles infected per unit time by each infective this implies that as fraction of susceptibles falls fraction of susceptibles surrounding each infective falls the rate of new infections falls less fuel for the fire leads to a smaller burning rate a critical throttle on infection spread fraction susceptible f the fraction susceptible here s n is a key quantity limiting the spread of infection in a population recognizing its importance we give this name f to the fraction of the population that issusceptible if contact patterns infection duration remain unchanged and then mean of individuals infected by an infective over the course of their infection is f critical immunization threshold consider an index infective arriving in a worst case scenario when noone else in the population is infective or recovered from the illness in this case that infective is most efficient in spreading the goal of vaccination is keep the fraction susceptible low enough that infection cannot establish itself even in this worst case we do this by administering vaccines that makes a person often temporarily immune to infection we say that a population whose f is low enough that it is resistant to establishment of infection exhibits herd immunity critical immunization threshold vaccination seeks to lower fsuch that f worst case suppose we have a population that is divided into immunized vaccinated and susceptible let qc be the critical fraction immunized to stop infection then f qc f qc qc so if as in our example qc i e of population must be immunized just as we saw open closed population effects of an open population differentinpfecativerameters approaches endemic level where r rate of new infections rate of recoveries because no new influx of susceptibles fuel infectives in constant decline approaches disease free equilibrium time month infective baseline annual turnover infective baseline closed population effects of an open population susceptible approaches endemic level where r rate of arrivals via birth migration rate of new infections deaths approaches disease free level where no infection is occurring time month susceptible baseline annual turnover susceptible baseline closed population recovereds time month recovered baseline annual turnover recovered baseline closed population impact of turnover the greater the turnover rate the greater the fraction of susceptibles in the population the greater the endemic rate of infection fraction of susceptibles effective reproductive number prevalence effec i e reproduc i e number tme year fraction recovered adding ongoing vaccination process simulating introduction of vaccination for a childhood infection in an open population c beta duration of infection initial fraction vaccinated monthly birth death rate per year focusing on children years of age questions what is what level of susceptibles is required to sustain the infection what is the critical vaccination fraction ociiia ed rt q tioo ifl terl hccili tioo rme yem mt a what rate of vaccination eliminates p e lence dj rut rui j i urt ur l l l i l i ce j pl evi l i ce p time ear rl ce bi c r lllittl i hoo mt f rhc ion of suscep ible in popula ion ur qi s ti ts rn s s tibt i ri s ti l rn time y ief y i f y s representing quarantine p e alence tune ff ear o f o time ear endemic situations in an endemic context infection remains circulating in the population the common assumption here is that the susceptible portion of the population will be children at some point in their life trajectory at an average age of acquiring infection a individuals will be exposed to the infection develop immunity slides adapted from external source redacted from public pdf for copyright reasons age of exposure reproductive constant cf a natural non immunized constant size population where all die at same age and where mean age at death l mean age of exposure a i e we assume those above a are exposed fraction susceptible is s n a l i e proportion of population below age a recall for our and many but not all other models r s n s n thus a l l a this tells us that the larger the the earlier in life individuals become infected incompletely immunized population suppose we have q fraction of population immunized q qc suppose we have fraction f susceptible fraction of the population currently or previously infected is q f if we assume as previously that everyone lives until l and is infected at age a then fraction a l has been infected so a l q f a l q f this can be much higher than for the natural population this higher age of infection can cause major problems due to waning of childhood defenses i e incomplete immunization leads to older mean age ofexposure slides adapted from external source redacted from public pdf for copyright reasons scale free networks nathaniel osgood cmpt march recall heterogeneity in contact rates source schneeberger et al scale free networks and sexually transmitted diseases a description of observed patterns of sexual contacts in britain and zimbabwe sexually transmitted diseases june volume issue pp associated log log graph source schneeberger et al scale free networks and sexually transmitted diseases a description of observed patterns of sexual contacts in britain and zimbabwe sexually transmitted diseases june volume issue pp heterogeneity in contact rates this may significantly affect the spread of infection in the population source schneeberger et al scale free networks and sexually transmitted diseases a description of observed patterns of sexual contacts in britain and zimbabwe sexually transmitted diseases june volume issue pp intuitive plausibility of importance of heterogeneity someone with high of partners is both more likely to be infected by a partners more likely to pass on the infection to another person via targeted interventions on high contact people may be able to achieve great bang for the buck we may see very different infection rates in high contact rate individuals how to modify classic equations to account for heterogeneity how affects infection spread recall classic infection term y c y y n d xs are susceptibles ys are infectives c is contacts per unit time β is chance a given contact between an infective and a susceptible will transmit infection key step disaggregate by contact rate we break the population up in to groups according to their rate of contacts xi and yi are susceptibles infectives who contact i other people per unit time x is divided into y is divided into this rate of contact used to be a single constant c but now we ve captured the heterogeneity in rates first attempt y j this is the total number of y i j x yi infected people i n i d here we are capturing the higher levels of risk for someone of activity class i as i increases due to higher contact rates problem we are assuming that our i contacts are equally spread among other people in fact they are skewed towards others with a high of contacts people with high of contacts are more likely to be infected this is the total number of revised formulation contacts per unit time made by infectives jy j y y i j j jn j x j d this is the total number of contacts per unit time made by the entire population xi and yi are susceptibles infectives who contact i other people per unit time the fraction indicates fraction of contacts in the population that are with an infective person i times this is the rate of contacts with infectives per unit time experienced by a susceptible in class i force of infection j j jy j jn j will only grow if y grows reformulated equation e e j d this is exactly like the normal sir system with x c e e j is e d e j reformulating in more familiar terms var j e j e j e e j e e e j e j c m e j e j m m e cd d m d e j m rises proportional to the coefficient of variation ratio of the variance to mean scale free networks albert jeong and barabási nature july scale free networks a node number of connections a person of contacts is denoted k the chance of having k partners is proportional to k for human sexual networks is between and e g if likelihood having partner is proportional to of having is proportional to etc nb it appears that anylogic algorithm from barabasi albert science imposes a of power law scaling this frequency distribution is a power law that exhibits invariance to scale suppose we zoom in in terms of x by a factor of α cf p x cx γ p αx c αx γ cα γx γ α γcx γ dp x in other words the function p x looks the same at any scale it is just multiplied by a different constant we can get power law scaling from many sources a key source is dimensional structure power law probability distributions have long tails compared to e g an exponential or normal the signature of a power law plotting a power law function on a log log plot will yield a straight line cf p x cx γ log p x c γlog x this relates to the fact that the impact of scaling scaling is always the identical divides the function by the same quantity e g if γ doubling x always divides p x by no matter what x is e g if γ doubling x always divides p x by observation great heterogeneity in contact rates source schneeberger et al scale free networks and sexually transmitted diseases a description of observed patterns of sexual contacts in britain and zimbabwe sexually transmitted diseases june volume issue pp this may significantly affect the spread of infection in the population deriving the probability distribution function for scale free networks k x dx k x pdf is x mean mean x xp x dx x x x dx x x variance x p x dx x x dx x x x e e x only valid if variance of human scale free networks for the variance of the degree distribution for an infinitely large population is infinite dies off too slow recall e cd d m d implications e j m for a poisson network m and c barely increases for a scale free network with a sufficiently large population will always be the disease will not die out even if most people have low partners discrete event network modeling in anylogic nathaniel osgood cmpt march hands on model use ahead load model emergency department tulsa alp recall network modeling irregular spatial embedding discrete event modeling resource based modeling queues processes flow charts capacitated resource pools send to attachment detachment central concepts in discrete event modeling entities flowing through processes being processed at successive stages flow charts guide entity progress resources required for processing queues for waiting entities capacitated resource pools from which resources are drawn entity interaction with resources attachment detachment seizing physical homes for resources movement paths via polygons entities entities are the central parties on which the processes take place cf patients in a hospital or clinic primarily passive things happen to them flow through are routed around the flow charts associated with the system only exist for the duration of time that are in the system are injected into the system multiple entities can be in the system at one time if wish to maintain extra information on an entity can subclass the entity class entities are often associated with a physical representation which can travel around parameter estimation a glimpse of backing out sensitivity analysis meta analysis nathaniel osgood cmpt sources for parameter estimates surveillance data controlled trials outbreak data clinical reports data intervention outcomes studies calibration to historic data expert judgement systematic reviews content redacted for copyright compliance introduction of parameter estimates annual likelihood of non diabetes mortality for asymptomatic population undx uncomplicated dying other causes annual not at risk births being born non obese being born at risk annual likelihood of becoming obese becoming obese developing diabetes zed mortality for obese pulation obese mortality non obese mortality annual mortality rate for non obese population hassmiller k m the impact of smoking on population level tuberculosis outcomes unpublished phd university of michigan ann arbor ann arbor preparation for pooling hassmiller k m the impact of smoking on population level tuberculosis outcomes unpublished phd university of michigan ann arbor ann arbor hassmiller k m the impact of smoking on population level tuberculosis outcomes unpublished phd university of michigan ann arbor ann arbor example of other pooling hassmiller k m the impact of smoking on population level tuberculosis outcomes unpublished phd university of michigan ann arbor ann arbor pooled results hassmiller k m the impact of smoking on population level tuberculosis outcomes unpublished phd university of michigan ann arbor ann arbor forest plot hassmiller k m the impact of smoking on population level tuberculosis outcomes unpublished phd university of michigan ann arbor ann arbor forest plot hassmiller k m the impact of smoking on population level tuberculosis outcomes unpublished phd university of michigan ann arbor ann arbor sensitivity analyses same relative or absolute uncertainty in different parameters may have hugely different effect on outcomes or decisions help identify parameters that strongly affect key model results choice between policies we place more emphasis in parameter estimation into parameters exhibiting high sensitivity types of sensitivity analyses variables involved one way multi way type of component being varied parameter sensitivity analysis parameter values structural sensitivity analysis examine effects of model structure on results type of variation single alternative values monte carlo analyses draws from probability distributions many types of variations frequency of variation static parameter retains value all through simulation ongoing change stochastic process accomplished via monte carlo analyses content example spider diagram each axis represents a change in a particular redacted for copyright parameter this proportional change is identical for the different parameters compliance the distance assumed by the curve along that axis represents the magnitude of response to that change note that these sensitivities will depend on the state of system systematic examination of policies female male uptake relapse cessation tengs t osgood n lin t public health impact of changes in smoking behavior results from the tobacco policy model medical care sensitivity in initial value frequently we don t know the exact state of the system at a certain point in time a very useful type of sensitivity analysis is to vary the initial value of model stocks in vensim this can be accomplished by indicating a parameter name within the initial value area for a stock varying the parameter value imposing a probability distribution monte carlo analysis we feed in probability distributions to reflect our uncertainty about one or more parameters the model is run many many times realizations for each realization the model uses a different draw from those probability distribution what emerges is resulting probability distribution for model outputs example resulting distribution empirical fractiles static uncertainty impact on cost of uncertainty regarding mortality and medical costs incremental costs b b b b multi way sensitivity analyses when examining the results of changing multiple variables need to consider how multiple variables vary together if this covariation reflects dependence on some underlying factor may be able to simulate uncertainty in underlying factor performing monte carlo sensitivity analyses in vensim need to specify three things the parameters to vary how to vary those parameters which model variables to save away how what parameters to vary s control edit the filename to save changes to a different control file filename simple sir vsc number of noise simulations seed warning messages active parameters drag to reorder annual birth and death r ate ran mal delete selected selected add editing distribution parameter per contact risk of infection random uniform model minimum maximum value value value model values to save away monte carlo analyses sensitivity results prevalence sensitivity results fraction of susceptibles stochastic processes examples of things stochastically approximated stock market rainfall oil prices economic growth what considered stochastic will depend on the scope of the model detailed model individual behaviour transmission etc a meteorological model may not consider rainfall stochastic stochastic processes baseline time day dealing with data gradients often we don t have reliable information on some parameters but do have other data some parameters may not be observable but some closely related observable data is available sometimes the data doesn t have the detailed breakdown needed to specifically address one parameter available data could specify sum of a bunch of flows or stocks available data could specify some function of several quantities in the model e g prevalence some parameters may implicitly capture a large set of factors not explicitly represented in model there are two big ways of dealing with this manually backing out and automated calibration backing out sometimes we can manually take several aggregate pieces of data and use them to collectively figure out what more detailed data might be frequently this process involves imposing some sometimes quite strong assumptions combining data from different epidemiological contexts national data used for provincial study equilibrium assumptions e g assumes stock is in equilibrium cf deriving prevalence from incidence independence of factors e g two different risk factors convey independent risks example suppose we seek to find out the sex specific prevalence of diabetes in some population suppose we know from published sources the breakdown of the population by sex cm cf the population wide prevalence of diabetes pt the prevalence rate ratio of diabetes in women when compared to men rrf we can back out the sex specific prevalence from these aggregate data pf pm here we can do this backing out without imposing assumptions backing out male diabetics female diabetics diabetics pm cm pf cf pt cm cf further we know that pf pm rrf pf pm rrf thus pm cm pm rrf cf pt cm cf pm cm rrf cf pt cm cf thus pm pt cm cf cm rrf cf pf pm rrf rrf pt cm cf cm rrf cf disadvantages of backing out backing out often involves questionable assumptions independence equilibrium etc sometimes a model is complex with several related known pieces even thought we may know a lot of pieces of information it would be extremely complex or involve too many assumptions to try to back out several pieces simultaneously another example joint marginal prevalence perhaps we know the count of people in each sex geographic category the marginal prevalences pr pu pm pf we need at least one more constraint one possibility assume pmr pmu pr pu we can then derive the prevalences in each sex geographic category example tying together meta analysis calibration hassmiller k m the impact of smoking on population level tuberculosis outcomes unpublished phd university of michigan ann arbor ann arbor dealing with data gradients backing out calibration nathaniel osgood cmpt sources for parameter estimates surveillance data controlled trials outbreak data clinical reports data intervention outcomes studies calibration to historic data expert judgement systematic reviews content redacted for copyright compliance introduction of parameter estimates annual likelihood of non diabetes mortality for asymptomatic population undx uncomplicated dying other causes annual not at risk births being born non obese being born at risk annual likelihood of becoming obese becoming obese developing diabetes zed mortality for obese pulation obese mortality non obese mortality annual mortality rate for non obese population sensitivity analyses same relative or absolute uncertainty in different parameters may have hugely different effect on outcomes or decisions help identify parameters that strongly affect key model results choice between policies we place more emphasis in parameter estimation into parameters exhibiting high sensitivity dealing with data gradients often we don t have reliable information on some parameters but do have other data some parameters may not be observable but some closely related observable data is available sometimes the data doesn t have the detailed breakdown needed to specifically address one parameter available data could specify sum of a bunch of flows or stocks available data could specify some function of several quantities in the model e g prevalence some parameters may implicitly capture a large set of factors not explicitly represented in model there are two big ways of dealing with this manually backing out and automated calibration backing out sometimes we can manually take several aggregate pieces of data and use them to collectively figure out what more detailed data might be frequently this process involves imposing some sometimes quite strong assumptions combining data from different epidemiological contexts national data used for provincial study equilibrium assumptions e g assumes stock is in equilibrium cf deriving prevalence from incidence independence of factors e g two different risk factors convey independent risks example suppose we seek to find out the sex specific prevalence of diabetes in some population suppose we know from published sources the breakdown of the population by sex cm cf the population wide prevalence of diabetes pt the prevalence rate ratio of diabetes in women when compared to men rrf we can back out the sex specific prevalence from these aggregate data pf pm here we can do this backing out without imposing assumptions backing out male diabetics female diabetics diabetics pm cm pf cf pt cm cf further we know that pf pm rrf pf pm rrf thus pm cm pm rrf cf pt cm cf pm cm rrf cf pt cm cf thus pm pt cm cf cm rrf cf pf pm rrf rrf pt cm cf cm rrf cf disadvantages of backing out backing out often involves questionable assumptions independence equilibrium etc sometimes a model is complex with several related known pieces even thought we may know a lot of pieces of information it would be extremely complex or involve too many assumptions to try to back out several pieces simultaneously another example joint marginal prevalence perhaps we know the count of people in each sex geographic category the marginal prevalences pr pu pm pf we need at least one more constraint one possibility assume pmr pmu pr pu we can then derive the prevalences in each sex geographic category calibration triangulating from diverse data sources calibration involves tuning values of less well known parameters to best match observed data often try to match against many time series or pieces of data at once idea is trying to get the software to answer the question what must these less known parameters be in order to explain all these different sources of data i see sometimes we learn from this that our model structure just can t produce the patterns calibration calibration helps us find a reasonable specifics for dynamic hypothesis that explains the observed data not necessarily the truth but probably a reasonably good guess at the least a consistent guess calibration helps us leverage the large amounts of diffuse information we may have at our disposal but which cannot be used to directly parameterize the model calibration helps us falsify models calibration a bit of the how calibration uses a global optimization algorithm to try to adjust unknown parameters so that it automatically matches an arbitrarily large set of data the data often in the form of time series forms constraints on the calibration the optimization algorithm will run the model many minimally thousands typically or more times to find the best match for all of the data example global optimization algorithm starts at random position tries to improve match minimize error by tweaking parameters running model recording error function keeps on improving until reaches local minimum in error of fit may add some randomness to knock out of local minima running calibrations in vensim under model simulate commands optimization control control edit the filename to save changes to a different control file filename voe choose new file clear settings level multiple start ra ioff j sensitivity random type off linear r estart ptimizer pass limit fractional tolerance max iterations max sims toler anee multiplier absolute toler anee scale absolute vector points t qr r l j flcj r lfj p j a j delete selected net ri emigration weight coefficient for age sex postr eproduc d l net ri emigration weight coefficient for age sex child m ale net ri emigration weight coefficient for age sex r eproductive j net ri emigration weight coefficient for age sex postr eproduc net gp emigration weight coefficient for age s ex child female net gp emigration weight coefficient for age s ex r eproductive add editing net gp emigration weight coefficient for age sex postr eprodu v j i net ri emigration weight coefficient for age sex i model value of constant select constant ok i payoff definition definition edit the filename to save changes to a different control file filename gdmcalibration vpd choose new file i r calibration r elements clear settings add editing variable sel compare to is used for compare to sel calibration weight sel the weight should be positive for calibration for optimizations use a positive number when more is better and a negative number when less is better assessing model goodness of fit to improve the goodness of fit of the model to observed data we need to provide some way of quantifying it within the model we for each historic data calculate discrepency of model figure out absolute value of discrepency from comparing historic data the model calculations convert the above to a fractional value dividing by historic data sum up these discrepency hicsfrniral o al po p uh ion si zie by population by ethnicity e hnicif y f ractional crep en y hicsroiira l t al popuh ti al popuh ion si zie by eelmiciey fur ime si zie by e hniciey time tl lle popuhtion by age sex ethnicit f ra ional dikrepency in hicsoori co er ed popuh ti oo by a e sex e lmicity if u e cse oric c m ereid p o p u h fion by a e sex e hniciey isoori preri alen care by age sex eflmiciey his trni preri ralent correction fa tor of rj ital stats cac es by age sex ethni his torical birth count to cm erf d population compatible count by age and s ethnicity ion al dicocrepen y in deafhg by eflmici y hicsoori o fur entir e cam di pop uh fion by age l m t required information for calibration specification of what to match and how much to care about each attempted match involves an error function payoff function penalty function energy function that specifies how far off we are for a given run how good the fit is a statement of what parameters to vary and over what range to vary them characteristics of desired tuning algorithm single starting point of search considerations adding constraints helps increase identifiability selection of realistic best fit adding parameters to tune leads to larger space to explore adding too many parameters to tune can lead to underdetermined situation all fits are within constraints of model the pieces of the elephant example model of underlying process time series it must match single model matches many data sources one of slides adapted from external source redacted from public pdf for copyright reasons pieces of the elephant sti science inci cl enoe of risk faotors by ethmoity ti dm in c rate by f l devblopment pop uajl ott rn at for ti dm ti d m p op ujlatt ron k death r popujlatt ron for ti dm popufa tt ron at jricsjk d e ath rate deathcs of p op ujlatti on at risk deoeassed p op ujlatt ron at riisk for ti dm q vv q w time year fl rle gl dll l lci i f t im e t ll lki dm y e tlul ici y ri llycellki tioo ul civelf ci risl e ga cioo civelr ri r discrete event network modeling in anylogic nathaniel osgood cmpt march hands on model use ahead load model emergency department tulsa alp recall network modeling irregular spatial embedding discrete event modeling resource based modeling queues processes flow charts capacitated resource pools send to attachment detachment central concepts in discrete event modeling entities flowing through processes being processed at successive stages flow charts guide entity progress resources required for processing queues for waiting entities capacitated resource pools from which resources are drawn entity interaction with resources attachment detachment seizing physical homes for resources movement paths via polygons entities entities are the central parties on which the processes take place cf patients in a hospital or clinic primarily passive things happen to them flow through are routed around the flow charts associated with the system only exist for the duration of time that are in the system are injected into the system multiple entities can be in the system at one time if wish to maintain extra information on an entity can subclass the entity class entities are often associated with a physical representation which can travel around hands on model use ahead load model opthalmology department alp flow charts entities flow in a single direction on flow charts start finish can contain a variety of substeps flow charts flow charts can be hierarchical frequently not linear e g branches joins elements to build edit flow charts major operators of interest source sink network enter exit enter into a particular network select output based on predicate split delay network move to move to a physical node or resource see below resource related see following slides network seize release network attach detach network send to determining factor can either be deterministic e g based on condition or stochastic based on probability select output network delay i file edit view model window help x j q q j i if i qi l cj j t get support t project l search el oj person ophthalmology department mainphasel ports f c j embedded objects lij source qj sink qj networkenter lij networkexit qj network lij doctor p procroom qj scope lij procedure qj movetoexit lij movetoprocroom presentation q simulation j jl problems l el oj main qi person oj ii ecprocess el i palette i el i model action illii analysis i t presentation i ii connectivity ft enterprise aa source sink hold g delay dod queue match select output split combine resource pool i description lm properties l console ll seize el release i procedure delay aar service enter general name iprocedure show name d ignore d public show at runtime create presentation i exit c lock statistics descrii tion type idelay t extends entity package icom xj anylogic libr aries enterprise generic parameters entity aa conveyor jfj batch unbatch delay time is d elay time specified explicitly path length i speed uniform dropoff ft pickup restricted are r l c i j resources frequently resources are required to initiate a particular phase of processing a doctor resource to administer surgery to a patient entity a piece of diagnostic equipment resource to image a patient entity an ekg to resource to record from a patient entity a gurney or bed resource for a patient entity distinctions amongst these resources portable vs fixed mobile with agency resources a network is often associated with multiple types of resources when an agent cannot obtain seize a resource they enqueue and wait for that resource to be released by another entity these resources live in pools of interchangeable resource units a seized resource comes from the pool a released resource returns to the pool if wish to be able to choose particular resources from a pool create in different pools and select desired pool seizes seeks to achieve exclusive association with a resource otherwise waits types of resources associated releases association with a resource so others can with the network each in a be associated with use it resources resource pool main flow operators associated with resources all resources network seize network release portable resources network attach networkattach detach networkattach mobile resources network sendto networksendto defining resource pools resource pool type capacity of pool number of units of resource present static fixed moving mobile portable can be carried another flow chart source source of entities defines rules governing origination of entities to enter into network network enter informing newly created entities of the available resources securing association with or more resources network seize resource pools with whose resources entity is seeking association this seizes one resource unit from each pool one resource may be seized while waiting for blocking for the other network send to moving a seized resource to a resource entity or place here sending an already seized resource to another resource example of simultaneously moving multiple resources together via sendto here sending already seized resource to the entity the patient network attach associating entity with specified seized resources or those nearby so move together henceforth network move to moving an entity to a resource or node resource to which agent should move here already seized unit from this resource pool nb because resources are attached seized this moveto will move entity but also bring moving portable resources along doctor scope network detach so entity can be physically separated from resources while remaining associated w them releasing associated resources resource pools whose resource units one is releasing moving resource doctor returns to home location after release of association with entity visual depiction entities are associated with icons resources are associated with locations icons networks are associated with routing paths often want to move resources or icons among different visual locations specific points e g a storage closet for mobile resources points associated with fixed resources e g a mri scanner association of network with paths i t anylogic advanced educational use only eile dit liew model iindow t elp q q i l q i ffl i lj ii qi yj t get support oj mainphasel oj oj ecprocess li palette w ophthalmology department mainphase t networkattach movetoprocroom procedure networkdetach returnscope j model ports ffi embedded objects j ij source ij j sink ij j networkenter qj networkexit release movetoexit network exit sink action analysis presentation tr connectivity qj network qj doctor qj procroom j ij scope ij j procedure ij j movetoexit ij j movetoprocroom presentation simulation l i problems w rlnrtnr nrnrp nnm rnnf enterprise w source sink hold delay dod queue match select output split combine resource pool i descrip ti on l console i j network network r j seize release o jr service general parameters statistics description name inetwork show name d ignore d public show at runtime create presentation type inetwork package icom xj anylogidbraries enterprise enter exit clock aa conveyor group of network shapes hide network shapes i net workgroup batch dropoff when item is at a node draw at random position within node draw at the top left corner of the node gdl pickup restricted are enable priorities d replication restricted are a network l il l j jig more libraries associated group of presentation the network will know about these e g forrouting presentation of entity appearance of the entity when moving through the network presentation properties of a resource appearance of the resource units in this resource in idle busy states home position of resource in presentation network associated with network entering the network where with what logical presentation network speed to use when entity moves around movement network defined by polygons rectangles recall this is the visual presentation network associated with the logical network these polylines and rectangles are the elements over which the entities resources move recall the location of the rooms is given as being the path across nodes defined by the polyline polyline describes the location of the procedure rooms the rectangles touched by this poly ine vertices are the room locations moving entity to a node n liilli l i il iiilll lr file edit view model window help j igi g lg j j t i c j q jj i if i li ii qi l c i j ft get support j o r mi i a t project ll i search i el al person iol main iol person iol mainphasel ol i ol ecprocess i el i palette n l lijjl ophthalmology department mainphasel ports ai embedded objects ij j source a model i action i source networkfnter networkseize sendtostorage sendtopatient analysis i i ll al if j networkfnter ij j networkfxit r i connectivity i tj enterprise aa if j network aa ll i ij j doctor networkattach movetoprocroom procedure networkdetach returnscope source a if j scope ij j procedure qj movetoexit il l e hold delay if j movetoprocroom dod queue presentation l release networkfxit sink match select output i simulation j v i i v split combine problems n l el resource pool i descrietion i loec properties ll i console i v el m seize ij j movetoexit networkmoveto release service general parameters name imovetoexit i show name d ignore d public show at runtime i create presentation i enter exit statistics type inetworkmove to t extends entity i generic parameters i entity i clock description package icom xj anylogic libraries enterprise i cc conveyor j batch unbatch dropoff pickup f restricted are restricted are a network replication l il l j i i g more libraries i selection i j subclassing a valuable tool so as to customized the desired system behavior it can be useful to customize entities resources resource units to e g carry around additional information e g associated external agent in agent based model history information etc particular specialized network types because the original entities resource units are classes this can be accomplished via subclassing subclass entity resourceunit if do this parameterize generics by subclass type s e g networkresourcepool s collecting outputting data from anylogic nathaniel osgood march recording of results a frequent modeler need is to record some components of model state over time state variables e g stocks states of agents summaries of model state we informally term this a trajectory file trajectory recording is only supported by anylogic professional anylogic does allow for definition of datasets that record recent values of parameters statistics summarizing model state reporting on values of data sets as a graph or table techniques for outputting data ad hoc exports from variables manual copies from visible datasets export to files writing to console export to databases anylogic professional dataset archiving capturing images of graphs cross method output tips a convenient mechanism is to periodically output data using events e g every time unit beyond output be sure to save information on context of run model version use unique id that increment whenever change model parameter assumptions intention think carefully about whether want to save away intermediate data hands on model use ahead load sample model sir agent based calibration via sample models under help menu techniques for collecting outputting data ad hoc exports from variables pre prepared methods statistics charts manual copies from visible datasets export to files writing to console export to databases anylogic professional dataset archiving capturing images of graphs add an experiment anylogic university evaluation use only fil e ed it view mod el wind ow help t e t i p rojects el el person el m a in n el i pa lette ll el o i sir agen n i ii a maio i new igi mod el i t gen e ra l i pe rscg op e n ct rl o pa ramele r f cali b event save ct rl s cf exp e rim ent dyna mic ev ent lhl a save as b d im ension p revert j ava class p la in va ri a bl e colle ct io n cl o se ci j ava int erf ac e f u nct io n close othe rs libra ry q ta bl e fu nctio n cl o se all g nln f ecli o us p o rt coll a pse all i on infe ct iou sds conn ec tor sdu rnlio n environment cut ctr l x copy ctrl c babil ity past e ctr l v delete delete system dyn a mics tz ref resh i s tatec hart bu il d act io nc hart run ii an a lysis pro ble m ll cl export ii presen tatio n descripti o n crea t e do cum ent at i o n h d tea m i il l i controls ch eck for sn aps hot comp at i b ili ty nsol e l v el s r agent based calibration model pict u res i ill i sir age nt bas ed ca li brat io n mod el a general n a m e sir ag e nt ba se d ca li brat i o n bject de pend enc i e w enterprise libra ry descr ipt io n pa ck ag e a ai ibration ped est ria n librnnj fil e c progr a m fil e anyl og ic un iversity plug ins co m xj a nylog ic exa mp l e pa le tt e i add an experiment save the resulting model to avoid overwriting the other model run the experiment to verify functionality click on variable ninfectious graph of variable right click to copy the numeric data pasting into excel press red stop button to terminate execution techniques for collecting outputting data ad hoc exports from variables pre prepared methods statistics charts manual copies from visible datasets export to files writing to console export to databases anylogic professional dataset archiving capturing images of graphs statistics charts a population of agents can have associated statistics that calculate values examples of things that can be computed with using anylogic statistics count of agents in the population for which certain condition predicate evaluates to true function of the values of some expression over the population maximum value minimum value average value sum total over population statistics can be defined as properties of the population select people and choose statistics also expand statechart under person click add statistics fill in the condition predicate on person continue typing full expression expression item statechart isstateactive person susceptible example statistics the population in which the statistics are to be calculated what statistics we wish to calculate name the statistic countsusceptible run the model and click on people the statistic should be visible close the model after you re done drag a time plot from the palette to the main canvas enlarge the chart any lo g ic un iv e rs ity evalu ation use o n ly eil e d it liew m od el indo w e l p ill t i i ql q t q i f t i i q qi cij g et su ppo rt t proj ects n i el bj pe rso n main l j al ple expe rim ent i el i p a l ett e n i el my sir agent ba sed ca li b rat i o n exte nsi o ge n e ra l ma p e rson syst e m dyna mics i c st at ec h a rts stat ech a rt areaside nln fec l io u i a t ec hart state ch a rt tota lpop u la t io n inl e ct iou sds ad io n c ha rt susc e ptib le ave rag elll n ess du ra t i inf e ct i o n con tactr ate inf e ct i o us inf e ct io n proba bilily rec ov e ry re co v e red cont a ct sim pl e experim ent m a in environment ca li b rat io n m a in i i ii ana ly is f i data se t l ist ics al o g ra m da t a l l is t og da t a ba r chart mont eca dh i st og ra m m a in lht an a l y i dat a pre se nt at i o n p e op le i st ac k cha rt pie cha rt p l o t time p l o t time stac k cha rt i m i time go l o r cha rt i ill j prob l e ms n l v el m prop e rties n i co ns o l e v el l plot time plot l l ist o g ra m l l is t o q prese nl at io n des c ri pt i o n loc at i g e ner al n a m e p l ot lo sho w nam e c ign o re pub li c adva nc e d j coni rols add da ta it e m i ap pea ran ce des crip t i o n tim e wind ow i conn ec t ivity pict u res ve rti c al c ale iauto from i i to i o bjects if ent e rp rise lib ra ry do not upd at e a uto m at i c a ll y p l ot t im e plot sel ect io n i x y click add data item put in people and press ctrl space choose count susceptible now run the model notice that this only goes back to time stop the simulation and click on the plot change time window display size to this captures the full time range techniques for collecting outputting data ad hoc exports from variables pre prepared methods statistics charts manual copies from visible datasets export to files writing to console export to databases anylogic professional dataset archiving capturing images of graphs datasets datasets store recent values of some quantities from the model datasets can be exported easily using custom code this can simply call the dataset to string method output datasets run the experiment click on infectious ds click on infectiousds to see data in dataset right click and select copy call up excel and paste into it dataset properties chart use of datasets ad hoc export begins as a small chart copying data data exported from ad hoc chart techniques for outputting data ad hoc exports from variables manual copies from visible datasets capturing images of graphs export to files writing to console anylogic professional dataset archiving export to databases manual output from datasets right clicking gives context menu copied data can be pasted into excel declaratively specifying datasets simple supported dataset types holds values only no timestamps timed holds values and timestamps phase holds pairs of values but no timesamps histogram can define bins for data set data set will record falling in each bin techniques for outputting data ad hoc exports from variables manual copies from visible datasets capturing images of graphs export to files writing to console anylogic professional dataset archiving export to databases pros output to console easy to program activeobject traceln stringstr outputs string to console system out println stringstr readily visible copy paste to another document cons may be mixed with other output easy to miss other output limited length depends on memory to copy techniques for outputting data ad hoc exports from variables manual copies from visible datasets capturing images of graphs export to files writing to console anylogic professional dataset archiving export to databases data output to file pros simple to perform relatively easy to import into e g excel r etc files can be readily archived cons awkward to draw combine from multiple files denormalization requires either duplication of scenario wide information e g parameter values on each row separate header section later section example code to export dataset to file fileoutputstream fos new fileoutputstream filename printstream p new printstream fos p println datasetname tostring outputs tab delimited values techniques for outputting data ad hoc exports from variables manual copies from visible datasets capturing images of graphs export to files writing to console anylogic professional dataset archiving export to databases output to databases tradeoffs pros more flexible than string output to file can query from diverse tools e g excel r spss sas etc can easily clean up for larger databases transactional either writes entirely or not at all can query from remote machines cons more programming need to set up a database output to databases steps one time install database on computer add reference to database libraries each time during simulation open database connection at start of model optionally insert model version parameter information into the database periodically during simulation insert values into databases at end of model execution close database connection database dependencies mysql database example simple database class setup for database class example database output code a database query language sql statement requesting that the database class execute the sql statemen checking to make sure that the insert worked properly individual based models introduction tradeoffs tools nathaniel osgood april dynamic models for health classic aggregate models differential equations population classified into or more state variables according to attributes state variables parameters population recent individual based models governing equations approach varies each individual evolves state variables parameters population contrasting model granularity birth and death rate time random seed mortality rate birth rate births contacts per susceptible per contact risk of infection force of infection incidence fractional prevalence mean time with disease recovery population initial population initial fraction vaccination susceptible mortality mortality rate infective mortality recovered mortality mortality rate mortality rate vaccinated q annual likelihood of vaccination susceptible fraction vaccinated mortality time of population mortality rate interacting individuals age smoker never aging never never smokers age doctor initiation smokers initiation smoker never age aging current smokers smoker current cessation relapse cessation aging former smokers relapse friends age smoker current friends age smoker current parent age smoker former network embedded individuals irregular spatial embedding regular spatial embedding agenda motivations context comparing aggregate individual based models granularity tradeoffs tools for individual based modeling individual based modelers in sd individual based models in agent based tools other tradeoffs looking forward importance of heterogeneity heterogeneity often significantly impacts policy effectiveness policies preferentially affect certain subgroups infection may be maintained within certain subgroups even though would tend to go extinct with random mixing in the entire population policies alter balance of heterogeneity in population shifts in the underlying heterogeneity can change aggregate population statistics given a non linear relationship inaccurate to use the mean as a proxy for whole distribution assessing policy effectiveness often requires representing heterogeneity flexibility in representating heterogeneity is hard to achieve in aggregate coarse grained models longitudinal heterogeneity there can be great heterogeneity not only cross sectionally but also longitudinally particularly in a path dependent system trajectories that are originally close may diverge dramatically capturing this longitudinal disparity can be important for understanding intervention effects agenda motivations context comparing aggregate individual based models granularity tradeoffs tools for individual based modeling individual based modelers in sd individual based models in agent based tools other tradeoffs looking forward elements of individual state example discrete ethnicity gender categorical infection status continuous age elements of body composition metabolic rate past exposure to environmental factors glycemic level example of continuous individual state example of discrete states binary presence in discrete state example of likelihood of presence in discrete state feedbacks some aggregate feedbacks lie within individual agent new nicotine exposure likelihood of smoking severity of nicotine addiction likelihood of johnny smoking johnny smoking severity of addiction feedbacks many aggregate feedbacks are between agents new smokers aggregate model johnny father smoking johnny perception of desirability of smoking perceived desirability of smoking agent based model timmy perception of desirability johnny smoking of smoking agenda motivations context comparing aggregate individual based models granularity tradeoffs tools for individual based modeling individual based modelers in sd individual based models in agent based tools other tradeoffs looking forward granularity selection problem specific selection of granularity is a function of question that are asking not of the true nature of the system quanta of most obvious system components may not align with needs for insight may gain benefits from higher level representation many high level behavior of complex systems can be explained with very simple models often gain greater insight from simpler model cf gas laws vs lattice gas model may wish to seek lower level model small infection spread model characterization at level of immune response rather than monolithic person myth of individual based models as modeling from the bottom up a single person is a natural locus of description presents for care lives dies coupled internal systems but the world has no natural bottom it is frequently desirable to include within a person a great deal of within the skin detail the issues of model depth breath are just as pressing in individual based models as in aggregate modeling contrasting benefits aggregate models individual based models easier construction calibration parameterization formal analysis control theoretic eigenspace techniques understanding performance lower baseline cost population size invariance less pronounced stochastics less frequent need for monte carlo ensembles better fidelity to many dynamics stronger support for highly targeted policy planning ability to calibrate to validate off of longitudinal data greater heterogeneity flexibility better for examining finer grained consequences e g transfer effects w i pop quicker construction runtime network spread more time for understanding refinement simpler description of some causal mechanisms key needs motivating individual based modeling need to calibrate against information on agent history need to capture progression of agents along multiple pathways e g co morbidities wish to characterize learning by and or memory of agents based on experience or strong history dependence in agents need to capture distinct localized perception among agents seeking to intervene at points in change behavior on explain phenomena over or explain dynamics across networks seek distinct interventions for many heterogenous categories need to capture impact of intervention across many categories when it is much simpler to describe behavior at indiv level seek flexibility in exploring different heterogeneity dimensions needs of stakeholders to engage with individual based models key needs motivating aggregate need to executebqausicekdly m e go dfoerluinsegr interaction describe understand system behaviour across all possible values for parameters seeking to mathematically analyze the model e g to determine location or stability of equilibria need to calibrate to match lots of data want to use mathematical tools such as control theory or proofs desire of stakeholders to work at higher level behavior for different subgroups differs only in degree no recourse to software engineering knowledge lack of detailed knowledge of network structure individual level behaviour individual level data simpler causal description understanding of individual behavior sometimes exceeds that of collective behavior response to locally visible incentives company response to competition young person response to peer pressure individual response to scarcity of good sometimes it is very difficult to derive a priori the aggregate dynamics resulting from individual behavior individual model can be simpler more transparent fidelity to dynamics adequate characterization of system causal processes may require fine grain representation rich heterogeneity learning and adaptation response to local incentives memoryful processes behavior over persistent networks aggregate behavior is not necessarily the same as population behavior of average individual may be able to calibrate an aggregate model to results of individual level model post hoc example of concern history information heterogeneity with respect to individual history can be highly important for future health whether vaccinated in utero exposure degree of glycemic control over the past decade exposure to adiposity previous exposure to a pathogen in some areas of health we have access to longitudinal data that provides information on individual historical trajectories capturing history information individual based model both discrete continuous history information can be readily captured categorical discrete state in statechart or variable continuous variable readily able to capture records of trajectories aggregate model categorical discrete limited discrete history information can be captured by disaggregating stocks curse of dimensionality provides tight limits on of aspects of history can be recorded continuous almost always infeasible very complex to provide distributions of trajectories via convolution of potentially changing psfs of stocks longitudinal fidelity individual based models an individual based model provides easily accessible cross sectional and longitudinal descrip of system state the system state at a particular moment in time is cross sectional by following recording the trajectories of particular individuals we can obtain longitudinal description in calibration validation we can do rich comparison of both longitudinal and cross sectional descriptions against available point or time series data it is in principle possible to have a model that accords with cross sectional data but which is at odds longitudinally longitudinal fidelity aggregate models an aggregate model provides an ongoing series of cross sectional descriptions of system state in calibration validation we can do rich comparison of these cross sectional descriptions against available point or time series data because the model does not track individuals we generally cannot explicitly extract model longitudinal trajectories from the model for comparison with historical data we have longitudinal trajectories aggregate models trajectories while they may not be easy to study explicitly aggregate models do impose some assumptions about the trajectories of individuals this reflects the assumption of a markovian system an aggregate model will assume that the placement of an individual at a particular stock in the model adequately summarizes all the historical information needed to describe future dynamics while it is somewhat awkward to do we can test the longitudinal data at different particular components to see how well it holds up to markov example of markovian concern for example such a model assumes that the route of entry to a stock is independent of the route of exit e g if in longitudinal data we don t see independence between routes of entry to a model stock routes of leaving that stock that feature of the system may be poorly approximated by that model in some cases this could be of concern shortcomings of aggregate comparisons if we find that aspects of the data are markovian with respect to model stocks we can be hopeful about our structure common problems due to attribute based disaggregation a model that incorporates all necessary historical information is too big we may not have data on transitions through particular model stocks and thus cannot test if it adheres to markovian assumptions with respect to those stocks we cannot easily compare longitudinal model predictions vs historic data see next slide comparisons of model history that are difficult in an aggregate model proportions of people with certain history characteristics e g fraction of women who develop who have had or more bouts of gestational diabetes those with a certain duration of time separating tb infection sand active tb can be very valuable for calibration this is critical for assessing model accord with observed effect size relative risk odds ratio model vs historic trajectories e g for timing of some transitions for people with certain history characteristics example of additional information from longitudinal data consider trying to distinguish pairs of situations e g smoking situation one set of people quit stay quit as former smokers another set remain as current smokers situation the entire set of people cycle through situations where they quit relapse repeat these two situations have very different health consequences we d probably choose vary different sets of interventions for these two situations similar examples are easy to imagine for obesity stis tb glycemic control diabetes etc trajectories summary if either or both of the following is true you have significant longitudinal information you d strongly like the model to match you have good reason to think that trajectory history has important consequences for health then you should build a model that captures this history information by disaggregating stocks you can capture limited discrete history information in an aggregate model e g whether a person was exposed in utero time since quit for fs whether a woman has had a history of gestational diabetes there is significantly greater flexibility for collecting continuous or discrete history information for guiding individual dynamics for calibration validation comparison to historic longitudinal data calibration validation comparisons we can compare statistics from histories in an individual based model to statistics from actual histories see if matches non markovian nature see how matches distribution of times recall importance of heterogeneity heterogeneity often significantly impacts policy effectiveness policies preferentially affect certain subgroups infection may be maintained within certain subgroups even though would tend to go extinct with random mixing in the entire population policies alter balance of heterogeneity in population shifts in the underlying heterogeneity can change aggregate population statistics given a non linear relationship inaccurate to use the mean as a proxy for whole distribution assessing policy effectiveness often requires representing heterogeneity flexibility in representating heterogeneity is hard to achieve in aggregate coarse grained models impacts of heterogeneity on policy effectiveness value of breast cancer detection park lees impact of airbags on deaths shepherd zeckhauser value of hernia operations neuhauser impact of cardiovascular disease interventions chiang controlling blood pressure shepherd zeckhauser effectiveness of mobile cardiac care unit shepherd zeckhauser value of breast cancer treatment fox taeuber paradox keyfitz frequent heterogeneity concerns no clear boundaries at which to divide people up into discrete categories many dimensions of heterogeneity simultaneously capturing state with respect n factors requires n dimensions of heterogeneity need to consider progression along many dimensions simultaneously challenges for aggregate model formulation heterogeneity two aggregate means for representing heterogeneity are limited attribute based disaggregation need n dimensions to capture individual state with respect to n factors poor geometric scaling to large dimensions of heterogeneity global structural equation changes required to incorporate new heterogeneity dimensions awkwardness in stratifying co flows efficient and precise but highly specialized fragility of multi dimensional subscripting combinatorial subscripting multi dimensional progression parallel transitions parallel state transition diagrams a person is in some particular state with respect to each of these condition specific state transition diagrams this requires representing combinations of possibilities in an aggregate model capturing heterogeneity in individual based vs aggregate models consider the need to keeping track a new piece of information for each person with d possible values e g age sex ethnicity education level strain type city of residence etc aggregate model add a subscript this multiplies the model size number of state variables into which we divide individuals by d individual based model add field variable param if model already has c fields this will increase model size by a fraction c desired flexibility in representing heterogeneity it is desirable to capture heterogeneity in a flexible fashion more judicious exploration of whether to represent heterogeneity examine whether some observed covariation might simply be due to colinearity represent added heterogenity dimensions with no causal interaction see if model covariations matches what is seen in external world e g represent age in a tb model see if rates of ltbi by age in the model match age specific infection rate observations try adding in new dimension of heterogeneity effects and see if has impact that is both substantive plausible challenges for model formulation persistent interaction network topologies can affect qualitative behavior aggregate representations of network structure are expensive and awkward ibm permit expressive efficient characterization of both dense sparse networks while percolation over many topologies can be simulated in aggregate models parameter calibration often requires finer grained simulation network clustering preliminary case contact network restricted to nodes of degree clusters distinctive by geography ethnicity data extraction a al azem identifying bridging individuals preliminary case contact network restricted to nodes of degree data analysis image a al azem slides adapted from external source redacted from public pdf for copyright reasons multi scale phenomena frequently we are concerned about phenomena on a variety of scales aggregate societal policy level institutional level individual level intra institutional level slides adapted from external source redacted from public pdf for copyright reasons finer grained policy planning in the presence of networks or non well mixed populations big difference in effects of targeted interventions e g targeted intervention within scale free network impact of incentives on competition and cooperation impact of road structure on traffic jams parameterization calibration individual based models have many parameters estimating all of the parameters can require much effort calibration generally underdetermined large of possible sets of parameter values that could calibrate well may need to make simplifying assumptions pronounced individual level stochastics frequently require monte carlo calibration individual based model performance scaling performance varies with population size large populations impose high computational resource demands scaling can be superlinear e g o connections to consider this can frequently lead to simulations taking minutes at the least commonly hours or even days desire to characterize stochastic nature of individual level behavior typically requires monte carlo approaches this can lead to days or weeks to complete memoryless vs memoryful processes ode models can adequately capture only memoryless transition processes out of a stock stocks treated as well mixed transition probability does not depend on residence time memoryful processes can be approximated but requires changing model structure to reflect a simple functional relationship nth order delays ibm can record residence time in state allow probability of transitions to depend on this slides adapted from external source redacted from public pdf for copyright reasons individual vs aggregate models necessary tradeoffs both individual level and aggregate modeling have inherent and non trivial tradeoffs both approaches likely to retain strong appeal in systems modeling areas of advantage of individual based modeling examining finer grained consequences network spread transfer effects within population detailed spatial dynamics effects of population heterogeneity effects of highly targeted policies effects of individual level synergies e g multiple risk factors simple individual based description of causal mechanisms sufficient individual level distributional data are available for policy modeling beyond exploratory models inevitable tradeoffs high scope breadth of boundary practical constraints data time cost transparency low limited value high low aggregation agenda motivations context comparing aggregate individual based models granularity tradeoffs tools for individual based modeling individual based modelers in sd individual based models in agent based tools other tradeoffs looking forward hallmarks of complex systems delays represented at individual environment level generality of rules allows for memoryful stochastic processes nonlinearities rules expressed with arbitrary algorithms can encode arbitrary functions of model state stochastics or uncertainties many agents behaviors will be stochastic system dynamics individual based modeling individual based models can be created using traditional system dynamics software small populations separate stocks for each individual hand drawn connections larger populations subscripting stocks by population member binary network matrices stock flows in other dynamic modeling software e g in anylogic system dynamics methodology feedback centric reasoning process based work individual based model in vensim all of these stocks their associated fl population member via population m population member subscripting example interactions between global local levels a global level aggregate cross population factor example individual level risk factors an individual level risk factor another individual level risk factor here represented categorically but we could represent it as a continuous variable e g cumulative smoke exposure some estimate of cumulative physiologic damage from smoke a moving average of smoke exposure etc impact of risk factors on individual dynamics population subscripting tradeoffs advantages conceptually simple can sd tools state trajectory file recording easy construction structure visualization no programming sensitivity analysis easy to aggregate disadvantages difficult to visualize network structure spread or spatial embedding awkward to realize changing population size agent based systems a glimpse current agent based model characteristics one or more populations composed of individual agents each agent is associated with some of the following state continuous or discrete e g age health smoking status networks beliefs parameters e g gender genetic composition preference fn rules for interaction traditionally specified in general purpose programming language embedded in an environment communicate via messaging environment emergent aggregate behavior array output fonnat f l lll connectivity atrix population at time runs test agent based modeling we can capture individuals in many ways i view agent based models abm as a type of individual based modeling that encapsulates a given individual as a software object with methods properties objects provide a convenient abstraction for individuals agent based models currently require writing at least some code in programming languages we can formulate sd models w i agent based tools i view such models as simultaneously sd abm we can follow an sd process to build use agent based models a model in anylogic dynamic behavior early steady state behavior agent based modeling we can capture individuals in many ways i view agent based models abm as a type of individual based modeling that encapsulates a given individual as a software object with methods properties objects provide a convenient abstraction for individuals agent based models currently require writing at least some code in programming languages we can formulate sd models w i agent based tools i view such models as simultaneously sd abm we can follow an sd process to build use agent based models the current package deal abm anylogic supports individual based or aggregate no trajectory files both discrete continuous rules states primarily imperative specification algorithmic imperative little no explicit mathematical semantics modularity mechanisms no metadata traditional system dynamics packages supports individual based or aggregate trajectory files well supported poor discrete rule support declarative specification equational notation reasoning explicit mathematical semantics monolithic limited metadata unit checks current package deal modeling implications from my perspective current abm and tsd packages both have important advantages central points looking forward most current differences reflect important but non essential methodological choices tool characteristics in the long run these differences will likely lessen and the choice that will remain is that of model granularity both individual based models and aggregate models will play important roles in system dynamics there are good reasons to use all of individual based models aggregate models and hybrid systems uncertainty stochastics sensitivity analysis nathaniel osgood cmpt march types of sensitivity analyses variables involved one way multi way type of component being varied parameter sensitivity analysis parameter values structural sensitivity analysis examine effects of model structure on results type of variation single alternative values monte carlo analyses draws from probability distributions many types of variations frequency of variation static parameter retains value all through simulation ongoing change stochastic process accomplished via monte carlo analyses key for des abm model uncertainty here we are frequently examining the impact of changing our assumptions about how the system works our decision of how to abstract the system behaviour structural sensitivity analyses vary structure of model see impact on results tradeoffs between choices frequently recalibrate the model in this process here we are considering uncertainty about how the current state is mapped to the next state predictor corrector methods dealing with an incomplete model some approaches e g kalman filter particle filter are motivated by awareness that models are incomplete such approaches try to adjust model state estimates on an ongoing basis given uncertainty about model predictions new observations assumption here is that the error in the model is defined by some probability distribution static uncertainty sensitivity analyses in variation one can seek to investigate different assumptions policies same relative or absolute uncertainty in different parameters may have hugely different effect on outcomes or decisions help identify parameters initial states that strongly affect key model results choice between policies we place more emphasis in parameter estimation interventions into parameters exhibiting high sensitivity content redacted for copyright spider diagram each axis represents a change in a particular parameter this proportional change is identical for the different parameters compliance the distance assumed by the curve along that axis represents the magnitude of response to that change note that these sensitivities will depend on the state of system systematic examination of policies female 44 13 male 44 uptake relapse cessation tengs osgood lin sensitivity analyses in vensim s control edit the filename to save changes to a different control file filename monte carlo sir vsc i choose new file clear settings number of j noise simulations seed i multivariate c latin univariate c latin grid warning messages c file l iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii active parameters drag to reorder noise s eed parameter mean time to recover model value delete selected selected add editing distribution vector r ok cancel sensitivity in initial states frequently we don t know the exact state of the system at a certain point in time a very useful type of sensitivity analysis is to vary the initial model state in vensim this can be accomplished by indicating a parameter name within the initial value area for a stock varying the parameter value in an agent based model state has far larger dimensionality can modify different numbers of people with characteristic location of people with characteristic etc imposing a probability distribution monte carlo analysis we feed in probability distributions to reflect our uncertainty about one or more parameters the model is run many many times realizations for each realization the model uses a different draw from those probability distribution what emerges is resulting probability distribution for model outputs example resulting distribution empirical fractiles static uncertainty impact on cost of uncertainty regarding mortality and medical costs incremental costs b b b b multi way sensitivity analyses when examining the results of changing multiple variables need to consider how multiple variables vary together if this covariation reflects dependence on some underlying factor may be able to simulate uncertainty in underlying factor performing monte carlo sensitivity analyses in vensim need to specify three things the parameters to vary how to vary those parameters which model variables to save away how what parameters to vary s control edit the filename to save changes to a different control file filename simple sir vsc number of noise simulations seed warning messages active parameters drag to reorder annual birth and death r ate ran mal delete selected selected add editing distribution parameter per contact risk of infection random uniform model minimum maximum value value value model values to save away monte carlo analyses sensitivity results prevalence an observation at this point in time would produce a histogram approximating a distribution for fraction of susceptibles sensitivity results fraction of susceptibles stochastic processes system dynamics models are traditionally determnistic as will be discussed in tutorials models can be made stochastic transitions between states e g duration of infection or immunity transmission of infection as a result there will be variation in the results from simulation to simulation summarizing variability to gain confidence in model results typically need to run an ensemble of realizations deal with means standard deviations and empirical fractiles as is seen here there are typically still broad regularities between most runs e g rise fall need to reason over a population of realizations statistics are very valuable fractile within which historic value falls mean difference in results between interventions closing question how can we best adapt our policies to deal with ongoing uncertainty we are dealing here with making decisions in an environment that changes over time this uncertainty stochastic variability time day uncertainty regarding parameter val there is an incredibly vast of possible policies stochastic processes in vensim baseline 5828 time day uncertainty stochastics sensitivity analysis nathaniel osgood cmpt march types of sensitivity analyses variables involved one way multi way type of component being varied parameter sensitivity analysis parameter values structural sensitivity analysis examine effects of model structure on results type of variation single alternative values monte carlo analyses draws from probability distributions many types of variations frequency of variation static parameter retains value all through simulation ongoing change stochastic process accomplished via monte carlo analyses key for des abm model uncertainty here we are frequently examining the impact of changing our assumptions about how the system works our decision of how to abstract the system behaviour structural sensitivity analyses vary structure of model see impact on results tradeoffs between choices frequently recalibrate the model in this process here we are considering uncertainty about how the current state is mapped to the next state predictor corrector methods dealing with an incomplete model some approaches e g kalman filter particle filter are motivated by awareness that models are incomplete such approaches try to adjust model state estimates on an ongoing basis given uncertainty about model predictions new observations assumption here is that the error in the model is defined by some probability distribution static uncertainty sensitivity analyses in variation one can seek to investigate different assumptions policies same relative or absolute uncertainty in different parameters may have hugely different effect on outcomes or decisions help identify parameters initial states that strongly affect key model results choice between policies we place more emphasis in parameter estimation interventions into parameters exhibiting high sensitivity spider diagram each axis represents a change in a particular parameter this proportional change is identical for the different parameters the distance assumed by the curve along that axis represents the magnitude of response to that change note that these sensitivities will depend on the state of system systematic examination of policies fem ale 44 13 m ale 44 64 uptak e relapse cess ation tengs osgood lin add new parameters variation experiment setting ranges for parameter variation sensitivity exploration in anylogic sensitivity analyses in vensim sensitivity in initial states frequently we don t know the exact state of the system at a certain point in time a very useful type of sensitivity analysis is to vary the initial model state in vensim this can be accomplished by indicating a parameter name within the initial value area for a stock varying the parameter value in an agent based model state has far larger dimensionality can modify different numbers of people with characteristic location of people with characteristic etc imposing a probability distribution monte carlo analysis we feed in probability distributions to reflect our uncertainty about one or more parameters the model is run many many times realizations for each realization the model uses a different draw from those probability distribution what emerges is resulting probability distribution for model outputs example resulting distribution empirical fractiles static uncertainty impact on cost of uncertainty regarding mortality and medical costs incremental costs b b b b multi way sensitivity analyses when examining the results of changing multiple variables need to consider how multiple variables vary together if this covariation reflects dependence on some underlying factor may be able to simulate uncertainty in underlying factor performing monte carlo sensitivity analyses in vensim need to specify three things the parameters to vary how to vary those parameters which model variables to save away how what parameters to vary model values to save away monte carlo analyses sensitivity results prevalence an observation at this point in time would produce a histogram approximating a distribution for fraction of susceptibles sensitivity results fraction of susceptibles monte carlo analyses in anylogic when running monte carlo analysis we d like to summarize the results of multiple runs one option would be to display each trajectory over time downside quickly gets messy anylogic solution accumulate data regarding how many trajectories fall within given areas of value for a given interval of time using a data display the chart hands on model use ahead load sample model sir agent based calibration via sample models under help menu histogram data important distinction declining order of aggregation experiment collection of simulation simulation collection of replications that can yield findings across set of replications e g mean value replication one run of the model flexibility typically ignored in most anylogic models an experiment is composed of a single simulation which is composed of a single replication in most anylogic models which run ensembles of realizations a simulation is composed of only a single realization accumulating the dataset from other datasets monte carlo sensitivity analyses in anylogic monte carlo analyses in anylogic specifying distributions for parameters monte carlo output after all runs monte carlo output after all runs a glimpse of representing stochastic processes nathaniel osgood cmpt march recall project guidelines creating one or more simulation models placing data into the model to customize it to a particular context e g to a particular region running a baseline scenario with the existing model parameters and commenting on its plausibility running one or more what if scenarios with the model to explore different possible situations these situations could reflect the results of implementing different policies or different possible external conditions performing one or more sensitivity analyses in which assumptions in the model in the form of parameter values or structural elements of the model are changed a well structured written report describing the above and your findings report due date april what i d like to see in the report introduction background comment on baseline scenario choice plausibility of model results sensitivity analyses parameters and or structural investigation of what if scenarios potential policies external conditions model limitations ideas for possible extensions process learning implications for our understanding of the world project presentations presentations will be minutes in length seeking half days of presentations encourage all students to attend likely scheduling late in week of april just after report due afternoon of april monte carlo analyses in anylogic when running monte carlo analysis we d like to summarize the results of multiple runs one option would be to display each trajectory over time downside quickly gets messy anylogic solution accumulate data regarding how many trajectories fall within given areas of value for a given interval of time using a data display the chart hands on model use ahead load sample model sir agent based calibration via sample models under help menu i ct rram data r il l sll i b m file edit view model window help j rir j oi t i p q mi j iila di i ia i li ii c b l c i j jt get support t project n i search i el al ll i al calibration i ii main al montecarlolstorder i ii main i ii family al parametersvariation i el ll palette n i el ii sir agent based calibration i j b model i main parameters parameter plain variables flow aux variable fl environments d stock variable ffi embedded objects agent based sir event ihh analysis data dslnfectious dynamic event presentation plain variable person run replicat collection variable f calibration main g function main table function ihh analysis data l port main j i i initial state pointer plain variables ffi embedded obr ts laalllld i console i v el history state i g histogram data final state problems n i v el general name i show name d ignore d public d show at runtime environment j idescrip tion ito cation description horizontal axis value i i vertical axis value i i action i horizontal intervals i range from i i to i i analysis i vertical intervals i range from i i to i i presentation i envelopes lo o o i connectivity i do not update automatically enterprise library i i ii i auto update after every iteration i more libraries j important distinction declining order of aggregation experiment collection of simulation simulation collection of replications that can yield findings across set of replications e g mean value replication one run of the model flexibility typically ignored in most anylogic models an experiment is composed of a single simulation which is composed of a single replication in most anylogic models which run ensembles of realizations a simulation is composed of only a single realization accumulating the dataset from other datasets monte carlo sensitivity analyses in anylogic monte carlo analyses in anylogic specifying distributions for parameters automatic throttling of monte carlo analyses reminder statistical scaling consider taking the sample mean of n samples that vary independently around a mean if two samples x and y are independent samples of random variables x and y then var x y var x var y so if we have n indep samples xi from distribution x var n x nvar x i i if we scale a random variable by a factor the standard deviation scales by the same factor of the variance scales by i e stddev x stddev x var x var x statistics of sample mean recall sample mean xi m i from the preceding we have n n var n x xi i nvar x var x var m var i i n n this means that standard deviation for the sample mean of n samples varies as stddev m var m stddev x n so if we wish to divide the standard deviation of the sample mean by a factor of we need to take the number of monte carlo samples dynamic uncertainty stochastic processes examples of things commonly stochastically approximated stock market rainfall oil prices economic growth what considered stochastic will depend on the scope of the model detailed model individual behaviour transmission etc a meteorological model may not consider rainfall stochastic stochastic processes in vensim baseline 5828 time day making a vensim flow stochastic treat as a sensitivity analysis eile dit iew ayout model iools indows t simulation control standard i changes sensitivity advanced i pre post i run name u carlo small timestep umonte carlo sir vsc i edit noise seed imonte carlo sir j edit mean time to recover runs lo infective new recoveries tep total population view i hide times new roman setting the random seed to differ between simulations monte carlo analysis with fixed parameter values results of monte carlo simulation even without parameter variation substantial variability is still present stochastic processes in anylogic in anylogic abm and discrete event models network based modeling are typically stochastic transitions between states event firing messages frequent timing of message send target of messages duration of a procedure as a result there will be variation in the results from simulation to simulation summarizing variability to gain confidence in model results typically need to run an ensemble of realizations deal with means standard deviations and empirical fractiles as is seen here there are typically still broad regularities between most runs e g rise fall need to reason over a population of realizations statistics are very valuable fractile within which historic value falls mean difference in results between interventions closing question how can we best adapt our policies to deal with ongoing uncertainty we are dealing here with making decisions in an environment that changes over time this uncertainty stochastic variability 5828 time day uncertainty regarding parameter value there is an incredibly vast of possible policies stochastic processes in vensim baseline 5828 time day dynamic decision problems hybrid use of decision trees system dynamics models nathaniel osgood with some slides courtesy of karen yee cmpt march local context saskatchewan suffered the highest incidence of wnv in canada in and saskatoon health region shr reported and of the provincial cases in and respectively mosquito species culex tarsalis primarily responsible for spreading wnv in saskatchewan source penn state university adult pupa source html source terrestrial aquatic mosquito lifecycle larvae source unknown egg raft source dv mosquito environmental dependencies for increasing their numbers temperature average number of night temperatures above heat accumulated days habitat availability rainfall human dependencies for using protective measures perceived risk knowledge of wnv temperature health managers dilemma need to make decisions now taking into account uncertainties regarding mosquito population abundance wnv prevalence environmental conditions current forecasted human behaviour different levels of challenge in dynamic decision making type a making complex dynamic choices given some expected typical course of important factors outside our control here the focus is centred on building models that help us understand the complex impact of our choices given this expected course tough type b making complex dynamic choices when we can t anticipate the course of the important factors outside our control focus on both dynamic model and adaptive planning given uncertainty tougher implications type a when important exogenous conditions are known we often seek to identify stick to an optimal pre set plan don t have to worry much about unfolding external conditions they are known or unimportant type b rather than putting all our eggs in one basket it is typically best to avoid a pre set plan and instead to adaptively make choices over time what we will do over time will depend on what is observed decision making under dynamic uncertainty adaptive planning type a when important exogenous conditions are known we often seek to identify stick to an opttimhealp prerese snettaptiloann focuses on this type of dynamic decision problems don t have to worry much about unfolding external conditions they are known or unimportant type b rather than putting all our eggs in one basket we typically seek to avoid a pre set plan and instead to adaptively make choices over time what we will do over time will depend on what is observed adaptive decision problems relevant questions how do we make decisions now when the choice of the best decision depends so much on what plays out unfolds in things beyond our control temperature trends precipitation prevalence of infection in migratory bird populations should we make our decisions now despite these uncertainties or should we wait to see how things are trending before making decisions characteristics of adaptive decision problems can t count on one particular future trajectory unfolding for things outside our control choosing decisions now requires considering the different possibilities of what might unfold in the future we must make decisions over time as we observe things unfold the later we wait the more information we ll have it may be advantageous to decide to wait and see as to how things play out until a later decision in these conditions what decision we make at a particular point in time will depend on our current situation what we ve observed as happening to this point what we ve learned e g recent levels of growth state as given by stocks derived quantities possible future eventualities in light of what we ve already seen e g future levels of growth given recent growth our possible decision points in the future here we are balancing two desires to seize the moment and act early to wait and see what happens and decide on the basis of this a hybrid system architecture to address these tougher problems introduction to decision trees we will use decision trees both for diagrammatically illustrating decision making w uncertainty quantitative reasoning represent flow of time decisions uncertainties via events consequences deterministic or stochastic decision tree nodes decision choice node chance event node terminal consequence node time identifying the optimal decision rule to select decision rules we perform a rollback of the tree dynamic programming for terminal nodes pass up value for event nodes pass up expected value of children for decision nodes select whichever child offers highest value and pass up that value for this node example tree feature decision making best option extended example extended rule decision rules decision trees can be used to identify optimal decision rules remember optimality is in light of simplified assumptions a decision rule specify what we should do given any possible eventuality decision tree to structure policy space event node time expand capacity high later growth pl low later growth decision node expand capacity initial decision high early growth pe low early growth pe no expansion expand capacity no expansion pl high later growth pl low later growth pl high early growth pe expand capacity no expansion high later growth pl low later growth pl high later growth pl low later growth scenario terminal no expansion pl node expand capacity high later growth pl low later growth consequence low early growth pl for scenario pe no expansion high later growth pl low later growth pl terminology a static decision rule pursues the same predetermined decisions actions regardless of eventualities an adaptive decision rule varies its decisions actions based on which events have occurred observation static decision rules are rarely optimal a static decision rule observation the consequences observed at a particular terminal node are a function of the associated scenario particular sequence of decisions and events on the path leading to that terminal node and are the same regardless as to which decision rule that gives rise to this sequence an adaptive decision rule consequence7 consequence10 consequence14 observation the consequences observed at a particular terminal node are a function of the associated scenario particular sequence of decisions and events on the path leading to that terminal node and are the same regardless as to which decision rule that gives rise to this sequence analysis using decision trees decision trees are a powerful analysis tool addition of symbolic components to decision trees greatly expand power example analytic techniques strategy selection one way and multi way sensitivity analyses value of information decision tree w variables sens itivity ana lysis on poslate and possupportsorprise rely on newos fea ures d not rel on new os features c i zl o poslate risk preference people are not indifferent to uncertainty lack of indifference from uncertainty arises from uneven preferences for different outcomes e g someone may dislike losing x far more than gaining x value gaining x far more than they disvalue losing x individuals differ in comfort with uncertainty based on circumstances and preferences risk averse individuals will pay risk premiums to avoid uncertainty risk preference decision tree preview categories of risk attitudes risk attitude is a general way of classifying risk preferences classifications risk averse fear loss and seek sureness risk neutral are indifferent to uncertainty risk lovers hope to win big and don t mind losing as much risk attitudes change over time circumstance preference function formally expresses a particular party degree of preference for satisfaction with different outcomes time level of conflict quality can be systematically derived used to identify best decision when have uncertainty with respect to consequences choice with the highest mean preference is the best strategy for that particular party challenge identify these preference functions on the board risk attitude in preference fns identifying preference functions simple procedure to identify utility value associated with multiple outcomes interpolation between these data points defines the preference function notion of a risk premium a risk premium is the amount paid by a risk averse individual to avoid risk risk premiums are very common what are some examples insurance premiums higher fees paid by owner to reputable contractors higher charges by contractor for risky work lower returns from less risky investments money paid to ensure flexibility as guard against risk consider a risk averse individual with preference fn f faced with an investment c that provides chance of earning chance of earning average money from investment average satisfaction with the investment f f this individual would be willing to trade for a sure investment yielding satisfaction instead can get satisfaction for a sure f we call this the certainty equivalent to the investment therefore this person should be willing to trade this investment for a sure amt of money certainty equivalent example mean satisfaction with investment certainty equivale of investment mean value of investme example cont d risk premium the risk averse individual would be willing to trade the uncertain investment c for any certain return which is equivalently the risk averse individual would be willing to pay another party an amount r up to for other less risk averse party to guarantee assuming the other party is not risk averse that party wins because gain r on average the risk averse individual wins b c more satisfied certainty equivalent more generally consider situation in which have uncertainty with respect to consequence c non linear preference function f note e x is the mean expected value operator the mean outcome of uncertain investment c is e c in example this was the mean satisfaction with the investment is e f c in example this was f f we call f e f c the certainty equivalent of c size of sure return that would give the same satisfaction as c in example was f f risk attitude redux the shapes of the preference functions means can classify risk attitude by comparing the certainty equivalent and expected value for risk loving individuals f e f c e c for risk neutral individuals f e f c e c for risk averse individuals f e f c e c motivations for a risk premium consider risk averse individual a for whom f e f c e c less risk averse party b a can lessen the effects of risk by paying a risk premium r of up to e c f e f c to b in return for a guarantee of e c income the risk premium shifts the risk to b the net investment gain for a is e c r but a is more satisfied because e c r f e f c b gets average monetary gain of r multiple attribute decisions frequently we care about multiple attributes cost time quality relationship with owner terminal nodes on decision trees can capture these factors but still need to make different attributes comparable wnv hybrid approach sd model decision tree average lifespan of a human interventionselected effects to be factored in season temperature dependence of mosquito biting preferences presumably through baby bird depletion late season mosquitoes heading into burrows to hibernate this being more driven by length of day mean time to waning immunity deaths of vaccinated humans mean time to waning immunity rate of loss immunity of vaccinal humans loss immunity of vaccinated humans vaccine rate vaccination loss of immunity of recovered and wnv immune patients humans per bite probability of transmission from infected mosquito to human recruitment rate of susceptible humans force of infection for humans entrants of humans average lifespan of a human deaths of susecptible humans asymptomatic infection fraction of exposed deaths from mean time to recover for asymtomatically infected patients recovery of asymptomatically infected patients number of bitings of susceptible humans by infected mosquitoes per day newly infected pre symptomatic human cases exposed humans humans that remain asymptomatic mean time until wnv incubates in humans number of human cases per day completing wnv incubation asymptomactically infected humans average lifespan of a human deaths of recovered and wnv immune patients average lifespan of a human mean time to recover for posthospital wnf patients number of human cases per day completing wnv incubation fraction of exposed hospitalized for deaths of exposed humans progression to n on hospitalized wnf fraction of exposed humans with nonhospitalized wnf deaths of wnf patients under recovery recovery of wnf patients humans with wnf that are hospitalized wnf average lifespan of a human average lifespan of a human discharge of wnf cases from hospital wnv induced death rate for humans deaths due to wnv deaths of hospitalized wnv patients average lifespan of a human mean time in hospital for wn f patients progression to non hospitalized wnf hospitalized for wnf new symptomatic cases negativeofcumulativewnvsymptomaticcases user interface the hybrid approach critical points is a framework geared toward an ongoing process of observation decision making captures uncertainties as time progresses simulates a broad range of possibilities e g for temperature and not just a single scenario allows for staging of decisions over different time points including decisions to wait see exploiting future options could be used for diverse planning challenges e g given uncertainty regarding public reaction vaccine availability responsibilities in the hybrid approach simulation model decision tree calculates dynamic consequences of a sequence over time of events choices represents over time possible sequences of uncertainties event nodes decisions decision nodes takes care of deterministic simulation consequences outcomes e g given events decisions cost quality of life etc takes care of encapsulating capturing all uncertainties policy space where policies are made over time example wnv hybrid approach simulation model decision tree mosquito lifecycle includes temperature effects bird lifecycle transmission between mosquitos bird human infection disease progression future costs resource use via resource intensity weights length of stay quality of life decision options over time source reduction larvaciding vaccination wait see uncertainties temperature consequences all wnv cases severe neurological cases costs etc larvacide reduction death rate of larval female mosquitoes source reduction death rate of larval female mosquitoes interventionselected number of adult blood meals per day for given temperature mosquitoes larvacide reduction death rate of immature female mosqutioes source reduction death rate of immature female mosquitoes natural death rate of larval female mosquitoes natural death rate of immature female mosquitoes death rate for larval female mosquitoes mean time to larval maturation for given death for larval female mosquitoes mean time as egg eggs hatching to mosquito larvae egg laying rate for adult female mosquitoes number of adult blood meals per day egg laid per blood meal currenttemperaturein temperature female eggs hatching to mosquito larvae egg laying by adult female mosquitoes density currenttemperatureincentigrade centigrade mean time to larval maturation maturation of larvae egg to mosquito larva ratio total adult female mosquitoes birth coefficient virus incubation virus incubation natural death rate of mean time to rate threshold pupal female mosquitoes source reduction death rate of pupal female mosquitoes death for pupal female mosquitoes death rate for pupal maturation of pupae pupal maturation disease transmission to susceptible mosquitoes through contact with infectious adult birds ab to m extrinsic incubation period temperature west nile virus sd larvacide reduction death rate of pupal female mosquitoes female mosquitoes interventionselected natural death rate for adult mosquitoes death of susceptible adult female mosquitoes death rate for adult mosquitoes adulticiding death rate pathogen transmission from infected bird to susceptible mosquitoes disease transmission to susceptible mosquitoes through contact with infectious juvenile birds jb to m interventionselected death of exposed female adult mosquitoes death rate for adult disease incubation death of infectious mosquitoes death rate for adult mosquitoes model mean time to waning immunity average lifespan of a human deaths of vaccinated humans mean time to waning immunity effects to be factored in season temperature dependence of mosquito biting preferences presumably through baby bird depletion late season mosquitoes heading into burrows to hibernate this being more driven by length of day rate of loss immunity of vaccinal humans loss immunity of vaccinated humans vaccine rate vaccination loss of immunity of recovered and wnv immune patients humans recruitment rate of susceptible humans entrants of humans average lifespan of a human mean time to recover for asymtomatically infected patients per bite probability of transmission from infected mosquito to human force of infection for humans deaths of susecptible humans asymptomatic infection fraction of exposed deaths from asymptomatically infected patients number of bitings of susceptible humans by infected mosquitoes per day newly infected pre symptomatic human cases humans that remain asymptomatic asymptomactically infected humans average lifespan of a human deaths of recovered and wnv immune patients average lifespan of a human mean time until wnv incubates in humans number of human cases per day completing wnv incubation non hospitalized wnf patients under recovery recovery of mean time to recover for posthospital wnf patients average lifespan of a human deaths of exposed humans hospitalized for wnf neurological symptoms fraction of exposed humans with neurological symptoms that are hospitalized fraction of exposed humans with wnf that are hospitalized progression to non hospitalized wnf fraction of exposed humans with nonhospitalized wnf deaths of wnf patients under recovery average lifespan of a human wnf patients hospitalized wnv patients discharge of wnf cases from hospital back wnv induced death rate for humans deaths due to wnv deaths of hospitalized wnv patients average lifespan of a human mean time in hospital for wnf patients decision tree dealing with data gradients backing out calibration nathaniel osgood cmpt term project updated due date because of holiday weekend date is now midnight april a key deliverable model scope boundary causal loop diagrams selection stock flow diagrams model time horizon policy structure specification of parameters reference mode reproduction matching of intermediate time parameter sensitivity analysis cross validation specification investigation of intervention scenarios investigation of learning environm ents mic roworlds quantitative causal robustness extreme casheypothetical external flight identification of key variables diagrams relations series matching of tests conditions simulator reference modes for explanation group model building decision rules initial conditions observed data poinutnit checking problem domain tests constrain to sensible bounds structural sensitivity analysis cross scenario comparisons e g cea some elements adapted from h taylor sources for parameter estimates surveillance data controlled trials outbreak data clinical reports data intervention outcomes studies calibration to historic data expert judgement metaanalyses introduction of parameter estimates annual likelihood of becoming diabetic annual likelihood of non diabetes mortality for asymptomatic population annual at risk births undx uncomplicated dying other causes annual not at risk births being born non obese being born at risk annual likelihood of becoming obese developing diabetes undx prediabetics recovering undx prediabetic popn zed mortality for obese becoming obese annual likelihood of undx prediabetic recovery dx prediabetics recovering diagnosis prediabetic annualized density of recon pulation non obese mortality annual mortality rate for non obese population obese mortality annual likelihood of dx prediabetic recovery dx prediabetic popn dx uncomplicated dying otehr causes annual likelihood of non diabetes mortality for sensitivity analyses same relative or absolute uncertainty in different parameters may have hugely different effect on outcomes or decisions help identify parameters that strongly affect key model results choice between policies we place more emphasis in parameter estimation into parameters exhibiting high sensitivity dealing with data gradients often we don t have reliable information on some parameters but do have other data some parameters may not be observable but some closely related observable data is available sometimes the data doesn t have the detailed breakdown needed to specifically address one parameter available data could specify sum of a bunch of flows or stocks available data could specify some function of several quantities in the model e g prevalence some parameters may implicitly capture a large set of factors not explicitly represented in model there are two big ways of dealing with this manually backing out and automated calibration backing out sometimes we can manually take several aggregate pieces of data and use them to collectively figure out what more detailed data might be frequently this process involves imposing some sometimes quite strong assumptions combining data from different epidemiological contexts national data used for provincial study equilibrium assumptions e g assumes stock is in equilibrium cf deriving prevalence from incidence independence of factors e g two different risk factors convey independent risks example suppose we seek to find out the sex specific prevalence of diabetes in some population suppose we know from published sources the breakdown of the population by sex cm cf the population wide prevalence of diabetes pt the prevalence rate ratio of diabetes in women when compared to men rrf we can back out the sex specific prevalence from these aggregate data pf pm here we can do this backing out without imposing assumptions backing out male diabetics female diabetics diabetics pm cm pf cf pt cm cf further we know that pf pm rrf pf pm rrf thus pm cm pm rrf cf pt cm cf pm cm rrf cf pt cm cf thus pm pt cm cf cm rrf cf pf pm rrf rrf pt cm cf cm rrf cf disadvantages of backing out backing out often involves questionable assumptions independence equilibrium etc sometimes a model is complex with several related known pieces even thought we may know a lot of pieces of information it would be extremely complex or involve too many assumptions to try to back out several pieces simultaneously another example joint marginal prevalence perhaps we know the count of people in each sex geographic category the marginal prevalences pr pu pm pf we need at least one more constraint one possibility assume pmr pmu pr pu we can then derive the prevalences in each sex geographic category calibration triangulating from diverse data sources calibration involves tuning values of less well known parameters to best match observed data often try to match against many time series or pieces of data at once idea is trying to get the software to answer the question what must these less known parameters be in order to explain all these different sources of data i see observed data can correspond to complex combination of model variables and exhibit emergence frequently we learn from this that our model structure just can t produce the patterns calibration calibration helps us find a reasonable specifics for dynamic hypothesis that explains the observed data not necessarily the truth but probably a reasonably good guess at the least a consistent guess calibration helps us leverage the large amounts of diffuse information we may have at our disposal but which cannot be used to directly parameterize the model calibration helps us falsify models calibration a bit of the how calibration uses a global optimization algorithm to try to adjust unknown parameters so that it automatically matches an arbitrarily large set of data the data often in the form of time series forms constraints on the calibration the optimization algorithm will run the model many minimally thousands typically or more times to find the best match for all of the data required information for calibration specification of what to match and how much to care about each attempted match involves an error function penalty function energy function that specifies how far off we are for a given run how good the fit is alternative specify payoff function objective function a statement of what parameters to vary and over what range to vary them the parameter space characteristics of desired tuning algorithm single starting point of search envisioning parameter space for each point in this space there will be a certain goodness of fit of the model to the collective data τ μ assessing model goodness of fit to improve the goodness of fit of the model to observed data we need to provide some way of quantifying it within the model we for each historic data calculate discrepancy of model figure out absolute value of discrepancy from comparing historic data the model calculations convert the above to a fractional value dividing by historic data sum up these discrepancy characteristics of a desirable discrepancy metric dimensionless we wish to be able to add discrepancies together regardless of the domain of origin of the data weighted reflecting different pedigrees of data we d like to be able to weigh some matches more highly than others analytic we should be able to differentiate the function one or more times concave two small discrepancies of size a should be considered more desirable than having one big discrepancy of size for one and no discrepancy at all for the other symmetric being off by a factor of two should have the same weight regardless of whether we are or non negative no discrepancy should cancel out others finite finite inputs should yield infinite discrepancies a good discrepancy function assuming non negative h m exponent concave with respect to h m taking average in denominator together w squaring of result ensures symmetry with respect to h m h m h m w average h m w h m division dimensionless judging by proportional error not absolute only zero if h m denominator is only very small if numerator is as well considerations for weighting purpose of model if we care more about a match with respect to some variables we can more heavily weight matches for those variables uncertainty in estimate the more uncertain the estimate of the quantity the lower the weight whether data exists no data weight should be zero dealing with data gradients calibration nathaniel osgood cmpt march a key deliverable model scope boundary causal loop diagrams selection stock flow diagrams model time horizon policy structure specification of parameters reference mode reproduction matching of intermediate time parameter sensitivity analysis cross validation specification investigation of intervention scenarios investigation of learning environm ents mic roworlds quantitative causal robustness extreme casheypothetical external flight identification of key variables diagrams relations series matching of tests conditions simulator reference modes for explanation group model building decision rules initial conditions observed data poinutnit checking problem domain tests constrain to sensible bounds structural sensitivity analysis cross scenario comparisons e g cea some elements adapted from h taylor recall dealing with data gradients often we don t have reliable information on some parameters but do have other data some parameters may not be observable but some closely related observable data is available sometimes the data doesn t have the detailed breakdown needed to specifically address one parameter available data could specify sum of a bunch of flows or stocks available data could specify some function of several quantities in the model e g prevalence some parameters may implicitly capture a large set of factors not explicitly represented in model there are two big ways of dealing with this manually backing out and automated calibration recall calibration triangulating from diverse data sources calibration involves tuning values of less well known parameters to best match observed data often try to match against many time series or pieces of data at once idea is trying to get the software to answer the question what must these less known parameters be in order to explain all these different sources of data i see observed data can correspond to complex combination of model variables and exhibit emergence frequently we learn from this that our model structure just can t produce the patterns recall calibration a bit of the how calibration uses a global optimization algorithm to try to adjust unknown parameters so that it automatically matches an arbitrarily large set of data the data often in the form of time series forms constraints on the calibration the optimization algorithm will run the model many minimally thousands typically or more times to find the best match for all of the data recall required information for calibration specification of what to match and how much to care about each attempted match involves an error function penalty function energy function that specifies how far off we are for a given run how good the fit is alternative specify payoff function objective function a statement of what parameters to vary and over what range to vary them the parameter space characteristics of desired tuning algorithm single starting point of search recall example global optimization algorithm starts at random position tries to improve match minimize error by adjusting parameters running model recording error function keeps on improving until reaches local minimum in error of fit may add some randomness to knock out of local minima running calibrations in vensim under model simulate commands optimization control payoff definition the pieces of the elephant example model of underlying process time series it must match single model matches many data sources one of example iteration calibration from sterman time day from sterman expanding the boundary behavioral feedbacks infection rate b average incubation time emergence rate average duration of illness removal rate depletion r contagion r total infectious contagion infectivity contacts contact rates safer b social distancing social distancing b hygiene media attention public health warnings delay practices from sterman cumulative cases pieces of the elephant sti science population rlsk death r population development p opul ati de a th hs of for vi on at risk hands on model use ahead load sample model sir agent based calibration via sample models under help menu an optimization experiment in anylogic stops after best objective ceases to significantly improve caveat modelor may prematurely terminate the optimization stops after optimization iterations varying these parameters defining a payoff function caveat non analytic non concave computing discrepancy between historic model values at this point during the run historic data captured via table function how to interpolate fill in between data points stochastics in agent based models recall that abms typically exhibit significant stochastics event timing within outside of agents inter agent interactions when calibrating an abm we wish to avoid attributing a good match to a particular set of parameter values simply due to chance to reliably assess fit of a given set of parameters we need to repeatedly run model realizations we can take the mean fit of these realizations distinction replication run one realization particular random number seed iteration evaluation of a particular parameter set this can contain many realizations replications confusingly the term simulation appears to sometimes be used for either of the above populating the appropriate datasets populates historic data up front from table fn these datasets are within the experiment persist beyond the simulation saves away best simulation within in iteration retaining the current value after the realization simulation run running calibration in anylogic best payoff objective yet reached lower is better values of parameters being calibrated at best calibration thus far optimization constraints tests on legitimacy of parameter values optimization requirements tests on emergent results to sense validity enabling multiple realizations replications runs per iteration fixed number of replications per iteration specifies stopping condition once minimum replications have been run indicates that the x confidence interval around the mean is within error percent of the iteration mean obtained as of the most recent replication example x e g confidence interval for sample mean average of e replications e to this poin t x e payoffr payoffr r payoffr r e x e r x e x minimum and m aximum observed values from replications after replications after replications after replications automatic throttling of replications based on empirical fractiles for the average of the differences between best and current enabling random variation between realizations replications understanding replications report results for each replication during first several realizations replications runs no results appear report on iteration appears after a count of runs equal to replications per iteration reports best payoff objective yet reached lower is better but from where did this number come output the reported payoff for the iteration is the average of the payoffs for each replication within the replication average of results for replications is the reported score for the iteration debugging in anylogic nathaniel osgood cmpt avoiding debugging defensive programming offensive programming offensive programming try to get broken program to fail early hard asserts actually quit the program fill memory allocated with illegal values fill object w illegal data just before deletion set buffers at end of heap so that overwrites likely trigger page fault setting default values to be illegal in enums we will talk about assertions error handling later this week assertion goal fail early alert programmer to misplaced assumptions as early as possible benefits documents assumptions reduces likelihood that error will slip through helps discourage lazy handling of only common case forces developer to deal explicitly with bug before continuing reduces debugging time helps improve thoroughness of tests avoid side effects in assertions because assertions may be completely removed from the program it is unsafe to rely on side effects occuring in them arnold et al the java programming language fourth edition enabling assertions in anylogic arnold et al the java programming language fourth edition enabling assertions in java ways usual via java runtime command line e g less common via reflection classloader debugging anylogic debugging is the process of locating the faults behind observed failures anylogic education now contains a debugger you can attach to anylogic from debuggers such as eclipse the key thing is to set anylogic to use a port debugging options using output for manual tracing reporting using aspectj tracing using an external debugger e g via eclipse using anylogic professional research debugger using output for manual tracing reporting pros minimal learning curve flexible easily targeted cons requires time consuming manual markup de markup can require many build simulation iterations to localize problem limited capacity of console output to the console how to system err println string system err println sent cure message to person associatedperson traceln string system out println string use in anylogic aspectj and eclipse aspectj is a language that allows for succinctly describing cross cutting functionality in programs such as tracing or logging requests aspectj can automatically insert tracing instrumentation into our code this gives us many of the benefits of manual tracing program execution without the need for the markup mark down work anylogic built in debugger running the debugger running the models setting a breakpoint when we hit the breakpoint components to direct execution visible in scope variables exploring composite variable values in the debugger inspecting composite variables changing variable values during debugging stepping into auto generated code eile yiew m od el indo w l e lp x get su pport e i debug de bu g n l el brea kpo int ll i i x el c l va ria bles ll h express i o nsi el pa l ette n i el dlil i j networktypetost rin g fun cti o n bo dy re lat ive li n e e th is etworks m a in id i gene ra l aa n l ii thr ea d awt wind ow n a pe int fi thr ea d awt shutdo wl stepove r ind ex int pa r a meter ii thr ea d awt event queu e event ii thr ea d anylog i c simu lat io r g dynamic event ii thr ea d dest royj avavm ru g plain ll lil thr ea d anylog i c presentat i oj m a in f main java u j el coll e ct io n present a ble main a int i c a e t e x t return i nf e c t i v i t y rr g fu nct io n pr esent a ble main dr aws ca e t e x t r e t u r n i fu n cl io n present a ble main a int i f o rma t i av er a g e l l l n ess dru at i o n da y pr esent a ble main dr aws po rt conn ector pr esent a ble main dr awp c a e t e x t l r e t u r n av er age i l l n ess du r a t i on pa nelsd run q lin e c a e t e x t return envi o nmen t n e t wor kt yp e tos tr i n g l e nv ir o llille n t g e t ne t wor kt yp e ca e t e x t return network t yp e syst em yn am ics i m i ca e t e x t return t p rojects n l e n v ir onme n t ge t co nn e c t i o ns per ag e n t i stat ec hart el agent ba sed networks ca e t e x t return links p er a g e n t n d aclio nc hart main c a e t e x t return max i mum link dis t an c e i anal is pa ra meters ca e t e x t r e t n r n f b fun cti o ns e nv ir onme n t ge t co ru e c t i o n ran g e i p resen tatio n g o rkl y pe l a f c en vironm ents c a e t e x t r e t n r n perc ent o f long distance l i nks b p i emb e dd ed objects gontrols lhh an a l ysis dat a ii pr op erties ll i con sole v el go nn ec t ivily i pr esent at io n pe rson g function p ict u res fl s imu lat io n main do bjects mon te ca rlo main g ene ral na m e n etworktypetostr in g show n a m e j lgn o re pub li c sh ow at runt im e code b ff ent e rprise library desc ripti o n acc ess i defa u lt tl iq stat i c pe d estrian library i ret urn type void boole a n int do u ble st ring oth ec i s tr i n g i i i n i n i in t r t qi rail libra ry i pa lette i t i c c n seeing result of expression evaluation note that this doesn t update immediate may need to switch stack frames in the debug method to see the update external debugging in eclipse the eclipse editor is one of the most popular extant software development tools eclipse offers plug ins of many sorts debuggers profilers visualization tools version control of models eclipse can be used to debug anylogic models at the java source code level steps required for eclipse debugging one time set up for a particular model set up anylogic to allow debugging connections set up eclipse to know how to connect to anylogic where to look for source code files every time want to debug go to eclipse tell debugger to connect to anylogic process interrupt process set breakpoints etc setup in anylogic xdebug xnoagent djava compiler none xrunjdwp transport server y suspe nd n address these go under the advanced tab of the simulation run to use set up creating a debugging configuration in eclipse setting up source code folders add source folder once set up can set breakpoints see the variables with symbolic information suggestions set a breakpoint on a thrown runtime exception regardless of whether caught throw a caught runtime exception from model startup code when catch this in eclipse can then use to set breakpoints including in other files example setup set up function to trigger the debugger in startup code for model call function in eclipse open debug perspective request creation of exception breakpoint request as breakpoint regardless of handling should now be in list of enabled breakpoints start anylogic model experiment with extra debugging jvm arguments leave on opening screen for now so we can set up eclipse go to eclipse request anylogic debugging debug configuration previously set up should immediately see something like this return to anylogic start simulation via button push back in eclipse the debugger should have been triggered at exception handle if not close main java and double click on topmost stack frame where exception is triggered now can set breakpoints in main java or elsewhere here person java warning breakpoints are not shown in source window just in breakpoints area press resume to continue awaiting a breakpoint exam le break in main w debug c users nate a nyl og ic worlcsp ace eclipsedeb ugg ing exam ple src generate d abm mode lw ithb irthdeath ma in java eclipse gj l i eil e f dit ource refaqor avig ate se eroject bun windo w l elp i i l t debug j debug c o lj i t i i el g out li ne fi lil if el th re ad awt sh utdo wn ru nn ing fa daemon thre ad awt windo ws runn ing lat io n doub le et hn ic ity sex b c reate q v oid rfj daemon th read anyl og ic mode l execut io n t h rea d suspended brea kpo int at li ne in ma in c re ate lat io pe rson int vo id i ma in lat io n doub le personsethn icity personssex boo le a n li ne drawmode lele ments pane l grap main execu t eact io nof ev e nt rate li ne evaluaterateof ev e nt rate double ev e nt rate execute q line not avail ab le evaluate timeou tof ev e ntt imeout dou eng ine hq li ne not availab le executeac t io nof ev entrate vo id eng ine a eng ine line not avail ab le execut eact io nof ev en tt imeout vo id eng inesa run q li ne not availab le t gete m beddedobjects q list o bject x va riab les e breakpo ints o express io ns x sfc f i ll g i j i el getf irstoc cu rre ncet ime ev e ntt imeout getmode of ev e nttimeout int ji main li ne ma in get na me of act iv e object string ji ma in li ne ma in get na meof act iv e objectco llect io n ji ma in li ne ma in get na meof ev e nt rate string ji ma in li ne ma in get na meof ev e ntt imeout string el ma in li ne ma in mainclass li ne ma in st ring l get na meofshape int string get pe rsiste ntshape int object ji pe rson li ne pe rson getshape em beddedobject int object ji person li ne person t getshape re plic at io n int int lj ma inc lass ja va lj defa u ltt rac ingf ilte r ma inclass java defa u ltt rac ingf il te r gil person java gil ma in java el gets hapet y pe int int getshap ex int int double return newly created embedded object getshape y int int double instant iate lat io l int person public person double initialage person ethnicity ethnicity person sex sex boolean islnit o nchange q vo id i person object instantiate ind ex o nchange ialprev ale nce oflnfe ct io nq object initialage initialage ffsp ringd ist ance fro m o b j ect ethnicity ethnicity o lat io nsize q void o revale nceo flnfect io namon object islnitiallylnfected islnitiallylnfected ii finish embedded object creation onc lic kmode lat pane l doub le doub le li create object in dex onde st ro yq v o id object start re lat io n per son boolean migrantspe ryea r doub le v o id t nceo flnfe ct io n doub le t l j l d d bu il d project example breakpoint in person once at breakpoint can look at variables single step etc variables displayed terminating execution from anylogic console eclipse is now detached debug c us e rs nate anyl ogicwo rkspace eclipsede bugg ing exam ple rc gene rate d abmmode lwithb irthdeath pers o n java eclipse eil e d it iou rce refa ctor j javigate sej rch e ro ject bu n in d ow elp i i debug de bug ll c o w i i jf i v el ou t lin e ll q v el t e rm in at ed anylog i c app li c at io n re mot e java appli cat i o n gl n ew sh a pe grou po a d isconn e cted java ho t sp ot tmj cli ent vm lo ca lho st cu rrentag e o double q d rawmo del elem ent pa ne l g ra ph i c i ent at e sh ort boole a n void e st a biish offspring conn e cti o ns based oni e sta blis h offspring loe at i o n based on mot l e eva luat e rat eof trans it io nrat e double eva luat etimeoutof tram it i o nt im eo ut q execut eacti o nof st at ec h a rt void e execut eacti o nof tra nsit i o nm essag e ob x va riables u brea kpoint expre ssi o t v el e execut eacti o n of tra n sit ion rate void q execut eacti o nof tra ns it i o ntimeo ut vc q exit stat e sh o rt tra nsit io n bo ol e a n s t at fe rtility rat eagesexet hn ic ity d ou ble sex finalizedeat h o void q g ain o main q g et n a m eof st at ec h a rt string q get n a m eof tra nsit i o nm essag e string e g et n a m eof tra n sit i o nr at e string g et n a m at e sh o rt string void p er f o rmbir t h i i e g et at ech a rtof tra nsit io nm ess a g e st q g et ech a f tra ns it io nrate statecr per on mn t h er t h is q g et ech a f tra nsitio ntim eout sta per on o f fsp r i n g g e t i n i a d d p op u l a t i o n d oub l e o e t hn i c i t y ran do ms e x th is is i n f e c t e d l slnf e ct e dq boole n tr a c e l n a baby has b e e n bo rn baby i d is o f fs p r i n g wh i l e t h e mo t h er is th is isln re pr odu ctiveyea rs doub l e boolean i es t ab l is h connections o f i n f an t q es t ab l is hof fs p r i n g co ru e c t i ons bas e donmo t h er co n n e ct i o ns o f fs p r i n g mot h er d on ch a ng eo void o n ch a ng i c ityo void now position the baby t o be close t o che mo ther och erwise leads t o strecching o f moth er connections aci es t ab l is hof fs pr i n g i o c a t i onbas e donmo th er i o c a t i on o f fs p r i n g mo t h er i on ch a ng iti a lag eo void o nch a ng l n iti allylnf e ct e dq void onch a ng void i on clickm odelat pa n e ll do u ble doub le i j q ondestroyo void void es t ab l is hof fsp r i ngc o nn e c t i o ns as e donmo t h er co nn e c t i o ns p er on o f fs p r i n g per on mot her j e onre c e ive object agent void i t pe rfor m b irtho void t l m i l m i o b u il d proj e ct remembering breakpoints note eclipse does remember breakpoints from session to session so breakpoints that set earlier in an anylogic session will work again even after close eclipse and restart it again suggestions consider creating a common breakpoints e g at main start disable and enable breakpoints rather than deleting them example of debugging session agent based and aggregate modeling tradeoffs limitations nathaniel osgood april inherent e g distinctions qualitative vs quantitative static vs dynamic stochastic vs deterministic capacity to understand single scenario vs range of scenarios magnitude of computational resources required interactive or not under vs over determined calibration ability to calibrate to make behaviour depend on individual history important software skills mediation required level of software development sophistication dynamic models for health classic aggregate models differential equations population classified into or more state variables according to attributes state variables parameters population recent individual based models governing equations approach varies each individual evolves state variables parameters population contrasting model granularity birth and death rate time random seed mortality rate birth rate births contacts per susceptible per contact risk of infection force of infection incidence fractional prevalence mean time with disease recovery population initial population initial fraction vaccination susceptible mortality mortality rate infective mortality recovered mortality mortality rate mortality rate vaccinated q annual likelihood of vaccination susceptible fraction vaccinated mortality time of population mortality rate granularity selection problem specific selection of granularity is a function of question that are asking not of the true nature of the system quanta of most obvious system components may not align with needs for insight may gain benefits from higher level representation many high level behavior of complex systems can be explained with very simple models often gain greater insight from simpler model cf gas laws vs lattice gas model may wish to seek lower level model small infection spread model characterization at level of immune response rather than monolithic person myth of individual based models as modeling from the bottom up a single person is a natural locus of description presents for care lives dies coupled internal systems but the world has no natural bottom it is frequently desirable to include within a person a great deal of within the skin detail the issues of model depth breath are just as pressing in individual based models as in aggregate modeling contrasting benefits aggregate models individual based models frequently easier construction calibration parameterization formal analysis control theoretic eigenspace techniques understanding performance lower baseline cost population size invariance less pronounced stochastics less frequent need for monte carlo ensembles better fidelity to many dynamics stronger support for highly targeted policy planning ability to calibrate to validate off of longitudinal data greater heterogeneity flexibility better for examining finer grained consequences e g transfer effects w i pop quicker construction runtime network spread more time for understanding refinement simpler description of some causal mechanisms key needs motivating individual based modeling need to calibrate against information on agent history need to capture progression of agents along multiple pathways e g co morbidities wish to characterize learning by and or memory of agents based on experience or strong history dependence in agents need to capture distinct localized perception among agents seeking to intervene at points in change behavior on explain phenomena over or explain dynamics across networks seek distinct interventions for many heterogenous categories need to capture impact of intervention across many categories when it is much simpler to describe behavior at indiv level seek flexibility in exploring different heterogeneity dimensions needs of stakeholders to engage with individual based models want to describe behaviour at multiple scales key needs motivating aggregate based modeling need to execute quickly e g for user interaction understand describe system behaviour across all possible values for parameters seeking to mathematically analyze the model e g to determine location or stability of equilibria to determine shape of all possible trajectories want to use mathematical tools such as control theory to identify high leverage parameters optimal policies need to extensively calibrate to much historic data desire of stakeholders to work at higher level behavior for different subgroups differs only in degree no recourse to software engineering knowledge lack of detailed knowledge of network structure individual level behaviour individual level data representing interventions cmpt nathaniel osgood representing interventions in anylogic vensim interventions disturb the baseline operation of the system interventions can be represented by several types of changes namely modifications to parameter initial state values model structure incentives represented in model system state at one or more particular points in time running interventions typical run baseline and alternatives each in series compare results as if sensitivity analysis radical but effective e g for cost effectiveness arguments vensim subscripting vensim model by intervention baseline intervention a intervention b and having run in parallel anylogic run several populations in parallel each associated with a different intervention model granularity can limit options in representing interventions model specificity provides limits our ability to investigate targeted interventions model granularity may force us to represent more detail with respect to an intervention model granularity intervention specificity all other things being equal the more detailed the model the greater detail with which we can and sometimes must specify interventions examples a model stratified by age sex permits vaccinations to be rolled out at different times according to these factors a model incorporating network structure allows us to target our interventions at network hubs a model in which contacts emerge from agents moving between locations to would allow us to examine how changing those locations would affect contact patterns capturing history supports history specific interventions fine grained models oblige specifying added intervention details more detail in a model generally requires making more specific statements about intervention effects contrast changes to mixing assumptions unstratified aggregate model changing c stratified aggregate model changing mixing matrix abstracting over exactly how this is accomplished individual based model with network change certain areas of network e g add delete modify connections individual based model where contacts emerge from move change something about specific factors driving mobility patterns common phrasing of interventions what would be impact of reducing uptake rate by increasing cessation rate by lowering mortality rate by reducing mixing levels by increasing emergency room staff by reducing the rate of progression of diabetes by changing parameter values frequently we can approximate an intervention impact by changing behaviors already represented in the model this is abstracting over the issue of the exact nature of how this is caused this might affect parameters or initial values often several parameters may need to be changed together e g higher smoking cessation rate lower smoking relapse rate lower value of c lower value of β be sure to restore parameters to their baseline values after experiments changing parameters in vensim in synthesim no worries that saving away the model will disturb baseline functioning easy setting of values left click on variable to set exact value via gaming variables can adjust over time via changes files to remember exact changes across multiple parameters these can let you systematically save away parameter sets each associated with a particular intervention scenario by modifying value of parameter within the model specification to constant formula or time series remember to restore indicate change with color eg red synthesim setting the value of a parameter in synthesim loading changes file altering variable value in model setting the color of parameter to remind us that it is changed following color change restore color once restore value gaming variables gaming gaming interface advancing time w move forward button click to change a gaming variable interface for a non subscripted variable click to change a gaming variable interface for a subscripted variable accomplishing live changes in anylogic via user interface elements experiment user interface normally just provides parameter values for starting up model modifying an anylogic model operation during simulation itself can most easily be accomplished via a ui based in the main object hands on model use ahead load sample model predatory prey agent based via sample models under help menu main interface with sliders slider logic modifies parameter logic for initial values passing on modified parameter values to the simulation another option note slider names changed for clarity setting initial values run time parameter modifications changing parameters in anylogic changing value of parameter explicitly in model avoid if possible could forget to restore create a new experiment set the parameter value as a parameter for main here easiest if the operational parameter in main if parameter is not located in main main should pass on parameter value to e g the agent class via an interface in the main class or agent class itself structural modifications sometimes capturing the effects of an intervention requires representing a different processes than are present in the baseline model e g vaccination quarantine intervention group educated given a treatment genetically immune mosquitoes capturing structural modifications in vensim adding stocks e g vaccinated people quarantined people flows e g to vaccinated stock or quarantined stock subscripts e g intervention group may run in parallel with other group subject to the same forces modifying existing flows e g disabling smoking relapse when intervention is enabled capturing structural modifications in anylogic statechart based adding states e g vaccinated quarantined transitions e g to vaccinated state or quarantined state or to a new cured state system dynamics flows modifying an existing transition so that it is contingent on an intervention being disabled for targeted intervention may wish to capture people as having been affected by the intervention representing intervention mechanisms two choices some interventions are representing in a stylized fashion that abstracts away from dynamics of intervention implementation here we just examine proximal distal effects of certain modifications to baseline model assumptions ignoring the issue of how these modifications would be achieved some intervention representations include characterizing both the intervention effects its dynamics e g dynamics of training teachers to deliver anti smoking lessons in the classroom dynamics of vaccine production endogenous intervention impacts on behaviour current practice behaviour is exogenous to many models models link behavior to distal impacts modelers impose assumptions of how interventions affect behaviour models offer value in understanding emergent distal implications of behaviour change we gain little insight into the counter intuitive behavioral impacts of intervention example behavioral feedbacks underlying much policy resistance cutting cigarette tar levels reduces cessation cutting cigarette nicotine levels leads to compensatory smoking arvs prolong lives of hiv carriers but lower risk perception availability of reduced fat calorie varieties undercuts changes to eating habits antilock brakes lead to more risky driving endogenous intervention impacts on behaviour vision modelers characterize intervention impacts on environment e g prices tax burden incentives laws capture indiv preferences mental models learning model endogenously compute individual localized behavioural responses cf discrete choice theory psych models models provide insight into both distal implications of interventions behavioral impacts of intervention individual collective additional factors accumulating costs for interventions accumulating costs for other factors so can see what intervention eliminates stocks flows first order delays aging chains nathaniel osgood cmpt february example model stocks example model flows key component stock flow flow stock flow structure behavior stocks determine flows flows determine change in stocks net flow impact on stock flow flow current time month stock current time month impact of lowering flow rate to month flow stock time month flow stock and flow alternative flow current time month stock stock and flow alternative stock current example model auxiliary variables constants time series parameters for similar reasons to auxiliary variables we give names to model constants time series example model parameters example system structure diagram note treatment of flows as links from flow to stock inflows as positive links outflows as negative links simple first order decay use initial value pevoiprulelewntith meadnetiamthe until infection dienaftehcstiforonm use formula deaths from infection mean time until death set model settings model menu settings item dynamics of stock people with virulent infection people withvirulent infection current time month dynamics of rate of death flow deaths from infection deaths frominfection current time month stocks as accumulations we often use stocks to accumulate integrate other evolving quantities over time example assume time measured in years another example of stocks as accumulations slightly more sophisticated qncalit y of ife for nbpopl dafiloo qntal it y ellg ited qntal i y of ife for smbpopl a non f qncalit y ellghted tron swe of f orm quality ife for opula tion s e of subpopula tion e for opula tion size subp opulation of snbpopl non snbp op l tcon quality eiighted o rmu e t j enme p op l ti on f q i u subp opula tion quality eightoos u b po pual toi a v ife yea r w e d principle structure determines behaviour feedback stock and flow structure of a system determines the possible patterns of behaviour different sets of parameters e g values for constants will select particular behaviour within these behaviour patterns changes to the feedback structure can change behaviour in fundamental ways simple sit model mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible total population incidence rate prevalence recovery delay initial population new infections new recovery newly susceptible immunity loss delay classic feedbacks susceptibles contacts of susceptibles with infectives new infections infectives dynamics person person person person person person person person person person person person person person person state variables over time time months s alternative hc workers exogenous recovery delay person i alternative hc workers exogenous recovery delay person r alternative hc workers exogenous recovery delay person broadening the model boundaries endogenous recovery delay mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible total population staff time per patient healthcare workers initial population incidence rate fractional prevalence recovery delay time until seek treatment new infections new recovery newly susceptible immunity loss delay broadening the model boundaries endogenous recovery delay susceptibles contacts of susceptibles with infectives n ew infections infectives people presenting for treatment waiting times health care staff a different behaviour mode prevalence infectious person person person time day prevalence baseline hc workers i baseline hc workers person structure as shaping behaviour system structure is defined by stocks flows connections between them nonlinearity the behaviour of the whole is more than the sum of the behaviour of the parts emergent behaviour would not be anticipated from simple behaviour of each piece in turn stock and flow structure including feedbacks of a system determines the qualitative behaviour modes that the system can take on first order delays in action simple sit model mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible incidence rate total population prevalence recovery delay initial population t new infections new recovery new illness department of computer science cumulative illnesses newly susceptible immunity loss delay first order delays in action simple sit model scenarios for first order delay variation in inflow rates for different immigration inflows what do you expect inflow inflow inflow inflow why do you see this goal seeking pattern what is the goal being sought behaviour of stock for different inflows why do we see this behaviour goal seeking behaviour the goal seeking behaviour is associated with a negative feedback loop the larger the population in the stock the more people die per year if we have more people coming in than are going out per year the stock and hence outflow rises until the point where inflow outflows if we have fewer people coming in than are going out per year the stock declines outflow declines until the point where inflow outflows as a causal loop diagram what does this tell us about how the system would respond to a sudden change in immigration response to a change feed in an immigration step function that rises suddenly from to at time set the initial value of stock to how does the stock change over time create a custom graph display it as an input output object editing create input output object for synthesim stock starting empty finlfoloww anrdaotuetflsow idmeamthigsr atsiotenp how would this change with alpha stock starting empty value of stock alpha how would this change with alpha for different values of alpha flow rates outflow rises until inflow this is for the flows what do stocks do for different values of alpha value of stocks why do we see this behaviour a longer time delay or smaller chance of leaving per unit time requires x to be larger to make outflow inflow outflows as delaved version of inputs in flo v and outflo v inflovv and outflovv iq rn time y ecn lq q q lq sq lqq time year illlfilligrn lion step functimu yr d elay dea ths step yr delay mmiigrnrtr ion step yr clejla y eaths step yr d ejla y inflow and outflow rn inflow and outflow q time y eru rn time y ear nrumgratfon step funoliorn q yr delay deaths step functions yt d elay higher order delays aging chains moving beyond the memoryless assumption recall that first order delays assume that the per time unit risk of transitions to the outflow remains equal throughout simulation i e are memoryless problem often we know that transitions are not memoryless e g it may be the transition reflects some physical delays not endogeneously represented e g slow growth of bacterial buildup of damage of high blood sugars glycosylation higher orders of delays we can capture different levels of delay with increasing levels of fidelity using cascaded series of order delays we call the delay resulting from such a series of k order delays a kth order delay e g first order delays in series yield a order delay the behaviour of a kth order delay is a reflection of the behaviour of the order delays out of which it is built to understand the behaviour of kth order delays we will keep constant the mean time taken to transition across the entire set of all delays recall simple order decay meadnetiamthe until pevoiprulelewntith infection dienaftehcstiforonm initial value use formula people with virulent infection mean time until death recall order delay behaviour conditional transition prob for a order delay the per time unit likelihood of leaving given that one has not yet left the stock remains constant unconditional transition prob for a order delay the unconditional per time unit likelihood of leaving declines exponentially i e if were were originally in the stock our chance of having left in the course of a given time unit e g month declines exponentially this reflects the fact that there are fewer people who could still leave during this time unit recall order delay behaviour per month chance of transitioning out during this month likelihood of still being in system order delay use formula mean time to transition across all stages stage count use value of use value of initial value initial value order delay per month chance of transitioning out during this month likelihood of still being in system order delay order delay per month chance of transitioning out during this month likelihood of still being in system through orderdelays per month chance of transitioning out during this month likelihood of still being in system mean times to depart final stage mean time of k stages is just k times mean time of one stage e g if the mean time for leaving stage requires time mean time for k k in our examples as we added stages we reduced the mean time per stage so as to keep the total constant i e if we have k stages the mean time to leave each stage is k times what it would be with just stage infinite order delay as we add more and more stages k the distribution of time to leave the last stage approaches a normal distribution if we reduce the mean time per stage so as to keep the total time constant this will approach an impulse function this indicates an exactly fixed time to transition through all stages distribution of time to depart final stage the distributions for the total time taken to transition out of the last of k stages are members of the erlangdistribution family these are the same as the distribution for the kth interarrival time of a poisson process k gives exponential distribution first order delay as k approaches normal distribution gaussian pdf from wikipedia aging chains including successive order delays competing risks in our model of chronic kidney disease introduction to stocks flows nathaniel osgood osgood cs usask ca cmpt state of the system stocks levels state variables compartments stocks levels represent accumulations these capture the state of the system mathematically we will call these state variables these can be measured at one instant in time stocks start with some initial value are thereafter changed only by flows into out of them there are no inputs that immediately change stocks stocks are the source of delay in a system in a stock flow diagram shown as rectangles examples of stocks water in a tub or reservoir people of different types susceptible infective immune people pregnant women women between the age of x and y high risk individuals healthcare workers medicine in stocks money in bank account in atmosphere blood sugar stored energy degree of belief in x stockpiled vaccines goods in a warehouse beds in an emergency room owned vehicles example model stocks the critical role of stocks in dynamics stocks determine current state of system stocks often provide the basis for making choices stocks central to most disequilibria phenomena buildup decay lead to inertia give rise to delays state changes flows fluxes rates derivatives all changes to stocks occur via flows always expressed per some unit time if these flow into out of a stock that keeps track of things of type x e g persons the rates are measured in x time unit e g persons year month gallons second typically measure over certain period of time by considering accumulated quantity over a period of time e g incidence rates is calculated by accumulating people over a year revenue is time water flow is litres minute can be estimated for any point in time examples of flows inflow or outflow of a bathtub litres minute rate of incident cases e g people month rate of recovery rate of mortality e g people year rate of births e g babies year rate of treatment people day rate of caloric consumption kcal day rate of pregnancies pregnancies month reactivation rate of tb cases reactivating per unit time revenue month spending rate month power watts rate of energy expenditure vehicle sales vaccine sales shipping rate of goods example model flows key component stock flow flow stock flow net flow impact on stock flow flow current time month stock current time month impact of lowering flow rate to month flow stock time month flow stock and flow alternative flow current time month stock stock and flow alternative stock current loops stocks causation does not effect big change instantaneously loops are not instantaneous stocks only change by changes to the flows into out of them there are no inputs that immediately change stocks all causal loops must involve at least one stock the state of the world must change as part of the process absent a stock loop would be instantaneous slides adapted from external source redacted from public pdf for copyright reasons auxiliary variables auxiliary variables are convenience names we give to concepts that can be defined in terms of expressions involving stocks flows at current time adding or eliminating an auxiliary variable does not change the mathematical structure of the system critical for model transparency can be reused at many places references to auxiliary variables prevents need for modeler to think about all of details of definition enhanced modifiability single place to define convenient for reporting graphing tables analyzing model dynamics example model auxiliary variables constants time series parameters for similar reasons to auxiliary variables we give names to model constants time series example model parameters stocks flows compared with markov models open population births deaths non constant likelihood density of transitions likelihood of leaving a stock per unit time can depend on other stocks force of infection likelihood of susceptible becoming infected can depend on prevalence of illness likelihood of initiating smoking could depend on accumulated current or former smokers multiple types of stocks e g costs qalys hosts reservoir species etc continuous time distinctive stock flow features multi species model west nile virus refinement of causal loop diagrams system structure diagrams still essentially a qualitative model but less ambiguous by clearly distinguish stocks flows this helps reduce the artifactual loops discussed with clds combine causal loops diagram elements with stock flow structure if complete all loops will go through a stock loop goes into the flow of a stock as one variable in the diagram loop comes comes out of stock as next variable in diagram example system structure diagram note treatment of flows as links from flow to stock inflows as positive links outflows as negative links slides adapted from external source redacted from public pdf for copyright reasons stocks flows diabetes assume diabetes is not curable stocks people without diabetes at different stages of risk people with diabetes flows incident cases both diagnosed undiagnosed deaths from both stocks stocks flows tuberculosis assume that tb infection cannot be totally eliminated stocks susceptible people immunized people people with latent tb infection people with active tb infection flows people becoming latently infected people being vaccinated people with infection going to active tb primary progression people with infection going on to latent tb people with secondary infection going on to active tb deaths from each stock use initial value diabetes model stocks flows for a challenge try creating this in vensim people diabetes inciddeinatbceatesses of pdeoiapbleetwesith deaths ofpe otpimlee dweaithsdoiafbpeetoepsle without diabetes use value use value use value interactive steps view flows and stocks does this make sense hitch up constant auxiliary variables to flows how does changing constant variables change the stock condsiabteatens ftlowfslows dinecaidthesntocfapseospolfedwiaitbhedteiasb ecteusrr ecnturrent what happens to the stock resulting stock green dinecaidthesntocfapseospolfedwiaitbhedteiasb ecteusrr ecnturrent suppose we have these flows rates diabetes flows what happens to the stock some questions diabetes flows when is the stock of people with diabetes at its lowest value dinecaidthesntocfapseeospolefdwiiatbhedteisa bestteosc ksatnodckfalonwd fdleomwodnsetmraotinosntrtateiostn test when is the stock of people with diabetes at its greatest value is the value of the stock of people with diabetes larger at the beginning or end when is the stock of people with diabetes not changing stock green diabetes stock flows dinecaidthesntocfapseeospolefdwiiatbhedteisa bestteosc ksatnodckfalonwd fdleomwodnsetmraotinosntrtateiostn test flows and feedbacks stocks are always changed by flows in your experiments we ve used constant values for flows in general the formulas for the flows will depend on things that are changing state ultimately these things must depend on the things that collectively specify the state the stocks example simple first order decay create this in vensim use initial value pevoiprulelewntith meadnetiamthe until infection dienaftehcstiforonm use formula deaths from infection mean time until death set model settings model menu settings item dynamics of stock people with virulent infection people withvirulent infection current time month dynamics of rate of death flow deaths from infection deaths frominfection current time month stocks as accumulations we often use stocks to accumulate integrate other evolving quantities over time example assume time measured in years example slightly more sophisticated qncalit y of ife for nbpopl dafiloo qntal it y ellg ited qntal i y of ife for smbpopl a non f qncalit y ellghted tron swe of f orm quality ife for opula tion s e of subpopula tion e for opula tion size subp opulation of snbpopl non snbp op l tcon quality eiighted o rmu e t j enme p op l ti on f q i u subp opula tion quality eightoos u b po pual toi a v ife yea r w e d principle structure determines behaviour feedback stock and flow structure of a system determines the possible patterns of behaviour different sets of parameters e g values for constants will select particular behaviour within these behaviour patterns changes to the feedback structure can change behaviour in fundamental ways simple sit model mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible total population incidence rate prevalence recovery delay initial population new infections new recovery newly susceptible immunity loss delay classic feedbacks susceptibles contacts of susceptibles with infectives new infections infectives dynamics person person person person person person person person person person person person person person person state variables over time time months s alternative hc workers exogenous recovery delay person i alternative hc workers exogenous recovery delay person r alternative hc workers exogenous recovery delay person broadening the model boundaries endogenous recovery delay mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible total population staff time per patient healthcare workers initial population incidence rate fractional prevalence recovery delay time until seek treatment new infections new recovery newly susceptible immunity loss delay broadening the model boundaries endogenous recovery delay susceptibles contacts of susceptibles with infectives n ew infections infectives people presenting for treatment waiting times health care staff a different behaviour mode prevalence infectious person person person 1125 time day prevalence baseline hc workers i baseline hc workers person structure as shaping behaviour system structure is defined by stocks flows connections between them nonlinearity the behaviour of the whole is more than the sum of the behaviour of the parts emergent behaviour would not be anticipated from simple behaviour of each piece in turn stock and flow structure including feedbacks of a system determines the qualitative behaviour modes that the system can take on first order delays nathaniel osgood cmpt simple first order decay create this in vensim use initial value pevoiprulelewntith meadnetiamthe until infection dienaftehcstiforonm use formula people with virulent infection mean time until death first order delays and transition processes we can think of first order delays as representing a deterministic approximation to a population experiencing a memoryless poisson stochastic transition process the system is memoryless because the chance of e g a person leaving in the next unit of time is independent of how long they ve been there the probability distribution of residence time in the stock is exponentially distributed dynamics of stock people with virulent infection people withvirulent infection current time month dynamics of rate of death flow deaths from infection deaths frominfection current time month deathrate alpha alpha is per time unit likelihood of death chance of death over small t is t if x people are at risk dying over t is x likelihood of death over t x t x t when people die they flow out cause a negative change in x we denote the change in x over the time t as x thus x x t as x is depleted becomes smaller x becomes smaller as well for a fixed t approximate dynamics suppose x t flow rate dynamics the total change in x over the time t is x thus x x t this might be people over a timeframe of year days the rate of change of x over given time t is x t this is just the sum of all of the flows for system x t x t t x people deathrate because x people changes this flow rate changes over the course of the time we are observing suppose time is measured in years then for our example above x t people per year approximate dynamics net flow rate reminder suppose initial x t why is this approximate our previous graphs used a value of t in calculating the change x from t to t t here t we are assuming that the flow rate people year stays constant in that time recall in general this flow rate will be determined by the value of stocks so in assuming that the flow rate remains constant we were basically assuming that the values of the stocks stay constant over time t for our system given that the value of the stock x people declines by around per time unit this is not a very good assumption how can we reduce the error try a smaller t let work forward for of a year at a time instead of for a full year x t approximate dynamics net flow rate t t t vensim has a step size set via model menu settings item impact of step size on simulation continuous mathematics calculus inciddeinabete alpha is per time unit likelihood of death chance of death over small dt is dt if x people are at risk dying over dt is x likelihood of death over t x dt x dt when people die they flow out cause a negative change in x we denote the change in x over the time dt as x thus dx x dt as x is depleted becomes smaller dx becomes smaller as well for a fixed dt flow rate dynamics continuous the total change in x over the time dt is dx thus dx x dt this might be people over a timeframe of year days the rate of change of x over given time dt is dx dt this is just the sum of all of the flows for system dx dt x dt dt x people deathrate because x people changes this flow rate changes over the course of the time we are observing we will sometimes write dx dt as x dx dt x x the concept of analytic solutions the model structure describes system behaviour implicitly this indicates how short term changes flows depends on the state of the system this does not explicitly state how the system evolves analytic closed form exact solutions describe system behaviour as an explicit function of time e g a b t c a b t a sin t e t for many systems we will be dealing with nonlinear systems an analytic solution is simply not derivable even when an analytic solution is possible it is often most convenient to deal with simulations for most needs an exact solution to our problem the state equation formulation of our system is dx dt x x this is a linear differential equation with constant coefficients a type of system that can be solved exactly solution procedure dx x dt suppose we start x at time with initial value x and we want to find the value of x at time t assuming that x does not start at it will never reach exactly so we can divide the left side by it and multiply the right side by dt dx dt x t t dx t t integrating both sides t t dt completion of derivation t t dx t t t t t t dt dt t ln x t t t t t t t ln x t ln x t ln ln x x t t x t eln x t eln x e t x e t so the stock x declines as a negative exponential in time t of people remaining in the stock goes down exponentially w time fraction of original people still in stock or who have left assuming no inflows the fraction of people still in the stock at time t is just of people in the stock at time t initial of people in the stock t x t x e e t x x given that people either stay in the stock or leave the fraction that havxe ltef t by time t x e t at time at time t we have a fraction e e in the stock and a e who have left note by its taylori expansion e t t t t t i i t t for small t the higher order terms are very small and this will be approximately t so by time for small approx will remain after and a fraction of will have departed mean time to transition people are leaving via the flow suppose we wish to determine the mean average time for a given person in the stock to leave recall a mean for a continuous probabilti ty distribution p t is given by tp t dt since p t dt t is the probability that will leave between t and t dt this is just the continuous version of e q a a possible values of a aq a mean time to leave p t dt here is the likelihood of a person leaving exactly between time t dt t we start the simulation at t so p t for t for t p leaving exactly between time t and dt t p leaving exactly between time t and t dt still have not left by time t p still have not left by time t for t p still have not left by time t e t for p leaving exactly between time t and t dt still have not left by time t recall for us probability of leaving in a time dt always dt thus p leaving exactly between time t and t dt still have not left by time t dt p t dt p leaving exact b t time t dt t e t dt e t dt derivation of mean p t dt p leaving exactly between time t dt t e t dt e t dt now that we hatv e found the function p t we must do the integral t tp t dtto derive the mean here e p t t t tp t dt t t t tp t dt t t t e t dt t te dt recall integration by parts t t we have e p t te t dt te t dt t t to solve the term in brackets we will use integration by parts integration by padr tusve xplodvits thdeu following l u v dt dt dt d uv udv vdu d uv udv vdu uv udv vdu and thus udv uv vdu recall integration by parts to solve parts t te t dt we will use integration by du here u t du dt dt dt dv e t dt v e t dt e t from the previous page we know t t t e t t t e t te t dt udv uv vdu t dt t te t t t t t t t t e t dt e t t t t thus the mean time the delay associated with a first order delay is thtu given by t e p t t te t dt t te t dt so e g if we have an annualized rate of diabetes incident the mean time to develop diabetes independent of other risks is just the reciprocal of that rate i e over that rate computer exercise simulating a first order delay create a first order delay feed in a step function that rises suddenly at time how does the output from the stock change over time competing risks suppose we have another outflow from the stock how does that change our mean time of proceeding specifically down flow here developing diabetes annruaaltizee daldphiaab etes inciddeinatbcetaesses of annruaatleiz ebdedtae ath deaths competing risks stock trajectory solution procedure dx x x dt x suppose we start x at time with initial value x and we want to find the value of x at time t this is just like our previous differential equation except that has been replaced by the solution must therefore be the same as before with the appropriate replacement thus x t x e t mean time to leave competing risks p t dt here is the likelihood of a person leaving via flow e g developing exactly between time t dt t we start the simulation at t so p t for t for t p leaving on flow exactly between time t dt t p leaving on flow exactly between time t t dt still have not left by time t p still have not left by time t for t p still have not left by time t e t for p leaving exactly between time t and t dt still have not left by time t recall for us probability of leaving in a time dt always dt thus p leaving exactly between time t and t dt still have not left by time t dt p t dt p leaving exact b t time t dt t e t dt mean time to transition via flow competing risks by the same procedure as before we have t e p t t te t dt using the formula we derived for the integr al expression we have e p t note that this correctly approaches the single flow case as equilibrium value of a first order delay suppose we have flow of rate i into a stock with a first order delay out this could be from just a single flow or many flows the value of the stock will approach an equilibrium where inflow outflow equilibrium value of order delay recall outflow rate for order delay x note that this depends on the value of the stock inflow rate i at equilibrium the level of the stock must be such that inflow outflow for our case we have x i thus x i the lower the chance of leaving per time unit or the longer the delay the larger the equilibrium value of the stock must be to make outflow inflow computer exercise simulating a first order delay create a first order delay feed in a step function that rises suddenly from to at time use formula if then else time questions to ponder how does the output from the stock change over time how does the equilibrium value of the stock vary with chance of proceeding alpha first order delays in action simple sit model mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible incidence rate total population prevalence recovery delay initial population t new infections new recovery new illness department of computer science cumulative illnesses newly susceptible immunity loss delay first order delays in action simple sit model recall simple first order decay use initial value pevoiprulelewntith meadnetiamthe until infection dienaftehcstiforonm use formula people with virulent infection mean time until death first order decay variant of last time recall how does this relate to the mean time until death use value use initial value use formula people with virulent infection per month likelihood of death people in stock flow rate of deaths cumulative deaths closeup why this gap per month risk of deaths why this gap answer the gap is present because not all people are at risk for a month the value of the stock is declining over the first month the rate of death indicates that of the population will die per month while we may have been expecting people of the to die this erroneously assumes that all were at risk for the entire month in fact because the stock was declining there were considerably fewer people at risk meaning that we have fewer deaths if we had maintained people in the stock for the month people would have died recall first order delay use value use value immigrationrate immigration adnenautahl ralipskhao f people x deaths use initial value use formula people x annual risk of death alpha questions what is behaviour of stock x what is the mean time until people die suppose we had a constant inflow what is the behaviour then answers people x behaviour of stock people x baseline mean time until death time year recall that if coefficient of first order delay is then mean time is here years equilibrium value of a first order delay suppose we have flow of rate i into a stock with a first order delay out this could be from just a single flow or many flows the value of the stock will approach an equilibrium where inflow outflow equilibrium value of order delay recall outflow rate for order delay x note that this depends on the value of the stock inflow rate i at equilibrium the level of the stock must be such that inflow outflow for our case we have x i thus x i equivalently x i mean time to transition the lower the chance of leaving per time unit or the longer the delay the larger the equilibrium value of the stock must be to make outflow inflow scenarios for first order delay variation in inflow rates for different immigration inflows what do you expect inflow inflow inflow inflow why do you see this goal seeking pattern what is the goal being sought behaviour of stock for different inflows why do we see this behaviour behaviour of outflow for different inflows why do we see this behaviour imbalance gap causes change to stock rise or fall change to outflow to lower gap until outflow inflow goal seeking behaviour the goal seeking behaviour is associated with a negative feedback loop the larger the population in the stock the more people die per year if we have more people coming in than are going out per year the stock and hence outflow rises until the point where inflow outflows if we have fewer people coming in than are going out per year the stock declines outflow declines until the point where inflow outflows as a causal loop diagram what does this tell us about how the system would respond to a sudden change in immigration response to a change feed in an immigration step function that rises suddenly from to at time set the initial value of stock to how does the stock change over time create a custom graph display it as an input output object editing create input output object for synthesim stock starting empty finlfoloww anrdaotuetflsow idmeamthigsr atsiotenp how would this change with alpha stock starting empty value of stock alpha how would this change with alpha for different values of alpha flow rates outflow rises until inflow this is for the flows what do stocks do for different values of alpha value of stocks why do we see this behaviour a longer time delay or smaller chance of leaving per unit time requires x to be larger to make outflow inflow outflows as delaved version of inputs in flo v and outflo v inflovv and outflovv iq rn time y ecn lq q q lq sq lqq time year illlfilligrn lion step functimu yr d elay dea ths step yr delay mmiigrnrtr ion step yr clejla y eaths step yr d ejla y inflow and outflow rn inflow and outflow q time y eru rn time y ear nrumgratfon step funoliorn q yr delay deaths step functions yt d elay what if stock doesn t start empty decays at first no inflow then output responds with delayed versi people x o i p e e x s unc ions at i i l x alpha people x step functions at initial x alpha deaths step functions at alpha simple sit model mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible total population incidence rate prevalence recovery delay initial population new infections new recovery newly susceptible immunity loss delay classic feedbacks susceptibles contacts of susceptibles with infectives new infections infectives dynamics person person person person person person person person person person person person person person person state variables over time time months s alternative hc workers exogenous recovery delay person i alternative hc workers exogenous recovery delay person r alternative hc workers exogenous recovery delay person broadening the model boundaries endogenous recovery delay mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible total population staff time per patient healthcare workers initial population incidence rate fractional prevalence recovery delay time until seek treatment new infections new recovery newly susceptible immunity loss delay broadening the model boundaries endogenous recovery delay susceptibles contacts of susceptibles with infectives n ew infections infectives people presenting for treatment waiting times health care staff a different behaviour mode prevalence infectious person person person 1125 time day prevalence baseline hc workers i baseline hc workers person structure as shaping behaviour system structure is defined by stocks flows connections between them nonlinearity the behaviour of the whole is more than the sum of the behaviour of the parts emergent behaviour would not be anticipated from simple behaviour of each piece in turn stock and flow structure including feedbacks of a system determines the qualitative behaviour modes that the system can take on essentials of aggregate system dynamics infectious disease models nathaniel osgood cmpt february mathematical models link together diverse factors typical factors included infection mixing transmission development loss of immunity both individual and collective natural history often multi stage progression recovery birth migration aging mortality intervention impact sometimes included preferential mixing variability in contacts strain competition cross immunity quality of life change health services interaction local perception changes in behavior attitude immune response emergent characteristics of infectious diseases models instability nonlinearity tipping points oscillations multiple fixed points equilibria endemic equilibrium disease free equilibrium oscillations delays the oscillations reflect negative feedback loops with delays these delays reflect stock and flow considerations and specific thresholds dictating whether net flow is positive or negative stock flow stock continues to deplete as long as outflow exceeds inflow rise as inflow outflow the stock may stay reasonably high long after inflow is low key threshold r when of individuals being infected by a single infective this is the threshold at which outflows inflows jan mar may july sept nov jan mar may july sept nov jan mar may july sept nov jan mar may july sept nov jan mar may july sept nov jan mar may july sept nov jan mar may july sept nov jan mar may july sept nov saskatchewan childhood diseases p o p o b b b s o o p i d d d z d 1921 l t i i t i l v j r c n ii it ii i id l j 1973 i i l j 19eil8 j t l l i i fa i i en q e i i r year fig weekly case notifications of measles in england and wales for the period to prior to the introduction of mass vaccination l o vig correlogram of weekly measles reports for england and wales here nd in subsequent correlograms the solid triangles indicate the per cent confidence its for the zero correlogram from a completely random series and pis the probability at such data could generate the observed correlogram see appendix in anderson et nonlinearity in state variables effect of multiple policies non additive doubling investment does not yield doubling of results leads to multiple basins of tracking equilibrium multiple equilibria tipping points separate basins of attraction have qualitatively different behaviour oscillations endemic equilibrium disease free equilibrium equilibria disease free no infectives in population entire population is susceptible endemic steady state equilibrium produced by spread of illness assumption is often that children get exposed when young lo u l b q xo q a o c u if co o eo u c l age years age years fig measles a proportion of children who had experienced an attack of measles at various ages in england and wales in based on case notification records dots observed values full curve predictions of a simple catalytic mode with age dependent rates of infection see text b the age dependency in the rate or force of infection i a dots calculated values full curve best fit linear model of the form a m va where m and v r tb in sk example stis hc workers hc workers hc workers kendrick mckermack model partitioning the population into broad categories susceptible s infectious i removed r births contacts per susceptible per contact risk of infection force of infection incidence fractional prevalence mean time with disease recovery initial population initial fraction vaccinated mortality rate mortality rate population fractional prevalence shorthand for key quantities for infectious disease models stocks i or y total number of infectives in population this could be just one stock or the sum of many stocks in the model e g the sum of separate stocks for asymptomatic infectives and symptomatic infectives n total size of population this will typically be the sum of all the stocks of people s or x number of susceptible individuals mathematical notation i key quantities for infectious disease models parameters contacts per susceptible per unit time c e g contacts per month this is the number of contacts a given susceptible will have with anyone per infective with susceptible contact transmission probability this is the per contact likelihood that the pathogen will be transmitted from an infective to a susceptible with whom they come into a single contact intuition behind common terms i n the fraction of population members or by assumption contacts that are infective important simplest models assume that this is also the fraction of a given susceptible contacts that are infective many sophisticated models relax this assumption c i n number of infectives that come into contact with a susceptible in a given unit time c i n force of infection likelihood a given susceptible will be infected per unit time the idea is that if a given susceptible comes into contact with c i n infectives per unit time and if each such contact gives likelihood of transmission of infection then that susceptible has roughly a total likelihood of c i n of getting infected per unit time e g month key term flow rate of new infections this is the key form of the equation in many infectious disease models total of susceptibles infected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n note that this is a term that multiplies both s and i this is much different than the purely linear terms on which we have previously focused likelihood is actually a likelihood density e g can be indicating that mean time to infection is another useful view of this flow recall total of susceptibles infected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n the above can also be phrased as the following s c i n i c s n i c f of infectives mean susceptibles infected per unit time by each infective this implies that as of susceptibles falls of susceptibles surrounding each infective falls the rate of new infections falls less fuel for the fire leads to a smaller burning rate a critical throttle on infection spread fraction susceptible f the fraction susceptible here s n is a key quantity limiting the spread of infection in a population recognizing its importance we give this name f to the fraction of the population that issusceptible the importance of susceptible fraction recall total of susceptibles infected per unit time of susceptibles likelihood a given susceptible will be infected per unit time s force of infection s c i n the above can also be phrased as the following s c i n i c s n of infectives average susceptibles infected per unit time by each infective this implies that as fraction of susceptibles falls fraction of susceptibles surrounding each infective falls the rate of new infections falls less fuel for the fire leads to a smaller burning rate basic model structure immigration rate immigration associated feedbacks susceptibles csounstcaecptstibblestwaenedn infectives new infections new recoveries recall our model set c people month chance of transmission per s i contact μ birth and death rate initial infectives other susceptible mathematical notation i people people people people people people people people people people people people people people people example dynamics of sir model no births or deaths sir example 170 time days susceptible population s sir example people infectious population i sir example people recovered population r sir example people explaining the stock flow dynamics initially infectives su sceptibles over time more infectives and each infective infects c s n c people on average for each time unit the maximum possible rate the rate of recoveries is in short term infectives grows quickly rate of infection rises quickly positive feedback susceptibles starts to decline but still high enough that each infective is surrounded overwhelmingly by susceptibles so efficient at transmitting fewer susceptibles fewer s around each i rate of infections per i declines many infectives start recovering slower rise to i tipping point of infectives plateaus rate of infections rate of recoveries each infective infects exactly one replacement before recovering in longer term declining of infectives susceptibles lower lower rate of new infections negative feedback change in i dominated by recoveries goal seeking to negative feedback case outbreak people people people people people people people people people people people people people people people sir example 170 time days susceptible population s sir example people infectious population i sir example people recovered population r sir example people shifting feedback dominance people people people people people people people people people people people people people people people sir example 170 time days susceptible population s sir example people infectious population i sir example people recovered population r sir example people introducing births deaths consider the introduction of birth death changes the behaviour why would this affect things how would it make it a difference table t lnter epi kmic period t of some common infections from anders n and may and theoretical predictions of the period eqn t s inter epidemic period t years infection observed geographical location and time period average age at infection latent plus infectious period d d days inter epidemic period t years calculated measles england and wales i aberdeen scotland l mumps poliomyelitis echovirus type il smallpox chickenpox coxsackie virus type england and wales baltimore usa england and wales england and wa es india l new york city usa glasgow scotland england and wales mrcoplasma pneumoniae england and wales i a u j c la o o r t qt time i yr lo u l b q xo q a o c u if co o eo u c l age years age years fig measles a proportion of children who had experienced an attack of measles at various ages in england and wales in based on case notification records dots observed values full curve predictions of a simple catalytic mode with age dependent rates of infection see text b the age dependency in the rate or force of infection i a dots calculated values full curve best fit linear model of the form a m va where m and v r u l l o c j baste reproductive rate vrg the peak fraction infected y ax and the fraction ever infected plotted a functions of see text and eqns and table estimated values of the basic reproductive rate r for various infections data from anderson anderson and may anderson et al nokes and anderson delays for a while after infectives start declining i e susceptibles are below sustainable endemic value they still deplete susceptibles sufficiently for susceptibles to decline for a while after susceptibles are rising until susceptibles endemic value infectives will still decline for a while after infectives start rising births of infections susceptibles will rise to a peak well above endemic level blue susceptible red infective green force of infection this is the point where rate of new infections rate of recoveries a person infects on average person before recovering the level of susceptibles is at the lowest level where the infection is sustainable in the short run at this point susceptibles susceptibles at endemic equilibrium why is the of susceptibles rising to well above its sustainable value why is the of susceptibles still declining this fraction of susceptibles at endemic equilibrium is the minimum sustainable value of susceptible i e the value where the properties above hold above this fraction of susceptibles the infected will rise below this fraction of susceptibles the infected will fall the rise is occurring because infectives are so low that so few infections occur that births infections deaths s rises above the sustainable value because infectives are still in decline until that point so infectives remain low for a while the susceptibles are still declining here because the large of infectives still causes enough infections that rate of immigration rate of infections deaths equilibrium behaviour with births deaths the system can approach an endemic equilibrium where the infection stays circulating in the population but in balance the balance is such that simultaneously the rate of new infections the rate of immigration otherwise of susceptibles would be changing the rate of new infections the rate of recovery otherwise of infectives would be changing tipping point now try setting transmission rate β to case infection declines immediately infectives 90 time month infectives infection extinction recall closed population no birth death infection always dies out in the population some infections will take longer to die out there is a tipping point between two cases of people infected declines out immediately infection causes an outbreak before the infection dies down of people infected rises and then falls recall simple model incorporating population turnover contacts per susceptible infection fractional prevalence mean time with disease susceptible incidence infective mortality recovery recovered recovered mortality mortality mortality rate mortality rate recall our model set c people month chance of transmission per s i contact μ birth and death rate initial infectives other susceptible here the infection can remain endemic damped oscillatory behavior modify model to have births and deaths with an annual birth and death rate set model settings final time to long time frame in synthesim running man mode set birth death rates 01 exploring the tipping point now try setting transmission rate β to infection extinction as for the case with a closed population an open population has two cases infection dies out immediately infectives 90 outbreak infection takes off time month here in contrast to the case for a closed population the infection will typically go to an endemic equilibrium table t lnter epi kmic period t of some common infections from anders n and may and theoretical predictions of the period eqn t s inter epidemic period t years infection observed geographical location and time period average age at infection latent plus infectious period d d days inter epidemic period t years calculated measles england and wales i aberdeen scotland l mumps poliomyelitis echovirus type il smallpox chickenpox coxsackie virus type england and wales baltimore usa england and wales england and wa es india l new york city usa glasgow scotland england and wales mrcoplasma pneumoniae england and wales i a u j c la o o r t qt time i yr typically in endemic equilibrium the uninfected fraction of the population s n is the young u l l o c j baste reproductive rate vrg the peak fraction infected y ax and the fraction ever infected plotted a functions of see text and eqns and table estimated values of the basic reproductive rate r for various infections data from anderson anderson and may anderson et al nokes and anderson delays for a while after infectives start declining i e susceptibles are below sustainable endemic value they still deplete susceptibles sufficiently for susceptibles to decline for a while after susceptibles are rising until susceptibles endemic value infectives will still decline for a while after infectives start rising births of infections susceptibles will rise to a peak well above endemic level infection recall for this model a given infective infects c s n others per time unit this goes up as the number of susceptibles rises questions if the mean time a person is infective is μ how many people does that infective infect before recovering with the same assumption how many people would that infective infect if everyone else is susceptible under what conditions would there be more infections after their recovery than before fundamental quantities we have just discovered the values of famous epidemiological quantities for our model effective reproductive number r basic reproductive number effective reproductive number r number of individuals infected by an index infective in the current epidemological context depends on contact number transmission probability length of time infected fraction of susceptibles affects whether infection spreads if r of cases will rise if r of cases will fall alternative formulation largest real eigenvalue endemic rate basic reproduction number number of individuals infected by an index infective in an otherwise disease free equilibrium this is just r at disease free equilibrium all other people in the population are susceptible other than the index infective depends on contact number transmission probability length of time infected affects whether infection spreads if epidemic takes off if epidemic dies out alternative formulation largest real eigenvalue initial infection rise exp t d endemic rate basic reproductive number if contact patterns infection duration remain unchanged and if fraction f of the population is susceptible then mean of individuals infected by an infective over the course of their infection is f in endemic equilibrium inflow outflow s n every infective infects a replacement infective to keep equilibrium just enough of the population is susceptible to allow this replacement the higher the the lower the fraction of susceptibles in equilibrium generally some susceptibles remain at some point in epidemic susceptibles will get so low that can t spread our model set c people month chance of transmission per s i contact μ birth and death rate initial infectives other susceptible what is what should we expect to see thresholds r too low susceptibles r of infectives declining too high susceptibles r of infectives rising infection is introduced from outside will cause outbreak herd immunity infection is introduced from outside will die out may spread to small number before disappearing but in unsustainable way this is what we try to achieve by control programs vaccination etc outflow from susceptibles infections is determined by the of infectives equilibrium behaviour with births deaths the system can approach an endemic equilibrium where the infection stays circulating in the population but in balance the balance is such that simultaneously the rate of new infections the rate of immigration otherwise of susceptibles would be changing the rate of new infections the rate of recovery otherwise of infectives would be changing disease free equilibria no infectives in population entire population is susceptible endemic steady state equilibrium produced by spread of illness assumption is often that children get exposed when young the stability of the these equilibria whether the system departs from them when perturbed depends on the parameter values for the disease free equilibrium on vaccination adding vaccination stock add a vaccinated stock a constant called monthly likelihood of vaccination vaccination flow between the susceptible and vaccinated stocks the rate is the stock times the constant above set initial population to be divided between stocks susceptible vaccinated incorporate vaccinated in population calculation additional settings c beta duration of infection birth death rate adding stock experiment with different initial vaccinated fractions fractions infectives time month w ec t i es no inun i gra tion i es t f rac tion i ac cinated w ec t i es no lnun i gra tion i es t fraction vaccinated infecti es no hmnigra tion tes t fraction vaccinated infec t i es no inunigra tion tes t fr ac tion i ac cinated w ec t i es no inun i gra tion i es t f rac tion i ac cinated wec t i es no lnun i gra tion i es t fraction vaccinated infecti es no hmnigra tion tes t fraction vaccinated recall thresholds r too low susceptibles r of infectives declining too high susceptibles r of infectives rising outflow from susceptibles infections is determined by the of infectives delays for a while after infectives start declining they still deplete susceptibles sufficiently for susceptibles to decline for a while after infectives start rising the of infections is insufficient for susceptibles to decline effective reproductive number r number of individuals infected by an index infective in the current epidemiological context depends on contact number transmission probability length of time infected fraction of susceptibles affects whether infection spreads if r of cases will rise if r of cases will fall alternative formulation largest real eigenvalue endemic rate basic reproduction number number of individuals infected by an index infective in an otherwise disease free equilibrium this is just r at disease free equilibrium all other people in the population are susceptible other than the index infective depends on contact number transmission probability length of time infected affects whether infection spreads if epidemic takes off if epidemic dies out alternative formulation largest real eigenvalue initial infection rise exp t d endemic rate recall a critical throttle on infection spread fraction susceptible f the fraction susceptible here s n is a key quantity limiting the spread of infection in a population recognizing its importance we give this name f to the fraction of the population that issusceptible if contact patterns infection duration remain unchanged and then mean of individuals infected by an infective over the course of their infection is f recall endemic equilibrium inflow outflow s n f every infective infects a replacement infective to keep equilibrium just enough of the population is susceptible to allow this replacement the higher the the lower the fraction of susceptibles in equilibrium generally some susceptibles remain at some point in epidemic susceptibles will get so low that can t spread critical immunization threshold consider an index infective arriving in a worst case scenario when noone else in the population is infective or recovered from the illness in this case that infective is most efficient in spreading the goal of vaccination is keep the fraction susceptible low enough that infection cannot establish itself even in this worst case we do this by administering vaccines that makes a person often temporarily immune to infection we say that a population whose f is low enough that it is resistant to establishment of infection exhibits herd immunity critical immunization threshold vaccination seeks to lower f such that f worst case suppose we have a population that is divided into immunized vaccinated and susceptible let qc be the critical fraction immunized to stop infection then f qc f qc qc so if as in our example qc i e of population must be immunized just as we saw infectious disease models vaccination cmpt nathaniel osgood equilibrium behaviour with births deaths the system can approach an endemic equilibrium where the infection stays circulating in the population but in balance the balance is such that simultaneously the rate of new infections the rate of immigration otherwise of susceptibles would be changing the rate of new infections the rate of recovery otherwise of infectives would be changing disease free equilibria no infectives in population entire population is susceptible endemic steady state equilibrium produced by spread of illness assumption is often that children get exposed when young the stability of the these equilibria whether the system departs from them when perturbed depends on the parameter values for the disease free equilibrium on adding vaccination stock add a vaccinated stock a constant called monthly likelihood of vaccination vaccination flow between the susceptible and vaccinated stocks the rate is the stock times the constant above set initial population to be divided between stocks susceptible vaccinated incorporate vaccinated in population calculation additional settings c beta duration of infection birth death rate adding stock experiment with different initial vaccinated fractions fractions infectives time month w ec t i es no inun i gra tion i es t f rac tion i ac cinated w ec t i es no lnun i gra tion i es t fraction vaccinated infecti es no hmnigra tion tes t fraction vaccinated infec t i es no inunigra tion tes t fr ac tion i ac cinated w ec t i es no inun i gra tion i es t f rac tion i ac cinated wec t i es no lnun i gra tion i es t fraction vaccinated infecti es no hmnigra tion tes t fraction vaccinated critical immunization threshold consider an index infective arriving in a worst case scenario when noone else in the population is infective or recovered from the illness in this case that infective is most efficient in spreading the goal of vaccination is keep the fraction susceptible low enough that infection cannot establish itself even in this worst case we do this by administering vaccines that makes a person often temporarily immune to infection we say that a population whose f is low enough that it is resistant to establishment of infection exhibits herd immunity critical immunization threshold vaccination seeks to lower f such that f worst case suppose we have a population that is divided into immunized vaccinated and susceptible let qc be the critical fraction immunized to stop infection then f qc f qc qc so if as in our example qc i e of population must be immunized just as we saw intervention impact on an open population nathaniel osgood open closed population effects of an open population differentinpfecativerameters approaches endemic level where r rate of new infections rate of recoveries because no new influx of susceptibles fuel infectives in constant decline approaches disease free equilibrium 350 550 time month infective baseline annual turnover infective baseline closed population effects of an open population susceptible approaches endemic level where r rate of arrivals via birth migration rate of new infections deaths approaches disease free level where no infection is occurring 350 550 950 time month susceptible baseline annual turnover susceptible baseline closed population recovereds 350 550 650 700 850 950 time month recovered baseline annual turnover recovered baseline closed population impact of turnover the greater the turnover rate the greater the fraction of susceptibles in the population the greater the endemic rate of infection fraction of susceptibles effective reproductive number prevalence effec i e reproduc i e number tme year fraction recovered adding ongoing vaccination process simulating introduction of vaccination for a childhood infection in an open population c beta 05 duration of infection initial fraction vaccinated monthly birth death rate per year focusing on children years of age questions what is what level of susceptibles is required to sustain the infection what is the critical vaccination fraction ociiia ed rt q rme yem ftrl oti oo tioo ifl terl hccili tioo l mt a what rate of vaccination eliminates p e lence dj rut rui j i urt ur l l l i l i ce j pl evi l i ce p time ear rl ce bi c r lllittl i hoo mt f rhc ion of suscep ible in popula ion ur qi time un i s ti ts rn s s tibt i ri s ti l rn y ief y i f y s representing quarantine cl3 p e alence tune ff ear o f o time ear endemic situations in an endemic context infection remains circulating in the population the common assumption here is that the susceptible portion of the population will be children at some point in their life trajectory at an average age of acquiring infection a individuals will be exposed to the infection develop immunity table t lnter epi kmic period t of some common infections from anders n and may and theoretical predictions of the period eqn t s inter epidemic period t years infection observed geographical location and time period average age at infection latent plus infectious period d d days inter epidemic period t years calculated measles england and wales i aberdeen scotland l mumps poliomyelitis echovirus type il smallpox chickenpox coxsackie virus type england and wales baltimore usa england and wales england and wa es india l new york city usa glasgow scotland england and wales 12 12 18 mrcoplasma pneumoniae england and wales age of exposure reproductive constant cf a natural non immunized constant size population where all die at same age and where mean age at death l mean age of exposure a i e we assume those above a are exposed fraction susceptible is s n a l i e proportion of population below age a recall for our and many but not all other models r s n s n thus a l l a this tells us that the larger the the earlier in life individuals become infected incompletely immunized population suppose we have q fraction of population immunized q qc suppose we have fraction f susceptible fraction of the population currently or previously infected is q f if we assume as previously that everyone lives until l and is infected at age a then fraction a l has been infected so a l q f a l q f this can be much higher than for the natural population this higher age of infection can cause major problems due to waning of childhood defenses i e incomplete immunization leads to older mean age ofexposure 7 c q asc e c age years b 7 c c a e age years fig the age and sex dependent risk of serious complications arising from infection hy the mumps vi rus the points represent the proportion of cases of mumps ad m i tte d to ho sp it in england and wales in that presented with complications data from rcg p adjusted to mirror the proportion of the cotal number of cases of mumps in each age class see texl for further deta ils the recorded risk values denote relative as opposed to absolute changes with respect to age full curves best fit polynomials of the form m aj a hna a solid squares males and open circles females denote the total relative risk of complications b complications divided into the risk of meningitis a nd o r encephalitis in males solid sq ua re and females open diamonds and the risk of orchitis in males open ci rcles parameter values as defined in anderson et al cmpt nathaniel osgood lecture cmpt focus systems simulation models for public health purpose of models model strength limitations diversity of classes of models available how models are built refined analyzed software analytic tools for working with models how models mesh with traditional techniques linkage databases real time data collection ema biostatistics class objectives to help students learn to appreciate and critique existing models understand the proper limitations and limitations of such models understand the mathematical foundation on which models are based gain familiarity with modeling software learn how to conceptualize formulate and analyze dynamic models regardless of application area gain experience in applying such models in the public health context understand some open areas of modeling research department of computer science class will be highly interactive informal adapted to student interests aimed for accessibility to diverse audience some material presented in additional sessions for certain backgrounds department of computer science anticipated class coverage motivations basics of systems thinking causal loop diagrams stock flow diag state equations focused discussion of particular areas chronic infectious c i interactions individual based vs aggregate tradeoffs network models department of computer science class coverage cont d modeling process scoping formulation parameterization calibration validation confidence building model analysis tools techniques possible uncertainty stochastics department of computer science class diversity our class is expected to be diverse in many ways students faculty observers student backgrounds in ch e mph biostats computer science economics participant interests participant background in particular subject the instructor will make efforts to address diverse backgrounds interests please be respectful of those from all backgrounds extra resources for students office hours focused tutorials providing extra background context providing more advanced material upon student interest likely topics software basics epi terminology calculus intuitions elements of of differential equations analysis techniques department of computer science what is expected of you attendance participation reading papers before class modeling exercises project with instructor guidance end of term presentation department of computer science classroom exercises interactive modeling exercises on laptops will be a key component of the course we will have pre installed laptops delivered to the classroom for students who need them please speak with the instructor if you d like a laptop department of computer science administrative info good reference sterman j business dynamics boston mcgraw hill higher education office hours friday thorv by appointment especially important b c of diversity of backgrounds limited time course website at usask ca department of computer science project information multi person projects project can be modeling application in area for which data is readily available paper review critique methodological study instructor can help facilitate talk with instructor about any ideas of strong personal interest meet early with the instructor to discuss possdepiabrtmienltioftcioempsuter science resources vensim download vensim is also installed on lab computers laptops provided by dept webct department of computer science motivation assisting management of complex situations serve as what if tool for counterfactuals identifying desirable policies cost effective high leverage robust prioritizing research data collection help make sense of interaction of diverse information processes understanding drivers for trends communication e g learning labs for stakeholders department of computer science complexities regularities department of computer science measles mumps in sk department of computer science slides adapted from external source redacted from public pdf for copyright reasons public health as redirecting the course of change adapted from tom wong data for and are preliminary and are anticipated to change source surveillance and epidemiology unit community acquired infections division phac public health as redirecting the course of change tom wong data for and are preliminary and are anticipated to change source surveillance and epidemiology unit community acquired infections division phac delays complexities presentation of symptoms contact tracing identification of asymptomatic interactions e g stis hiv hcv hiv chronic infectious illness feedbacks intergenerational social network mediated immune system with healthcare system behavior change after knowledge of health status risk perceptions nonlinear risk cost intervention synergies heterogeneity in progression behaviour complexities matter for intervention selection blowback multiplier effects presence of tipping points tradeoffs of prevention vs screening vs contact tracing treatment interaction between infections with chronic d evaluation of focused intervention on presenting individuals risk perception youth risk attitudes social network effects sex workers social network effects centrality in social network peer effects immuno compromised evaluation of intervention portfolios common phenomena in complex systems snowballing when things go bad they often go very bad very quickly vicious cycles lead to cascading of problems path dependence different starting points can lead to divergence in project progress lock in policy resistance situation can be unexpectedly difficult to change slides adapted from external source redacted from public pdf for copyright reasons a metaphor for scientific exploration department of computer science pieces of the elephant esrd rtment of computer pieces of the elephant tb saskatchewan war on white plague l l llt ad a anced cmes o n d otl l s cases and contact tracing contacts examined incident cases o o ooi is i j l i l o lloo l o o kl o w c ont lc t tr aced per ase ines rate pieces of the elephant sti regularities arise from underlying processes the time series shown are tightly interrelated not independent many of the features of the time series are driven by the same underlying processes natural history of infection demographic change of the population mechanisms of infection transmission risk behaviour risk perception health system response simulation seeks insight from characterizindepgartmcent aof cuomsputaer slciesncetructure of those systems simulation models simulation models can be viewed as dynamic hypotheses concerning the causal structure underlying observed patterns we need to understand causal structure to understand counterfactuals how patterns would change if we were to change x all simulation models are computational realizations of a mathematical process there are many dynamic mathematical frameworks for defining simulation models department of computer science all of these frameworks characterize processes slides adapted from external source redacted from public pdf for copyright reasons the pieces of the elephant example model of underlying process time series it must match death time year h t ie l t od ti d m d ibt ti rr b d di ry c p j poo stior e pi c li tot di b tic d itl b by e tt l dc ity g popul rioo e pi c li m ti dj i hi t ie t od t u d ibt t irr d ic iry p jj popr b ior pi c i i t tor di tic d id b d di ty psj poj tior e i c li m tior m ri lnt rnd uction o mass x ra scre e nin 948 1i nts oft sl rt ptomycln du pa l nl son l soi g cl o pas l lo be i du t l t nc u pas l l ofli ld l o used o ibu du tr n i smlll t year of diagnosiis aggregate simulation models tb desired calibr ated calibar cti m f inal ip dpulaticm size is cr ep eqn fll tlriml si ii e ls ckd stage ot dependent on diabetes f a i m o ent death rate rate of tb ifrac n of tb patients a are trna ed dur atidn o t reatment tbac vtia tdin reac tivaoti n o from ltb death death of causes c other than tb tb graft ifailure graft failure rate treatment dx tb graft failure graft failure rate pieces of the elephant sti an example sti model mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible incidence rate total population prevalence recovery delay initial population new infections new recovery new illness newly susceptible immunity loss delay department of computer science individual based modeling simulation models meandaystonatur all y clea tbpro gressi onst atediart ection w het heri nfe cted whet herpri mary progressio aggregate simulation models esrd f c e ffiffii al likehe d t rid pr gres by ethni ity ba eline annual likeh e d t ld ri k pmy e birthv drical birth for by etbnicity ye h e populatie n wiih rhisp e a f ll lj su one fo int in rime popuh ti ckdl f e ha e p a p uh ti e n with p e a diabeii e srd birth populaiie n at stages t f e i f a l but n i ckd sta es c flfient l at ea h gkd stage diagnosed p cl tion flow ri k death rate k out e a t data on a ggrega nm th o f p e puh tioo at ckd sta e i a u l l l l a j t v if individual based simulation models esrd deleterious feedbacks cutting cigarette tar levels reduces cessation cutting cigarette nicotine levels leads to compensatory smoking targeted anti tobacco interventions lead to equally targeted coupon programs by tobacco industry charging for supplies for diabetics leads to higher overall costs by increases costs due to reduced self management faster disease progression arvs prolong lives of hiv carriers but lead to resurgent hiv epidemic due to lower risk perception saving money by understaffing sti clinics leads to long treatment wait greater risk of transmission by infectives bigger epidemics antibiotic overuse worsens pathogen resistance antilock breaks lead to more risky driving natural feedback intergenerational vicious cycles examples of high profile simulation modeling projects cdc diabetes model influenced health goals adapted to states simsmoke us national tobacco policy countries department of computer science outline motivation for systems modeling concepts model vignettes aggregate type gestational diabetes mellitus models individual based hiv spread in papua new guinea multi scale immune viral transmission model concluding remarks slides adapted from external source redacted from public pdf for copyright reasons common misconceptions about causality focus on single events focus on proximal causes close in time and space focus on one way chains of cause and effect assume unchanging strength of cause effect links such narrow framing of issues overlooks the importance of cumulative effects delays feedback loops and nonlinearities richmond b peterson s high performance systems inc an introduction to systems thinking hanover nh high performance systems adapted from homer milstein complex system characteristics feedbacks nonlinearities delays path dependence lock in behavior a result of internal structure the enemy is us result emergent behavior whole greater than the sum of its parts department of computer science common phenomena in complex systems snowballing when things go bad they often go very bad very quickly vicious cycles lead to cascading of problems path dependence different starting points can lead to divergence in project progress lock in policy resistance situation can be unexpectedly difficult to change slides adapted from external source redacted from public pdf for copyright reasons deleterious feedbacks cutting cigarette tar levels reduces cessation cutting cigarette nicotine levels leads to compensatory smoking targeted anti tobacco interventions lead to equally targeted coupon programs by tobacco industry charging for supplies for diabetics leads to higher overall costs by increases costs due to reduced self management faster disease progression arvs prolong lives of hiv carriers but lead to resurgent hiv epidemic due to lower risk perception saving money by understaffing sti clinics leads to long treatment wait greater risk of transmission by infectives bigger epidemics antibiotic overuse worsens pathogen resistance antilock breaks lead to more risky driving natural feedback intergenerational vicious cycles complexity of systems dynamics dynamic problems are harder than static problems there are time delays involved between causes and effects between actions and reactions feedback the problem is further complicated when dynamics are created by operation of feedback loops it means that which way the system will move is not easily predictable the evolution path unfolds gradually and continuously determines its own path into the future path dependent dynamics non linearity most system dynamics problems are non linear this means that the cause effect relations between variables are not proportional non linear effects are subtle because a certain effect observed in a one range may not be valid at all in another range non linearity furthermore often means that there are interaction effects between variables cause and effect separated in time and space in a non linear dynamic feedback model with several variables the cause effect relations become detached in time and space scale as the number of variables increases the complexity of the problem increases nonlinearly even small size policy problems involve tens of variables at this scale a non linear feedback problem immediately becomes impossibly hard to track analytically and intuitively human dimension typical system dynamics problems involve human actors so we must model not only the physics of the system including information flows but also how people react to situations make decisions set goals make plans etc adapted from barlas principles laws of systems principle meaningful macro behavior emerging from the interactions of micro components the macro dynamics is not built into the behavior of individual components nor is it obviously predictable from the action rules of these agents dividing an elephant in two does not produce two small elephants principle counter intuitive nature of systems we human beings are naturally equipped only to deal with cause effect relation close in time and space the baby touches the stove with his index finger his index finger burns and it burns now and he learns our intuitive ability is further impeded by delays errors omissions and bias in data information that we use in real life systems may exhibit better before worse dynamics or vice versa principle systemic misperceptions biases and omissions are typical in decision making in a dynamic feedback environment experiments show that we are poor decision makers in dynamic non linear feedback environments our intuitive time and space constrained notion of causality cannot cope with systemic complexities we ignore distort or misperceive feedbacks time delays and non linearities in making decisions yesterday solutions can be today problems principle learning by experience is difficult and flawed in complex systems perhaps the most critical of all learning is not natural intuitive in complex dynamic environments experimental evidence shows that with our reductionist intuition of causality we make incomplete or plain wrong causal inferences about effectiveness of actions decisions there is no enemy out there faster is slower barlas slides adapted from external source redacted from public pdf for copyright reasons linked communities of scholars research biomathematics complex systems system dynamics operations research public health informatics department of computer science slides adapted from external source redacted from public pdf for copyright reasons outline motivation model vignettes aggregate infectious disease models broadening classic compartment model formulations emerging type gestational diabetes mellitus model individual based hiv spread in papua new guinea multi scale immune viral transmission model feedbacks susceptibles contacts of susceptibles with infectives new infections infectives simple sit model mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible incidence rate total population prevalence recovery delay initial population new infections new recovery new illness newly susceptible immunity loss delay department of computer science stocks state variables over time person person person person person person person person person person person person person person person time months s alternative hc workers exogenous recovery delay person i alternative hc workers exogenous recovery delay person t alternative hc workers exogenous recovery delay person broadening the model boundaries endogenous recovery delay susceptibles contacts of susceptibles with n ew infections infectives infectives people presenting for treatment waiting times health care staff broadening the model boundaries endogenous recovery delay mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible total population staff time per patient healthcare workers time long term healthcare workers initial population incidence rate prevalence recovery delay time until seek treatment new infections new recovery new illness newly susceptible immunity loss delay department of computer science prevalence implications prevalence time day prevalence baseline hc workers prevalence alternative hc workers prevalence alternative hc workers prevalence alternative hc workers prevalence alternative hc workers how does count of health care workers affect treatment delay recovery delay 1125 time day recovery delay baseline hc workers day recovery delay alternative hc workers day recovery delay alternative hc workers day recovery delay alternative hc workers day recovery delay alternative hc workers day oops late hiring of hc workers prevalence 1125 1250 1625 2125 time day prevalence baseline hc workers prevalence alternative hc workers late prevalence alternative hc workers late prevalence alternative hc workers late prevalence alternative hc workers late prevalence alternative hc workers late lock in effect path dependence investing early in hc workers small prevalence fewer hc workers needed to maintain low prevalence limited of hc workers high prevalence more hc workers needed to achieve low prevalence department of computer science susceptible population average incubation time average duration of illness s b depletion infection rate r emergence rate removal rate contagion r infectivity total infectious contacts contact rates contagion seir model vs data taiwan cumulative cases no behavioral response ctua time day insert source expanding the boundary behavioral feedbacks susceptible population average incubation time average duration of illness s b depletion infection rate r emergence rate removal rate contagion r total infectious contagion infectivity contacts contact rates safer b social distancing social distancing b hygiene media attention public health warnings practices model vs data with behavioral feedback cumulative cases ctual mo time day overall model structure birth rate normal and pregnancies to additional stratifying age normal weight deaths underweight weight pregnancies of non overweight women completion of pregnancy to non overweight state pregnancy duration shedding obesity non overweight mother developing gdm pregnant women developing persistent sex in uteroexposure developing obesity overweight obesity normal weight individuals developing weight babies born to gnant normal weight mothers completion of pregnancy to overweight state pregnancies to overweight mother developing gdm pregnancies developing gdm from mother with gdm history completion of gdm pregnancy born from ther with dm oveweight babies born from mothers pregnancies of overweioghvterweight women deaths overweight individuals developing deaths pregnant women with gdm that continue on to postpartum women with history of pregnancies for women with gdm completion of pregnancy for mother with new pregnancies from mother with gdm developing mpletion of non gdm gnancy for woman with history of gdm pregnant with deaths from non women with history of gdm outline motivation model vignettes aggregate infectious disease models broadening classic compartment model formulations emerging type gestational diabetes mellitus model individual based hiv spread in papua new guinea multi scale immune viral transmission model complicating factors co morbidities influences of parents peers on risks heterogeneity department of computer science slides adapted from external source redacted from public pdf for copyright reasons contrasting model granularity normal weight babies born to overweight mothers without gdm normal weight babies born to mothers without gdm normal weight babies born from pregnancy normal weight deaths developing obesity normal weight individuals developing normal weight babies born from non gdm mother with history of gdm normal weight babies born from gdm pregnancy birth rate pregnancies of non overweight women pregnancies to non overweight mother developing gdm pregnant women developing persistent overweight obesity pregnancies to overweight pregnancies developing gdm from mother with gdm history overweight babies born overweight babies born to completion of mother developing gdm om pregnant overweight pregnant normal weight pregnancy to mothers mothers overweight state completion of gdm pregnancy babies born overweight babies born from o born pregnancies of pregnancy non gdm mother with f history of gdm thers overweioghvterweight women deaths overwe developing deaths pregnant women with gdm that continue on to postpartum women with history of pregnancies for women with gdm completion of pregnancy for mother with new pregnancies from mother with gdm developing mpletion of non gdm gnancy for woman with history of gdm deaths from non women with history of gdm inevitable tradeoffs high scope breadth of boundary practical constraints data time cost transparency low limited value high low aggregation slide adapted from geoff mcdonnell contrasting benefits aggregate models individual based models easier construction calibration parameterization analysis understanding performance lower baseline cost population size invariance less pronounced stochastics less frequent need for monte carlo ensembles quicker construction runtime more time for understanding refining examining finer grained consequences network spread e g transfer effects w i pop learning bounded ration fidelity to some dynamics support for highly targeted policy planning better heterogeneity flexibility simpler description of some causal mechanisms better understanding by some clients slides adapted from external source redacted from public pdf for copyright reasons areas of advantage of individual examining finer grained consequences network spread transfer effects within population detailed spatial dynamics effects of population heterogeneity effects of highly targeted policies local risk perception effects of individual level synergies e g multiple risk factors simple individual based description of causality sufficient individual level distributional data are available for policy modeling beyond exploratory models pri cl icn b c n hl r jds tor h tn mrh o pa clltl te bip de l i n rh th t le dgtm ijf cjujibll in te i ln te i ln te i ln te i ln te i ln te i m aeim isr dvo te t e outline motivation model vignettes aggregate infectious disease models broadening classic compartment model formulations emerging type gestational diabetes mellitus model individual based hiv spread in papua new guinea multi scale immune viral transmission model slides adapted from external source redacted from public pdf for copyright reasons multi scale model conceptual model network structure dynamics continuous condition state model presently being generalized parameterized for specific disease w b sahai department of computer science individual model structure population size person mean viral load virion production irion production rate if on quantized infection virion clearance virion production rate per contact virions rate mean of viral load of neighbors uninfected cell replentishment rate likelihood density of infection by single virion from infected cells mean virion lifetime per infected cellvirion uninfected cell new cell infected cell death production rate replentishment uninfected cell infections infected cell by ctls rate which infected cel population size mean uninfected cells death mean uninfected cell lifetime death mean infected cell lifetime mean infected cells are killed by ctls population size ctl mean ctl responsiveness immune response to infected cells ctl turnover lifespan governing equations for individual x i xi d vi y i xi vi yi a pzi v i kyi uvi z i ci yi zi hzi p rodnction rate of nnin fect ed cells d rate of nnin fe cted cell die off j rate in fe ct ed cells are prodneed from cells d a j o l d a y virion day fected cells a n d free viru a in fect ed cell death r at e dne t o ru d a p r at e th at in fe cted cells are killed b y ctle c ell h rate of ctle d ie off k r at e at w fr e e v irions are prodneed l cells d a d a vi rim cell d a y frnm e ct ed cell death u ir al deca rate network embedded individuals outline motivation basic systems modeling concepts model vignettes aggregate emerging type gestational diabetes mellitus model individual based hiv spread in papua new guinea multi scale immune viral transmission model concluding remarks concluding remarks simulation modeling complements existing methodologies for insight into health issues different simulation modeling approaches offer insight into different aspects of obesity challenge model design is specific to questions context models require significant investment to build provide ongoing insight department of computer science individual and agent based models introduction tradeoffs tools nathaniel osgood january complementary model types static models models help us understand connections between system components but don t explicitly represent time aid reasoning about structure of system dynamic models aid in understanding dynamic implications consequences over time of system structure choices social network analysis understanding structural relationship between parties understanding how network position influences patterns of health identifying highly influential or critical parties an important enabler for and synergizer with dynamic modeling dynamic extensions are possible dynamic models simulation models represent hypothesized causal relationships between diverse factors models provide a provide a way to examine diverse consequences of changes in one area of the system to the whole system models help us and system actors to understand system vulnerabilities leverage points ways of fruitfully changing system structure improved ways of working together analogy other simulators to improve performance lower risk pilot decision making flight simulators climate policy climate simulators process power plants plant simulators driver training vehicular simulators street design traffic flow regulation traffic simulators construction coordination construction process simulators regularities arise from underlying processes the time series shown are tightly interrelated not independent many of the features of the time series are driven by the same underlying processes natural history of infection demographic change of the population mechanisms of infection transmission risk behaviour risk perception health system response simulation seeks insight from characterizing causal structure of those processes department of computer science understanding intervention impact seek to understand causal relationships progression of infection immunity response to treatment mixing patterns e g between communities intergenerational social network mediated effects role modelling behavior change emulation diversity in contact rates strain interaction diversity in symptoms simulation models some uses make explicit mental models of causality for discussion and collective refinement assist in management of complex situations serve as what if tool for identifying desirable policies understand trends help make sense of interaction of diverse information processes prioritizing research data collection identifying inconsistencies understanding commonalities between contexts infection spread communication e g learning labs simulation models as dynamic hypotheses explaining drivers for trends or anticipating intervention impact requires understanding processes underlying observables a model represents a hypothesis regarding the possible causal interaction of diverse factors often studied in isolation operationally captures a hypothesis for how the system works at certain level of description model parameters detailed assumptions for particular epidemiological contexts coevolution observations evaluation external world actions choice of observations mental model birth rate normal weight deaths developing obesity pregnancies of non overweight women completion of pregnancy to non overweight state pregnancy duration shedding obesity pregnancies to non overweight mother developing gdm pregnant women developing persistent overweight obesity born from o normal weight individuals developing born completion of pregnancy to overweight state pregnancies of pregnancies to overweight mother developing gdm pregnancies developing gdm from mother with gdm history completion of gdm pregnancy ther with f dm thers overweioghvterweight women deaths overwe developing deaths pregnant women with gdm that continue on to postpartum women with history of pregnancies for women with gdm completion of pregnancy for mother with new pregnancies from mother with gdm developing mpletion of non gdm gnancy for woman with history of gdm simulated dynamics deaths from non women with history of gdm scenarios for understanding how does x affect system policy formulation evaluation model can be used to investigate scenarios fractional prevalence time day fractional prevalence alternative sir fractional prevalence baseline sir examples of dynamic modeling approaches system dynamics modeling feedback centric modeling approach focuses on feedbacks accumulations spans qualitative quantitative methods supports rich mathematical analysis interactive model runs agent based modeling captures interactions between individuals within populations captures individual histories trajectories gracefully represents network connections easier capturing of heterogeneity detailed policy planning discrete event simulation simulates flow of individuals through processes captures resource use dynamic models for health classic aggregate models differential equations population classified into or more state variables according to attributes state variables parameters population recent individual based models governing equations approach varies each individual evolves state variables parameters population contrasting model granularity birth and death rate time random seed mortality rate birth rate births contacts per susceptible per contact risk of infection force of infection incidence fractional prevalence mean time with disease recovery population initial population initial fraction vaccination susceptible mortality mortality rate infective mortality recovered mortality mortality rate mortality rate vaccinated q annual likelihood of vaccination susceptible fraction vaccinated mortality time of population mortality rate key needs motivating individual based modeling need to calibrate against information on agent history need to capture progression of agents along multiple pathways e g co morbidities wish to characterize learning by and or memory of agents based on experience or strong history dependence in agents need to capture distinct localized perception among agents seeking to intervene at points in change behavior on explain phenomena over or explain dynamics across networks seek distinct interventions for many heterogenous categories need to capture impact of intervention across many categories when it is much simpler to describe behavior at indiv level seek flexibility in exploring different heterogeneity dimensions needs of stakeholders to engage with individual based models want to describe behaviour at multiple scales agent based modeling we can capture individuals in many ways i view agent based models abm as a type of individual based modeling that encapsulates a given individual as a software object with methods properties objects provide a convenient abstraction for individuals agent based models currently require writing at least some code in programming languages we can formulate sd models w i agent based tools i view such models as simultaneously sd abm we can follow an sd process to build use agent based models agent based systems agent based model characteristics one or more populations composed of individual agents each agent is associated with some of the following state continuous or discrete e g age health smoking status networks beliefs parameters e g gender genetic composition preference fn rules for interaction traditionally specified in general purpose programming language embedded in an environment typically with localized perception communicate via messaging and or flows environment emergent aggregate behavior elements of individual state example discrete ethnicity gender categorical infection status continuous age elements of body composition metabolic rate past exposure to environmental factors glycemic level example of continuous individual state example of discrete states binary presence in discrete state parallel state transition diagrams a person is in some particular state with respect to each of these condition specific state transition diagrams this requires representing combinations of possibilities in an aggregate model interacting individuals age smoker never aging never never smokers age doctor initiation smokers initiation smoker never age aging current smokers smoker current cessation relapse cessation aging former smokers relapse friends age smoker current friends age 01 smoker current parent age smoker former network embedded individuals irregular spatial embedding example of emergence waves in regular spatial embedding network embedded individuals emergence interaction of very simple components can lead to surprising emergent dynamic patterns in the behaviour of a given component over time the patterns that are seen are quite different than what would be expected through any single component of the system these often relate to variables in the underlying system in complex ways observation change what we observe most directly in the world are emergent properties of the system changes to these emergent patterns must be accomplished by changing the underlying system the emergent behaviours can change significantly with changes in model structure parameter values models help with understanding how the emergent patterns reflect the characteristics of the underlying system changes in the underlying causal factors yield changes in the results emergent behaviour modeling types we see emergent behaviour in both system dynamics and agent based models agent based models especially in interaction of multiple agents higher level patterns of behaviour system dynamics especially in interaction of multiple stocks flows agent based modeling particularly emphasizes multi level emergence how distinctive patterns can emerge at different levels of the system ability to look at high level emergence reflects the presence of many individual agents within one model the emergent behaviours can change significantly with changes in model structure parameter values feedbacks some aggregate feedbacks lie within individual agent new nicotine exposure likelihood of smoking severity of nicotine addiction likelihood of johnny smoking johnny smoking severity of addiction feedbacks many aggregate feedbacks are between agents new smokers aggregate model johnny father smoking johnny perception of desirability of smoking perceived desirability of smoking agent based model timmy perception of desirability johnny smoking of smoking limitations of agent based methods inability to mathematically analyze generalize behaviour beyond a single run long ensemble run times lack of interactivity opportunity cost calibrating yields no unique interpretation requirement of some software engineering in building modifying achieving model transparency is difficulty lack of defined modeling process or qualitative modeling artifacts hands on model use ahead first glimpse of agent based modeling sir agent based is located here load model sir agent based from anylogic sample models via help menu viewing the model structure double click on person to see the associated state transition diagram run the model via the run button press this button to start model execution example of emergent behaviour make sure model time is visible if no model time is visible on the bottom of the window press this button to add a model time output select model time here so a check mark appears if a checkmark is already present just click back on the output window the updated window should include a model time output stylized measurement how long does it take for the infection to reach the top or left boundaries we ll compare this to the situation with other parameter assumptions press this button to stop model execution right click here to bring up the select copy from the menu right click here to bring up the menu select paste from the menu to pas new experiment a copy of the existi your screen should look as follows eil e d it liew mod e l windo w l e lp t e hl t i al x q q eiil o t i i co n get s uppo rt g p ro ject ll sea rc h el q pe rson ll el i pa lett e ll l f i t a i t i ef mod el m a in pa ra m eters avera gellln essdu rat io n conta ct rat e inf ecti o n pr o ba bility tota lpo pulat i o n q plain va ria ble enviro nm ent a emb e d d e d objects l i presentat i o n perso n simu lat i o n m a in simu lat i o nl m a in l p robl e ms ll descript i o n lor at i o n e co lo r re covery reco vered ill pa ra met er fl o w aux var ia ble d st o c k va ria bl e if event dyn a m i c event p lain va ria bl e coll e ct io n va ria ble g fu nct i o n ta ble fun cti o n l port co nn ector ent ry po int q st at e tra n sit i o n in it iai st at e p o int e r bran c h his t o ry stat e fina l st at e prop e rti e ll co nso le el m enviro nm ent act io n i i an alysis l i presen tatio n lo co nn ect ivity ff ent e rprise libra ry mo re libra ries it ems sele cted changing the name of the experiment select here the new experiment so we can edit its properties characteristics select the general tab type the name slowrecovery for the new experiment select the parameters tab make the illness duration run the model via the run button be sure to select the slowrecovery experiment for running you should see somethin like this sir age nt based simu lat io n anylogic adva nce d ed ucatio na l use only dd iii i i et suppo rt susceptible infectious recovered l m el i pa lett e l m od el e pa ra met er fl o w aux var ia ble d stoc k va ria bl e if event dyn a m i c event p lain va ria bl e co ll e ct io n va ria ble g fu nct i o n i ta ble fun cti o n l p o rt a co nn ector ent ry po int q st at e tra n sit i o n in it ia i st at e po int e r bra n c h histo ry stat e final st at e environ ment es anyl og ic j re bin j avaw exe j u l pm run running i time i st ep g i i eps i fps i men ac t io n i i an alysis pr esenlatio n co nn ect ivity ty en t e rprise libra ry m o re libra ries adding a transition select transition on the palette so we can add a transition to the state chart connecting the two states while holding down the mouse button drag the mouse to here and only then release the mouse button click here first to start the transition give the transition a name type the name waningimmunity here setting the duration until immunity wanes make sure this is set to timeout set the waning time to use the run button and select the original experiment after starting the model you should see something like this what happens as time progresses what happens as time progresses use the run button select the slowrecovery experiment slow recovery results this time only a few scattered yellow susceptible individuals are visible as time progresses little internal structure why stylized measurement how long does it take for the infection to reach the top or left boundaries how does this compare with the earlier experiment with a shorter duration of immunity bonus question what would an aggregate random mixing model have predicted project possibilities gonorrhea cwd tb diabetes esrd hpv smoking mrsa smoking overview of key anylogic infrastructure modeling types and interface nathaniel osgood cmpt january announcements links to installers distributed department machines requisitioned tutorial time feedback sought project ideas hpv cervical cancer smoking age ethnicity vac cination adapting existing aggregate model contact karen yee returns monday elearning system facilitating learning by tapping into social networks mrsa nosocomial institutional spread tie in with institutional flow models gonorrhea mutation drug resistance sexual networks chronic wasting disease movement of animals over landscape seasonal changes in behaviour contact cheryl waldner tb contact ndo assaad al azem work with tb control project ideas gestational type diabetes intergenerational effects of diabetes in the mother spread network micro data aggregate data contact cheryl waldner simon kapaj lifecourse weight trajectories year longitudinal data national childhood development survey physiological model of weight body composition interaction with food intake physical activity anylogic basics multi platform declarative graphical languages basic language java rich library of built in objects continuous or discrete time space modeling approaches supported system dynamics agent based regular irregular spatial embedding network embedding discrete event system dynamics feedback focus traditional graphical depict stocks state of system flows rates of change to the continuous variation in state stocks are initialized are then change according to flows values of flows are determined by stocks any other variables hands on model use ahead load model sir agent based alp agent based approaches agent actor focused traditional graphical depiction state transition diagram states transitions discrete variation in state regular or irregular topologies between agents messages sent via connections hands on model use ahead load model emergency department tulsa alp discrete event modeling resource based modeling queues processes flow charts capacitated resource pools send to attachment detachment network modeling irregular spatial embedding r t alloadofneighbors vir on pro du c t ion i d vironsclearance ld i i i u ivirionlnjection naturallnfectedcelldeath d a efficient y lecolor infection olor newlnfection p uneresponsiveness settingcolor g console person active object class gener ali name igno r e network embedded individuals regular spatial embedding hybrid models much of the power of anylogic lies in its ability to integrate multiple types of modeling in a single model attractive schemes agent based using system dynamics for continuous agent state c f age system dynamics using agent based to determine flows agent based using system dynamics for global dynamics agents entering into process based health services example hybrid model advantages of anylogic as compared to other agent based modeling software primarily declarative specification less code great flexibility access to java libraries support for multiple modeling types support for mixture of modeling types painful sides of anylogic education advanced export of model results lack of trajectory files lack of debugger need for bits of java code many pieces of system using the lens of systems science to understand population health implications of the diabetic uterine environment nathaniel osgood joint work with roland dyck winfried grassmann department of computer science associate community health epidemiology and school of public health university of saskatchewan motivation talk outline research questions the gdm model structure parameterization calibration findings conclusions department of computer science slides adapted from external source redacted from public pdf for copyright reasons saskatchewan standardized prevalence rates figure adapted from dyck r osgood n lin t h gao a stang m r the epidemiology of diabetes in saskatchewan adults from a comparison of first nations people and other saskatchewan residents canadian medical association journal in press incident cases of non first nations osk females figure adapted from dyck r osgood n lin t h gao a stang m r epidemiology of diabetes mellitus among first nations and non first nations adults canadian medical association journal incident cases of first nations skfn females figure adapted from dyck r osgood n lin t h gao a stang m r epidemiology of diabetes mellitus among first nations and non first nations adults canadian medical association journal diabetes arises at different points in the lifecourse other residents predominantly caucasian first nations population predominantly caucasian figures adapted from dyck r osgood n lin t h gao a stang m r the epidemiology of diabetes in saskatchewan adults from a comparison of first nations people and other saskatchewan residents canadian medical association journal in press diabetes in the lifecourse age specific incidence rates prevalence rates non fn females figure adapted from dyck r osgood n lin t h gao a stang m r epidemiology of diabetes mellitus among first nations and non first nations adults canadian medical association journal prevalence rates fn females figure adapted from dyck r osgood n lin t h gao a stang m r epidemiology of diabetes mellitus among first nations and non first nations adults canadian medical association journal prevalence rates fn males figure adapted from dyck r osgood n lin t h gao a stang m r epidemiology of diabetes mellitus among first nations and non first nations adults canadian medical association journal talk outline findings on sk diabetes epidemiology gestational diabetes background risks our research efforts the gdm model preliminary results conclusions department of computer science epidemiological patterns rising gdm prevalence as bellweather for rise in prevalence substantially higher rates of and overweight obesity amongst women association between in utero exposure to dm infant macrosomia childhood obesity elevated risk plateauing at very high obesity gdm prevalence gestational diabetes mellitus gdm gestational diabetes is a form of diabetes that first manifests during pregnancy approximately of pregnancies in sk as a whole are accompanied by gdm of women with gdm continue continue on to type diabetes mellitus immediately following pregnancy subsequently of cases of gdm continue on annually to type diabetes mother gdm as a risk factor subsequent gdm rr incidence rate year after gdm child macrosomia obesity rr or likelihood of by pregnancy likely considerably higher gdm effects may be much stronger in aboriginal people slides adapted from external source redacted from public pdf for copyright reasons gdm macrosomia gestational diabetes significantly elevates risk of macrosomia this risk appears to be significantly higher amongst aboriginal peoples macrosomia is an important risk factor for overweight obesity later in life department of computer science maternal bmi gdm risk pregravid bmi kg department of computer science dyck r f prevalence rates of and obesity by aboriginal community obesity dyck r f pregnancies risk of recurrent gdm in mother risk of in mother risk of macrosomia risk of obesity in offspring risk in offspring pregnancies intragenerational ef cts risk of recurrent gdm in mother risk of in mother risk of macrosomia risk of obesity in offspring intergenerational effects risk in offspring research questions is the hypothesized intergenerational driver consistent with the historic growth in obesity gdm how much of the rise of might be due to gdm how does the magnitude of the impact of gdm vary by ethnic sex group how much of the impact of gdm is mediated via intra vs inter generational effects why gdm contribution to diabetes burden is difficult to estimate diverse pathways intergenerational via macrosomia offspring overweight obesity epigenetic effects intragenerational direct via recurrent maternal gdm long time delays diverse mediators moderators fertility rates age risk factors dynamics e g weight recall simulation models as dynamic hypotheses explaining drivers for trends or anticipating intervention impact requires understanding processes underlying observables a model represents a hypothesis regarding the possible causal interaction of diverse factors often studied in isolation operationally captures a hypothesis for how the system works at certain description level model parameters specify detailed assumptions for particular epidemiological contexts coevolution observations evaluation external world actions choice of observations mental model birth rate normal weight deaths developing obesity pregnancies of non overweight women completion of pregnancy to non overweight state pregnancy duration shedding obesity pregnancies to non overweight mother developing gdm pregnant women developing persistent overweight obesity born from o normal weight individuals developing born completion of pregnancy to overweight state pregnancies of pregnancies to overweight mother developing gdm pregnancies developing gdm from mother with gdm history completion of gdm pregnancy ther with f dm thers overweioghvterweight women deaths overwe developing deaths pregnant women with gdm that continue on to postpartum women with history of pregnancies for women with gdm completion of pregnancy for mother with new pregnancies from mother with gdm developing mpletion of non gdm gnancy for woman with history of gdm simulated dynamics deaths from non women with history of gdm simulation models some uses make explicit mental models of causality for discussion and collective refinement assist in management of complex situations help make sense of interaction of diverse information processes serve as what if tool for identifying desirable policies cost effective high leverage robust prioritizing research data collection identifying inconsistencies between dynamic hypotheses and observables communication e g learning labs what simulation models are not crystal balls perfect representation of real system dependent upon complete data replacements for traditional e g epidemiological biostatistical analyses black boxes for decision making department of computer science talk outline research questions approach the gdm model structure parameterization calibration sensitivity analysis findings conclusions department of computer science some tools of system science system dynamics agent based modeling microsimulation social network analysis dynamical systems theory complexity theory control theory projects applying other methods may be seen at the system science poster session high level gdm model structure birth rate normal and pregnancies to normal weight deaths underweight weight pregnancies of non overweight women completion of pregnancy to non overweight state pregnancy duration shedding obesity non overweight mother developing gdm pregnant women developing persistent developing obesity overweight obesity normal weight individuals developing weight babies born to gnant normal weight mothers completion of pregnancy to overweight state pregnancies to overweight mother developing gdm pregnancies developing gdm from mother with gdm history completion of gdm pregnancy born from ther with dm oveweight babies born from mothers pregnancies of overweioghvterweight women deaths overweight individuals developing deaths pregnant women with gdm that continue on to postpartum women with history of pregnancies for women with gdm completion of pregnancy for mother with new pregnancies from mother with gdm developing mpletion of non gdm gnancy for woman with history of gdm pregnant with deaths from non women with history of gdm model scope saskatchewan population weight change demographics development of women pregnancy development of gdm recurrence of gdm development of from gdm history births deaths migration bill c status reclassification additional gdm model characteristics trended exogenous risk stratification age year age categories through age sex ethnicity first nations skfn non first nations osk in utero exposure normoglycemic population overweight births macrosomia time horizon with exceptions time step months talk outline research questions approach the gdm model structure parameterization calibration sensitivity analysis findings conclusions department of computer science model parameter estimation direct estimation primary clinical survey data saskatchewan health administrative databases secondary literature calibration less easily recognizable parameters model structure specific parameters department of computer science saskatchewan health administrative diabetes data use of validated algorithm for identifying cases used for model incident cases prevalent cases deaths department of computer science data sources demographics births age specific fertility rates osk sask vital stats skfn health canada vital stats of the skfn population of sk deaths death rates osk sask vital stats skfn sask vital stats health canada vital stats of the skfn initial breakdown skfn inac osk sask vital statistics bill c effects vital stats of the skfn population of sk clatworthy services canada migration osk sask vital stats skfn health canada vital stats of the skfn population of sk department of computer scienpce opulation of sk data sources weight change pregnancy related risks weight gain during pregnancy gunderson abrams et al birth weightlink with maternal status primary data collected for dyck klomp et al obesity risk skfn bruner chad dyck gdm risks initial preliminary data collected for dyck klomp et al recurrence kim berger et al reeder cchs department of computer science data sources risks following history of gdm feig et al no history age sex ethnicity specific administrative data hazard rate ratio of ow ob field et al in utero exposure franks et al department of computer science talk outline research questions approach the gdm model structure parameterization calibration sensitivity analysis findings conclusions department of computer science calibration an analytic triangulation approach formulate initial model as dynamic hypothesis parameterize models from local data where possible secondary literature calibrate remaining parameters to simultaneously best match diverse historic time series data points example of calibration points from our esrd work example calibration constraints birth rate normal and pregnancies to normal weight deaths underweight weight pregnancies of non overweight women completion of pregnancy to non overweight state pregnancy duration shedding obesity non overweight mother developing gdm pregnant women developing persistent developing obesity overweight obesity normal weight individuals developing weight babies born to gnant normal weight mothers completion of pregnancy to overweight state pregnancies to overweight mother developing gdm pregnancies developing gdm from mother with gdm history completion of gdm pregnancy born from ther with dm oveweight babies born from mothers pregnancies of overweioghvterweight women deaths overweight individuals developing deaths pregnant women with gdm that continue on to postpartum women with history of pregnancies for women with gdm completion of pregnancy for mother with new pregnancies from mother with gdm developing mpletion of non gdm gnancy for woman with history of gdm pregnant with deaths from non women with history of gdm 450 time year hi t icr l ti dm p v e f c t irr by e l c ry g p j p opr ht ior e p i c li di t xitic by e l c ry g p j p opr ht ior e p i c t ior ti dm ri k hi t icr l ti dm p v e f c t irr by e l c ryfrij popr b ior e p i c li di t xitic by e l c ryfrij popr b ior e p i c t ior v ti dm r i k s t time year hi t ic ti dm l cic f c t irr by e ic ty c p p opu b tior e p i c rior tl l ci e f c of ti dm g p p opr l rior e i c l t ior ti dm ri k s t s t v i t hi t ic ti dm l cic f c t irr by e ic ty fri popr l rior e i c l t ioc v tl d l ci e f c of ti dm rij p opu b tior e p i c rior ti dm r i k s r pr v i t time year hi t icr l t od popr b ior si t irr by e l c ry g p j t od p opr b ior by e l c ry g p j p opr ht ior e p i c t ior hi t icr l t od popr b ior si t irr by e l c ryfrij popr k t od p opr b ior by e i c ryfrij popr b ior e p c rior v esrd prevalence esrd incidence esrd death time year time year time year hi r iee l e srd pf f c t irr by e i ci ry c p popr k rior e p i c ri hi r icll e srd l d c rc t irr by e i ci ry c p popr b ior e i c rior hi r icll e srd d rh t irr by e i c i ry c p popr k rior e p i incorporating calibration results compare quality of calibrated models use cross validation to test model predictions strongly question models lacking consistency with historic data or predictive ability use models with closest calibrations as best guesses concerning drivers for observable epidemiologic trends underlying epidemiology of infection use variance sensitivity in calibrated values to prioritize data collection calibration against time series incident cases age sex ethnicity prevalent cases age sex ethnicity sex ethnicity deaths gdm rates by ethnicity total population size by ethnicity by age sex ethnicity historic deaths ethnicity age ethnicity age sex ethnicity macrosomia levels ethnicity weight skfn age sex all age calibration against time series incident cases age sex ethnicity prevalent cases deaths gdm rates by ethnicity total population size by ethnicity by age sex ethnicity overweight rates by ethnicity sex general pop sex overall historic deaths ethnicity age ethnicity age sex ethnicity macrosomia levels by ethnicity department of computer science an example of some calibration matches female postreproductive skfn calibration results prevalent cases males females model historic data model historic data figure adapted from osgood n dyck r grassmann w the inter and intra generational impact of gedsteaptaiortnmaelndt ioafbceotemspountetrhseceiepnicdeemic of type diabetes submitted to american journal of public health october key uncertainty rate of amongst gdm survivors calibration is tightest when using shared skfn osk on low side of empirical observations in caucasians below rates in past studies of aboriginal people high risk of underestimation calibration with a higher assumed rate leads to higher attribution of rise to gdm the quality of the calibration is sensitive to this parameter incidence following gdm conservative assumption figure adapted from kim c newton k m knopp r h gestational diabetes and the incidence of type diabetes a systematic review diabetes care calibration findings model calibrates adequately multiple calibrations appear to yield consistent picture calibration places important and verifiable constraints on certain less well known parameters cross calibration the model reproduces the trends in other time series not used in parameterization calibration talk outline research questions approach the gdm model structure parameterization calibration sensitivity analysis findings conclusions department of computer science p revalent cases by sex ethnicity p revalent cases by sex ethnicity time year t dm dy s crl btlm icity f l c miei f l mi l b ti l time year t d i dy s ff lil eltt k i ty f ffi l gi f v vl bi ef li fl hi te ic t l dm fl if efi t c c s b li k ii t in f i a i gi r v l vl b c hi t ft v if f c dy s eltt i ici ty ii t i ra f mje l i f je l v b l pre valent cases by sex ethnicity p re alent cases by sex ethnicity ooo time year t dm by s f lc rl eltt i di ty f if o r i ei r l v l b ti l his ol i c t l d m fl vl if r l c o s btlm ici ty i r t i fl c f l no r jr i gi f l vl b c 1996 time i ear f dy s rn d ett k i ty fffi if no rc il b ei r l vl b lik hi tlmi c t d f pl l t c dy s btlmicity t in fffi h o ilil t mi gi r v v v l structural sensitivity analysis trending vs no trending prevalent cases male female skfn osk department of computer science scenarios depicted here highly conservative calibration baseline standard calibrated model no intergenerational gdm effect no elevation in risk of offspring from mother gdm no intra or inter generational effect no effects of gdm department of computer science f ractiona l p revalence of i time ear 1996 l fure rg re tio r t to of gd i s o ri csh i to f i le gdi crude prevalence skfn fractional p revalence of td ti 08 06 04 02 d r time ear 1996 cumulative cases osk figure adapted from osgood n dyck r grassmann w the inter and intra generational impact of gestational diabetes on the epidemic of type diabetes submitted to american journal of public health october cumulative cases skfn figure adapted from osgood n dyck r grassmann w the inter and intra generational impact of gestational diabetes on the epidemic of type diabetes submitted to american journal of public health october inter vs intra generational effects inter generational effects are significant but more distal a generation down the road occur more in a higher birth rate context development recurrence of gdm are masked by high numbers of other births these impacts grow significantly over time intra generational impacts are also pronounced and short term department of computer science intragenerational exposure cumulative skfn cases preceded by gdm department of computer science intergenerational exposure in utero exposure department of computer science intergenerational exposure fraction of populations with exposure cumulative cases of tv1 by e thnicity t 1956 1966 ll time ear 6 lf t ive u f t l dj f gi e l jiji di c i t i v e u f t d m gi elt lm di ty no rc dmi g i r v vl b li i o m co r a t f ile wrctil i ty no rc blm i g i r v v vl no te l g f li tio r l t f g dl i fll c f i t f ile eti l c mrmlu ivf u t rn r gi etlm no rc ig i r vlfo a o vl no gd i i o m co re f rd f ile ma il i cunnllative cases of by e thn icity vv f v i 1956 1966 6 1996 time c ie ar 2046 if tive q c oftl dm r j btl cri i i ty a xm gijif r v vl b li s oo i c tf fl ts f ih muil u l ii v e q cs i t l d u rij bt trtdi ty a oo ig i cs l v vl no e l iim cs l e ts m gd s c m c ia cis f ib b fliil j l tive q c f t d m iy bt i di ty a oo rig u i rv v vl no gdm s om oo i c tf fl t f ile b flii l l findings summary gdm very likely contributing heavily to growth in prevalence effects much larger amongst aboriginal peoples gdm raises cumulative cases by 44 the effects of gdm on are growing glycemic control in women of childbearing age has disproportionate effect on future health intragenerational intergenerational effects large key research priority rate of incidence in those with history of gdm limitations very limited health related data in early decades reliance on a few self report measures dichotomous weight categories poor overweight incidence data department of computer science talk outline research questions approach the gdm model structure parameterization calibration sensitivity analysis findings conclusions department of computer science closing thoughts gdm is not only important but prevalent readily identifiable preventable and treatable the findings here have worldwide implications rate of diabetogenesis in those with history of gdm across sk subpopulation is a priority for investigation simulation models can help complement leverage data clinical knowledge to gain insight into interventions interpret trends closing thoughts simulation models can help help understand the consequences of early experiences on later life future generations describe evaluate understand implications of dynamic hypotheses complement leverage data clinical knowledge and statistical approaches shed light on how the interactions of diverse factors lead to the observed patterns prioritize data collection interpret trends lend insight into interventions tradeoffs acknowledgements co investigators roland dyck winfried grassmann nserc discovery grant funding mary rose stang saskhealth jing bai amy yu gao department of computer science thank you available for discussion at system science poster session department of computer science basics of causal loop diagrams cmpt nathaniel osgood causal loop diagram food ingested focuses on capturing causality and especially feedback effects indicates sign of causal impact vs x y indicates x y indicates causal loop diagram an arrow with a positive sign all else remaining equal an increase decrease in the first variable increases decreases the second variable above below what it would otherwise have been an arrow with a negative sign all else remaining equal an increase decrease in the first variable decreases increases the second variable below above what it otherwise would have been reasoning about link polarity easy to get confused regarding link polarity in the context of a causal pathway tips for reasoning about x y link polarity reason about this link in isolation do not be concerned about links preceding x or following y ask if x were to increase would y increase or decrease compared to what it would otherwise have been increase in y implies decrease in y implies if answer is not clear or depends on value of x need to think about representing several paths between x and y consider a b we are reasoning here about causal influences the changes on b caused by changes in a this is not merely an associational relationship this should not merely be a matter of definition notion of increase must clearly distinguish if x were to increase would y increase or decrease compared to what it would have otherwise been if x were to increase would y increase or decrease over time i e if x were to increase would y rise or fall over time causal pathways we can reason about the influence of one variable and another variable by examining the signs along their causal pathway two negatives whether adjacent or not will act to reverse each other consider a b c an increase to a leads b to be less than it otherwise would have been b being lower than it otherwise would have been causes c to be higher than it otherwise would have been compared to what it otherwise would have been tips variables will often be noun phrases variables should be at least ordinal links should have unambiguous polarity indicate pronounced delays avoid mega diagrams label loops distinguish perceived and actual situation incorporate targets of balancing loops try to stick to planar graphs diagrams describe causal not casual factors ambiguous link ambiguous link sometimes sometimes food intake energy surplus rate of weight gain replace this by disaggregating causal pathways by showing multiple links food intake calories taken in energy surplus rate of weight gain basal metabolism calories burrt example ambiguous link sometimes sometimes overtime work accomplished per day replace this by disaggregating causal pathways by showing multiple links overtime fatigue greater incorporation of outside tasks at work efficiency work accomplished per day more time working example ambiguous link sometimes sometimes proportion of fat in foods calories ingested replace this by disaggregating causal pathways by showing multiple links diet caloric proportion of fat density calories ingested in foods satiety amount of food eaten feedback loops loops in a causal loop diagram indicate feedback in the system being represented qualitatively speaking this indicates that a given change kicks off a set of changes that cascade through other factors so as to either amplify reinforce or push back against damp balance the original change loop classification product of signs in loop best to trace through conceptually balancing loop product of signs negative reinforcing loop product of signs positive example vicious virtuous cycles positive reinforcing feedback can lead to extremely rapid changes in situation of infectives individual target weight word of mouth sal es new infections prevalence of obesity weight perceived as normal prevalence of gdm prevalence of mean weight in population customers of activated memory cells of clonal macrosomic infants expansions example balancing loops balancing loops tend to be self regulating new infections of susceptibles adaptation policy reevaluation policy effectiveness food ingested hunger mistakes learning from mistakes best practice incorporating thresholds balancing loops tend to be self regulating policy adaptation policy reevaluation policy effectiveness food ingested hunger threshold hunger to motivate eating treshold for policy dissatisfaction to lead to best practice indicating pronounced delays balancing loops tend to be self regulating policy adaptation policy reevaluation policy effectiveness food ingested threshold hunger to motivate eating treshold for policy dissatisfaction to lead to action hunger elaborating causal loops creation of nutrition and exercise programs study of obesity prevalence of obesity prevalence of gdm prevalence of macrosomic infants of infectives new infections of susceptibles classic feedbacks susceptibles contacts of susceptibles with infectives new infections infectives broadening the model boundaries example vicious virtuous cycles positive reinforcing feedback can lead to extremely rapid changes in situation existing users likelihood of cross listing and listing on search engines new users word of mouth sal es customers number of connections to music download server discovering site length of time per download likelihood of user starting multiple simultaneous downloads confusing code ease of understanding where to make a change confusing additions elaborating causal loops length of time per download number of connections to music download server likelihood of user starting multiple simultaneous downloads number of connections to music download server length of time per download users abandoning download in frustration likelihood of user starting multiple simultaneous downloads more elaborate diagrams karanfil more elaborate diagrams lavallee osgood causal loop structure dynamic implications each loop in a causal loop diagram is associated with qualitative dynamic behavior most common single loop modes of dynamic behavior exponential growth goal seeking adjustment oscillation when composed get novel behaviors due to shifting loop dominance behaviour of system more than sum of parts cl dynamics exponential growth first order reinforcing loop example word of mouth sale site popularity likelihood of cross listing and listing on search engines customers from tsai dynamic graph for stock tim e month stock current cl dynamics goal seeking balancing loop example word of pote ntial custome rs dynamic behavior mouth sale from tsai tim e month inventory current cl dynamics oscillation balancing loop with delay causal structure desire inventory invento ry producing s tarts finishing production dynamic behavior from tsai demand vs product ion tim e y ear dem and oscil tons year producing oscil tons year growth and plateau loop structure potential customers existing users likelihood of cross listing and listing on search engines reinforcing loop balancing loop word of mouth sales new users discovering site internet users yet to customers dynamic behavior from tsai 75 graph for customer discover site tim e month customer current complexities regularities department of computer science measles mumps in sk department of computer science t p iocidlcm i nc id e ncfl ct in c iden cfl example stls a i i u 1902 196 6 20r o year three stis test volume vs case tb saskatchewan war on white plague cases and contact tracing contacts examined incident cases 2020 r l l ltirii i l k i o ooi js a c ontacts traced per ase in clence rate o ocm 0001 o o io so broadening the model boundaries endogenous recovery delay common phenomena in complex systems counter intuitive behaviour often fb interactions snowballing when things go bad they often go very bad very quickly vicious cycles lead to cascading of problems due to positive feedback path dependence different starting points can lead to divergence in project progress due to positive feedback interacting w mult negative fb policy resistance situation can be unexpectedly difficult to change typically due to negative feedbacks that resist change examples of policy resistance cutting cigarette tar levels reduces cessation cutting cigarette nicotine levels leads to compensatory smoking targeted anti tobacco interventions lead to equally targeted coupon programs by tobacco industry charging for supplies for diabetics as cost cutting measure leads to higher overall costs due to reduced self management faster disease progression higher demand for dialysis transplants arvs prolong lives of hiv carriers but lead to resurgent hiv epidemic due to lower risk perception saving money by understaffing sti clinics leads to long treatment wait greater risk of transmission by infectives bigger epidemics antibiotic overuse worsens pathogen resistance antilock breaks lead to more risky driving natural feedback intergenerational vicious cycles examples of policy resistance image source larson g cutting cigarette tar levels reduces cessationthe far side series cutting cigarette nicotine levels leads to compensatory smoking targeted anti tobacco interventions lead to equally targeted coupon programs by tobacco industry charging for supplies for diabetics as cost cutting measure leads to higher overall costs due to reduced self management faster disease progression higher demand for dialysis transplants arvs prolong lives of hiv carriers but lead to resurgent hiv epidemic due to lower risk perception saving money by understaffing sti clinics leads to long treatment wait greater risk of transmission by infectives bigger epidemics antibiotic overuse worsens pathogen resistance antilock breaks lead to more risky driving natural feedback intergenerational vicious cycles slides adapted from external source redacted from public pdf for copyright reasons issues with causal loop diagrams unclear variables diagrams can become very large confusion regarding polarity non causal relationship conservation not captured behavior not always same as archetype unclear paths missing causal factors missing links asymmetry in direction of change unclear variables variables lacking clear polarity gender ethnicity shape often categorical non ordinal ask whether more x is meaningful unambiguous implicit polarity population size revenue amount of sound color more of socioeconomic status more of unclear links causal loop diagrams should make clear the causal pathway one has in mind one of the most common problems in causal loop diagrams is showing a link without the meaning being clear often there are many possible pathways and distinguishing them can help make the diagram much clearer refining a diagram it takes time to arrive at an acceptable diagram some of the biggest investments lie in figuring out the appropriate variables to use illustrating the different pathways refining the names of the variables very large diagrams still useful for getting big picture identifying where research fits in research gaps polarity a b does not mean that if a rises then b will rise over time just says that b will be higher than it would otherwise have been b may still be declining over time but is higher than it otherwise would have been a b does not mean that if a rises then b will decline over time just says that b will be lower than it would otherwise have been b may still be risingover time but is higher than it otherwise would have been reminder an arrow with a positive sign all else remaining equal an increase decrease in the first variable increases decreases the second variable above below what it would otherwise have been an arrow with a negative sign all else remaining equal an increase decrease in the first variable decreases increases the second variable below above what it otherwise would have been critical notion of increase must clearly distinguish correct interpretation if x were to increase would y increase or decrease compared to what it would have otherwise been different notion if x were to increase would y increase or decreaseover time i e if x were to increase would y rise or fall over time artifactual loop artifactual loop artifactual loop state of the system stocks levels state variables stocks levels represent accumulations these capture the state of the system mathematically we will call these state variables these can be measured at one instant in time stocks are only changed by changes to the flows into out of them there are no inputs that immediately change stocks examples of stocks water in a tub or reservoir people of different types susceptible infective immune people pregnant women women between the age of x and y high risk individuals healthcare workers medicine in stocks money in bank account in atmosphere blood sugar stored energy degree of belief in x stockpiled vaccines goods in a warehouse beds in an emergency room owned vehicles changes to state flows fluxes these are always associated with rates if these flow out of or into a stock that keeps track of things of type x the rates are measured in x unit time e g person year typically measure by accumulating people over a period of time e g incidence rates is calculated by accumulating people over a year examples of flows inflow or outflow of a bathtub litres minute rate of infection e g people month rate of recovery rate of mortality e g people year rate of births e g babies year rate of treatment people day rate of caloric consumption rate of pregnancies pregnancies month reactivation rate of tb casess reactivating per unit time revenue month spending rate month power watts rate of energy expenditure vehicle sales flows may be measured by totalling up over a period of time and dividing by the time we can ask conceptually about the rate at any given point and may change over time when speaking about rates for flows we always mean something measured as x unit time also called a rate of change per time not all things called rates are flows exchange rate rate of return key component stock flow flow stock flow flow impact on stock flow stock 90 10 90 flow current time month stock current time month impact of lowering flow rate to month stock 10 90 time month stock stock and flow alternative stock current loops stocks causation does not effect big change instantaneously loops are not instantaneous stocks only change by changes to the flows into out of them there are no inputs that immediately change stocks all causal loops must involve at least one stock delayed impact system structure diagrams semi quantitative models combine causal loops diagram elements with stock flow structure clearly distinguish stocks flows if complete all loops will go through a stock loop goes into the flow of a stock as one variable in the diagram loop comes comes out of stock as next variable in diagram slides adapted from external source redacted from public pdf for copyright reasons causal loop wrap up stocks flow nathaniel osgood osgood cs usask ca cmpt february feedbacks driving infectious disease dynamics susceptibles csounstcaecptstibblestwaenedn infectives new infections new recoveries example dynamics of sir model no births or deaths people people people people people people people people people people people people people people people sir example 170 time days susceptible population s sir example people infectious population i sir example people recovered population r sir example people shifting feedback dominance people people 000 people people people people 000 people people 000 people people people people people people 000 people sir example 170 190 time days susceptible population s sir example people infectious population i sir example people recovered population r sir example people issues with causal loop diagrams unclear variables diagrams can become very large confusion regarding polarity non causal relationship conservation not captured behavior not always same as archetype missing causal factors missing links asymmetry in direction of change unclear variables variables lacking clear polarity gender ethnicity shape often categorical non ordinal ask whether more x is meaningful unambiguous implicit polarity population size revenue amount of sound color more of socioeconomic status greater lesser very large diagrams still useful for getting big picture identifying where research fits in research gaps artifactual loop artifactual loop artifactual loop artifactual loop introduction to stocks flows nathaniel osgood osgood cs usask ca cmpt state of the system stocks levels state variables compartments stocks levels represent accumulations these capture the state of the system mathematically we will call these state variables these can be measured at one instant in time stocks start with some initial value are thereafter changed only by flows into out of them there are no inputs that immediately change stocks stocks are the source of delay in a system in a stock flow diagram shown as rectangles examples of stocks water in a tub or reservoir people of different types susceptible infective immune people pregnant women women between the age of x and y high risk individuals healthcare workers medicine in stocks money in bank account in atmosphere blood sugar stored energy degree of belief in x stockpiled vaccines goods in a warehouse beds in an emergency room owned vehicles example model stocks the critical role of stocks in dynamics stocks determine current state of system stocks often provide the basis for making choices stocks central to most disequilibria phenomena buildup decay lead to inertia give rise to delays state changes flows fluxes rates derivatives all changes to stocks occur via flows always expressed per some unit time if these flow into out of a stock that keeps track of things of type x e g persons the rates are measured in x time unit e g persons year month gallons second typically measure over certain period of time by considering accumulated quantity over a period of time e g incidence rates is calculated by accumulating people over a year revenue is time water flow is litres minute can be estimated for any point in time examples of flows inflow or outflow of a bathtub litres minute rate of incident cases e g people month rate of recovery rate of mortality e g people year rate of births e g babies year rate of treatment people day rate of caloric consumption kcal day rate of pregnancies pregnancies month reactivation rate of tb cases reactivating per unit time revenue month spending rate month power watts rate of energy expenditure vehicle sales vaccine sales shipping rate of goods example model flows flows we can ask conceptually about the rate at any given point in time and may change over time measuring it would have to be over some period when speaking about rates for flows we always mean a rate of change over time something measured as x unit time not all things called rates are flows exchange rate prevalence rate rate of return distinguishing stocks flows heuristics to determine if a quantity is a stock or flow snapshot test if you were only to consider a moment in time a snapshot of the system could the quantity be clearly quantified by the information available at that moment if yes stock cannot quantify a value of a flow using only the information for an instant must measure over time time unit change test if we were to change the unit by which we measure time would the numeric value of the quantity change if yes quite likely to be a flow exception beliefs about flows accumulation test is this quantity an accumulation of the time varying values of other quantities if yes stock exercise stocks or flows account balance income incidence prevalence temperature births profits interest principal shipments car accidents patients on dialysis deaths heart attacks arrests police patients in hospital hospital admissions position speed key component stock flow flow stock flow 9 net flow impact on stock flow 000 000 flow current time month stock current time month impact of lowering flow rate to month flow 7 000 000 stock 70 90 time month flow stock and flow alternative flow current 70 90 time month stock stock and flow alternative stock current loops stocks causation does not effect big change instantaneously loops are not instantaneous stocks only change by changes to the flows into out of them there are no inputs that immediately change stocks all causal loops must involve at least one stock the state of the world must change as part of the process absent a stock loop would be instantaneous slides adapted from external source redacted from public pdf for copyright reasons auxiliary variables auxiliary variables are convenience names we give to concepts that can be defined in terms of expressions involving stocks flows at current time adding or eliminating an auxiliary variable does not change the mathematical structure of the system critical for model transparency can be reused at many places references to auxiliary variables prevents need for modeler to think about all of details of definition enhanced modifiability single place to define convenient for reporting graphing tables analyzing model dynamics example model auxiliary variables constants time series parameters for similar reasons to auxiliary variables we give names to model constants time series example model parameters stocks flows compared with markov models open population births deaths non constant likelihood density of transitions likelihood of leaving a stock per unit time can depend on other stocks force of infection likelihood of susceptible becoming infected can depend on prevalence of illness likelihood of initiating smoking could depend on accumulated current or former smokers multiple types of stocks e g costs qalys hosts reservoir species etc continuous time distinctive stock flow features multi species model west nile virus refinement of causal loop diagrams system structure diagrams still essentially a qualitative model but less ambiguous by clearly distinguish stocks flows this helps reduce the artifactual loops discussed with clds combine causal loops diagram elements with stock flow structure if complete all loops will go through a stock loop goes into the flow of a stock as one variable in the diagram loop comes comes out of stock as next variable in diagram example system structure diagram note treatment of flows as links from flow to stock inflows as positive links outflows as negative links headley et al tccess ild f ood wou w mk i r l l t i mv tramms sion risk change headley et al c c r j d jli a j q j i o qc ct il k li q l c ifi jli ic c r f c ld c ct n rj stocks flows diabetes assume diabetes is not curable stocks people without diabetes at different stages of risk people with diabetes flows incident cases both diagnosed undiagnosed deaths from both stocks stocks flows tuberculosis assume that tb infection cannot be totally eliminated stocks susceptible people immunized people people with latent tb infection people with active tb infection flows people becoming latently infected people being vaccinated people with infection going to active tb primary progression people with infection going on to latent tb people with secondary infection going on to active tb deaths from each stock use initial value diabetes model stocks flows for a challenge try creating this in vensim people diabetes inciddeinatbceatesses of pdeoiapbleetwesith deaths ofpe otpimlee dweaithsdoiafbpeetoepsle without diabetes use value use value use value interactive steps view flows and stocks does this make sense hitch up constant auxiliary variables to flows how does changing constant variables change the stock condsiabteatens ftlowfslows 20 10 10 20 60 70 80 90 dinecaidthesntocfapseospolfedwiaitbhedteiasb ecteusrr ecnturrent what happens to the stock resulting stock green 10 20 60 70 80 90 dinecaidthesntocfapseospolfedwiaitbhedteiasb ecteusrr ecnturrent suppose we have these flows rates diabetes flows what happens to the stock some questions diabetes flows when is the stock of people with diabetes at its lowest value dinecaidthesntocfapseeospolefdwiiatbhedteisa bestteosc ksatnodckfalonwd fdleomwodnsetmraotinosntrtateiostn test when is the stock of people with diabetes at its greatest value is the value of the stock of people with diabetes larger at the beginning or end when is the stock of people with diabetes not changing stock green diabetes stock flows 10t0ime1 dinecaidthesntocfapseeospolefdwiiatbhedteisa bestteosc ksatnodckfalonwd fdleomwodnsetmraotinosntrtateiostn test flows and feedbacks stocks are always changed by flows in your experiments we ve used constant values for flows in general the formulas for the flows will depend on things that are changing state ultimately these things must depend on the things that collectively specify the state the stocks stocks as accumulations we often use stocks to accumulate integrate other evolving quantities over time example assume time measured in years example slightly more sophisticated qncalit y of ife for nbpopl dafiloo qntal it y ellg ited qntal i y of ife for smbpopl a non f qncalit y ellghted tron swe of f orm quality ife for opula tion s e of subpopula tion e for opula tion size subp opulation of snbpopl non snbp op l tcon quality eiighted o rmu e t j enme p op l ti on f q i u subp opula tion quality eightoos u b po pual toi a v ife yea r w e d principle structure determines behaviour feedback stock and flow structure of a system determines the possible patterns of behaviour different sets of parameters e g values for constants will select particular behaviour within these behaviour patterns changes to the feedback structure can change behaviour in fundamental ways simple sit model mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible total population incidence rate prevalence recovery delay initial population new infections new recovery newly susceptible immunity loss delay classic feedbacks susceptibles contacts of susceptibles with infectives new infections infectives dynamics 000 person 20 000 person 000 person 000 person 15 000 person 000 person 000 person 10 000 person 000 person 000 person 000 person 000 person person person person state variables over time 500 1250 time months s alternative hc workers exogenous recovery delay person i alternative hc workers exogenous recovery delay person r alternative hc workers exogenous recovery delay person broadening the model boundaries endogenous recovery delay mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible total population staff time per patient healthcare workers initial population incidence rate fractional prevalence recovery delay time until seek treatment new infections new recovery newly susceptible immunity loss delay broadening the model boundaries endogenous recovery delay susceptibles contacts of susceptibles with infectives n ew infections infectives people presenting for treatment waiting times health care staff a different behaviour mode prevalence infectious 000 person 5 000 person person 375 1125 time day prevalence baseline hc workers i baseline hc workers person structure as shaping behaviour system structure is defined by stocks flows connections between them nonlinearity the behaviour of the whole is more than the sum of the behaviour of the parts emergent behaviour would not be anticipated from simple behaviour of each piece in turn stock and flow structure including feedbacks of a system determines the qualitative behaviour modes that the system can take on introduction to stocks flows nathaniel osgood osgood cs usask ca institute for system science health state of the system stocks levels state variables compartments stocks levels represent accumulations these capture the state of the system mathematically we will call these state variables these can be measured at one instant in time stocks start with some initial value are thereafter changed only by flows into out of them there are no inputs that immediately change stocks stocks are the source of delay in a system in a stock flow diagram shown as rectangles examples of stocks water in a tub or reservoir people of different types susceptible infective immune people pregnant women women between the age of x and y high risk individuals healthcare workers medicine in stocks money in bank account in atmosphere blood sugar stored energy degree of belief in x stockpiled vaccines goods in a warehouse beds in an emergency room owned vehicles example model stocks the critical role of stocks in dynamics stocks determine current state of system stocks often provide the basis for making choices stocks central to most disequilibria phenomena buildup decay lead to inertia give rise to delays state changes flows fluxes rates derivatives all changes to stocks occur via flows always expressed per some unit time if these flow into out of a stock that keeps track of things of type x e g persons the rates are measured in x time unit e g persons year month gallons second typically measure over certain period of time by considering accumulated quantity over a period of time e g incidence rates is calculated by accumulating people over a year revenue is time water flow is litres minute can be estimated for any point in time examples of flows inflow or outflow of a bathtub litres minute rate of incident cases e g people month rate of recovery rate of mortality e g people year rate of births e g babies year rate of treatment people day rate of caloric consumption kcal day rate of pregnancies pregnancies month reactivation rate of tb cases reactivating per unit time revenue month spending rate month power watts rate of energy expenditure vehicle sales vaccine sales shipping rate of goods example model flows key component stock flow flow stock flow 9 9 net flow impact on stock flow 000 40 flow current time month stock current time month impact of lowering flow rate to month flow 000 000 500 stock 40 60 80 time month flow stock and flow alternative flow current 40 60 80 time month stock stock and flow alternative stock current loops stocks causation does not effect big change instantaneously loops are not instantaneous stocks only change by changes to the flows into out of them there are no inputs that immediately change stocks all causal loops must involve at least one stock the state of the world must change as part of the process absent a stock loop would be instantaneous delayed impact auxiliary variables auxiliary variables are convenience names we give to concepts that can be defined in terms of expressions involving stocks flows at current time adding or eliminating an auxiliary variable does not change the mathematical structure of the system critical for model transparency can be reused at many places references to auxiliary variables prevents need for modeler to think about all of details of definition enhanced modifiability single place to define convenient for reporting graphing tables analyzing model dynamics example model auxiliary variables constants time series parameters for similar reasons to auxiliary variables we give names to model constants time series example model parameters stocks flows compared with markov models open population births deaths non constant likelihood density of transitions likelihood of leaving a stock per unit time can depend on other stocks force of infection likelihood of susceptible becoming infected can depend on prevalence of illness likelihood of initiating smoking could depend on accumulated current or former smokers multiple types of stocks e g costs qalys hosts reservoir species etc continuous time distinctive stock flow features multi species model west nile virus refinement of causal loop diagrams system structure diagrams still essentially a qualitative model but less ambiguous by clearly distinguish stocks flows this helps reduce the artifactual loops discussed with clds combine causal loops diagram elements with stock flow structure if complete all loops will go through a stock loop goes into the flow of a stock as one variable in the diagram loop comes comes out of stock as next variable in diagram example system structure diagram note treatment of flows as links from flow to stock inflows as positive links outflows as negative links headley et al tccess ild f ood wou w mk i r l l t i mv tramms sion risk change headley et al c c r j d jli a j q j i o qc ct il k li q l c ifi jli ic c r f c ld c ct n rj stocks flows diabetes assume diabetes is not curable stocks people without diabetes at different stages of risk people with diabetes flows incident cases both diagnosed undiagnosed deaths from both stocks stocks flows tuberculosis assume that tb infection cannot be totally eliminated stocks susceptible people immunized people people with latent tb infection people with active tb infection flows people becoming latently infected people being vaccinated people with infection going to active tb primary progression people with infection going on to latent tb people with secondary infection going on to active tb deaths from each stock use initial value diabetes model stocks flows for a challenge try creating this in vensim people diabetes inciddeinatbceatesses of pdeoiapbleetwesith deaths ofpe otpimlee dweaithsdoiafbpeetoepsle without diabetes use value use value use value interactive steps view flows and stocks does this make sense hitch up constant auxiliary variables to flows how does changing constant variables change the stock condsiabteatens ftlowfslows 20 20 40 60 70 80 90 dinecaidthesntocfapseospolfedwiaitbhedteiasb ecteusrr ecnturrent what happens to the stock resulting stock green 20 5m0onth 60 70 80 90 dinecaidthesntocfapseospolfedwiaitbhedteiasb ecteusrr ecnturrent suppose we have these flows rates diabetes flows what happens to the stock some questions diabetes flows when is the stock of people with diabetes at its lowest value 50 50 100 dinecaidthesntocfapseeospolefdwiiatbhedteisa bestteosc ksatnodckfalonwd fdleomwodnsetmraotinosntrtateiostn test when is the stock of people with diabetes at its greatest value is the value of the stock of people with diabetes larger at the beginning or end when is the stock of people with diabetes not changing stock green diabetes stock flows 50 10t0ime1 50 dinecaidthesntocfapseeospolefdwiiatbhedteisa bestteosc ksatnodckfalonwd fdleomwodnsetmraotinosntrtateiostn test flows and feedbacks stocks are always changed by flows in your experiments we ve used constant values for flows in general the formulas for the flows will depend on things that are changing state ultimately these things must depend on the things that collectively specify the state the stocks example simple first order decay create this in vensim use initial value pevoiprulelewntith meadnetiamthe until infection dienaftehcstiforonm use formula deaths from infection mean time until death set model settings model menu settings item dynamics of stock 000 500 250 people with virulent infection people withvirulent infection current time month dynamics of rate of death flow 10 000 7 500 5 000 500 deaths from infection 4 5 deaths frominfection current time month stocks as accumulations we often use stocks to accumulate integrate other evolving quantities over time example assume time measured in years example slightly more sophisticated qncalit y of ife for nbpopl dafiloo qntal it y ellg ited qntal i y of ife for smbpopl a non f qncalit y ellghted tron swe of f orm quality ife for opula tion s e of subpopula tion e for opula tion size subp opulation of snbpopl non snbp op l tcon quality eiighted o rmu e t j enme p op l ti on f q i u subp opula tion quality eightoos u b po pual toi a v ife yea r w e d principle structure determines behaviour feedback stock and flow structure of a system determines the possible patterns of behaviour different sets of parameters e g values for constants will select particular behaviour within these behaviour patterns changes to the feedback structure can change behaviour in fundamental ways simple sit model mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible total population incidence rate prevalence recovery delay initial population new infections new recovery newly susceptible immunity loss delay classic feedbacks susceptibles contacts of susceptibles with infectives new infections infectives dynamics 200 000 person 20 000 person 100 000 person 150 000 person 15 000 person 000 person 100 000 person 10 000 person 50 000 person 50 000 person 5 000 person 000 person person person person state variables over time 250 500 1250 time months s alternative hc workers exogenous recovery delay person i alternative hc workers exogenous recovery delay person r alternative hc workers exogenous recovery delay person broadening the model boundaries endogenous recovery delay mean contacts per capita per infected contact infection rate mean infectious contacts per susceptible per susceptible total population staff time per patient healthcare workers initial population incidence rate fractional prevalence recovery delay time until seek treatment new infections new recovery newly susceptible immunity loss delay broadening the model boundaries endogenous recovery delay susceptibles contacts of susceptibles with infectives n ew infections infectives people presenting for treatment waiting times health care staff a different behaviour mode prevalence infectious 200 000 person 5 100 000 person person 375 750 1125 2250 time day prevalence baseline 30 hc workers i baseline 30 hc workers person structure as shaping behaviour system structure is defined by stocks flows connections between them nonlinearity the behaviour of the whole is more than the sum of the behaviour of the parts emergent behaviour would not be anticipated from simple behaviour of each piece in turn stock and flow structure including feedbacks of a system determines the qualitative behaviour modes that the system can take on overview of the simulation modeling process nathaniel osgood cmpt january announcements tutorial vote link sent please vote by wednesday evening download install vensim ple overview of modeling process typically conducted with an interdisciplinary team an ongoing process of refinement best iteration with modeling intervention implementation data collection often it is the modeling process itself rather than the models created that offers the greatest value modeling process overview a key deliverable reference mode parameter sensitivityspecification learning model scope boundary causal loop diagrams specification of reproduction analysis investigation of intervention scenarios environm ents mic selection state charts parameters matching of intermediate time cross validation investigation of hypothetical external roworlds flight model time horizon quantitative causal robustness extreme identification of system structure diagrams relations series matching of value tests conditions simulator key variables decision behavior unit checking cross scenario reference modes for multi agent interaction diagrams rules observed data points problem domain testscomparisons e g cea constrain to sensible explanation multi scale hierarchy diagrams process flow structure initial conditions bounds structural sensitivity analysis group model building recall coevolution observations external world actions choice of observations mental model birth rate normal weight deaths developing obesity pregnancies of non overweight women completion of pregnancy to non overweight state pregnancy duration shedding obesity pregnancies to non overweight mother developing gdm pregnant women developing persistent overweight obesity born from o normal weight individuals developing born completion of pregnancy to overweight state pregnancies of pregnancies to overweight mother developing gdm pregnancies developing gdm from mother with gdm history completion of gdm pregnancy ther with f dm thers overweioghvterweight women deaths overwe developing pregnant women with gdm that continue on to postpartum pregnancies for women with gdm deaths women with history of completion of pregnancy for mother with d new pregnancies from mother with gdm developing deaths from non mpletion of non gdm gnancy for woman with history of gdm simulated dynamics formsacl modeling artifacts women with history of gdm modeling process overview a key deliverable reference mode parameter sensitivityspecification learning model scope boundary causal loop diagrams specification of reproduction analysis investigation of intervention scenarios environm ents mic selection state charts parameters matching of intermediate time cross validation investigation of hypothetical external roworlds flight model time horizon quantitative causal robustness extreme identification of system structure diagrams relations series matching of value tests conditions simulator key variables decision behavior unit checking cross scenario reference modes for multi agent interaction diagrams rules observed data points problem domain testscomparisons e g cea constrain to sensible explanation multi scale hierarchy diagrams process flow structure initial conditions bounds structural sensitivity analysis group model building identification of questions the problem all models are simplifications and wrong some models are useful attempts at perfect representation of real world system generally offer little value establishing a clear model purpose is critical for defining what is included in a model understanding broad trends insight understanding policy impacts ruling out certain hypotheses think explicitly about model boundaries adding factors often does not yield greater insight often simplest models give greatest insight opportunity costs more complex model takes more time to build less time for insight importance of purpose firmness of purpose is one of the most necessary sinews of character and one of the best instruments of success without it genius wastes its efforts in a maze of inconsistencies lord chesterfield the secret of success is constancy of purpose benjamin disraeli the art of model building is knowing what to cut out and the purpose of the model acts as the logical knife it provides the criterion about what will be cut so that only the essential features necessary to fulfill the purpose are left john sterman h taylor common division endogenous things whose dynamics are calculated as part of the model exogenous things that are included in model consideration but are specified externally time series constants ignored excluded things outside the boundary of the model example of boundary definition modeling process overview a key deliverable reference mode parameter sensitivityspecification learning model scope boundary causal loop diagrams specification of reproduction analysis investigation of intervention scenarios environm ents mic selection state charts parameters matching of intermediate time cross validation investigation of hypothetical external roworlds flight model time horizon quantitative causal robustness extreme identification of system structure diagrams relations series matching of value tests conditions simulator key variables decision behavior unit checking cross scenario reference modes for multi agent interaction diagrams rules observed data points problem domain testscomparisons e g cea constrain to sensible explanation multi scale hierarchy diagrams process flow structure initial conditions bounds structural sensitivity analysis group model building example causal loop diagram departm science a second causal loop diagram susceptibles contacts of susceptibles with infectives n ew infections infectives people presenting for treatment waiting times health care staff qualitative causal loop diagram qualitative transitions no likelihood yet specified these variables are aspects of state weight these parameters give static characteristics of the agent these describe the behaviours the mechanisms that will govern agent dynamics stock flow structure birth rate normal weight deaths developing obesity pregnancies of non overweight women completion of pregnancy to non overweight state pregnancy duration shedding obesity pregnancies to non overweight mother developing gdm pregnant women developing persistent overweight obesity normal weight individuals developing weight babies born to gnant normal weight mothers completion of pregnancy to overweight state pregnancies to overweight mother developing gdm pregnancies developing gdm from mother with gdm history completion of gdm pregnancy born from ther with dm oveweight babies born from mothers pregnancies of overweioghvterweight women deaths overweight individuals developing deaths completion of pregnancy for mother with pregnant women with gdm that continue on to postpartum new pregnancies from mother with women with history of gdm developing pregnancies for women with gdm mpletion of non gdm gnancy for woman with history of gdm deaths from non women with history of gdm problem mapping qualitative models system structure diagram headley j rockweiler h jogee a women with hiv aids in malawi the impact of antiretroviral therapy on economic welfare proceedings of the international conference of the system dynamics society athens greece july modeling process overview a key deliverable reference mode parameter sensitivityspecification learning model scope boundary causal loop diagrams specification of reproduction analysis investigation of intervention scenarios environm ents mic selection state charts parameters matching of intermediate time cross validation investigation of hypothetical external roworlds flight model time horizon quantitative causal robustness extreme identification of system structure diagrams relations series matching of value tests conditions simulator key variables decision behavior unit checking cross scenario reference modes for multi agent interaction diagrams rules observed data points problem domain testscomparisons e g cea constrain to sensible explanation multi scale hierarchy diagrams process flow structure initial conditions bounds structural sensitivity analysis group model building model formulation model formulation elaborates on problem mapping to yield a quantitative model key missing ingredients specifying formulas for statechart transitions flows in terms of other variables intermediate output variables parameter values example conditional transition the incoming transition into whetherprimaryprogre ssion will be routed to thisoutgoing transitionif this condition is true transition type message triggered transition type fixed rate transition type variable rate transition type fixed residence time timeout simple intermediate variable simple intermediate variable simple basis for formula order delay model stock flow structure annual likelihood of becoming diabetic annual likelihood of non diabetes mortality for asymptomatic population annual at risk births undx uncomplicated dying other causes annual not at risk births zed mortality for obese being born non obese being born at risk annual likelihood of becoming obese becoming obese developing diabetes undx prediabetics recovering annual likelihood of undx prediabetic recovery dx prediabetics recovering diagnosis prediabetic annualized density of recon pulation non obese mortality annual mortality rate for non obese population obese mortality annual likelihood of dx prediabetic recovery dx prediabetic popn dx uncomplicated dying otehr causes annual likelihood of non diabetes mortality for more sophisticated formula contact rates and transmission probs contacts per susceptible c fraction of contacts that are infective y n per contact transmission probability force of infection likelihood each susceptible will be infected per unit time common formulation c y n flow total infections per unit time x force of infection x c y n note that this y c x n sources for parameter estimates surveillance data controlled trials outbreak data clinical reports data intervention outcomes studies calibration to historic data expert judgement systematic reviews introduction of parameter estimates some dynamics models will provide much more detail on networks of factors shaping these rates but ultimately there will be constants that naennueal ldikeltihoood bof e specified annual likelihood of becoming diabetic non diabetes mortality for asymptomatic population annual at risk births undx uncomplicated dying other causes annual not at risk births zed mortality for obese being born non obese being born at risk annual likelihood of becoming obese becoming obese developing diabetes undx prediabetics recovering annual likelihood of undx prediabetic recovery dx prediabetics recovering diagnosis prediabetic annualized density of recon pulation non obese mortality annual mortality rate for non obese population obese mortality annual likelihood of dx prediabetic recovery dx prediabetic popn dx uncomplicated dying otehr causes annual likelihood of non diabetes mortality for modeling process overview a key deliverable reference mode parameter sensitivityspecification learning model scope boundary causal loop diagrams specification of reproduction analysis investigation of intervention scenarios environm ents mic selection state charts parameters matching of intermediate time cross validation investigation of hypothetical external roworlds flight model time horizon quantitative causal robustness extreme identification of system structure diagrams relations series matching of value tests conditions simulator key variables decision behavior unit checking cross scenario reference modes for multi agent interaction diagrams rules observed data points problem domain testscomparisons e g cea constrain to sensible explanation multi scale hierarchy diagrams process flow structure initial conditions bounds structural sensitivity analysis group model building calibration often we don t have reliable information on some parameters some parameters may not even be observable some parameters may implicitly capture a large set of factors not explicitly represented in model often we will calibrate less well known parameters to match observed data analytic triangulation often try to match against many time series or pieces of data at once sometimes we learn from this that our model structure just can t produce the patterns single model matches many data sources one of the pieces of the elephant example model of underlying process time series it must match example iteration calibration susceptible population average incubation time average duration of illness s b depletion infection rate r emergence rate removal rate contagion r infectivity total infectious contacts contact rates contagion from sterman time day from sterman expanding the boundary behavioral feedbacks susceptible population average incubation time average duration of illness s b depletion infection rate r emergence rate removal rate contagion r total infectious contagion infectivity contacts contact rates safer b social distancing social distancing b hygiene media attention public health warnings practices from sterman cumulative cases 28 time day modeling process overview a key deliverable reference mode parameter sensitivityspecification learning model scope boundary causal loop diagrams specification of reproduction analysis investigation of intervention scenarios environm ents mic selection state charts parameters matching of intermediate time cross validation investigation of hypothetical external roworlds flight model time horizon quantitative causal robustness extreme identification of system structure diagrams relations series matching of value tests conditions simulator key variables decision behavior unit checking cross scenario reference modes for multi agent interaction diagrams rules observed data points problem domain testscomparisons e g cea constrain to sensible explanation multi scale hierarchy diagrams process flow structure initial conditions bounds structural sensitivity analysis group model building units dimensions distance dimension length units meters fathoms li parsecs frequency growth rate etc dimension time units year sec etc fractions dimension dimensionless unit units dimensional analysis da exploits structure of dimensional quantities to facilitate insight into the external world uses cross checking dimensional homogeneity of model deducing form of conjectured relationship including showing independence of particular factors sanity check on validation of closed form model analysis checks on simulation results derivation of scaling laws construction of scale models reducing dimensionality of model calibration parameter estimation sensitivity analyses same relative or absolute uncertainty in different parameters may have hugely different effect on outcomes or decisions help identify parameters that strongly affect key model results choice between policies we place more emphasis in parameter estimation into parameters exhibiting high sensitivity sensitivity in initial value frequently we don t know the exact state of the system at a certain point in time a very useful type of sensitivity analysis is to vary the initial value of model stocks in vensim this can be accomplished by indicating a parameter name within the initial value area for a stock varying the parameter value imposing a probability distribution monte carlo analysis we feed in probability distributions to reflect our uncertainty about one or more parameters the model is run many many times realizations for each realization the model uses a different draw from those probability distribution what emerges is resulting probability distribution for model outputs example resulting distribution empirical fractiles static uncertainty impact on cost of uncertainty regarding mortality and medical costs 60 80 incremental costs b b b b 2026 dynamic uncertainty stochastic processes baseline dynamic uncertainty stochastic processes 60 70 80 95 100 6 15 2914 time day mathematical analysis of models system linearization jacobian fixed point criteria s c i ˆs r state space diagram reasoning about many scenarios at once i c i ˆs i h r i i r h eigenvalues e g for stability analysis around fixed point applied math dynamic modeling although you may not use it the dynamic modeling presented rests on the tremendous deep rich foundation of applied mathematics linear algebra calculus differentia integral uni multivariate differential equations numerical analysis including numerical integration parameter estimation control theory for the mathematically inclined the tools of these areas of applied math are available comments on mathematics dynamic modeling many accomplished well published dynamic modelers have limited mathematical background can investigate pressing important issues software tools are making this easier over time can gain extra insight flexibility if willing to push to learn some of the associated mathematics achieving highest skill levels in dynamic modeling do require mathematical facility and sophistication to do sophisticated work often those lacking this background or inclination collaborate with someone with background examples of mathematical insights from system dynamics models identification of long term behavior eventual outcome the impact of parameters on outcomes the robustness of these outcomes to disturbance insight into key causal linkages driving the system at each point in time identification of high leverage parameters interventions explanation for elements of observed behavior example simple sits model mean contacts c per capita per month mean infectious contacts per susceptible total population per infected contact infection rate per susceptible incidence rate prevalence recovery delay initial population new infections new recovery newly susceptible immunity loss delay associated system of state equations s c i s r these represent the same s i r infection flow flowing out of s and into i i c s i i i r these represent the same recovery flow flowing out of i and into r r i r these represent the same loss of immunity flow flowing out of r hence the minus sign flowing into s modeling process overview a key deliverable reference mode parameter sensitivityspecification learning model scope boundary causal loop diagrams specification of reproduction analysis investigation of intervention scenarios environm ents mic selection state charts parameters matching of intermediate time cross validation investigation of hypothetical external roworlds flight model time horizon quantitative causal robustness extreme identification of system structure diagrams relations series matching of value tests conditions simulator key variables decision behavior unit checking cross scenario reference modes for multi agent interaction diagrams rules observed data points problem domain testscomparisons e g cea constrain to sensible explanation multi scale hierarchy diagrams initial conditions bounds structural sensitivity analysis group model building late availability of hc workers prevalence infection extinction at hc workers 375 500 625 875 1125 1625 2125 2375 time day prevalence baseline hc workers prevalence alternative hc workers late prevalence alternative hc workers late 100 prevalence alternative hc workers late prevalence alternative hc workers late 250 prevalence alternative hc workers late 1 simulation analysis scenarios for understanding how x affects system policy formulation evaluation policy comparison stochastic processes policy comparison stochastic processes modeling process overview a key deliverable reference mode parameter sensitivityspecification learning model scope boundary causal loop diagrams specification of reproduction analysis investigation of intervention scenarios environm ents mic selection state charts parameters matching of intermediate time cross validation investigation of hypothetical external roworlds flight model time horizon quantitative causal robustness extreme identification of system structure diagrams relations series matching of value tests conditions simulator key variables decision behavior unit checking cross scenario reference modes for multi agent interaction diagrams rules observed data points problem domain testscomparisons e g cea constrain to sensible explanation multi scale hierarchy diagrams initial conditions bounds structural sensitivity analysis group model building overview of the model extracellular fluid volume ecfv atrial natriuretic hormone anh extracellular osmolality drinking extracellular sodium ecna antidiuretic hormone adh 1 urinary concentration total body water tbw urine flow uflow 5 mean arterial pressure map na out in urine aldosterone ald simplified causal loop diagram of the overall model the interactive dynamic simulator bwatergame barlas karanfil results of the game tests by players 130 125 player4 normal dynamics of ecna concentration for five players hours 44 dynamics of total body water player4 normal barlas karanfil team meetings stakeholder action labs mabry simulating the dynamics of cardiovascular health and related risk factors key take home messages from this morning models express dynamic hypotheses about processes underlying observed behavior models help understanding how diverse pieces of system work together sd focus on feedbacks as the fundamental shapers of dynamics models are specific to purpose system dynamics includes both qualitative quantitative components sd models admit to formal reasoning analysis department of computer science introduction to the anylogic interface supporting concepts announcements lecture recording links posted tutorial time extended class hours on tuesday or thursday choice will depend on other classes following ours thursday is likely the anylogic user interface common configuration the project view overview of projects components note double clicking on a tab opens view as full screen palette for adding items to canvas problem area indicates problem building running model properties area shows info on selected element in project or palette window the project view hierarchically shows the project components the means that the model has changed since the last time it was saved you should consider saving the model when you see this hands on model use ahead load sample model predator prey agent based via sample models under help menu example classes main classes define the stage agent classes define the actors experiment classes define the scenarios key customized classes the structure of the model is composed of certain key user customized classes main class normally just one instance this will generally contain collections of the other classes agent classes your agent classes subclasses of activeobject there are typically many instances objects of these classes at runtime experiment classes these describe assumptions to use when running the model double click on main class name to view this class should appear on top tab double click here main class defines the environment where agents interact defines interface cross model mechanisms the main object normally contains one or more populations of replicated agents each population consists of agents of a certain class or a subclass therefore e g hares lynxes the agent classes are defined separately from the main class agent populations in the main class through the replication property the number of these agents can be set the environment property can be used to associated the agents with some surrounding context e g network embedding in some continuous space with a neighborhood statistics can be computed on these agents within the main class you can create representations of subpopulations by dragging from an agent class into the main class area elements of a main class these parameters specify static model wide characteristics visual input elements used during simulation param setting these represent the agent populations these functions calculate things or can change model behavior visual output elements used during simulation agent class defines the characteristics behaviour of agent population members double click on lynx a critical distinction design specification vs execution run times the computational elements of anylogic support both design execution time presence behaviour design time specifying the model execution time runtime simulating the model it is important to be clear on what behavior information is associated with which times generally speaking design time elements e g in the palettes are created to support certain runtime behaviors a familiar analogy the distinction between model design time model execution time is like the distinction between time of recipe design here we re deciding what exact set of steps we ll be following picking our ingredients deciding our preparation techniques choosing making our cooking utensils e g a cookie cutter time of cooking when we actually are following the recipe a given element of the recipe may be enacted many times one step may be repeated many times one cookie cutter may make many particular cookies cooking analogy to an agent class a cookie cutter we only need one cookie cutter to bake many cookies by carefully designing the cookie cutter we can shape the character of many particular cookies by describing an agent class at model design time we are defining the cookie cutter we want to use this defines the visual elements to be used for this object when it is displayed at runtime common agent class elements these introduce methods functions that include some java code these parameters specify static agent characteristics these describe the agent state behaviour the mechanisms that will govern agent dynamics this defines the visual elements to be used for this object when it is displayed at runtime these introduce methods functions that include some java code for custom behaviours these parameters give static characteristics of the agent these describe the behaviours the mechanisms that will govern agent dynamics experiments experiment classes experiment classes allow you to define run scenarios in which global parameters i e parameters defined in main may hold either default or alternative values experiment classes are also used to set the time horizon for a simulation memory limits important for large models details of simulation run details on random number generation virtual machine arguments properties allow one to set the values for each parameter right click on these choose run to run such a scenario setting memory virtual machine arguments the notion of a build we prepare a fully specified model to run a simulation using a build if all goes well this translates project to executable java this may alert you to errors in the project a compiler is a tool to convert from a program specification e g state charts action diagrams etc to a representation that can be executed normally a compiler is applied to each of several components of a program e g classes anylogic build process applies a compiler to the components of the anylogic model cooking analogy to build ing obtaining preparing the ingredients before we can actually realize the recipe we need to go collect prepare all ingredients we re not yet cooking but what we are doing makes the cooking possible the cooking here is running the modle a bit on java java is a popular cross platform object oriented programming language introduced by sun microsystems anylogic is written in java and turns models into java anylogic offers lots of ways to insert snippets hooks of java code you will need these if you want to e g push anylogic outside the envelop of its typical support e g enabling a network with diverse agent types exchange messages between agents put into place particular initialization mechanisms collect custom statistics over the population stages of the anylogic build build buttons one just for this project one for all projects build all projects build just this project alternative building via context menu anyl og ic advanced educ ationa l use only f ile f dit yiew model t indow elp i i g il q q m le i qi qi qi get support o open ct rl o save ctr l s o save as f a ppea rancet ime as o revert niti allylnfe cted sex initi alage d e t hn icity currentage close plain vari preg nancystatu ciose ers close all nonpregnant g fina lizedeat h co llectio g fun ctio n cut ctrl x g fertilityrateag esexet hn icity g pe rfo rm birth qi ta ble fu n port copy ctrl c g esta blis hoffspr ingc onne ctio ns bas edonmoth ersc onne ctions a conne cto paste pregnant g esta blis hoffspr ingl ocat io nbasedonm othersl o cat io n ent ry poin delete delete state refresh console v el tran sit ion bu ild debuggingexample model in it ial stat descr export na me eclipsedebugg ing examp le bra nch history st team fina l state des criptio n pac kage abmm odelwit h birthdeath file u research eclipsedebugg ing examp le ecli psede bugg ing examp le alp environm i action i ana lysis i fa presen tati conne ctivi en erprise builds gone bad the problems view builds gone good model execution the simulation is running time is advancing in steps or as necessary to handle events each agent class will typically have many particular agents in existence each agent will have a particular state this population may fluctuate variables will be changing value presentation elements will be knit together into a dynamic presentation press this button to run an experiment a simulation you can pull down the menu to choose which experiment to simulate initial screen experiment set up use to set speed parameters via ui press this button to switch to the model presentation display presentation of the model main object in operation network embedding of agents dynamic color updates via agent logic pausing the model drill down from the model to particular agents runtime view of particular agent drill down from previous view use this selection to switch between viewing the state of different agents customizing the model running user interface switching back to view the main object controlling simulation speed speeding up controlling simulation speed slowing down toggling between maximum and a throttled speed terminating model execution another way to terminate a simulation use this console stop button to terminate the simulation examples of where to insert code object properties advanced examples of where to insert code object properties general example of where to insert code presentations properties dynamic properties of presentation elements especially of agents tips to bear in mind while writing code click on the light bulb next to fields to get contextual advice e g on the variables that are available from context while typing code can hold down the control key and press the space key to request autocompletion this can help know what parameters are required for a method etc java is case sensitive can press control j to go to the point in java code associated with the current code snippet can press build button after writing snippet to increase confidence that code is understood example of contextual information file edit view model window help anyl ogic advanced educational use only vt d o t l j q q i d i j tpget suppa rt pro j e ct n l bi pe r so rn el m a el el ll tt el a t o n o o t i e l op filt ha l m ology d ep art ment mainphasel simu lat ion netwo rk seir ma i n parameters l pl ain variable b furnct ions env iro nmen ts e rnv i ro nm ent f b embedded obj ect ana ly si data g datasetab soiute pr eva lence g cou rnt l rnf ect i ou l l pr ese i o per so n l pla in var iabie sta t e ch a rt stat e chart a tated f i pro blem ll i p l on j pe ople statistks de scri ption p eo ple e n v i ro nment g count lnfe ctiou nam c ount l nfe ct i ous t yp e count q sum q av rag e q min q max eact i v e per so iadd st at i st i cs i param ete r flow am var iab le d stock variab le lj event dynamic event plain variab le co ll e ct ion variable fun ct i on ti tabl e fun ct i on i port con rne ct or erntry po int state t rarnsit iorn lrni t ia l state po int er bram ch hist ory state fina l state ernv i ro rnm ernt ac t io rn i ii analysi ii pr e sentat i orn ci c onn ect iv ity ffi ernt erpri se libr ary more libr ari e autocompletion info via control space j anylogic university evaluation use only gj f ile edit view model window help f t p r o je l id did li fftle i ri j iq a get support el palette l el hybridabmnetworkmodeling l j o main 1 j o person color agen tentity entity sta ts eb o simulation main multipleagentcl asseslnnetworkl st at echait suscept ible infected i i gene ral parameter i ev en t dynamic event plain var iable i collection var ia bl e g function cl table function l port connector environmen t i console 0 person active object class genera l x ad v an ced v v el receivemessage lnt boolean stat echart e pr oblems j v el agent receivemessage object boolean sta tech reoeivemessage description location pr evie w description movement parameters 1 ve lo city ro tation on arrival on messagereceiv ed tatechart rece public boolean receiv emessage int ms same as receivemessage object msg but with an integer as message parame ter mg g th e in tege r po sted to the sta techart on before step enterpr i se library pedes tri an library i on step i selection i x y subscripting in vensim subscript introduction selection progression mapping subranging nathaniel osgood cmpt march a common pattern in aggregate models lattice structure income change education decile income change education decile income change education decile education rise from decile to for income decile education rise from decile to for income decile education rise from decile to for income decile education rise from decile to for income decile income change education decile education rise from decile to for income decile income change education decile income change education decile education rise from decile to for income decile income change education decile income levels income change education decile education rise from decile to for income decile income change education decile education rise from decile to for income decile unexposed income decile education decile recall a means to simplification subscripting we can simplify lattice structure by subscripting the structure by discrete properties this structure is then replicated for every subscript combination we can perform operations to create aggregate totals from this disaggregated data recall reading a subscripted equation suggestion read as follows variable total population for a specific age group member of agegroups ethnic group member of ethnicgroups and sex member of sex is just the sum of the non diabetic population for that same age ethnic sex group and of the diabetic population for that same age ethnic sex group recall vensim model subscript control subscript control interface yensim stratified demographic model mdl yar age specific fertility rat eile liew insert odel iools iindows t elp i i i i jb e i iq baseline c i vil i i vf f fi j il cim i fk ji c i y i j c totalpo t ion total population by age doc t total population agegroups idhnicgroups ses i subranges edit selected elements total population by le subscript sum total population total population by age runs age specific fer all none ageg ageg years in each age l per looo wo aiibutoldeslagegroup e tlmicil m iddleageg roups available elements t l j ageg group fractional prevalence o fd iaebt by age r ageg d ageg roup r keep on top r skip undefined ageg i j ageg r i ageg j ageg ageg ageg ageg ageg ageg clear seleded populat diabetic population diabetic population by age diabetic poi by age eh mean age f age catego subscript control interface the elements in this pane are those that will be shown in graphs tables subscript control interface these controls add or remove elements whose values should be displayed a key piece of functionality subscript control the selected subset of subscripts are shown here all subscript control interface maximum length subset of those selected are shown the elements in this pane are those that will be shown in graphs tables subscripts tables the selected subset of subscripts are shown j file edit view insert model tools windows help i l liiirl i i j e i iq in baseline subscri ts gra hs of liet gra h for fractional prevalence of diabetes b a e c f fractional prevalence of diabetes by age doc total tsum total populatiott byarp o total populatiott total populatiott run lo bya f o ep o y o years in i grol 2020 time year fractional prevalence of diabetes by age c u fractional prevalence of diabetes by age c uf years in each fractiottal prevalettee fractiottal prevalettee of group f p ttee of diabetes by sex diabetes b ethnicity o e t y f diabetjc diabetic populatiott diabetjc popula jott by sex by ethnicity bya f o o diabetjc by fa sex b e hnic initia diabeticcase b etic populatiott diabetic populatiott diabetic pop i by a f el y diabetjc populatiott o diabetic mortality rate by a f sex ethnicity d iabet deaths fractiottal prevalettee of diabetes by a f sex iti aggregate mortality rate ratio for diabetics ethnicity total populatiott by a f ethnicity sep meatta f f a f total populatiott by a f ethnicity catego choosing additional tools from tools analysis menu histogram tool j yensim stratified demographic model mdl yar fractional prevalence o liete ii iew layout jodel iools indows tfelp g c i vtti i vf f i i il c im i fj ji c i y i c is total population by age l l ill totalpo p ulta io total population total population by rum l total population by sex ethnicity by a g e total population by total population by double subscript sum for total population by age total population total population b e p o by et it y total population by age sex age ethnicity o ethnicitys om years in each age group total population by age ethnicity sex years in each age group f ractiottal prevalettge ofd iaebt by fractional prevalettge fractional prevalettge of nm o o diabet g by fa sex b e hnic nott diabetic population fc c a diabetic population diabet g diabetic pop by a geet i y t ttgidetttcasesofdiabetet imtia diabet g population i o diabet g mortality rate by age sex ethnicity d iabet deaths fractional prevalettge of diabetes by age sex aggregate mortality rate ratio for diabet g o ethnicity total population by age ethnicity sex mean age f age catego total population by age ethnicity sex i hide times new roman eile dit iew ayout iools indows t elp subscriots at d g c i t i i v r l v i o i h ii i ew i q bar graph options t i y mas r l bar total population by age r ling t normal i min small mas i background double subscript sum for total total population total population total population ino sorting jlabel ibar graph foreground cancel ye rl ll lffll j by age years in each ff b 7 e x o by et it y o fractional prevalence fractional prevalence of age ethnicity sex ofdr ofdia b o br loric yo group o tionby sex total population by diabetes incidence rate group fractional prevalence y a diabehc diabetic population non by age ethnicity sex diabetjg populatjon by sex by ethnicity populat byage o o o t diabetjg diabetic diabetjg by fa sex b eth non d iabetic population t ac la incidentcasesofdiabete by a geet i y i o initialdiabeticcase b d iabet diabetic mortality rate ge sex ethnicity diabetic population fractional prevalence of diabetes by age sex aggregate mortality rate ratio for diabetics ethnicity total population by age ethnicity sex mean age f age catego l i lfl l i hide times new roman total population by age ethnicity sex in control panel select all select eile dit iew insert jodel iools iindows tielp t c i vtti t i i vr lt l ii i il cim i c i bx i c f ts i c total population by age f l ill to t a po p ulta imof mmlil izmzll j run r a total subranges edit by age tota l none gegroup a middleagegroups selected elements c total population by et it y c yeam in each age group tionby sex non populat n available elements i r i t j agegroup60to64 y seox fractional prevalence of d ia b etic p op u l at io n by ethnicity non d etic popul clear selected population diabetic pop sex o by ethniciti iabetic ulation prevalence of es by age sex simple i thnicity o e a i p j ulta ionby age ethnicity sex mean age f age catego c total population by age ethnicity sex displaying histogram across subscript values file edit view layout model tools windows help setting subscript groups run show link r cause r use i none number format i pretty r scienlific r r square against percen ti els t c total population by age over r sensitivity double subscript sum lo x a label tats ij slats running down background foreground j r time i subs at t ime r special for total population c total population by age c total population c total population b ep o by et it y column width first rest r o r start r select group fjactional prevalence m n e dimor b ru ri diabetes incidence rate by age ethnicity sex non d iabetic population t a incidentcasesofdiabete b diabettc diabetic population diabettc populatjon by sex by ethnicity diabettc population irutia diabett iti o by age sex ethnicity aggregate mortality rate ratio for diabettc l fractional prevalence of diabetes by age sex ethnicity t c total population by age ethnicity sex view l j hide times new roman c total population by age ethnicity sex file edit view insert model tcols windows help statistics across subscri ts c ui b n e stats d ariable count hi n c e fract ional pre alence of di abetes by age agegroups l result across subscri pts at tine ru ns fractional pre alence of di abetes by age i ill runs o ilil x a hax hean hed ian c us ask c lasses hodels i stde m orn al population total population e x byet it y prevalence fractional prevalence of d bo b r hmchy diabetic diabetic population population by set by ethnicity i iia r r iimiii lw by ta sex o by ethnicit non dlilbetic population l i lncident casesofdiabete b o lrutia diabeticcase i diabetic population j iabet deaths o diabetic mortality rate total population by age ethnicity sex mean age f age catego tot al population by age ethnicity sex subscript control interface can use this button to introduce a new subscript i e stratification dimension creating a new subscript dit liew insert ijodel iools iindows t elp new subscript name i j total population by c run lo x a yeam in each age group available elements r selected elements ageg ageg ageg ageg ageg ageg ageg total population byet it y fractional prevalence of i ageg tionby sep 0 non populat n non d tlbetic popul j ageg ageg ageg ageg ageg ageg roup ageg ageg clear selected diabetic population by ethnicity populahon diabetic pop sex o by ethnicit iabetic i ulation 0 prevalence of esby a f sex simple i thnicity o l e d it l n e w s d e lni e d l i a i po i ulta ino by a f ethnicity sep 0 meana f f a f catego total population by 0 a f ethnicity sep 0 defining a new subscript elements new subscript appears in subscript control edit subscript can use this button to edit the a subscript e g to add or remove elements example subscripted stock youngest must consider birth flow in and aging to next higher age group for middle age categories uses subscript mapping to find previous age category to this must consider both aging in from previous category aging out for oldest age category previous age category is hard coded must consider aging in but there is no aging out finding the mapping yen stratified demographic model mdl yar non diabetic agegroups eth dit iew ayout jodel iools iindows tielp editing equation for non diabetic population n on d iabelic population sex t births e thnicg roups s ex incidentcaseso fdiabeles ageg e thnicg roups s ex non diabetic deaths ethnicgroups sex inte g i non diabetic population aging ethnicgroups sex j eile initial initial non diabetic population ageg elhnicg roups sex value total population by agr y p e leve l undo i u u l j t j jilliti i j lj variables isubscripts i functions i more choose variable i r p u t i al population total population by inormal j j j j r supplementary l lj d ll lj j non diabetic population births incidenlcaseso fd iabetes non diabetic deaths non diabetic population aging by sex ethnicity total population by total population by agr sex ethnicity s double subscript sum for total population c total population by ay u nils j j total population by agr ethnicity sex c years in each age group fractional prevalence ofd i abet by agr group i r s t r a l i i t j i i i go to errors iequation ok previ nex d ej hiliteiisiilj newi l diabetes incidence rate by agr ethnicity sex d populat i diabetic population initial non diabetic population j initia diabeticcases d i abet deaths by agr sex ethnicity non diabetic mortality aggregate mortality rates by agr sex ethnicity rate ratio for diabetics mean agr f agr catego hide times new roman finding among variables i l fied demographic model mdl var non population eile liew layout jodel iools liindows tielp editing equation for non diabe modify equation for double sum im total population i non diabetic population ageg elhnicgroups s es elhnicg roups births ethnicgroups ses lncidentcasesoidiabetes etl flnal tlme diabetic deaths ethnicgroups ses fractional prevalence of diabetes by age integ non diabetic population aging ethnicgroups fractional prevalence of diabetes by age ses ethnicity ses fractional prevalence of diabetes by ethnicity fractional prevalence of diabetes by ses initial lncidentcasesoidiabetes initial non diabetic population ethnicgroups ses income total population value initial non diabetic population rt ype a z j i ji 1 j i initial time by age 1 lev_e j i jcj mean age of age category normal j ff l i 3 1 1 middleagegroups i c j j j j non diabetic population model diabetic population by se i supplementary j births model diabetic population by ses ethnicity re l l c 1 lncidentcasesoidiabetes mortality for youngest group non diabetic deaths mortality hazard age coefficient in esponent i non diabetic population agit non diabetic deaths total population ft by ar com ment j non diabetic mmtality rates by age sex ethnicity non diabetic population non diabetic po ulationa in fractional prevalence of diaebt by age group i strati fied de j range go to o 1 name or im err ok i check syn check model o births initial non diabetic population e ll c i incidentcasesofdiabetes initia diabeticcases type diabetic population diabetic population by age diabetic by age eh d iabet deaths diabetic mortality rate by age sex ethnicity non diabetic mortality aggregate mortality rates by age sex ethnicity rate ratio for diabetics mean age f age catego middleagegroup subranges in subscript control finding middle age group subrange definition subrange definitions mapping definition finding the other sex for a given sex using the opposite sex in an equation clever ways of defining flows can lead to fewer equations definition of aging fraction leaving each year is set to 0 for oldest age group previous age group for youngest age group is oldest so no aging into youngest age group subscripting in vensim nathaniel osgood cmpt march notable features of the aggregate model we count individuals separately due to differences properties that include both evolving states static characteristics changes to a given piece of state e g income captured by flows from stock group representing old value to group representing new value a given individual will flow down flow depending on other characteristics individual properties captured in discrete fashion maintenance of discrete history information e g exposure is secured via disaggregation of stocks rises geometrically with dimensions of heterogeneity a common pattern in aggregate models lattice structure income change education decile income change education decile income change education decile education rise from decile to for income decile education rise from decile to for income decile education rise from decile to for income decile education rise from decile to for income decile income change education decile education rise from decile to for income decile income change education decile income change education decile education rise from decile to for income decile income change education decile income levels income change education decile education rise from decile to for income decile income change education decile education rise from decile to for income decile unexposed income decile education decile lattice structure we distinguish individuals according to or more attributes by which we categorize the population we have full or mostly parallel structure for these categorizations e g no matter in what income decile we are located we can progress in education except in boundaries we can gain or lose income no matter in what age category we are located we can progress through a similar set of stages of infection no matter in what sex category we are located we can age die etc a means to simplification subscripting we can simplify lattice structure by subscripting the structure by discrete properties this structure is then replicated for every subscript combination we can perform operations to create aggregate totals from this disaggregated data vensim model dit l iew ayout t iools t t elp cf c doc l w r i vf l t i i il m i i to op to op rn total population by sex by total population by total population by age sex total population by double subscript sum for total population total population by age total population by age runs l age specific fertility rates per iodo women for ethnicity total population by age ethnicity otal population b age ethnicity sex years in each age group fractional prevalence by age specific fertility age ethnicity sex diabetes incidence rate rates for ett ity by age ethnicity sex diabetic population by age total births for ethnicity births initial non diabetic population group non diabetic population l i incidentcasesofdiabetes i nti ialdiabte icc aess diabetic population diabetic by age eu j iabef deaths diabetic mortality rate by age sex ethnicity non diabetic mortality aggregate mortality rates by age sex ethnicity rate ratio for diabetics mean age f age catego hide times new roman yensim stratified demographic model mdl yar age specific fertility rat eile liew insert odel iools iindows t elp i i jb e i iq baseline subscripts c i vil i i vf f fi j il cim i fk ji c i y i j c totalpo t ion total population by age doc t _ total population agegroups idhnicgroups ses i subranges edit selected elements total population by le subscript sum total population total population by age runs age specific fer all none ageg ageg years in each age l per looo wo aiibutoldeslagegroup e tlmicil m iddleageg roups available elements t l j ageg group fractional prevalence o fd iaebt by age r ageg d agegroup15to19 agegroup55to59 ageg roup r keep on top r skip undefined ageg i j ageg r i ageg j ageg ageg ageg ageg ageg agegroup75to79 ageg clear seleded populat diabetic population diabetic population by age diabetic poi by age eh mean age f age catego example subscripted stock reading a subscripted equation suggestion read as follows variable total population for a specific age group member of agegroups ethnic group member of ethnicgroups and sex member of sex is just the sum of the non diabetic population for that same age ethnic sex group and of the diabetic population for that same age ethnic sex group antipattern unaided aggregation problem we are hard coding knowledge of our divisions of the population here this is fragile if we change that division later we ll have to remember to change this vector sum a larger vector sum some vector operators in vensim sum vmax vmin related elmcount gives count vector elm map entering constant data vensim provides some conveniences for entering subscripted constant data example single subscript a second way to enter constant subscripted data note tabbed array selection here we are entering data for combinations of age sex for a particular ethnic group a third way to enter subscripted data all in one line separate data for different values of inner subscript by separate data for different values of outer subscript by example initial population age sex 201 piecewise definition of subscripted equations frequently it is convenient define a subscripted equation in pieces each piece covers a particular set of combinations of subscripts as long as the union of these sets of subscripts covers all that has to be specified this is fine there is a dropdown to the right of the equation name that lets you choose which equation to view example 1 constant data equation number to view example stock equation each equation handles a range of ages here we are dealing with lowest age category need to deal with births here we are dealing with middle age categories need to deal with aging in and out here we are dealing with oldest age category no aging outflow next time capturing progression between subscripts subscript mapping subscript subranges subtyping subclassing a brief glimpse additional java tips nathaniel osgood cmpt recall a key motivator for abstraction risk of change abstraction by specification helps lessen the work required when we need to modify the program by choosing our abstractions carefully we can gracefully handle anticipated changes e g choose abstracts that will hide the details of things that we anticipate changing frequently when the changes occur we only need to modify the implementations of those abstractions recall defining the interface knowing the signature of something we are using is necessary but grossly insufficient if could count only on the signature of something remaining the same would be in tremendous trouble could do something totally different we want some sort of way of knowing what this thing does we don t want to have to look at the code we are seeking a form of contract we achieve this contact through the use of specifications recall types of abstraction in java functional abstraction action performed on data we use functions in oo methods to provide some functionality while hiding the implementation details we previously talked about this interface class based abstraction state behaviour we create interfaces classes to capture behavioural similarity between sets of objects e g agents the class provides a contract regarding nouns adjectives the characteristics properties of the objects including state that changes over time verbs how the objects do things methods or have things done to them encapsulation key to abstraction by specification separation of interface from implementation allowing multiple implementations to satisfy the interface facilitates modularity specifications specify expected behavior of anything providing the interface types of benefits locality separation of implementation ability to build one piece without worrying about or modifying another see earlier examples modifiability ability to change one piece of project without breaking other code some reuse opportunities abstract over mechanisms that differ in their details to only use one mechanism e g shared code using interface based polymorphism two common mechanisms for defining interfaces interface alone explicit java interface constructs interface defines specification of contract interface provides no implementation interface implementation classes using java class construct a class packages together data functionality superclasses provide interface implementations abstract classes as mechanism to specify contract define some implementation but leave much of the implementation unspecified we will focus on this what is a class a class is like a mould in which we can cast particular objects from a single mould we can create many objects these objects may have some variation but all share certain characteristics such as their behaviour this is similar to how objects cast by a mold can differ in many regards but share the shape imposed by the mould in object oriented programming we define a class at development time and then often create multiple objects from it at runtime these objects will differ in lots of parameterized details but will share their fundamental behaviors only the class exists at development time classes define an interface but also provide an implementation of that interface code and data fields that allow them to realized the required behaviour recall a familiar analogy the distinction between model design time model execution time is like the distinction between time of recipe design here we re deciding what exact set of steps we ll be following picking our ingredients deciding our preparation techniques choosing making our cooking utensils e g a cookie cutter time of cooking when we actually are following the recipe a given element of the recipe may be enacted many times one step may be repeated many times one cookie cutter may make many particular cookies cooking analogy to an agent class a cookie cutter we only need one cookie cutter to bake many cookies by carefully designing the cookie cutter we can shape the character of many particular cookies by describing an agent class at model design time we are defining the cookie cutter we want to use familiar classes in anylogic main class person class simulation class work frequently done with objects reading fields variables within the object setting fields calling methods to compute something a query to perform some task a command creating the objects distinction between class and object sometimes we want information or actions that only relates to the class rather than to the objects in the class conceptually these things relate to the mould rather than to the objects produced by the mould for example this information may specify general information that is true regardless of the state of an individual object e g agent we will generally declare such information or actions to be static example static non object specific method subtyping relationship informal we say that type a is a subtype of type b if we can safely substitute an a where a b was expected e g substitute in a person argument where an agent was expected by the parameter a subtype must be in some sense compatible with its supertype this compatibility is not merely a matter of signatures but also involves behaviour it is not possible for a compiler to verify the behavioural compatibility of a subtype supertype if we are expecting a b we should not be surprised by the behaviour of an a domain specific subtyping frequently we will have a taxonomy of types of objects classes that we wish to model people chiropractors physiotherapists licensed practical nurses registered nurses patients orthopedic surgeons radiologists we may group objects into classes but there are commonalities among the classes as well commonality among groups frequently one set of objects c is just a special type of another d all of the c share the general properties of the d and can be treated as such but c have other more specialized characteristics as well for example radiologists orthopedic surgeons are both types of doctors licensed practical nurses andn registered nurses are types of nurses chiropractors physiotherapists doctors and nurses are types of health professionals all health professionals and patients are types of people and share the characteristics of people e g susceptibility to aging illness and death example person interface might provide methods including but not limited to isinfected infect age sex in addition to the above a healthprofessional interface might provide a method recentpatients yielding patients seen by the prof over a period of time e g the most recent year the doctor interface might further provide a method residencyinsitution health professional hierarchy some benefits of type hierarchies polymorphism we can pass around an object that provides the subtype as an object that provides the supertype e g any method expecting a person argument can take a doctor radiologist understanding capturing specialization hierarchies reuse code can be written for supertypes but reused for subtypes extensibility open closed principle ideally no need to modify code of superclass when add a subtype polymorphism we can pass around an object that provides the subtype as an object that provides the supertype polymorphism enables decoupling of apparent type actual type programming against apparent type interface dispatching is against actual type e g reference to dictionary but actual object is a hash table anylogic subtyping relationships anylogic models are built around a set of classes with subtype relationships to each other the presence of these subtype relationships allows us to pass instances objects of a subtype around as if it an instance of the supertype one anylogic hierarchy nodes colored in blue are built in to anylogic the other nodes could be generated automatically e g person bird deer or built man woman buck doe as part of a model other anylogic hierarchies transitions in statecharts model experiments experimentsimulation o java util type hierarchies java io type hierarchies subtyping anylogic objects one of the most powerful ways of customizing anylogic behavior is by subtyping classes in anylogic that are either built in or auto generated examples resourceunit entity here instances of your class can circulate as if it an instance of the original class capturing hierarchies via subtyping we can capture a hierarchy such as that in the previous slide by defining interfaces each interface would specify the methods that are to be supported by any object that provides supports that interface setting up subclass relationships of these interfaces through the use of the extends keyword scoping when information is placed in a certain context e g within an object or static things in a class we have to retrieve it from those places subclassing subclassing is a special type of subtyping that also allows the subtype to reuse inherit the implementation of the supertype this means that to achieve a small modification for the supertype behavior the subtype doesn t have to go through and re implement everything that is supported by the supertype subclassing brings two things subtyping provides e g polymorphism code reuse via inheritance of methods fields interfaces contrasting tradeoffs class based inheritance advantages more flexible capture non hierarchical relationships easily added to definition of an existing class enables mixin like style cleaner type inheritance hierarchy disadvantages cannot easily extend existing interfaces no default implementations can be provided advantages easier extension with new functionality permits implementation reuse disadvantages subtype constraint lsp violation desire to reuse code can lead to deliberate ignoring inheritance can lead to accidental violation violation of open closed principal distort inheritance hierarchy abstract classes pushed up combinatorial explosion for dual interfaces single inheritance limits to tree multiple inheritance is dangerous semantically tricky confusing some items adapted from bloch effective java pearson education network with multiple agent classes realizing multiple agent classes sharing same network create an agent superclass create multiple subclasses of that superclass in properties indicate that extends superclass provide constructor to associate with agent population main class for the agent population use a replication of create startup code for main that adds the various types of agents to the model this uses code adopted from java code output by build common problems references to concrete classes leads to multiple changes for a simple conceptual change can be fixed by consistent programming against interfaces claimed subtypes are not behavioural subtypes of supertypes subclassing for code reuse or mistaken notion of specialization is a causes flawed design defects we ll comment on these fraudulent subtypes when building a subtype class hierarchy we specify tell the compiler which units are subtypes of which in java this is specified using implements extends the compiler generally accepts user information on type structure at face value full checking is not possible limited checking e g on signatures errs on the side of being conservative may report error even in cases where legitimate e g incompatible signatures it is very easy to create a subtype that is not a safe behavioural subtype of its alleged supertype subclasses a particularly common type of fraudulent subtype misplaced use of subclassing can very easily create classes that are not subtypes when such fraudulent subclasses are used with polymorphism the code can break easily two prime ways in which code can break implementers deliberately chooses subtype behaviour that makes it behaviorally incompetible with the superclass type supertype implementers try to make this a behavioral subtype but don t have the necessary guarantees on superclass implementation later why are fraudulent subclasses so common subclassing is abused as way to reuse code via inheritance this is a matter of convenience want to avoid redefining a broad set of methods just to override a few classes are used to group a set of objects where an is a relationship applies but which are not behavioural subtypes e g square is a type of rectangle liskov substitution principle principle is key to recognizing a legitimate subtype relationship the principle reflects the need to reason safely about types in the presence of polymorphism statement of principle liskov wing let q x be a property provable about objects x of type t then q y should be true for objects y of type s where s is a subtype of t persistent metaphor service contracts desire for encapsulation clear understanding of what is guaranteed example franchise of delivery service question given parent company guarantees what must a franchise offer to be legitimate precondition condition for guarantee to hold parent company customer must drop off package by noon ok franchise customer can drop off up to illegal franchise customer must drop off package by postcondition service guarantee if precondition met parent company delivery is by the next day ok franchise delivery is by noon the next day illegal franchise delivery is by next year contract hierarchy liskov substitution principle intuition consider a situation in which a programmer is creating code with a variable v whose apparent type is actual type is a subtype of due to polymorphism to avoid risk this code will have to be changed with every new subtype of it is critical that anything the programmer can rely upon for a variable of type is also true for v despite being of type any type that which departs from the contract of can break this code bear in mind that other code may treat v as a parameterized types via generic classes and interfaces java supports parameterized types here the definition of one class can be defined with respect to an arbitrary number of classes that are provided via type parameters examples arraylist classname set classname this is an array list and set that can hold any type of classes as specified by classname a given use of such a generic class will specify a specific class name for the type parameter e g set person arraylist double list deer the definition of the generic can restrict the types that can be used for the type parameter via constraints examples of type parameterization in anylogic experiment mainclass and other experiment classes resourcepool resourceunit networkresourcepool resourceunit activeobjectarraylist activeobject typically used among other things for the population in a main class activeobjectlist activeobject defensive programming naming conventions formatting separate commands side effects queries pure don t do side effects in e g macros mark temporary code e g scaffolding using a convention avoid manifest constants consolidate condition checks in methods or objects specification pattern minimize variable lifetime span between references use dog tags to recognize overwrites double deallocation check return values value legality display results of successive language processing naming conventions always handle all cases even illegal overriding default methods as a rule always put in after if beware empty catch blocks use finally blocks don t reuse temporary variables initialize vars member data as they are declared or in constructor use pseudocode programming process other suggestions strive for transparent code use variable name conventions consistent formatting strive for higher abstraction level spot commonality use explicit in and out parameters use restrictive modifiers const private protected encapsulation information hiding program to interfaces design by contract use type abstractions generics delegate use enumerations encapsulate repetitive actions move whole partial conditionals to methods bad smells many from mcconnell code complete duplicate code long routine deep long if loops inconsistent interface abstraction lots of special cases poor cohesion too many parameters single update yields changes to many places keep on creating ad hoc data structures classes global variables primitive types need to update multiple inheritance hierarchies subclasses not really subtypes related items spread among multiple classes method deals more with other classes than its own need to know implementation of other class unclear name setup takedown code around call style convention naming conventions commenting metadata e g javadocs indentation module naming construct placement compiler pragma mechanisms naming conventions naming conventions are a powerful tool benefits reduce risk of errors easier understanding of others code easier understanding of code in future lower risk of name clashes easier search for desired item e g method variable class java naming conventions distinguish typographic grammatical packages short lowercase alphabetics digits rare start with organization internet domain name e g ca usask classes interfaces first word of each capitalized taghasher avoid all but most common abbreviations generally nouns noun phrase interfaces sometimes adjective java naming conventions method fields same as classes but first letter lowercase const static fields all uppercase as separ action methods named with verb is for booleans query noun noun phrase or verb w get prefix converters tox primitivevalue local variables same as members but can be short context dependent scope naming conventions mcconnell code complete booleans base name should give clear sense of condition in question use common convention to indicate boolean f prefix e g fopen is prefix e g isopen suffix e g open legal scheme avoid negation in names e g isnotopen loop etiquette make clear what iterating over label index variables with the type of thing being iterated over avoid overly deep loops confusion control flow break continue placement of items in loop consider making internals of loop a separate method function enumerations enumerations help avoid manifest constants group common names good for bitwise operations consider values that will allow this rather than combinatorial names if language does not support enumerations use carefully named global constants leverage compiler checking if no class prefix consider naming enumeration values with prefix giving type enumeration make default enumeration value illegal always explicitly handle all values example of enums in anylogic eil e d it l iew del in do w l e l p i rqj c o l i y h qi co get s uppo rt t p roject n i el person j el ll pa lett e n i el tbri kfa cto rs j mu lt i p le agent cia ss e sln n e tw o rkl co lo r i t mod el i d abm m od elwit h birth dea th circ lesize pa ra m et er main suscept i b l e j flo w aux var iab l e p e rson ci cl e siz e simu lat io n ma in i d stoc k var ia b le ev e nt f fective j dynam i c ev e nt ap pearancet ime plain va riab le isln it iia ll y l nfe cted 0 sex 0 initi a lag e coll e ct i o n var iab l e p re gnan cyst at us 0 ethn ici ty 4 cu r e ntage g function final iz e dea th tab l e funct io n non pr eg n a nt l port fe rtili ty rat e ag esexethn ic ity a conn ecto r p e rf o rm birth e stab li hoff sp ringcon n e ct io ns ba sedonmoth e rsconn e cti o ns e stab li hoff sp ringl ocat i o nba e donmoth e rsl oc at io n ie prop e rtie n l i ill i v el tran iti o n initial stat e po int e r branc h h i st o ry stat e person active object class final st ate g ene ra l adr a ncedl po rt ecti o n en v iro nm e nt agent w e nd sin gle act iv e ob ject or agent ubc la ss parameters ad io n descr i pt i o n imp l e m ents com ma se parat ed li t of int e rfa ces iui analy is add iti o na l class c od e ii prese n t at io n pub li c e n llill se x male female co nn ect iv ity pub l ic e n wll e t hn icity fi r st na t i on metis e asta i an sou tha i an ca pub li c t a t ic j a v a u t il random random new j ava u t i l random t l ill i i s e le ct i o n m o re libr a ries a closer look use of enums to delineate possible parameter values use of enums to delineate possible parameter values generating random possible values the associated code use modifiers use const or final including for parameters in java to prevent side effects examples prevent modification to this in method prevent assignment to parameter poor man s option use const in name static can prevent needless memory use process complexity a barrier to quality system dynamics modeling medium scale sd projects generate a large diversity versions of related artefacts careful coordination of these artefacts is important for ensuring quality insights efficient coordination is important for productivity existing tools offer limited support for such coordination difficulties limit what can be accomplished recall process suggestions use peer reviews to review code design tests perform simple tests to verify functionality keep careful track of experiments use tools for version control documentation referent integrity do regular builds system wide smoke tests integrate with others work frequently in small steps use discovery of bugs to find weaknesses in the q a process final java modifiers indicates that the value of a field cannot be changed indicates that method cannot be overridden static associated with a class only one variable associated with the class no how many objects of the class are circulating annotations access modifiers access modifiers public visible to fields modifiable by other classes in package or not private not visible to fields modifiable by by any other classes package default only visible to fields modifiable by other classes protected only visible fields modifiable by in this class subclasses annotations allows custom indications concerning program elements e g field declarations class declarations uses compiler processing advising deployment time custom runtime information availability syntax indicated by a word identifier with sign optional additional information custom annotations example example interface dataprovenance string originalreference string intermediatederivationlocation default string sourcepersonname interface uncertainty double stddev default 1 dataprovenance originalreferen ce tb control report intermediatederivationlocation historical tb data xls sourcepersonname nate osgood uncertainty stddev 3 5 double meanyearsbetweenrelapses annotation retention annotation information can be used at different time different levels of retention of annotation information are possible via the retention meta annotation the retentionpolicy enum options source only preserved during compilation class presenrved in class information but not necessarily available at runtime runtime annotations are preserved in class representation are available at runtime for access via reflection except for local variables which are not preserved annotations with compiler support override compiler issues error if not found to be overriding method deprecated compiler warns when used suppresswarnings can instruct compiler to suppress one or both of common types of warnings valuable uses of annotations documentation authorship information revision information data provenance pedigree capturing intentions consistency with verifying that goal is being met e g that are in fact overriding cmpt simulation principles cmpt special topics in operations research modeling for decision making course contents this course will introduce students to the theory and practice of dynamic modeling and simulation to inform decision making in addition to covering elements common to diverse dynamic modeling traditions including the challenges of framing dynamic hypotheses at varying degrees of abstraction calibration sensitivity analysis parameter estimation monte carlo method and issues of metalinguistic abstraction the course presents compares and contrasts essential elements of three popular types of modeling for decision making agent based modeling system dynamics modeling and discrete event simulation given that this is a graduate course the need to substantively discuss the three types of modeling and the fact that there are extensive online tutorials available offering training on use of the anylogic software to accomplish many common tasks most notably the youtube videos from the instructor see reference resources many sessions of this course will use an inverted classroom model where students will be required to review relevant material prior to each such session and most classroom time for these sessions will be devoted to activities that take key advantage of the shared classroom experience these include but are not limited to addressing student questions regarding the materials assisting students with exercises and especially with advancing their projects work on and completion of a project as described below will constitute a central component of the course reference resources while the course will involve some lectures this graduate course will limit the amount of training provided and will tend to place greater emphasis on using the classroom time to take advantage of the unique opportunities available by in person meetings students are required to take time outside of class sessions to review only materials while the anylogic software comes with some excellent tutorials students should refer to the following materials which include detailed training sessions http tinyurl com consensusabm http tinyurl com ncbootcamplinked http www cs usask ca faculty classes index html reference books there is no required reference book for the class interested students may find the following books of value in understand elements of agent based modeling railsback s f and grimm v agent based and individual based modeling a practical introduction princeton princeton university press isbn 2012 sterman j business dynamics boston mcgraw hill higher education software anylogic topic plan key concepts to which students will be exposed in this class include those from programming languages domain specific languages metalinguistic abstraction event based generative and declarative programming mathematics state space classes of networks parameter spaces glimpses of complexity theory dynamic systems theory ordinary differential equations and numerical integration and software engineering application of ideas from dependency injection class and functional abstraction encapsulation and interface based programming for medium and large scale models peer review and testing uml diagrams java tutorials will be available online and in specially scheduled sessions for students not from computer science background student evaluation the proposed weighting for coursework varies for the two co located courses is as follows please note that the estimated due dates may be off by a few days and that additional material is anticipated for some graduate assignments cmpt option deliverable detail mark estimated due date participation in classroom and office hours n a assignment system dynamics exercise feb assignment agent based modeling exercises feb assignment agent based sd modeling exercises mar assignment agent based modeling exercises mar pop quizzes throughout term in class tbd final exam closed book tbd cmpt option deliverable detail mark due date participation in classroom and office hours n a term project part informal description of area in which you d like to wor your model informal estimate of scope of model to be implemented further details on data sources planned f use causal loop and or systemstructure diagrams tbd term project part informal mapping diagrams stock and flow statechart object diagrams showing hierarchies etc with equatio descriptions of scenarios sensitivity analyses that you plan to investigate tbd pop quizzes tbd term project part final report end of term presentation tbd final exam closed book tbd cmpt option deliverable detail mark due date participation in classroom and office hours n a assignment system dynamics exercise tbd assignment agent based modeling exercises tbd term project part informal description of area in which you d like to wor your model informal estimate of scope of model to be implemented further details on data sources planned f use causal loop and or systemstructure diagrams tbd assignment agent based sd agent based modeling exercises tbd term project part informal mapping diagrams stock and flow statechart object diagrams showing hierarchies etc with equatio descriptions of scenarios sensitivity analyses that you plan to investigate tbd assignment agent based modeling exercises tbd pop quizzes tbd term project part final report end of term presentation tbd cmpt option deliverable detail mark due date participation in classroom and office hours n a term project part informal description of area in which you d like to wor your model informal estimate of scope of model to be implemented further details on data sources planned f use causal loop and or systemstructure diagrams tbd term project part informal mapping diagrams stock and flow statechart object diagrams showing hierarchies etc with equatio descriptions of scenarios sensitivity analyses that you plan to investigate tbd pop quizzes tbd term project part final report end of term presentation tbd please note that a significant fraction of students grades will be based on class participation in recognition of differences in communication styles and interests among students this participation score will reflect interaction in class and in office hours and where appropriate tutorials the contents of the term project will be decided under discussion with the instructor topic plan a preliminary schedule for topics is included below this schedule is tentative and subject to change as student progress and discussion direction warrant note again that many sessions may be given in an inverted classroom model that requires student watching a video ahead of time major section focus topics for discussion in class anticipate d date introductory topics orientation motivation what to expect what not to expect from this course complementary nature of systems based and reductionist approaches emergence and systems effects generating vs hardwiring patterns sep first hands on exposure experiential encounter with some of the essential ideas of modeling simulation sep dynamic modeling process including the overview of qualitative mapping model scoping conceptualization formulation model testing model use phenomenology pattern oriented modeling incremental development ongoing docking as a key to insight and containing process complexity sep later revisit as time allows scoping a model scoping a model tensions causal interactions both structured and random between the parties higher level environmental influences generating vs hardwiring patterns key question what is likely to be important to answer the research questions being asked endogenous exogenous ignored classification interaction scenarios sep later revisit as time allows breadth vs depth opportunity costs the critical role of incremental development system dynamics compartmental modeling system dynamics modeling essentials feedback focus looking endogenously for the causes of behavior structure drives behavior spectrum of qualitative to quantitative modeling stakeholder involvement in the modeling process incorporating less tightly known values models as thinking tools sep model mapping causal loop diagrams basics of qualitative and semi quantitative sys thinking diagramming feedback rich systems influence causal loop diagrams causal loo diagram examples limitations of causal loop diagrams sep stocks flows basics structure as shaping behavior stocks as accumulations stock behaviour as a function of flows sep mathematical underpinnings of stocks and flows state equation formulation of stock flow diagrams as ordinary differential equations simulation as numerical integration value of a formal mathematical framework sep stocks flows first order delays relation to stochastic processes exponentially distributed residence times sep via video stocks flows higher order delays competing risks erlang distributions sep via video delay driven oscillations sep non linearity complex emergent behavior as resulting from tipping points equilibria non linearities delays impact on dynamics simulation reasoning about management and interventions location of equilibria sep application of stock flow reasoning influence infection spread essential dynamics of contagion herd immunity sep additional mathematics of contagion assessing stability and regions of stability in parameter space endemic and disease free equilibria separate tutorial around sep graduate students only agent based modeling specific motivations for abm network spatial context heterogeneity individual decision making preferences and longitudinal information stakeholder understanding sep abm specific issues thinking in a structured way about the state of an agent distinguishing static from dynamic representing inter and intra individual feedbacks in agent based models key problem features motivating agent based approaches multi level interactions high level summary measures statistics oct anylogic abm core concepts core concepts for creating runnable agent based models separation of models scenarios experiments design vs execution environment the notion of builds and components with representation at both design execution time use of multiple types of uml diagrams to sketch agent class relationships agent interactions oct computational architecture of agent based models key classes for all types of models main agent classes presentations experiments relationship of classes run time objects design time objects expressing agent heterogeneity with subclassing vs parameterization oct computational components parameters state variables methods functions inversion of control the hollywood principle action handler based code fragments entering leaving state at transition startup destruction of main person objects oct agent characteristics heterogeneity parameters variables dynamic static creating heterogeneous populations oct specifying discrete intra agent dynamics statecharts uml state diagrams transitions branching sources sinks transitions rates timeouts conditions message based oct discrete inter messages broader motivations in oct via agent dynamics computer science sending handling contagion of pathogens ideas network neighbour selection options video events event driven programming static events dynamic events their uses event scheduling the role of the scheduler oct environments networks their relationships to populations topological network geometric regular irregular spatial context oct via video network environments networks structure and dynamics network embedded agents anylogic built in network types and creating custom networks network classes poisson random ring lattice small world networks scale free networks mathematics of scale free networks impact on network dynamics impact of network fitting and aggregation on results oct via video population structural dynamics implementing dynamically varying populations dynamic networks oct the embedded environment focus on regular spatial embedding regular spatial embedding cellular automata neighbourhoods inter agent communication color boundary conditions alternative tessellation schemes discrete continuous geometries oct agent mobility movement regular vs irregular discrete vs continuous oct discrete event modeling a glimpse of discrete event modeling simulation basics the notion of process centered modeling entities resources queues situations under which this is desirable oct taxonomy of resources and resource pools moving portable fixed resource interchangeability oct visualization representation of elements identification of visual representation with entities resources paths for movement irregular geometries oct cross methodology topics metalinguistic abstraction graduate students only domain specific languages describing the what vs the how declarative languages generative programming separate tutorials around oct customizing model appearance before during execution creation of custom user interface elements to present data via graphs tables dictate assumptions via user input widgets sliders checkboxes text fields etc o initial assumptions e g regarding parameter values o live changes to assumptions intermixed starting sep analyzing reporting data reporting data for analysis built in statistics custom statistics stratification event use visualization in state space oct model debugging distinguishing the failure from the fault tracing isolation binary search common defects remote debugging via eclipse basic principles practices of debugging tips and techniques for effective debugging debugging build problems mechanisms for logging tracing multiple debugging methodologies as possible sensitivity analyses parameter structural automated manual one way multi way parameter uncertainty model uncertainty stochastics uncertainty in evolution over time histograms understanding the dynamic response of the model under uncertainty empirical fractiles plots of per realization behaviour nov model parameter estimation diverse sources of parameters challenges of network data opportunities for employing novel data collection mechanisms tie in with sensitivity analysis nov calibration adjusting assumptions to best match nov via historical data behavioural reference modes basic concepts of calibration o parameter space o objective function the implied error model mle o weighing multiple matches o optimization algorithm assessing convergence handling pathologies calibration differences between stochastic deterministic models video multiple systems science methods complementary strengths and weaknesses building anylogic models incorporating multiple modeling approaches stocks flows in abm why how using several sorts of models together as synergistic lenses presentation of four effective hybrid modeling strategies when to best characterize structured individual level processes within an abm or within a discrete event model nov additional coverage of hybrid systems science methods graduate students only separate tutorial building up types of hybrid modeling strategies around nov by video modeling for dynamic decision problems linkages of simulation models with other modeling techniques such as multi attribute decision theory theoretical and practical basis for tying together decision trees simulation models nov best practices overview and process oriented peer reviews version control versioning check in commit check out locking incremental delivery yagni principle continuous integration smoke testing refactoring unit checking brief exposure to principles of testing test drivers harnesses mocks fakes unit tests risk management continuous process improvement nov best practices technical eliminating manifest constants assertions abm as realization of mathematical process documenting mathematical description checking adherence of code to mathematical specification reducing software engineering complexity via modularity model transparency trajectory recording abstraction using methods classes separating interface specification implementation commenting using intention revealing naming conventions individual versioning mocks fakes incremental delivery change configuration management exceptions vs return codes clean coding suggestions test first coding nov dimensional reasoning and dimensional homogeneity testing understanding and using dimensional systems dimensions and unit expressing dimension of a quantity as a product of powers of dimensions the impact of the power of a dimension on unit change for that dimension the privileged character of quantities of unit dimension dimensionless quantities dec dimensional analysis de dimensionalizing and scale models identification of parameters of unit dimension by defining known blocks of dimensional matrices and solving for remaining block buckingham theorem capacity to lower parameter count by de dimensionalizing risk of changing parameters of non unit dimension creating scale models so as to enable controlled experiments dec nb the schedule above is subject to change on occasion updated schedules may be provided lectures slides will be provided via the course website on moodle we will seek to have all course sessions recorded as screencasts for later viewing on youtube project the completion of a project forms an important but optional part of the class the project type and topic will be chosen by the students in conjunction with the instructor because of the steepness of the learning curve and the benefits of assembling team members with complementary backgrounds we recommend pursuing group projects with or students projects should have a client associated with them individual projects may be pursued with instructor permission we also strongly recommend projects which have an external client contact information for clients associated with many possible project are included in a separate document based on verbal discussion and on a series of incremental deliverables submitted during the term the instructor will help guide the students on the project please note that these early deliverables for the project may be informal and rough while all deliverables should attempt to communicate material no focus on creating a careful presentation of these materials is required prior to the final report for the project the project emphases may vary but most projects should include the following creating one or more simulation models that describe these phenomena or adapting a particular model to a different context placing data into the model to customize it to a particular context running a baseline scenario with the existing model parameters and commenting on its plausibility running one or more what if scenarios with the model to explore different possible situations these situations could reflect the results of implementing different policies or different possible external conditions performing one or more sensitivity analyses in which assumptions in the model in the form of parameter values or structural elements of the model are changed a well structured written report describing the above see below an in class minute presentation where you summarize the model its construction and findings please note that while most projects will fall into the framework described above the instructor is open to the idea of students exploring alternative methodologically oriented project types these could include but are not limited to investigation of meta level issues in modeling e g challenges in characterizing a certain dynamic phenomenon in different modeling frameworks or an analysis of or proposal for innovations involving the modeling tools and methodologies used for simulation modeling if you wish to pursue one of these other types of projects please discuss your interests with the instructor final report the emphasis for the final report will be on quality of reflection and depth of the comments rather than on length conciseness in text is a virtue while bearing this in mind it would be somewhat unusual for the length ignoring figures to be less than pages or more than pages the level of the report should be such that it would speak to other course members the report should include an abstract the outline below provides an idea of what the report could cover it does not specify what is required in the report nor is it comprehensive the template should be used to stimulate ideas not as a rigid template or requirement please note that while the sections below are numbered individual reports are not expected or required to use this numbering system a given document may well omit certain of the sections below based on the specific circumstances of that project there are also quite a few considerations that could be fruitfully included in many reports that are not reflected below the project emphases may vary but many projects would include the following abstract mandatory background while the instructor often does not require this knowledge per se it can help convince the instructor that you have a strong grasp of this material it can also help frame the scoping of the model and of the scenarios this should characterize factors such as a motivation b context c goals of model is this a model primarily geared at providing higher level insights without focusing on a particular external situation or does it aim to characterized some particular external situation research questions hypotheses of greatest interest model this section of the report describes the model that was created a model scope high level description of what is included considered and left out of the model i would suggest use of the endogenous exogenous ignored distinction introduced in the first or second lecture of class b model architecture this should address the choice of modeling approach or approaches e g system dynamics agent based modeling discrete event modeling c model formulation for an agent based model you might consider using the odd protocol to high level this could include stocks flow diagrams state charts a description of the various sectors views in the model or various agents or in discrete event modeling networks represented variables or parameters associated with them key outputs and dependencies on any input data discussion of how the initial state is selected what particular situation if any is depicted in the initial state of the model lower level formulation of key transitions or flows within the models discussion of general approach used calibrations any calibrations by hand or automated performed sensitivity analyses these could be a structural sensitivity analyses certain areas of the model are altered or disabled b parameter sensitivity analysis c extreme value tests conducted as part of model testing or out of curiousity these would test or seek to understand model behavior in extreme conditions e g where one or more regions of the model are disabled scenarios a scenario descriptions description of one or more what if scenarios with the model to explore different possible situations describe the a baseline scenario and the motivation for choosing it alternative situations examined these situations could reflect for example a the results of implementing different policies b different possible external conditions c understanding behavior b findings from scenario runs findings related to the domain area from insights from the runs findings related to the model other findings learning insights and learning from different aspects of the project this could include for example learning about what would you do differently if you were to undertake this project again what other approaches might you take how might you change the scope or architecture of the model how might you approach the steps of modeling differently what different steps might you use in assembling the model a the domain area from insights from the runs b learning about technical challenges of modeling what was easy what was hard learning about tradeoffs between model architectures c learning about the process side of modeling for example what makes modeling easier harder what processes help lower risk of error what processes help identify errors earlier future work promising avenues for future work conclusions highest level key points that you d like to convey to someone from this project these could cover issues included anywhere in the above course syllabus cmpt computer systems and performance evaluation catalogue description provides a comprehensive overview of the quantitative aspects of computer systems with a particular focus on performance evaluation topics include performance measurement the analysis and interpretation of measurement data workload characterization and modeling the design and evaluation of performance experiments and the design and application of analytical techniques a variety of application domains will be considered prerequisite no formal prerequisite class time location m w 50 spinks website moodle course objectives and overview this offering of cmpt will provide an introduction to computer systems modeling and evaluation the course will cover basic concepts and techniques in this area with emphasis on their application rather than on the details of the underlying theory illustrative applications will be drawn from topics in networks and distributed systems including green computing internet content delivery structural characterization of online social networks internet traffic and application workload characterization and network protocol design student evaluation grading scheme there will be equally weighted regular assignments likely approximate due dates early october mid november and early december as well as a capstone assignment that will done in mid december assignments will include modelling and data analysis exercises the capstone assignment will also include short at most one page answer questions concerning the required readings students will be expected to complete a project on a mutually agreeable topic in the computer systems modeling and evaluation area with due date in mid december evaluation will also be based on the extent of participation in class discussions and short summaries of selected papers from the reading list the weightings for these components are as follows class participation and paper summaries assignments project capstone assignment note all students must be properly registered in order to attend lectures and receive credit for this course reference materials a reading list will be provided schedule of topics modeling workload and structure important types of probability distributions including pareto zipf exponential and others poisson processes network structural metrics correlation stationarity self similarity simulation models mainly we will focus here on the problem of assessing validity to what extent can the results from a simulation study be believed queueing models metrics and fundamental laws pasta and its application single resource queueing systems multiple resource systems state transition models discrete time continuous time flow balance equations problem of determining a suitable state description for a system fluid models motivation and comparison with other types of models assessing validity the following problem models a simple distributed system wherein agents contend for resources but backoﬀ in the face of contention balls represent agents and bins represent resources the system evolves over rounds every round balls are thrown independently and uniformly at random into n bins any ball that lands in a bin by itself is served and removed from consideration the remaining balls are thrown again in the next round we begin with n balls in the ﬁrst round and we ﬁnish when every ball is served a if there are b balls at the start of a round what is the expected number of balls at the start of the next round let xi be a binary variable indicating the event that bin i has only one ball for a given bin pr lim e n b b xi n e n n inf n where the distribution of balls is assumed to be poisson with a mean of m n n e x e i e xi l be n since balls that fall in a bin only by themselves are removed the number of balls at the start of the next round will be balls remaining b e n b suppose that every round the number of balls served was exactly the expected number of balls to be served show that all the balls would be served in o loglogn rounds using xj xj n and xj n x xj e n n x e n n let k xj n e k k true for k let be the initial number of balls then using n x n n x n xj let xk then all balls will be served in k rounds x logn logn logn logn logn k loglogn log logn since n k loglogn k o loglogn chapter exercise an undirected graph on n vertices is disconnected if there exists a set of k n vertices such that there is no edge between this set and the rest of the graph otherwise the graph is said to be connected show that there exists a constant c such that if n cnlogn then with probability o e n a graph randomly chosen from gn n is connected we can model this problem in the context of ball and bins by considering n bins and mean λ cnlogn n balls in every bin λ pr a single node is disconnected e e logn pr no single node is disconnected n e e n pr graph is connected pr no k nodes are disconnected k pr graph is connected e which is o e n for c mohamed aly christine chung mahmoud elhaddad scribe sherif khattab jonathan misurda cs randomness and computation homework due monday february chapter exercise page prove that for every integer n there exists a way to color the edges of kx so that there is no monochromatic clique of size k when x n n k k hint start by coloring the edges of kn then fix things up consider a coloring of kn chosen uniformly at random out of the possible colorings n fix an arbitrary ordering of the n k cliques in kn and for i n let xi be an x k xi then by linearity of expectations we have n e x e xi i n k k suppose we remove a vertex from each monochromatic k clique then the residual graph is a complete graph without monochromatic k cliques the expected number of vertices in the residual graph is n e x n n k k x then by lemma there is a coloring of kn where removing a vertex from each monochro matic k clique results in a ky y x that does not contain any monochromatic k cliques a prove that for every integer n there exists a coloring of the edges of the complete graph kn by two colors so that the total number of monochromatic copies of ka is at mist b give a randomized algorithm for finding a coloring with at most that runs in expected time polynomial in n monochromatic copies of c show how to construct such a coloring deterministically in polynomial time using the methods of con ditional expectations proof a each edge has probablity to be colored in each of the two colors there are edges in each so the probability that a is monochromatic is n the expected number of monochromatic is thus so that there exists such coloring such that the number of monochromatic is at most b by fliping a coin if it is head we use color otherwise we use color assuming we have probability p to get such a coloring then a consider a random walk on the dimensional integer lattice where each point has four neighbors up down left and right is each state transient null recurrent or positive recurrent give an argument figure a d integer lattice figure shows the infinite integer lattice on which we define each transition probability to be as we can see it is possible to go out a distance and then return back to the origin in some sense there is a probability of moving away from the origin at any step and a chance of making a beneficial step using this loose interpretation we can use a modified representation that is easier to reason about which we show in figure figure a chain representation of the lattice the chain fails to handle the case that you have moved to a state where there are multiple paths that can return you to in the same time meaning that the probability of moving to a better state is greater than for instance being in state affords two paths of length back to giving a proper transition probability of it stands to reason that it is more difficult to get back to in the chain of figure so if we can prove that this chain is recurrent we have proven it for the lattice following the procedure outlined under definition page to show that the lattice is recurrent and not transient we need to show that the probability of never returning to is zero the probability of never returning can be thought of as the probability of moving away from the origin each step notice this doesn t take into account loops where we come back and then go out again but asymptotically we will always be moving away an equation to represent this is j t which approaches zero as t grows infinitely large knowing that we must eventually return the next question is whether we return in finite time we can model the probability of returning at time t as t t we use the definition of hitting time as below t t t 16 4 since this converges and is not unbounded the chain must be positive recurrent we can apply the same reasoning as before to say that the integer lattice is positive recurrent since our chain makes it more difficult to return to the origin notice however that the exact value does not hold for the lattice being positive recurrent is surprising since the necessary requirement for a null recurrent state is an infinite set of states which we have a possible intuition is that due to the relatively high probabilities and the high connectivity that a random walk never travels too far from the origin b answer the problem in a for the dimensional integer lattice using the solution from part a we can make similar inferences about a dimensional lattice we modify equation to use the probabilities and and plug into equation this yields the finite result 6 which means the dimensional integer lattice is also positive recurrent at the first step of the algorithm we consider the n edges from vertex with i i d uniformly distributed weights and select their minimum with expected value lemma page at step we consider n edges those except the edge to vertex in general at step i we select the minimum of n i i i d uniformly distributed edge weights with expected value the last edge that completes the cycle has an expected value of thus the n i expected weight of the hamiltonian cycle is e cycle weight n e weight of edge added at step i e last edge n then we consider when edge weights are i i d exponential random variables with parameter at step i we select the minimum of n i i i d exponentially distributed edge weights with expected value lemma page the expected weight of the last edge is e cycle weight n e weight of edge added at step i e last edge n chapter exercise page a each arc starts at a point and its length is measured in some direction either clockwise or anti clockwise we first find the probability that an arc with length zi is longer than c ln n this happens when there is a gap of length greater than c ln n that is all the other n n points do not fall into the gap p r z c ln n n c ln n n e c ln n using the union bound the probability that one or more zi c ln n n is n nc because we have n arcs each with probability of exceeding c ln n n in length the probability that all zi are less than c ln n is thus one minus the above probability that a coloring c of an undirected graph g v e is an assignment labeling each vertex with a number representing a color from the set an edge u v is improper if both u and v are assigned the same color let i c be the number of improper edges of a coloring c design a markov chain based on the metropolis algorithm such that in the stationary distribution the probability of a coloring is proportional to λi c for a given constant λ pairs of states of the chain are connected if they correspond to pairs of colorings that differ in just one vertex the state space is all of the possible colorings of g the neighborhood of a state x which is a coloring of g is all the colorings that differ from x in exactly one vertex and is denoted n x this state space is irreducible under the markov chain since its graph representation h will be strongly connected i e any state in h is reachable from any other state in h we design a markov chain whose states are colorings of the graph g as follows start with as an arbitrary coloring of g let i 2 to compute xi a choose a vertex v uniformly at random from v b define xi v to be the color of v in xi c choose a color c uniformly at random from 2 d if xi v c and coloring v with c decreases i xi by k then let xi be xi with v colored c with probability min e if xi v c and coloring v with c increases i xi by k then let xi be xi with v colored c with probability min λk f if xi v c and coloring v with c neither increases nor decreases i xi then let xi be xi with v colored c with probability g otherwise let xi xi to show the probability of being in a coloring c is proportional to λi c for the above markov chain we have to show that πx λi x b is the stationary distribution of this markov chain i x where i x is the number of improper edges in coloring x and b x λ the algorithm in steps a and c first picks a node color pair at random from the m v possibilities to be the move under consideration note that m n x v if x is the current state and y is the state that we might move to by coloring the randomly picked node with the randomly picked color note that πy πx is λk if the number of improper edges increases by k in the move it is λk if the number of improper edges decreases by k in the move and it is if the number of improper edges doesn t change so the markov chain picks a specific y with probability m then moves from x to y with probabilty min πy πx also note that if x y then y is necessarily a neighbor of x since any coloring that differs in just one vertex is a neighbor therefore we have the probability that we will go from state x to state y in one step now by lemma 10 8 on page we have shown the stationary distribution is given by the probabilities πx random algorithm hw march a first it is clear that the extracted bits are independent since are independent on different i so we only need to show the ith bit has equal probability to be or this is actually also quite obvious since the probability for the original bias coin to generate a heads tails is the same as to generate a tails heads since they are independent trails b there are l n j pairs one pair will only generate a bit if it is a heads tails or tails head the expected number of bits one single pair will generate is p p p p p by the linearity of expectation the expected total number of bits will be l n p c y is a sequence of independent trials with head probability p p so from the result of a running a on y would produce bits that are independent and unbiased furthermore since the original trials are independent and the sequence x and y will generate bits on are mutually exclusive so the bits generated from y are independent from the bits generated from x d z is a sequence of independent trials with head probability p 2 so the bits extracted from z using a is independent and unbiased it is also independent from x and y since giving x and y will not affect z e from the result of b c and d the total number of bits generated is the sum of those generated from x y z so a p p rob xwillgenerateabit a x p rob y willgenerateabit a y p rob zwillgenerateabit a z p p a a 2 q2 2 f substitute the function a with the entropy function h the recursive function holds 1 the cocaine epidemic after the model showed persuasively that the survey data significantly underestimated cocaine use and highlighted the failure of the supply side strategy as and reuter p put it the probability of a cocaine or heroin seller being incarcerated has risen sharply since about but that has led neither to increased price nor reduced availability however a close look at the simulation in figure shows that by the late the number of people who had ever used cocaine though still rising was growing at a diminishing rate therefore the ini tiation rate must have been falling by the mid the epidemic began to abate the growth of cocaine related medical emergencies and deaths slowed arrests fell slightly the ondcp estimated net imports in at between and tons with metric tons seized leaving net cocaine available on the streets of america at about three quarters the level the model originally developed in the late forecast these dramatic shifts in cocaine use quite well fig ure note that the point by point fit of the model in the t perfect and you should not expect it to be simulated arrests are too high and the model does not track the temporary dip in cocaine related medical emergencies in never theless the model ability to capture the turning point in the epidemic from exponential growth to gradual decline is quite remarkable considering that the simulations shown in figure were based on data available only through the only exogenous inputs affecting model behavior after are the target population those age and over and the prevalence of marijuana use a proxy for social tolerance of drugs changes in data reporting systems and definitions were not included chapter dynamics of stocks and flows simulated vs actual cocaine p a dashed lines data lines model m history forecast source homer adding additional exogenous inputs could improve the fit to the data but models should not be tuned to fit data by introducing exogenous variables whose sole function is to improve the correspondence of model output to data exogenous variables must be justified by significant real world evidence independent of their potential contribution to historical fit further variables involved in any feedback loops judged to be potentially significant relative to the model purpose must be captured as part of the model endogenous structure and cannot be used as exoge nous inputs to improve historical fit part tools for systems while the model shows that the survey data overestimated the decline in co caine use model generated estimates of the actual number of active users while remaining significantly higher than the estimates reported in the surveys do show a decline the field research and model results showed the drop in cocaine use was not caused primarily by the supply disruption loop b in figure or by the clean up the streets loop as supporters of the interdiction policy claimed rather the exponential growth of cocaine use was eventually halted by two nega tive feedbacks involving public perceptions of cocaine health and legal risks first cocaine is not the benign substance it was thought to be in the as peo ple began to experience or hear about the negative health and social effects of the drug they became less likely to start and more likely to stop balancing loop in figure second growing legal risks of drug use due to higher arrest rates and longer sentences decreased the willingness of people to start and increased the quit rate the fear of arrest reduced usage balancing loop as the population of active users began to fall the social exposure of nonusers also fell weakening the reinforcing word of mouth loop unfortunately both of these negative loops involve long delays first there is a lag between growth in cocaine use and the incidence of harmful health and legal effects as the initiation rate grew exponentially so did the stock of active casual users the stock of compulsive users also rose exponentially though with a sub stantial lag the lag in the growth of the compulsive user population is important because compulsive users are more likely to experience severe health effects es pecially as they turn to crack and more likely to commit drug related crimes in cluding pushing the drug to finance their own habits thus the exponential growth in cocaine related crime arrests medical emergencies and deaths lags behind the growth of the casual user population which in turn lags behind the initiation rate there is a further lag in the perception by the public of the true health effects of cocaine most people don t read the new england journal medicine or the annals addiction to learn about the health risks of illegal drugs instead public perceptions of risk are strongly conditioned by personal experience personal acquaintance with someone harmed by cocaine and media reports of high profile individuals who were arrested for injured by or died from cocaine use such as the comedian richard who was severely burned while freebasing or the uni versity of maryland basketball star len bias who died of acute heart failure while doing cocaine to celebrate his selection as a top draft pick by the boston celtics of the national basketball association the strength of all these channels of public awareness therefore lags behind the population of active users driving the growth of the epidemic exponential growth in cocaine use did eventually reduce the social acceptability of the drug and thus the initiation rate however the stock of active users lags well behind the ini tiation rate the stock of active users will rise as long as initiation exceeds the rate at which people stop using and the stock of compulsive users increases as long as the escalation rate exceeds the rate at which compulsive users stop the dynamics of the stock and flow structure inevitably mean that the population of drug users especially the compulsive users responsible for most of the crime and health ef fects continues to grow even after the initiation rate peaks and falls the delay en sures that the reinforcing social exposure and word of mouth feedbacks dominate chapter dynamics of stocks and flows the negative risk perception loops in the early years of the epidemic leading to a later and higher peak for incidence and prevalence still by the late nearly every community had experienced the arrest in jury or death of at least one of its promising young people slowly strengthening the negative feedbacks that slow the initiation rate ironically the cocaine epidemic did not abate because interdiction made the drug less available on the contrary the data showed growing accessibility purity and affordability throughout the instead the very abundance of cocaine by leading to a large increase in personal knowledge of its harmful effects led people to turn away from the drug no longer chic stripped of its social aura and benign image those who craved escape from the world turned from cocaine to other drugs thus the cocaine epidemic was ul timately self limiting the feedback structure outlined in figure is quite general and applies to any harmful drug legal or illegal the positive feedbacks generating growth in us age act swiftly while the negative feedbacks that deter usage particularly public recognition of a drug harmful effects are only perceived slowly the result is the characteristic boom and bust pattern for drug use each new or newly popular drug generates a wave of naive enthusiasm in which users extol its benefits only to dis cover as the population of users grows and more people escalate to compulsive use that the drug isn t as benign as people were led to believe in fact the cocaine epidemic of the not the first a similar boom and bust in cocaine use occurred in the late it began with medicinal use as co caine was praised by the medical community including freud in his famous paper on coca as a cure for opium addiction alcoholism fatigue depression nervousness timidity impotence and seasickness among other complaints fol lowing the classic pattern cocaine moved into more general and recreational use becoming an ingredient in coca cola and some cigarettes as use spread avail ability and purity increased instead of injecting or the preparation pow der for snorting became popular soon the harmful effects began to be experienced observed and reported in the medical and popular press by the early co caine use had spread from social elites to lower social classes communities across the country struggled to deal with compulsive users known as coke fiends and by atlanta police chief was blaming percent of the crimes in the city on cocaine grinspoon and bakalar p in response legal restrictions and prohibitions grew increasingly severe in congress defined cocaine as a narcotic and banned importation of coca by state had restricted its sale and most made possession a crime cocaine use fell from its peak and remained low as people turned to other drugs until the current epidemic began similar waves of drug use have been repeatedly observed for the opiates for psychedelics and for various stimulants and barbiturates while epidemics of any particular illegal drug are ultimately self limiting if the drug is harmful enough people have always sought out mind altering sub stances even as one drug falls out of favor new epidemics begin centered on new drugs for which there is as yet no experience of harmful effects or on old drugs for which the hard won knowledge of harm gained by prior generations has faded from collective memory the modest decline in cocaine use in the led to an increase in the use of other drugs including marijuana methamphetamine and part tools for systems most troubling a resurgence of heroin use more than years after the last wave of heroin crested this latest heroin epidemic was stimulated by the usual reinforcing word of mouth and media feedbacks including the glorification of heroin chic in popular culture and calvin klein underwear summary this chapter showed how stocks and flows generate dynamics the process of ac cumulation is equivalent to integration in calculus the amount added to a stock in any period is equal to the area swept out by the net rate of change in the stock over that period conversely the slope of the trajectory of a stock at any time is its de rivative the net rate of change graphical methods for integration and differentia tion were introduced given the behavior over time for the rates affecting any stock you can deduce the behavior of the stock given the trajectory of the stock you can deduce its net rate of change all without use of calculus the ability to re late stocks and flows intuitively is essential for all modelers even those with ex tensive mathematics training because most realistic models have no analytical solutions examples show that understanding the dynamics of stocks and flows even without feedback can yield insight into important problems reading shreckengost developed a model for the us cia to estimate heroin imports by integrating prevalence crime price purity and other data gardiner and shreckengost shreckengost applies the framework to cocaine levin hirsch and roberts in the poppy develop a system dynamics model of heroin use and abuse in a community based on a case study of the south bronx they use the model to explore a variety of policy options in cluding demand side policies increased enforcement and methadone maintenance see also levin hirsch and roberts richardson develops a simple model to explain why aggressive police effort to seize street supplies of heroin actually increases drug related crime goluke landeen and meadows developed a model of addictive behavior focusing on alcoholism holder and develop a model of community level policy responses to alcoholism homer et al present a system dynamics model of tobacco and analyze a variety of policies closing the loop dynamics of simple structures a mathematical theory is not to be considered complete until you have made it clear that you can explain it to thefirst man whom you meet on the street davidhilbert i hope to show that mathematicalnotation can be kept close to the vocabulary of business that each variable and constant in an equation has individual meaning to the practicing manager that the required mathematicsis within the reach of almost anyone who can successfully manage a modern corporation jay w forrester p this chapter formalizes the connection between structure and behavior by feedback with stock and flow structures the focus is the simplest feedback sys tems those with one stock known as first order systems linear first order sys tems defined in this chapter can generate exponential growth and behavior nonlinearity in first order systems causes shifts in the dominant loops leading for example to s shaped growth the chapter also introduces the concept of a phase plot a graph showing how the net rate of change of a stock related to the stock itself and shows how dynamics can be derived from the phase plot without calculus or differential equations systems chapter discussed the basic modes of behavior generated by complex systems and the feedback structures responsible for them the most fundamental modes are part tools for systems figure growth and goal seeking structure and behavior increase state of the goal system desired discrepancy corrective action exponential growth and goal seeking positive feedback causes exponential growth and negative feedback causes behavior figure the simplest system that can generate these behaviors is the first order linear feedback system the order of a dynamic system or loop is the number of state variables or stocks it contains a first order system contains only one stock lin ear systems are systems in which the rate equations are linear combinations of the state variables and any exogenous inputs the term linear has a precise meaning in dynamics in a linear system the rate equations the net inflows to the stocks are always a weighted sum of the state variables and any exogenous variables denoted net inflow where the coefficients and are constants any other form for the net inflows is nonlinear feedback and exponential growth the simplest feedback system is a first order positive feedback loop in a order system there is only one state variable stock denoted here by the state of the system accumulates its net inflow rate in turn the net inflow depends on the for example formulations for the net inflow such as a a or are all nonlinear the term nonlinear is often used in other senses for example to describe the nonchronological narrative structure of novels such as hopscotch the term nonlinear in these contexts actually means nonsequential and has nothing to do with the technical meaning of linearity chapter closing the loop dynamics of simple structures state of the system for now assume no exogenous inputs in general the net in flow is a possibly nonlinear function of the state of the system inflow net inflow if the system is linear the net inflow must be directly proportional to the state of the system net inflow where the constant has units of and represents the fractional growth rate of the figure shows the structure of this system as a causal diagram and also as a set of equations as examples consider the accumulation of interest income into a bank account or the growth of a population the principal and prevailing interest rate determine the interest payment population and the fractional net birth rate determine the net birth what will the behavior of the system be section uses basic calculus to solve the differential equation the solution is the exponential function where is the value of s at the initial time t the state of the system grows exponentially from its initial value at a constant fractional rate of per time unit analytic solution for the linear first order system to solve the differentialequation for the first order linear system first separate variables to obtain gdt s now integrate both sides to get gt c where c is a constant exponentials of both sides gives the general case of a multistate system the rates of change are a of the state vector and any exogenous variables u u in a linear system the rates are linear combinations of the states and exogenous variables as bu where a and b are matrices of coefficients for good treatments of linear system theory see ogata and margolis and rosenberg population growth as a first order process assumes there is no delay between birth and the ability to reproduce a poor assumption for mammals but reasonable for many unicellular and other small organisms part tools for systems thinking net inflow rate examples interest rate net interest principal net interest income interest rate principal fractional net birth rate net birth population net birth rate fractional net birth rate population where is the value of at the initial time when is by defi nition so must equal substitution yields equation graphical solution of the linear first order positive feedback system you do not need calculus to solve the equation for the first order linear system you can also deduce its behavior graphically figure shows a third representa tion of the structure of the system a phase plot a graph showing the net rate as a function of the state of the system the graph shows that the net inflow rate is a straight line starting at the origin with positive slope g chapter closing the loop dynamics of simple structures plot for the first order linear positive feedback system net inflow rate unstable equilibrium note that if the state of the system is zero the net inflow is also zero zero is an equilibrium of the system no savings no interest income no people no births however the equilibrium is unstable add any quantity to the stock and there will now be a small positive net inflow increasing the state of the system a bit the greater state of the system leads now to a slightly greater net inflow and a still larger addition to the stock the slightest departure from the equilibrium leads to further movement away from the equilibrium just as a ball balanced exactly at the top of a hill if disturbed even slightly will roll ever faster away from the balance point the greater the state of the system the greater the net inflow this is pre cisely the meaning of the positive feedback loop coupling the stock and its net in flow in the general case where the phase plot of a state variable can be nonlinear the state of the system will grow whenever the net rate is an increasing function of the stock an equilibrium is unstable whenever the slope of the net rate at the equi librium point is positive because the rate equation in the example is linear the net increase rate grows exactly in proportion to the state of the system every time the state of the system doubles so too will its absolute rate of increase therefore the trajectory of the system in time shows an ever increasing acceleration figure shows the tra jectory of the first order linear positive feedback system on the phase plot and as a time series in the figure the growth rate is period and the initial state of the system is unit the arrows along the phase plot show that the flow of the system is away from the unstable equilibrium point from any nonnegative start ing point the state of the system grows at an ever accelerating rate as it moves along the line net inflow the accelerating growth is easily seen in the time system is symmetric for negative values of the state variable if s will become ever more negative at exponential rates in most systems however the state variables are restricted to nonnegative values there can be no negative populations part tools for systems figure exponential growth structure phase plot and behavior time plot the fractional growth rate g unit initial state of the system unit points on plot show every doubling time periods structure o behavior e a state of system units domain the slope of the state variable at every point is exactly proportional to the quantity in the stock and the state of the system doubles every time periods see section note that changing the fractional growth rate changes the slope of the line net inflow and therefore the rate of growth but not the exponential shape of the curve the power of positive feedback doubling times before continuing try the following challenge the rule of positive feedback loops are the most powerful processes in the universe their power arises from the fact that the rate of increase grows as the state of the system chapter closing the loop dynamics of simple structures grows when the fractionalnet increase rate is constant positive feedback leads to exponential growth exponential growth has the remarkable property that the state of the system doubles in a fixed period of time no matter how large it is in the ex ample in figure the state of the system doubles every time periods it takes time periods to grow from the initial value of to units and only time periods to grow from to or from billion to billion any quan tity that grows by positive feedback that doubles in a fixed period of time gets very large afterjust a few doublings you can readily determine the doubling time for any exponential growth process to do so solve equation for the interval of time that satisfies the equation when the stock has reached twice its initial value the result is where is the doubling the natural log of rounding to and expressing the fractional growth rate in percent per time period gives the rule thus an investment earning doubles in value after as shown in figure the average growth rate of real gdp in the us over the past years has been so the doubling time is roughly years the past years have witnessed doublings increasing the size of the us economy by roughly a factor of one thousand misperceptions of exponential growth while the rule of is simple and easy to apply the implications of exponential growth are difficult to grasp intuitively wagenaar and wagenaar and studied people ability to extrapolate exponential growth processes they found people grossly underestimated the rate of growth tending to extrapolate linearly instead of exponentially that is we tend to assume a quan tity increases by the same absolute amount per time period while exponential growth doubles the quantity in a fixed period of time when the growth rate and forecast horizon are small linear extrapolation is a reasonable approximation to equation through by yields that is the doubling time is in dependent of the initial size of the stock the natural log of both sides and dividing through by g gives rule of is based on the assumption that the growth process is continuous in time in the investment example the assumption is that interest is compounded continuously compounding at discrete intervals reduces the effective yield and lengthens the doubling time in discrete time equation no longer holds instead the state variable is given by where p is the compounding period for example p for monthly compounding when g is the interest rate per year the doubling time of the discrete time process is given by since for small the rule of remains a good approximation of discrete time positive feedback processes as long as the compounding interval is relatively short compared to the doubling time for example a process growing at a rate of compounded only an nually doubles in years compared to years when compounding is continuous using the exact value of to calculate in the limit as p part tools for systems exponential growth however as the growth rate increases or the forecast horizon lengthens the errors become huge how thick did you think the sheet of paper would be after folding it times after times most people estimate the paper will be less than a meter thick feet even after folds in fact after folds the paper would be kilometers thick more than the distance from the earth to the moon and after the paper would be an incomprehensibly immense trillion times the distance from the earth to the the underestimation of exponential growth is a pervasive and robust phenom enon wagenaar and colleagues found that the underestimation was robust to the presentation of the data in tabular vs graphic form surprisingly showing more data tends to worsen the underestimation and training in mathematics did not help wagenaar and the counterintuitive and insidious character of exponential growth can be seen by examining it over different time horizons figure shows a positive feedback process growing exponentially at a constant rate of period with four different time horizons by the rule of the doubling time time peri ods over a time horizon of one tenth the doubling time growth is imperceptible over a time horizon of one doubling time the growth appears to be close to linear over doublings the accelerating character of exponential growth is clearly vis ible over doublings it appears that nothing happens at all until about of the time has passed many people examining the behavior over the long time hori zon conclude that there must have been a dramatic change in the structure of the system around time in fact the same process of accumulation powered by positive feedback is operating throughout the entire history but only the last few doublings are noticeable of course no real quantity can grow forever because exponential growth dou bles in a fixed time interval positive feedback processes approach their limits rapidly and often unexpectedly meadows et al p illustrate with an old french riddle suppose you own a pond on which a water lily is growing the lily plant doubles in size each day if the lily were allowed to grow unchecked it would completely cover the pond in days off the other forms of life in the water for a long time the lily plant seems small and so you decide not to worry about cutting it back until it covers half the pond on what day will that be on the twenty ninth day of course you have one day to save your fold doubles the thickness of the paper a typical sheet of paper is about mm thick after two folds it is mm thick after five folds just mm after doublings the thickness has increased by a factor of trillion multiplying by the initial thickness of mm and converting to gives a thickness of km after folds the thickness has in creased by a factor of x multiplying by the initial thickness converting to ters and dividing by the mean earth solar distance of million miles million km gives a thickness of times the mean solar distance more than billion light years of course you would need a very large sheet of paper to carry the experiment through fact the lily pad would be microscopic for much of the days initially the lily covers only x of the pond area it reaches of the area only after the day part tools for systems thinking as various limits are approached nonlinearities always weaken the positive loops and strengthen the negative feedbacks until the exponential growth halts these nonlinear effects are illustrated by another famous story about exponential growth as told by meadows et al p there is an old persian legend about a clever courtier who presented a beautiful chessboard to his and requested that the king give him in return of rice for the first square of the board grains for the second square grains for the third and so forth the readily agreed and ordered rice to be brought from his stores the fourth square of the chessboard required grains the tenth square took grains the fifteenth required and the twenty first square gave the courtier more than a million grains of rice by the fortieth square a million million rice grains had to be brought from the storerooms the king entire rice supply was exhausted long before he reached the sixty fourth square in fact the total quantity of rice on all squares would have covered all of mod ern day iran to a depth of more than feet process point overcoming overconfidence consider again your answers to the paper folding challenge not only do people underestimate the thickness of the paper but the correct answers fall outside their confidence bounds almost all the time that is people are grossly overconfi dent in their judgments overconfidence is one of the most robust judgmental biases documented in the psychological literature in a careful review scott plous writes no prob lem in judgment and decision making is more prevalent and more potentially cat astrophic than overconfidence overconfidence means the confidence bounds people provide around their es timate of an unknown quantity are too narrow causing an unexpectedly high rate of incorrect predictions lichtenstein and fischoff found that people were to confident of being right in answering a variety of questions when in fact they answered correctly at only about the chance rate of such results have been replicated in a wide range of tasks and with experts as well as novices lichtenstein fischoff and phillips reviewed studies totaling nearly judgments and found that people confidence bounds contained the correct response only of the time an error rate times greater than expected extensive study of the relevant issue can actually worsen overconfidence os kamp showed that the more information people received about an issue the more confident they became while accuracy did not improve financial incen tives do not reduce overconfidence in some studies subjects were given the option of betting on their answers at the odds they estimated for being correct they con sistently lost money situations in which people confidence bounds are approximately correct are rare weather forecasters and professional card players are among the few groups whose judgments have been found to be well calibrated these are narrowly bounded contexts where the relevant factors are well known thousands of repe titions provide feedback enabling the meteorologist and gambler to learn from experience these conditions do not hold for most dynamically complex settings chapter closing the loop dynamics of simple structures including judgments about the likely behavior of feedback systems the causal structure relevant variables and parameters are highly uncertain and largely un known see chapter in most social and business situations the time delays are so long there is little or no chance to learn from experience by the time feedback is available it will be too late to take effective action many investors and money managers during the great bull markets of the and never experi enced a market crash and underestimated the likelihood of declining share prices and for issues such as global climate change the world must adopt policies re garding emissions of greenhouse gases decades before the full impact of human activity on global climate will be known overconfidence means you will proba bly buy too little insurance and are likely to wake up one day to find you have in sufficient coverage before the challenger explosion nasa estimated the risk of catastrophic launch failure at in as an illustration that experts are far from immune to overconfidence consider the debate over global warming the economist william nordhaus con ducted a survey of distinguished experts on global climate change to assess their views on the likely economic effects of global warming nordhaus asked the panel to estimate the loss of gross world product gwp caused by various amounts of warming the results showed a huge gulf between the estimates of the scientists compared to the economists scientists estimated the probability of a consequence event a catastrophic change in climate cutting gwp by or more as to times more likely than the economists did estimates of the most likely reduction in gwp were similarly bimodal with scientists generally estimat ing large impacts and economists generally estimating small or even positive im pacts of warming on the global economy no one knows which group is correct what is striking however is the small range of uncertainty each expert allowed each provided confidence bands around their best estimates yet in many cases these ranges were so small they excluded the majority of the other experts views economists who tended to predict small effects tended to have the narrow est confidence bands one economist wrote it is impossible to contemplate what society will be like a century from now as technology changes yet estimated that a c rise in global mean temperature by would produce a change in gwp ranging from to one of the smallest ranges offered by any respondent overcoming overconfidence requires greater humility about the limits of our expertise several techniques can help list all the reasons your opinion could be wrong try to identify the implicit assumptions of your mental model and consider how the outcome might change if different assumptions were used because iden tifying the hidden biases in our own mental models is difficult it is especially valu able to solicit the judgments and opinions of a diverse group of people especially those with opposite views your critics will usually be far more effective in help ing you calibrate and improve your judgment than your friends seek out and wel come their views you should be especially suspect of statements that something is absolutely certain inevitable without doubt or a one in a million chance espe cially if the situation involves human behavior or if people judgments require course nasa estimate could have been correct and the challenger disaster just extraordinarily bad luck while logically possible careful studies of the disaster belie that view part tools for systems mental simulation of dynamically complex systems when assessing the confi dence intervals provided by formal models or statistical analysis of data remem ber that the confidence bounds provided by statistical models measure only the uncertainty due to sampling error and not that due to specification error errors in the model boundary and in the maintained hypotheses of the statistical method these latter sources of error are typically much larger than uncertainty due to sam pling error when formal models are available conduct extensive sensitivity tests not only of the response to parametric uncertainty but also to uncertainty about the model boundary feedback structure and other structural assumptions negative feedback and exponential decay first order linear positive feedback systems generate exponential growth order negative feedback systems generate behavior when the system is linear the behavior is pure exponential decay the feedback structure responsible for exponential decay is shown in fig ure as examples consider the death rate of a population or the depreciation of an asset in both cases the net outflow is proportional to the size of the stock the equation for the net rate of change of the stock is net inflow net outflow ds where d is the fractional decay rate its units are the reciprocal of the frac tional decay rate is the average lifetime of units in the stock see chapter on de lays for a proof to deduce the behavior of the linear first order negative feedback system note that equation is the same as equation except that the negative of the fractional decay rate replaces the fractional net increase rate the solution is there fore given by the same exponential function with replacing g figure shows the phase plot for the first order linear negative loop system the net rate of change of the stock is now a straight line with negative slope d as be fore the point s is an equilibrium where there are no people there can be no deaths when the value of an asset has declined to zero no further depreciation can be taken unlike the positive feedback case the equilibrium is stable increasing the state of the system increases the decay rate moving the system back toward zero a system with a stable equilibrium is like an orange resting at the bottom of a bowl if you push the orange up the side of the bowl and release it it rolls back down until it comes to rest again at the bottom deviations from the equilibrium are self correcting figure shows the dynamics of the system on both the phase plot and in the time domain the fractional decay rate in the example is period and the initial state of the system is units initially the decay rate is pe riod the decay rate is directly proportional to the state of the system so as the state of the system declines so too does the decay rate the flow of the system de noted by arrows in the phase plot is always toward the stable equilibrium the dots on the phase plot show the location of the system every time units note how chapter closing the loop dynamics of simple structures first order linear negative feedback structure and examples general structure net inflow rate net outflow rate d fractional decay rate examples population death death rate fractional death rate population fractional death rate book value asset depreciation rate book lifetime lifetime phase plot for exponential decay via linear negative feedback net inflow rate net outflow rate part tools for systems thinking figure exponential decay structure phase plot and behavior time plot the fractional decay rate d unit initial state of the system units structure state of system units behavior time the adjustment is rapid at first and falls over time the state of the system falls at a diminishing rate as it approaches zero the exponential decay structure is a special case of the first order linear nega tive feedback system as discussed in chapter all negative feedback loops have goals in the case of exponential decay such as the death rate and depreciation ex amples the goal is implicit and equal to zero in general however the goals of negative loops are not zero and should be made explicit figure shows the gen eral structure for the first order linear negative feedback system with an explicit goal examples include the process by which a firm adjusts its inventory or work force to the desired level possible delays in changing production or hiring new workers are ignored including such delays would add additional stocks to the model in the general case the action determining the net inflow to the state of the system is a possibly nonlinear function of the state of the system s and the desired state of the system net inflow the simplest formulation however is for the corrective action to be a constant fraction per time period of the discrepancy between the desired and actual state of the system net inflow s where the parameter at is known as the adjustment time or time constant for the loop note the units of equation the net inflow has dimensions of period the discrepancy between the desired and actual state of the system has di mensions of units for example if the desired inventory of a firm is units and current inventory is only units the discrepancy is units the adjustment time represents how quickly the firm tries to correct the shortfall if the firm seeks to correct the shortfall quickly the adjustment time would be small for example the firm may set the adjustment time to at meaning that they would correct chapter closing the loop dynamics of simple structures figure first order linear negative feedback system with explicit goals general structure net inflow desired state of the system net inflow rate examples desired inventory nlet hiring labor desired labor force i at adustrrient time labor shortfall net hiring rate labor desired labor the inventory at an initial rate of being more cautious and setting at weeks would entail an initial net inflow of and an even more aggressive firm might set at weeks leading to an initial net inflow of inventory of of course these corrective actions cause the inventory shortfall to diminish reducing the net inflow over time until the discrepancy is eliminated the discrepancy can be negative as when there is excess inventory in this case the net inflow is negative and the state of the system falls part tools for systems the reciprocal of the adjustment time has units of and is equivalent to the fractional adjustment rate corresponding to the fractional decay rate in the ex ponential decay case the phase plot for the system figure shows that the net inflow rate to the state of the system is a straight line with slope and equals when s the behavior of the negative loop with an explicit goal is also exponential decay but instead of decaying to zero the state of the system reaches equilibrium when s s if the initial state of the system is less than the desired state the net inflow is positive and the state of the system increases at a diminishing rate until s s if the state of the system is initially greater than the goal the net inflow is negative and the state of the system falls at a diminishing rate until it equals the goal the flow of the system is always toward the stable equilibrium point at s fig ure figure phase plot for first order linear negative feedback system with explicit goal figure exponential approach to a goal the goal is units the upper curve begins with the lower curve begins with the adjustment time in both cases is time units net inflow rate net outflow rate s chapter closing the loop dynamics of simple structures time constants and half lives just as exponential growth doubles the state of the system in a fixed period of time exponential decay cuts the quantity remaining by half in a fixed period of time the half life of an exponential decay process is calculated in the same fashion as the doubling time the solution to equation is in equation is the initial gap between the desired and actual states of the system the term decays from to as time increases it is the fraction of the initial gap between desired and actual states remaining at any time t the product s is therefore the current gap remaining between the desired and actual states when the term has decayed to zero the state of the system equals the goal figure the half life is given by the value of time which satisfies where the fractional decay rate d solving for yields the half life is given by the same rule of characterizing exponential growth equivalently the half life is given by of the adjustment each time period equal to at the gap remaining falls to of its initial value and of the gap is corrected why isn t the entire gap corrected after one time constant has passed from equation the initial rate of change of the state of the system is s that is the ini tial gap divided by the adjustment time if the initial rate of adjustment remained constant the entire gap would be eliminated after at time units note that the tan gent to the state of the system at time eliminates the gap after one adjustment time see figure however the net rate of change of the state of the system does not remain constant as the state of the system approaches the goal the gap remaining falls and so too does the corrective action the negative feedback grad ually reduces the adjustment rate as the goal is approached the table at the bottom of figure shows the fraction of the gap remaining for different multiples of the adjustment time after one adjustment time of the initial gap has been corrected after two adjustment times the state of the sys tem has moved of the way to the goal after three adjustment times the ad justment is complete technically the gap is never fully corrected there is always some small fraction of the gap remaining at any finite time however for all practical purposes adjustment is complete after three to four adjustment times have passed logs of both sides gives ln or at at after four adjustment times the gap remaining is just a quantity often smaller than the accuracy with which the state of the system can be measured control engineers speak of a system time defined as the time required for a system after a shock to settle within a small per centage of its equilibrium value part tools for thinking figure relationship between time constant and the fraction of the gap remaining rate equation for first order linear negative loop system net inflow rate net outflow rate s analytic solution state of desired state of initial fraction of initial gap the system the system remaining state of desired state of the system the system gap remaining at at time multiples of at time at fraction of initial gap remaining fraction of initial gap corrected chapter closing the loop dynamics of simple structures goal see ng behavior consider the labor force structure in figure assume the net hiring rate for a firm is proportional to the gap between the desired and actual workforce without using a computer or calculator sketch the behavior of the workforce and net hiring rate for the following situations the desired workforce increases from to at week then steps down to at week figure assume the workforce adjustment time at weeks and the actual workforce initially equals the desired workforce repeat step the case where the labor force adjustment time at weeks sketch the workforce and net hiring rate for the case where the desired workforce increases linearly beginning in week figure assume the 1250 time part tools for systems thinking multiple loop systems the discussion up to now treated positive and negative feedback in isolation what is the behavior of a first order system when the net rate of change is affected by both types of loop consider the example of a population figure disaggre gate the net birth rate into a birth rate br and a death rate the net rate of change for the system is then population birth rate net birth rate br dr consider the linear case where the fractional birth rate is a constant denoted b and the fractional death rate is also constant denoted d the net birth rate is then net birth rate b d p chapter closing the loop dynamics of simple structures figure shows the phase plot for the system only three behaviors are pos sible if the environment contains abundant resources births will exceed deaths b d and the population grows exponentially without limit alternatively births and deaths might exactly offset each other b d and population is in equilib rium finally the environmentmight be so harsh that deaths exceed births b d figure a linear order system cain generate only growth equili or decay structure phase plot behavior time domain population population population time part tools for systems and population declines exponentially to zero because the system is linear the fractional net birth rate is a constant independent of the size of the population fixed for all time once the values of b and d are chosen the behavior of the system is the sum or superposition of the behaviors generated by the individual loops because the system is linear b and d are constants the dominance of the two loops can never change the population will grow without bound remain constant or decay to extinction the superposition property is true of linear systems of any order and com plexity as long as all the rate equations in a system are linear the relative impor tance of the different feedback loops can never change there can be no shifts in loop dominance superposition means linear systems can be analyzed by reduction to their components as a result linear systems no matter how complex can be solved analytically a distinct advantage in understanding their dynamics however realistic systems are far from linear the behavior of the linear pop ulation growth system shows why because the dominance of the feedback loops can never change the population can only grow forever remain constant or go ex tinct real populations introduced into a new habitat with abundant resources grow at first then stabilize or fluctuate the relative strength of the positive and negative loops must therefore shift as a population grows relative to the carrying capacity of the environment in real systems there must be shifts in feedback loop dominance and therefore there must be important nonlinearities in all real systems unfortunately many modelers have restricted their attention to models that can be expressed as linear systems so that they can apply the powerful tools of linear systems theory while the heroic assumption that the linear approximation is reasonable in fairness the reliance on linear theory and the avoidance of non linear systems was justifiable prior to the development of computer simulation be cause analytical solutions to nonlinear dynamic systems cannot in general be found early theorists of dynamic systems made the assumption of linearity be cause it was the only way to make progress even after the advent of computer simulation however too many modelers and mathematicians continued to stress linear theory and build linear models the tendency to treat every system as a lin ear nail because the hammer of linear theory is so powerful has hampered the de velopment of realistic and robust models of complexity of course the triumph of linear methods has never been complete even prior to the computer era several important nonlinear models were developed most no tably verhulst famous logistic population growth model see section and the equally famous lotka volterrapredator prey model lotka and the qualitative theory of dynamic systems developed by and others to analyze the three body problem in celestial mechanics is fundamentally nonlinear see diacu and holmes for a nontechnical treatment of the history and theory in the past few decades there has been an explosion of interest in theories of and data supporting the importance of nonlinear behavior in all branches of dynamics the rise of so called chaos or complexity theory still yoshisuke ueda who dis covered chaos in a nonlinear oscillator as a graduate student in the late was unable to get his work published for over a decade because his advisors steeped in linear theory asserted that his measurements and analysis must be wrong because they knew that systems could not generate the strange nonlinear behavior chapter closing the loop dynamics of simple structures chaos that today we recognize as ubiquitous in physical biological and other systems see ueda linear analysis remains an important tool often a system is close to linear in a certain neighborhood and can be usefully analyzed by linearization that is by approximating the nonlinear rate equations at a particular operating point set of state variable values with the best linear approximation and a great deal of im portant intuition about dynamics comes from understanding simple linear systems such as the first order linear systems responsible for exponential growth and decay still understanding the dynamics of real systems requires nonlinear models nonlinear first order systems s shapedgrowth no real quantity can grow forever every system initially exhibiting exponential growth will eventually approach the carrying capacity of its environment whether that is the food supply for a population of moose the number of people suscepti ble to infection by a virus or the potential market for a new product as the system approaches its limits to growth it goes through a nonlinear transition from a regime where positive feedback dominates to a regime where negative feedback dominates the result is often a smooth transition from exponential growth to equi librium that is s shaped growth see chapter in real systems the fractional birth and death rates cannot be constant but must change as the population approaches its carrying capacity hence the equation for the net birth rate becomes net birth rate br dr where the fractional birth and death rates b and are now functions of the ratio of the population p to the carrying capacity c for now assume the carrying capacity is fixed neither consumed nor augmented by the activity of the population fig ure shows the causal diagram for the system figure diagram for population growth in a fixed environment carrying capacity part tools for systems nonlinear birth and death rates before continuing sketch a graph showing the likely shape of the fractional birth and death rates for a population as it approaches its carrying capacity fig ure the carrying capacity is defined as the population that can just be sup ported by the environment be sure to consider extreme conditions that is what will the fractional birth and death rates be for very low or very large populations from your estimates draw the fractional net birth rate the difference between the fractional birth and death rates capacity dimensionless when population density the ratio is small both fractional birth rate and life expectancy should be at their biological maxima as population grows re sources per capita decline fractional birth rate and life expectancy must fall do they decline immediately in some cases even small reductions in resources per capita could cause a decline in fertility and life expectancy for other resources such as food individuals cannot consume more than a certain amount so fractional birth and death rates should remain constant as long as resources per capita exceed the maximum each individual can consume reducing available food from times more than needed to times more than needed has no impact since each in dividual still gets all they can in this case the fractional birth and death rates remain constant up to a point as increases once resources per capita fall below a certain level the fractional birth rate falls and the fractional death rate increases by definition the carrying capacity is the population that can just be supported by the resources available so the fractional birth rate must equal the assumption that excess food has no impact on fertility or mortality holds only if the organisms in question eat only what they need and do not gorge themselves when a surplus is available for the human population in contrast an abundant food supply and diet rich in animal protein and fat tends to lead to obesity and significantly higher morbidity and mortality in such a case the effect of food per capita on the fractional death rate would actually rise when food per capita exceeds a certain level chapter closing the loop dynamics of simple structures fractional death rate when if the population were to rise above the carry ing capacity the birth fraction would continue to fall and the death fraction would continue to rise as continues to increase the birth fraction must fall to zero and the death fraction must rise to a very large value therefore as shown in fig ure the fractional net birth rate will be positive for p c equal zero when p c and fall below zero at an increasing rate when population exceeds the carrying capacity of the environment while the numerical values for these rela tionships would differ for different populations their qualitative shape is not in doubt next construct the phase plot for the system using these nonlinear fertility and life expectancy relationships the birth and death rates are now curves given by the product of the population and fractional birth and death rates figure first note that the point p is an equilibrium as in the linear system since the frac tional birth rate remains nearly constant when population is small relative to the carrying capacity the birth rate in period is nearly linear for p c as population density rises and the fractional birth rate falls the birth rate while still growing rises with a shallower and shallower slope at some point the decline in the fractional birth rate reduces total births more than the increase in sheer numbers increases them and the birth rate reaches a maximum since the fractional birth rate falls to zero for high population densities so too the total birth rate must approach zero likewise the death rate rises nearly linearly for p c but as greater population density boosts the fractional death rate the total death rate increases at an increasing rate turning to the dynamics imagine the initial population is small relative to the carrying capacity the net birth rate rises nearly linearly for p c the behavior of the system in this regime will be nearly pure exponential growth as population density increases the net birth rate continues to rise but at a shallower and shal lower slope the population continues to grow at an increasing rate but the frac tional growth rate is steadily diminishing at some point the net birth rate reaches a maximum this point comes at a lower population density than the peak in the birth rate since deaths are increasing at an increasing rate the peak of the net birth rate curve on the phase plot corresponds to the inflection point in the trajectory of figure nonlinear relationship between and the growth capacity dimensionless part tools for systems figure phase plot for nonlinear population system arrows show di rection of flow positive feedback dominates the sys tem in the region dominant dominant eath rate birth rate where the net birth rate has positive slope negative feedback is domi nant where the net birth rate has neg ative slope the maximum net birth rate occurs at the point the inflection point in the trajectory of population eauilibrium i net birth rate capacity dimensionless population in the time domain the point at which the population is rising at its maximum rate beyond the inflection point the increase in population density re duces the net birth rate more than the increase in total population size increases it the net birth rate while still positive drops falling to zero just when the popula tion equals the carrying capacity if the population exceeded the carrying capacity resources per capita would be so scarce that deaths would exceed births and the population would fall back toward the carrying capacity the equilibrium at p c is therefore stable figure shows the behavior of the system over time for two cases when the initial population is much smaller than the carrying capacity and when the initial population is much larger than the carrying capacity when c the net birth rate is increasing in the population as long as the slope of the net birth rate curve in the phase plot is positive the system is dominated by the positive feedback loop and population grows exponentially unlike the linear sys tem however the slope of the net birth rate curve is not constant so the growth is not a pure exponential instead the fractional growth rate falls as population grows population growth reaches its maximum when the population reaches the value denoted the inflection point in the trajectory of the population at that point the slope of the net birth rate curve is zero the positive and negative loops exactly offset one another as population continues to grow the slope of the net birth rate curve in the phase plot becomes negative for the sys tem is dominated by negative feedback because the net birth rate has a negative slope in this region the equilibrium point at p c is stable a population less than the carrying capacity will grow at a diminishing rate until it reaches the carrying capacity a population larger than the carrying capacity will fall until it reaches the carrying capacity from above formal definition of loop dominance the phase plot shows the origin of the terms positive and negative feedback posi tive feedback dominates whenever the rate of change of the state variable is chapter closing the loop dynamics of simple structures figure nonlinear population grovvth m time top c population follows an s shaped trajectory with m inflection point at bottom c the population decays back to the stable equilibrium at p c the time axis both simulations is the same vertical scales increasing in the state variable that is as long as the slope of the net rate of change as a function of the state variable is positive negative feedback dominates when ever the net rate of change is decreasing in the state variable that is as long as the slope of the net rate is negative this observation leads to a formal definition of loop dominance for first order systems richardson positive feedback dominant where no net feedback from state to rate negative feedback dominant dt determining whether a system is dominated by positive or negative feedback is more difficult in higher order systems because a loop with time delays can have a weak short run but large long run effect kampmann provides some meth ods to determine the dominant loops in multiloop high order systems see also n forrester and mojtahedzadeh part tools for systems first order systems cannot oscillate a final observation on the general nonlinear first order system consider whether a first order system can oscillate section demonstrated that the linear first order system can generate only exponential growth decay or equilibrium nonlinear first order systems generate more complex dynamics but can never cillate no matter the form of the nonlinearity to see why consider the phase plot for the first order system in figure to oscillate the state variable must go through periods of increase followed by periods of decrease therefore the net rate of change in the phase plot must cross from positive to negative values at least in one place however any point where the net rate of change is zero is an equilib rium of the state variable since first order systems have only one state variable every such point is an equilibrium for the system as well every equilibrium point is either stable the slope of the net rate curve in the neighborhood of the equilib rium is negative or unstable positive slope in the neighborhood of the equilibrium if a first order system is disturbed from an unstable equilibrium it will di verge from it either without bound not oscillating or until it approaches a stable equilibrium point where all change ceases therefore to oscillate a system must be at least second order meaning there must be a feedback loop with at least two stocks in summary this chapter explored the dynamics of simple systems specifically first order linear systems systemswith only one stock state variable and in which the rates of flow are linear functions of the system state these simple systems are the build ing blocks out of which all models are built and from which more complex dynamics emerge first order linear positive feedback systems produce pure expo nential growth exponential growth has the remarkable property that the state of the system doubles in a fixed period of time no matter how large it is the doubling time characterizes the strength of the positive loop similarly first order linear negative feedback systems generate exponential decay to a goal the decay rate is characterized by the half life the time required for the gap between the state of the system and the goal to be cut in half the chapter also introduced the phase the net rate of change is zero over a finite interval in state space these points have neutral stability a disturbance within that range causes neither a restorative nor divergent change in the net rate just as a ball placed anywhere on a flat surface will remain at that point first order systems cannot oscillate provided time is treated continuously order systems in discrete time can oscillate for example the logistic map the first order nonlinear discrete time map x t where k and not only oscil lates for certain values of k but generates period doubling and chaos as well however the state ment that oscillation requires a feedback loop with at least two stocks is still valid in discrete time models the time step between iterations constitutes an irreducible time delay in every feedback loop every time lag contains a stock which accumulates the inflow to the delay less its outflow every discrete dynamic system can be converted into an equivalent continuous time system by introducing a lag equal to the time step at every state variable increasing the order of the system see low for an example chapter closing the loop dynamics of simple structures plot a useful tool to analyze the dynamics of systems graphically without the use of calculus analysis of the phase plots for first order systems shows that in systems with more than one feedback loop the dynamics depend on which loop is dominant in linear systems the dominance of the different loops can never change thus linear first order systems can only exhibit three behaviors exponential growth when the positive loops dominate exponential decay when the negative loops dominate and equilibrium when the loops exactly offset one another nonlinear first order systems can exhibit s shaped growth because the dominant feedback loops shift as the system evolves as the population approaches its carrying capacity the positive loops driving growth weaken and the negative loops restraining growth strengthen until the system is dominated by negative feedback and the population then smoothly approaches a stable equilibrium at the carrying capacity growth epidemics innovation diffusion and the of new products everything that rises must converge as seen in chapter positive feedback creates exponential growth but no real quantity can grow forever every system initially dominated by positive feedbacks eventually approaches the carrying capacity of its environment as the limits to growth approach there is a nonlinear transition from dominance by positive feed back to dominance by negative feedback under certain conditions the result is s shaped growth where the growing population smoothly approaches equilibrium this chapter shows how s shaped growth can be modeled with applications to the diffusion of innovations the spread of infectious diseases and computer viruses the growth of the market for new products and others a variety of important and widely used models of s shaped growth are introduced and analyzed the use of these models for forecasting is discussed and extensions to the models are presented cases examined include the spread of mad cow disease and hiv and the growth of the markets for high tech products such as computers and consumer services such as cable television part the dynamics of growth modeling growth the nonlinear population model developed in chapter is quite general the pop ulation in the model can be any quantity that grows in a fixed environment for ex ample the number of adopters of an innovation the number of people infected by a disease the fraction of any group adhering to an idea or purchasing a product and so on if the population is driven by positive feedback when it is small relative to its limits then the resulting behavior will be s shaped growth provided there are no significant delays in the negative feedbacks that constrain the population if there are delays in the response of the population to the approaching carrying ca pacity the behavior will be s shaped growth with overshoot and oscillation if the carrying capacity is consumed by the growing population the behavior will be overshoot and collapse see chapter conversely whenever you observe a sys tem that has experienced s shaped growth you know that initially the behavior was dominated by positive feedback loops but as the system grew there was a nonlinear shift to dominance by negative feedback logistic growth as illustrated in the nonlinear population growth example in chapter the net fractional growth rate of the population p must fall from its initial value pass through zero when the population equals the carrying capacity c and become neg ative when p c consequently the phase plot of the net birth rate must have a shape roughly like an inverted bowl net births are zero when the population is zero rise with increasing population up to a maximum fall to zero at the carrying capacity and continue to drop becoming increasingly negative when population exceeds the carrying capacity however there are an infinite number of fractional net birth rate curves and hence phase plots satisfying these general constraints an important special case of s shaped growth is known as logistic growth or verhulst growth after verhulst who first published the model in see richardson the logistic growth model posits that the net fractional population growth rate is a downward sloping linear function of the population that is net birth rate c p where c the fractional growth rate is a function of the population and carry ing capacity and is the maximum fractional growth the fractional growth rate when the population is very small the logistic model conforms to the require ments for s shaped growth the fractional net growth rate is positive for p c zero when p c and negative for p c the logistic model has some additional characteristics rearranging equation gives net birth rate the first term standard first order linear positive feedback process the second term is nonlinear in the population and represents the stronger negative feedback caused by the approach of the population to its carry ing capacity chapter s shaped growth epidemics innovation diffusion and the growth of new products when does the net growth rate reach its maximum in the logistic model the net birth rate given by equation is an inverted parabola which passes through zero at the points p and p c because a parabola is symmetric around its peak the maximum net birth rate occurs when where is the value of population where the net growth rate is at a maximum and therefore the inflection point in the trajectory of the population the maximum net growth rate occurs precisely halfway to the carrying capacity figure plots the fractional growth rate phase plot and time domain behavior for the logistic model the logistic model is important for several reasons first many s shaped growth processes can be approximated well by the logistic model despite the re striction that the inflection point occurs at precisely second the logistic model can be solved analytically finally the logistic model though intrinsically non linear can be transformed into a form that is linear in the parameters so it can be estimated by the most common regression technique ordinary least squares see section analytic solution of the logistic equation though it is nonlinear the logistic model shown in equation can be solved analytically first separate the variables then integrate rearranging the left hand side gives p integrating both sides yields p g t c where c is a constant since by definition when t g t exponentials yields which can be rearranged as the maximum net birth rate and therefore the infection point in the population occurs when solving for p yields part the dynamics of growth figure the logistic model top the fractional growth rate de clines linearly as population grows middle the phase plot is an inverted parabola symmet ric about m k m capacity dimensionless follows an s shaped curve with inflection point at the net growth rate follows a shaped curve with a maximum value of per time period the time axis is scaled so that unit with the inflection point centered at time h capacity dimensionless time or equivalently as where h is the time at which the population reaches half its carrying ca pacity setting in equation and solving for h yields h equations and are two forms of the analytic solu tion to the equation for logistic growth given by equation chapter s shaped growth epidemics innovation diffusion and the growth of new products other common growth models due to its simplicity and analytic tractability the logistic model is the most widely used model of s shaped growth however there are many other models of s shaped growth these models relax the restrictive assumption that the fractional growth rate declines linearly in the population these growth curves are in general not symmetric the richards curve is one commonly used model richards in richards model the fractional growth rate of the population is nonlinear in the population net birth rate when m the richards model reduces to the logistic other values of m cause the fractional growth rate to be nonlinear in the population try sketching the frac tional growth rate as a function of population for different values of m the solu tion of the richards model is c l where k is a parameter that depends on the initial population relative to the carry ing capacity a special case of the richards model is the gompertz curve given by the richards model in the limit when m note that while equation is un defined when m lim x so the gompertz curve is given by in the gompertz model the fractional growth rate declines linearly in the loga rithm of the population and the maximum growth rate occurs at another commonly used growth model is based on the weibull distribution where a b are known as the shape and scale parameters respectively the case a is known as the rayleigh distribution the richards and weibull models provide the modeler with analytically tractable growth functions that can represent a variety of nonlinear fractional net increase rates however there is no guarantee that the data will conform to the as sumptions of any of the analytic growth models fortunately with computer simu lation you are not restricted to use the logistic gompertz richards weibull or any other analytic model you can specify any nonlinear relationship for the frac tional birth and death rates supported by the data and then simulate the model to explore its behavior over time part the of growth testing the logistic model to illustrate the use of the logistic model consider the examples of s shaped growth in figure figure shows the result of fitting the logistic model to the data for the growth of sunflowers the best fit logistic model matches the sun flower data reasonably well though it underestimates the growth in the first month and overestimates it later these differences suggest a better fit might be gained through use of a different growth model such as the richards model in which the fractional growth rate is nonlinear in the population section provides addi tional examples dynamics of disease modeling epidemics epidemics of infectious diseases often exhibit s shaped growth the cumulative number of cases follows an s shaped curve while the rate at which new cases oc cur rises exponentially peaks then falls as the epidemic ends figure shows the course of an epidemic of influenza at an english boarding school in the epidemic began with a single infected student patient zero the flu spreads through contact and by inhalation of virus laden aerosols released when infected individuals cough and sneeze the flu spread slowly at first but as more and more students fell ill and became infectious the number they infected grew exponen tially due to the close quarters and thus high rate of exposure about two thirds of the population eventually became ill and the epidemic ended due to the depletion of the pool of susceptible people figure also shows the course of an epidemic of plague in bombay in the behavior is quite similar despite the differ ences in time frame mortality and other aspects of the situation the pathogen does not have to be a biological agent epidemics of computer viruses follow sim ilar dynamics a simple model of infectious disease figure shows a simple model of infectious disease the total population of the community or region represented in the model is divided into two categories those susceptible to the disease s and those who are infectious i for this reason the figure the growth of sunflowers and the best fit logistic model e chapter s shaped growth epidemics innovation diffusion and the growth of new products model is known as the si model as people are infected they move from the sus ceptible category to the infectious category the si model invokes a number of simplifying assumptions section develops a more realistic model first figure dynamics of epi demic disease influenza epidemic at an english school january february the data show the number of to bed for influenza at any the stock of symptomatic source medical journal march p bottom pidemic of bay india data show the death rate k source kermack and p for further dis cussion of both cases see murray weeks structure of a model of an epidemic births deaths and migration are omitted so the total population is a constant and people remain infectious indefinitely total i c population n part the dynamics of growth births deaths and migration are ignored second once people are infected they remain infectious indefinitely that is the model applies to chronic infections not acute illness such as influenza or plague the si model contains two loops the positive contagion loop and the negative depletion loop infectious diseases spread as those who are infectious come into contact with and pass the disease to those who are susceptible increasing the in fectious population still further the positive loop while at the same time deplet ing the pool of susceptibles the negative loop the infectious population i is increased by the infection rate ir while the sus ceptible population s is decreased by it i s integral ir n where n is the total population in the community and is the initial number of infectious people a small number or even a single individual to formulate the infection rate consider the process by which susceptible people become infected people in the community interact at a certain rate the contact rate c mea sured in people contacted per person per time period or period thus the susceptible population generate sc encounters per time period some of these en counters are with infectious people if infectious people interact at the same rate as susceptible people they are not quarantined or confined to bed then the proba bility that any randomly selected encounter is an encounter with an infectious in dividual is not every encounter with an infectious person results in infection the infectivity i of the disease is the probability that a person becomes infected after contact with an infectious person the infection rate is therefore the total number of encounters sc multiplied by the probability that any of those encounters is with an infectious individual multiplied by the probability that an encounter with an infectious person results in infection ir the dynamics can be determined by noting that without births deaths or migra tion the total population is fixed though the system contains two stocks it is actually a first order system because one of the stocks is completely determined by the other substituting n i for s in yields ir equation is identical to equation l the net birth rate in the logistic model an epidemic in this model grows exactly like a population in a fixed en vironment the carrying capacity is the total population n in the si model once an infectious individual arrives in the community every susceptible person even tually becomes infected with the infection rate following a bell shaped curve and chapter growth epidemics innovation diffusion and the growth of new products the total infected population following the classic s shaped pattern of the logistic curve figure the higher the contact rate or the greater the infectivity the faster the epidemic progresses the si model captures the most fundamental feature of infectious diseases the disease spreads through contact between infected and susceptible individuals it is the interaction of these two groups that creates the positive and negative loops and the nonlinearity responsible for the shift in loop dominance as the susceptible population is depleted the nonlinearity arises because the two populations are multiplied together in equation it takes both a susceptible and an infectious person to generate a new case modeling acute infection the sir model while the si model captures the basic process of infection it contains many sim plifying and restrictive assumptions the model does not represent births deaths or migration the population is assumed to be homogeneous all members of the community are assumed to interact at the same average rate there are no subcul tures or groups that remain isolated from the others in the community or whose be havior is different from others the disease does not alter people lifestyles infectives are assumed to interact at the same average rate as susceptibles there is no possibility of recovery quarantine or immunization all these assumptions can be relaxed the susceptible population can be dis aggregated into several distinct subpopulations or even represented as distinct in dividuals each with a specific rate of contact with others an additional stock can be added to represent quarantined or vaccinated individuals birth and death rates can be added random events can be added to simulate the chance nature of con tacts between susceptibles and infectives the most restrictive and unrealistic feature of the logistic model as applied to epidemics is the assumption that the disease is chronic with affected individuals remaining infectious indefinitely consequently once even a single infectious in dividual arrives in the community every susceptible eventually becomes infected while the assumption of chronic infection is reasonable for some diseases herpes simplex many infectious diseases produce a period of acute infectiousness and illness followed either by recovery and the development of immunity or by death most epidemics end before all the susceptibles become infected because people recover faster than new cases arise kermack and mckendrick de veloped a model applicable to such acute diseases the model contains three stocks the susceptible population s the infectious population i and the recov ered population r figure long known as the sir model the kermack formulation is widely used in epidemiology those contracting the disease become infectious for a certain period of time but then recover and develop permanent immunity the assumption that people recover creates one additional feedback the negative recovery loop the greater the number of infectious part the dynamics of growth individuals the greater the recovery rate and the smaller the number of infectious people remaining all other assumptions of the original si model are the susceptible population as in the si model is reduced by the infection rate the infectious population now accumulates the infection rate less the recovery rate rr and the recovered population r accumulates the recovery rate s integral ir n i rr r the initial susceptible population is the total population less the initial number of infectives and any initially recovered and immune individuals the recovery rate can be modeled several ways in the sir model the average duration of infectivity d is assumed to be constant and the recovery process is as sumed to follow a first order negative feedback process the average duration of infectivity d represents the average length of time people are infectious the assumption that the recovery rate is a first order process means the sir model the recovered population is often termed removals and the recovery rate is then called the removal rate many applications of the model interpret the removal rate as the sum of those recovering from the disease and those who die from it however this interpretation is in correct since those who die reduce the total population while in the sir model the total population is constant the aggregation of deaths and recoveries into a single flow of removals and a single stock of cumulative removals is usually justified by arguing that mortality is often a small fraction of the total population even when this is true it is bad modeling practice to aggregate the living with the dead since their behavior is often quite different in this case those who recover continue to interact with the remaining susceptible and infectious populations while those who die usually do not chapter s shaped growth epidemics innovation diffusion and the growth of new products people do not all recover after exactly the same time but rather a given population of infectious individuals will decline exponentially with some people recovering rapidly and others more the infection rate is formulated exactly as in the si model in equation model behavior the tipping point unlike the models considered thus far the system is now second order there are three stocks but since they sum to a constant only two are independent however it is still possible to analyze its dynamics qualitatively first unlike the si model it is now possible for the disease to die out without causing an epidemic if the in fection rate is less than the recovery rate the infectious population will fall as it falls so too will the infection rate the infectious population can therefore fall to zero before everyone contracts the disease under what circumstances will the introduction of an infectious individual to the population cause an epidemic intuitively for an epidemic to occur the infec tion rate must exceed the recovery rate if so the infectious population will grow leading to still more new cases if while each person was infectious they passed the disease on to exactly one more person then the stock of infectives would re main constant since the infection rate would be just offset by the recovery rate therefore for an epidemic to occur each infective must on average pass the dis ease on to more than one other person prior to recovering the question of whether an epidemic will occur is really a question about which feedback loops are dominant when the disease arrives in a community if the positive contagion loop dominates the recovery and depletion loops then the in troduction of even a single infective individual to a community triggers an epi demic the infection rate will exceed the recovery rate causing the infection rate to grow still further until depletion of the pool of susceptibles finally limits the epidemic if however the positive loop is weaker than the negative loops an epi demic will not occur since infectious people will recover on average faster than new cases arise the number of new cases created by each infective prior to their recovery and therefore the strength of the different loops depends on the average duration of infection and the number of new cases each infective generates per time period the higher the contact rate or the greater the infectivity of the disease the stronger the positive loop likewise the larger the fraction of the total popula tion susceptible to infection the weaker the depletion loop finally the longer the the assumption that removals are first order is reasonable in the simple sir model the course of many diseases is more complex and the delay between infection and removal is often not exponential if a group were all infected at once the removal rate would be small initially then build to a peak before tapering off chapter discusses how different types of delays can be mod eled in depth and shows how modelers can select robust formulations for delays to match the data for different distributions part the dynamics of growth average duration of infection the weaker the negative recovery loop and the more likely an epidemic will for any given population of susceptibles there is some critical combination of contact frequency infectivity and disease duration just great enough for the posi tive loop to dominate the negative loops that threshold is known as the tipping point below the tipping point the system is stable if the disease is introduced into the community there may be a few new cases but on average people will recover faster than new cases are generated negative feedback dominates and the popula tion is resistant to an epidemic past the tipping point the positive loop dominates the system is unstable and once a disease arrives it can spread like that is by positive feedback limited only by the depletion of the susceptible population figure shows a simulation of the model where the system is well past the tipping point the population of the community is initially everyone is susceptible to the disease at time zero a single infective individual arrives in the community the average duration of infection is days and infectivity is the average contact frequency is six people per person per day each infective therefore generates new cases per day and an average of three new cases before they recover the positive loop therefore dominates and the epidemic quickly spreads the infection rate peaks at more than people per day around day nine and at its peak more than one quarter of the population is infectious the sus ceptible population falls rapidly and it is this depletion of potential new cases that halts the epidemic by the tenth day the number of susceptibles remaining is so low that the number of new cases declines the infectious population peaks and falls as people now recover faster than new cases arise the susceptible population continues to fall though at a slower and slower rate until the epidemic ends in less than weeks a single infectious individual led to a massive epidemic in volving nearly the entire community note that a few lucky individuals never con tract the disease unlike the chronic infection model in which everyone eventually contracts the disease in the sir model the epidemic ends before the susceptible population falls to zero the stronger the positive loop however the fewer ceptibles remain at the end of the epidemic also note that unlike the logistic model the behavior is not symmetric the infectious population rises faster than it falls si and sir models were originally formulated as deterministic systems representing the average contact rate and abstracting from the individual encounters among members of the popula tion keep in mind that the deterministic formulation is a modeling assumption appropriate in some situations and not appropriate in others particularly when the populations are small or the variance in the distribution of contact rates infectivity and recovery time is large the models are easily generalized to incorporate stochastic encounters infectivity and recovery either by adding random variation to the rate equations of the sir model or by representing the members of the pop ulation as discrete individuals and specifying decision rules for their interaction an agent based model incorporating these random effects means there will be a distribution of possible outcomes for any set of parameters the sharp boundary between an epidemic and stability defined by the tip ping point in the deterministic models becomes a probability distribution characterizing the chance an epidemic will occur for any given average rates of interaction infectivity and recovery like wise the si and sir models assume a homogeneous and well mixed population while in reality it is often important to represent subpopulations and the spatial diffusion of an epidemic for spatial models of the spread of disease and other refinements see murray chapter s shaped growth epidemics innovation diffusion and the growth of new products to illustrate the tipping point figure shows the susceptible population in several simulations of the model with different contact rates the other parameters are to those in figure at the tipping point two contacts per person per day the number of new cases each infective generates while infectious is just equal to one contacts per person per day probability of infection days of infectivity contacts at a rate less than two per person per day do not cause an figure of an epiidemic in the sir model the total is contact rate is per person peir day infectivity is average of infectivity is clays initial infective population is and all are initially susceptible a days q days figure epidemic dynamics for different contact the rate is rioted on curve all other q 7500 c parameters are as in figure e q days part the dynamics of growth epidemic when the contact rate rises above the critical threshold of two the sys tem become unstable and an epidemic occurs the higher the contact rate the stronger the positive contagion loop relative to the negative recovery loop and the faster the epidemic progresses further the stronger the positive loop the greater the population ultimately contracting the disease any change that increases the strength of the positive loops will yield similar results an increase in infectivity strengthens the positive loop and is identical in impact to an increase in contact fre quency an increase in the duration of the infectious period weakens the recovery loop and also pushes the system farther past the tipping point the exact tipping point in the sir model can easily be calculated for an epidemic to occur the infection rate must exceed the recovery rate ir rr or equivalently in equation the product of the contact rate and infectivity is the number of infectious contacts per time period each infectious person generates multiplying by the average duration of infection d yields the dimensionless ratio cid known as the contact however not all these contacts will be with susceptibles so not all will result in a new case the number of infectious contacts that actually result in the infection of a susceptible person depends on the probability that the infectives encounter susceptibles assuming the population is homogeneous the probability of encountering a susceptible is given by the prevalence of susceptibles in the population the expression is also known as the reproduction rate for the epidemic equation therefore defines the tipping point or thresh old at which an epidemic occurs in a population and is known as the threshold the orem in epidemiology note that the contact number can be large if infectivity is high or if the dura tion of infection is long the duration of the infectious period for diseases such as measles and chicken pox is very short a matter of days but these diseases have high contact numbers because they are easily spread through casual contact in contrast the contact rate and infectivity of hiv are much lower hiv cannot be spread through casual contact but only through sexual contact or exchange of blood or blood products nevertheless the contact number for hiv is high among those who engage in risky behaviors because the duration of infection is so long the incubation period prior to the development of clinical symptoms of aids av erages about years see section chapter s shaped growth epidemics innovation diffusion and the growth of new products figure shows how the tipping point depends on the parameters the curve is the boundary between stable and unstable regimes to the left of the curve the system is stable and there is no epidemic because the infectivity contact rate du ration of infection and fraction of susceptibles in the population are too low to the right of the curve the system is unstable and there is an epidemic immunization and the eradication of smallpox the existence of the tipping point equation means it is theoretically pos sible to completely eradicate a disease eradication does not require a perfect vaccine and universal immunization but only the weaker condition that the repro duction rate of the disease fall and remain below one so that new cases arise at a lower rate than old cases are resolved the stock of infectious people will then de cline further reducing the infection rate until the population becomes disease free for many diseases it is difficult or impossible to achieve or maintain this condi tion due to high infectivity the existence of reservoirs of the disease outside the human population as in malaria or lyme disease both of which have animal hosts or the rapid influx of susceptible people through births migration or the decay of immunity smallpox however is different smallpox was once one of the most deadly diseases and endemic throughout the world the infectivity of smallpox is high but the duration of infection is short survivors acquired long lived immunity most important the smallpox virus cannot survive outside a human host there are no animal or other reservoirs to harbor the virus these conditions meant that the development of an effective vaccine deployed sufficiently broadly could re duce the infection rate below the recovery rate and eliminate the virus even if not every person could be immunized the history of smallpox eradication is well known edward jenner developed the first effective vaccine in despite the success of jenner vaccine it took many years for vaccination to be accepted smallpox was still a major cause of death at the start of the century by the due to improvements in public health programs and in the effectiveness and shelf life of the vaccine smallpox figure dependence the tipping point on the contact number and susceptilble population epidemic unstable positive loop dominant susceptible fraction of population dimensionless part the dynamics of growth had been eradicated in most of the industrialized world in the the world health organization who coordinated a vigorous worldwide campaign to track the disease immunize the susceptible and quarantine the sick the last known naturally occurring case was reported in in somalia in some smallpox virus escaped from a research lab in england and caused two cases one fatal since then no further cases have been reported and in one of the greatest tri umphs in the history of medicine the nations of the world declared in that smallpox had been eradicated from the earth almost during the cold war both the us and soviet union maintained stocks of smallpox virus as part of their biological warfare programs though both nations signed the biological weapons convention banning bioweapons and biowarfare research they continued to maintain their stocks of smallpox and the soviet union continued biowarfare research in violation of the convention while these smallpox stocks are maintained in the highest security biocontainment labs there is natural concern that the virus might escape accidentally through terrorism or in war a who panel after long and sometimes acrimonious debate recom mended in that all us and russian stocks of the virus be destroyed by june however many analysts believe terrorists or nations such as iraq and north korea may have acquired smallpox from the former soviet union because people no longer receive smallpox vaccinations and because the immunity conferred by childhood vaccination decays much of the world population today is susceptible the release of smallpox from these stocks could trigger a massive pandemic in re sponse president clinton ordered us smallpox stocks be preserved for research and who suspended its attempt to have declared stocks of smallpox destroyed the efficacy of immunization programs equation and figure show how the vulnerability of a population to epi demic depends on the parameters of the sir model many infectious diseases are highly contagious and it is not feasible to reduce the contact number immuniza tion where vaccines are available can be highly effective not only in protecting the immunized individuals but also in moving an entire population below the tip ping point for example polio has all but vanished in nations with strong public health programs and who hopes to eradicate it worldwide within a few years effectiveness of immunization the contact number for polio is estimated to be roughly to fine what fraction of the population must be vaccinated to ensure that no epidemic will occur assume the vaccine is effective now consider measles and pertussis whooping cough diseases whose contact numbers are estimated to be to fine what fraction of the population must be vaccinated to ensure no epidemic will occur what fraction must be vaccinated if the vaccine is only effective why do measles and pertussis persist while polio has been effectively eliminated next simulate the sir model with the parameters in figure c i d n but assume that of the population has chapter s shaped growth epidemics innovation diffusion and the growth of new products been immunized set the initial recovered population to half the total population what is the effect on the course of the epidemic what fraction of the population must be immunized to prevent an epidemic effectiveness of quarantine examine the effectiveness of quarantine as a policy to prevent an epidemic to do so modify the sir model to include a stock of quarantined individuals i assume people are quarantined only after they exhibit symptoms so the quarantine rate the rate of inflow to the quarantined population flows from the infectious population to the quarantined population formulate the quarantine rate as follows assume it takes a certain period of time denoted the quarantine time to identify infectious people and move them to a quarantine area further assume only a fraction of the infectious population denoted the quarantine fraction is identified as infectious and willing or able to be quarantined be sure your formulation for the quarantine rate is dimensionally con sistent assume quarantined individuals recover from the disease with the same average duration of infectivity as those not quarantined ii iii quarantined individuals are not completely removed from contact with the rest of the population during an epidemic of smallpox in cen tury boston for example the sick were quarantined but still permitted to attend church on sunday modify the equation for the infection rate to include the possibility that quarantined people come in contact with susceptibles at a certain rate denoted the quarantine contact rate assume the infectivity of quarantined individuals is the same as that for other infectives how does the addition of a stock of quarantined individuals alter the feedback structure of the sir model assume the quarantine time is half a day and use the parameters for the simulation in figure c d n assume the quarantine is perfect so that the quarantine contact rate is zero run the model for various values of the quarantine fraction and explain the resulting behavior what fraction of the infectious population must be quarantined to prevent an epidemic now assume that the contact rate of quarantined individuals is half the normal contact rate what fraction must now be sequestered to prevent an epidemic and how fast must people be moved to quarantine once they become infectious explore the response to other includ ing partial immunization of the population compare the efficacy of immunization to quarantine in preventing or slowing epidemics what policy considerations would influence your choice of these policies in different situations loss of immunity for some diseases immunity is not permanent but decays over time leaving formerly immune people susceptible to reinfection modify the model to incorporate loss of immunity assume immunity decays at a rate determined by a constant average duration of immunity run the model for different part the dynamics of growth herd immunity in the real world a population is repeatedly challenged by exposure to different diseases infectious individuals unwittingly carrying the disease may arrive from other communities the spread of the black death in century europe was ac celerated by extensive trade networks with other regions the high rate of travel of pilgrims and by the flight of the terrified and unknowingly infected to towns as yet unexposed susceptible individuals can also come into contact with other reser voirs of the disease such as contaminated drinking water as in cholera or ani mals bubonic plague is not only transmitted from person to person but by fleas who pass the plague bacillus from infected rats to people some pathogens mutate and cross the so called species barrier jumping from animal reservoirs to human populations as occurs in influenza and likely occurred with hiv outbreaks of the ebola virus and apparently with bovine spongiform encephalopathy mad cow disease if the contact rate infectivity and duration of infection are small enough the system is below the tipping point and stable such a situation is known as herd immunity fine because the arrival of an infected individual does not produce an epidemic though a few unlucky individuals may come in con tact with any infectious arrivals and contract the disease the group as a community is protected however changes in the contact rate infectivity or duration of ill ness can push a system past the tipping point figure shows how changes in the reproduction rate can dramatically change the response of a population to exposure in the simulation the population is challenged every days by the arrival of a single infected individual the pop ulation infectivity and duration of infection are identical to those in figure and days respectively however the contact rate is assumed to increase linearly over time beginning at zero the contact rate might increase as population density grows or as changes in social structures or cultural practices bring people into more frequent contact the rapid urbanization of the industrial revolution for example increased the contact rate and incidence of epidemics for many diseases during the first days the reproduction rate the number of new cases gen erated by each infective prior to recovery is less than one the negative loops dom inate and the system is below the tipping point there is no epidemic every days an infectious individual arrives but any people infected by this person re cover before they can replace themselves in the pool of infectives the population enjoys herd immunity at day the tipping point is crossed now the contagion loop dominates and the next infectious person to arrive triggers an epidemic the epidemic ends after the susceptible population falls enough for the depletion and recovery loops to overpower the contagion loop by about day the decline in chapter s shaped growth epidemics innovation diffusion and the growth of new products figure successive epi demic waves ated by increasing contact rate every days the population is challenged by the arrival of a sin gle infectious vidual at first the population has herd immunity the contact rate increases linearly zero increas ing the re production rate and eventually causing a wave of epidemics days days days the susceptible population reduces the reproduction rate below one and the nega tive loops once again dominate the positive contagion loop though infected indi viduals continue to arrive at day intervals the system has become stable again however the contact rate keeps rising increasing the strength of the contagion loop by day the contagion loop is once again dominant the arrival of the next infectious person triggers another epidemic since there are even fewer ceptibles this time around it is a bit milder by day the depletion of the sus ceptible pool has once again overwhelmed the contagion loop and the reproduction part the dynamics of growth rate falls below one the community is once again resistant to epidemic until the contact rate rises enough to push the reproduction rate above one again triggering the third wave of infection in this simple example the periodic epidemics arise from the assumed steady rise in the contact rate successive waves of epidemics are in fact observed for many infectious diseases perhaps most notably measles prior to the introduction of mass immunization in the industrialized nations such as the us and uk experienced large amplitude measles epidemics about every years due to im munization programs the amplitude of the cycles is much reduced today but the tendency toward cyclic waves of measles persists in contrast to the example above the contact rate for measles remains reasonably constant the cyclic char acter of the epidemics arises from the interaction of herd immunity with population growth each epidemic increases the immune fraction of the population enough to confer herd immunity preventing another epidemic the following year however during this time the population of susceptibles increases as children are born eventually the susceptible fraction of the population rises enough to push the sys tem past the tipping point again and the next epidemic begins the sir model can easily be extended to include the age structure of the population including births and deaths see chapter with realistic parameters these models generate per sistent oscillations in disease incidence as the system repeatedly cycles above and below the tipping point the reproduction rate for an infectious disease is not solely a matter of the vir ulence and other biological attributes of the pathogen it is strongly influenced by social structures and the physical infrastructure of a community the contact rate obviously represents the nature of social structures in the community the contact rate in rural communities with low population densities is lower than that of highly urbanized populations the infectivity of a disease is only partly determined by bi ological factors casual contact and inhalation can spread influenza while hiv can only be contracted through exchange of blood or other body fluids a biological factor but infectivity is also strongly affected by social practices and public health policies such as the availability of clean water the prevalence of hand washing or the frequency of condom use moving past the tipping point mad cow disease the epidemic of bse or mad cow disease in great britain during the illus trates how changes in technical and social structures can push a population past the tipping point prior to the epidemic the incidence of bse and related degenerative neurological diseases such as scrapie in sheep and cruetzfeldt jacob disease cjd in humans was extremely low but between and approximately of bse were confirmed in the uk and almost one million cattle out of a total livestock population of about million were estimated to be infected prusiner bse scrapie and cjd progressively and at present irreversibly destroy brain tissue symptoms include uncontrollable tremor disorientation loss of motor and cognitive function and ultimately death chapter s shaped growth epidemics innovation diffusion and the growth of new products the cause of bse is still debated most scientists believe bse is caused by ons abnormal proteins that are hypothesized to replicate even though they do not contain any dna or rna biologist stanley prusiner received the nobel prize for his pioneering and still controversial work on prions see prusiner for further details on prions and bse others believe bse scrapie and cjd are caused by an as yet undetected and possibly novel type of virus since bse is not thought to be directly communicable from animal to animal the positive contagion feedback did not exist under traditional animal husbandry practices how then did the epidemic arise to reduce costs over the past few decades cattle producers began to supplement the diets of their herds with meat and bone meal mbm prepared from the offal of slaughtered livestock including sheep cattle pigs and chickens to reduce costs further a new process for prepa ration of mbm was introduced in the late the new process involved lower temperatures for rendering offal into feed pellets and left more fat in the product it is thought that this change allowed bse to enter the uk cattle population through mbm made from sheep infected with scrapie in its search for lower costs the livestock industry converted its herds from herbivores to unwitting cannibals and created a pathway for infected cattle to contact susceptibles thus closing the contagion loop the practice of feeding mbm to cattle dramatically boosted the re production rate for bse in the cattle population and pushed it well above the tip ping point further whereas most diseases are communicated only by close contact between infected and susceptible individuals so that epidemics tend to be localized mbm was distributed all over the uk allowing a single in fected animal to pass bse to others hundreds of miles away the epidemic figure spread rapidly much faster than medical knowl edge of it or the reactions of public health authorities by the mid british public health officials knew there was a new disease afflicting the cattle industry bse was first identified as the culprit only in there was a further delay of several years before the uk banned the use of mbm as a feed supplement due to o mad cow first histopathological disease the of bse in the united kingdom confirmation of clinical onset source uk ministry of agriculture fisheries and food august part the dynamics of growth the long incubation delay however confirmed cases continued to rise through declining only as animals in infected herds were destroyed by then how ever british beef was banned by the european union and shunned throughout the world worse many scientists fear that bse has been passed from contaminated beef or milk to the human population by confirmed and up to a dozen possible cases of a new variant of cjd had been identified all in the uk or france the new variant unlike traditional cjd primarily strikes young people because cjd has a very long incubation time years to decades it is not yet known whether these cases represent an isolated group or the first cases of a human epidemic caused by consumption of bse contaminated beef many sci entists fear that bse has crossed over from the livestock to the human population and may now begin to spread through exchange of blood products in july the uk government authorized its national health service to import blood plasma from nations apparently free of bse after it was discovered that two of the victims of were blood donors potentially threatening the integrity of uk blood supplies and vaccines prepared from blood products extending the sir model the sir model useful as it is invokes a number of restrictive assumptions like the si model of chronic infection the sir model does not incorporate births deaths or migration assumes the population is homogeneous does not distinguish between persons removed from the infectious population by recovery and the de velopment of immunity or by death and assumes immunity is permanent most importantly the model assumes there is no incubation period individu als infected with a disease in the sir model immediately become infectious in re ality most diseases have a latency or incubation period and people become infectious before exhibiting any symptoms of illness people exposed to chicken pox become highly infectious several days prior to the emergence of symptoms some to days after initial exposure many people infected with hepatitis a start to exhibit symptoms about a month after infection but become highly infec tious about weeks earlier the average incubation period for hiv the time be tween infection with hiv and the development of aids for adults not receiving treatment is about years the latency period between infection and the appear ance of symptoms for hepatitis c is even longer averaging perhaps years some four million people are thought to be infected with hepatitis c in the us and while about spontaneously recover in many other cases the disease even tually produces irreversible and often fatal liver damage hepatitis c is spread by exchange of blood products but only rarely through sexual contact modify the sir model by disaggregating the stock of infectious individuals into two categories asymptomatic infectives and symptomatic infectives the in fection rate moves people from the susceptible category into the asymptomatic infective population that is people who are infected with the disease but do not yet exhibit any symptoms after the incubation period people begin to exhibit symptoms typically while remaining infectious and move into the symptomatic chapter s shaped growth epidemics innovation diffusion and the growth of new products infective category assume that the rate at which people become sick is a order process with a constant average incubation period susceptible people can contract the disease by coming into contact with either symptomatic or asymptomatic infectives the contact rate and infectivity for asymptomatic and symptomatic individuals often differ once people fall ill be come symptomatic they often reduce their contact rate with the outside world ei ther to avoid infecting others or simply because they are too sick to follow their normal routine asymptomatic individuals in contrast usually do not know they are infected do not exhibit any symptoms and continue to contact others at their normal rate similarly the infectivity of a disease prior to the emergence of symp toms is often different from the infectivity after symptoms appear in measles for example people are most infectious from days prior to days after the ap pearance of the characteristic rash modify the formulation for the infection rate to capture the differing contact rates and infectivities of the symptomatic and asymp tomatic infective populations run the model for a hypothetical disease with an incubation period similar to chicken pox because the incubation period for chicken pox is to days as sume an average of days assume the average duration of illness is days set the contact rate for asymptomatic infectives to four per person per day but because those exhibiting symptoms remain in bed in self imposed quarantine set the con tact rate for the symptomatic population to only one per person per day assume in fectivity is for both asymptomatic and symptomatic populations also assume an initial population of all of whom are initially susceptible except for one asymptomatic infective person run the model and describe the results how does the inclusion of an incuba tion period affect the dynamics by the time of the population exhibits symp toms what fraction of the susceptible population remains uninfected how many susceptibles remain by the time of the population has become sick what is the impact of an incubation period on the effectiveness of quarantine you can simulate a perfect quarantine policy by setting the contact rate for the symptomatic population to zero or include the quarantine structure developed above explore the response of the epidemic to different incubation times and infectivities what is the effect of a long incubation period on the course of an epidemic and the abil ity of a population to enjoy herd immunity policy analysis just in time immunization evaluate the effectiveness of a policy of just in time jit vaccination that is vaccinating people only after evidence of an epidemic appears many people only get flu shots when they believe the flu in their area is particularly severe that year similarly some vaccines are so expensiverelative to the incidence of infection that they are not routinely given when previously unknown diseases strike vaccines cannot be made until after the disease emerges and is identified the appearance of new computer viruses leads to frantic efforts by programmers to come up with countermeasures but the resulting vaccines are not available until after the virus is identified the british government ban on the use of mbm as a feed part the dynamics of growth supplement for cattle is roughly equivalent to a policy of jit immunization the ban reduced the number of infectious contacts by removing the vector for bse from the diet of the cattle at risk only after the scientific evidence that mbm was the source of the epidemic became compelling enough and the public outcry great enough to overcome the political resistance of the cattle industry to model a jit vaccination policy create an immunization rate that transfers people from the susceptible population to a new stock the immunized population keeping the immunized population separate from the recovered population makes it easy to determine how many people ultimately get the disease formulate the immunization rate so that the vaccination program is deployed only after a certain number of cases have been diagnosed immunization if symptomatic population rate threshold for vaccination program rate vaccine effectiveness otherwise vaccination rate fraction vaccinated susceptible population time to deploy vaccine the immunization rate the actual rate at which people develop immunity from the vaccine differs from the vaccination rate by the effectiveness of the vaccine once the program is deployed it takes a certain amount of time the time to deploy vac cine to carry out the program the vaccination program may only reach a fraction of the population as well note that because the susceptibles cannot be distin guished from the asymptomatic infectives or recovered populations the entire population would have to be immunized with the possible exception of the symp tomatic infectives for whom vaccination would not be effective consequently the costs of the program depend on the total number vaccinated however those who have already recovered from the disease remain immune so they stay in the pool of recovered individuals the formulation also assumes that the vaccine is ineffective for those who have already been infected so there is no flow from the asymptomatic infective population to the immunized population to test the effectiveness of a jit immunization program make the strong as sumptions that a vaccine with effectiveness is available and that the fraction of the population vaccinated is further assume that the entire population can be vaccinated in just days once the threshold has been reached and the deci sion to deploy the vaccine has been made these conditions are unlikely to be achieved in reality but provide a strong test of the potential for jit vaccination strategies to address acute infectious disease explore the effectiveness of the jit vaccination policy by running the model for various thresholds starting with a threshold of of the total population what fraction of the total population eventually gets the disease how does that compare to the case without jit vaccination what if the threshold were just of the to tal population cases what is the effectiveness of jit vaccination when the vaccine is only effective only of the population is immunized and if the delay in deploying the vaccination program is week comment on the types of diseases for which jit vaccination is likely to be effective and those situations in which it will be ineffective be sure to consider the social as well as biological de terminants of compliance with a crash vaccination program chapter s shapedgrowth epidemics innovation diffusion and the growth of new products modeling the epidemic so far the hypothetical diseases examined have been highly infectious acute in fections similar to chicken pox or measles that generate rapid short lived epi demics over the course of such an epidemic it is reasonable to assume that the contact rate and other parameters are constants the epidemic develops too fast for significant changes in people behavior or for research on prevention or treatment to come to fruition these assumptions are not appropriate for diseases such as where the incubation time is long figure shows the incidence and mortality of aids in the us from through figure shows the prevalence of aids among adults in the the data show important shifts in the dynamics of the epidemic in the us in cidence of clinical aids indicated by the aids curve in figure grew steadily until about and has declined significantly since mortality closely follows incidence and exhibits an even larger decline the decline in mortality ex ceeds that in incidence due to the development of therapies including azt and most importantly the so called multidrug cocktails or haart highly active retroviral therapy overall incidence has declined due to a reduction in new cases transmitted by male homosexual contact and sharing of dirty needles by intra venous drug users incidence among women and due to heterosexual contact was still rising as i wrote this note that even before the introduction of haart how ever the growth of the epidemic was not a pure exponential as would be expected if the contact rate or infectivity were constant instead the fractional growth rate of aids incidence declined as the epidemic spread despite the great strides in treatments the improving outlook for the hiv epi demic in the wealthy nations is only part of the story there is as yet no effective vaccine and the long run effectiveness and side effects of haart remain un known more importantly haart is exceedingly costly and in many nations simply unavailable the incidence of hiv infection globally continues to rise and in many nations has long since passed the crisis point the scale of the epidemic is almost impossible to comprehend the who estimated that in about quarter of the entire population of zimbabwe was infected with hiv incidence in much of africa and some other developing nations is estimated to exceed ten million people have already died of aids in africa and without dra matic changes in access to treatment and drugs million more people are pro jected to die mortality and morbidity from hiv and aids in many of these nations have overwhelmed the health care systems the who estimates that in there were fewer than hospitals and doctors in all of zimbabwe a nation of more than million life expectancy is falling despite the decline in aids related mortality in some of the affluent nations of the world the pandemic is far from over the who reported that in aids was the fourth largest cause of death worldwide up from seventh in the massive effort and careful work of the cdc the data are highly uncertain and are adjusted in several ways to overcome various limitations in the us surveillance and reporting system the definitions of aids have changed over the years as understanding of the disease has improved data on hiv incidence and prevalence are even less complete and reliable readers are urged to consult the full surveillance reports and references therein for details part the dynamics of growth figure incidence and mortality of aids in the us estimated incidence aids opportunistic illness aids and deaths in persons with aids adjusted for delays in report ing by quarter year of united states quar terly data reported at annual rates estimated incidence quarter year of diagnos of aids includes persons diagnosed using the ex panded surveillance case definition which counts persons with se verely suppressed immune systems even though they do not yet suffer from opportunistic infec tions the old defini tion is now tracked by the incidence of which pro vides a more con sistent estimate of incidence over time points on the figure represent quarterly incidence rescaled to annual rates lines repre sent smoothed inci dence estimated incidence of aids estimatedaids ols and deaths are all adjusted for delays in reporting esti mates are not ad justed for incomplete reporting of cases source us centers for disease control and prevention surveillancereport midyear edition no figure and caption to interpret the data and develop a model it is useful to review the nature and course of hiv and aids the progression of can be divided roughly into the following categories after initial infection with hiv the virus replicates rapidly stimulating an immune response including the production of hiv specific antibodies the common clinical test for hiv does not detect the presence of the virus itself but rather the antibodies indicating the presence of sufficient virus to trigger the immune response there is a delay of several weeks to months before the body produces sufficient antibodies to yield a positive result from an hiv test prior to the point at which an infected person begins to test posi tive infected people can transmit the virus to others but will not test positive af ter seroconversion there is a long latency period during which there are no clinical symptoms the length of the incubation period varies widely the median incuba tion period is estimated to be about in previously healthy adults though it is shorter for children the elderly and those with prior health problems cooley myers and eventually the virus compromises the immune system so severely that the patient begins to suffer from a wide range of opportunistic in fections such as pnuemocystis pnuemonia and kaposi sarcoma which lead to the diagnosis of aids prior to the development of haart the mortality rate was extremely high about of all those diagnosed prior to in the us had died by the end of the mean survival time from the time of diagnosis in the absence of treat ment with haart is about to months though as with incubation survival times vary the long run effectiveness of haart is still unknown while it can reduce the viral load below detectable levels in some patients haart does clinical and epidemiological literature on is enormous and evolves rapidly a good source of information and references is available at the hiv developed by the university of california at san francisco http hivinsite ucsf edu chapter s shaped growth epidemics innovation diffusion the growth of new products figure prevalence of aids in the united states adults adolescents living with aids by quarter through june adjusted for reporting delay united states source us centers for disease control and prevention report no not completely eliminate hiv from the body cohen people who discover they are can begin and other treatments prior to the emergence of symptoms and are more likely to alter their behavior not all those who are at risk are tested of course so for many people the first indication that they are comes when they develop symptoms of aids m ing h ids a stock and flow structure of the epidemic based on the description above develop a stock and flow diagram representing the progression of individuals from susceptible through the various stages of hiv infection and aids include a stock of cumulative deaths in the sir model the population is assumed to be homogeneous a very poor assumption for modeling many epidemiological models of disaggregate the population into several categories that represent the different modes of transmission primarily homosexual contact heterosexual contact and intravenous drug use as well as gender age socioeconomic status region and perhaps other these groups overlap and interact creating an intricate feedback structure and a very complex model for the purpose of this challenge do not disaggregate the population after you develop a single aggregate model you can consider disaggregation to capture the different risky behaviors and their interactions are dozens of published epidemiological models of and other sexually transmitted diseases good starting points include anderson and anderson heidenberger and roth and roberts and dangerfield the entire may june issue of interfaces was devoted to modeling aids including policy issues such as needle exchanges vaccine development and hiv screening part the dynamics of growth b feedback structure of the epidemic once you have developed the stock and flow map add the feedback structure for the rates by following the assumptions of the extended sir model above including the infection rate rate of seroconversion aids diagnosis rate and death rate over the time frame for the development of the aids epidemic the parameters of the sir model such as the mortality rate and the contact rate between susceptibles and the various categories of individuals cannot be considered constant modify your causal diagram to incorporate feedbacks you believe are important be sure to consider the following the average contact rate may fall as people become aware of the risk of hiv and the ways in which it can be transmitted that is some people may reduce or abstain from use of intravenous drugs or sexual contact the infectivity of contacts depends on people behavior safer sex practices and the use of clean needles by iv drug users can reduce the infectivity of those contacts that do occur the use of safer sex practices and clean needles in turn depends on people awareness of the risk of hiv infection and its consequences and on the availability of information and resources about these practices in turn the availability of information about safer sex and the importance of needle cleaning along with condoms and clean needles depends on social attitudes and public health programs in the media schools and other community organizations research and development into treatments such as can reduce the mortality rate the availability of these treatments depends on the extent to which they are reimbursable through health insurance and on the willingness of people to get tested for hiv many people are unwilling to be tested even if they know they are at risk out of fear frequently well founded that they may be stigmatized including the possibility they might lose their jobs homes and friends in representing changes in people behavior changes in contact rates and infectivity consider how people might become aware of the existence severity and risks of hiv and different behaviors do they read the new england journal or get their information through word of mouth or personal acquaintance with someone suffering from aids how do people judge the risk of infection and the consequences of infection people are much more likely to contract the common cold than hiv but a cold does not inspire the dread hiv does what information sources are ordinary people exposed to and how persuasive are these in inducing changes in behavior what is the role of social attitudes toward the behaviors through which hiv can be transmitted what is the role of government policies be sure to consider the time delays in the feedbacks you identify use your causal diagram to explain the dynamics of the aids epidemic in the us in particular explain in terms of the feedback structure of the system why the fractional growth rate of the epidemic fell in the chapter s shaped growth epidemics innovation diffusion and the growth of new products early years explain the decline in incidence beginning about and the even steeper decline in mortality explain why the number of people living with aids continues to increase figure treatments such as haart hold the promise to convert hiv infection from a death sentence to a chronic infection with low mortality based on your diagram what changes in behavior might arise as a side effect of the development of haart how might these changes affect the contact rate or infectivity would these feedbacks increase or decrease the incidence of hiv infection explain what are the public health implications of successful treatments such as haart c simulating the epidemic develop a formal model of the epidemic based on the structure you identify above to do so you will need to use a number of nonlinear behavioral functions to capture the way perceptions of risk alter the contact rate or infectivity guidelines for the development of such nonlinear functions are found in chapter work at least initially with a single aggregate stock and flow structure and do not disaggregate your model into subpopulations sim ulate your model under at least two conditions on the assumption of no be havioral change and no improvement in treatments and including the feedbacks you identified in part b that might lead to changes in the contact rate in infectivity and in mortality explain the results test the policy recom mendations you identified in part b discuss the policy as infection modeling new and new products the diffusion and adoption of new ideas and new products often follows s shaped growth patterns what are the positive feedbacks that generate the initial exponen tial growth of a successful innovation and what are the negative feedbacks that limit its growth consider the spread of cable television figure the growth of the population of cable television subscribers cannot be explained by the birth of children to existing cable subscribers though the offspring of heavy tv view ers do tend to grow up to be couch potatoes what then are the positive loops re sponsible for the growth of the cable industry the spread of rumors and new ideas the adoption of new technologies and the growth of new products can all be viewed as epidemics spreading by positive feed back as those who have adopted the innovation infect those who have not the concept of positive feedback as a driver of adoption and diffusion is very general and can be applied to many domains of social contagion the feedback struc ture of the cocaine epidemic described in section a rumor spreads as those who have heard it tell those who have not who then go on to tell still others new ideas spread as those who believe them come into contact with those who do not the purpose of this challenge it is acceptable to model the transitions from one category of infected individual to the next as first order processes more realistic models represent the incuba tion and mortality distributions derived from empirical studies more accurately through the use of higher order delays see chapter part the dynamics of growth and persuade them to adopt the new belief the new believers in turn then persuade others as early adopters of new technology and early purchasers of a new product expose their friends families and acquaintances to it some are persuaded to try it or buy it themselves in all these cases those who have already adopted the prod uct idea or technology come into contact with those who have not exposing them to it and infecting some of them with the idea or the desire to buy the new product and further increasing the population of adopters any situation in which people imitate the behavior beliefs or purchases of others any situation in which people jump on the bandwagon describes a situation of positive feedback by social con tagion of course once the population of potential adopters has been depleted the adoption infection rate falls to in the cable television case important factors in a household decision to sub scribe assuming cable is available in the community include favorable word of mouth from those who already subscribe and positive experiences viewing cable at the homes of friends and family people hear about programs only available on ca ble and feel they must subscribe to be hip and knowledgeable among their peers in school or at the workplace additionally people may subscribe to keep up with the that is to maintain or enhance their status or their perception of their sta tus among their peer group all of these channels of awareness and motivations for adoption create positive feedbacks analogous to the contagion loop in the basic epidemic model figure adapts the si epidemic model section to the case of inno vation diffusion the infectious population now becomes the population of adopters a those who have adopted the new idea or purchased the new product the susceptible population becomes the pool of potential adopters p adopters and potential adopters encounter one another with a frequency determined by the con tact rate c unlike infectious diseases word of mouth encounters that might lead to adoption could occur by telephone mail or other remote means and do not require physical proximity as in infectious disease not every encounter results in infection the proportion of contacts that are sufficiently persuasive to induce the potential adopter to adopt the innovation is termed here the adoption fraction and denoted i since the adoption fraction is analogous to the infectivity of a dis ease in the epidemic model the equations for the simple innovation diffusion model are identical to those for the si model of chronic infection described in using the terminology in figure the model is a a p integral ar n ar as in the si model the total population n is constant literature on diffusion of new products and of social and technical innovations is huge a good place to start is everett rogers diffusion of innovations a classic originally published in for diffusion models applied to the sales of new products see parker and mahajan muller and bass chapter s shaped growth epidemics innovation diffusion and the growth of new products potential adopters adoption of a new idea or product as an epidemic potential adopters come into contact with adopters through social interactions a fraction of contacts result in adopters a contact adoption infection that is rate total fraction adoption of the new idea or purchase of the new product compare to population i n the interpretation is the same as in the si model people in the relevant com munity come into contact at a rate of c people per person per day the total rate at which contacts are generated by the potential adopter pool is then the propor tion of adopters in the total population gives the probability that any of these contacts is with an adopter who can provide word of mouth about the innovation finally the adoption fraction is the probability of adoption given a contact with an adopter as before these equations constitute an example of the logistic growth model discussed in section the behavior of the model is the classic s shaped growth of the logistic curve figure the logistic model of innovation diffusion examples the diffusion of many new products follow roughly logistic trajectories as an ex ample figure shows sales of the digital equipment corporationvax minicomputer in europe modis the vax series was a very successful line of minicomputers they sold for about a unit depending on which peripherals such as tape drives were included an excellent value com pared to the mainframes of the day typical customers were large companies research organizations and universities who used them for data processing appli cations and to support scientific and engineeringcomputation in labs prod uct development departments and academic research the was introduced in sales follow the classic bell shaped product life cycle peaking in mid the product was withdrawn from the market around accumulated sales follow an s shaped path since the useful lifetime of the vax is long com pared to the time horizon for the product life cycle it is reasonable to assume that few units were discarded prior to when the product was withdrawn from the market therefore cumulative sales is a good measure of the installed base to fit the logistic product diffusion model to the vax sales data assume the total population n cumulative sales by about units the remaining parameter of the model representing the product of the contact rate and adoption figure sales of the digital equipment corporation vax in europe top sales rate quarterly data at annual rates bottom cumula tive sales roughly equal to the installed base part the dynamics of growth source modis fraction ci can be estimated easily by linear first recall from equa tion that the solution to the logistic equation can be expressed using the vari able names for the innovation diffusion model above as where a is the number of adopters the installed base a is the initial installed base n is the total equilibrium or final value of adopters and is the initial frac tional growth rate of the installed base which at the initial time when there are very few adopters is equal to the number of infective contacts ci taking the nat ural log of both sides yields a relationship that is linear in the parameters and can be estimated by ordi nary least squares note that since n a p the difference between the total the adoption rate depends the product of the contact rate and infectivity these parameters cannot be estimated separately from sales data however market research techniques such as test markets focus groups surveys and so on can help the modeler develop estimates of the adoption fraction and contact rate for forecasting purposes only the product is needed chapter s shaped growth epidemics innovation diffusion and the growth of new products population and the number of adopters is the number of potential adopters equa tion can be written more intuitively as a function of the ratio of adopters to potential adopters equation or is known as the logistic or simply logit transformation figure shows the logit transformation of the vax sales data along with the estimated linear regression the data are close to linear which indicates excellent correspondence to the logistic model the bottom panels of figure compare the estimated logistic curve to the sales and installed base data while the logistic innovation diffusion model fits the vax sales data quite well the estimation of the model was retrospective the entire sales history was used and the estimation method required knowledge of the final value of the in stalled base in most business situations however the clients want to know the likely growth path prospectively when the market potential is not known so they can decide whether the market will be big enough to justify entry and plan strategy for capacity acquisition pricing marketing and so on one way to fit the logistic growth model to data prior to saturation is to estimate the rate at which the frac tional growth rate declines with growing population recall from equation that the fractional growth rate of the logistic model declines linearly as the popu lation grows figure shows the fractional growth rate in cable television sub scribers in the us along with the best linear fit calculated by ordinary least squares as expected the fractional growth rate declines as the population grows though there is considerable variation around the best linear fit the logistic growth path implied by these parameters fits the data well through and predicts a maximum of about million households subscribing to cable reached shortly after there is however considerable uncertainty in this prediction first there is uncertainty regarding the best fitting linear fractional growth rate the actual frac tional growth rate varies substantially around the best fit other parameters for the straight line will fit nearly as well yet yield large differences in the maximum num ber of subscribers and time to saturation second the best fit was estimated for the period prior to the fractional growth rate was much higher this is typical of growth processes the fractional growth rate early in the history of a new product or innovation is often very high since the population of adopters is so small and of course when a new product is introduced the growth rate for the first reporting period is infinite changing the historical period over which the logistic model is estimated will therefore change the best fit parameters and the forecast third the logistic model presumes a linear decline in the fractional growth rate as the population grows there is however no compelling theoretical basis for lin earity other shapes for the fractional growth rate curve will yield very different predictions to illustrate figure shows the cable television data against the best fit of both the logistic and gompertz curves equation the gompertz curve fits the data about as well as the logistic curve but suggests continued growth to nearly million subscribers in double the final level predicted by the logistic model part the dynamics of growth figure fitting the logistic model of innovation diffusion top applying the transforma tion equation shows that the log of the ratio of adopters to po tential adopters over time is very close to linear the best fit is found by linear regression middle estimated and actual in stalled base adopters using the estimated pa rameters bottom estimated and actual sales adoption rate using the esti mated parameters process point historical fit and model validity the logistic model is widely used to explain and predict the diffusion of innova tions the growth of populations and many other phenomena marchetti and modis among many others have fit the logistic curve to a wide range of data from the compositions of mozart to the construction of gothic cathedrals the logistic curve can fit data for a wide range of growth processes reasonably well but you should not use the logistic model or any model as a curve fitting procedure for black box atheoretical forecasting the logistic model often works well because it includes the two feedback processes fundamental to every growth chapter s shaped growth epidemics innovation diffusion and the growth of new products figure fitting the logistic model to data us cable tv suibscribers top estimatedl arid fractional growth rate of cable bottom actual subscribers vs subscribers from cable subscribers million households the growth rate figure predicted cable subscribers differ greatly depending on the growth model used i process a positive loop that generates the initial period of accelerating growth and a negative feedback that causes the growth to slow as the carrying capacity is ap proached any system growing by positive feedback must include these two types of loops coupled nonlinearly any growth model must be characterized by a frac tional growth rate that ultimately declines to zero as the population approaches its carrying capacity however as discussed above the logistic model makes restric tive assumptions about the nature of the growth process advocates of the logistic model often present evidence selectively to show how well the model fits certain part the dynamics of growth data but omit the many other growth processes for which it does not the same considerations apply to all other single equation growth models such as the richards or weibull family the ability of the logistic model to fit a wide range of growth processes also illustrates several important lessons about the validity of models in general first the contrast between the forecasts of the logistic and gompertz models for the ca ble television case shown in figure shows that different diffusion models can produce wildly different predictions while fitting the data equally well the ability to fit the historical data does not therefore provide a strong basis for selecting among alternative hypotheses about the nature or strength of different feedbacks that might be responsible for a system dynamics second getting more data does not solve the problem estimating the parame ters of different growth models and hence the trajectory of growth by economet ric techniques requires a long enough set of time series data to provide stable parameter estimates and to discriminate among the different growth models in a review of innovation diffusion models for new product sales forecasting mahajan muller and bass note that by the time sufficient observations have devel oped for reliable estimation it is too late to use the estimates for forecasting pur poses by the time cable television has progressed far enough to discriminate between the logistic and gompertz and possibly other models so much of the diffusion life cycle will be past that the model will no longer be useful the wildly different forecasts of cable diffusion are generated years after the introduction of cable after about half the households in the us had already adopted it and well after the entry to the industry of formidable competitors third a main purpose of modeling is to design and test policies for improve ment to do so the client must have confidence that the model will respond to poli cies the same way the real system would fitting the logistic curve or any model to a data set does not identify the specific feedback processes responsible for the dynamics the ability of a model to fit the historical data by itself provides no in formation at all about whether its response to policies will be correct to illustrate note that the logistic model like all first order growth models presumes that the adopter population or installed base moves steadily upward the number of adopters can never decline yet the history of new products and new technologies is replete with innovations whose pattern of emergence is boom and bust or fluctuation it is easy to imagine credible scenarios in which for example cable television use declines including rising prices declining quality of pro gramming increasing competition from new technologies such as digital satellite broadcasts and the internet and even a decline in television viewing well perhaps that last one isn t credible yet the logistic model and all first order models can never generate anything but growth these models do not include the rich feedback structure needed to generate more complex and realistic patterns such as overshoot and oscillation or overshoot and collapse innovation diffusion usually involves many positive feedbacks driving growth besides word of mouth see chapter examples for example the availability of third party software is a powerful driver of product attractiveness for computers in turn third party developers will write software for those platforms they believe have the greatest market potential thus the larger the installed base of a particular chapter s shaped growth epidemics innovation diffusion and the growth of new products computer such as the vax the more software will be written for it the more at tractive it becomes to potential customers and the larger the installed base will be the positive software availability loop and many others can stimulate growth just as the word of mouth loop can because these other positive loops are omitted from the simple innovation diffusion model statistical estimates of the strength of the word of mouth loop will reflect the impact of all positive loops contributing to the growth the importance of word of mouth will be greatly overestimated while at the same time the strength of all the omitted loops is assumed to be zero the model would indicate that a good policy to stimulate the early growth of the mar ket would be to strengthen word of mouth say by sponsoring conferences or hir ing key opinion leaders as spokespeople however the best policy may actually be to stimulate software availability by partnering with third party developers such a model will not yield reliable policy recommendations despite the excellent historical fit a model may fit the data perfectly for the wrong reasons is it difficult to overemphasize the implications for modelers and clients the ability of a model to replicate historical data does not by itself indicate that the model is useful and failure to replicate historical data does not necessarily mean a model should be dismissed the utility of a model cannot be judged by historical fit alone but requires the modeler to decide whether the structure and decision rules of the model correspond to the actual structure and decision rules used by the real people with sufficient fidelity for the client purpose to do so requires the mod eler and client to examine the assumptions of the model in detail to conduct field studies of decision making and to explore the sensitivity of model results to plau sible alternative assumptions among other tests determining whether a model provides a sound basis for decision is never a matter only of statistical test ing or historical fit but is essentially and unavoidably a value judgment the mod eler and client must make unfortunately clients and modelers frequently give historical fit too much weight judging the appropriateness of the model structure its robustness and its sensitivity to assumptions takes time while historical fit can be demonstrated quickly graphs showing a close fit between data and model are dramatic and compelling clients are too easily swayed by such graphs and by impressive tables of and other statistics modelers even when they know better too often over emphasize statistics showing how well their models fit the data to persuade the au dience that the strong historical fit of the model means it must be correct you should not conclude from this discussion that historical fit is unimportant or that you do not need to compare your models to the numerical data on the con trary comparing model output to numerical data is a powerful way to identify lim itations or flaws in model formulations but there is a profound difference between using historical data to identify flaws so your models can be improved and using historical fit to assert the validity of your model in the latter case showing how well the model fits the data is a defensive maneuver designed to protect a model and the modeler from criticism and seals the client and modeler off from learn ing in the former case the historical fit is used to find problems and stimulate learning examining historical fit should be part of a larger process of testing and model improvement designed to yield a model suitable for policy design and deci sion making see chapter part the dynamics of growth the bass diffusion model one of the flaws in the logistic model of innovation diffusion is the startup prob lem in the logistic and the other simple growth models including the richards and weibull families zero is an equilibrium the logistic model cannot explain the genesis of the initial adopters prior to the introduction of cable television the number of cable subscribers was zero prior to the first sales of the vax minicom puter the installed base was zero when growth processes begin positive feed backs depending on the installed base are absent or weak because there are no or only a few adopters initial growth is driven by other feedbacks outside the bound ary of the simple diffusion models there are several channels of awareness that can stimulate early adoption of new innovations besides word of mouth and related feedback effects that depend on the size of the adopter population these include advertising media reports and direct sales efforts frank bass developed a model for the diffusion of innovations that overcomes the startup problem the bass diffusion model has become one of the most popular models for new product growth and is widely used in marketing strategy management of technology and other fields bass solved the startup prob lem by assuming that potential adopters become aware of the innovation through external information sources whose magnitude and persuasiveness are roughly constant over time the original bass model was introduced primarily as a tool for forecasting sales of new products and bass did not specify the nature of the feedbacks at the operational level the positive feedback is usually interpreted as word of mouth social exposure and imitation and the external sources of awareness and adoption are usually interpreted as the effect of advertising figure shows the feedback structure of the model with this interpretation in figure the total adoption rate is the sum of adoptions resulting from word of mouth and implicitly other positive feedbacks driven by the population of adopters or the installed base of the product and adoptions resulting from adver tising and any other external influences adoptions from word of mouth are for mulated exactly as in the logistic innovation diffusion model equation bass assumed the probability that a potential adopter will adopt as the result of ex posure to a given amount of advertising and the volume of advertising and other external influences each period are constant therefore the external influences cause a constant fraction of the potential adopter population to adopt each time pe riod hence the adoption rate ar is ar adoption from advertising adoption from word of mouth adoption from advertising original model in continuous time was specified as ar where a and b were parameters to be estimated statistically from the data on sales or adopters bass did not explicitly discuss the feedback loop structure of the model or specify what the processes of adoption were operationally instead calling them innovation and imitation others refer to the two loops as external and internal influences on adoption the model was also criticized for omitting economic and other variables that affect the adoption decision such as price or advertising effort see the challenges below see also bass krishnan and jain chapter s shaped growth epidemics innovation diffusion and the growth of new products figure the bass diffusion model the model includes an external source of and adoption usually interpreted as the effect of is ng adoption from word of mouth where the parameter a advertising effectiveness is the fractional adoption rate from advertising period the two sources of adoption are assumed to be independent collecting terms the model can be expressed more compactly as ar when an innovation is introduced and the adopter population is zero the only source of adoption will be external influences such as advertising the advertising effect will be largest at the start of the diffusion process and steadily diminish as the pool of potential adopters is depleted part the dynamics of growth behavior of the bass model the bass model solves the startup problem of the logistic innovation diffusion model because the adoption rate from advertising does not depend on the adopter population when the innovation or new product is introduced the adoption rate consists entirely of people who learned about the innovation from external sources of information such as advertising as the pool of potential adopters declines while the adopter population grows the contribution of advertising to the total adoption rate falls while the contribution of word of mouth rises soon word of mouth dom inates and the diffusion process plays out as in the logistic diffusion model as an example consider again the vax sales data figure to model the vax product life cycle with the logistic diffusion model it was neces sary to start the simulation after the product was introduced so that there was a nonzero installed base a close look at figure shows that the pure logistic model underestimates sales during the first year and a half and overestimates sales at the peak consistent with the hypothesis that initial adoptions were stimulated not by word of mouth or other positive feedbacks but by external sources of aware ness such as marketing effort figure compares the behavior of the bass model to the logistic diffusion model and the vax sales data as in the simulation of the logistic model the total population n is assumed to be units advertis ing effectiveness a and the number of contacts resulting in adoption from word of mouth ci were estimated by regression to be year and per year re spectively the contribution of sales from advertising to total sales is small after the first year as seen in the bottom panel of figure nevertheless this mod est change in the feedback structure of the diffusion process improves the model ability to fit the sales data both in the first years and at the peak most important the inclusion of the advertising effect solves the startup problem of the logistic model the bass model is a significant and useful extension of the basic logistic model of innovation diffusion the model itself or variants of it is broadly ap plicable to a wide range of diffusion and growth phenomena and there is a large literature applying the bass model and related models to innovation diffusion and sales of new products see mahajan muller and bass and parker for reviews chapter s shaped growth epidemics innovation diffusion and the growth of new products figure the bass arid logistic models compared to actual vax sales m 1983 1986 challenge extending the bass model as noted above the bass model assumes the total size of the market total popula tion n is constant in general the population of a community or the number of households in a market grows over time through births deaths and migration in the context of innovations with very short life cycles the latest generation of video games or acute diseases such as measles the assumption of constant popu lation is reasonable but for innovations or diseases whose life cycles extend over many years the diffusion of cable television or the aids epidemic popula tion growth can be significant part the dynamics of growth a revise the bass model to incorporate growth in the size of the total market assume the total population size is a stock increased by a net population increase rate that aggregates births deaths and net migration it is easy but not necessary to represent explicit birth death and net migration rates separately see chapter the net population increase rate is given by the total population and the fractional net increase rate which can be assumed constant now you must decide how the increase in population is partitioned between potential adopters and adopters the simplest assumption is that all increases in population size add to the pool of potential adopters recalling that all people or households in the population are either potential or actual adopters reformulate the potential adopter population as p n a even though the potential adopter population is a stock it is fully determined by the total population and adopter population and so can be represented as an auxiliary variable apply your extended model to the cable television industry the unit of adoption for cable television is not the individual but households and possibly businesses the cable television industry in the us began in the early at that time the number of households was about million by the late there were nearly households an average household formation growth rate of about select parameters for advertising effectiveness and word of mouth that approximately replicate the pattern of cable adoption figure it is not necessary to match the data exactly an approximate fit is sufficient explore the sensitivity of the model to different population growth rates what is the impact of variable population size on the dynamics b response of total market size to price in most markets only a fraction of the total population will ever adopt a new innovation the fraction of the population that might ever adopt typically depends on the benefits of the innovation relative to its cost its price and any other associated costs switching costs costs of complementary assets training costs etc see rogers innovations are not static their benefits often increase over time as research and product development lead to improvements in features functionality quality and other attributes of product attractiveness similarly the price of new products often falls significantly over time through learning curves scale economies and other feedbacks see chapter modify the model you developed in part a to include the effect of product price on the size of the potential adopter pool assume the potential adopter population is a fraction of the total population less the current number of adopters p fraction willing to adopt n a where in general the fraction willing to adopt depends on the overall attractiveness of the innovation or product its benefits relative to costs fraction willing to adopt attractiveness chapter s shaped growth epidemics innovation diffusion and the growth of new products to keep the model simple assume that the only feature of the innovation that varies is the price a simple assumption is that the demand curve for the product is linear for the case of cable television assume that demand is a linear function of the monthly cost ignore installation charges since nearly all us households have television assume the fraction willing to adopt would be when cable is free and would fall to zero if the cost were to test the model assume price is exogenous first run the model with price constant at an initial value of to focus on the dynamics of the price effect alone assume the fractional net increase rate for the total population is zero since price is constant the total population should be constant and the model reduces to the original bass formulation simulate your model to check that it behaves appropriately when the price is constant now test the impact of varying prices by assuming the price of cable falls over time first test the effect of a sudden drop in price to say try the price drop at different points in the diffusion life cycle c is your model robust typically prices fall over the life cycle of a successful new product or innovation but models must be robust and behave appropriately under all potential circumstances not only the historical or expected behavior a common and important way to test for robustness is the extreme condi tions test chapter in an extreme conditions test an input to a model is assumed to suddenly take on extreme values the model must continue to behave appropriately even if that extreme value will never arise in reality for example suddenly destroying all inventory in a model of a manu facturing firm must force the shipment rate immediately to zero if shipments do not fall to zero the modeler and client immediately know there is a basic flaw in the model as an extreme conditions test in your model suddenly raise the price to a very large number such as million per month if prices were to rise that much what must happen to the potential adopter population to the population of subscribers implement the price rise at various points in the life cycle does your model behave appropriately what problem or problems are revealed by the test propose implement and test revisions that correct any problems you identify d interaction of diffusion and the learning curve the prices of many new products and services fall over time as learning scale economies and other effects lower costs and as competition intensifies make the product price an endogenous part of the model structure by incorporating a learning curve learning or experience curves capture the way in which producers distributors and others in the value chain learn to produce at lower costs as they gain experience usually costs are assumed to fall as cumulative experience with the product or service grows in a manufacturing setting cumulative experience is usually proxied by cumulative production in a service industry cumulative experience might better be represented as depending on the cumulative number of transactions part the dynamics of growth and will depend on the adopter population and the number of transactions each adopter generates per time period typically unit costs fall by a fixed percentage with every doubling of experience cost reductions of to per doubling of cumulative experience have been documented in a wide range of industries see teplitz gruber argote and epple to incorporate a learning curve into your innovation diffusion model first assume that any cost reductions are fully passed into price price initial price effect of learning on price effect of learning on price cumulative experience initial cumulative experience cumulative experience rate initial cumulative experience the exponent c determines how strong the learning curve is and should be negative costs fall as cumulative experience grows to represent a learning curve in which costs fall for each doubling of experience set c test your model for a hypothetical manufacturing firm that introduces a new consumer durable product in the year the product has high market potential set the following parameters assume that of the total population of million households will purchase the product if it is free but that demand falls to zero if the price is per unit set the initial price to and set initial experience to million units reflecting learning gained on prior products and prototypes research on consumer durables shows typical product life cycles last from about a year to as long as years or more parker depending on the cost benefits size trialability novelty and other attributes of the product along with the role of complementary assets and other infrastructure computers aren t valuable without software a television is useless without programming people want to watch and networks or cable operators to distribute it to capture the range define the following two scenarios for the diffusion of the product a slow scenario in which the product of the contact rate and adoption fraction ci is and a fast scenario in which the product of the contact rate and adoption fraction ci is assume for both scenarios that of the potential adopter pool will purchase as the result of advertising per year a learning curve where costs c fall by a fixed fraction per doubling of experience e costs are given by c when e has doubled costs have fallen by a fraction so or c for a learning curve with f c since the fractional cost reduction per doubling of experience has a more intuitive meaning than the exponent c it is convenient to formulate c the model as a computed constant c and then specify the cost reduction fraction f as a constant chapter s shaped growth epidemics innovation diffusion and the growth of new products fad and fashion modeling the abandonment of an innovation the bass diffusion model is analogous to the si model of chronic infection every one eventually adopts the product and adopters never abandon the innovation or discontinue use of the product these assumptions are appropriate for some inno vations but do not apply to the huge category of fashions and fads a fad by definition involves the temporary adoption of a new idea or product followed by its abandonment in a fad those who adopt sooner or later usually sooner discontinue their use of the product and no longer generate word of mouth that might lead to further adoption though many baby boomers wore nehru jack ets or granny dresses in the polyester leisure suits or rainbow colored plat form shoes in the and power suits with yellow ties or shoulder pads in the few are seen wearing them fad and fashion are of course common in the apparel industry but also arise in nearly every other domain from home fur nishings vacation destinations cuisines automobiles and investments to styles in the arts and music academic theories in the sciences and humanities and hot new buzzwords and gurus in corporate strategy organizational theory and management consulting the bass model cannot capture the dynamics of fads in part because adopters never discontinue their use of the innovation further because the contact rate and adoption fraction are constant the earliest adopters are just as likely to infect po tential adopters as those who just purchased the product for many innovations not only fads however people propensity to generate word of mouth and their enthusiasm and persuasiveness vary over time usually word of mouth decays as people become habituated to the innovation those who have recently embraced a fashion industry frequently reintroduces old fashions the fashions of the were recycled in the late including platform shoes and bellbottoms but thankfully not polyester leisure suits part the dynamics of growth new idea or purchased a new item are much more likely to talk about it than those who have long since adopted the innovation even if they continue to use it indoor plumbing can hardly be considered a passing fad yet people do not rush out to tell their friends how wonderful flush toilets are in the selchow and righter sold tens of millions of copies of the game trivial pursuit sales boomed as trivial pursuit became one of the hottest products in the toy and game market the boom was fed largely by word of mouth and so cial exposure as people played it at the homes of their friends after a few years however sales slumped much of the decline can be attributed to the familiar mar ket saturation loop the game was so successful the company depleted the pool of people who had not yet purchased a copy but the fading novelty of the game was also a factor the population of adopters is still large in the sense that many people still own a copy of trivial pursuit however most of these copies are in attics and closets the population of active adopters those who still play the game and gen erate word of mouth contacts with others is small discontinuation of use and the decay of word of mouth can easily be incorpo rated in the innovation diffusion framework by disaggregating the adopter popula tion into different categories each representing different degrees of use and propensities to generate word of mouth the simplest extension of the model is to divide the total adopter population into two categories active adopters and for mer adopters the discontinuation rate the rate at which active adopters b ecome former adopters depends on the average duration of use for the innovation the simplest assumption is that the discontinuation rate is a first order process word of mouth and hence the adoption rate would be generated only by the population of active adopters the revised model is analogous to the sir epidemic model now only active adopters analogous to the infectious population generate word of mouth that might induce additional adoption former adopters are analogous to the population of recovered individuals having purchased the product but no longer actively us ing it the former adopters are no longer infectious to others and are also immune to reinfection exposure to advertising or word of mouth from active adopters won t induce aging baby boomers to buy another leisure the key insight from the sir epidemic model is the concept of the tipping point exposure to infectious individuals will not produce an epidemic if people re cover and develop immunity faster than they can infect others similarly new in novations might fail to take hold even if they generate positive word of mouth because active adopters discontinue usage faster than they persuade others to adopt though a distressingly large number of bizarre fashions and useless prod ucts are embraced by eager consumers who mindlessly allow marketers to manip ulate their tastes many more fail to take hold in the sir model the tipping point is defined by a reproduction rate of one the reproduction rate is the number of new cases generated by each infective prior in the extended sir models developed in section the population of adopters could be disaggregated further for example into cohorts of people who adopted the innovation n time periods ago to capture situations where the contact rate and adoption fraction decay gradually rather than in a first order pattern chapter chapter s shaped growth epidemics innovation diffusion and the growth of new products to recovery and is defined by the product of the contact number the number of in fectious contacts generated by each infective prior to recovery and the probability of contacting a susceptible individual equation using the terminology of the innovation diffusion framework the reproduction rate is the product of the number of persuasive word of mouth contacts generated by each active adopter prior to discontinuation and the probability of encountering a potential adopter where c is the contact rate is the adoption fraction d is the average duration of active use for the innovation p is the population of potential adopters and n is the total population if the reproduction rate is greater than one the positive word of mouth feed back dominates the system and a fad is born the fad ends when the pool of po tential adopters falls enough to bring the reproduction rate below one if the reproduction rate is less than one the positive word of mouth loop is dominated by the negative feedbacks and there will be no epidemic of adoption however unlike the sir model adoption in the bass innovation diffusion framework also arises from advertising and other external influences in the sim ple bass model the effectiveness of advertising is a constant implying that the ad vertising budget is constant through time even when the system is below the tipping point everyone will eventually adopt the innovation though it may take a very long time in the real world advertising is expensive and does not persist indefinitely the marketing plan for most new products includes a certain amount for a ad campaign and other initial marketing efforts if the product is successful further advertising can be supported out of the revenues the product generates if how ever the product does not take off the marketing budget is soon exhausted and ex ternal sources of adoption fall advertising is not exogenous as in the bass model but is part of the feedback structure of the system there is a tipping point for ideas and new products no less than for diseases part the dynamics of growth replacement purchases the bass diffusion model is often described as a first purchase model because it does not capture situations where the product is consumed discarded or upgraded all of which lead to repeat purchases one popular way to model repeat purchase behavior is to assume that adopters move back into the population of potential adopters when their first unit is dis carded or consumed the rate at which the product is discarded and therefore the rate at which people move from the adopter population to the pool of potential adopters depends on the number of adopters and the average life of the product figure modeling replacement demand in this fashion is analogous to the loss of immunity to a disease now instead of falling to zero the potential adopter population is constantly replenished as adopters discard the product and reenter the market figure the adoption rate sales rate for a product rises peaks and falls to a rate that depends on the average life of the product and the parameters de termining the adoption rate discards mean there is always some fraction of the population in the potential customer pool by varying the product life and strength of the word of mouth feedback the rate of diffusion including the height of the sales peak and the depth of the bust when the market saturates can be varied the model shown in figure assumes a first order discard process but can easily be modified to represent any distribution of discards around the average product life using higher order delays chapter because those discarding the product reenter the potential customer pool they are treated exactly like first time buyers and must go through another process of becoming aware of and being persuaded to buy the product through advertising or word of mouth in some cases the lifetime of the product is so long and the attri butes of the product change so much over this span that prior experience is largely irrelevant and repeat purchase decisions are reasonably similar to initial pur chase decisions but for most products the customer has already made the decision to continue using the product and simply purchases a new one in such cases the initial and repeat purchase decisions must be represented separately as shown in figure here the adoption process is separated from the flow of purchases the total sales rate is the sum of initial purchases and repeat purchases the repeat purchase rate is the product of the number of adopters and the average number of units purchased by each adopter per time period figure shows typical model behavior chapter s shaped growth epidemics innovation diffusion and the growth of new products figure discard and replacement purchases customers discard the product with a constant given by the rage product life then return to potential adopter pool figure behavior of the bass model with discards and repurchases shows the ior of the model in figure with an average i a c years part the dynamics of growth figure modeling repeat purchases sales repeat average consumption total sales consist of initial and repeat purchases each potential adopter buys initial sales per adopter units when they first adopt the product and continues to purchase at the rate of average consumption per adopter thereafter figure behavior of the repeat purchase model behavior of model in figure with the same parameters as in figure and a total population of million initial purchases of unit per person and replacement purchases of units per person per year per adopter rate rate years per adopter years chapter s shaped growth epidemics innovation diffusion and the growth of new products modeling the life cycle of durable products the model in figure does not explicitly represent the stock of product held by the adopter population repeat purchases are modeled as depending on the current adopter population this formulation is appropriate for nondurable consumables such as food where the lifetime of the product is very short relative to the diffu sion process and the acquisition and consumption or discard of the product do not need to be represented separately for durable products however it is usually im portant to represent the stock of product and the discard rate explicitly modify the model shown in figure to represent the adoption and installed base of a consumer durable such as video cassette recorders vcrs you may use either the simple bass formulation or the extended model you developed in section including the effect of price on the size of the market and the learning curve when vcrs were introduced the average price was about but by the the average price had fallen to the discard rate for durable products is often strongly age dependent and is not well approximated by a first order process to model the installed base of vcrs create two stocks new vcrs and old vcrs the stock of new vcrs is increased by the purchase rate as vcrs age they move into the stock of old vcrs as sume the average vcr remains new for years and that the aging rate is a order process though some new vcrs do break down and are discarded for simplicity assume the discard rate of new vcrs is zero assume the average life time of old vcrs is years giving a total average life of years what determines the purchase rate first households that have adopted the vcr will seek to replace those that break down and are discarded or discard a us able unit to buy a new one with better features second those households will buy more than the discard rate when the number of vcrs they collectively desire ex ceeds the number they actually have and will buy less than the discard rate should they find they have more than they desire the purchase rate is then the sum of the discard rate and an adjustment for installed base the adjustment for the base is most simply formulated as a simple first order negative feedback process adjustment for installed base desired vcrs total vcrs stock adjustment time where the total number of vcrs is the sum of the new and old vcr stocks and the stock adjustment time represents the average time required for people to shop for and purchase a vcr define the desired stock of vcrs as depending on the num ber of households that have adopted the vcr and the average number of vcrs de sired per household the challenge to extend the bass model in section introduced the notion that the fraction of the population willing to adopt the prod uct depends on its overall attractiveness including price the growth of markets for many important products involves both a process of adoption by an increasing fraction of the population and a gradual increase in the number of units owned by each household as real prices fall and as household income rises when they were first introduced households made do withjust one car phone tv and computer as prices fell quality rose and the importance of these products in daily life grew part the dynamics of growth the number per household grew by explicitly representing the number of units de sired per adopter the model can represent situations where adopters increase the number of units they desire as their income grows as price declines or as other at tributes of product attractiveness improve for now assume the number of vcrs desired by each adopter household is one note that if the desired stock of vcrs increases above the actual stock say because prices fall so more adopting households decide to buy a second vcr the purchase rate will rise above the discard rate and the stock will increase until the gap is closed should the desired stock of vcrs fall the purchase rate will fall below the discard rate and the stock of vcrs will gradually fall test the robustness of your formulation for the purchase rate implement an extreme conditions test in which the desired stock of vcrs suddenly falls to zero from an initial situation in which the desired and actual stocks are equal and large modify your formulation for the purchase rate to ensure that it behaves appropri ately even if there are many more vcrs than desired vcrs for the home market were introduced by sony in by the early approximately of us households had at least one vcr select parame ters for your model that are roughly consistent with these data use your judgment to estimate the other parameters such as the stock adjustment time for simplicity assume the total number of households in the us is and constant sim ulate the model and discuss the results in particular what is the pattern of adop tion what is the pattern of sales since the introduction of the vcr the average duration of product life cycles for consumer electronics computers and many other products has shrunk life cy cles of just a few years or even less are common simulate the model assuming the word of mouth feedback is three times as strong as the you selected for the vcr case how long does it now take for of households to adopt the product what are the implications for sales why what difficulties do short product life cycles pose for firms summary s shaped growth arises through the nonlinear interaction of positive and negative feedback loops any growing quantity can be thought of as a population growing in an ecological niche with a certain carrying capacity s shaped growth arises when the carrying capacity is fixed and when there are no significant delays in the reaction of the population growth rate as the carrying capacity is approached the structure underlying s shaped growth applies to a wide range of growth processes not only population growth these include the adoption and diffusion of new ideas the growth of demand for new products the spread of information in a community and the spread of diseases including biological pathogens and com puter viruses chapter growth epidemics innovation diffusion and the growth of new products a number of analytically tractable models of s shaped growth were intro duced including the logistic growth model the sir epidemic model and the bass diffusion model important extensions to the basic epidemic and innovation diffu sion models were developed to illustrate how modelers can identify the restrictive assumptions of a model both explicit and implicit and reformulate the model to be more realistic the logistic epidemic and innovation diffusion models can be fit to historical data and the fit is often excellent however though nonlinear growth models such as the logistic and bass models are widely used and often fit certain data sets quite well you should not use these or any models as black boxes for forecasting to create realistic and useful models of product diffusion and innovation adop tion you must explicitly portray the feedback structure of adoption and growth in cluding the sources of attractiveness for the new idea or product the competition technical innovation changing criteria of use and other factors that influence adoption and growth many rich and insightful system dynamics models of inno vation diffusion have been developed and are used successfully to anticipate growth and design policies for success see homer for a model of emerging medical technologies and urban and roberts for feed back models for prelaunch forecasting of new automobile models the historical fit of a model does not show that the model is valid many models each with different assumptions about the feedback structure and each generating different dynamics can fit any set of data equally well ground your models in careful investigation of the physical and institutional structures and de cision making processes of the actors in the system and don t force fit data into the assumptions of any preselected functional form or model models should not be used as exercises in curve fitting using the aggregate data only models that cap ture the causal structure of the system will respond accurately as conditions change and policies are implemented path dependence and positive feedback unto every one that hath shall be given and he shall have abundance but from him that hath not shall be taken away even that which he hath matthew this chapter explores path dependence a pattern of behavior in which small ran dom events early in the history of a system determine the ultimate end state even when all end states are equally likely at the beginning path dependence arises in systems whose dynamics are dominated by positive feedback processes the chap ter explores the circumstances in which positive feedback can create path depen dence the role of random events early in the history of a path dependent system and the ways in which a path dependent system can lock in to a particular equilib rium feedback theories of path dependence and lock in are developed for a num ber of important examples in business technology and economics path dependence why do clocks go clockwise why do people in most nations drive on the right why is the diamond business in new york concentrated into the area around west street why do nearly all typists the inefficient qwertykeyboard lay out how did microsoft windows and intel processors come to dominate the market for personal computers why are there so many winner take all part the dynamics of growth situations where success accrues to the successful where the rich get richer and the poor get poorer and what do these questions have to do with each other all are examples of systems exhibiting path dependence path dependence is a pattern of behavior in which the ultimate equilibrium depends on the initial conditions and random shocks as the system evolves in a path dependent system small unpre dictable events early in the history of the system can decisively determine its ulti mate fate the eventual end state of a path dependent system depends on the starting point and on small unpredictable perturbations early in its history even when all paths are initially equally attractive the symmetry is broken by microscopic noise and external perturbations positive feedback processes then amplify these initial differences until they reach macroscopic significance once a dominant de sign or standard has emerged the costs of switching become prohibitive so the equilibrium is self enforcing the system has locked in the exact gauge of a railroad is of little consequence within broad limits at the start of the rail age no one gauge was a better choice than any other yet the standard gauge used in the us and most of the world is meters feet inches how did this convergence arise early railroads each unconnected to the others utilized a wide range of different gauges one early line used a foot gauge rolling stock was specific to each network and could not be used on lines with a different gauge but as rail networks grew compatibility became more and more important when gauges differed goods transshipped from one line to an other had to be unloaded from one train and reloaded on another greatly raising costs and slowing delivery railroads offering compatibility enjoyed a huge cost advantage since the same rolling stock could use any part of the network gradu ally the smaller railroads adopted the gauge used by the largest networks the at tractiveness of that gauge was then increased still further forming a positive feedback smaller railroads using incompatible gauges lost business or converted their road and rolling stock to be compatible soon a single gauge with the un likely dimensions of meters emerged as the dominant standard by the the costs of switching to another gauge abraham lincoln is reported to have argued for feet were prohibitive the system had locked in to the standard similar positive feedbacks are responsible for other examples of path depen dence the more typewriters with the qwerty keyboard were sold the more people learned to type with that layout and the more successful qwerty ma chines became while makers of alternative keyboards lost business as the market share of wintel computers grew more software was written for that platform and less developed for other platforms and operating systems the more software available for a particular operating system the greater the demand for computers compatible with that system increasing wintel market share still further some claim that the standard gauge emerged because it was the width of jigs designed origi nally for wagons which in turn had those dimensions to fit the ruts on the roads which in turn were determined by the ruts in roman roads which were set by the width of roman chariots and wag ons which in turn were sized to accommodate the width of two roman horses if true it illustrates the way in which positive feedback can a standard to persist long after the initial rationale for its selection has vanished chapter path dependence and positive feedback what causes some systems to exhibit path dependence but not others path de pendence arises in systems dominated by positive feedback figure l illustrates the difference between a system dominated by negative feedback and a pendent system dominated by positive feedback first imagine a smooth sided bowl the lowest point of the bowl is an equilibrium a marble placed there will remain there the equilibrium is stable pushing the marble off the equilibrium cre ates a force opposing the displacement a marble dropped anywhere in the bowl will eventually come to rest at the bottom though it may roll around a while first the equilibrium is not path dependent the marble comes to rest at the same spot no matter where it is dropped and no matter its initial velocity as long as it stays within the bowl the equilibrium is only locally stable a stable equilibrium is also called an attractor because all points are attracted to it technically because the equilibrium is locally and not globally stable it is an attractor only for points within its basin attraction the basin of attraction is the bowl inside it all points lead to the attractor at the bottom outside the bowl the dynamics are dif ferent the dead sea and great salt lake are examples rain falling anywhere over these watersheds ends up in their salty brine rain falling over other water sheds flows to the sea path dependence arises in systems with locally unstable equilibria a locally stable equilibrium the system is governed by negative feedback the greater the of the ball from the equilibrium p the greater the force pushing it back toward the center equilibrium a ball placed anywhere in the bowl eventually comes to rest at the bottom perturbations don t affect the equilibrium reached right a locally equilibrium the system is governed by positive feedback the greater the displacement of the ball the steeper the hill and the greater the force pulling it away from the equilibrium at p the slightest disturbance causes the ball to fall off the peak the initial perturbation determines the path taken by the ball and perhaps the ultimate destination the system is path position of ball p equilibrium m position force on discrepancy p p force on discrepancy ball p p part the dynamics of growth now turn the bowl upside down the top of the bowl is still an if the marble is balanced exactly at the top it will remain there however the equi librium is now unstable the slightest perturbation will cause the marble to move slightly downhill as the slope increases the downward force on the marble in creases and it moves still further downhill in a positive feedback an unstable equilibrium is also termed a because nearby trajectories are forced away from it the system is path dependent because the direction taken by the ball de pends on the initial perturbation a small nudge to the left and the marble rolls to the left an equally small shock to the right and it moves farther right of course though the equilibrium at the top of the bowl is locally unstable the system as a whole must be globally stable the marble eventually comes to rest but the dependent nature of the ball motion near the equilibrium point means it can come to rest anywhere on the floor and the particular spot it reaches depends on that small initial disturbance imagine rain falling near the continental divide of north america two rain drops fall a few inches apart one just to the east of the divide and one just to the west the difference in landing spot might be due to small unobservable differ ences in the wind as the two drops fall from the clouds though they begin only inches apart one ends up in the pacific the other thousands of miles away in the gulf of mexico microscopic differences in initial conditions lead to macroscopic differences in outcomes the inverted bowl illustrates another important feature of path dependent sys tems in when the ball is balanced at the top of the bowl all equilibrium po sitions are equally likely you can influence where the marble comes to rest with the slightest effort blow gently to the left and the marble ends up on one side of the room blow the other way and the marble rolls to the other side once the ball has started to move down the slope a bit however it takes a great deal more energy to push it back to the top and over the other way the farther the ball has moved and the faster it is going the harder it is to alter its course at the dawn of the automobile age it didn t matter which side of the road peo ple drove on but as traffic density increased the importance of a consistent stan dard grew the more people drove on one side the more likely it was new drivers in adjacent regions would drive on the same side increasing the attractiveness of that side still further in a positive loop most nations rapidly converged to one of the two standards with great britain and her colonies along with japan and a few other nations electing left hand drive while most of the rest of the world con verged to right hand drive initially the swedes elected to drive on the left as in great britain as traffic and trade with the rest of europe grew and as the swedish auto industry sought to increase sales in the larger right hand drive market it be came increasingly inconvenient and costly for the swedish system to be at odds with the prevailing standard in europe and north america seeing that the swedish road and auto system was rapidly in the swedes engineered a remarkable change at am on september the entire nation began to drive on the right sweden ability to effect the switch smoothly was partly due to massive prior education and a huge public works effort to change road but the suc cess of the switch also depended on the small size and low density of the popula tion both human and automobile in the total population of sweden was less chapter path dependence and positive feedback than million and there were only about million cars or people and cars per square mile most of the growth in sweden auto population and highway net work lay ahead the disruption and costs of the switch were small compared to the benefits imagine what it would cost today to switch from to right hand drive in japan though japan is only about as large as sweden in the mid it was home to about million people and million cars more than people and cars per square mile the disruption and cost would far outweigh any benefits japan has long since locked in to left hand drive path dependent systems are more common than many of us imagine the choice of standards such as the shape of electrical plugs the location of the prime meridian and the length of the standard meter in paris are all arbitrary but once a given choice becomes accepted the system locks in to that choice even though other alternatives were just as attractive early on path dependence and lock in are not restricted to economic technical or human systems complex organic mole cules such as amino acids and the sugars in dna can exist in two different forms identical except each is the mirror image of the other these enantiomers are known as the l levo or left handed and d dextro or right handed forms the chirality handedness of the enantiomers does not matter in isolation the physi cal properties of l and d molecules are the same yet the proteins in essentially all life on earth have levo chirality positive feedback and lock in are responsible just as you cannot put a right handed glove on your left hand the different three di mensional structures of the two types mean the d amino acids are physically in compatible with proteins built of the left handed form most chemical reactions tend to produce different enantiomers in equal proportions leading many scientists to conjecture that both left and right amino and nucleic acids were equally com mon in the primordial soup of the early oceans by chance the proteins that be came the basis for life on earth were formed from left handed amino acids as new organisms evolved from their left handed ancestors the web of left handed life grew in magnitude and complexity while any right handed forms became extinct life on earth has remained locked in to the left handed forms ever and dna are also chiral one form twists left one right but only the right handed forms are stereoscopically compatible with the l amino acids so essentially all natural terrestrial nucleic acids have the same chirality some physicists conjecture that the initial push favoring the left handed amino acids derived from parity violations of the weak nuclear force in which certain radioactive decay reactions favor one chiral form however a mechanism for prefer ential selection of the l form by the weak force or other physical processes such as polarized light has not yet been demonstrated part the dynamics of growth model of path dependence the polya process you can easily construct a simple and compelling example of path dependence imagine a jar filled with small stones there are black stones and white stones stones are added to the jar one at a time the color of the stone added each period is determined by chance the probability of selecting a black stone is equal to the proportion of black stones already in the jar it is this last assumption that gives the system its unique character and creates path dependence suppose the jar initially contains one black and one white stone the probability the next stone you choose will be black is then suppose it turns out to be black now there are two black and one white stones in the jar the probability of black on the next draw is now suppose it is black now of the stones are black the preponder ance of black stones means it is more likely than not that still more black stones will be added and the jar is likely to end up with more black than white stones but suppose on the first draw a white stone had been chosen the likelihood of draw ing a black stone on the second round would then have been instead of the jar is then likely to end up with more white than black stones the trajectory of the system and the ultimate mix of stones in the jar depends on its history on the par ticular sequence of random events figure shows a causal diagram of this sys tem known as a polya process after its inventor the mathematician george polya the polya system contains two feedback loops one positive and one negative for each type of the greater the number of black stones the greater the chance of adding another black stone a positive loop at the same time the greater the number of black stones the greater the total number of stones and so the smaller the impact of any new black stone added to the jar on the proportion of black stones a negative loop figure shows simulations of the polya process each is periods long at first each stone added to the jar has a large influence on the probability of choosing the next stone the first stone added determines whether the probability of choosing a black stone is or the positive loop dominates but as the number of stones grows each new stone has a smaller and smaller effect on the proportions the positive loop weakens relative to the negative loop eventually the number of stones is so large that the next stone added has a negligible effect on the proportion of each color in the jar the positive and negative loops are exactly balanced at that point the proportion of each color will then stabilize since on average stones will be added in the future in the same proportion as those already in the jar the ratio of black to white stones eventually reaches equilibrium but that ratio depends on the history of the colors selected small random events early in the process can easily be generalized to any number of colors the probability the next stone added is any color then equals the proportion of that color in the jar only one color is added per period chapter path dependence and positive feedback figure the polya process every period one stone is added to the total the probability of choosing a stone of a given color equals the proportion of that color in the total population the rule adding stones of a given color is black added per period stones added per period if random draw proportion of black stones otherwise if random draw proportion of white stones where random draw is a number drawn at random from a uniform distribution on the interval history of the system tip it toward one path rather than another the equilibrium is path dependent the accumulation of stones eventually locks the system in to equilibrium at a particular proportion of each color to reverse the proportion of black stones from when there are three stonesin the jarrequires drawing three white stones in a row an event with a probability of white black but to move from a ratio of to when there are black and white stones requires drawing white stones in a row an event with a vanishingly small probability x to be precise part the dynamics of growth figure ten realizations of the polya process e e time periods figure equilibrium distribution of the polya process histogram shows the proportion of black stones by decile after periods in simulations the distribution is quite uniform all proportions are equally likely in equilibrium proportion of black stones after periods the more stones the less likely there will be any movement away from the current proportion the system locks in to whatever balance emerges from its early history polya proved that the process will always converge to a fixed proportion of black stones and that the particular proportion depends on the history of the random events along the way polya also proved the remarkable result that the dis tribution of final proportions is uniform that is the final fraction of black stones is equally likely to be anywhere between and figure shows the distribution of the proportion of black stones after periods in a set of simulations the distribution is nearly uniform all proportions of black stones are equally likely in the long run see arthur for further examples distribution is uniform only for the special case where the number of stones added per period is one and the jar initially contains one white and one black stone other initial conditions or rules for selecting the number and type of the stones added lead to different equilibrium distri butions johnson and kotz provide a comprehensive treatment of urn models of this type chapter path dependence and positive feedback phase plot for the linear polya process the line shows the probability a black stone as a of the proportion of black stones every point on the line is an riurn every equi librium point neutral iity proportion of black stones generalizing the model nonlinear polya processes the polya process illustrates how path dependence comes about but it is a very special model with a number of restrictive and unrealistic assumptions first the dynamics depend on the fact that the flows are quantized though stones are added with probabilities in proportion to their prevalence in the jar each stone is either all black or all white each period the proportion of each color must change instead of a jar of stones imagine filling the jar with black and white paint mixed in pro portion to the current shade of gray in the jar the shade of gray would never change no matter what it was initially you can approximate the continuous time continuous flow situation in the model shown in figure allowing fractional stones to be added per period or reducing the time step between periods second the probability of adding a particular color is linear in the proportion of that color figure the function defining the probability of adding a ball of a given color lies exactly on the line so every point on the line is an equilibrium in general however the decision rules determining the flows in path dependent sys tems are nonlinear functions of the state variables if the probability of adding a stone of a given color is a nonlinear function of the proportion with that color the number location and stability of equilibria and the dynamics all change suppose the probability of choosing a black stone is characterized by the non linear function in figure the system now has only three equilibria points part the dynamics of growth figure nonlinear polya process the probability of choosing a black stone is now a nonlinear function of the proportion of black stones in the jar the system has three equilibria proportion of black stones where the proportion of black stones and the probability of adding a black stone are equal and the points zero and are equilibria if the jar is all black or all white it will remain so likewise when the proportion of black stones is one half the probability of choosing a black stone is one half so on average the proportion of stones remains constant when the proportion of black stones rises above half however the probability of choosing black increases more than proportionately and when it is less than one half it falls more than proportionately the equilibrium at is unstable if the first stone added is black the probabil ity of adding more black stones increases dramatically moving the system on av erage toward the stable equilibrium at black stones similarly if the first stone is white the probability of drawing more white stones increases sharply and the system will tend toward the stable equilibrium of all white stones of course since the system is stochastic sometimes a run of one color will move the state of the system back across the ratio figure shows of the nonlinear polya process shown in fig ure all trajectories move rapidly away from the initial ratio of and after periods the jar is nearly all one color or the other where the linear polya process has an infinite number of equilibria this nonlinear process has only three of these only two are stable yet the system is still strongly path dependent which of the two stable equilibria dominates depends entirely on the history of the ran dom events as the system evolves like the linear process the system locks in to whichever equilibrium the positive loop reinforces as determined by the chance chapter path dependence and positive feedback figure dynamics of the nonlinear polya the system tends toward all one color or all the other depending on early his tory of random events note the trajectory with a line where the early lead of white stones is reversed by run of black leading the system to lock in to the predominantly black iiibriu the realizations of the random in these simula tions are sarne as in figure time periods events early on note the trajectory in figure shown as a bold line where the jar is mostly white in the beginning but due to a run of black stones the ratio re verses after about periods the positive feedbacks then favor black stones and the jar soon locks in to the predominantly black equilibrium lock in is much swifter and stronger in the nonlinear system in the linear case the system has neutral stability every point is an equilibrium and none of the points is better than any other in the nonlinear example here the two stable equilibria are strong attractors the positive feedback continues to dominate the dynamics even as the proportion of a given color increases like the two rain drops falling on either side of the continental divide trajectories on either side of the point are attracted on average toward one of the stable equilibrium points the jar ends up nearly all one color winner take all dependence in the economy versus videocassette recorders vcrs are ubiquitous in homes businesses and you can buy or rent videos at nearly every mall and main street the film industry earns significant revenue from sales of video rights and many films are made directly for the home video market never enjoying theatrical release much of this success depends on the common format used by the vast majority of vcrs known as vhs which ensures machines made by different companies are compatible with one another and with the tapes available in the how did vhs become the standard vcr industry data and history presented here are based in part on data collected and a model developed by ed anderson personal communication i m grateful to ed for permis sion to use his data and materials vhs is now the standard for inch vcrs around the world different regions do use incompatible signal formats north america uses the ntsc format while europe uses the pal format part the dynamics of growth vhs was actually a latecomer to the home videorecorder market home video recording technology came of age in when sony introduced the betamax sys tem offering ordinary people the chance to record television broadcasts and play movies in the comfort of their own homes vcrs soon became the hot home elec tronics product of the late and early figure as is common in con sumer electronics larger production volumes learning effects and increasing competition led to huge price drops for vcrs even as their features and function ality increased demand soared by about of us households owned at least one vcr and sales had reached million units per year in the us alone vcr adoption in europe and the rest of the world followed similar dynamics when vcrs first became available a prime use was time shifting the recording of broadcasts to be played back at a more convenient time time shifting also made it possible to fast forward through commercials within a few years however the principal use of vcrs became the playing of prerecorded films music videos exercise tapes and so on sales of prerecorded tapes in the us exploded to more than million per year by and the video rental industry took off figure the data for the aggregate vcr market conceal the fight for dominance among different vcr formats sony proprietary betamax technology was the first cassette based home video technology to reach the market some months ahead of its principal rival the vhs standard launched by a consortium of sushita jvc and rca cusumano mylonadis and rosenbloom though betamax and vhs technologies cost about the same the tapes and machines were not compatible consumers had to choose which standard to adopt the attractive ness of each format depends on various factors including price picture quality play time and machine features such as programmability ease of use size and re mote control among others the most important determinant of product attractiveness is compatibility to swap tapes with their friends and families people had to have compatible ma chines as the installed base of machines of a given format increased the attrac tiveness of that format to potential new buyers increased which in turn increased the market share of that format and boosted the installed base even further even more importantly people tended to buy machines compatible with the broadest se lection of prerecorded tapes video rental shops chose to stock tapes in the most common format since these would rent more often and yield more profit movie studios in turn chose to offer their films in the format compatible with the most popular technology and the orders placed by the video stores these positive feedbacks mean that the format with the largest installed base of machines all else equal will be the most attractive to consumers and content providers unchecked by other loops or outside events these positive feedbacks confer greater and greater market share advantage to the leader until one format completely dominates the market and the other disappears as shown in figure this is precisely what happened by the late vhs had gained a mar ket share advantage over betamax soon the majority of prerecorded tapes were also coming out in the vhs format vhs market share and sales continued to grow while the betamax share steadily shrank by the triumph of vhs was chapter path dependence and positive feedback figure diffusion of vcrs in the us sales of vcrs and prerecorded tapes in the us m prerecorded tape sales a right scale fraction of us households with at least one vcr source anderson complete sony was forced to abandon technology for the home market and in announced that it was switching its product line to the vhs format the strong effect of compatibility on product attractiveness explains how vhs rapidly achieved dominance over betamax once it achieved a lead in market part the dynamics of growth figure betamax vs vhs formats in the home vcr market us sales of vcrs by format us sales of prerecorded tapes by format market share of vhs format vcrs and tapes source anderson share and in the share of the installed base but the existence of strong positive net work and compatibility effects does not explain how vhs first achieved that lead a close look at the data in figure shows that from its introduction through a period of years betamax was the market share leader as the first prod uct to market betamax should have been able to use the positive network and compatibility feedbacks along with learning curves scale economies and other chapter path dependence and positive feedback positive feedbacks favoring the early leader to gain a commanding advantage and prevent later entrants from succeeding what happened in the polya model of path dependence random events early in the history of a system can tip the system toward one outcome as seen in figure these ran dom events can sometimes reverse the ratio of colored stones in the jar perhaps beta failure was just bad luck perhaps chance led to a run of events favoring vhs destroying sony early lead such an explanation is unsatisfying unlike the random selection of stones in the polya process electronics makers did not flip coins to determine which type of vcr to make and customers did not spin a wheel of fortune to decide which format to buy the random shocks in a path dependent system stand for events outside the boundary of the model that is those events for which we have no causal theory a goal of modeling is to expand the boundary of our models so that more and more of the unexplained variation in the behavior of a system is resolved into the theory there are many theories to explain how betamax lost its early lead stein notes that vhs offered longer play and record time originally the vhs playtime was hours to hour for betamax by the ratio was hours for vhs to hours for betamax longer play time klopfenstein argues gave vhs the early edge in contrast arthur argues that betamax had a sharper picture than vhs and was actually the superior technology an early vhs price advantage is another theory but data supporting it are weak price data are hard to get but suggest that while vhs machines were about cheaper than betamax machines in they were actually more expensive than beta machines the following years price does not seem to be a decisive fac tor in explaining how vhs overtook betamax cusumano mylonadis and rosenbloom point to the different business strategies employed by sony and matsushita sony seeking to profit from their proprietary technology was reluctant to license betamax to other firms in con trast jvc and its parent matsushita aggressively sought partners among other manufacturers set lower licensing fees than sony and even delayed the introduc tion of vhs until they and their allies could agree on common technical standards matsushita also built sold under the label of other firms speeding produc tion ramp up matsushita thus gained access to the distribution channels of these firms and also gained larger production volume than if they had kept their tech nology proprietary consequently matsushita enjoyed greater scale economies in distribution and production and gained experience that moved it down the learning curve more rapidly the development of the prerecorded tape industry played a key role prior to the majority of prerecorded tapes were aimed at the adult entertainment sec tor similar to the early days of the worldwide web rca matsushita largest customer in the us sought to jump start the market for general audience videos and thus vcr sales by offering two free vhs tapes with each vcr it sold rca also encouraged firms such as magnetic video to invest in vhs equipment to sup ply prerecorded tapes for the us market large scale production of prerecorded betamax tapes lagged behind by about a year cusumano mylonadis and bloom note from figure that the vhs share of prerecorded tape pro duction actually exceeds share of the installed base until 1983 which further increased the attractiveness of vhs to video rental stores and customers part the of growth formulating a dynamic hypothesis for the vcr industry using the information above and any additional sources you wish develop a causal loop diagram to explain the dominance of vhs in the home vcr market your diagram should be simple but should capture the important feedbacks described above both positive and negative use your diagram to explain why the market converged to a single format and why vhs won the format battle how might sony have altered its strategy to prevent vhs from becoming dominant sony format lost to vhs in the inch home vcr market but remains the market leader in the market for professional quality inch equipment used by television and news organizations how do the feedback structure and strength of the various loops differ in the professional market compared to the home market what impact do these differences have on effective strategy since vhs became the dominant standard other tape formats and video technologies have been introduced for the home market especially inexpensive camcorders a variety of camcorder tape and cassette formats coexist including mm super or hi mm panasonic cassette technology and others none of these has become dominant how do the uses of camcorders and the determinants of camcorder attractiveness differ compared to the home vcr market how do these differences affect the strength of the feedback loops in your model what is the likely impact of these differences on the dynamics and on strategies for success in the camcorder market positive feedback the engine of corporate growth the network and complementary goods effects that dominated the evolution of the vcr market are but two of many positive feedbacks that can drive the growth of a business this section surveys some of the important positive feedbacks that can cause a firm to grow since path dependence arises when positive feedbacks dom inate a system the prevalence of positive loops in corporate growth means that the potential for path dependence in the evolution of corporations industries and the economy as a whole is great the diagrams below present the loops in a highly simplified format focusing on the sales of a single firm in an industry the diagrams do not explicitly show the competitors but all firms in a particular market are linked through competition for market share and through materials and labor markets financial markets distribu tion networks the media and the social fabric in general the diagrams also omit the many negative feedbacks that can halt the growth of the firm chapter path dependence and positive feedback product awareness how do potential customers become aware of a firm products there are four principal channels advertising direct sales effort word of mouth and media attention each of these channels creates positive feedbacks figures and in most firms the advertising budget supporting ads trade shows and the like grows roughly as the company and revenue grow larger advertising budgets have two effects more potential customers are made aware of the product and choose to enter the market loop rl to the extent the advertising is effective more of those who are aware and in the market are likely to buy the product of fered by the company similarly the larger the revenue of the firm the greater the sales budget the more sales representatives and the greater their and ex perience the more calls they can make the more time they can spend with cus tomers and the more effective their calls will be increasing both total industry demand and the share of the total demand won by the firm while a firm controls its advertising and sales budgets word of mouth and me dia attention are largely outside the firm direct control figure as sales boost the installed base and the number of customers who have experience with the product favorable word of mouth increases awareness increasing total demand and also persuading more people to purchase the products of the firm a hot product or company will also attract media attention which if favorable stimulates additional awareness and boosts market share still more there are many processes by which a firm or product can become hot popular and attract unsolicited media attention strongly favorable word of mouth can stimu late media coverage especially for highly innovative new products and products figure advertising and direct sales effort drive awareness of the product part the dynamics of growth figure how word of mouth and media reports create a hot product sales industry awareness share installed base sales or firm growth rate word of mouth hot product effect share from media reports perception of hot product that show well on television rapid growth of sales revenue profit or stock price can also attract media attention and turn the product into a social phenomenon amazon com provides a prominent example from the late product shortages and the price gouging profiteering and near riots they can create also attract media attention shortages are especially important in creating the impression a product is hot for consumer goods such as toys recent examples include beanie babies and furbies footage of frenzied shoppers trampling each other to get the last tickle me elmo the week before christmas can multiply the crowds at the mall exponentially the strength of these loops depends of course on the attractiveness of the prod uct an excellent product offered at a good price will be easier to sell and will generate more favorable word of mouth than an overpriced poor quality product which of these channels of awareness dominates depends on the particular prod uct and market and not all firms utilize all three channels fast food chains do not have a direct sales force and rely instead on advertising and word of mouth specialty toolmakers focus their resources on direct sales effort and advertise much less the time delays and stock and flow structure of these four channels of aware ness also differ the delays in starting and stopping advertising campaigns are short relative to building a competent and skilled sales force word of mouth is weak when a new product is first launched but can grow rapidly and dominate the chapter path dependence and positive feedback figure spreading fixed costs over a larger volume lowers price and leads to larger volumes expected fixed costs of market size development and production unit fixed demand costs demandfrom fsikperdeacdoinsgts share from spreading fixed costs unit costs product unit variable attractivene price costs sources of information availableto the market as the installed base grows the me dia spotlight tends to burn very bright for a short while then fades as the attention of editors and audience moves on to the next object of desire further while the strength of the advertising and direct sales loops are under the direct control of the firm the word of mouth and media loops are not word of mouth favorable and unfavorable is difficult for a firm to control though firms can stimulate communication among current and potential customers by sponsor ing users groups and conferences and by hiring opinion leaders as spokespeople similarly though many firms today are highly in media relations there is no sure fire way to get your product featured on a network magazine show or a list of hot web sites unit development costs many products and services involve considerable up front development and ca pacity acquisition costs the greater the expected lifetime sales of the product the lower the fixed price per unit and the lower price can be while still achieving the required return on investment figure lower prices stimulate industry de mand and lead to a greater share of that total l boosting sales and cut ting fixed costs per unit still more the larger the up front costs of product development and production capacity the stronger these loops will be in a labor and materials intensive industry such as subsistenceagriculture fixed costs are small in technology and tensive industries involving significant product development effort nearly all the costs are incurred prior to production of the first unit developing a new automo bile or commercial aircraft costs several billion dollars all the design and devel opment costs and all the costs for capacity tooling training and marketing must be borne before job one rolls off the line there a saying in the semiconductor part the dynamics of growth industry that it costs a few billion to make the first chip but then all the rest are free software development is the paradigm case as the internet expands the mar ginal cost of distribution is rapidly approaching zero while up front development costs are rising the industries powering the world economy are increasingly knowledge based and up front costs capture a growing share of the total costs of production in a world dominated by these positive loops traditional rules of thumb for pricing no longer apply note how fixed cost per unit depends on expected volume when these loops dominate the dynamics expectations about how large lifetime volume will be can be strongly self fulfilling imagine two firms with identical costs launching identical products at the same time one expects to win about half the market and estimates market potential conservatively believing it can lower prices as volume expands the other expects to win the dominant share of the market and believes lower prices will greatly expand total demand for the category the ag gressive firm therefore sets prices much lower than the conservative firm and might even initially sell at a loss the aggressive firm wins the largest share of the market which allows it to lower prices still further while the conservative firm finds sales of its product are disappointing the expectations of both firms are ful filled and their mental models are reinforced the aggressive firm learns that pric ing low even below current unit costs can lead to market dominance and huge profits while managers at the conservative firm learn to be even more cautious about projecting sales a classic example of a self fulfilling prophecy merton though not shown in the diagram expectations of lifetime volume can depend not only on current sales but also on forecasts of potential industry demand mar ket research and knowledge about the development of complementary goods many of these other possible inputs to a firm belief about market potential also close positive loops software sales forecasts rise as technical progress in computer hardware leads to faster and cheaper computers lower software prices in turn stimulate the demand for hardware that helps make that belief a reality price and production cost spreading up front development costs over a larger volume is not the only way to lower unit costs figure shows the positive loops created by economies of scale in production economies of scope learning curves and process innovation economies of scale differ from the development cost loops discussed above in many industries unit costs fall as the scale of production rises at least up to a point larger paper mills oil refineries and thermal power plants are often more efficient than smaller ones there are both thermodynamic and organizational rea sons larger boilers in a coal fired power plant have a larger ratio of volume to surface area and therefore higher thermal efficiency in addition every paper mill oil refinery and power plant requires instrumentation safety systems logis tics capacity to handle incoming and outgoing materials and other facilities sim ilarly every business must have a certain minimal amount of administrative staff and overhead the cost of these activities usually does not rise as quickly as pro duction volume so firms can lower prices as they grow which creates opportuni ties to increase the scale of operations further the opportunity to realize chapter path dependence and positive feedback figure scale and scope economies learning curves and process improvement each effect creates two positive loops one increases sales through market share gains and one increases sales through expansion of the total size of the market such economies of scale by consolidating general administrative and overhead functions is a powerful driver of mergers and acquisitions see section an other powerful source of scale economies arises from division of labor larger or ganizations can afford to divide the work into increasingly specialized tasks it has long been observed at least since adam smith famous discussion of a pin fac tory in the nations that division of labor boosts individual productivity and leads to lower unit costs see section economies of scope arise when a firm is able to share capacity labor technical know how and other resources across multiple product lines and busi ness units cable television companies can offer high speed internet access using the same cable network with low incremental capital costs shopping malls and so called category in office supplies toys hardware and other retail markets reduced unit costs dramatically by offering a huge range of products under one very large roof these big box retailers also reduced search costs for their customers by providing one stop shopping just off the freeway which boosted product attractiveness and market share at the expense of the smaller stores on main learning curves also create positive loops favoring the leading firm learning or experience curves have been documented in a wide range of industries from commercial aircraft to broiler chickens teplitz the learning curve arises as workers and firms learn from experience as experience grows workers find ways to work faster and reduce errors typically the unit costs of production fall by a costs of malls in terms of traffic congestion decay of the central business district and so on are all externalized lowering their apparent costs below their true costs to the community part the dynamics of growth fixed percentage every time cumulative production experience for ex ample costs might fall with each doubling of cumulative output learning curves with improvement per doubling of experience are typical in many industries lower unit costs enable lower prices increasing both market share and industry demand and boosting sales still more finally the larger the firm the greater its investment in research and develop ment leading to process innovations that lower costs such research can in clude the development of more highly automated tooling more reliable machines and more efficient plant layout it can also include training in process improvement techniques such as total quality management which enhance the ability of workers to detect and correct the sources of defects thus boosting productivity and lower ing costs the delays and stock and flow structure of scale and scope economies differ from learning curves and process improvement scale and scope economies de pend on the current volume of sales and breadth of the firm activities an acqui sition for example can quickly boost scale and scope similarly if the firm shrinks its scale and scope economies are quickly lost and the positive feedbacks reverse speeding the decline in the firm attractiveness learning by doing and the results of process improvement are embedded in the organization capital stock worker knowledge and routines they are slower to develop and if sales turn down cumulative experience and process productivity tend to persist decay ing much more slowly though to be sure know how and experience are often lost depending on how the firm downsizes network effects and complementary goods as illustrated by the vcr industry the utility of a product often depends on how many others are also using it the network effect in figure and on the availability of compatible products to use with it the complementary good effect compatibility and network effects boost product attractiveness and thus ex pand the total size of the market as the growth of the internet made computer ownership more attractive leading more people to use the internet these loops tend to favor the market share leader within an industry assuming competing prod ucts are incompatible besides the classic example of these loops in the and was the battle for control of personal computer operating systems particularly the eclipse of the technically superior macintosh architecture by the wintel platform just as in the case of fixed costs section the decision by third parties to produce complementary goods for a particular product depends on their expec tation of the market potential and hence the expected profitability of that platform firms can shape those expectations in a variety of ways including early sharing of technical specifications with potential third party developers and subsidies for adoption of the platform other strategies include consortia andjoint ventures with the learning curve is formulated as depending on cumulative investment rather than cumulative production as in arrow chapter path dependence and positive feedback figure network and compatibility effects each effect creates two positive loops one increases sales through market share gains and one increases sales through expansion of the total size of the market industry demand market share product attractiveness attractiveness from network size installed base expected market size complementary attractiveness of attractiveness from goods market to third availability of parties complementary products complementary goods third parties the formationby matsushita of the vhs consortium horizontal and vertical expansion into the markets for complementary products sony purchase of film studios to control content and media for their hardware products or purchase of lotus and free distribution of complementary goods netscape decision to give away its web browser to stimulate sales of its server software a strategy soon imitated by microsoft in the software industry some firms strategicallytime the announcement of new products to preempt their rivals and influence third party developers sometimes even announcing the near avail ability of vaporware products that don t yet exist even in prototype form when the network and complementary goods loops are strong expectations about which platform will ultimately triumph can be strongly self fulfilling product differentiation another set of positive feedbacks arises from the ability of firms to invest in prod uct differentiation figure as firms grow they can invest more in activities that improve the attractiveness of their products to customers most products can be differentiated from those of competitors through enhanced features function ality design quality reliability and suitability to the current and latent needs of part the of growth figure product differentiation price premium us market sales revenue i demand share price premium for superior technology differentiation total demand effect product product differentiation share effect product attractiveness product features functionality suitability to customer needs quality and reliability service and support other attributes customers firms can also invest in superior service and customer support infra structure to the extent these investments increase the attractiveness of the prod ucts in the eyes of customers the firm can gain market share boosting revenue and enabling still more investment in differentiation more capable and useful products also increase total demand finally companies offering clearly su perior products can often charge a price premium without off growth the higher margins enabled by such a price premium enable the firm to increase its in vestment in differentiation still further many high tech firms are engaged in a technology race in which competition is primarily focused on the earliest introduction of the fastest most powerful prod uct with the most features but differentiation does not have to focus on technol ogy and product features ibm successfully pursued the differentiation strategy for decades and dominated the computer industry from its inception through the per sonal computer revolution in the differentiation investments how ever focused on product reliability and especially on customer service and support tom watson jr like his father understood that the most important deter minant of product attractiveness for their core market middle managers in large corporations was peace of mind especially when computers and data processing were novel in the and these organizations were reluctant to invest in chapter path dependence and positive feedback figure product development creates new demand boosting development resources price sales revenue industry price premium for demand unique new products new uses new needs new products product development pabiiity and effort computing unless they were sure it would be highly reliable and that when some thing went wrong they could get it fixed quickly ibm focused its differentiation strategy on product quality and reliability building the largest and most responsive sales and service organization in the busi ness its success not only enabled it to gain market share and increase the size of the data processing market but also to charge the highest prices in the industry other firms entered the mainframe business some created by former ibm people amdahl but could never gain much market share even though they offered lower prices for machines of comparableperformance ibm maintained its domi nance of the mainframe industry by continuously investing huge sums in further development and articulation of its service and support infrastructure even while generating consistently strong profit growth for its shareholders of course while differentiation strategy was spectacularly successful for decades all posi tive loops eventually encounter limits and the company stumbled badly in the when it failed to anticipate the fundamental changes in the computer indus try caused by the microprocessor and personal computer revolution new product development the development of entirely new products is a core engine of growth for many firms figure the greater the revenue of a firm the larger and more effec tive the new product developmenteffort can be new products create new demand boosting revenue and increasing investment in new product development still more and just as differentiationenables firms to charge higher prices firms that bring novel and importantproductsto market can often command a price premium until imitators arrive higher prices further increase the resources available to fund the development of still more new products so the firm can stay ahead of competitors part the dynamics of growth intel has successfully used these new product development loops to fend off competition from clone makers such as amd and cyrix who develop chips com patible with but cheaper than intel intel invests about of its revenue in more than billion in which enables it to offer the fastest and best compatible chip at any time and leads to more sales growth and more power hungry computer users are willing to pay a substantial price premium for the lat est fastest most powerful chip the firms profiting most from the new uses new needs and price premium loops are those best able to identify the latent needs of potential customers they understand what people don t yet know they want or create a need people did not have before and then bring products addressing those needs to market quickly effectively and at low cost the capability to do so is not simply a matter of the budget but depends on the size of the installed base of users and the firm ability to collect and act on their suggestions it is a competence built up over time through experience and through investment in product development process im provement the strength of these loops also depends on the ability to protect innovative new products from imitation by competitors patents offer an obvious method to protect such innovations and are critical to the success of the new product devel opment loops in the pharmaceutical industry among others more important how ever is the ability to weaken competitors ability to use the same loops intel not only charges a price premium for its latest fastest chip but also uses the margins from these top of the line chips to lower prices on older processors as or even be fore the clone makers bring their chips to market by cutting prices for older chips intel limits the margins of the clone makers weakening their ability to use the positive differentiation loops to erode intel lead market power the larger a firm the more clout it has with its suppliers workers and customers such old fashioned monopoly power enables firms to lower their unit costs and prices leading to larger market share and sales and still more bargaining power in figure the benefits of monopoly power do not show up only in the firm unit costs suppliers will give or be forced to give preferential treatment to their large cus tomers on delivery terms and payment schedules to share technical knowledge to respond to customer change requests and to make other accommodations that give the firm an advantage over its smaller rivals who get the short end of the stick in terms of supplier attention and resources for the smaller firms the positive loops act as vicious cycles large firms can often receive preferential treatment from their distribution channels and customers as for example when the large consumer products firms demand the best shelf space in retail outlets similarly the larger a firm share of total jobs in a community the fewer op portunities for alternative employment there are so job turnover may fall reduc ing training costs firms whose workers have no alternative sources of employment not only can pay lower wages and benefits but can also save money by scrimping on investments in worker health and safety sweatshops are all too chapter path dependence and positive feedback figure monopoly power over customers suppliers and workers is self reinforcing each effect creates two loops one increases market share and one increases total demand demand market market market market powerover powerover suppliers customers power over power over power over suppliers product attractiveness price unit costs common in many industries including the apparel industry and not only in in donesian sneaker plants the company town of the century was the ultimate expression of this process where for example a steel or mining company not only paid low wages but also owned all the housing and stores and charged exorbitant rents and prices workers often fell deeply in debt trapping them in what amounted to de facto slavery these dominant firms also used the extra profit gen erated from their power over workers to hire pinkertons and other private security forces to put down efforts to organize or strike a strategy andrew carnegie em ployed effectively in his pittsburgh steel mills such practices still exist today in the sugar cane industry for example large firms also have the resources to import workers from other regions to ensure the balance of supply and demand in the labor market continues to favor the employer even as the firm grows in the century the robber barons brought chinese laborers to the american west to keep wages low while they built the railroads today large agribusinesses import work ers to harvest crops at low wages mergers and acquisitions growth can be powered by acquisitions of rival firms horizontal expansion and of suppliers and customers vertical integration the larger a firm the more capi tal it can raise to finance mergers and acquisitions if acquisitions consolidate the firm dominant position profits may rise through the exercise of monopoly power over labor suppliers and customers enabling the firm to buy still more of its rivals in figure if vertical integration enables the firm to lower its costs it can gain further market share and stimulate industry demand and grow still more acquisitions can also enhance economies of scale and scope or permit firms to guarantee a steady flow of complementary products see sections and a process important in the convergence of the film television entertain ment and news industries disney purchase of capital in the mid part the dynamics of growth figure self reinforcing growth through acquisition each effect creates two loops one increases market share and one increases total demand market share vertical integration finan resour a of suppliers and customers monopoly power acquisition of rivals attractiveness economies of scale market power over suppliers workers price unit customers costs provided abc with access to content for its entertainment programming while at the same time giving disney access to news and magazine shows to market their products along with a network of television stations to broadcast their films videos and promotional specials of course the synergy often touted as the rationale for mergers can be elusive many acquisitions fail to lower unit costs stimulate economies of scope or build monopoly power negative loops arising from incompatible corporate cultures overcentralization or loss of focus can dilute the earnings of the combined firm ul timately leading to the divestiture of the disparate business units the consolidation of market dominance through the acquisition of weaker ri vals has long been a common strategy most famously used in the late and early centuries by the great trusts such as us steel consolidated tobacco amalgamated copper american smelting and refining northern securities and of course standard oil in according to the census bureau of all goods in the us were made by just of the industrial firms many of these controlled more than half the total market in their industries the pace of merger acquisition and consolidation in the late century has been surpassed only in the and the rise of the trusts in the late century led to a backlash in the form of the sherman antitrust act and trustbusters like teddy roosevelt see it remains to be seen whether these same negative feedbacks will arise once again in response to the growing consolidation of market power in the global econ omy today workforce quality and loyalty the ability of a firm to offer superior products and service depends on the com mitment skill experience and quality of its employees the more profitable a firm the higher the wages and benefits it can pay to recruit and retain the best and chapter path dependence and positive feedback figure profitable growth leads to recruitment and retention of the best people each effect creates two loops one increases market share and one increases total demand industry market wage premium profit growth product attractiveness benefits perceived career quality of opportunities workforce the brightest in figure and the faster a company grows the greater the career opportunities and job security for employees the strength of the wage premium loop has increased greatly in recent years as firms especially firms with bright growth prospects have increasingly turned to stock options as a form of compensation employees whose compensation is tied to profit sharing or the company stock often work harder and longer than those on straight salary stock options bonuses and profit sharing allow firms to recruit highly qualified people while reducing base salaries freeing additional resources that can be invested in strengthening other feedbacks driving growth such as new product development differentiation or acquisitions as growth accelerates and the stock price soars the company can pay people even less up front the positive feedbacks in figure are highly nonlinear it takes many years to build up a loyal high quality workforce but a firm can destroy that capability very quickly when growth stalls or the firm downsizes opportuni ties for advancement and promotion quickly disappear the best and most capable are the first to leave as they have the brightest prospects and best outside opportu nities the loss of these above average employees further erodes the firm capa bility to deliver attractive products or services leading to still more downsizing and attrition in a vicious cycle firms that rely heavily on stock options are espe cially vulnerable to a slowdown if the growth prospects of the company dim and the multiple falls people options may become worthless leading to demands for higher cash compensation that steal resources needed to promote growth just when they are needed most these positive loops can speed the implo sion of a declining organization ibm again provides an example for decades success enabled it to offer excellent salaries and benefits de facto lifetime employment and excellent oppor tunities for promotion consequently the firm was able to recruit the cream of the part the dynamics of growth crop and its employees were renowned for their loyalty and commitment qualities that greatly strengthened ability to provide the service and support its cus tomers required when the rise of the personal computer eviscerated the main frame industry and growth stalled opportunities for promotion dried up hiring plummeted the company was much less successful in attracting top candidates for the few new jobs they did offer in an attempt to preserve the decades old off practice there were several early retirement programs during which many top people left for greener pastures their departures further eroded the capabilities of the organization even these generous programs proved inadequate and soon lay offs and massive reorganizations began morale sank further and productivity suf fered as employees and managers worked to protect their job or find a new one leaving less time and energy for the the loss of loyalty experience and deepened and prolonged the crisis the cost of capital profitable growth leads to higher expectations of future earnings and a higher mar ket value for the firm the higher the market value and stock price the lower the cost of raising new capital through the equity market figure similarly though not shown in the figure the greater a firm profits and cash flow and the higher market value relative to book value the lower the risk of default so the lower the cost of debt as the premium over the prime interest rate falls the lower the cost of capital the lower the firm costs of development and production lower costs increase profits and cash flow still further leading to even higher mar ket value and a still lower cost of capital as lower unit costs permit lower prices while maintaining healthy profit margins market share and industry demand rise leading to even greater market value and further cutting the cost of capital a lower cost of capital also allows the firm to increase its investment in ca pacity and new product development service and support infrastructure hu man resources acquisitions and other resources that strengthen the positive feedbacks driving growth finally as the capital markets respond to the greater growth rate of the firm by raising expectations of future earnings the mar ket value will rise even higher further lowering the cost of capital these loops are often quite powerful for rapidly growing high tech firms ini tial public offerings of internet companies in the mid provide an example many of these firms were able to raise substantial capital at relatively low cost by selling only a small fraction of their equity relative to the risk are many examples of firms experiencing this death spiral sastry develops a sys tem dynamics model addressing these issues and shows how another death spiral can be created by too frequent reorganizations in response to poor business performance describes a feedback model of organizations in which positive loops can lead to downsizing and decline case studies of these dynamics include glucksman and mass who show how positive feedbacks led to different fates for two initially similar uk insurance companies risch and sterman who show how these positive loops defeated a new strategy for a maker of specialty paper and sterman repenning and kofman simulation model of a high tech company where path dependence and positive feedback led to unanticipated side effects in the firm quality improvement program chapter path dependence and positive feedback figure profitable growth lowers the cost of capital stimulating further growth effect creates two loops one increases market share and one increases total demand comparable loops for debt financing are not shown recent revenue industry demand product attractiveness effect of cost reduction effect of cost reduction on profit earnings cost of capital effect of cost reduction on investment investments in innovation quality and differentiation even though many had never made any money among the more established firms enjoying rapid growth many are able to pay no dividends as investors prefer to let the firm reinvest its earnings in additional growth because the market value of a firm is quite sensitive to recent profits and es pecially growth rates the strength of these loops can change quickly a drop in growth expectations for any reason a sales slowdown the entry of a strong com petitor to the market can swiftly reduce market value effectively the firm out of the equity market for new capital as market value and cash flow fall rela tive to current obligations the perceived risk of debt increases and the bond mar ket will require a higher risk premium on any new borrowing while these loops can give a healthy growing firm still greater advantage over its slower growing less profitable rivals they can swiftly become a death spiral for an organization in financial distress figure the golden rule whoever has the gold makes the rules part the dynamics of growth the rules of the game the larger and more successful an organization the more it can influence the in stitutional and political context in which it operates large organizations can change the rules of the game in their favor leading to still more success and more power figure shows the resulting golden rule loop the golden rule loop manifests in many forms through campaign contributions and lobbying large firms and their trade associations can shape legislation and public policy to give them favorable tax treatment subsidies for their activities protection for their markets price guarantees and exemptions from liability through overlapping boards the revolving door between industry and government and control of me dia outlets influential and powerful organizations gain even more influence and power in nations without a tradition of democratic government these loops lead to self perpetuating oligarchies where a tightly knit elite controls a huge share of the nation wealth and income while the vast majority of people remain impover ished the philippines under marcos indonesia under suharto and countless others the elite further consolidates its control by subsidizing the military and se cret police and buying high tech weaponry and technical assistance from the de veloped world to keep the restive masses in check even in nations with strong democratic traditions these positive loops can overwhelm the checks and balances designed to ensure government of by and for the people ambition and aspirations another powerful positive feedback arises from the aspirations and ambitions of a firm founders and leaders figure all organizations must choose whether to eat their seed corn or plant it to seek an even larger crop next season firms can pay out their profits to shareholders in the form of dividends or they can invest in the further growth of the enterprise which course they take depends on their aspi rations for growth growth aspirations themselves are frequently flexible and adapt favorable rules of the game organizational size and influence chapter path dependence and positive feedback to actual accomplishment see chapter cyert and march forrester lant by constantly revising aspirations upward the leadership of an organization can create perpetual pressure for greater achievement many top managers believe the proper way to motivate their troops is by setting aggressive stretch goals that are far above current achievement see and prahalad concept of strategy as stretch as the organization responds and actual achieve ment rises the bar is raised further thus business units are often given aggressive sales and profit growth goals for the coming fiscal year these high level goals are then translated into specific targets for the individual activities within each unit each based on recent performance but adjusted by a stretch factor for example each function in the business unit might be required to cut costs by next quar ter while raising the quota for each sales representative the use of floating goals based on recent performance plus a stretch factor can be highly effective setting goals out in front of actual accomplishment often helps people reach their ultimate potential athletes seek to exceed their personal best or break the most recent record when the record falls the goal shifts as well as stu dents master a subject or concept they are given more difficult tasks managers set their sights on the next promotion politicians aspire to the next highest office but there are dangers lifting goals as accomplishment rises means there will always be tension and dissatisfaction a hunger for more that hunger can be a powerful motivator but it can also lead to burnout frustration and feelings of in adequacy see homer for models of worker burnout under stretch ob jectives also see simon and it can lead to monomaniacal behavior in which people sacrifice their friends family and ethics in endless pursuit of the next level the ability of leaders to articulate their vision and spur the best efforts of their employees depends not only on their personal charisma but also on the size of the figure floating goals and stretch objectives level of stretch desired stretch factor objectives motivation performance shortfall effort gap between investment aspiration and achievement achievement part the dynamics of growth organization and the integrity of its culture traditions and folklore the larger the organization and the less cohesive its culture the harder it is for leaders to project their goals and motivate employees creating negative feedbacks that can limit the ability of stretch objectives to generate growth creating synergy for corporate growth the preceding sections identified more than three dozen positive loops that can drive the growth of a business enterprise how important are they if these loops are significant the firms or industry groups most successful in exploiting them should exhibit growth rates profitability and market shares significantly higher than average over extended periods of time firms where the positive loops oper ate as vicious cycles should yield persistently lower returns however traditional economic theory suggests that markets are dominated by negative feedbacks if profits in an industry were significantly higher than average existing firms would expand and new firms would enter the market expanding production and pushing prices down until profits were no higher on average than in any other industry or for any other firm on a risk adjusted basis the existence of persistent differences in profitability across firms and indus tries has been studied intensively mueller 1986 examined a large sample of the biggest us industrial firms and found significantly different rates of profit across firms even for firms within the same industry group and that these differ ences persist over very long time periods at least several decades and document similar results for uk firms and concluded that whilst two thirds of our sample converged towards a common profitability level a solid core of firms appear able to maintain some independence from market forces more or less indefinitely mueller also examined the dynamics of profitability ably if the negative feedbacks of traditional economics dominate firms whose profits are far from average at any time due to transient shocks would tend to ward the average while those near average would tend to remain there mueller found just the opposite firms with average profitability were over time more likely to migrate to states of either significantly higher or significantly lower profits firms with high profits had higher than expected probabilities of continu ing to generate high profits the performance of firms with low profits was more likely than expected to remain disappointing these dynamics are consistent with the differentiating disequilibrium effects of the many positive feedbacks discussed above what determines whether the positive loops will operate as virtuous cycles leading to growth and high profitability or vicious cycles trapping a firm in a reinforcing cycle of decline and low profit in the most successful firms many of these loops act in concert generating substantial synergies achi et al ex amined the performance of the fastest growing and most profitable firms in the us so called growth tigers to see if the feedbacks driving their growth could be identified they defined a growth tiger as a firm whose sales growth rate over the develops a model exploring the dynamics of goal formation in organizations addressing these issues of leadership and growth chapter path dependence and positive feedback prior years was three times greater than the average of the and which outperformed the in total return to shareholders over the prior years in a sample of more than firms met these criteria the growth tigers were not all small startups sales ranged from million to billion nor were they all high tech firms though of the tigers were in the computer sec tor including intel microsoft compaq and sun the remainder included firms in such low tech mature industries as industrial equipment business and fi nancial services retail distribution and wholesaling apparel fashion and sports and health care nike the home depot us healthcare united asset management werner enterprises and nautica sustained growth and profitability are not merely a function of being in a hot industry of course hot industries are hot because the positive loops driving their growth are strong the growth tigers generate a disproportionate share of the growth in the econ omy as a whole while comprising just of the firms in the sample they created of the total sales growth of the job growth and of the profit growth close examination showed that the tigers did not rely on any single posi tive loop to drive their growth but successfully used many of the positive feed backs discussed above to create synergy microsoft is the paradigm case the costs of producing software are almost entirely up front development costs so the re duction in unit costs as the software market exploded is a very powerful growth driver similarly microsoft expansion from operating systems to applications the internet news networks publishing automotive computing and other markets creates powerful scale and scope economies microsoft also benefits from learning curves and from substantial investment in process improvement focused on im proving customer needs assessment and speeding software development it invests heavily in product differentiation and new product development microsoft fi nancial clout enables it to preempt competition by acquiring rivals and potential ri vals often buying software start ups and incorporating their products into microsoft own applications and operating systems microsoft market power en ables it to negotiate favorable distribution agreements and prices with computer makers its growth allows it to recruit the best programmers and managers and compensate them with stock options which builds a dedicated and productive workforce and frees up resources for other investments microsoft positive cash flow and high multiple cut its cost of capital far below that of weaker rivals and risky startups and growth is powerfully driven by the expansive aspirations of bill gates a vision he has through a well funded public relations effort successfully articulated not only within microsoft but in society at large through ghost written books and media appearances most of all microsoft success stems from powerful network and comple mentary goods effects these feedbacks operate through many channels hardware architecture and operating systems operating systems and applications applications and users and software and programmers the larger the installed base of microsoft products the more attractive are computers compatible with those products powered by intel and intel compatible chips the more personal computers sold with intel inside the larger microsoft installed base the larger the installed base of microsoft operating systems the more software will be de veloped for those systems by third party developers and the more third party part the dynamics of growth software there is the greater the attractiveness of the wintel platform the larger the number of people using microsoft applications the more important it is for others to have compatible software to exchange documents with colleagues and friends so the greater the attractiveness of microsoft applications and the larger microsoft share of the installed base the greater the number and higher the qual ity of programmers support personnel and it managers trained in those systems and the scarcer are those familiar with other operating systems and applications powered by these positive feedbacks microsoft grew from its founding to a firm with revenues of billion by august its market capi talization was nearly billion by comparison general electric with about times more revenue and times more employees than microsoft was worth less than billion bill gates is quite aware of these positive feedbacks and their role in his suc cess table of course growth cannot continue forever ultimately as the limits to growth are approached various negative feedbacks must grow stronger until they over whelm the positive loops if microsoft continues to grow at its historical rate its sales would exceed the gross domestic product of the united states by when bill gates will be only years old even if the us economy keeps growing at its historical some of the negative feedbacks were already apparent by the mid con cern over microsoft ability to use the positive feedbacks driving growth to dom inate the software market prompted its competitors to join together to promote sun java microsoft has proven to be adept at blunting these moves through what bill gates calls embracing and extending the innovations of its competitors it was precisely concern over microsoft ability to embrace and extend to use the positive loops to dominate the emerging digital economy that prompted the us justice department antitrust suit over the bundling of microsoft internet explorer with the windows operating system not all the positive feedbacks that can drive corporate growth are compatible with one another pursuing the differentiation strategy by charging higher prices based on the superiority of your products and support capabilities conflicts with using low initial prices to drive the scale economy learning curve and network complementary goods effects many long successful firms stumbled when the pos itive loops driving growth in their industry changed while their strategy did not for example the failure of sony can be traced to a mismatch between their strategy and the dominant loops in the home vcr market sony had long pur sued a strategy emphasizing product differentiation and innovation and sony products typically commanded a significant price premium relative to those of competitors sony strategy worked very well in markets where standards were already established such as television stereo amplifiers and cassette tape players but was ineffective in a market where network effects and complementary assets from through microsoft sales grew from million to a compound growth rate of over the same period nominal us gdp grew from bil lion to billion a compound growth rate of at these growth rates the two curves intersect after years chapter path dependence and positive feedback bill gates uses positive feedbacks comments of bill gates referenced positive feedback the network has enough users now that it is microsoft strategy benefiting from the positive feedback loop the more users it gets the more content it gets the more users it gets we witnessed this same phenomena sic with application availability the more applications are out there the more attractive the device becomes and the better the software business becomes interview red herring oct network and complementary goods this is a time period where now there a broad awareness that windows nt is by far the volume general purpose server platform the growth there continues to amaze us and it a positive feedback loop as we got more applications nt servers got more popular as it gotten more popular we ve got more applications computer news sept all about scale economies and market share when you re shipping a million units of windows effects unit development costs and scale economies software a month you can afford to spend loops million a year improving it and still sell at a low price fortune june the biggest advantage we have is that good developers like to work with good developers cusumano and selby microsoft secrets most people don t get millions of people giving them feedback about their products we have this whole group of two thousand people in the us alone that takes phone calls about our products and logs everything that done so we have a better feedback loop including the market cusumano and selby microsoft secrets loyalty and quality of workforce learning curve and process improvement loops and the installed base and availability of tapes rather than features quality or repu tation were the most important determinants of product attractiveness managers and entrepreneurs must design their growth strategy by identifying those positive loops likely to be most important in their markets most compatible with one an other and most consistent with the capabilities and resources the firmeither has or can develop positive feedback increasing returns and economic growth the many positive feedbacks discussed in section not only drive the growth of individual corporations but power the growth of entire industries and of the part the dynamics of growth economy as a whole the recognition that positive feedback is the engine of eco nomic growth can be traced back at least to adam smith wealth nations smith and the other classical economists did not draw causal diagrams but the various feedbacks are clearly seen in their writings smith focused on division of labor as the principal source of productivity growth as a process is divided into a larger number of routinized operations productivity grows because specialization enables people to learn faster to customize their tools and capital to the specific task and to eliminate the wasted effort that comes from moving from one opera tion to another smith noted that the division of labor is limited by the extent of the market recognizing the positive feedback by which economic growth enables greater specialization which in turn leads to greater productivity and still more economic growth economists generally refer to these positive loops as increasing returns the term denotes a situation in which the output of a process increases more than pro portionately as its inputs grow in contrast to the usual situation of diminishing re turns where output saturates as inputs grow as in agriculture where harvests are limited by the extent and fertility of the land no matter how much fertilizer or la bor are applied besides adam smith other early theories of increasing returns were developed by alfred marshall in and allyn young in see buchanan and for an excellent collection of key works in the econom ics of increasing returns formal models embodying positive feedbacks include paul krugman models of international trade and paul romer models of endogenous economic growth for example noted that in the traditional economic theory of trade dominated by diminishing returns negative feedbacks two identical economies would have no incentive to trade since the greater transportation costs of trade would make it more efficient for each to produce the goods they need locally however in the presence of positive feedback it becomes advantageous for the two economies to trade even though they have identical resources technologies and consumer preferences the apparently paradoxical result arises because both economies can produce more if each specializes in the production of one class of goods and trades with the other for the rest they desire specializationboosts pro ductivity hence total output increases interestingly in the case of initially identi cal economies it doesn t matter which subset of goods each chooses to produce as long as they specialize in practice the choice would be determined by chance events early in the history of trading relations leading to the classical path depen dence analyzed above the implications of positive feedback apply not only to nations engaged in in ternational trade but also to any distinct economic entities that can exchange goods with others including regions within a single nation cities and towns within a re gion neighborhoods within a city or even the members of a family when the pos itive feedbacks created by division of labor scale and scope economies learning by doing and so on are strong specialization and trade can quickly transform an initially identical geography into a highly variegated landscape with specialized centers of industry such as silicon valley or the new york diamond district romer showed how growth for an economy as a whole could arise from some of the positive loops described above particularly those relating to research and chapter path dependence and positive feedback development learning by doing and other investments in human capital increas ing returns arise because the knowledge created by or employee training for example cannot be kept fully private while a machine tool can only be used in one place at a time knowledge of how to design a machine tool can be used by more than one firm at a time knowledge is not consumed by usage the way mate rial goods are consequently a firm investments in r d and training for exam ple not only benefit the firm but also spill over to benefit other firms in the language of economics these spillovers create externalities that is benefits exter nal to the firm these externalities speed economic growth because they benefit many besides the firm the investment increasing the total size of the market and further strengthening the many positive loops that depend on the scale of activity in an industry or region romer also showed that because individual firms generally don t understand and can t take advantage of the benefits their knowledge investments create for the economy as a whole there is a tendency for firms to underinvest in human capital and does the economy lock in to inferior technologies one consequence of path dependence is that random events early in the evolution of a system can push it down one path or another these random shocks can be small and might go unnoticed at the time or even in hindsight they can involve chance events within the firms in the industry or spillovers from unrelated politi cal technical or social events in the world at large the positive feedbacks amplify the differences among the contenders until one emerges as the standard and domi nates the industry success begets success as the winner emerges the costs of switching from one standard to another become greater and greater until the sys tem locks in to that equilibrium the process described in section shows how path dependence and lock in can occur when all equilibria are initially equally attractive it doesn t matter whether we drive on the right or left or whether clocks go clockwise or counterclockwise so long as we all choose the same direction more controversial is the notion that path dependence can lead the economy to lock in to equilibria to products technologies and ways of life that are inferior to others that might have been chosen see arthur if the dominant determinant of product attractiveness is compatibility and the availability of complementary goods personal computers keyboard layouts then a firm might become the market leader even though its technology is inferior many argue that the vcr industry provides an example of lock in to an inferior technology pointing out that offered superior picture quality and is today the standard for professional video equipment others focus on longer play time to argue that it was after all the superior technology the macin tosh operating system was clearly superior to microsoft dos and early versions of windows yet microsoft systems became the standard while the macintosh steadily lost market share the qwerty keyboard invented by christopher sholes in the is widely considered to be inferior to the dvorak key board in terms of training time typing speed error rates balance between the left and right hands and comfort yet nearly everyone continues to learn the qwerty part the dynamics of growth the irrational english system of measurement with its feet yards pounds gallons and acres is clearly inferior to the metric system yet continues to be used in the us the likelihood of in to an inferior technology increases with the strength of the positive loops that confer advantage to the market leader indepen dent of the attributes of the technology itself the stronger the network compati bility development cost market power and golden rule loops the more likely it is the ultimate winner will be determined by factors unrelated to product quality functionality and features continued lock in to the qwerty keyboard is due to the great importance of complementary assets specifically typists trained in qwerty the switching costs of retraining the huge installed base of typists in the dvorak system outweigh the advantage of dvorak perpetuating the dominance the prevalence of positive feedbacks in the economy does occasionally cause lock in to inferior technologies but the issue is considerably more complex tech nologies evolve an initially inferior technology might win the battle for market share and emerge as a new standard but later improvements might overcome its initial deficiencies microsoft again provides an example the dos operating sys tem was unquestionably inferior to the macintosh yet microsoft became the in dustry standard while the mac withered microsoft was then able to imitate the graphical interface of the mac incorporating many of its features in the windows operating system the first versions of windows through windows were still clearly inferior to the macintosh but microsoft dominance allowed it to invest heavily in further improvements windows and in the judgment of many closed most of the gap and further innovation will no doubt lead to still greater functionality while the network and complementary goods loops did lead the soft ware industry to lock in to a technology that was inferior at the time the new prod uct development and differentiation loops gradually erased the deficit of course the macintosh operating system would presumably have evolved at a higher rate had it won the battle and become the standard it is entirely possible that computer users would have been better off if the initially superior technology had won it is not possible to answer such questions definitively because we can never know how much better the losers might have become a more subtle issue concerns the coevolution of people tastes with technol ogy people preferences are not static they evolve and change with experience your likes and dislikes adapt to your circumstances the amount of salt or hot pepper people consider palatable the amount of personal space people require the relative merits of qwerty and dvorak are still debated liebowitz and margolis argue that many of the studies showing the superiority of the dvorak layout are flawed the pre ponderance of the evidence however suggests dvorak layout is more efficient than qwerty another example develops a model showing how an economy can lock in to an inferior energy supply system see also fiddaman sterman and wittenberg develop a model of scientific revolution whose dynamics exhibit strong path dependence and find that the probability a given theory rises to dominance in its discipline is only weakly related to its intrinsic explanatory power while strongly determined by environmental conditions at the time of its founding chapter path dependence and positive feedback amount of leisure time and access to open space people desire all vary widely across cultures habituation is a powerful process similarly people evaluation of a technology can differ over time even though the technology itself may not change many city dwellers live more or less happily in environments noisier more crowded and more polluted than any their ancestors could have imagined or tolerated our evaluations of the attractiveness and desir ability of the ensemble of technologies and social structures modern society has been locked into for the past years differ from the way we would have evaluated them in because people preferences tastes and standards are malleable technology and our assessments and reactions to it coevolve garud and show how such coevolution shaped the emergence of cochlear implants a technology to provide hearing for the profoundly deaf rival technologies led to competing notions of what success would mean for patients receiving the technol ogy the ability to decode speech at a lower cost or to hear a wider spectrum of sound at a higher cost ultimately affecting government regulations and stan dards for the technology to lock the polya model and examples of path dependence suggest that path dependent systems rapidly lock in to a stable equilibrium which then persists indefinitely the clockwise convention was established by the the prime meridian con tinues to be located in greenwich though the sun has long since set on the british empire and the qwerty keyboard has been the bane of typing students for over a century are all path dependent systems perpetually trapped in the equilibria to which chance events lead them is there no escape there are many examples in which a dominant standard was overthrown such revolutions usually occur when the system in which standard is dominant be comes obsolete or is itself overthrown the dinosaurs ruled the earth for millions of years but after a catastrophic asteroid impact caused mass extinctions through out the plant and animal dinosaurs did not reemerge the impact de stroyed the ecosystem in which the dinosaurs had become the dominant standard in terms of the polya process the mass extinction event removed most of the stones species from the jar available ecological niches so that the selection of new stones the evolution of new species was once again strongly influenced by random events life filled the jar once again but different forms of life became in a process schumpeter famously dubbed creative destruction economic de pressions can unfreeze an economy that has locked in to certain technologies every economy needs basic technologies for energy transportation and commu nications an ensemble of technologies and infrastructure built around coal steam rail and the telegraph dominated the industrialized world in the late and early centuries populations and industry were concentrated in large cities sur rounded by farm and forest these technologies and settlement patterns were reinforcing coal has a fairly low energy density and is difficult to handle which gould for discussion of path dependence in evolution part the dynamics of growth favors centralized settlement patterns and transport modes like rail and steamship telegraph lines were often strung along the railroad right of way lowering the cost of infrastructure and maintenance the ensemble re mained dominant until the great depression of the the depression bankrupted many of the firms in these industries their physical infrastructure de teriorated and the power of their leaders waned when the economy began to recover from the depression in earnest after new investment did not recreate and refurbish the old networks and tech nologies but focused instead on a new ensemble of basic technologies the new economy of the postwar era was built around oil natural gas and electricity for energy internal combustion and electric motors for mechanical power auto mobiles and aircraft for transportation and telephone radio and television for communication the suburbs emerged and industrial location patterns became less centralized these technologies were also mutually reinforcing catalytic enabled crude oil to be refined into gasoline at low cost gasoline is an dense easily handled fuel suitable for a large fleet of small vehicles and decentral ized settlement patterns internal combustion engines are small and powerful enough to use in aircraft and so on all these technologies were invented well be fore the but the costs of switching were prohibitive because they were in compatible with the existing ensemble of technologies and social structures despite their great potential the new inventions could not achieve widespread use until the old infrastructure physical social and political was swept away by the great depression and second world war the depression and war functioned as a mass extinction event that erased the basis for the old technologies and the firms that dominated them just as new forms of life evolve after every mass extinction a new and different economy emerges with the recovery from every major great upheavals such as depressions or wars are not needed to unfreeze a sys tem that has locked in to a particular equilibrium shifts in technological archi tecture often undermine the basis for the dominance of a particular technology standard or firm the transistor made vacuum tubes obsolete and none of the leaders in the vacuum tube industry were able to translate their dominance in the old technology into a leadership role in the solid state world henderson and clark show that dominant firms at least in some industries rarely maintain their leadership positions or even survive after such changes in product architecture the same positive loops that confer cumulative advantage to a firm by building up networks of relationships and know how specific to the firm technology and market also create inertia and rigidity that make it difficult to adopt a radical and incompatible new technology see sastry the architectural shifts that undermine the dominant design and dominant firms in an industry often arise from innovations created by those very firms the computer industry provides another example firms such as ibm and digital equipment became hugely successful through exploitation of many of the positive further discussion of the interaction between economic cycles and the evolution of basic technologies see graham and senge and sterman 1986 chapter path dependence and positive feedback feedbacks described above especially the differentiation and innovation loops sections and by providing superior service and support ibm and technically excellent products digital these firms were able to charge compara tively high prices in turn high margins provided the resources for further invest ment in differentiation and innovation these differentiation strategies worked very well during the early years of the computer industry when the costs of computers were very high volumes were small development and capacity costs were a mod est fraction of total costs and computers were used for a limited set of specialized functions in central data processing centers as computers became cheaper more widely available and easier to use ser vice and support became less important when people buy a new pc every years to keep up with technical progress warranty and service capability are less important when applications use a point and click interface training and support are less important as employees teach themselves and each other as the cost of manufacturing fell while the complexity of designs increased up front develop ment costs became more and more important as computing costs fell computing became decentralized instead of a multimillion dollar mainframe sequestered in a cold clean room the employees now had a computer on their desk and compatibility became much more important the exploding number of com puters in use created lucrative markets for applications that induced third parties to enter the software market both greatly strengthening the complementary goods feedback and reducing the hardware makers control over these complementary goods the very success of the computer industry in exploiting the positive innovation and product differentiation loops caused these feedbacks to weaken destroying the effectiveness of the strategies that had created that success differentiation became less and less important while compatibility and software availability became more and more important success in a market dominated by compatibility software availability and economies of scale required aggressively lower prices to generate the volume required to offset high development costs and win the battle for market share mainframe and minicomputer makers like ibm digital equipment wang laboratories data general and prime computer failed to recognize the shift in loop dominance they themselves helped to bring about these firms suddenly found themselves with capabilities resources strategies and cost structures grossly out of alignment with the requirements for success where once they rode the positive differentiation feedbacks to greater and greater success now these loops became death spirals leading to faster and faster collapse some of these for mer industry giants survive as mere shadows while many vanished altogether modeling path dependence and standards formation the linear and nonlinear polya processes above provide simple illustrations of path dependent systems but they do not provide realistic models of path depen dence in economic or social systems such as the competition between and vhs or the triumph of the wintel architecture over the macintosh this section part the dynamics of growth develops a simple model of path dependence in the economy a model with more realistic formulations for the decision rules and which can be elaborated to include the many positive feedbacks described above model structure the battle for dominance between and vhs is typical of standards for mation for new products in markets where the utility of the product depends on the size of the installed base and the network of users one fax machine is not fax machines only become useful when there is a network of other compatible ma chines many products depend on the availability of complementary resources personal computers are not useful without compatible software automobiles are not useful without networks of roads gasoline stations and other auto friendly in frastructure in such markets the attractiveness of a product based on a given stan dard depends on its installed base and market share will depend on the relative attractiveness of the different competing standards figure shows the struc ture of a simple model to capture these feedbacks the diagram represents two products competing to be the standard in a market the products are assumed to be incompatible to keep the model as simple as possible only the most basic posi tive feedback through the installed base is represented explicitly prices and other determinants of product attractiveness are deliberately excluded the challenge at the end of this section invites you to extend the model to include these variables and other important loops such as the process by which developers of complemen tary products choose which format to adopt the installed base of each firm is increased by the sales of each firm product two firms i are assumed in the simulations below but the model can accommodate any number of firms for simplicity assume no discards and no repeat purchases so there is no outflow from the installed base installed base product of product initial installed base of product the sales rate of each firm is the product of industry demand and its market share sales of product i total demand market share product for now assume industry demand is exogenous and constant in reality of course there are many feedbacks to industry demand section market share is determined by the attractiveness of each firm products rela tive to the attractiveness of the other firms products the formulation for market share must meet several criteria first market share should be increasing as the attractiveness of the firm product rises and decreasing as the attractiveness of competitors products rises second market share must be bounded between and finally the sum of the market shares of all firms must equal at all times a formulation that meets these requirements is market share product attractiveness of product total attractiveness of all products totalattractiveness of all products attractiveness of product j l figure structure for a simple model of network effects initial installed base product initial installed base product threshold for compatibility effect of effects compatibility on attractiveness of sensitivity of attractiveness to installed base noise seed for random effects on effect of other attractiveness of factors on product attractiveness of total product deviation demand attractiveness of of random effects on attractiveness share attractiveness effect of other factors on attractiveness of noise seed for product product effects on attractiveness of effect of product market compatibility on share product effect installed base attractiveness of product sales of product product w part the dynamics of growth where n is the total number of firms total attractiveness is the sum of the attrac tiveness levels of all products in the marketplace how should attractiveness be specified attractiveness depends on a wide range of variables including price availability quality service features and so on in this simple model overall attractiveness is the product of two terms the ef fect of compatibility on attractiveness the network effect and the effect of all other factors of attractiveness the formulation aggregates the effects of price fea tures availability and so on into a single factor which in this simple model is as sumed to be exogenous attractiveness of effect of compatibility on attractiveness of product i product effect of other factors on attractiveness of product i the effect of compatibility on attractiveness captures the network and compatibil ity effects the larger the installed base the greater the attractiveness of that prod uct there are a number of plausible shapes for the relationship between installed base and attractiveness one commonly used relationship is given by the exponen tial function effect of compatibility on attractiveness exp of product i in this equation attractiveness rises exponentially as the installed base grows rela tive to the threshold for compatibility effects the parameter sensitivity of at tractiveness to installed base controls the strength of the effect the threshold is a scaling factor that represents the size of the installed base above which network ef fects become the exponential curve for attractiveness is plausible when there were only two telephones in the united states the utility of the phone was not very great to the third potential buyer but when there were million the utility of the telephone to the next buyer was much much greater the expo nential function means attractiveness rises at an increasing rate as the installed base the larger the threshold for compatibility effects the larger the installed base must be before its effect on attractiveness begins to outweigh the effects of other factors of attractiveness for example betamax as the first home vcr format to reach the mar ket had a large relative advantage in installed base in the early years but even though there were many more betamax machines than vhs machines early on the effect of this relative advantage was slight so few people had machines that only the ratio of the sensitivity to the threshold matters nevertheless they are conceptually distinct applying the sensitivity to the normalized ratio of installed base makes it much easier to interpret the model and parameters exponential function is simple and convenient analytically but is not robust with the exponential function for attractiveness the increase in attractiveness from adding another unit to the installed base is always greater than that of the unit before a more realistic function would saturate for high levels of the installed base representing the eventual dominance of diminishing returns as the installed base becomes very large chapter discusses the construction of such nonlinear functions chapter path dependence and positive feedback compatibility was not yet an issue for most potential purchasers as the installed base grew however compatibility began to loom large in people assessments of product attractiveness in this simple model the other factors of attractiveness are exogenous and as sumed to vary randomly around the neutral value of one effect of other factors standard deviation noise seed for on attractiveness normal of random effects random effects on product i on attractivenes attractivenessof product where the standard deviation noise seed function samples from a normal distribution with a mean and standard deviation set by the modeler the noise seed is different for each product to ensure that the random effects for each product are independent the formulation for market share meets all three criteria for a good formula tion the greater the attractiveness of firm the greater its market share will be market share is zero if the attractiveness of the firm products is zero and if the competitors products are completely unattractive the sum of the market shares for all firms will always equal for any number of firms these prop erties hold for any functions relating product attributes to attractiveness many shapes for the individual attractiveness functions are plausible the exponential function used here is especially convenient because it can be transformed into a form in which market shares can be expressed as a linear function of the attributes of product attractiveness allowing the attractiveness functions to be estimated by standard regression techniques when product attractiveness is specified as the product of exponential functions of each attribute the formulation for market share is known as a logit function because market share as a function of product attrib utes follows a logistic figure illustrates the logit model for various values of the parameters the graph shows the market share of firm in a two firm market as its installed base varies the installed base of firm is assumed to be constant and equal to the threshold for compatibility effects the graph shows the resulting market share of firm different values of the sensitivity of attractiveness to installed base in all cases when the installed bases of the two products are equal along with all other factors of attractiveness each firm receives half the market market share follows the logistic curve as installed base varies note that the marginal impact of an increase in installed base on market share diminishes as installed base becomes very large once market share approaches further increases in attractiveness formulation for the random effects on attractiveness used here selects a new random draw every time step in the simulation this is technically not correct since changing the time step for updating the states of the model will dramatically alter the random shocks affecting the system random shocks in real systems are correlated especially over short time frames since real sys tems have inertia that prevents very large changes in the values of variables from one moment to the next a more appropriate model of the noise process would be so called pink noise that is noise that is serially correlated see appendix b for models of pink noise suitable for use in continuous time simulations properties and estimation issues for logit models and other models of choice are discussed in many statistics texts aldrich and nelson 396 part the dynamics of growth figure behavior of the model for market share two firms are assumed the graph shows mar ket share of firm l as a function of its installed base relative to the ln ln o share threshold for com patibility effects for various values of the sensitivity of attractiveness to installed base the installed base of firm is as sumed to be constant and equal to the threshold in all cases installed base of product i threshold for compatibility effects dimensionless have a smaller and smaller effect since there is simply less additional market share to gain the greater the sensitivity of attractiveness to the installed base the sharper and steeper the logistic curve and the more rapidly share approaches its extreme values as installed base varies model behavior to simulate the model the parameters were set as shown in table in particu lar the sensitivity of attractiveness to installed base is set to representing a mod est network effect if early in the history of the market the installed base of product is of the threshold while that of the competitor is the market share of firm be only even though it enjoys a advantage in installed base if the competitor had of the threshold while firm as much still a the market share of firm would then be reflecting the greater impact of a large installed base on attractiveness the simulation begins with a level playing field the parameters for both firms are identical the only difference between the firms arises through the random variations in the attractiveness of each product from other factors these random effects are assumed to have a very small standard deviation just figure shows simulations of the model prior to the introduction of any random variations in product attractiveness the two firms have the same over all attractiveness and market share remains at the initial equilibrium of when the random effects begin at time zero the network effect is weak so mar ket share fluctuates randomly in the neighborhood of as the installed base of each firm grows however the positive network feedback gains in strength and am plifies any small advantage in installed base created by the random shocks in prod uct attractiveness as the installed base advantage of one firm grows the positive network feedback gains even more strength further boosting the market share of the leader until share approaches there are only two stable equilibria com plete market dominance or extinction given the parameters in the simulation the system locks in to one of these equilibria quite rapidly figure shows the distribution of market shares for firm at various times in a sample of simulations prior to year there are no random effects chapter path dependence and positive feedback table total demand million parameters for sensitivity of attractiveness from installed base dimensionless installed base threshold for compatibility effects million units model standard deviation of random effects on attractiveness dimensionless initial installed base product i unit initial time years time step for simulation years figure simulations of the installed base twenty are shown at lime zero the staindard deviation of the random ef fects on producl attractiveness rises to years and the system is balanced on the unstable equilibrium of market share at time zero the first random shocks begin to perturb the system but the positive feedbacks have not yet begun to operate market share is tightly clustered between about and and the distribution of market shares is normal bell shaped the distribution changes only slightly for the first few years even after the feed backs in the system begin to operate by year the variance of the distribution of market shares has grown substantially but the distribution still appears to be roughly normal with a single peak at market share by year the distribu tion has spread still further and has begun to bifurcate into two modes the posi tive network feedback now rapidly differentiates the two firms from one another until one gains of the market and the other is wiped out by year the market share of the winning firm in nearly all simulations is greater than the behavior of the model is similar to the nonlinear polya process in section however the model relaxes the restrictive assumptions of the polya model first the model is formulated in continuous time second where the polya process selects only one stone per period either black or white here total sales are divided into simultaneous and continuous flows of sales for each product where the polya process chooses which color to add based on a single random event the model here includes multiple sources of random variation in consumer choices most importantly the attractiveness of each product depends not on the size of the installed base relative to that of other products but on the absolute size of each product installed base in the polya process the probability of selecting a given color depends only on the proportion of stones with that color already in the jar part the dynamics of growth figure evolution of the distribution of market share the distribution of market share for firm in simulations shown every years vertical axis is the proportion of simulations falling within each increment of market share market share firm chapter path dependence and positive feedback this assumption is not realistic for products with compatibility and network effects first consumers are not likely to know the installed base of each product and the decision rules in models should not use information the real decision mak ers do not have second the assumption means the effect of compatibility on market share is the same for a given ratio of the installed bases of the different products no matter how large the installed base a installed base advantage for vhs would yield the same market share advantage whether the installed base was vhs to machines or million to million people decisions are influenced by compatibility with the machines owned by others in their social network the larger the installed base of each product the greater the chance that any potential buyer will have friends and family who al ready own that format clearly when the installed base of products is very low compatibility is not yet a factor for prospective purchasers as the total installed base grows and more of the people a potential buyer interacts with have the prod uct compatibility becomes progressively more important the formulation for product attractiveness meets this criterion because it depends on the size of the in stalled base of each product scaled by the threshold for compatibility effects the exponential function for attractiveness reduces the effect of differences in in stalled base when the total installed base is very small and amplifies the difference as the total installed base grows as a result the strength of the positive network feedback increases as the mar ket grows these shifts in loop dominance can be illustrated by constructing the phase plot for the model the phase plot shows how market share for a given prod uct depends on that product share of the total installed base the phase plot is analogous to the phase plot for the nonlinear process shown in figure the fraction of the installed base of a given product is analogous to the proportion of stones of a given color already in the jar market share is analogous to the prob ability of adding a stone of a given color to the jar as in the prior phase plots the fixed points points where the phase plot crosses the line are equilibria for market share figure whenever the curve defining market share lies above the line market share for firm ex ceeds firm share of the installed base causing firm share of the installed base to rise the trajectory of the system flows along the market share curve to the right toward a higher share of the installed base until share reaches equilibrium at a fixed point where it meets the line conversely when the phase plot lies be low the line firm l market share is less than its current share of the installed base so its share of the installed base will fall the trajectory of the system flows along the phase plot to the left until it comes to another equilibrium where it meets the line because the share of installed base rises whenever market share is above the line and falls whenever market share is below it the stability of any equilib rium point is easily determined from the phase plot if the slope of the phase plot at an equilibrium point is greater than that equilibrium point is unstable a slight increase in the product share of installed base causes an even greater increase in market share further boosting the product share of the installed base and pro gressively moving the system away from the equilibrium a slight decrease in the product share of installed base causes a larger drop in market share further re ducing the share of installed base and moving the system farther to the left away part the dynamics of growth figure hypothetical phase plot showing location and stability of equilibria fixed points where the phase plot crosses the line are the equilibria of the system fixed points where the phase plot has a slope greater than are unstable those with slope less than are stable arrows indicate the flow along the phase plot installed base of product total installed base dimensionless from the equilibrium point when the slope of the phase plot is greater than unity the system dynamics are dominated by the positive feedbacks when however the slope of the phase plot at an equilibrium point is less than then a slight drop in product share of the installed base causes a smaller drop in market share since market share exceeds the current share of the installed base the share of the installed base will increase raising market share and moving the system back to ward the equilibrium point an increase in installed base has a similar compen satory effect because the market share rises by less than the installed base diluting the installed base until the system returns to the original equilibrium when the slope of the phase plot is less than unity the system dynamics are dominated by negative feedback because in general the phase plot is nonlinear its slope varies and as it does so too does the relative importance of the positive and negative loops in the system points where the slope of the phase plot shifts from less than greater than mark shifts in loop dominance from net negative to net positive feedback figure shows the phase plot for the market share model the phase plot shows the market share of firm as a function of the proportion of product in the total installed base however unlike the nonlinear polya process figure the strength of the positive network effect loop grows as the total installed base grows therefore the shape of the phase plot relating firm market share to its fraction of the total installed base changes as the total installed base grows the figure shows four of these curves for situations where the competitor installed base is and times the size of the threshold for compatibility effects the system always has an equilibrium where the market share and share of the total chapter path dependence and positive feedback figure phase plot for network effects model the phase plot shows the market share of firm as a function of its share of the total installed base function depends on the size of the installed base of the competitor and is shown for four values of the competitor installed base relative to the threshold for compatibility effects base product for compatibility effects the arrows show the direction of flow for each curve to derive the phase plot note that market share for firm is given by the model where ms is market share and a is the attractiveness of each product assuming the effects on attractiveness have a neutral effect attractiveness is determined only by the effect a where sensitivity of attractiveness from compatibility and is the installed base of product relative to the threshold for compatibility effects the ratio of the installed base of product to total base r is r b r and substituting into the equation for attractiveness yields market share for product as a function of share of the total installed the four curves in the figure assume and oo installed base of product total installed base dimensionless installed base are however the shape of the curves and the number and stability of equilibria change dramatically as the market grows when the total installed base of the industry is small the net work effect is weak the curve in figure labeled shows how market share evolves when the competitor installed base is just of the threshold for compatibility effects when the installed base is very small the network effect is so weak that the equilibrium at of the installed base is stable the phase plot crosses the line at share with a slope less than over a wide range random shocks affecting market share are self correcting to the extent a shock moves the system away from market share adjusts to compensate gradually return ing the installed base to a ratio of note that there are two ad ditional equilibria an unstable point when the share of installed base is about and a stable point when the share of in stalled base is to dominate the market when the total installed base is small firm have to have at least of the installed base as the total installed base rises the positive network effect loop grows stronger the slope of the phase plot at the point rises and the unstable equilibrium point at share moves to the left when the competitor installed base is half the threshold the curve labeled the slope of the phase plot at the equilibrium is just about equal to at this point the initial part the dynamics of growth equilibrium is bistable market share now always lies above the line if the share of installed base for firm drops market share rises compensating for the disturbance and returning the share of installed base to but an increase in installed base causes an even greater rise in share moving the system away from the equilibrium if random shocks initially give firm a small advantage in installed base market share will tend to rise further until firm the mar ket and reaches the stable equilibrium at of the installed base further growth in the total installed base continues to increase the strength of the positive network effect loop until it dominates the dynamics of the system when the competitor installed base is equal to the threshold the curve labeled the slope of the phase plot at the equilibrium is greater than and the equilibrium at is unstable there are now two stable equilibria one at of the installed base and one at about the positive loops dominate the system the firm that gains the largest share of the installed base wins an even larger share of the market and begins to consolidate its dominance of the industry while those finding themselves with the smallest shares of the installed base fall farther and farther behind as growth continues the strength of the positive network loop rises still more further accelerating the leader rise to dominance by the time the competitor in stalled base has reached twice the threshold the curve labeled the phase plot is quite steep around the equilibrium and the two stable equilibria have moved closer to and the positive loops are now so strong that lock in to a single standard is quite rapid and the chance that any random shocks or policies might reverse the outcome is vanishingly small policy implications the model results have clear implications for firms to use positive feed backs such as network effects to gain a decisive market share advantage and elim inate their competitors when a new product is first introduced to a market where no prior standards have been established the network effect is likely to be quite weak market share will be determined primarily by other product attributes such as quality price features and so on during this period a late entrant might by of fering a superior product aggressive pricing joint ventures with providers of com plementary assets and other means overcome the first mover advantage in installed base and take leadership of the industry the window of opportunity for such action is limited however as the market grows network effects and the availability of complementary products compatible prerecorded tapes for compatible software for computers grow in importance a firm that estab lishes a lead in installed base in the availability of complementary assets and in the perception that it is the market leader is likely to gain an edge in market share that leads to further gains in a self fulfilling prophecy as the installed base grows and the network effects become even stronger the chance that a late entrant can overcome the advantage of the first mover declines rapidly both because the total installed base is growing requiring the upstart to sell more units and because compatibility becomes a more and more important determinant of customer pur chase decisions giving the current leader more of an edge chapter path dependence and positive feedback these dynamics describe what happened in the vcr industry and help explain why sony as the first mover was unable to convert its early lead into market dom inance despite the large number of positive feedbacks conferring cumulative ad vantage to the leader when vhs was introduced the installed base of was so small that compatibility was not yet an issue for most customers other attributes of product attractiveness dominated in the purchase decision whereas sony hop ing to monopolize the format they believed would become the industry standard kept tight control of the technology thus restricting its availability and keeping the price relatively high matsushita decided to license vhs widely and cheaply the vhs consortium though the later entrant to the market was able to gain the largest share of the market just at the point where total sales growth exploded and rapidly overcame the initial installed base advantage of betamax vhs became the leader around the time film studios began to issue films for the home video market once the film studios decided to produce tapes for the home market compatibility became the dominant attribute of attractiveness in the purchase decision of most customers and film studios chose to issue tapes in the most prevalent format matsushita strategy gave them the lead in share of vhs tapes just at the time compatibility became critical though sony tried to fight back by lowering prices and encouraging production of betamax format tapes the window of opportunity had shut the growth of the installed base had strengthened the network effects so much that lead could not be overcome the fate of betamax was sealed policy analysis use the model developed in section to explore the policies suggested below in these tests start your simulations at time zero with the parameters described in table however you should eliminate the random shocks by setting the stan dard deviation of random effects on attractiveness to zero suppose firm attempts to gain initial advantage by seeding the marketplace with some free units so that at the start of the simulation the installed base of firm is while firm initial installed base remains run the model what is the initial market share of firm what happens to market share over time and why suppose firm attempts to counter firm effort to win the market by doling out free units of its own product however it takes time for firm to react so the free units of firm product don t begin to reach the market until months have passed suppose further that it takes year to distribute all to implement this policy modify the equation for sales of product as follows sales of product total demand market share product extra sales of product sales start time duration of extra sales extra sales start time duration of extra sales part the dynamics of growth where if start time time start time duration duration otherwise the pulse function is zero until the start time then takes a value of for duration time units and returns to zero thereafter the modified equation therefore increases sales of product at a rate of per year for starting at time years increasing the in stalled base of product exactly units all else equal does firm policy of seeding the market with extra units to counter firm initial advantage work not how many units must firm add to its sales rate over the course of a year starting at time to overcome firm initial advantage and win the mar ket estimating this quantity to the nearest is sufficient pre cision suppose firm waits until year to counter firm advantage again firm starts with an initial installed base of units and firm starts with how many units must firm now add to its installed base over the course of year to overcome the lead of firm capture the market why estimating this quantity to the nearest units per year is suffi cient precision what do you conclude about the optimal strategy for a firm in markets char acterized by strong positive network effects how would you implement the winning strategy what considerations might temper or reverse this conclusion what other strategies besides free distribution of product might a firm use to counter the initial advantage of a rival give examples chapter path dependence and positive feedback explore the behavior of the revised model for different values of the product lifetime setting the average lifetime of the product to a very large number such as one trillion gives you the base case of the original model what is the effect of discards on the rate at which the system locks in to a standard explain in terms of the feedback structure hint plot the market share and share of installed base for firm how does their relationship change as the average lifetime of the product changes the simple model aggregates many positive feedbacks into a single effect of installed base on product attractiveness however the network effect is only one of many important positive loops the availability of comple mentary resources is often even more important without compatible tapes and computers without compatible software are useless the dvorak typewriter keyboard is faster than the qwerty keyboard but is useless without dvorak trained typists aggregating the effect of complementary products into the network effect is not generally appropriate because these two loops operate with different time delays and involve decisions made by different groups complementary products can be produced by third parties modify the model to include the availability of complementary products explicitly to do so make the following assumptions a the total production of complementary products is divided into pro duction of goods compatible with product and production of goods compatible with product b the total production of complementary goods should be proportional to the total installed base of product and product the bigger the size of the market the greater the output of complementary products videotapes software typists will be c use the formulation to determine the share of total complemen tary good production going to each product the share of complemen tary goods produced for each format is given by the attractiveness of that format relative to the attractiveness of all format options the at tractiveness of a given format to a producer of complementary goods depends on the size of the installed base of products using that format aggregate the effects of all other considerations into an exogenous term attractiveness of product i to third parties from other factors d production of each type of complementary good accumulates in a stock assume complementary goods have an average useful life of years assume a first order discard process unlike part above dis cards of complementary goods are not automatically replaced that is the total production of complementary goods does not include the total discard rate assume the initial installed base of each type of comple mentary good is zero you may vary this as a policy later e modify the formulation for the attractiveness of each product equation to include an effect of the availability of complementary goods the effect should be formulated analogously to the network effect se lect parameters you think are reasonable use the vcr case as a guide part the dynamics of growth summary path dependence is a common phenomenon in natural and human systems path dependence arises in systems dominated by positive feedback even when all paths are initially equally attractive the symmetry is broken by microscopic noise and external perturbations the positive feedbacks then amplify these small initial dif ferences to macroscopic significance once a dominant design or standard has emerged the costs of switching become prohibitive so the equilibrium is forcing the system has locked in lock in persists until an architectural shift or large external shock renders the dominant design obsolete a wide range of posi tive feedbacks drives the growth of businesses the evidence suggests that the profitability of individual firms and the evolution of the economy as a whole is strongly influenced by these positive loops and exhibits path dependent behavior successful firms are able to strengthen several of the positive loops that can drive growth to create synergies that leads to cumulative success path dependence in the economy is common because the growth of business enterprises is driven by a host of positive feedbacks these feedbacks involve scale economies learning network effects market power and many other processes the most successful firms are able to create synergy by using ensembles of these feedbacks to create a mutually consistent strategy however success with one set of these positive loops can lead to inertia and rigidity that prevent a firm that dom inates in one regime from maintaining its dominance as the technical economic political or social environment changes delays delay always breeds danger de cervantes don quixote book chap never do today what you can put off till tomorrow delay may give clearer light as to what is best to be done aaronburr delays are a critical source of dynamics in nearly all systems some delays breed danger by creating instability and oscillation others provide a clearer light by filtering out unwanted variability and enabling managers to separate signals from noise in this chapter you will explore the structure and behavior of delays develop various models of delays and test their response to a range of inputs the chapter will help you understand the dynamics of delays so that you can use them appropriately in more complex models the chapter also presents case studies highlighting the use of delays in various contexts including capital invest ment in the macroeconomy and forecasting demand at a successful semiconductor manufacturer deilays an introduction part iv tools for modeling dynamic systems computer simulations give your best intuitive estimate don t spend more than a few minutes on this challenge manufacturing firms determine the amount of plant and equipment they need based on the demand they expect for their products as well as the expected profitability of the new equipment suppose there is a sudden unanticipated increase in orders for the firm product how long does it take on average before the firm production capacity increases to the new level assume investments in new capacity are expected to yield the firm required return on investment suppose there is a sudden unanticipated increase of in the total demand for manufactured goods throughout the economy how long does it take for the economy as a whole to increase total manufacturing capacity to the new rate of aggregate orders consider the market for agricultural commodities such as pork what is the average delay between a rise in the price of pork and the resulting increase in pork supply how long does it take economic forecasters to revise their estimates of inflation that is if there is an unanticipated increase in the inflation rate how long will it take for the forecasts of the experts to adjust to the new rate how long does it take a nation like the united states to respond to an environmental challenge such as air or water pollution that is what is the time required to recognize high levels of pollutants such as carbon monoxide emitted by automobiles and reduce them within safe limits consider the post office suppose you deposit a mass mailing of letters all sent first class to various destinations around the country sketch the pattern of deliveries you expect assuming no letters get lost consider a firm forecast of the order rate for its product suppose the actual order rate and the forecast have been equal for a long time now suppose the actual order rate suddenly and unexpectedly increases by and remains at the new rate sketch the response of the forecast suppose it takes days for a manufacturer to receive parts from a supplier if the firm orders per day how units arc in the stock of parts on order suppose the parts order rate suddenly and unexpectedly increases to per day and remains at the higher rate sketch the response of the delivery rate and of the stock of parts on order suppose the part order rate for the firm in question remains constant at suddenly the time required to deliver the parts perma nently increases from to days sketch the response of the delivery rate and the stock of parts on order chapter delays defining delays delays are pervasive it takes time to measure and report information it takes time to make decisions and it takes time for decisions to affect the state of a system modelers need to understand how delays behave how to represent them how to choose among various types of delays in any modeling situation and how to esti mate their duration a delay is a process whose output lags behind its input in some fashion see the top panel in figure consider what inside the box marked delay a lit tle reflection shows there must be at least one stock within every delay since the output generally differs from the input it lags behind there must be a stock inside the process to accumulate the difference between input and output consider the process of mailing letters the input to the delay is the rate at which you mail let ters the output is the rate at which your letters are delivered where are the letters between the time you mail them and the time they are delivered they reside in a stock of letters in transit within the post office system figure shows the structure for the post office and the general structure for material delays the type of delay shown in figure known as a since it captures the physical flow of material in case letters through a delay process other examples of material delays include the flow of product through a supply chain the construction of buildings or the progression of design tasks through a product development process in each there are physical units cases of beer square feet of space or engineering drawings moving through the process notice that the only outflow from the stock of letters in transit in the diagram is the deliv ery rate in the model letters are never lost or misdirected unlike in the real post office the flow of letters through the delay is conserved figure delays always contain stocks the output of a delay lags behind the input input output general structure of a material delay material transit outflow rate rate the post office as a delay rate rate part iv tools for modeling dynamic systems many delays represent the gradual adjustment of perceptions or beliefs these are delays the delay between a change in the order rate for your com pany products and your belief about the likely future order rate is an example of an information delay suppose orders for your product have been steady at units per day and you expect them to continue at that rate suddenly orders jump to you are unlikely to immediately increase your belief about to morrow orders to units but if the order rate remains at day after day you will gradually increase your expectation of future orders until it eventually reaches the new rate there is a delay between the receipt of new infor mation and the updating of your beliefs though there is no physical flow of mate rial information delays still involve stocks in the example the stock in the delay is your belief about future orders a psychological state residing in your mental model in general any belief or perception involves an information delay because we cannot instantaneously update our mental models as new information is re ceived other examples of information delays include averages such as the average production rate of a product as shown below information delays do not involve conserved flows and cannot be modeled with the same structures used for material delays material delays structure and behavior having defined the stock and flow structure for a material delay figure it is necessary to formulate a decision rule equation for the outflow rate in many sit uations the outflows from stocks are constrained by various resources and you must explicitly model the way these resources determine the outflow see chapters and production cannot occur without labor materials capital and other re sources the capacity of the delay is sometimes high enough relative to the inflow rates that you can assume the outflow depends only on the past inflows in model ing the commercial real estate market of a city you might conclude that the capac ity of the construction industry in the region is ample or sufficiently flexible and model the construction delay for new buildings as a constant you might model the diffusion of dioxin through a town groundwater supply as a pure delay with a constant delay time based on the characteristics of the soil and subsurface mor phology in such pure delays the process governing the outflow from the stock of material in transit depends only on how much material is in transit and how long it been there not on any external resources delay time is independent of the input or stock in transit and the process is linear such pure delays are modeled as uncapacitated queuing processes the assumption that a delay is not capacity constrained is always an approxi mation and holds only over a certain range of inputs if the real estate market in the city booms orders for new buildings may outstrip the capacity of the construction industry and the average delay between commissioning a new building and its completion will increase in these cases the resources constraining the capacity of the process must be modeled explicitly you must answer two principal questions for every delay first what is the av erage length of the delay second what is the distribution of the output around the average delay time chapter delays figure some distributions of the outflow from a delay the input in all cases is a unit at time zero outflow a is a pipeline delay in which items together exactly delay time after they enter outflow distributions b d exhibit differenl degrees of variation in processing times for items so some arrive before and after the average delay time in all cases the average delay is the same and the areas under distribution are equal of the quantity by the input each curve represents the probability distribution describing the chance that any particular item exit the delay at a particular time time multiples of average delay time what is the average length of the delay how long on average does it take items to flow through the delay equivalently what is the average residence time for a unit in the delay how long on average does a unit stay in the stock of material in transit for the us post office the average delay for domestic first class mail might be on the order of days for the average delay between sending and receiv ing messages via the internet might be on the order of a few seconds in what do your reside between the time you send them and the time they are received in any application the length of the delay is an empirical issue to be investigated by data collection and field study see section what is the distribution of the output around the average delay time what happens once items enter the delay are they processed first come served or is there some mixing and reshuffling do all units spend the same time in the delay or is there some variation around the average with some units flow ing through the delay faster and some slower than average figure shows some possibilities for the outflow from a delay the figure shows the response of several different delays to a pulse input a pulse input is analogous to a mass mailing of a large number of letters a certain quantity of material is injected into the delay at a single instant the figure shows pulse function also known as the dirac delta function is the limit of a rectangular pulse starting at time with duration width w and height as the duration of the pulse goes to zero for t t t w for t t t w fort t w the pulse function has an area of unity thus an arbitrary pulse input of q units at time t is given by t in simulation models t is approximated by a rectangular pulse with a duration equal to the simulation time step dt and a height of part iv tools for modeling dynamic systems the output of the delay as a percent per time period of the total quantity input to the delay at time zero in all four cases the average processing time is the same one possibility outflow a is that the items entering the delay all proceed through the delay in exactly the same order and exit after exactly the same time in this case the output of the delay is also a pulse exiting the delay exactly delay time after the pulse input an automobile assembly line approximates a pipeline delay the cars move down the line in sequence each exiting in the same order they entered when the line is running smoothly the delay time or residence time in the delay is the same for all and the order of entry determines the order of exit in the language of queuing theory the service discipline of the assembly line is fifo first in first out the term service discipline refers to the decision rule for choosing which of the units in the stock of material in transit will be processed and exit first other types of service discipline include lifo last in first out a common situation in my pantry where the most recently purchased items are often placed at the front of the shelves and are then used first because they block the older items be hind them when you rotate your stock to reduce spoilage you are shifting from lifo to fifo discipline many other rules are possible including random selec tion or selection based on some other attribute as when candidates for organ trans plants are selected based on how sick they are or the chances of success rather than on how long they ve been on the waiting when large numbers of items or multiple servers are aggregated together ser vice discipline is often neither strictly fifo nor lifo if you mail a large number of letters all at once they will not all be delivered at once there will be a distrib ution around the average delivery time with some letters arriving sooner than av erage and some arriving later the variation arises because the letters are destined for different recipients and the travel times to each destination differ more im portant unlike car bodies on the assembly line letters are not processed in the same order they are mailed during the various stages of processing the letters are mixed with others sorting the letters by destination so they can be routed properly causes some of the mixing some is inadvertent as when the contents of a corner mailbox are dumped into a bin for transport to the local branch the consequence of mixing is some randomization of the processing order another source of dispersion in the outflow distribution is caused by random vari ations in the processing time itself consider the checkout delay at a supermarket you might choose to model the checkout process as a single material delay where the inflow rate is the rate at which shoppers join a checkout line and the outflow rate is the rate at which they leave the market variations in the amount of food in each shopper basket and in the speed of the clerks mean the processing time for each customer and each checkout lane can differ customers joining the checkout line next to yours after you do sometimes leave before you do so the order of exit is not the same as the order in which people queue as everyone knows the line you are in is always the slowest and forthcoming develop a dynamic model to evaluate various policies for allocating kidneys to transplant candidates chapter delays these sources of dispersion mean that in general when many items are intro duced into a delay at one time some items will exit earlier than others spreading out the distribution of the delivery rate the response of a delay to a pulse input such as shown in figure can be thought of as the probability distribution de scribing the likelihood that any given item is delivered at a particular time distri bution a has no variability in delivery times distributions b d have different degrees of mixing of these distribution b has the most variability in delivery times while distribution d has the least all four distributionsa d have the same average delay time and all conserve the inflow so the area under each distribution is the same of the inflow eventually exits from the stock of material in tran sit or equivalently the probability that any given letter is eventually delivered is in specifyingdelays you must consider not only the average length of the de lay but also the distribution of deliveries around the mean delay sometimes you can estimate the output distribution from the data other times you must estimate it by direct inspection of the delay process to see whether there is mixing or strict discipline and whether the processing time of individual items is constant or varies randomly from item to item pipeline delay as in the example of the auto assembly line you sometimesneed to model a delay in which the delay time is constant and in which the order of exit from the delay is precisely the same as the order of entry to do so requires delay also known as transportation lag the metaphor is an assembly line in which items are transported in order and at a constant rate figure in the presentation below the inflow to the delays will be exogenous explor ing the response of different delays to idealized exogenous inputs such as a pulse step ramp and fluctuation helps develop your intuition for their behavior so you can select the appropriate type in any modeling situation of course in your models the inputs to delays will in general be an endogenous part of the feedback structure the stock of material in transit for any material delay is given by material in transit material in for the pipeline delay the outflow is simply the inflow lagged by the average de lay time d d distribution a in figure is a pipeline delay when the inflow is a pulse the outflow is a pulse exactly d time units later there is no mixing in the processing order nor any variation in individual processing times the delay time for each item equals the average delay time first order material delay many delays do not approximate a pipeline delay there is mixing and variation in the individual processing times causing some variance in the distribution of part iv tools for modeling dynamic systems figure pipeline delay structure in a pipeline delay individual items exit the delay in the same order and after exactly the same time like widgets moving down an assembly line at a constant speed material in material in average delay time deliveries consider an example at the opposite extreme from a pipeline delay say water draining from a sink further imagine that the water in the sink is thoroughly mixed at all times figure in the case of perfect mixing the probability that any particular water mole cule is the next to flow out of the sink is the same for all the molecules in the sink independent of how long that molecule has been in the sink perfect mixing means the order of entry is irrelevant to the order of exit put another way perfect mixing destroys all information about the order of entry the outflow from a first order material delay is always proportional to the stock of material in transit outflow material in where d is again the average delay time note that the only inputs to the outflow rate are the stock of material in transit and the delay time information about the order of entry of individual items to the stock is not used to determine the out flow rate equation is the familiar linear first order negative feedback system chapter the outflow rate forms a negative feedback loop since the greater the stock of material in transit the greater the outflow lowering the stock distribu tion b in figure shows the response of a first order material delay to a pulse input the response is the familiar pattern of exponential decay figure chapter delays figure first order material delay structure the outflow is proportional to stock of material in the contents of the stock are perfectly mixed at all times so all iterns in the stock have same probability of exit of their arrival time average delay time outflow material in delay time immediately after the pulse the stock of material in transit jumps up to of the quantity added by the pulse input the outflow rate immediately rises to units per time period since the outflow now exceeds the inflow the stock of material in transit starts to fall as it falls so too does the outflow rate so the stock of material in transit and outflow rate fall at diminishing rates the initial outflow rate would deplete the stock in transit in delay time but as the stock in transit falls so does the outflow rate after delay time has passed the stock in transit has fallen by after periods of the items have been delivered and after periods have been higher order material delays pipeline delays with their rigid fifo service discipline are good models for some processes such as assembly lines first order delays with their assumption of per fect mixing are reasonable models of other delay processes such as chemical and heat diffusion in physical and biological systems and some analogous diffusion processes in social systems between these extremes lie many intermediate cases where there is some mixing in the processing order in these cases the outflow that exponential decay is given by s so when t d the stock s has fallen to of its initial level see chapter 418 part iv tools for modeling dynamic systems figure pulse response of first order material delay the input to the delay is a unit pulse at time zero the stock of material in transit instantly jumps to then decays exponentially with a time constant equal to the average delay time the initial rate of outflow would deplete the stock in transit in delay time note the tangent to the trajectory of the stock in transit at time zero but as the stock in transit falls so does the outflow rate yielding the familiar pattern of exponential decay l a a time multiples of average delay time stock of material in transit time multiples of average delay time gradually rises reaches a peak and then tails off to zero similar to distributions c and d in figure consider again the post office letters do not all arrive at one time as in a pipeline delay but neither is the delivery rate greatest immediately after your letters are mailed though letters are not processed in lockstep neither is the order of delivery independent of the order of mailing there is partial mix ing partial mixing can arise when a delay consists of multiple stages of processing in which items flow sequentially from one stage to the next but where each stage introduces some mixing for the case of the post office you can easily identify many stages of process ing letters first go into the corner mailbox then onto the truck that collects the mail then into bins at the local post office then after sorting onto trucks for de livery to the central post office then through more stages of sorting and process ing then onto trucks trains or planes for transport to the destination cities then to the local post offices in the destination communities and so on until they arrive at the mailboxes of the recipients each stage introduces some mixing and variability in individual processing times if the purpose of your model was to reengineer the post office workflow system you might have to represent all these stages separately and explicitly account for the different delay times and capacities of each stage chapter delays figure higher order delays are formed by cascading first order delays together rate stage stage stockin stockin transit stage transit outflow exit rate rate inflow stage average delay time sta e average delay time stage exit rate stage stock in average delay time rate stage stock in average delay time you would have a very detailed model indeed for other purposes such detail would not be necessary and a pure delay might be appropriate in many settings the stages of processing in such a system can be approxi mated well by cascading several first order material delays together in series for example a second order material delay consists of two first order delays in which the input to the second stage is the output of the first stage figure part tools for modeling dynamic systems the total stock in transit is the sum of the stock in transit at each stage the av erage total delay from inflow to outflow is the sum of the average delays of the in dividual stages in this fashion you can construct delays with an arbitrary number of stages delay times for the individual stages can differ if the data and model purpose warrant it though often it is fine to assume each stage has the same delay time a delay with n stages each with of the total delay time is known as an nth order material delay the equations for the nth order material delay denoted by the function are outflow d total material in transit i i material in transit material in transit inflowi material in material in inflow inflow exit rate stage fori net inflow rate exit rate exit rate stage fori e n exit rate stage outflow exit rate stage material in for n outflow material in fori n the initial condition material in transiti inflow initializes the delay in equilibrium so that the initial outflow equals the initial inflow distributions c and d in figure show the response to a unit pulse for a third and twelfth order delay respectively the higher the order of the delay the less mixing and the smaller the variance of the output in the limit an der delay consists of an infinite number of stages each with an infinitesimal delay time such a delay provides one bin or stock of material in transit for all items en tering at a given instant and moves them from one stage to the next before the next set of items entering in the next instant are added thus an infinite order delay preserves the order of entry and permits no mixing it is equivalent to a pipeline delay figure shows the pulse response of a third order delay figure shows the stocks and flows for the intermediate stages of processing immediately after the pulse the stock of material in stage to of the quantity added each stage of the delay is a first order delay in a third order delay the average de lay for each stage is one third of the total delay thus the stage l exit rate is expo nential decay with a time constant of the exit rate from stage the input to stage the stock of material in transit in stage rises as long as its input exceeds its output at about time the stock in stage has risen enough for the exit rate of stage to equal its inflow from stage the stock in stage peaks from then on the stage exit rate exceeds the inflow to stage so the stage stock in transit falls and with it the stage exit rate similarly the stage exit rate is the input to stage the stage stock in transit rises until its outflow equals the inflow chapter delays response of a third order delay a time multiples of average delay time stock of material in transit time multiples of average delay time which occurs at about t then gradually falls off as the outflow from the delay exceeds the inflow to stage similar dynamics apply to delays with order higher than referring back to figure note that except for the pipeline delay the peak response of material delays of order precedes the mean delay and there is a long tail in the distribution of deliveries many items are delivered earlier than average but some are delivered much later note also that as the order of the delay increases and hence as the de gree of mixing decreases the delivery distribution tightens up fewer items are de livered earlier than average more are delivered near the average delay time and fewer are delivered much later than average the higher the order of the delay the smaller the variance in the delivery distribution how much is in the delay little law the stock of material in transit accumulates the difference between the inflow and outflow to the delay it important to know how big the stock in transit will be for part iv tools for modeling dynamic systems figure pulse response of third order delay by stage of processing time multiples of average delay time 75 stock of material in transit time multiples of average delay time any given delay and inflow suppose the inflow has been constant long enough for the delay to reach equilibrium how big is the stock in transit consider the pipeline delay with input i output and delay time d i t d suppose the inflow and stock in transit are initially zero at time zero the inflow suddenly increases to a constant level i the outflow will continue to be zero until d periods have passed during this time the stock in transit s is increased by i units each period after d periods i and the stock of material in transit reaches equilib rium the equilibrium quantity in transit is therefore di stock in transit for any delay with input i and output is for a pipeline delay with and a step increase in the input from to i at time zero for t d and i for t d so the equilibrium value of the stock in transit is i di chapter delays now consider a first order delay equation the outflow of a first order delay is since in equilibrium the inflow and outflow are equal the equilibrium stock in transit is di units the same as that for the pipeline delay in fact the equilibrium stock in transit for a delay is always di units regardless of the probability distribution of the this remarkable property is known as little law after john little an mit professor of operations research who first proved it little law means that in equilibrium the stock in transit is fully characterized by the average delay time and inflow rate by little law a firm ordering widgets per day from a supplier requiring days to deliver will in equilibrium have widgets on order independent of the delivery distribution little law helps explain how delays give systems inertia if business goes sour and the company cuts part orders to zero its widget inventory will still swell by an additional before deliveries from the supplier can be cut off as suming no order cancellations are possible little law can also be used to estimate the average length of a delay from knowledge of the stock in transit and flows through the delay in equilibrium the average residence time of items in the delay is given by the ratio of the stock in transit to the outflow rate d thus if an insurance company has a pending pool of unresolved claims and settles an average of 000 per month the average time claimants wait to receive payment is months again this measure of delivery time holds strictly only in equilibrium example construction delays in the electric utility industry little law has dramatic implications for the cash flow and financing require ments of a business consider the electric power industry up through the early typical lead times for new plants were about years and the average service life of plants was about years if the demand for power was constant an in vestor owned utility with gigawatts gw billion watts of capacity would there fore need to add an average of gw of capacity per year to replace retirements of old plants with a year construction delay the utility would have to have and finance of capacity under construction at all times one quarter of its existing capacity in the lead times for large plants increased as utilities built larger and larger plants in a search for returns to scale and as environmental and regulatory constraints lengthened permitting delays lead times rose to about years for large coal plants and even longer for nuclear plants to offset the re tirement of old plants when the lead time is construction work in progress must double to gw half of capacity in reality the situation was far worse since the demand for power was grow ing at about through the early to offset the retirement of old plants and increase capacity a gw utility would need to complete construc tion of gw that year with a year construction delay the utility would need to start construction of gw of capacity and would have to finance the construction of about gw of capacity under construction when the construction time doubles to years the required completion rate of gw forces the utility to start construction of gw of new capacity and part iv tools for modeling dynamic systems finance more than gw of capacity under construction a increase and an investment greatly exceeding the book value of existing orders for power plants surged in the mid as utilities tried to respond to the rising lead times huge debts were taken on to finance the ever greater stock of construction work in progress in many cases electric power rates were raised to enable utilities to service these debts however as higher rates and lower than expected eco nomic growth caused power demand to fall the utilities suddenly found them selves carrying debt for power plants they didn t need orders for new plants plummeted and many were canceled but as the huge stock of plants under con struction continued to come on line the industry found itself with excess capacity profits fell and rates rose still more in some regions the higher rates led to even lower growth in power demand forcing rates even higher in what many analysts called the spiral of impossibility a number of major utilities went bankrupt dur ing this period especially those building large long lead time plants the excess capacity lasted through most of the many utilities realized that power plants with short planning permitting and construction times were a better investment even though their costs per of capacity were higher in an environment of uncertain demand growth the value of improved cash flow and lower risks of having the wrong capacity exceed the generation cost savings of fered by larger long lead time plants ford example is adapted from ford note that little law holds only in equilibrium when the inflow to a delay is growing the steady state size of the stock in transit is not indepen dent of the outflow distribution the calculations in the text assume the construction process for power plants is characterized by a pipeline delay in this case the construction completion rate s t d where s is the construction start rate and d is the construction delay in the steady state of exponential growth at fractional rate the start rate must therefore be given a year construction delay and demand growth rate completion of requires the start rate to be and a year completion time yields s at any time t the stock of capacity under construction cuc is cuc without loss of generality assume the construction completion rate is c at time zero in the steady state of exponential growth the stock of capacity under construction at time zero is then with and g a year construction delay requires construction work in progress to be gw with a year delay construction in progress rises to 38 gw to test the sensitivity of this calculation to the as sumed distribution of power plant deliveries consider the extreme assumption that the construction delay for power plants is first order the actual distribution of the delay outflow must be much closer to a pipeline delay then a construction completion rate of requires gw under construction or gw for a year delay and gw for a year delay for the stock of capacity under construction to grow at fractional rate g requires dt s s s implying s for a year delay and s 04 for a year delay chapter delays example accumulation of toxic compounds in the food chain little law also helps explain why toxins such as dioxin accumulate in the food chain and in humans and may cause significant health problems even though their concentration in the environment is very low the dioxin family including some furans and polychlorinated biphenyls are widely considered to be among the most potent carcinogens known they are also estrogen mimics that may dis rupt endocrine and reproductive function and have been associated with learning disabilities dioxins and other chlorinated hydrocarbons commonly used in pes ticides and herbicides are soluble in fat and persist in the body for years the half life of dioxin and related toxins in fatty tissue has been estimated to be to years corresponding to average residence times of to by little law the equilibrium concentration of dioxin in any level of the food chain would be years worth of average intake predators consuming those organisms would then ingest much higher concentrations than their prey at each level of the food chain the accumulation of toxins caused by the long degradation delay am plifies the concentration of toxins some species of fish have dioxin concentrations that of the surrounding water while typical human intake rates of dioxin are very small concentrations can build up to higher levels over a lifetime average daily exposure is estimated at to picograms of dioxin toxic equivalent per of body weight per day pg most of which we ingest in our how can such small intake rates have any effect on human health besides the extreme toxicity of dioxin the answer is the long residence time for dioxin in the body assuming a yearhalf life the equilibrium concentration of dioxin in humans ingesting pg would be 000 pg teq per kilogram of body mass this value is roughly consistent with though somewhat smaller than estimates of average loads of 000 to 000 pg body mass suggesting either the half lives are longer or the intake rates are higher than currently thought note that little law applies only in equilibrium in the dioxin example it would take to years to reach the equilibrium level assuming a constant intake rate a first order delay ad justs of the way to equilibrium after time constants further suggesting that estimates of dioxin ingestion rates or its half life in the human body are too low response of material delays to steps ramps and cycles the discussion so far described the response of material delays to a pulse input analogous to a single mass mailing of letters other common inputs used to test systems are the step a sudden permanent increase in the input from one rate to from chapter that the half life of an exponential decay process with time constant d is picogram is a trillionth of a gram one part in a toxic equivalent converts the toxicity of different dioxin like compounds into the equivalent quantity of the parent compound in the dioxin family dibenzo p dioxin sources for dioxin infor mation us epa see also part iv tools for modeling dynamic systems another the ramp a sudden transition from a constant level to linear growth ex ponential growth and cycles to test your understanding of delays do the follow ing challenge before proceeding withoutusing computer simulation sketch the response of a first order material delay with an average delay time of days to the inputs shown in figure in all cases assume that prior to time zero the delay is in equilibrium with the outflow and inflow both equal to a step input at time zero the inflow steps up to and remains at the higher rate b ramp input at time zero the inflow starts to rise linearly at a rate of c exponential growth at time zero the input starts to grow exponentially at d oscillation at time zero the input begins to fluctuate with an amplitude of and a period of days after you have sketched your intuitive estimate of the response of the delay to these inputs test your understanding by building a model of a first order delay and simulating its response to these inputs were you correct repeat steps and for a third order delay and for a pipeline delay how does the order of the delay affect the response to the different types of inputs does the steady state response of the different delays differ the steady state response is the behavior after a long time has passed and the relationship of input and output is no longer changing how does the transient short run response of the different delays vary explore how the response of the different delays to the different inputs is affected by changes in the delay time in particular explore the response of the different delays to the fluctuating input for different delay times information delays structure and behavior the discussion so far examines material delays in which the input to the delay is a physical inflow of items to a stock of units in transit and the outflow is the physi cal flow of items exiting the stock however many delays exist in channels of in formation feedback for example in the measurement or perception of a variable or in the updating of beliefs and forecasts such as the perceived order rate for a firm product or management belief about future inflation rates why do perceptions and forecasts inevitably involve delays all beliefs ex pectations forecasts and projections are based on information available to the de cision maker at the time which means information about the past it takes time to gather the information needed to form judgments and people don t change their minds immediately on the receipt of new information reflection and deliberation often take considerable time we often need still more time to adjust emotionally to a new situation before our beliefs and behavior can change part iv tools for modeling dynamic systems information delays cannot be modeled with the same structure used for mate rial delays because there is no physical inflow to a stock of material in transit the inputs and outputs of material delays are conserved for example a strike at the post office lengthens the delay in delivering mail reducing the delivery rate and causing the stock of mail in transit to build up in contrast information such as per ceptions and beliefs is not conserved consider a firm forecast of the order rate for its products the expected order rate responds with a delay to changes in actual market conditions the physical order rate does not flow into the delay rather in formation about the order rate enters the delay because information unlike mate rial flows is not conserved a different structure is needed to capture information delays modeling perceptions adaptive expectations and exponential smoothing the simplest information delay and one of the most widely used models of belief adjustment and forecasting is called exponential smoothing or adaptive expecta tions adaptive expectations mean the belief gradually adjusts to the actual value of the variable if your belief is persistently wrong you are likely to revise it until the error is eliminated figure shows the feedback structure of adaptive expectations in adaptive expectations the belief or perceived value of the input x is a stock x in perceived value figure feedback structure of adaptive expectations the perceived value of the input adjusts to the actual input in proportion to the size of the error in your belief the adjustment time determines how rapidly beliefs respond to error input reported value of variabie x in perceived value change in perceived value x chapter delays the rate of change in the belief is proportional to the gap between the current value of the input x and the perceived value change in perceived value x in a material delay the stock is the quantity of material in transit and the output of the delay is a flow in information delays the belief itself x is a stock why a perception or belief is a state of the system in this case a state of mind your belief about the value of some quantity tends to remain at its current value until there is some reason to change it in adaptive expectations a belief changes when it is in error that is when the actual state of affairs differs from the perceived state of af fairs the larger the error the greater the rate of adjustment in your belief you should recognize this structure as another example of the familiar first order linear negative feedback system chapter the state of the system adjusts in response to the gap between your current belief and the actual value of the variable this structure is known as afirst order delay or asfirst order exponential smoothing figure shows the response of first order smoothing to a permanent change in the input starting from an initial equilibrium in which the perceived and actual values of the variable are equal the response is classic exponential seeking behavior the rate of belief updating is greatest immediately after the figure response of adaptive expectations to a step change in the input the response to a permanent change in the input variable is exponential adjustment to the new level perceived and actual values time multiples of average delay time rate of change in perceived value erceived value time multiples of average delay time part iv tools for modeling dynamic systems change in the actual value of the variable when the error in the belief is greatest as the belief is updated the error falls and subsequent adjustments diminish until after about four time constants have passed the belief is once again correct a firm forecasts of incoming orders illustrate firms must forecast demand because it is costly and time consuming to alter production rates inventories and backlogs should buffer short term differences between orders and production a good forecasting procedure should filter out short term random changes in incom ing orders to avoid costly changes in output setups changeovers hiring and fir ing overtime etc while still responding quickly to changes in trends to avoid costly stockouts or excess inventories the challenge is to be responsive without overreacting to noise that is to tell which change in demand is the beginning of a new trend and which is a mere random blip exponential smoothing is widely used in forecasting due to its simplicity and low cost of computation additionally exponential smoothing has the desirable property that it automatically attempts to eliminate forecast errors figure shows the response of adaptive expectations to a simulated order stream for a prod uct the simulated order rate in this example follows a random walk varying widely from day to day week to week and month to month the expected order rate is formed by adaptive expectations with a day time constant exponential smoothing does a good job of smoothing out the short term high frequency noise while still following the slower movements in orders such as the rise from about around day to about around day note that the peaks and troughs in the expected order rate lag the turning points in the actual or der rate the process of smoothing inevitably introduces a delay to see why exponential smoothing introduces a delay notice the role of the ad justment time constant d in the negative feedback structure of adaptive expecta tions the negative loop functions to eliminate the error in the forecast but does so gradually so as not to overreact to temporary changes in the input your belief is a weighted average of the current value of the variable and your past belief which in turn reflects the prior history of the the analogy with a weighted average can be made exact consider again the problem of forecasting a firm order rate a common way to filter out quency noise is with a moving average for example a day moving average of daily sales would be of the sum of the daily sales for the past week every day the average would be updated in general a moving average can be represented as a weighted sum of all past values of the variable x i o the analytic solution of the first order linear negative feedback loop chapter when the input is a constant x the current value of the state of the system here x is given by where the weight w that is the current value of the perception is a weighted average of the initial value of the belief and the actual value of the variable the weight on the initial value of the belief declines exponentially at a rate determined by the time constant d chapter delays figure aptive expectations term noise response of exponential smoothing to hypothetical order for product 1250 m order rate the expected order rate is formed by exponential smoothing with a day adjustment the order rate follows a random walk change in daily orders is distributed with a standard deviation of 250 days where the weights must sum to in the case of a day moving average of daily values the weights are for the seven most recent values and zero for all prior values suppose sales had been constant for at least a week at a rate of the sales forecast would equal the actual sales rate x x now suppose sales suddenly doubled and remained at the higher level on the next day the moving average forecast would only rise by x x x x x each day the average would increase by another until after a week the forecast would finally equal the new sales rate the process of averaging necessarily introduces a delay because new values are weighted in with the old values the weights in a moving average indicate the relative importance of each past observation in forming the current perception or belief in the case of a day mov ing average yesterday sales are given just as much weight as the week old sales rate while all sales data prior to last week are ignored there is usually no strong reason to assume a sudden discontinuity in the importance of the past a more rea sonable model is to assume the importance of the data decline with age der smoothing is a moving average where the weights decline exponentially the most recent value gets the most weight with older values getting progres sively less adaptive expectations are a very simple model of expectation formation smoothing uses just a single input rather than drawing on many sources of data that single cue is then processed in a simple fashion can such a simple procedure actually be used to model the way firms form forecasts or the way people adjust their beliefs and expectations surprisingly the answer is often yes surveys of forecasting methods show exponential smoothing is one of the most common fore casting tools used smoothing is especially popular when a firm must forecast the demand for thousands of distinct items in these cases the simplicity low cost and error correcting properties of smoothing make it an excellent choice many studies show that first order adaptive expectations are often an excellent model of the way people forecast and update beliefs in a justly famous study et al ran a competition to identify the best time series forecasting methods they compared the forecasting performance of forecast ing techniques from naive forecasts tomorrow will be like today to sophisticated part iv tools for modeling dynamic systems methods such as arima models the methods were compared across data series encompassing a wide range of systems time horizons sampling frequen cies and patterns of behavior in general first order exponential smoothing per formed extremely well a second competition et al examined judgmental forecasting methods finding many judgmental forecasts are well ap proximated by simple smoothing armstrong provides a comprehensive re view of forecasting methods and documents the extensive literature showing the wide use and comparative accuracy of exponential smoothing in many contexts see also chapter higher order information delays just as there are cases where first order material delays are not appropriate so too there are situations where exponential smoothing is not the best model of an infor mation delay in a first order information delay like the first order material delay the output responds immediately to a change in the input in many cases however beliefs begin to respond only after some time has passed in these cases the weights on past information are initially low then build up to a peak before declining recent values of the input might receive low weight for several reasons often the delay intervening between the actual state of a system and the decisions that alter it involves multiple stages the current values of the in put may simply be unavailable due to measurement and reporting delays once data are reported there may be administrative delays reported information may not be taken up for consideration immediately finally there may be cognitive and decision making delays it takes time for decision makers to revise their beliefs and further time to finalize a judgment and act on it information delays in which there are multiple stages are analogous to the multiple stages in material delays and require analogous higher order delays one way to model a higher order information delay is with the pipeline delay structure in which the output is simply the input lagged by a constant time period such a delay might be used to model the measurement and reporting processes where the reported value available to decision makers is the actual value some pe riod of time in the past reported actual value t d where d is the reporting delay such a delay is analogous to the infinite order ma terial delay or pipeline delay discussed above the output of the delay tracks the input exactly but is shifted d units in time more often the measurement and reporting of information involves multiple stages and each stage involves some averaging or smoothing firms cannot report the instantaneous value of flows such as the rate at which orders are being placed this instant but must average sum up or accumulate sales over some finite time period to filter out short term variations and provide a meaningful estimate gen erating a forecast might actually involve several stages of information processing first order rates for a recent period such as a day or week are reported by indi vidual sales representatives introducing a reporting delay then the weekly sales figures are aggregated and reported to management introducing another delay chapter delays management periodically reviews the sales figures and then applies a forecasting procedure such as smoothing either formally or judgmentally these estimates can then be used to set production schedules further delays are introduced as the information is processed for use in other decisions such as budgets or earnings es timates prepared by market analysts in some cases the purpose of the model might require you to portray each of these steps explicitly usually it is sufficient to aggregate them together into a sin gle information delay just as first order material delays can be cascaded in series to generate higher order delays with more realistic response rates so too you can cascade first order smoothing structures to generate a family of higher order in formation delays figure an nth order information delay denoted by the function consists of n first order information delays cascaded in series the perceived value of each stage is the input to the next stage and the output of the delay is the perceived value of the final stage each stage has the same delay time equal to of the total delay d output d output in stagei input change in stagei input fori fori n figure structure of the third order information delay output d output in stage change in stage s in stage change in stage s in stage change in stage input part iv tools for modeling dynamic figure compares the response of the and twelfth order in formation delays to a step increase in the input as with the material delays the higher the order the smaller the initial response and the steeper and faster the even tual rise to the final value in the limit of an infinite order delay the output would exactly track the input t d periods in the past a pipeline delay response to variable delay times another important issue in modeling delays is whether the delay time is constant or changing relative to the purpose of your model can you consider the duration of the delay to be constant or might it vary if it varies does it vary exogenously or endogenously what happens when the delay time changes the delay times for both material and information delays can change raising the speed limit on us interstate highways from to reduced the delay in the transport of raw materials from supplier to customer assuming any truckers were actually obeying the mph speed limit in the first place replacing a based accounting system and manual data entry with a globally integrated real time client server network and point of sale scanner data can reduce the delay in the measurement and reporting of the sales rate for a firm products delay times can vary both exogenously and endogenously for example a crit ical parameter in a model of a firm supply chain is the average delay between placing and receiving orders for parts and materials can you consider this time to be fixed in many industries the resupply time is a variable and both exogenous and endogenous factors influence it for example the resupply time often depends on the season of the year an exogenous factor the time required to deliver fresh strawberries to market in boston is shorter in summer when local berries are in season and longer in winter when the supply line stretches to california and mexico the length of a delay often depends on the state of the system itself how long will you wait to withdraw cash from an atm if there are no people ahead of you in line the delay is the minimum time required for you to insert your card enter your code collect your cash and get your card back about a minute however figure response of higher order delays to a step input time multiples of average delay time chapter delays the length of time you must wait increases if there are people ahead of you in line in turn the rate at which people join the line depends on how many people are al ready in line when the line is long people will walk by and wait until the crowd isn t as big a behavior known as the average waiting time the delay in getting served therefore depends endogenously on the number of people in the delay the length of the queue of people awaiting service similarly the delay in receiving parts from suppliers depends on both the nor mal order processing time and on the suppliers backlog of orders relative to their capacity when suppliers have ample capacity they can deliver rapidly when ca pacity is fully utilized backorders accumulate and customers are put on allocation they receive a fraction of their order and are forced to wait longer than expected in the long run customers will seek alternative suppliers forming a negative loop that reduces the delivery delay but in the short run before new suppliers can be found and qualified customers may actually order more in an attempt to get what they really desire if your supplier tells you it can only ship part of your order this week you may order more than needed in the hope of receiving what you actually require placing such phantom orders creates a positive feedback that further in creases the supplier backlog and lengthens the delivery delay still more often leading to instability in orders production and inventory see chapter response of delays to changing delay times to develop your understanding of how the different types of delays respond to variations in the delay time answer the following questions consider a model of the post office as a third order material delay equation assume that the mailing rate is constant and that the system is in equi librium the mailing rate and delivery rate are equal without using simulation sketch the behavior you expect if the delay time suddenly and permanently in creases from days to days on day make two showing the mailing rate and delivery rate and the other showing what you expect to happen to the stock of letters in transit sketch the response you would expect if the delay time suddenly dropped from to days now consider a firm forecast of orders assume the firm uses adaptive ex pectations to forecast orders equation assume that the order rate is con stant and that the expected order rate is equal to the order rate the system is in equilibrium without using simulation sketch the behavior you expect if the time to adjust the forecast suddenly and permanently decreases from months to months after you ve sketched your intuitive estimates build the models and simu late their response to changes in the delay time was your intuition correct if there are differences in the response of the material and information delays to changes in the delay times explain why part tools for modeling dynamic systems nonlinear adjustment times modeling ratchet effects often the time constant for an information delay depends nonlinearly on the input the delay in adapting ourselves to a new situation may be longer than the delay in reacting to further instances of the stimulus once we have come to expect it peo ple get used to higher income faster than they adapt to a drop in their income we sometimes learn more rapidly than we forget because first order smoothing and all the delays discussed up to now is linear it responds symmetrically to inputs of any magnitude and to increases as well as decreases one way to model asymmet rical adjustments is with a nonlinear delay in which the time constant for the delay depends on the state of the system in the the economist james duesenberry noticed that aggregate con sumption expenditures seemed to rise faster than they fell as income fluctuated over the business cycle he hypothesized that people rapidly raised their expec tations of future income when income increased boosting their consumption quickly but were slow to give up their desired standard of living in the face of hard luck leading them to spend near the old rates even though income had fallen such ratchet effects can be modeled by assuming the time constant d takes on one value when the input to the delay x exceeds the output x and another when the input falls below the output x x where is the time constant that characterizes the adjustment when the output is increasing when x x and is the time constant governing the adjustment when the output is decreasing when x x in the case of income expectations the hypothesis suggests downward rigidity of expectations that is d sterman repenning and kofman used the nonlinear smoothing struc ture to capture the response of workers to news of layoffs in a model of process im provement improvement programs have the potential to create excess labor if productivity rises faster than the demand for the product the willingness of work ers to participate in improvement programs was hypothesized to depend on per ceived job security perceived job security depended on workers memory of past layoffs along with other factors such as excess capacity perceived job security is likely to be higher in a firm that hasn t laid off any workers for years than in one where layoffs are common the memory of past layoffs was modeled using the nonlinear delay in equation the input was the fraction of the workforce laid off in the recent past and the output was the memory of layoffs setting captured the results of our field studies showing that perceptions of job security fall swiftly on news of a layoff and take years to recover even if there is no subsequent downsizing figure the nonlinear smoothing structure also works with higher order information delays economists dating back to keynes have suggested that wages and prices might also exhibit ratchet effects rising more rapidly than they fall empirical studies are scarce some do not support the hypothesis rassekh and wilbratte chapter delays figure time coinstants structure and behavior sirnulation of response of perceivedjob in layoffs the constant for increasing the memory of d week for forgetting the history of past layoffs weeks a layoff rate weeks layoff memory memory time layoffs estimating the duration and distribution of delays the average length of a delay and the shape of the response distribution can be es timated by two principal methods statistical techniques and firsthand investigation of the process in the field estimating delays when numerical data are available a wide range of econometric and statistical tools can help you estimate the dura tion and distribution of lags from time series data when such data are available see hamilton for a review further details are provided in any good econo metrics text though you can write the output of a lag as the weighted sum of past values of the input see equation it is usually infeasible to estimate the weights di rectly due to multicollinearity and lack of data the main econometric techniques available for estimating lags from time series data include the koyck or geometric lag polynomial distributed lags rational distributed lags and arima models see section many econometric and time series statistical packages are available to estimate these models from time series data in choosing an estimation method you must trade off the flexibility of the for mulation against the number of parameters to be estimated some methods assume the shape of the response equivalent to assuming the order of the delay and esti mate the mean delay time the koyck or geometric lag for example is easily es timated but assumes the delay is first order see section other techniques such as the polynomial lag method impose fewer a priori restrictions on the shape of the lag distribution but require more data you should not constrain the shape of the delay in advance unless there is strong independent evidence to support a part iv tools for modeling dynamic systems particular lag shape or if sensitivity analysis suggests the results of interest are not contingent on the shape of the delay distribution while you may estimate the length and distribution of a delay using econo metric techniques you should not use the estimated regression equation in your simulation model instead you should replace the estimated distributed lag with the material or information delay that best matches the estimated lag there are several reasons first econometric techniques are designed for discrete time since most economic and business data are reported at regular discrete intervals such as a month quarter or year system dynamics models are usually developed for con tinuous time the time step for updating the state variables is often different from and usually shorter than the data reporting period used to estimate a delay using the continuous time delay that best matches the estimated discrete delay ensures that your model will be robust to changes in the simulation time step second re gression equations for lags have fixed lag weights implying a fixed delay time in many situations however the length of a delay is actually a variable even if the delay time is thought to be constant in the current version of your model further work may reveal that the delay time must be incorporated as an endogenous vari able the material and information delay structures used in system dynamics re spond appropriately to changes in the delay times while a regression equation for a distributed lag does not enable delay times to vary regression equations for dis tributed lags also do not distinguish between material and information delays ma terial and information delays respond differently to changes in delay times your model must properly distinguish between the two types of delay to respond appro priately to changes in delay times and to ensure conservation of material flows example the lagged response of energy supply to price delays played an important role in roger naill model of the natural gas industry exploration effort responds to changes in price but only after a consider able delay fortunately khazzoom had carefully estimated the distributed lag response of gas supply to changes in price rather than using the discrete time formulation however found that the estimated delay was approximated well by a third order delay with a year delay time figure whereas zoom treated the delay between price and supply as a single aggregate process naill model explicitly portrayed the exploration and discovery process explic itly modeling investment in exploration capital with a material delay meant could simulate the response of natural gas supply to changes in the delay between the initiation of exploration activity and its results changes that might arise from changes in exploration technology government regulations the location and depth of gas resources or the capacity of the industry supplying drill rigs example capital investment in the macroeconomy i capital investment is a major decision for any business and understanding the re sponse of investment to changes in economic conditions is critical in the formation of fiscal and monetary policy since investment takes time policy makers such as central bankers and governments must understand the length and distribution of the lags in the response of investment to changes in policy levers such as interest rates chapter delays figure approximating a discrete time lag with a continuous delay khazzoorn estiimatecl the a lagged response of natural gas supply to changes in price graph shows his of the model third order delay response to a thousand cubic feet impulse in the price of gas a third order delay matched the estiimatecl lag time elapsed in years after the impulse source p reproduced with permission taxes and the level of demand in the economy how long does it take to build new capital plant figure shows data for one part of the capital investment process the construction delay the figure shows the distribution of construction completions for private nonresidential investment projects derived by montgomery from us department of commerce survey data the surveys cover 000 construction projects from all sectors of the economy the mean delay between the start of con struction and completion is months years the data describe only the physical construction process and do not include planning and administrative de lays in the investment process the construction delay distribution is approximated extremely well by a sec ond order material delay with a month average delay time the same mean de lay as the data the low order of the delay consistent with the large variance in completion rates is due to the aggregation of many types of capital plant in the survey data data spanning all sectors of the economy delay distributions for cap ital plant at the level of particular industries or types of structures semicon ductor wafer fabs power plants office buildings would have lower variance and would require higher order delays interestingly montgomery found only small variations in the average delay across the decades and his estimate of months is very close to the month mean construction time estimated by mayer from a survey of us con struction projects the mean and distribution of construction times appears to be quite stable over the past years despite significant technical change and shifts in the composition of the economy the relatively small range of variation suggests that the construction delay and distribution can be modeled with the same structure and parameters over long time horizons example capital in the macroeconomy the roughly month average construction delay is only part of the total lag in the response of capital investment to changes in business conditions estimates of part iv tools for modeling dynamic systems figure the construction lag for capital plant data vs model data distribution of construction completion times for us private nonresidential structures as estimated by montgomery from us dept of commerce survey data the mean lag is months model order material delay with average delay time of months month the total delay between a change in say demand for a firm products and the com pletion of new capacity are much longer typically to years as they include the administrative decision making appropriations permitting design and other de lays as well as the physical construction process businesses and organizations such as the federal reserve must account for the entire delay when setting policy the lags in capital investment have been intensively explored in macro economics for more than years a common formulation known as the neo classical investment function see jorgenson hunter and nadiri presumes firms first calculate the optimal level of capital stock they desire based on traditional static profit maximization considerations then adjust the actual stock k toward the desired level d the lag operator l denotes a distributed lag with mean delay time d the optimal capital stock is calculated by assuming firms set the desired capital stock at the level that maximizes profits which in turn is a function of industry demand inter est rates taxes the marginal productivity of the capital stock and possibly other variables the distributed lag l is estimated from data on gross investment or cap ital expenditures by noting that the rate of change of the capital stock k is net in vestment that net investment is gross investment less capital discards and by assuming that discards depend on the current capital stock discards are usually as sumed to be a first order decay process with a constant average life of capital the resulting lag estimates subsume the construction delay to yield a mean and distri bution for the total delay between changes in business conditions and the response of capital investment a major problem with the neoclassical investment function is that the mean de lay and lag distribution are assumed to be fixed the time between placing an or der for new plant and equipment and receiving that capital from the supplier is assumed to be independent of the supplier capacity utilization yet when suppli ers have excess capacity the delivery time will be short while during booms when supplier capacity is fully utilized the delivery delay will increase senge found that delivery delays for capital goods varied by over the business chapter delays models where the lag distribution is fixed are misspecified because they implicitly assume suppliers of plant and equipment have unlimited or perfectly flexible production capacity a physical impossibility in general models in which delays are specified as distributed lags rather than conserved stock and flow struc tures are not robust and frequently violate basic laws of physics the system dynamics national model sdnm a macroeconomic model de veloped by the mit system dynamics group forrester forrester et al mass addressed this and other defects of the neoclassical function by explicitly representing the investment process at the operational level the model distinguishes between perception delays and material delays and captures the con served flows of orders acquisitions and discards of plant and equipment the ac quisition delay varies with the capacity utilization of the capital producing industries figure shows a simplified representation of the investment func tion instead of representing investment as a single distributed lag the model rep resents the stages of investment separately distinguishing between the planning process and the capital ordering and construction process capital stock is in creased by acquisitions and decreased by discards discards are assumed as in the neoclassical model to be a first order exponential decay process the acquisition rate depends on the backlog of orders for capital and the current delivery delay for capital the backlog of capital on order is increased by orders orders lag the rate of order starts capturing appropriation and administrative delays in investment the order start rate responds to four factors replacement of capital discards adjustment for the expected growth in demand based on past growth in ship ments the gap between the desired and actual stock of capital and the gap between the desired and actual supply line of capital on order see chapter the desired supply line depends on the perceived delivery delay for capital and the required replacement of discarded capital the desired stock of capital is pro portional to desired production but is modified by the perceived marginal return on new capital firms are assumed to respond to the profitability of a new investment but with a delay caused by the difficulty of assessing changes in the marginal pro ductivity and marginal cost of capital desired production depends on expected de mand and is then adjusted to correct discrepancies between the desired and actual levels of inventory and backlog expected demand is modeled as an information delay of shipment data the model provides an operational description of the cap ital investment process allowing the delays in the different parts of the process to be separately specified and estimated senge showed that the disequilibrium investment function used in the sdnm includes the neoclassical investment function as a special case the sdnm investment function reduces to the neoclassical function when a number of equilibrium and perfect information assumptions are made these include the assumptions that inventories backlogs and the stock of capital on order always large variation in capital delivery delays senge found does not conflict with the relatively stable distribution of construction times montgomery documented the total delivery delay includes the construction time plus any additional time an order spends in queue awaiting the start of construction during boom periods this preconstruction waiting period increases as the backlog of projects waiting for construction crews and equipment to become available builds up figure simplified causal structure of the investment function in the system dynamics national model rectangles with rounded corners denote information delays delays requiring estimation shown in italics inputs to the investment function shown in small caps link polarities not shown for clarity average time to perceive marginal return marginal return capital time to correct red inventory time to expectations from senge chapter delays equal their desired levels that firms instantly and perfectly perceive demand and the marginal productivity of capital that the delivery delay for capital is constant and that capacity utilization is always constant at the desired level senge used econometric techniques to estimate the parameters of the sdnm investment func tion while the system dynamics model relaxed the unrealistic assumptions of the neoclassical theory the added complexity of the model made econometric estima tion of the various delays much more difficult both statistically and in the creation of consistent data sets to ensure the robustness of the results senge tested eight different specifications each progressively relaxing more of the restrictive as sumptions of the neoclassical formulation senge tested the model with quarterly data for four industries durable and nondurable manufacturing electrical machinery and textile products these in dustries spanned two major levels of aggregation durable and nondurable manu facturing together account for the entire manufacturing sector of the economy while the other two industries tested the model ability to explain investment at a more disaggregate level the regression results supported the disequilibrium system dynamics function for all four industry groups the sdnm investment function explains more of the variance in the data with less autocorrelation in the residuals than the neoclassical function while yielding statistically significant plausible estimates for the model parameters the model also generates more realistic behavior when simulated than the neoclassical function table reports the estimation results for nondurable manufacturing the estimated distributions for three key lags are shown in figure these delays were estimated by the polynomial distributed lag method allowing the lag shape as well as the mean delay to be estimated rather than assumed the lag in averaging shipments to form demand forecasts was hypothesized to be first order and indeed the estimated distribution of lag weights for average shipments is approximated well by first order exponential smoothing with a delay time of about two quarters the response of investment to changes in the perceived delivery delay for capital was also expected to be first order and the estimated weights are well approximated by first order smoothing though with a much longer average delay of about years a longer delay in the response to changes in capital availability is expected delivery quotes for plant and equipment are un certain and unreliable managers must wait a substantial fraction of the normal de livery delay before they can glean reliable information on the progress of equipment orders or the rate of construction of new plant more time is required to determine how to alter investment plans to compensate for changes in lead time finally the response of growth expectations to changes in the growth rate of ship ments was hypothesized to be a higher order delay the regressions support this hypothesis the estimated lag distribution is bell shaped growth expectations do not respond significantly to short term changes in actual demand growth rates the estimated distribution is approximated reasonably well by a third order delay of the rate of change in average shipments average shipments are given by first order smoothing of actual shipments with the estimated quarter delay senge also compared the estimated delays to the a priori judgmental estimates developed by the modeling team table in some cases the a priori estimates part iv tools for modeling dynamic systems figure estimated lag distributions for investment compared to continuous time lags estimated lags found by polynomial distributed lag method with third degree polynomial consumer nondurables error bars show estimated standard error p a a quarters source senge are not statistically different from the estimated values for other parameters there is a large difference between the two these discrepancies led to reconsideration of the logic behind the judgmental estimates and the appropriateness of the data sources and estimation methods in some cases the judgmental estimates were revised in light of the estimation results or improved model formulations were chapter delays table comparison of estimated and judgmental estimates of investment delays estimated a priori estimated standard value estimate deviation of estimate parameter quarters quarters quarters tiime to averageshipments time to perceive marginal return to capital tiime to perceive the delivery delay for capital tiime to form growth expectations supply correction time 02 capital correction time to correct inventory and backlog desired inventory coverage desired coverage fractional discard rate average life of capital 033 years years estimates for the consumer nondurables sector indicates estimated and judgmental values differ by more than estimated standard deviations indicating a significant differ ence between the two estimates source pp developed in other cases the discrepancy was traced to limitations of the estima tion protocol or an imperfect match between the concept in the investment function and the data series used to proxy it and the a priori estimate was retained for sim ulation purposes estimating delays when numerical data are not available in many situations data from which to estimate the duration and shape of delays are not available in these cases you must estimate these parameters from direct in spection of the delay process experience with analogous delays in related systems or judgment judgmental estimates of aggregate delays can be quite unreliable and usually underestimate their duration recall the challenges at the start of this chapter what was your estimate of the investment delay for the manufacturing economy how about the time required for pork supplies to respond to price changes or for econ omists to update their inflation forecasts most people dramatically underestimate these and other delays the actual delays are roughly years years and year respectively senge meadows sterman the longer the delay part iv tools for modeling dynamic systems the greater the degree of underestimation what was your estimate of the delay in recognizing and reacting to air pollution as shown in table more than years have passed from the first undeniable evidence that air pollution causes significant health problems such as death yet most major metropolitan areas in the us are still not in compliance with the provisions of the clean air act decomposition is a useful strategy to minimize the underestimation of delays instead of estimating the total length of the delay decompose the process into its various stages then estimate the length of time required for each senge capital investment model decomposed the total response lag into a disaggregate opera tional model whose individual lags could be estimated judgmentally from direct table delays in societal response to air pollution in the united states widespread use of coal for industry and heating leads to growing air pollution in urban areas of europe and the united states smog in donora pennsylvania kills people and sickens soon after coal fumes kill nearly in london first us federal air pollution control act laid primary responsibility for limiting air pollution upon states and cities but allocated million for research federal clean air act recognizes air pollution does not respect state boundaries sets up regulations for control of interstate abatement provides more assistance for state and local governments clean air act strengthened by defining safe standards for co particulates volatile organic compounds no ozone and lead state plans to meet standards required by deadline postponed until as cities were in violation of the ozone standard ninety urban areas with million inhabitants exceed ozone standard violate co standard comprehensive amendments to clean air act require all cities to meet ozone standard by except los angeles which has until stricter regulations for auto emissions gasoline and many newly regulated poli tants ambient concentrations of all seven regulated pollutants dropping very slowly except lead which plummeted as soon as leaded gasoline was banned though much lead from prior emissions remains in soils medical evidence shows health problems and deaths from air pollution growing million in us live in areas violating the ozone standard epa seeks to stiffen ozone and particulate standards industry allocates millions to fight the strengthening of standards time from clear signal of problem to first meaningful law years time from first law to measurable steady improvements in air quality years time from first law to full compliance with law years and counting total delay from first clear signal to full compliance years source paraphrased and condensed with permission from d meadows and a the bulletin pp chapter delays observation of business decision making subsequent statistical estimation showed the judgmental estimates were often reasonable to decompose a delay for the purposes of judgmental estimation map the stock and flow structure of the process at the operational level for example consider the delay in the response of aggregate pork supply to price changes figure decomposition reveals the following sequence first hog farmers must decide that a rise in price is likely to persist long enough to justify investing in increasing production then they must increase their breeding stock by withholding some mature sows from market then breed the sows after the gestation delay the lit ters are born the piglets require further time to mature then spend additional time in a feedlot until they reach the optimal weight where the gain in market value from greater weight is balanced by the cost of additional feed only then are they sent to slaughter increasing the supply of pork most of these delays are biologi cally determined easily estimated and quite stable the gestation maturation and feedlot delays are about and months respectively a total material delay of about months meadows how long is the delay in adjusting producers expectations about the future price and in building up the breeding stock because it takes about a year between breeding and the resulting increase in hog supply producers cannot afford to react too quickly to price changes but must wait long enough to be confident higher prices will persist studies show forecasts of future hog prices are strongly influenced by recent prices with little weight on prices more than a year in the past bessler and brandt meadows estimated the expectation formation delay to be about months and the breeding stock ad justment delay at about months thus the total delay between a change in the price of hogs and the resulting change in hog production is about months not surprisingly such a long delay in the market feedback regulating prices leads to in stability hog prices tend to oscillate with an average periodicity of about years see chapter decomposition also gives insight into the shape of the outflow distribution for each delay the more stages in a delay the tighter the output distribution will be and the smaller the initial response the variance in the gestation process is small meadows reports of farrowings take place days after breed ing indicating a very high order delay the variance in the maturation and feedlot delays is greater than that of the gestation delay but the short run response to a pulse input is small these delays could probably be modeled adequately with a third or sixth order delay price expectations and the delay in adjusting the breed ing stock however can probably be modeled as first order processes both price expectations and the breeding stock are likely to respond quickest when the gap between the desired and actual states is greatest because the total delay cascades many distinct stages many of which have low variance the short run response of hog production to higher prices is actually the short run effect of price increases on supply is negative for the aggregate indus try the breeding stock can only be increased by withholding some sows from slaughter the first response of the slaughter rate to a rise in expected price is therefore a reduction in supply creating a positive feedback loop higher prices lead to lower short run supply and still higher prices as pro ducers send fewer sows to slaughter to increase their breeding stock a good model of the hog pro duction system must include this destabilizing loop a process that cannot be captured in models such as cobweb models that treat the supply response as an aggregate delay see chapter estimatinga delay by decomposition how long is the delay in the response of hog production to price changes mapping the stock and flow structure of the process identifies the delays of the individual activities creating the delay the individual stage delays are more reliably estimated than the aggregate total delay aggregate view after decomposition chapter delays process point walk the line even when numerical data are available direct inspection is important you should be suspicious of data in a firm information systems and take the time to investi gate the process firsthand in modeling a manufacturing process you should go to the actual plant and walk the line follow a few parts through the entire process from the time they arrive at the receiving dock until they are shipped to a customer in a service operation follow the customer and paperwork from start to finish studied the cycle times for fabrication of various parts at a major commercial aircraft manufacturer the firm order planning system was supposed to track parts and subassemblies as they flowed through the manufacturing process downloading the data for a representative sample of parts revealed that the recorded cycle times for each lot were always exactly equal to the time allot ted to take a typical example the scheduled completion time for a particular part was days data in the information system showed every lot was delivered ex actly after the order was received however other records the line and interviewing the workers showed the actual delay averaged days with a standard deviation of days only of lots examined were completed in or less obviously the start times recorded in the information system had been back calculated by subtracting the scheduled cycle time from the completion date of the lots firsthand investigation of the process on the factory floor not only yielded a better estimate of the delay but revealed significant errors and wasted effort in the information and control systems governing the operation not surprisingly the poor quality of systems and procedures kept the company from increasing produc tion smoothly and rapidly when orders surged the resulting production bottle necks extra costs and delays in deliveries to customers led to more than in extraordinary charges and a significant decline in profits just as demand reached an all time high system dynamics in action forecasting semiconductor understanding and modeling delays can often yield significant value even without the complexity of a full simulation model of the feedback structure of the business chipmaker symbios inc used simple models of delays to dramatically improve its ability to forecast demand for its integrated circuits stabilizing production sched ules improving capacity utilization and lowering production and capacity acqui sition costs symbios inc is a successful semiconductor and component manufacturer with headquarters in fort collins colorado symbios makes a full spectrum of hard ware and software for storage management and peripherals including standard and am indebted to symbios and to lyle wallis karl braitberg kevin gearhardt michael haynes and mark paich for permission to present this case their willingness to share their data and experiences and their assistance in its preparation in symbios was sold to lsi logic a chipmaker in milpitas california part iv tools for modeling dynamic systems application specific integrated circuits asics host adapters technologies and storage hardware for high performance workstations and servers their cus tomers are original equipment manufacturers oems in the computer and elec tronics industry throughout the symbios enjoyed revenue growth of about reaching about million in revenue in with employees worldwide like all chipmakers symbios is caught between the rapid growth technical change and volatility of the semiconductor market on the one hand and the high costs and long delays of adjusting manufacturing capacity on the other semi conductor wafer fabs are among the most technically sophisticated and expensive factories ever built typical fabs for asics cost about billion fabs for high per formance microprocessors cost a billion dollars more given the high fixed costs of semiconductor manufacturing consistently high utilization of a chipmaker fabs is essential for profitability however capacity can only be adjusted with long delays the delay between the decision to build a fab and the first output is several years long capacity adjustment delays mean chipmakers must be able to forecast demand reliably over quite long horizons as director of business planning and modeling for symbios lyle wallis struggled with this dilemma there is little room for error the integrated circuit market is very competitive many of symbios customers are very large and wield considerable market power over suppliers rapid technical change puts a premium on the quality and responsiveness of the firm design and engineering staff the quality standards oems require their suppliers to meet are among the most strin gent in any industry price competition is intense and perhaps most important de livery time is a critical competitive battleground because the life cycle of the products using these chips is often very short chipmakers must deliver on time as wallis commented when you underestimate demand your delivery time can stretch out from weeks to weeks which seems like infinity to your customers but at the same time you can t afford to hold excess capacity that is likely to go unutilized symbios like most chipmakers continuously developed and revised bottoms up forecasts of production requirements and revenues these customer demand fore casts were developed by collecting the customers own projections of de livery requirements by line item for the next four quarters a symbios manager described the process the cdf process begins with a symbios sales representative visiting a cus tomer to obtain the customer demand projections after obtaining the customer forecast the sales representative reviews the forecast with a symbios sales man ager together the sales manager and sales representative determine the likelihood makes about of its chips in its own fab like many firms in the industry symbios uses outside foundries to handle demand peaks and to produce some small volume older products while outsourcing to external foundries provides some flexibility there are still substan tial delays between the decision to outsource and the delivery of product and the less predictable the need for outsourcing the more expensive it is to locate qualify and contract with external foundries chapter delays figure actual billings to and month customer demand forecasts the graph shows the cdfs prepared in t h plotted actual billings for montlh t h is the forecast orizon of months if the forecasts were accurate would correspond exactly to actual billings all three series are month centered moving averages to filter out monthly fluctuations month that the customer will hit their projections if they have concerns that the customer may be over or underforecasting they enter a confidence factor into the forecast or adjust the projection the forecast is then submitted to symbios marketing man agers and product marketing engineers after reviewing the forecasts the marketing managers and engineers also have the opportunity to adjust the confidence factor and alter the customer forecast once reviewed by marketing the forecast be comes available for use company wide the rationale for the bottoms up forecasts was for many years unquestioned throughout the organization the demand forecast information came directly from the customers who should know their own requirements best using customer re quirements forecasts creates a strong channel of communication between the oems and chipmakers and demonstrates to the customers that the suppliers are lis tening and responding to their needs yet the more wallis examined the accuracy of the bottoms up cdfs the more concerned he became figure shows the and month revenue forecasts based on the cus tomer demand forecasts against actual billings the forecasts of future sales pre pared in month t h are plotted at time t against actual billings for month t h is the forecast horizon of or months the data are month centered moving averages to filter out high frequency noise if the forecasts were accurate the forecast and actual billings curves would be identical wallis immediately noticed several fea tures of the forecasts first the bottoms up forecasts are not very accurate the mean absolute percent error is for the month forecasts and for the month second the forecasts correlate poorly with actual billings the forecasts tend to move out of phase with actual billings that is they tend to be high when billings are low and vice versa third the forecasts are consistently too high some of the bias reflects overoptimistic forecasts of consumer demand by the oems some reflects each oem effort to ensure receipt of sufficient output by padding orders to the supplier and then canceling later if necessary fourth the forecasts are extremely volatile the forecasts fluctuate significantly around the mean absolute percent error mape is defined as mape part iv tools for modeling dynamic systems figure six and month customer demand forecasts compared to actual bookings the forecasts are plotted at the time they were made forecasts of future demand are highly correlated with current orders all three series are month month centered moving averages to filter out monthly fluctuations growth trend with much greater variance than actual production chasing the fluc tuations in forecasts caused costly errors in production planning and capacity acquisition figure compares the and month forecasts against current the order rate here the forecasts of future sales have been plotted at the date the forecasts were made to show the relationship between current bookings and the current beliefs of customers about future demand both the and month cus tomer forecasts are highly correlated with current customer orders the correlation between and the customer demand forecasts is about for both fore cast horizons the correlation between the two forecasts is closely you can see a lag of several months between the peak in actual and the peaks in the forecast customers appear to project their future requirements by ex trapolating their recent actual orders the lag arises from short term smoothing of recent orders and administrative delays in preparing the forecasts wallis concluded that the customer demand forecasts responded strongly to re cent events particularly the current demand requirements of the customers and contained little useful information on future requirements when customers need more product right now their and month forecasts increase sharply when they need less right now their forecasts of future needs drop consequently short term inventory and supply line adjustments find their way into forecasts of future demand even though these temporary influences on orders usually have little bear ing on demand or months out the errors and volatility of the bottoms up forecasts caused symbios to make frequent and costly changes in production schedules and capacity eating up profits and crimping the competitiveness of the business further because the customers mrp and production planning systems reacted to the availability of the products from suppliers forecast volatility was reinforcing fluctuating demand meant products would sometimes be placed on allocation stretching out delivery schedules during periods of allocation cus tomers mrp systems and production planners responded by seeking to hold greater safety stocks and ordering farther ahead forecasting still greater future re quirements and leading symbios to add capacity once adequate capacity came on line and the product went off allocation orders fell as customers responded to the chapter delays ready availability of the product by canceling their defensive orders leading to ex cess capacity as orders fell so too did forecasts of future requirements causing symbios to cut production plans and capacity acquisition and setting up the next cycle of inadequate capacity allocations and surging orders finally producing and updating the bottoms up forecasts took too long and cost too much it took too long to get the data from the customers and ate up a lot of symbios sales and management time the data frequently contained errors and inconsistencies that took further time to work out wallis ruefully concluded using a bottoms up forecast is worse than nothing for sizing the business the poor performance of the bottoms up forecasts was well known throughout the organization wallis noted the past reaction to each forecasting failure in each case we would find something to blame usually we blamed the sales force for not being able to forecast so we would have the marketing groups do the work then we would switch back to sales after some time or we blamed the software system and changed that he concluded that there were deeper structural reasons for the repeated failure of the bottoms up forecasts my position is that structurally each of these systems was similar and that each pro duced similar results they always take the current situation and project it into the forecast horizon in such a situation even normal seasonal fluctuation causes real problems we have looked at the data produced by sales versus marketing versus different business units and can find no real difference in behavior and he recognized that he had not been immune to these problems himself in fact i went back and looked at my forecasting data for when i was a sales man ager and when i was a business unit director same behavior despite the strong evidence of the failures of the bottoms up forecasts many in the organization not to mention the customers were strongly committed to the cur rent forecasting process and didn t believe the analysis as a very customer orientedcompany the customers forecast is a very compelling input even if intellectually one knows that the customers forecasting process is flawed first it makes some sense that the customers should know their business second they are in your face demanding whatever it is that they think they want whether it makes sense or not and selective memory is pervasive if they fall short of their forecast it is forgotten but miss their forecastjust once and there is hell to pay customers appear to believe that their forecasts are pretty good in spite of evidence to the contrary wallis knew that you can t beat something with nothing pointing out the problems caused by the current system without proposing a better alternative would only cre ate anger and frustration but what were the alternatives one possibility was to use simple trend pro jection based on actual aggregate billings simple extrapolation is fast and cheap and yielded reasonable results at the aggregate level however extrapolative methods didn t provide enough detail to plan production or capacity at the level of particular production lines or product families and many in the company didn t part iv tools for modeling dynamic systems believe trend projections could be trusted because they didn t take the customers own requirements forecasts into account another possibility was to use econometric forecasts of semiconductor indus try demand by segment many market research and consulting organizations sell such forecasts however the industry forecasts often even usually miss the turn ing points in the industry further the turning points in the demand for symbios products didn t always coincide with the segment turning points finally econo metric models had poor forecasting accuracy at the longer time horizons needed to plan capacity having played the beer distribution game sterman chap wallis recognized that much of the volatility in orders and customer demand forecasts symbios experienced was endogenously generated by feedbacks among the mem bers of the semiconductor industry supply chain figure each firm in the chain following its own self interest orders to meet anticipated customer demand and adjusts its inventories and backlogs of parts on order to ensure a steady supply of deliveries from suppliers the result is powerful amplification of demand fluc tuations from one level of the distribution chain to the next causing instability for all players in the industry as a chipmaker symbios held the tail position in the supply chain and experienced more volatility in demand than those downstream for symbios to temper the wild swings in orders caused by the amplification of in ventory and supply line adjustments up the supply chain it could no longer base capacity plans on forecasts derived from past actual order rates with the help of mark paich an experienced modeler wallis began to develop a system dynamics model wallis also recruited two symbios managers karl braitberg and kevin gearhardt from line positions into the team they began by mapping the stock and flow structure that generated demand and quickly deter mined that the key to improved forecasts was the link between design wins and customer orders a design win occurs when a customer commits to using a specific symbios chip in a specific product design wins are the focus of the sales force figure semiconductor industry supply chain as in the beer distribution game each layer in the distribution chain amplifies fluctuations in final demand until orders for and production of semiconductors fluctuate significantly semiconductors products products orders orders and cancellations and cancellations final demand chapter delays efforts to generate business sales people must persuade oems to use a symbios chip in their products the progression of asic design wins from customer commitment to produc tion is shown in figure an application specific chip cannot go into pro duction until the detailed design is developed prototypes are tested and the chip specific tooling in the fab is developed and tested hence new design wins accumulate in a stock of designs in development as designs are completed and reviewed prototyping starts after successful test and concurrent development and test of the fabrication process production begins production volume and revenue depend on the number of product designs in production and average sell ing prices the stock of designs in production decreases as the designs reach the end of their useful life and are discontinued the conceptual model shown in figure was developed very rapidly it was readily apparent that there were long delays in the design and prototyping process the long delays meant today revenue derived from past design wins so knowledge of the recent wins in the development and prototyping pipeline should provide a better forecast of future build requirements and revenue the next step was to convert the conceptual model shown in figure into an operational calibrated model the team quickly realized that an aggregate model the total number of design wins was not sufficient since different design wins generated very different production volumes and prices however previous attempts to forecast demand based on individual design wins had not been successful the delay between any particular design win and volume produc tion is quite variable as the product development time depends on the complexity figure the stock and flow structure of design wins volume and revenue current volume and revenue depend on past design wins due to development and prototyping delays the delays in each stage are high order processes average average development prototyping time time average product lifetime volume per design win average revenue selling price part iv tools for modeling dynamic systems of the particular chip the stability of customer specifications and other attributes of the product development process the sales organization routinely estimated the volume predicted to flow from each design win but the manufacturing and mar keting groups considered these projections unreliable indeed some design wins never resulted in volume production for example when the oem canceled the product before production began others became wild successes beyond the hopes of the customer the modeling team settled on an intermediate level of aggregation knowl edge of the volumes generated by individual design wins was not necessary pro duction planners needed to know how many wafers to start and how many units to build at the level of each production technology such as a line producing inch wafers with a micron line width and a certain number of layers sales and marketing needed to know likely future sales at the level of product families so they could allocate resources and set sales goals and sales force compensation the modeling team disaggregated the model at the level of each product family and process technology the delays volumes and revenues for are radically dif ferent from those for standard products and there are differences among the dif ferent standard product families disaggregation allowed the model to generate information needed by key organizational constituencies in a form they could use by providing forecasts of volume and revenue at the product family and process technology level the simple stock and flow structure in figure is appropriate if the vol ume and average price associated with each design win are the same in reality volume and prices change over time with changes in the product mix technology and market conditions the volume and revenue generated by design wins cur rently in production could be quite different from the volume and revenue antici pated from design wins farther upstream in the process to model this variability they disaggregated the model further by adding parallel stock and flow structures known as coflows to track the projected volumes and revenues associated with each design win figure coflow structures keep track of various attributes of the units in a stock and flow network see chapter each design win adds one design to the stock of designs under development and adds a certain expected volume to the stock of anticipated production volume from designs under development the ratio of the anticipated volume from designs under development to the total number of designs under development is the aver age volume expected from each chip currently in the design phase when the de sign moves from development to prototyping the average expected volume associated with the designs under development also moves into the parallel stock for the expected production volume of designs in prototyping when the design moves into production the production volume expected the designs in typing also moves into the stock of anticipated volume from designs in production the operational model included an additional coflow structure to track the revenue expected from each design win production and capacity planners could use the volume projection to plan wafer starts by applying the expected yield and wafer size to volume requirements and senior management could use the revenue pro jections to set budgets and generate pro forma financial statements part iv tools for modeling dynamic systems calibrating the model required estimating the length and distribution of the development and prototyping delays for each process technology and estimat ing the expected volume wafer requirements and price for design wins in each product category generating and collecting the data was a major challenge the team needed current data on design wins and their attributes fortunately in the sales or ganization had launched a new program in which sales representatives recorded the characteristics of each win at the time of the sale the database tracked the prod uct customer anticipated volumes average selling prices and other attributes for the next years these data were used by the sales organization to determine sales goals and compensation but were considered unreliable and unstable by the manu facturing and marketing organizations the estimates of price and volume recorded in the sales force design win data base could not be used in raw form because they didn t account for subsequent or der cancellations changes in requirements and changes in prices but the modeling team realized that the sales force database didn t have to be accurate as long as the relationships between projected and realized volume and revenue were stable the team then assembled the production histories of each product from data collected by the manufacturing organization cross tabulating the sales organiza tion design win database against the actual production histories enabled the team to assess the accuracy of the sales database regressions showed fairly stable rela tionships between the volume and revenue projections recorded at the time a con tract was won and the actual realized volumes and revenues when the chips were actually made these relationships were used to calibrate the model combining the sales force design win data with the design and production histories for each product also allowed the modelers to estimate the length and distribution of the de lays by product and process category since the design and prototyping processes are themselves composed of multiple stages product definition design layout wafer fabrication sorting prototype assembly testing etc the team ex pected the delays to be very high order but not pipeline delays as there is con siderable variation in processing times for each step while the delays for any particular design win were unpredictable the modeling team found that the distri bution of the delay outflows in each category were approximated quite well by var ious high order material delays generally between ninth and twentieth order a reflection of the many stages subsumed in the design and prototyping processes and the disaggregation of the model by process technology figure shows the overall response of the estimated model for to a single design win a unit pulse because custom chips have a long design time there is no re sponse at all for roughly a year production volumes then build rapidly to a peak roughly years after the design win before gradually tailing off as product de mand falls the calibrated design win model generated more accurate forecasts than the bottoms up procedure figure compares the revenue projection of the design win model to actual billings for product line a the model tracks actual billings more accurately than the bottoms up forecasts the model also captures key shifts in revenue growth such as the decline in sales between months and and the recovery in sales beginning around month chapter delays figure estimated reveinue impact of a design win for the curve shows the distrilbution of revenue over given a from a single design win actual and revenue design win model for product line a the projection is into revenue derived frorn prior year current year assumed future design wins o ao 96 month note that the model is far from perfect actual revenues fluctuate around the projection these errors caused intense discussion within the modeling team they found that some of the errors arose from variations in volume and prices generated by the occasional very large or very small design win or other unpredictable events however after careful review of the data and some additional modeling the modeling team ultimately concluded that much of the unexplained variation in production and billings was caused by fluctuations in overall industry demand around the long term growth trend most of these excursions are themselves caused by industrywide supply chain volatility situations where customers over react to short term inventory and lead time variations wallis commented we do not attempt to capture these effects with the design win model instead we think of the model results as representing the longer term growth trends for the busi ness forecasts based on design wins help symbios damp out temporary over reactions in customer orders tempering expensive swings in production and capacity acquisition the design win approach also provided insight into the drivers of growth for the business figure breaks the projected revenue stream into revenues from projected design wins from current year design wins and from earlier de sign wins products in line a have short win to production and lifetimes design wins already in the pipeline will support sales for only about years after just part iv tools for modeling dynamic systems months about a third of projected revenues are assumed to derive from design wins yet to be won the long term revenue forecast for line a is therefore highly sensitive to the assumed rate of future design wins for product line a the bound ary of the model might usefully be extended so design wins can be related to the size and experience of the sales force their compensation incentives and the rela tive attractiveness of the products more importantly the drivers of design wins emerge as a key leverage point for growth as wallis pointed out understanding these product design win and life cycle characteristics allows our top management team to balance the required investments in technology produc tion capacity and sales effectiveness against the likely timing and magnitude of the returns on those investments prior to development of the design win model ations of proposed investments generally underestimated the delays between invest ment and results without a model estimates of the total life of investment returns are generally too short causing total investment return to be underestimated the calibrated disaggregate design win model gained broad though not universal acceptance in the company it is used to generate a rolling estimate of future vol ume requirements and revenue to manage production and plan inventories the model is also used as a key input to the annual planning process to long term cap ital planning and as part of the product development planning cycle consistent with the experience of others the modeling team found that ab stract description and conceptual models did not change the or behavior of key decision makers in the organization rather the mental models and behav ior of the managers responsible for production planning and capacity acquisition changed only when they actively worked with the model to address important is sues the modeling team worked hard to involve current and future line managers in the development and testing of the model some members of the modeling team were subsequently promoted into line positions where they used the model to help them in production planning and inventory management karl braitberg who became manager for standard products commented on the business impact of the model we used to manage inventory by gut feel for example people would say this is a hot product so we better build a hundred thousand of them now they d base this on sample requests or other unreliable information coming from customers now we make better production scheduling decisions and time inventory builds better by the delays between design wins and volume demand into account customer service has improved our ability to meet customer requested delivery dates has im proved from about to about and on time delivery to our commit date is while inventory days on hand are at a year low all the inventory metrics have improved we have a better mix of product in stock and in wip work in process inventory now we build the right inventory at the right time it is important to recognize that the model does not replace other considerations in the production planning and capacity decision managers do not slavishly follow the model output nor should they rather the model informs the viewpoint of key participants in discussions about production and capacity planning providing a sanity check on the customers claimed requirements short term event oriented chapter delays still exists as wallis commented for some people when a customer calls and screams at them they don t care what your model says but the model helps temper the reaction to such pressure helping to stabilize the operation raise average utilization and increase delivery reliability all of which generate benefits for the customers as well as for symbios wallis recalled that in the old days during revenue shortfalls we d beat up the marketing and sales groups to go out and get some more design wins to fill the revenue hole next quarter of course this never worked because of the long delays and probably caused more instability the model helps people understand these dynamics stabilizing the business increasing the efficiency of the organization and boosting our growth while the model provides better forecasts of production volumes and revenues than the bottoms up approach modeling team members are careful to note its lim itations and monitor its accuracy as the firm and industry continue to evolve the model treats design wins as a continuous flow most of the time the con tinuous flow assumption works well however occasional extremely large indi vidual design wins violate the continuous flow assumption and must be handled separately they are added exogenously similarly some customer products fail leading to cancellation of orders at various points in the process finally as some customers are bought by others the resulting consolidation of oem product lines can affect the volume and revenue generated by design wins already in the pipeline these issues are growing in importance the computer industry in the became significantly more concentrated through mergers and acquisitions as the industry consolidates the number of design wins per year falls while their average size grows and forecasting becomes more difficult for all methods not just the design win model internal changes at symbios also mean the model must be continually updated the processes underlying the delay distributions estimated from the data change as the product mix and process technology change and as im provement programs shorten product development times for these reasons the model and all models can never be considered fin ished models are always works in progress and the model users must constantly ask whether the assumptionsof the model are still reasonable as conditions change sustainedimplementation success depends on creating an ongoing process of mod eling rather than a single model no matter how accurate or comprehensive rester team members now continuously track their forecasting record and compare it to the accuracy of the other forecasts analysis of their errors generates important insight into model limitations and helps them to calibrate and improve the model they continue to develop the model in concert with the needs and par ticipation of the people responsible for production planning capacity acquisition and strategy to help ensure that the model continues to be understood and used modeling efforts underway at the time this is written explicitly address the un certainty caused by lumpy design wins through the development of a monte carlo version of the model which will generate the range of uncertainty as well as the ex pected trajectory of volume and billings the modeling team is with key customers to develop models to reduce further the volatility of orders and improve delivery performance as one customer said we give you the worst information we have and then wonder why we have a problem part iv tools for modeling dynamic systems as often found in modeling projects the greatest insight into the structure and behavior of the business came when the model results were wrong yet because many organizations punish those who make mistakes mistakes are often hidden denying the organization the opportunity to learn from experience the modeling process has helped symbios overcome the natural tendency to find the people responsible for errors and blame them senior management is now more likely to interpret a forecasting failure as an opportunity to deepen their understanding of the business and less as an occasion to blame a bad outcome on sales marketing or customers 7 mathematics of delays lags and erlang distributions this section presents the mathematics of the basic delay types in continuous and discrete time system dynamics models typically treat time as continuous how ever the discrete time formulations are useful because the data from which delays are estimated are usually reported at discrete intervals in the following i assume the delay time is constant so the analysis applies equally to material and informa tion delays the assumption of constant delay times also allows delays to be treated as linear operators note that if the delay time is endogenous for example when the delay process is capacitated the delay time will in general be a nonlinear function of the history of the input 7 general formulation for delays the shape of the response of a delay to a unit pulse can be interpreted as the prob ability distribution of the outflow rate analogous to the delivery distribution of let ters following a mass mailing in discrete time the output of a delay at time t is a weighted sum of all past values of the input up to the present time where the lag weights w are the probabilities of exiting the delay in any time period and must sum to unity that is i o the constraint that the weights sum to unity ensures the conservation of material through the delay if the weights summed to less than one the quantity exiting the delay would be less than the quantity added to it if the weights totaled more than one more would leave the delay than entered violating the conservation principle in information delays weights summing to one ensure the equilibrium output equals the input giving an unbiased perception of the input chapter delays taking the limit of the discrete time formulation as the time interval between periods shrinks to zero yields the continuous time formulation the output is the integral of past values of the input weighted by the probability of delivery at time t where the probability of delivery time units after entering the delay is given by a continuous distribution in principle the pattern of weights the probability distribution of exiting the delay is arbitrary subject to the constraint that the input to the delay is conserved that the weights are nonnegative and sum to unity however in practice only a few patterns are reasonable and realistic at the instant a quantity of material is in jected into a delay the output has not yet had any time to respond so the probabil ity of exit at time in discrete time the weight on the current value of the input the output of all delays must approach zero after a sufficiently long time has passed that is once the items are deliveredthe exit rate must fall to zero therefore thus the probability of exiting a delay its response to a unit pulse must start at zero rise to a maximum then fall to zero it is reasonable to assume the exit distribution is smooth and that the dis tribution has a single maximum if the data suggest the output distribution of a delay has more than one peak it is almost certain the total output is the result of two different delays operating in parallel and you should model each delay sepa rately within these constraints there are two main types of responses a delay in which the output responds immediately after a pulse input then gradually declines and a delay in which there is no response for some period of time followed by a gradual increase peak and decline the first order delay models the former case and the higher order delays provide a flexible family of distributionsto model the latter case can also be derived by applying the convolution theorem of linear systems theory in general the response of any linear system to an arbitrary input can be expressed as the convolution of the input with the pulse response of the system that is the product of the input with the lagged pulse response of the system where is the response of the system to a unit impulse in general the pulse response of a linear system can take on negative as well as positive values and its integral need not be unity or even finite in the case of delays the pulse response must be nonnegative and conservation of material requires the integral of the response to equal unity so we may treat as a probability distribution the convolution integral can be derived by rewriting equation as and taking the limit as the interval between time periods becomes infinitely small further details can be found in control theory texts such as ogata or and 464 part iv tools for modeling dynamic systems 7 first order delay the first order delay assumes the contents of the stock of material in transit are perfectly mixed at all times perfect mixing randomizes the order of exit from the delay implying some items stay in the stock of material in transit longer than the average delay time and some stay for a shorter period since the first order delay is equivalent to the first order linear negative feedback system its characteristic re sponse to a pulse input is exponential decay that is the probability of exiting the first order material delay is given by the exponential distribution where the mean of the distribution is the average delay d what is the average delay or mean residence time for items in the delay is it in fact the delay time parameter d by the mean value theorem of calculus the average residence time for any delay process is the time weighted average of the outflow rate given a unit pulse input at time zero t t note that the time weighted average of the outflow from a unit pulse is the same as the time weighted mean of the outflow probability distribution for the case of a first order material delay the outflow is the stock of material in transit s di vided by the average delay time d immediately after a unit pulse input the initial value of the stock of material in transit is unity since the first order material delay is the linear first order negative feedback system the stock in transit then decays exponentially with time con stant d therefore the mean residence time is given by t note that in the latter expression is precisely the exponential prob ability distribution integrating by parts which confirms that the mean residence time is in fact given by the delay time parameter d in discrete time the weights for a first order delay decline geometrically by a fixed proportion over time chapter delays where the lag weight parameter l l is related to the average delay time d l the discrete time geometric lag formulationis quite common in econometricmod eling where it is also known as a koyck lag after koyck who showed how the lag parameter l can be estimated see any good econometrics text for details 7 higher order delays cascading n first order delays together in series creates the higher order delays discussed above mathematically the output of the nth order delay is the convolu tion of the sequence of first order delays each with identical delay times equal to the total delay time d divided by the number of stages in the delay in continuous time the higher order delays are equivalent to the erlang family of distributions named after the danish telephone pioneer known as the father of queuing theory the erlang distribution of order n is given by you can use the mean value theorem to check that the mean residence time of the nth order delay is in fact given by d the erlang distributionreduces to the first order exponential distribution for n in discrete time the higher order delays are also known as pascal lags given by the distribution i n l i i where again the mean delay is l l just as the first order erlang lag is equiv alent to the exponentialdistribution the pascal lag reduces to the geometric lag for if sufficient data are available the outflow distribution can be plotted and di rectly compared to the erlang family to see if it is a good model of the delay process and to select the appropriate order of the delay see section some times however only summary statisticssuch as the sample mean and variance are available while the data for the full distribution are not in this case the order of the delay can still be estimated subject only to the assumption that the lag is well approximated by a member of the erlang family the variance of the nth order erlang lag is given by consistent with intuition and the simulationre sults above the smaller the variance relative to the mean delay the higher the order of the delay approximating the mean delay and variance of the outflow from their sample values denoted d and respectively yields a simple estimator for the order of the delay n part iv tools for modeling dynamic systems where is the integer function of course the ratio won t in general be an integer but rounding to the nearest integer generally introduces little error com pared to the likely errors in the data remember however that this esti mator presumes the delay distribution is a member of the erlang family departures from the erlang distribution will yield poor estimates information about the order of a delay gleaned from fieldwork should be used to check estimates derived from equation 7 relation of material and information delays as seen above the outputs of material and information delays with equal delay times are identical provided the delay time remains fixed to see why consider the equation for the first order information delay with input i and output dt i the output of the delay the stock has a single net rate determined by the gap be tween the input and output the net rate can be disaggregated into explicit increase and decrease rates equation is equivalent to a first order material delay with inflow out flow and stock in transit as long as the delay time remains fixed the be havior of the two delays is identical however in a material delay the output is the exit rate from the stock while in the information delay the output is the stock changing the delay time causes the behavior of the two delays to differ even though their response under constant delay times is the same modelers must be careful to use the proper type of delays a delay time currently thought of as fixed may become variable as a model is developed summary this chapter discussed delays and showed how they can be modeled first all de lays include at least one stock second delays in material flow networks must be distinguished from delays in information feedback channels material flows are conserved while information is not the difference affects how the two types of delays respond to changes in the delay time every delay has two main characteristics the mean delay time and the distrib ution of the output of the delay around that average the chapter developed a fam ily of formulations for material and information delays enabling modelers to capture a wide range of plausible delivery distributions first order delays are characterized by an exponentially declining output in response to a pulse input the largest response occurs immediately after the pulse input the response of higher order delays formed by cascading first order delays in series is initially zero builds to a maximum and then dies away pipeline delays preserve the order of entry to a delay so the output is exactly the same as the input but shifted by the time delay the first order delay assumes the contents of the stock of material in chapter delays transit are perfectly mixed at all times so the outflow is independent of the order of entry the higher the order of the delay the less mixing is assumed pipeline de lays assume no mixing of the contents of the stock in transit at all the higher the order of the delay the lower the variance in the distribution of the output finally the chapter discussed how the length and output distribution of delays can be estimated when numerical data are available econometric tools can help estimate delay durations and distributions when numerical data are not available estimation by direct inspection of the relevant process can yield good estimates judgmental estimates are more accurate when you decompose the delay into its constituent steps and estimatethe delays of each separately you should use multi ple sources of information to help you specify delays and other model parameters and inspect the process firsthand whenever possible and aging chains mathematical demography is concerned with commonsense questions about for instance the effect of a lowered death rate on the proportion of old people or the effect of abortions on the birth rate the answers that it reaches are not always commonsense and we will meet instances in which intuition has to be adjusted to accord with what the mathematics shows to be the case even when the intuitive answer gives the right direction of an effect technical analysis is still needed to estimate its amount we may see intuitively that the dropfrom an increasing to a stationary population will slow the promotion for the average person in a factory or but nothing short of an integral equation show that each drop of percent in the rate of increase will delaypromotion to middle level positions by years nathan keyfitz applied mathematical demography p the stock and flow structures described in previous chapters keep track of the quantities flowing through various stages of a system often however modelers must not only capture the total quantity of material in a stock and flow network but also various attributes of the items in the network these attributes might include the average or experience of a workforce the quality of materials or the en ergy and labor requirements of a firm machines coflows are used to account for the attributes of items flowing through a stock and flow network the outflow rates of items from a stock often depend strongly on the age of the items human mor tality rates depend on age the rate at which people discard and replace their auto mobiles depends on the age of their cars machine breakdowns in a plant depend on the time since the machines were last overhauled and the probability ex convicts are re arrested depends on the time since their release aging chains are used to represent situations where the mortality rates of items in a stock and flow part iv tools for modeling dynamic systems structure are age dependent and allow you to model changes in the age structure of any stock this chapter shows how such situations can be modeled and provides examples including global population growth organizational aging on the job learning technical change aging chains the stock and flow structure of systems is a critical determinant of their dynamics and often there are significant delays between the inflow of material to a process and the outflow in material delays described in section the flow of material through the delay is conserved material enters the delay progresses through a number of intermediate stages and finally exits there are neither additions to nor losses from the intermediate stages every item that enters eventually exits and no new items can enter other than at the start of the delay in many situations there are additional inflows and outflows to the intermediate stages in these cases an aging chain is used to model the stock and flow structure of the system imagine the labor force at a firm since it takes time for new hires to become fully ex perienced and productive the modeler may choose to disaggregate the total stock of employees into two categories employees and experienced employees an important aspect of the structure is the delay in the assimilation of rookies however this situation cannot be modeled with a second order material delay be cause the firm can hire both and experienced people and both rookies and experienced employees can quit or be fired there are inflows and outflows to each of the stocks in the chain figure general structure of aging chains an aging chain can have any number of stocks called cohorts and each cohort can have any number of inflows or outflows figure shows the general struc ture for an aging chain the total stock is divided into n cohorts each with an figure example of an aging chain rookie employees rookie hire rookie quit rate rate assimilation rate experienced employees experienced experienced hire rate quit rate chapter coflows and aging chains inflow and outflow material in cohort moves to cohort i through the transition rate i t i general structure of an aging chain part iv tools for modeling dynamic systems there is no transition rate into the first cohort and no transition rate out of the last cohort and n in general the transition rates can be either positive or negative a negative transition rate means items flow from cohort to cohort i usually however the transition rates are formulated as a delay and most often as a first order process i where is the number of years per cohort the average residence time before exiting via maturation the average residence time for items in each cohort can differ from cohort to cohort recall from chapter that a first order outflow from a stock implies that the contents of the stock are perfectly mixed so that the probability a particular item exits is independent of when it entered the stock just as in higher order material delays the overall behavior of an aging chain with n cohorts will be similar to the nth order material delay the number of cohorts can be increased until the assumption of perfect mixing within each cohort becomes a reasonable approximation see section a formulation that pre serves the exact order of entry to each cohort the outflow rates can be formulated in a variety of ways often however the outflow represents the death rate that is the exit rate from the stock and is for mulated as where fdr is the fractional death rate for cohort aging chains can be applied to any population in which the probability of ex iting the population depends on the age of the items in the population besides the aging and mortality of a population examples include the failure of machines in a factory as a function of the time since the last maintenance activity default and re payment rates for loans of different ages the rate of divorce as a function of mar riage duration or the likelihood of re arrest following parole example population and infrastructure in urban dynamics forrester urban dynamics model includes aging chains for three key components of a city the stock of commercial structures the housing stock and the population figure forrester divided the total stock of commercial struc tures into three categories new enterprise mature business and declining in dustry the stock of new enterprise is increased by new enterprise construction the transition rate of new enterprise to mature business is the new enterprise decline rate mature businesses age into the stock of declining industry through the mature business decline rate finally the stock of buildings in the declining industry cohort falls through the declining industry demolition rate forrester chose to assume that all new construction adds to the new enterprise cohort there are no inflows to the mature or declining industry stocks other than the aging rates from the prior cohort he also assumed that the demolition rate for new and ma ture businesses was small enough to ignore so the only outflow from the aging chain is the declining industry demolition rate the aging chain for commercial figure aging chains for businesses housing and labor in urban dynamics industry mature declining new new mature declining enterprise construction premium housing enterprise decline business decline worker housing construction worker industry demolition low cost housing program premium premium housing worker housing housing obsolescence housing demolition from forrester p 474 part iv tools for modeling dynamic systems structures is therefore equivalent to a third order material delay though the life times of each cohort are not equal and vary with changes in economic and social conditions in the simulated city the housing stock chain is more complex forrester divided the total housing stock into three categories premium housing worker housing and underem ployed housing that correspond to the three types of people represented in the model managerial professional workers labor and the underemployed new housing of each type can be built as each type of housing ages it is gradually converted into housing for the next type of worker for example in many cities large victorian houses once occupied by the professional class were later divided into two or three family apartments occupied by the class as middle class apartment blocks in the bronx aged and deteriorated many became tenements primarily occupied by the underemployed each of the three population classes included a net birth rate births less deaths an inmigration rate and an outmigration rate the underemployed could by gaining jobs and experience move into the worker class and workers could ad vance into the managerial professional class workers could also sink into under employment as an initial model of urban problems and policies forrester deliberately kept the model as simple as possible not all possible flows in the aging chains are rep resented and the model is no more disaggregated than necessary for the purpose for example forrester ignored possible downward mobility of the professional class and did not explicitly represent the age structure of the population within each class of worker in representing the infrastructure of the city forrester as sumed that structures built for businesses could not be converted to housing and vice versa and that old decayed housing could not be rehabilitated into premium housing the experience of the past years shows that some of the excluded flows did become important in many cities a great deal of old industrial space was converted to housing lofts in and brooklyn gentrification rehabilitated much of the older housing stock and many new businesses were created in peo ple garages and spare bedrooms these flows could easily be added to the aging chains in the model for example homer adapted the urban dy namics model to study insurance redlining the modified model explicitly ac counts for gentrification and rehabilitation of older housing along with arson for profit mass and schroeder sweeney and alfeld present a number of extensions and elaborations of the original model generally showing that the policy recommendations of the original model were robust to major changes in the model boundary and level of aggregation see also alfeld and graham example the population pyramid and the demographic transition a common use for aging chains is capturing the age structure of populations es pecially for long lived species like humans young and old behave differently and for many purposes cannot be aggregated into a single stock chapter described chapter coflows and aging chains the simplest demographic model where the number of people are aggregated into a single stock with birth and death rates proportional to the total population population deaths births fractional birth rate population deaths fractional death rate population in this first order structure thosejust born can immediatelyreproduce and arejust as likely to die as the oldest members of the population for most real populations these are poor assumptions for humans mortality rates the fractional death rate are strongly age dependent mortality is high in the first years low from childhood through the end of middle age then rises with age the childbearing years are roughly between and and fertility is not uniform during this interval vari ations in the population growth rate alter the age structure and affect its overall behavior for example a nation with high life expectancy can have a higher frac tional death rate than one with a lower life expectancy if population in the nation with low life expectancy is growing rapidly then a much larger proportion of the population will be young reducing the total number of deaths per year per person despite lower average life expectancy the lag between birth and reproduction can induce fluctuations in the age structure modeling the effects of phenomena such as the baby boom of the includingthe extra demand placed on schools the job market and in coming decades the retirement system requires a model that distinguishes between age groups demographers often represent the age structure of a population by the popula tion pyramid a graph showing the number of people in each age group by sex figure the age structures for the world as a whole and for many develop ing nations such as nigeria resemble pyramids because rapid population growth means there are many more young people than old the age structures of the de veloped nations where growth rates have been low for a generation or more are more uniform though it is still quite easy to see the variations in cohort size caused by phenomena such as the great depression world war and the postwar baby boom in the us for example the depression and war cut birth rates so the cohorts born between and are much smaller than the baby boom cohorts born in the even after accounting for normal mortality the echo of the baby boom generation the large cohorts of year olds in the figure representing the chil dren of the baby boomers is also clearly visible in the us age structure for a number of developed nations including japan fertility has been below replace ment rates for some time so the youngest cohorts are smaller than those in the prime childbearing years to model these issues the total population can be represented by an aging chain in which the population is divided into n cohorts each representing a certain age range such as those age etc the final cohort represents all people over a certain age the following equations also disaggregate the popula tion by sex 7 q chapter coflows and aging chains where is the population in cohort i b is the birth rate is net immigration to each cohort is the cohort specific death rate and is the maturation rate from cohort to the subscript s denotes the sex m or f each cohort rep resents years per cohort the birth rate is the sum of the children born to all women in the childbearing years b where in this formulation is the female population in cohort a and tf is total fertil ity the total number of children born to each woman during the childbearing years where cy is the first and cy is the last childbearingyear considered the ratio cy is therefore the average number of births per woman per year during the childbearing years inclusive usually assumed to be ages to the age specific weights represent the fraction of lifetime births occurring in each of the childbearing years figure and depend on both biological fac tors such as nutrition and socioeconomic factors such as the role of women in the society marriage age and education the sex ratio s is the fraction of births of each sex these fractions are usually close to but not equal to the sex ratio is also not constant over time in societies where there is a preference for male off spring technology now enables people to selectively abort female fetuses reduc ing female infanticide which also occurs in a number of traditional societies would be captured in the model by higher mortality rates for the youngest female cohort world average distribution of births by mothers age age group source united nations population division population newsletter june part iv tools for modeling dynamic systems over time people through the aging chain the process can be modeled with equations most standard demographic models however use a slightly different formulation many of these models use discrete time intervals equal in duration to the number of years per cohort they further assume that the death rate within the cohort is constant and deduct the total number of deaths from the cohort population before moving the population from one cohort to the next exit exit the total rate at which people leave each cohort the exit rate is divided into two flows those who mature into the next cohort and those who die the survival fraction is the fraction maturing into the next cohort and is the fraction that died while in the cohort the survival fractions are easily derived from life tables or survival distributions for the population the age specific mortality rate the probability of death per year or fractional death rate fdr for a cohort with survival fraction is given by the rate of exponential decay that would leave the fraction remaining after ypc years or if the age specific mortality rates fdr are known then the survival fraction is given by if the age specific mortality rate for a year cohort in a population were fdr then after years the expected survival fraction would be the fraction surviving is greater than because the number of deaths during each of the years falls as the number surviving falls there are two common formulations for the exit rate standard demographic models assume discrete time intervals and constant death rates within them this formulation can be modeled using a pipeline delay where the exit rate from each cohort is the total rate at which people enter the cohort the sum of those maturing into the cohort plus any inmigration delayed exactly ypc years exit where the delayp function represents a pipeline delay with a delay time here equal to the number of years per cohort the pipeline delay is appropriate for situations where the population resides in each cohort for exactly the same period often however the residence times for individuals are not identical and the death rate varies continuously at the other end of the spectrum such situations can be modeled by a first order delay exit lee and rosner describe the mathematics of life tables and survival analysis in discrete and continuous time delayp function is defined as follows outflow delay time im plies delay time see section chapter coflows and aging chains the first order exit rate implies that while the average residence time in each co hort is years some people leave earlier and some leave later the formula tion in equation is appropriate in situations where the cohorts are defined not by age but by membership in a category such as the level in an organization where some are promoted to the next level faster than others see the model in sec tion as the number of cohorts increases and the number of years per co hort falls the behavior of an aging chain consisting of n first order cohorts converges to the pipeline formulation two formulations for mortality the formulation for the transition maturation rate in equations and differs from the formulation in equations in the former case the maturation rate is given by the size of the cohort divided by the average residence time while at the same time deaths occur at a rate proportional to the size of the cohort in the latter case deaths are considered to occur at the time members exit the cohort the two formulations are similar but not identical consider first equations and for simplicity assume the exit rate is given by the first order formulation in the total outflow from each cohort is exit the interpretation is that each member of the cohort resides in the cohort for an average of periods on exit ing the cohort the total outflow is divided into those maturing into the next cohort and those exiting the aging chain this formulation is common in discrete time demographic models based on the pipeline delay where each cohort represents a particular age range and by definition individuals remain in the cohort for a fixed period of time it is also a reasonable behavioral model for some organizational structures such as consulting firms law practices or universities where there is an up or out promotion policy in these settings the stocks in the aging chain repre sent categories such as associate senior associate and partner at every rank each professional is reviewed after a certain number of years and is either promoted or terminated section in the case of equations and the total outflow from each cohort is this situation represents a case where deaths continuously remove people from each cohort the average residence time is less than ypc periods this formulation is appropriate for example in the urban dynamics model where the population is divided into different socio economic categories workers move from one category to another with a certain probability but also continuously face a chance of which formulation is better returning to the case of a law firm or university with an up or out promotion policy both processes clearly play a role after an av erage period of say years all faculty are reviewed and either given tenure or ter minated as in equation ll but there is also a certain rate at which untenured faculty leave the university prior to their mandatory review date as in equation if necessary for the purpose of the model both formulations can be com bined usually however the data are not good enough to allow these processes to the distribution of the population in each cohort is exponential in the case of equation in the case of equation the distribution is hyperexponential part iv tools for modeling dynamic systems be estimated separately and the differences between the two formulations are small enough that it is not necessary to include both aging chains and population inertia an important consequence of the age structure of the human population is the enormous momentum of population growth world population crossed the billion mark in with a growth rate of about million people per year if fertility around the world instantly fell to replacement rates meaning that on av erage people havejust enough children to replace themselves the world population would not immediately stabilize instead population would continue to grow in the united nations instant replacement scenario world population would reach billion in and billion in a rise of more than a third as long as total births exceed total deaths the population continues to grow though each cohort just replaces itself those now in the childbearing years are much greater in number than those in the older cohorts because world population has been growing more and more people will reach the prime childbearing years for the next years or so increasing total births still more long human lifetimes and the long delay between birth and reproduction mean population is very slow to ad just to changes in fertility and mortality the tremendous inertia caused by the age structure of a population is further illustrated by the experience of china fertility in china fell below the replacement level beginning in the late as the result of the government one child policy and other changes in social and economic circumstances see figure never theless the population of china grew from million in billion in more than in less than years and although total fertility is expected to remain below replacement the population is projected to grow to a peak of more than billion by before gradually declining figure compares china figure 6 projected age structure of china compared to current distribution a 24 00 04 60 60 population millions solid bars open bars projection for source us census bureau chapter coflows and aging chains age structure in to the age structurethe us census bureau projects for while the population under age remains about the same or even falls the population over age increases dramatically simply because those aging into each cohort over age are much more numerous than those aging out these examples show that a population can continue to grow even though to tal fertility rates are below replacement and can shrink even when fertility is greater than replacement variations in fertility more generally in the rate of ad dition or removal from the cohorts can induce variations in the age structure of a population that can cause its behavior to differ significantly from a model in which all ages are lumped into a single stock system dynamics in action world population and economic development most demographic models such as the projections of the us census bureau and united nations assume total fertility births mortality and migration are exoge nous and calculate the resulting age distributions and total populations these mod els are essential tools for businesses and government agencies seeking to understand demographictrends over the short term for purposes such as forecast ing school age populations or the number of people entering the workforce or be coming eligible for social security over longer time horizons births and life expectancy should not be treated as exogenous inputs factors such as nutrition access to health care the material standard of living pollution and crowding all depend on the size and wealth of the population creating a huge number of feedbacks nevertheless virtually all de mographic models including those of the un cut all these loops official projec tions assume recent trends toward lower fertility will continue until total fertility falls enough to bring world population to eventual equilibrium the medium fertility scenario for example assumes replacement fertility is achieved worldwide in leading to an equilibrium population of about billion forrester in world dynamics and then meadows et al developed the first integrated models of world population the global economy natural resources and the physical environment these models were designed to investigate the effects of population and economic growth as human activity ap proaches the carrying capacity of the earth forrester model represented world population as a single stock meadows et al elaborated and expanded forrester model disaggregating world population into four cohorts ages 64 and and over meadows showed the four level aging chain behaved quite well and provided sufficient precision when applied to world population where many different populations are aggregated together and there is considerable measurement error and uncertainty about parameters wang and sterman applying the meadows population sector to the population of china used a co hort model one per year up to age and one for those over meadows et al soughtto model the demographic transition the demographic transition describes the pattern of change in population growth rates as nations industrialize in traditional societies prior to economic development both crude part iv tools for modeling dynamic systems birth and death rates births and deaths per thousand people tended to be high and variable average life expectancy was comparatively low and women bore many children to ensure that a few would survive to adulthood and support their parents in their old age population growth was slow according to the theory of the demographic transition life expectancy rises sharply with the arrival of industrialization and the introduction of modern sanita tion public health systems and medical care death rates fall birth rates eventu ally fall as well higher life expectancy and lower infant mortality mean more children survive to adulthood so women do not need to bear as many to achieve their desired family size further desired family size tends to fall as the cost of child rearing rises and as the contribution of children to the economic welfare of the family declines costs of child rearing rise and contributions fall in industrial societies because children enter the labor force much later than in traditional agri cultural societies and must be supported by their parents for much longer and at higher cost the decline in birth rates however is very slow since norms for fam ily size marriage age and other determinants of fertility are strongly embedded in traditional culture religious norms and other social structures fertility is not the result of economic utility maximization by couples consequently during the demographic transition population growth acceler ates sharply since death rates fall while birth rates remain high eventually ac cording to the theory fertility falls into rough balance with mortality and population approaches equilibrium figure 7 shows crude birth and death rates births or deaths per thousand people per year for sweden and egypt in sweden where industrialization began early death rates fell slowly so population growth was modest during the transition taking years to double to in egypt however as in many later developing nations the death rate fell sharply after world war the birth rate while starting to fall remains high so popula tion growth is very rapid population doubled in just years by the late the transition was far from over the meadows et al global model included a fully endogenous theory of the demographic transition to do so they not only had to represent the age structure of the world population but also had to model total fertility and changes in life expectancy after testing and cohort models they determined that the model provided precision consistent with the state of the data at that time while preserving model parsimony and helping to keep the model within the ca pacity of the computers available in the early in general mortality rates for each cohort in the model should be specified as a separate function of food health care material standard of living and other factors in practice the data were not available to do so meadows et al showed that the distribution of mortality by age is reasonably stable as life expectancy varies mortality follows a u shaped curve mortality rates are highest for the very young lowest for people between about age and and gradually rise as people as average life expectancy rates change with age with social and economic conditions debate continues about the maximum human life span and the effects of industrialization and economic development on life expectancy for an overview see vaupel et al chapter coflows and aging chains demographic transition m rises and falls the distribution is shifted downward or upward respectively but retains roughly the same shape assuming the shape of the distribution remains stable age specific mortality can then be modeled as where is the reference fractional death rate for each cohort and le is av erage life expectancy at birth the reference death rate is the fractional death rate in a reference year corresponding to the year in which life expectancy takes the value that yields the downward sloping age specific functions relate mortality to average life expectancy and can be estimated from actuarial or demographic data meadows et al modeled aggregate life expectancy as depending on four fac tors food per capita health care services per capita exposure to persistent pollu tion and crowding these factors interacted multiplicatively to capture important interdependencies and to ensure robustness under extreme conditions life ex pectancy must approach zero as food per capita approaches zero life expectancy remains finite even if all conditions are extremely favorable the determinants of part iv tools for modeling dynamic systems life expectancy were endogenously generated by other sectors of the model clos ing the feedback loops in the model total fertility tf depends on both a biological maximum and desired family size of each woman in the population the effectiveness of fertility control deter mines whether total fertility is closer to the biological maximum or the desired level a variety of socioeconomicfactors determined desired family size and fer tility control effectivenessin the model the model also represented delays in the adjustment of cultural for desired family size to new social and economic conditions the model did an excellentjob replicating the aggregate historical data and provided the first fully endogenous model of the demographic transition a model that after more than years can still be fruitfully used to explore differ ent policies relating to population growth by treating interactions of population economicgrowth and the environment in a fully endogenous fashion forrester and meadows et al provided the first in tegrated global models to study the dynamics of growth in a finite world in the meadows et al model the demographic transition is not automatic as assumed in models with exogenous fertility and mortality if resources and environmental ca pacity are sufficient and if economic growth and development are distributed fairly so even the poorest people have sufficientfood clean water access to health care and decentjobs then the world as a whole moves through the demographictransi tion and population eventually stabilizes with high life expectancy and low fertil ity but if global economic development passes the have nots by or if pollution resource shortages crowding insufficient food or other problems caused by growth limit development then the economic and social conditions that eventually lead to low birth rates will not arise and the demographic transition will not occur population and economic growth continue overshooting the earth carrying ca pacity environmental degradation reduces the carrying capacity and mortality rises within a hundred years population and economic output fall meadows et al pp 24 summarized the conclusions of the study as follows if the present growth trends in world population industrialization pollution food production and resource depletion continue unchanged the limits to growth on this planet will be reached sometime within the next one hundred years the most probable result will be a rather sudden and uncontrollable decline in both population and industrial capacity it is possible to alter these growth trends and to establish a condition of eco logical and economic stability that is sustainable far into the future the state of global equilibrium could be designed so that the basic needs of each person on earth are satisfied and each person has an equal opportunity to realize his or her individual human potential if the world people decide to strive for this second outcome rather than the first the sooner they begin working to attain it the greater will be their chances of success both forrester and the meadows team sought to encourage conversation and debate about growth and stimulate further scientific research leading to im proved models improved understanding and ultimately actions and policies to prevent overshoot and collapse and encourage what is now known as sustainable chapter and aging chains development toward that end the authors took pains to point out the limitations of their models meadows et al p wrote the model we have con structed is like every other model imperfect oversimplified and unfinished they published complete documentation for both world models so anyone with ac cess to a computer could replicate revise and modify the models many did so and dozens of critiques and extensions were published the model triggered a vigorous and sometimes acrimonious public debate over growth a debate still reverberating today it also stimulated many other global modeling efforts spanning a wide range of methods model boundaries time hori zons and ideological perspectives global models with narrow model boundaries where many of the feedbacks are cut tend to reach more optimistic conclusions models that capture the many feedbacks between human activity and the environ ment tend to reach conclusions consistent with the original 6 case study growth and the age structure of organizations variations in birth rates have dramatic effects on the age structure of the world population but growth also has profound implications on the age structure and maturation of organizations most organizations contain various chains that represent the different levels in the hierarchy within each department or func tion consulting firms for example typically include levels such as associate senior associate partner and director the growth rate of the organization has a dramatic impact on the balance among the levels in a promotion chain hierarchy and on opportunities for advance ment figure shows the promotion chain for a typical american university there are three faculty ranks assistant professors associate professors and full professors most us universities operate an up or out promotion system faculty are reviewed after a certain period and those not promoted are terminated faculty reaching full professor are granted life tenure and remain active until they choose to retire mandatory retirement in the us was abolished in the while occasionally senior faculty are hired from other institutions the vast majority of hiring is at the new assistant professor level the up or out policy means the transition and departure rates are formulated as follows the assistant professor flows are shown flows for associates are analogous assistant promotion rate assistant review rate assistant promotion fraction provides the first global model with fully endogenous population and carry ing capacity meadows et al provides full documentation for the model called meadows et al provides a nontechnical discussion of the assumptions and results of the study meadows meadows and randers updates the study and model and the best starting point for those wishing to dig more deeply into these issues other global models and the science of global modeling itself are critiqued in meadows richardson and bruckmann part iv tools for modeling dynamic systems figure promotion chain for a typical american university hiring of associate and senior faculty is omitted the review rate for the assistant and associate ranks is first order the full professor retirement rate is formulated as a third order delay of the associate promotion rate adapted from a model developed by david peterson and used with permission assistant promotion fraction associate promotion fraction average full professor service time assistant departure rate assistant review rate assistant promotion fraction assistant review rate assistant professors average assistant review time note that the assistant and associate ranks are formulated as first order processes even though contracts are nominally all for the same specified duration sug gesting a pipeline delay in practice some faculty are promoted earlier than others due to the varying incidence of personal and professional leaves of absence and to formulation assumes all faculty remain at the institution until they come up for review in fact there is some probability faculty leave prior to their promotion reviews a more realistic model would incorporate both effects combining the two formulations for mortality in practice the data to estimate the parameters are not available and the error introduced by aggregating those who leave prior to mandatory review with those who leave after a negative promotion review is negligible chapter coflows and aging chains differing market pressures a hot young professor may be promoted early to re spond to outside offers from competing universities these sources of variation imply that the formulation for the exit rate should allow some mixing the order formulation assumes perfect mixing certainly an overestimate of the variance in promotion times however given the relatively short residence times in each junior rank the first orderformulationis not likely to introduce significant error in contrast the long tenure of full professors means a first order formulation with its assumption that the youngest full professors are just as likely to retire as the oldest is clearly inappropriate in the model analyzed here a third order for mulation for the retirement rate is used given these assumptions what is the distribution of faculty across the three ranks the distribution depends on the average residence times of faculty in each rank and the average promotion fractions as well as the growth rate of the faculty in most universities faculty remain assistant professors for an average of years before promotion to associate associateprofessors are typically reviewed for pro motion to full professor after an average of years full professors typically serve about years before retiring at an average age of about 70 though promotion fractions vary over time typical values might be roughly at each rank the equilibrium distribution given these parameters is readily calculated by little law to be about assistant associate and full professors a distri bution top heavy with senior faculty where the total size of the faculty is fixed junior faculty can only be granted tenure when a full professor retires or dies however few universities are in equilibrium most us universities went through a period of rapid growth from the end of world war through the early when the baby boom generation graduated from college since then due to declining college age populations and stagnatingfederal supportfor higher educa tion growth slowed or even became negative figure shows the distribution of faculty ranks at mit since the pattern of behavior at other leading uni versities is similar though the timing and magnitudes differ until total fac ulty grew rapidly averaging the age distribution was skewed toward the younger ranks with full professors averaging only about of the faculty from in the essentially ceased and the total fac ulty remained roughly constant through the mid hiring of new assistant professors fell and the age distribution began to approach equilibrium by assistant professors made up less than of the faculty while more than were full professors very close to the equilibrium distribution the consequences of this transition were profound during the era of rapid growth the high proportion of young untenured faculty gave the institution tremendous flexibility and brought large numbers of talented people into the orga nization because there were relatively few senior faculty the chances of promo tion to tenure were good relatively young professors soon found themselves promoted to senior positions such as department chair or dean after growth stopped and most departments began to fill with tenured faculty flexibility de clined it became more difficult to get tenure in some particularly top heavy de partments those that had grown the fastest during the boom years there was almost no turnover little hiring and few junior faculty as similar dynamics played out at universities throughout the country many doctoral candidates found they part tools for modeling dynamic systems figure distribution of faculty ranks mit um 8 6 4 could not get tenure track positions after graduation as a result many were forced to accept low paying postdoctoral positions or leave academia altogether the aging of the faculty also had important financial implications because senior faculty are generally paid more than junior faculty the aging of the faculty increased the total cost of the faculty faster than the rise in salaries for each rank for individual faculty salaries to keep pace with inflation the total payroll had to grow even faster the resulting cost escalation helped push tuition up much faster than inflation during the and ultimately in part to relieve budget pres sure and in part to make room for fresh blood mit along with other top research universities where the same dynamics had played out implemented an early retirement incentive program to speed the outflow from the ranks of the full professors the discussion above suggests that the hiring rate and promotion fractions are not constant they change as conditions in the university evolve the data on fac ulty at each rank can be used to estimate what the hiring and promotion fractions must have been and therefore test these hypotheses given the average time spent in each rank figure shows the results no attempt has been made to specify the hiring and promotion fractions every year instead these parameters are set to round numbers at widely spaced intervals despite the low resolution with which the inputs are estimated the model closely tracks the actual data the excellent fit shows that the assumption of first order exit rates from each junior cohort and chapter coflows and aging chains figure simulated faculty age structure top simulated vs actual faculty by using imputed hiring rate promotion fractions imputed professor hiring rate bottom imputed promotion fractions u1930 1950 1960 e source adapted from a model developed by david peterson a third order retirement rate is acceptable and there is little need for further disaggregation of the age structure within each faculty rank consistent with the discussion above promotion fractions and hiring were high during the years of rapid growth and fell when growth stopped note also the large burst of hiring in the which dramatically increased the ranks of the assistant professors as intuition would suggest the hiring surge was followed by several years of depressed hiring more interestingly the increasing number of faculty in the upper ranks compared to the number of slots available meant that the probability of part tools for modeling dynamic systems promotion from assistant to associate after the hiring boom in the fell significantly in reality the size and composition of the faculty and more generally any workforce feeds back through the market and other channels to affect the hiring rate promotion fractions and other parameters while important insights can be gleaned from the promotion chain model with exogenous hiring promotion and departure parameters these structures are most useful when embedded in a full model of the organization the promotion fraction for example can be modeled endogenously as depending on the normal span of control and the balance of senior to junior personnel it is also affected by the financial health of the organization the rate at which employees voluntarily quit to take better opportunitieselsewhere depends on their perceptions of the chances for promotion these chances in turn depend on the age structure and hence the growth rate of the organization since the most talented employees will have the most attractive outside opportunities a slowdown in growth by reducing promotion opportunities can systematically drain an organization of its best talent loss of talent can then feed back to worsen performance in the marketplace further eroding growth in a vicious cycle see sec tion 4 the faculty example shows how an aging chain can be used to model the de mographic structure of organizations and illustrates the dramatic impact of growth on the distribution of personnel among the different ranks the steady state age structure of any population depends on its growth rate changes in population growth rates at the community national or global levels change the ratios of chil dren to people of childbearing age and of the working age population to retirees significantly changing the social economic and political pressures faced by the population similarly as the growth rate of a business or other organization changes it necessarily goes through large changes in the ratio of senior to junior employees in promotion opportunities and in the average cost of the workforce these changes arise solely as a function of changes in the growth rate of the orga nization since the growth of all organizations must slow as they become larger the age distribution tends to become top heavy posing great challenges to organi zations seeking to renew themselves while preserving attractive career paths for those already in the hierarchy the larger and faster the decline in growth rates the worse this problem becomes the fastest growing most successful organizations always face the greatest challenge when their growth inevitably slows 7 promotion chains and the learning curve consider again the two level promotion chain for rookie and experienced workers shown in figure this structure is very useful in modeling the effect of training and assimilation delays on the productivity of a workforce as the growth rate the promotion chain provides a simple and effective way to model the learning curve for new employees to keep the model simple assume it is not applies the promotion chain structureto service quality in a major uk bank abdel hamid and apply it to softwareproduct development packer applies it to a model of high tech growth firms chapter coflows and aging chains figure a two level promotion chain to explore worker training quit fraction possible to hire experienced people in some industries experienced hires are unavailable or too expensive more commonly becoming fully productive depends on the accumulation of situation specific knowledge so prior experience is of limited benefit the productivity of employees is typically a fraction of that for fully ex perienced employees the total potential output of the workforce is then given by potential experienced rookie productivity experienced output productivity average productivity is fraction employees employees average productivity potential employees formulating the flows as first order processes yields quit rate rookie employees quit fraction experienced quit rate experienced employees experienced quit fraction assimilation rate time 24 for purposes of testing assume the workforce grows at a constant exponential rate to do so the firm must replace all those who quit and continuously add a fraction of the current total workforce hire rate total quit rate growth rate total employees 25 total quit rate quit rate experienced quit rate 26 part iv tools for modeling dynamic systems what is the equilibrium distribution of employees between the rookie and experi enced categories the equilibrium conditions when the growth rate is zero are hire rate quit rate assimilation rate assimilation rate experienced quit rate given the definitions of the flows the equilibrium number of rookies is easily shown to be employees experienced experienced assimilation employees quit fraction time which means the equilibrium rookie fraction is experienced assimilation fraction quit fraction time experienced assimilation quit fraction time equilibrium average productivity as a fraction of the productivity of experienced employees is as intuition would suggest the lower the relative productivity of the lower the equilibrium productivity of the workforce will be unless are as similated instantly longer assimilation times mean there must be more rookies in training and higher experienced quit rates mean more must be hired to offset those experienced workers who leave both effects lower equilibrium productivity note that the quit fraction has no impact in this model because rook ies are represented as a single cohort those who quit are immediately replaced so rookie quits cancel out in the net rate of change of of course higher rookie quit rates would increase the load on and cost of the firm human resource organization in a more realistic model where the rookie population is disaggre gated into more than one cohort or where filling vacancies takes time the equilib rium would depend on the quit rates of the intermediate stocks as an example suppose the assimilation delay is weeks about years and experienced employees remain with the firm for an average of years the experienced quit fraction is assume rookie employees quit at a higher rate of as some of the wash out or decide the job doesn t suit them assume the average rookie is only 25 as productive as experienced workers the equilibrium roolue fraction is then and equilibrium productivity is 875 of the experienced loss of generality the productivity of experienced employees can be defined as allowing potential output to be measured in full time equivalent fte experienced personnel chapter coflows and aging chains the assimilation delay and rookie productivity fraction jointly determine the learning curve for new employees imagine hiring a group of rookie employees when there are no experienced employees productivity initially will be the productivity fraction and ultimately will be of the experienced level be cause the assimilation process is first order productivity must approach ex ponentially with a time constant equal to the assimilation delay if the evidence suggested the learning curve for new employees was not first the promotion chain could be disaggregated further to yield the appropriate pattern of productiv ity adjustment what happens if the firm is growing figure shows a simulation in which the workforce grows at an exponential rate of start ing in week the initial number of experienced workers is therefore the initial hiring rate is year the hiring rate immediately rises above the total quit rate and the workforce begins to grow at because all new hires are inexperienced the fraction immediately begins to rise and average produc tivity immediately begins to fall in the steady state the fraction rises to and productivity falls to just of the experienced level though every em ployee goes through a learning process that boosts individual productivity from 25 to the shift in the age distribution caused by growth lowers average pro ductivity consequently given the parameters in the example potential output shown on the graph in fte experienced employees barely changes for the first 6 months even though the total workforce and payroll begin to rise immediately after a year potential output has risen by only compared to the growth in total 8 mentoring and on the job training in the model so far employees gain experience automatically and without cost in reality on the job otj training often requires the help and mentoring of experienced employees inexperienced workers reduce the time experienced peo ple can devote to their own jobs by questions and by causing experienced people to work at a slower rate modifying the model to account for the impact of mentoring requires reformulating potential output as depending on the number of effective experienced employees where the effective number of experienced employees is the total number less the time devoted to training the inexperienced employees that is effective experienced experienced time spent employees employees training that the pattern of adjustment of productivity and the fraction is exponential a direct consequence of the first order assimilation rate a more realistic model disaggregating the population into more cohorts would show an even slower increase in potential output part iv tools for modeling dynamic systems figure response of level promotion chain to growth the total work force grows at starting in week the rookie produc tivity fraction 25 the assimi lation delay weeks the experienced quit fraction and the rookie quit fraction 01 week weeks weeks 8 c 6 4 weeks time spent rookies fraction of experienced time 34 training required for training each consumes an amount of experienced worker time equal to the fraction of experienced time required for training under extreme circumstances the number of roolues might be so high or their training demands might be so great chapter coflows and aging chains that the time remaining for experienced workers to actually do their jobs may fall to mentoring has only a small impact on the equilibrium productivity of the workforce equilibrium productivity for the case of mentoring by experienced workers is easily found to be with the parameters in the example above and assuming a rather high value of 5 for the fraction of an experienced employee time consumed in training each productivity in equilibrium falls to of the experienced level compared to 5 in the case with no mentoring by experienced workers while mentoring has only a modest effect on equilibrium productivity the im pact on productivity and potential output when the workforce is growing is dra matic figure shows the effect of mentoring in the same scenario as figure in the simulation each requires mentoring by the equivalent of 5 experienced people the and experienced employee stocks follow the same trajectory but now as the fraction rises the total time spent training rook ies grows and the time experienced workers can contribute to production drops given the parameters productivity falls to a steady state value of compared to in the case with no mentoring a drop of in the short run growth ac tually causes potential output to fall potential output drops to a minimum be low the initial equilibrium and only reaches the initial level after weeks realistically production pressures will slash the time devoted to training below what is required long before the number of effective experienced employees falls to zero therefore to be fully robust the assimilation time should be reformulated as a variable rising when total time is less than required part iv tools for modeling dynamic systems 4000 2000 weeks weeks 8 6 4 150 weeks how do these parameters affect the loss of productivity in the steady state as growth rates increase the current model assumes the learning and assimilation process is first order in many settings this is unrealistic modify the model to include a order training and assimilation process with the same fractional rookie quit rate and average assimilation time assume employees are equally likely to quit in each of the three trainee categories you create be sure you modify the total hir ing rate to replace all employees who quit each category of still requires chapter and aging chains the same time from experienced employees through otj training and mentoring initially assume the productivity of all rookies is still 25 that of experienced workers repeat the test shown in figure in which growth at a constant fractional rate begins from an initial equilibrium next assume more realistically that productivity is 25 and that of experienced workers for rook ies in stages and of their training respectively what is the impact of a higher order training process on the transition from stability to growth in many organizations such as consulting firms law firms and other profes sional service organizations experienced employees not only mentor junior em ployees on the job but also participate in recruiting new employees modify the model to capture the loss of productive time due to recruiting effort assume re cruiting each new employee requires a certain fraction of an experienced person time do not participate in recruiting pay careful attention to the units of measure select parameters to represent the case of consulting firms recruiting mba students the leading consulting firms invest heavily in the recruiting process besides on campus recruiting events and interviews promising candi dates face second third and often fourth rounds of interviews at company sites during which they meet many senior members of the firm senior people must then devote further time to discussion and selection of the finalists further many can didates must be interviewed for each one ultimately hired make the selectivity of the firm an explicit parameter in your model measured as a dimensionless ratio of the number of offers made per candidate considered also introduce a parameter reflecting the fraction of offers accepted the yield using your estimate of these parameters and the time senior employees invest in recruiting each candidate run the model for various growth rates what is the impact of recruiting effort on aver age productivity and effective production capacity what is the impact of growth on the time senior people have available for revenue generating activities what would happen if the firm tried to cut back on the time senior people invest in re cruiting what would happen if the firm became less selective if the reputation of the firm declined eroding their yield develop a causal diagram showing how po tential output and average productivity might feed back to affect the firm ability to deliver high quality results to their clients and to recruit and retain the best can didates what are the implications for the growth strategy of a firm modeling the attributes of a stock the stock and flow networks developed thus far keep track of the number of items in a stock and flow chain the size of a stock indicates how much material is in the stock but does not indicate anything about other attributes of the items a model of a firm might include stocks for different of employees but these stocks only indicate how many employees there are and do not reveal how productive they are their average age their training level or other characteristics that might be impor tant for the model purpose often it is necessary to keep track of attributes such as the and experience of workers the productivity of machine tools the defects embodied in designs moving through a product development process or the book value of a firm inventory coflow structures are used to keep track of the part iv tools for modeling dynamic systems attributes of various items as they travel through the stock and flow structure of a system as an example consider a model designed to help a company understand how fast new technology can be deployed and how it changes the number of workers it needs each machine the company buys from equipment suppliers requires a cer tain number of workers to operate it over time as technology improves the pro duction process grows more automated and fewer workers are required the company is interested in knowing how quickly the new labor saving machines will be deployed and how fast their total labor requirements will change a simple model of the situation begins with the firm stock of capital plant and equipment such as machine tools the capital stock is augmented by capital acquisitions and reduced by capital discards for simplicity assume the discard process is first order capital stock 36 acquisition capital discards capital capital acquisition exogenous capital discards capital life of capital the total labor requirements of the firm are equal to the product of the number of machines in the firm plants and the average labor requirements of each machine total labor requirements capital stock average labor requirements how should average labor requirements be modeled obviously if a new type of machine requiring only half as many workers suddenly became available the av erage labor requirements would change only slowly as the new machines gradually replaced the existing labor intensive machines there is a delay between a change in the labor requirements available in new machines and the adjustment of average labor requirements similar considerations apply to other factor inputs to the production process such as the energy requirements of the machines their total productivity and so on it is tempting to model the adjustment of these factor re quirements as a simple delay where the adjustment time is equal to the average life of capital average labor requirements requirements of new capital average life of capital however the delay formulation is fundamentally flawed and will lead to signifi cant errors if the capital stock is not in equilibrium an extreme conditions test ex poses the defect in the proposed formulation suppose the firm equipment suppliers introduce a new type of machine that requires only half as many workers now suppose that at the same time the firm stops buying new equipment alto gether say because of a recession the firm then must continue to use the exist ing inefficient machines and there is no change in the average labor required per machine however the delay in equation will continue to adjust average labor requirements to the new low level of the new machines even though the firm isn t buying any the firm required labor force falls as the delay magically con verts the old machines to the productivity of new ones without the need for any in vestment or expenditure the rate of change in average labor requirements depends chapter coflows and aging chains coflow to track the labor requirements embodied in a firm capital stock exogenous capital acquisition rate average life of labor requirements of new capital labor requirements on the rates at which new machines are added to and old machines are discarded from the capital stock the labor requirements of the firm equipment are embod ied in the machines themselves modeling the adjustment as a delay divorces changes in this attribute of the stock of machines from changes in the stock itself to model such a situation requires the modeler to keep track of the labor require ments of every machine added and every machine discarded coflow structures allow you to do this figure shows the structure for the capital stock model described above the coflow is a stock and flow structure exactly mirroring the main stock and flow structure the coflow tracks the labor requirements embodied in the capital stock as new machines are acquired and old ones are discarded the stock of capital is augmented by acquisitions and reduced by discards for now the capital stock is treated as a first order process the capital acquisition rate is exogenous some try to save the delay formulation in equation by making the average life of capital a variable or representing the adjustment process with a higher order delay but the critique remains valid in fact there are many models in the literature where the adjustment of input requirements is modeled as a delay including a wide range of econometric models of energy de mand in which the energy intensity of the economy of output or per of capital stock are assumed to adjust to changes in price with some form of distributed lag independent of the characteristics size or turnover of the capital stocks consuming that energy part iv tools for modeling dynamic systems the total labor required to operate the company existing machines is given by the labor requirements stock every time a new machine is added to the capi tal stock the total labor requirements of the firm rise by the labor required for that machine every time a machine is discarded the total labor requirements decrease by the average labor requirements of the discarded machine increase in labor requirements capital acquisition labor requirements of new capital for example if each machine requires say workers then a purchase of machines increases total labor requirements by people since the discard process is assumed to be first order the probability of discard is independent of ac quisition time and of any other attributes of the capital stock therefore the aver age labor requirements of the discarded machines equal the average for the entire existing stock that average is given by the total labor requirements divided by the total capital stock thus decrease in labor requirements capital discards average labor requirements average labor requirements labor stock labor increase in decrease in requirements integral labor labor requirements requirements if each existing machine required workers then the discard of machines would reduce labor requirements by 2000 people the replacement of these ma chines with new ones requiring instead of 200 people reduces total labor re quirements by people the coflow structure has some obvious and desirable properties in equilib rium capital acquisitions and discards are equal and if the labor requirements of new capital are constant the labor requirements of the firm will also be constant since the loss of jobs associated with discarded machines just offsets the increase in labor required to operate new ones the equilibrium labor requirements of the firm will be labor requirements of new capital capital stock changes in capital acquisition or discards have no effect on average labor re quirements as long as the labor requirements of new capital remain constant now imagine that a new labor saving technology suddenly becomes available so that all new machines require only half as much labor figure shows the response of labor requirements over time as expected given the first order structure for the capital discard rate the labor requirements of the firm approach the new equilib rium exponentially with a time constant given by the average life of capital if the response of the system is simple exponential decay why is a coflow for mulation needed the behavior shown in figure 15 is exactly what would be generated by the delay formulation in equation why not simply model the adjustment of average labor requirements with the simple and easy to explain de lay the answer is that the response of average labor requirements depends on the chapter coflows and aging chains figure 15 response of labor requirements to a sudden reduction in the labor required to new machines average labor requirements of capital labor requirements of new capital 60 80 years figure changes in the growth rate of the stock alter the adjustment of its attributes in all cases the la bor requirements of capital drop by 50 in year 5 the responses in situations of in capital acquisi tions equilibrium cline in acquisi tions are shown m behavior of the acquisition and discard rates in equilibrium with constant acqui sitions discards and capital stock the coflow behaves exactly like a first order de lay with a time constant equal to the average life of capital but now suppose the firm is growing so the acquisition rate rises exponentially at some rate new ma chines will be added ever faster quickly diluting the contribution of old machines to average requirements or suppose the discard rate varies with the utilization of the firm capital stock in these cases the rate at which old machines are replaced varies over time and so too will the evolution of average labor requirements fig ure 16compares the behavior of labor requirements in stationary equilibrium to the cases where the acquisition rate grows at and where it shrinks at when the capital stock is growing new machines are added at an ever greater rate so new machines with low labor requirements quickly dominate the stock of capital average labor requirements fall to the new level after only about years compared to more than years in the equilibrium case even more interesting in the case where the firm is shrinking at new investment quickly becomes so small that average labor requirements never reach the new level after about years new investment is negligible and the firm is stuck with a capital stock consisting primarily of old inefficient machines by explicitly modeling the part iv tools for modeling dynamic systems attributes of the items flowing into and out of the stock of machines the coflow model correctly tracks changes in the total and average labor requirements of the firm the labor requirements example can be generalized to any attribute of any stock figure shows the generic structure for a coflow for the case where there is a single inflow and single outflow to the stock in general the main stock may have any number of inflows and outflows say m inflows and n outflows stock inflow total outflow total inflow total outflow outflow j residence time for 48 each outflow is modeled as a first order process with an outflow specific time con stant the time constants can be variables the coflow structure tracking the attribute of the stock exactly mirrors the structure of the main stock each unit flowing into the stock adds a certain number of attribute units to the total attribute stock in the example each new machine adds a certain number of workers to the total number required to operate all the machines the number of attribute units added per stock unit denoted the marginal attribute per unit can differ for each inflow for example the firm might buy dif ferent types of machines each requiring a different number of workers to operate it thus total attribute total total increase in decrease in attribute attribute 49 m total increase in attribute marginal attribute per 50 similarly for each outflow from the main stock there is a corresponding drain from the total attribute stock each unit leaving the stock removes the average attribute per unit the average attribute per unit is simply the total attribute level divided by the total number of units in the stock total decrease in attribute average attribute per unit average attribute per unit total you can model as many different attributes as you desire each captured by a sep arate coflow structure for example one coflow might represent the labor require ments of the firm capital stock another might represent the energy requirements a third might represent the productivity of the machines a fourth might represent the defect rate in the output of the machines and so on chapter coflows and aging chains generic coflow structure each unit flowing into the stock adds the marginal attribute to the total attribute each unit flowing out removes the average attribute in general there can be any number of inflows and outflows to the main stock with a corresponding flow into the total attribute stock average residence time for outflow stock marginal attribute per attribute part iv tools for modeling dynamic systems deficit is expenditure less revenue assume revenue is constant at billion per year expenditures consist of interest on the debt and spending on gov ernment programs assume program spending is also constant at these values are approximately correct for in the outstanding debt was about 5 trillion interest payments are equal to the product of the out standing debt and the average interest rate first formulate the average interest rate as an exogenous constant initially equal to next replicate the model and formulate the average interest rate by using a coflow the coflow formulation ac counts for the fact that the average interest rate depends on the interest rates at which each bill bond or note was issued even if interest rates on new debt have changed verify that when the interest rates in both formulations are constant and equal the behavior of the two formulations is identical next compare the behavior of the two formulations for the case where the interest rate falls from to in what difference does the coflow structure make and why to approximate a continuous compounding situation use a small time step such as year for the simulations it is often important to model the average age of items in a stock or the aver age date at which the items entered the stock as an example consider a model of a firm labor force assume a single stock of labor increased by a hiring rate and decreased by an attrition rate assume that the attrition rate is first order and that employees stay with the firm an average of years formulate a coflow that keeps track of the average age of the people in the labor force and also the average date at which they joined the firm hint you only need one coflow stock to calcu late both the average date at which each person was hired and the average age of the workers formulate the model so it begins in equilibrium demonstrate that in equilibrium the average age of the workers is equal to the average tenure in the job plus their average age at the time of hiring explore the response of the average age of the employees to various test inputs such as changes in the average time people stay with the firm step and pulse changes in the hiring and attrition rate and ex ponential growth or decline in the hiring rate note this challenge requires that you introduce a flow that alters the attribute stock for which there is no corre sponding flow into or out of the stock of labor such a structure is called a conserved coflow because the attribute stock can change even when there is no inflow to or outflow from the main stock 4 the capital stock of a firm is increased by acquisitions and decreased by dis cards the average lifetime of each unit of capital is years given the cost of each unit of capital create a coflow that models the book value of the firm capi tal stock assume the value of each unit of capital is reduced by depreciation with an average depreciation life of capital that can differ from the actual lifetime coflows with nonconserved flows the coflow structures described so far represent the attributes of the stock as con served quantities the only way the total attribute stock can change is through the chapter coflows and aging chains inflow or outflow of a unit from the main stock often however the attributes as sociated with a stock can change without any change in the main stock retrofits can change the labor or energy requirements of a firm capital stock even though the firm doesn t buy or discard any new equipment the value of a firm inventory can be written down to reflect changes in its market value though the physical in ventory itself doesn t change in these cases the total attribute associated with a stock is not conserved and the coflow structure includes additional flows into or out of the total attribute stock flows for which there are no corresponding flows af fecting the main stock suppose as in figure you are modeling a firm labor force which is increased by hiring and decreased by attrition for the purposes of this example the hiring rate and fractional attrition rate are assumed to be exogenous though in figure example of a nonconserved coflow tracking the experience of a labor force employees bring a certain amount of experience with them departing employees take their experience with them in addition experience increases with tenure in the job and declines as workers or as changes in the process make existing experience obsolete fractional attrition rate labor force attrition experience decay rate average experience experience i decay rate total effective increase in experience experience experience from attrition increase in the job experience worked per year part iv tools for modeling dynamic systems general they will be modeled as endogenous variables the coflow measures the average and total effective experience of the workforce the stock total effective experience measured in person weeks is the effective number of weeks of ser vice each employee has summed over all employees each employee hired brings a certain amount of effective experience employees leaving the labor force take the average experience with them average experience total effective force total effective experience in experience from hiring increase in on the job experience loss of experience from attrition experience decay rate total effective increase in experiencefrom hiring average experience of new hires hiring loss of experience from attrition average experience attrition each employee accrues additional experience at the rate of week per week worked in this example the unit of time for the simulation is the year while aver age experience is measured in weeks the increase in total effective experience is the number of weeks each person works per year summed over the entire labor force increase in on the job experience labor force weeks worked per year finally effective experience also decays as people forget relevant knowledge and as changes in the production process render experience obsolete the fractional decay rate is assumed constant here but might vary with changes in organizational structure or process technology the total loss of experience is the average loss of experience summed over the entire workforce experience decay rate 58 labor force average experience fractional experience decay rate because the stock of effective experience is modified by the nonconserved flows of experience accrual and decay the equilibrium experience of the average worker will not in general equal the average experience of new hires as it would in a conserved coflow in equilibrium the sum of the four rates affecting total effective experience must be zero when hiring and attrition are also equal so the labor force is in equilibrium with hiring attrition labor fractional attrition rate a little algebra reveals equilibrium average experience to be in the simulation of this example is measured in years while experience is measured in weeks there is no contradiction consider the units of equation 57 the increase in otj ex perience is measured in determined by the labor force and the average number of weeks worked each year if time was measured in months the increase in on the job experience would be the labor force multiplied by the number of weeks worked per month note that the number of weeks worked per year will not in general equal vacation time sick leave strikes or promotion to management all reduce the rate at which employees accumulate experience chapter coflows and aging chains average experience equilibrium total effective experience is simply the equilibrium average experi ence multiplied by the labor force as expected the greater the experience of new hires or the number of weeks worked per year the greater the equilibrium average experience will be the faster experience decays or people leave the organization the lower the equilibrium experience level will be greater average experience should translate into greater productivity higher quality and lower cost learning curve theory provides a variety of models to re late experience with a process to attributes such as productivity quality or cost one common formulation for the learning curve posits that productivity rises by a given percentage with each doubling of relevant experience productivity reference productivity average experience reference experience 60 where reference productivity is the productivity attained at the reference experi ence level the exponent c determines the strength of the learning curve and is equal to c where is the fractional change in productivity per doubling of effective experi ence see the challenge in section 4 see also zangwill and kantor for a derivation of this and other forms of learning similar equations could be used to model other attributes such as defect rates mean time between failure for equipment or unit costs as they depend on average experience most learning curve models measure experience by cumulative production a stock that can never decline so productivity can only rise over time the model de veloped here represents productivity as dependent on the average effective experi ence of each worker modeling learning as a process embedded in the human capital of the firm means that in contrast to standard learning curve models it is possible for the productivity of the firm to fall productivity can fall if there is a sudden exodus of experienced workers or if there is a large change in technology that makes past experience obsolete clearly while worker specific knowledge is important learning is also em bedded in longer lived stocks such as plant and equipment organizational routines and other infrastructure cumulative experience with these infrastructures could be modeled in the same way as effective labor experience though these other ele ments of firm infrastructure would have smaller attrition rates than labor model ing productivity as dependent on experience embedded in a firm resources and infrastructure rather than as a function of some disembodied notion of cumulative fractional change in productivity will be positive because greater experience boosts productivity if the learning curve is used to represent unit costs will be negative since increasing experience reduces costs part iv tools for modeling dynamic systems experience allows productivity cost or quality to decay should experience de cline while standard learning curves cannot exhibit such behavior there is some evidence for such forgetting curves sturm estimated learning curve models for the nuclear power industry in europe the former soviet union and the usa surprisingly the number and duration of unplanned outages actually increased with cumulative operating experience in about half the coun tries primarily in the nations of the former soviet bloc sturm hypothesized that knowledge of safe operations fell in the wake of the political and economic turmoil caused by the fall of the command economies henderson and clark 1990 found that dominant firms in the semiconductor equipment industry often lost their lead ership position when there was a change in product architecture that rendered the cumulative experience of the firm obsolete eroding their competitive advantage and allowing younger and less experienced firms to overtake them accurately modeling such situations requires nonconserved coflows the dynamics of experience and learning explore the behavior of the workforce experience model in figure assume the hiring rate equals the attrition rate plus exogenous test inputs so that those leaving are instantly replaced assume the initial labor force is people con sider the following parameters fractional attrition rate average experience of new hires weeks average weeks worked per year 50 fractional experience decay rate year fractional improvement in productivity per doubling of experience 30 reference productivity reference experience 10 weeks what is the equilibrium average experience per worker explore how the equilibrium varies with the values of the different parameters what happens to average experience and productivity if no one ever leaves the firm generate the learning curve for a cohort of new employees by setting the initial labor force to a very small number one setting the fractional attrition rate to zero and then adding a large pulse of new employees at the start of the simulation without attrition experienced employees never leave does effective experience rise indefinitely not what is the equilibrium if it exists for average experience and productivity consider the response of average experience and productivity to changes in the various parameters from an initial equilibrium assume employee turnover doubles what is the impact on average experience and productivity and why chapter coflows and aging chains integrating coflows and aging chains the assumption that each unit leaving the main stock removes the average attribute per unit is clearly an approximation in particular the first order structure for the decrease in the total attribute implicitly assumes that all items in the stock are per fectly mixed as seen in chapter the assumption of perfect mixing is often not appropriate a better model requires higher order delays or a high order aging chain the coflow structure for these cases will exactly mirror the stocks and flows in the higher order delay or aging chain for example sterman developed a general model to capture the pro duction function of a firm or economic system production depends on inputs of capital labor energy and materials these input requirements are embodied in the firm capital stocks as in the labor requirements example however economet ric evidence and field study show that the distribution of discards from capital stocks is not first order for example new machines and facilities are not nearly as likely to be scrapped as older units the distribution is clearly higher order i there fore disaggregated the capital stock into an aging chain with corresponding coflows i further assumed that the labor energy and materials requirements of capital equipment were determined at the time construction starts the energy requirements of a new office building can t be changed significantly after except by costly retrofit the resulting vintaging structure and corre sponding coflows for the embodied input requirements are shown in figure 19 the number of vintages can be increased as needed to fit the data for the survival distribution of items in the main aging chain in the simplified representation of the model shown in the figure the construction delay is assumed to be first order while a third order delay with a corresponding third order coflow structure for the factor requirements of capital under construction would be more realistic like wise capital is discarded only from the oldest vintage if the data warrant it it is easy to include a discard rate from each vintage in that case the modeler must also include the reduction in the factor requirements of each vintage from discards equal to the product of the factor intensity of each vintage and the discard rate from each vintage finally note that the model shown in the figure does not permit changes to the factor requirements of capital this is known as a putty clay model because factor requirements can be varied prior to investment like putty but once the firm figure 19 vintaging structure for capital stock with coflow for embodied factor requirements the coflow structure exactly mirrors the aging chain of the capital stock one coflow structure is used for each factor input labor energy materials etc in this model retrofits are not allowed for a more detailed model with retrofits see sterman construction delay average life of capital construction requirements requirements requirements requirements requirements of capital under construction of capital of capital n total capital stock capital vintagei i l of capital from discards n total factor requirements of capital factor requirements of vintagei i l average factor intensity of capital total factor requirements of capital total capital stock chapter coflows and aging chains commits to an investment the embodied factor requirements are fixed until that capital is discarded the factor proportions harden like clay fired in a in real ity retrofits maintenance activity and wear and tear can alter the factor require ments of existing capital the model is easily modified to incorporate such changes in factor requirements sterman develops a general model of production in cluding a variable retrofit potential which allows the modeler to specify any degree of variability in the factor requirements of existing capital from pure putty clay to full putty putty summary aging chains are widely used to capture the demographic structure of a population the population need not be a living population but can be the stock of machines in a plant the number of cars on the road or the accounts receivable of a firm any time the rate at which items exit a stock and flow network depends on their age that is any time the mortality rates of individuals in the stock are age dependent an aging chain may be required to model the situation with sufficient accuracy for the purpose of the model part iv tools for modeling dynamic systems coflows are used to keep track of the attributes of the items in a stock and flow network attributes can include the age of the items the productivity and experi ence of labor the energy requirements or level of technology embedded in plant and equipment the level of defects in product designs or any property that is as sociated with the items in the stock and flow network coflows are useful in situa tions where the qualities of the items in a system stocks as well as their quantity affect the decision making of the agents in the system modeling decision making a model for simulating dynamic system behavior requires formal policy descriptions to how individual decisions are to be made flows of information are continuously converted into decisions and actions no plea about the inadequacy of our understanding of the decision making processes can excuse us from estimating decision making criteria to omit a decision point is to deny its presence a mistake of far greater magnitude than any errors in our best estimate of the process jay w forrester pp prior chapters discussed how to represent the physical and institutional structure of systems for example how to represent stock and flow networks and select the level of aggregation this chapter explores the formulation of the decision rules representing the behavior of the agents the decision rules in models must be for mulated so that they are appropriate for the purpose of the model they must be consistent with all available knowledge about the system including numerical and qualitative data the information used in the model of a decision process must be available to the actual decision makers and all formulations must be robust so that no matter how extreme the inputs the output behaves appropriately the chapter also presents common and important formulations that conform to these principles the structure and behavior of each formulation and examples these formulations constitute a library of frequently used components from which you can assemble a larger model principles for modeling decision making the structure of all models consists of two parts assumptions about the physical and institutional environment on the one hand and assumptions about the decision part iv tools for modeling dynamic systems processes of the agents who operate in those structures on the other the physical and institutional structure of a model includes the model boundary and stock and flow structures of people material money information and so forth that char acterize the system for example forrester urban dynamics sought to understand why america large cities continued to decay despite massive amounts of aid and numerous renewal programs to do so the model represented key physi cal components of a typical city including the size and quality of the housing stock commercial structures and other infrastructure the size skill mix income and other attributes of the population the flows of people and capital into and out of the city and other factors describing the physical and institutional setting the decision processes of the agents refer to the decision rules that determine the behavior of the actors in the system the behavioral assumptions of a simula tion model describe the way in which people respond to different situations in the urban dynamics model these included decision rules governing migration and construction in another pioneering simulation study cyert and march found that department stores used a very simple decision rule to determine the floor price of goods in essence the rule was to mark up the wholesale cost of the items by a fixed percentage if excess inventory piled up on the shelves a sale was held and the markup was gradually reduced until the goods were sold if sales goals were exceeded then prices were raised prices were also adjusted toward those of competitors the normal markup was determined by tradition it adjusted very slowly toward the actual markup on the goods sold taking account of any sales or other price changes cyert and march found that these rules for pricing re produced the pricing decisions of the store managers quite accurately portraying the physical and institutional structure of a system is rel atively straightforward in contrast discovering and representing the decision rules of the actors is subtle and challenging to be useful simulation models must mimic the behavior of the real decision makers so that they respond appropriately not only for conditions observed in the past but also for circumstances never yet en countered you must specify a robust realistic decision rule at every decision point in the model decisions and decision rules modelers must make a sharp distinction between decision rules and the decisions they generate decision rules are the policies and protocols specifying how the decision maker processes available information decisions are the outcome of this process in the department store example the decision rule is the procedure for up wholesale costs and adjusting the markup based on inventory turnover the agents in models need not be human decision makers they might be other types of organisms such as wolves and moose in a predator prey model or physical objects such as the sun and planets in a model of the solar system in these cases the decision rules of the agents represent the ways in which the moose wolves and planets respond to the state of the systems in which they operate in the solar system simulation the modeler would specify the forces acting on each mass according to either newtonian gravitation or general relativity in the predator prey case the decision rules specifying the behavior of the moose and wolves fertility mortality forag ing and hunting behavior migration etc would be grounded in field and perhaps laboratory study chapter modeling decision making figure rules govern the rates of in systems decisions are the i of a rule to the available information cues the cues are by the physical and institutional structure of the system including measurement and reporting processes the output of a decision process is rate of flow that alters the state of the system competitor prices and so on the decision rule leads to decisions such as pricing a particular item at say 9 95 it is not sufficient to model a particular decision modelers must detect and represent the guiding policy that yields the stream of decisions forrester every rate of flow in the stock and flow structure constitutes a decision point and the modeler must specify precisely the decision rule determining the rate every decision rule can be thought of as an information processing procedure figure the inputs to the decision process are various types of information or cues the cues are then interpreted by the decision maker to yield the decision the cues used to revise prices in the department store case include wholesale costs inventory turnover and competitor prices decision rules do not necessarily utilize all available or potentially relevant information the mental models of the decision makers along with organizational political personal and other factors influence the selection of cues from the set of available information those cues actually used in decision making are also not necessarily processed optimally cyert and march found that department store pricing decisions did not depend on interest rates or required rates of return store overhead trade offs of holding costs against the risk of stockouts estimates of the elasticity of demand or any sophisticated strategic reasoning the decision rules in a model embody explicitly or implicitly assumptions about the degree of rationality of the decision makers and process the spectrum of possibilities is broad at one extreme some models rep resent decision makers as simple automata making decisions by rote from a small fixed repertoire of choices without any possibility of learning or adaptation at the other extreme lies the theory of rational expectations which holds that decision part iv tools for modeling dynamic systems makers understand the structure of the system perfectly never make systematic er rors in their inferences about its future behavior and therefore always make opti mal decisions muth miller lucas nobel laureate gary becker p summarized the view of many economists when he said all human behavior can be viewed as involving participants who maximize their utility from a stable set of preferences and accumulate an optimal amount of information to do so in this view not only do people make optimal decisions given the informa tion they have but they also invest exactly the optimal time and effort in the deci sion process ceasing their deliberations when the expected gain to further effort equals the cost five formulation fundamentals the nature of a decision process and its rationality are empirical questions that must be addressed by primary field study experimental tests and other means chapters 15 and discuss different views on the rationality of decision and its implications for models of human behavior but whatever your view about the sophistication and rationality of decision making your models must conform to certain basic principles table the baker criterion the inputs to all decision rules in models must be restricted to information actually available to the real decision makers in during the us senate hearings on the watergate burglary rumors flew about the possible involvement of president nixon senator howard baker a moderate republican kept the witnesses before the committee what did the president know and when did he know it his point was that the president could not be implicated in the scandal if he was unaware of the actions of his staff and subordinates when it later became clear from the white house tapes that nixon had known early on and had participated in the cover up baker had the answer to his question and called for nixon to resign you must also apply the baker criterion when formulating the decision rules in your models you must ask what do the decision makers know and when do they know it to properly mimic the behavior of a system a model can use as an input to a decision only those sources of information actually available to and used by the decision makers in the real system if managers of an oil company do not know the true size of the undiscovered resource in a basin this information cannot be used in modeling their decision to drill if ship owners do not know how many ships are under construction around the world information about this supply line cannot be used as an input to their forecasts of future rates nor their decision to ex pand their fleets if production planners do not know the current order rate they cannot use that information to set the production schedule the true size of the basin the actual supply line of ships on order and the actual order rate be pre sent in the model but information about them cannot be used as inputs to the as sumed decision rules if these data are not known by the actual decision makers the principle that decisions in models must be based on available information has three important corollaries first no one knows with certainty what the future will bring all beliefs and expectations about the future are based on experience modelers must represent the chapter modeling decision table principles for modeling human the baker criterion the inputs to all decision rules in models must be restricted to information actually available to the real decision makers the future is not known to anyone all expectations and beliefs about the future are based on historical information expectations and beliefs may therefore be incorrect actual conditions and perceived conditions differ due to measurement and reporting delays and beliefs are not updated immediately on receipt of new information perceptions often differ from the actual situation the outcomes of untried contingencies are not known expectations about what if situations that have never been experienced are based on situations that are known and may be wrong the decision rules of a model should conform to managerial practice all variables and relationships should have real world counterparts and meaning the units of measure in all equations must balance without the use of arbitrary scaling factors decision making should not be assumed to conform to any prior theory but should be investigated firsthand desired and actual conditions should be distinguished physical constraints to the realization of desired outcomes must be represented desired and actual states should be distinguished desired and actual rates of change should be distinguished 4 decision rules should be robust under extreme conditions 5 equilibrium should not be assumed equilibrium and stability may or may not emerge from the interaction of the elements of the system way in which people and update their beliefs from information about the current and past states of the system you cannot assume that decision makers have perfect knowledge of future outcomes or that forecasts are correct even on average second perceived and actual conditions often differ information about the current state of a system is generally not known instead decisions are based on de layed sampled or averaged information plant managers may have some data about the order rate but their information may differ from the actual order rate models must account for the delays and other imperfections in the measurement and reporting of information measurement and reporting not only introduce de lays but can also create bias noise error and other distortions models should rep resent the processes by which information is generated and decisions should be represented as depending on the reported information not the true state of affairs third modelers cannot assume decision makers know with certainty the out comes of contingencies they have never experienced decisions involve choosing part iv tools for modeling dynamic systems from various alternatives the choices lead to consequences people usually but not always choose the alternative they believe will yield the best outcome how ever they define it some of the alternatives may have been chosen in the past and the decision maker may have a good idea of their likely consequences but others probably most have never been tried either by the decision maker or by anyone else from whom the decision maker might learn economic theory requires firms to allocate their resources to those activities that yield the highest return for example to choose the mix of capital labor and other inputs to the production process that maximizes profit but managers in a firm don t have any direct knowledge about the productivity of most of the possi ble combinations of these activities and factor inputs would a new fax machine in accounting increase productivity more than one in purchasing should the firm buy a new automated machine tool to reduce the number of workers required no one knows with certainty because there is no experience of these situations in stead impressions about which investments and combinations of inputs might be most productive are sketchy incomplete and conjectural these impressions are gleaned over time from anecdotes observations of other organizations experi ments the firm might conduct and so on information about the true consequences of contingencies and choices that have never been realized cannot be used in mod els instead models must represent the way in which people form expectations about the likely consequences of trying new things these beliefs are often incor rect and slow to adjust to new information the decision rules of a model should conform to managerial practice every variable and parameter in a model must have a real world counterpart and should be meaningful to the actors in the real system equations must be dimensionally consistent without the addition of fudge factors or arbitrary pa rameters managers and model users arejustly suspicious of models with variables such as technical adjustment factor or parameters with units of measure such as year suspecting often correctly that they are fudge factors used only to get the model to work and lack any empirical or theoretical justification many models especially in operations research and economics assume deci sion making is optimal simulation models in contrast must mimic the way peo ple actually make their decisions warts and all modelers must study the decision processes of the actors in the field through laboratory experiments or other means you should not assume people will behave according to any a priori theory such as the assumptions in economic models that people are motivated by narrow self interest and are perfectly rational or that they are automatons and un responsive to new information desired and actual conditions should be distinguished physical con straints to the realization of desired outcomes must be represented we live in a world of disequilibrium change arises from the gaps between the de sired and actual states of affairs models should separate desired states the goals of the decision makers from the actual states of the system the decision rules in models should explain how the actors would respond to problems shortfalls chapter modeling decision pressures and other indications that things aren t where they think they should be goals are themselves dynamic and modelers often need to represent the way the actors in the system form and update their aspirations modelers should separate the desired rates of change in system states from the actual rates of change decision makers determine the desired rates of change in system states but the actual rates of change often differ due to time delays re source shortages and other physical constraints a plant manager may determine the desired rate of production from cues such as inventory and order backlogs but the actual rate of production cannot immediately respond to changes in the desired rate actual production depends on stocks of labor capital equipment and materi als along with less tangible system states including the workweek workforce ef fort and and process quality decision makers cannot instantly change these states but can only affect the decision to hire the acquisition of new equipment the rate of worker training and so on actually managerial decisions only determine the authorization of vacancies and the ordering of new equipment the actual rate of hiring and installation of new equipment depend on the availability of workers and the ability of toolmakers to produce and deliver decision rules should be robust under extreme conditions complex systems often generate behavior far from the range of historical experience in deed one purpose of modeling is to design policies that move the system into an entirely new regime of behavior to be useful the decision rules in models must behave plausibly in all circumstances not only those for which there are historical records robustness means decision rules must generate outcomes that are physi cally possible and operationally meaningful even when the inputs to those deci sions take on extreme values production can never be negative shipments from a warehouse must fall to zero when the inventory of product is zero no matter how many orders there are robustness necessarily means models will include many nonlinear relationships under normal situations a firm liquidity has no impact on employment but if cash on hand approaches zero a firm may be forced to lay off its workers even though the backlog of work is high and the firm is profitable the impact of liquidity on net hiring is highly nonlinear equilibrium should not be assumed equilibrium and stability may or may not emerge from the interaction of the elements of the system the existence and stability of any equilibria in a system emerge from the inter actions of the decision rules of the agents with the physical and institutional struc ture of the system they are characteristics of system behavior modelers should not build into their models the presumption that the system has a particular equi librium or equilibria or that any equilibria are stable instead modelers should rep resent the processes by which decision makers respond to situations in which the state of the system differs from their goals model analysis then reveals whether these decision rules interacting with one another and with the physical structure result in stable or unstable behavior these principles may seem to be nothing more than common sense it seems obvious that people can t base their decisions on information they don t have that desires are not instantly and perfectly realized and that physical impossibilities part iv tools for modeling dynamic systems are well impossible yet many models routinely violate these principles in par ticular many economic and optimization models assume the agents have complete and perfect information about the preferences of customers the production func tion governing output and other information that real managers perceive through a fog if at all many others give decision makers perfect foresight endowing peo ple with crystal balls that give them perfect knowledge of the future and the abil ity to predict how other people would behave in hypothetical situations decision makers are assumed to be concerned solely with the maximization of their personal utility or profits in the case of a firm these assumptions are used to derive the equilibrium of the system and either no dynamics are considered or the system is assumed to be stable returning swiftly and smoothly to equilibrium after a at the other end of the spectrum some models assume decisions are made by rote from a limited repertoire of options these models often show that very com plex behavior can arise from extremely simple decision rules epstein and sugarscape model develops an artificial society in which agents with very simple rules compete for resources sugar complex behavior they interpret as coalition formation trade and war arises from the interaction of the agents the re sults are fascinating and can help build understanding of the behavior of complex systems however unless the decision rules are grounded in firsthand study of ac tual decision such models have limited utility to decision makers and the correspondence between their dynamics and the behavior of real systems remains are of course exceptions many economic models are dynamic others such as many game theoretic models restrict the information available to the agents a few explore alternatives to the assumption that people are motivated by selfish utility maximization still fewer are grounded in firsthand study of decision making fully dynamic disequilibrium behavioral models grounded in primary fieldwork remain rare in economics psychologists in contrast often utilize fieldwork and experiment to study decision and have developed models that accommodate motives for action other than utility maximization such as fairness altruism revenge and others however many of these models explain single decisions at the level of the individual in static one shot decision contexts and cannot capture the dynamics of a system or organization are exceptions since discovering and understanding the decision processes of people in complex systems is often difficult it is often useful to develop models that assume different degrees of rationality to test the robustness of results to a wide range of assumptions about human behavior models of fully rational behavior can also be used to establish upper bounds for the per formance of a system and help measure the value of potential improvements in decision models such as sugarscape can generate useful ideas for further research and illustrate the well known property of complex systems that their behavior arises more from the interaction of the elements and agents with one another than from the complexity of the individual components themselves see forrester and simon chapter modeling decision making in a model of a firm supply chain and inventory management policies the inventory of finished product was increased by the production rate and decreased by the shipment rate the following formulation for production was proposed production shipments inventory shortfall inventory shortfall desired inventory inventory where production rate at which products are completed and enter inventory shipments rate at which products are shipped to customers from inventory inventory shortfall shortage or surplus of inventory relative to the desired level desired inventory inventory level the firm considers appropriate inventory actual stock of product available for shipment to customers in the same model the modeler initially proposed shipments orders 3 but then realized that inventory could become negative if orders were large enough for a long enough period the modeler then proposed the following formulation to correct the flaw shipments inventory 4 3 a model of a firm investment in capital plant assumed investment was determined by the gap between the desired level of capital stock and the current level plus replacement of worn out capital the discard rate investment capital discard rate k constructiondelay where desired capital stock k capital stock third order material delay construction delay average construction delay for capital 4 in the same model the desired stocks of capital and other factors of 5 production such as labor were determined by the solution to the profit maximization problem for a firm under perfect competition in equilibrium the marginal revenue derived from use of an additional unit of any factor of production is just balanced by its marginal cost 6 where p price of output marginal revenue part iv tools for modeling dynamic systems formulating rate equations this section presents a library of common formulations with examples for each each conforms to the guidelines above these generic structures are the building blocks from which more complex and realistic models can be built in any real pro ject you will often have to customize and elaborate them in particular every con stant in a formulation can be modeled as a variable with its own decision rules governing its evolution whether a concept can be assumed constant exogenous or must be modeled as an endogenous part of the system structure depends on the purpose and time horizon of the model fractional increase rate consider a stock s with inflow rate often the inflow is proportional to the size of the stock the stock grows at a fractional increase rate g which may be constant or variable 8 examples birth rate fractional birth rate population 9 interest due interest rate debt outstanding 10 when the fractional growth rate g is a constant the formulation reduces to the linear first order positive feedback system described in chapter 8 and generates exponential growth the growth rate can be less than zero in which case becomes the net inflow rate and g becomes the net fractional growth rate however as shown in section 3 3 it is generally preferable to separate the inflows and outflows instead of lumping them into a single net rate different decision processes and physical con straints often govern inflows and outflows and it is difficult to formulate a single net rate that is transparent and robust the net rate of change of any stock can always be calculated as an auxiliary variable from the individual inflows and outflows chapter modeling decision fractional decrease rate consider the outflow rate from a stock s the outflow is often proportional to the size of the stock the outflow can be formulated either as depending on the fractional decrease rate d or equivalently as the stock divided by the average life time l for the items in the stock examples death rate fractional death rate population lifetime 12 defaults on accounts receivable fractional default rate accounts receivable accounts time to default these examples all form linear first order negative loops and generate exponential decay with a time constant of l they are equivalent to a first order mater ial delay the fractional rates or average residence times can be variables 3 adjustment to a goal managers often seek to adjust the state of the system until it equals a goal or de sired state the simplest formulation for this negative feedback is s where discrepancy is the gap between the desired state of the system and the actual state s the adjustment time at is the average time required to close the gap examples change in price competitor price adjustment time net hiring rate desired labor delay 15 heat loss from building temperature adjustment time temperature gap outside temperature inside temperature 17 production rate perceived inventory adjustment time perceived inventory discrepancy desired inventory perceived inventory 18 desired minus actual over adjustment time is the classic linear negative feedback system and in the absence of other rates generates exponential adjustment to the goal see chapters 4 and 8 in equation 15 a firm adjusts its price to match the competition in the modeler has chosen not to represent hiring firing and quits separately but to aggregate them into a single net hiring rate the hiring delay represents the average time required to adjust the actual workforce to the desired level when net hiring is negative the firm is implicitly laying off its workers in 17 the rate of heat loss from a building depends on the temperature difference part iv tools for modeling dynamic systems between the building and the air outside and the thermal resistance or r factor of the structure often the actual state of the system is not known to the decision mak ers who rely instead on perceptions or beliefs about the state of the system see the baker criterion in these cases the discrepancy is given by the difference between the desired and perceived state of the system as in 18 note that to be robust 18 should be modified so production never becomes negative 4 the stock management structure rate normal rate adjustments when there is an outflow from a stock the adjustment rate formulation s will produce a steady state error if there is an outflow r the stock s will be in equilibrium when s r at the larger the outflow or the longer the adjustment time the greater the equilibrium shortfall will be the stock man agement structure adds the expected outflow to the stock adjustment to prevent steady state error inflow expected outflow adjustment for stock adjustment for stock s 19 20 since the instantaneous value of rates cannot be measured the expected outflow is usually formed by averaging past outflows example a manufacturing firm may set production to replace the shipments it expects to make adjusted to bring inventory in line with the desired level expected ship ments are often estimated by smoothing past shipments production expected shipments adjustment for inventory adjustment for inventory desired inventory adjustment time expected shipments rate shipment averaging time 22 23 to be fully robust the production rate must be constrained to be nonnegative even when there is far too much inventory additional adjustments can be included for example to adjust for stocks of work in process inventory backlogs of unfilled orders and so on the stock management structure is one of the most important and useful for mulations and is discussed in detail in chapter 17 5 flow resource productivity the flows affecting a stock frequently depend on resources other than the stock it self the rate is determined by a resource and the productivity of that resource rate resource productivity or equivalently 24 rate required per unit produced 25 chapter modeling decision making examples labor force average productivity people 26 customers served service average time per customer people bug generation rate code production rate error density lines of of code in production depends on the labor force and average productivity the la bor force will typically be another stock in the model varying with hiring and at trition average productivity could be constant or a variable dependent on factors such as and experience motivation fatigue and the adequacy of other factors of production such as equipment in 27 the number of service personnel and the average number of person hours required to serve each customer determine the rate at which customers are served if a call center has 20 service representatives and each call requires an average of minutes then the rate at which cus tomers are processed is 2 per minute note that productivity is the inverse of aver age time per customer in the rate at which bugs are introduced into a software product is the product of the number of lines of code the programmers write each day and the bug density 2 6 y effect of on y effect of on y effect of on y in all the formulations above parameters such as the fractional change rates time constants and productivities can be variables they often depend nonlinearly on one or more other variables a common formulation sets a variable y to its normal or reference value y multiplied by the product of various effects each a possi bly nonlinear function of a variable y effect of x on y effect of on y effect of on y the variable y can be a rate or an auxiliary that feeds into a rate the nonlinear functions are often normalized by the normal or reference value of the inputs effect of on y 30 normalization ensures that when the inputs equal their reference levels the out put y equals its reference level normalizing means the input and output of the effect of on y are both dimensionless allowing the modeler to separate the nor mal values from the effect of deviations from normal the reference levels and can be constants or variables representing equilibrium levels the desired state of the system or the values of the variables at some time in the past a common variant is rate normal fractional rate stock effect of on rate effect of on rate part iv tools for modeling dynamic systems examples workweek standard workweek effect of schedule pressure on workweek 32 effect of schedule pressure on workweek schedule pressure desired production 33 34 the workweek for an individual or group can be modeled as a standard value such as adjusted by a nonlinear function of the workload measured by schedule pressure a dimensionless ratio of desired to standard production stan dard production is the rate of output achieved given the size and productivity of the workforce at the standard workweek the pressure is upward sloping and passes through the point it also must saturate at a maximum workweek when workload is high chapter 14 discusses the formulation of non linear functions in general and the workweek function in particular reference levels can also be defined as arbitrary constants chosen by the modeler so long as they are consistent consider the following formulation for labor productivity productivity reference productivity effect of experience on productivity effect of experience on productivity average experience reference experience 36 reference productivity could be defined as the productivity of the average new employee with a month of experience in which case reference experience is son month reference productivity could equally be defined as that of a 10 year veteran in which case the reference experience level is 10 person years section 12 2 applies this formulation to the learning curve reference values can also be defined as the values the variables take on in a reference year in the model forrester formulated the birth rate as birth rate normal fractional birth rate population effect of food on births effect of material standard of living on births effect of crowding on births effect of pollution on births forrester defined the normal fractional birth rate as the world average in the refer ence year the inputs to the effects modifying births were all normalized by their values for example the input to the effect of food on births was a di mensionless index given by the level of food per capita normalized by the world average in a common form of the multiplicative formulation in is the power law or log linear model where the effects are specified as power functions of the nor malized inputs effect of x on y chapter modeling decision making where the exponents are the elasticities of y with respect to the normalized inputs if 5 a increase in boosts y by 5 substituting the ef fects into the equation for y and taking logs of both sides gives the log linear formulation the log linear model is common because it can be estimated by linear regression process point variable names for nonlinear effects in keeping with the formulation principle that every variable should have a real life meaning you should avoid technical jargon for the names of the nonlinear effects in formulations such as equation the nonlinear function in a formulation such as y should be given a name that reflects what it does or the effect it cap tures a common convention is to denote such functions by the effect of x on y yielding y normal y effect of x on y the effect of x on y is then defined separately as a nonlinear function either analytically or as a table function chap ter 14 though this convention sometimes leads to long variable names the gain in clarity is worthwhile 2 7 y effect of on y effect of on y effect of on y the additive formulation y effect of on y effect of x on y effect of x on y 40 is sometimes seen the effects of the can still be nonlinear and the reference values can be constants or variables often however the linear form is used example consider a model of the wage rate paid by a firm or industry the average wage rate is a stock that responds to a variety of pressures including the demand and sup ply of workers the expected inflation rate the firm expectation for productivity growth firm profitability and comparisons to wages paid for comparable work in other firms or industries the fractional change in wages per year can be modeled as the sum of the fractional change in wage arising from each pressure wage in wage wage 42 change in wage fractional change in wage wage 43 fractional change in wage change in wage from labor availability change in wage from inflation change in wage from productivity 44 change in wage from profitability change in wage from equity change in wage from labor availability supply demand change in wage from inflation 46 part iv tools for modeling dynamic systems change in wage from productivity productivity change in wage from profitability roi reference change in wage from equity relative wage perceived industry average wage 47 48 49 50 presumably lower labor availability higher expected inflation or productivity growth higher profitability and higher wages in other firms all increase the rate of wage growth the labor balance could be measured different ways possibly including factors such as the average time to fill vacancies the number of unfilled positions and the unemployment rate in the industry or region expecta tions for future inflation and productivity growth could depend on the history of in flation and productivity see chapter 16 profitability could be measured by return on investment roi normalized by a reference reflecting industry or wide norms finally the fractional gap between wages at comparable firms and the firm in question provides a measure of wage equity adjusting the shapes and strengths of the different nonlinear functions can capture different labor market in stitutions from collective bargaining to the market for day laborers the shapes and values of the functions for each contribution to the total fractional change in wages must be carefully chosen so that the overall response of wages is appropri ate even when the inputs take on extreme values multiplicative or additive effects when should you choose a linear for mulation such as equation 40 and when is the multiplicative formulation in 29 better linear formulations are common because linear models are simple can be solved analytically and facilitate parameter estimation by linear however the multiplicative formulation is generally preferable and some times required the actual relationship between a variable and its inputs such as between births and food health care crowding and pollution is typically complex and non linear both the multiplicative and additive formulations are approximations to the underlying true nonlinear function y x each approximation is centered on a particular operating point given by the reference point both the additive and multiplicative approximations will be rea sonable in the neighborhood of the operating point but increasingly diverge from the true underlying function as the system moves away from it the additive formulation assumes the effects of each input are strongly sepa rable the impact of a change in any one input is the same no matter what values the other inputs have strong separability is clearly incorrect in extreme conditions in the birth rate example the birth rate must be zero when food per capita is zero no matter how favorable other conditions are the additive formulation can never growth in computer power and widespread availability of nonlinear estimation routines means there is now little reason to enforce linearity for estimation purposes you should capture the the data suggest and robustness requires then estimate them with an appropriate statistical technique chapter modeling decision making capture that nonlinear effect the multiplicative formulation should be used when ever an extreme value of any input dominates all other multiple nonlinear effects in a model of urban growth an analyst finds that the migration rate to the city is proportional to the current population and that the fractional inmigration rate de pends on the perceived availability of jobs and housing and the crime rate the analyst proposes the following formulation inmigration inmigration fraction population inmigration fraction normal inmigration fraction effect of jobs on inmigration effect of housing on inmigration effect of crime on inmigration job availability is measured by the ratio of jobs available to the labor force hous ing availability is measured by the ratio of housing to households the crime rate is measured by per thousand people is the proposed formulation reasonable not sketch the likely shapes of the functions for each effect on migration so that the individual functions take on appropriate extreme values if for example there were no jobs no housing or high crime 2 is the proposed formulation robust in extreme conditions if not give an example of a situation in which the formulation would generate unrealistic results 3 reformulate the inmigration rate so it is robust to any combination of inputs 2 8 fuzzy min function often a rate or variable is determined by the most scarce of several resources for example capacity or insufficient demand can limit production production production capacity 53 or more generally y where is the capacity of the process however the sharp discontinuity created by the min function is often unreal istic many times the capacity constraint is approached gradually due to physical characteristics of the system at the micro level consider a single worker and work station in a job shop as the required rate of production increases from the normal rate the worker at first can keep pace by speeding up and reducing idle time these responses gradually experience diminishing returns until the maximum rate of linear formulation in 40 corresponds to the first terms of the taylor series for the true underlying formulation thus assuming strong separability of the individual effects the power law formulation in 39 corresponds to the first terms of the taylor series of the log of the true function and assumes multiplicative separability it is possible to include higher order terms to capture interactions but care must be used to ensure that the resulting formulation is globally robust to extreme combinations of inputs part iv tools for modeling dynamic systems figure 2 the fuzzy minimum function 0 00 0 50 oo 50 2 00 2 50 dimensionless output is reached even when a capacity constraint is sharply discontinuous at the level of the individual unit the response of the aggregate system is likely to be smooth since there will typically be a distribution of individuals around the aver age capacity level the following formulation captures a soft or fuzzy mini mum function for use in situations where the effect of a constraint is gradually felt y effect of x on y 54 55 figure 2 illustrates the function the reference line represents y in the case of production and capacity utilization the line implies utiliza tion desired which means production desired produc tion the function labeled fuzzy min in the figure captures a gradual approach to the constraint as the input rises see chapter 14 for examples and the formulation can also be expressed equivalently as rate desired rate fraction of desired rate satisfied 56 fraction of desired rate satisfied rate 0 57 here the actual rate is expressed as a fraction of the desired throughput achieved given the capacity of the process the construction and interpretation of the func tion is the same the order fulfillment ratio defined in the manufacturing supply chain models of chapter 18 provides an example 2 9 fuzzy max function the fuzzy max function is analogous to the fuzzy min function in many situa tions a variable must remain nonnegative for example hiring rate desired hiring rate common alternative formulates the utilization function so in this case the function passes through the normal point and capacity represents normal out put the actual maximum rate of output for the process is given by chapter modeling decision making 3 the fuzzy furiction the effect can lie along the wx or eventually saturate combin ing the fuzzy max and fuzzy min functions 1 5 1 0 0 5 0 0 0 5 1 0 1 5 2 0 2 5 3 0 3 5 dimensionless the desired hiring rate may be negative if the firm has far too many workers but the hiring rate can fall at most to zero net hiring can still be negative since work ers quit and can be laid off see chapter 19 the fuzzy maximum function is y effect of x on y 59 60 the fuzzy maximum function is useful in situations where individual decision makers are reluctant to cut the output y to zero as x falls for example a firm may choose to keep a chemical plant running above the desired rate when demand is low to avoid shutdown expenses the function is also useful when the model ag gregates a population with a distribution of desired and normal rates around the averages when the average desired rate is zero some members of the population will have desired rates less than zero and will be shut down while others have de sired rates greater than zero so average output is greater than zero you can com bine the fuzzy minimum and fuzzy maximum into a single effect by including a saturation for large values of as shown in fig ure 3 example applying the fuzzy maximum function to the hiring rate hiring rate normal hiring rate effect of desired hiring on hiring 61 effect of desired hiring on hiring desired hiring rate normal hiring rate 62 the desired hiring rate would include replacement of those employees who quit or retire modified by an adjustment to close any gap between the desired and actual labor force as in the stock management structure the normal hiring rate repre sents the capacity of the firm human resource organization and can be modeled most simply as a normal fraction of the labor force actual hiring gradually ap proaches zero as desired hiring falls below normal because labor requirements part iv tools for modeling dynamic systems by facility and are imperfectly correlated some hiring will be needed to fill vacancies for particular in particular departments or firms even when the av erage desired hiring rate for the firm or industry as a whole is zero at the other ex treme the human resources organization cannot hire faster than a certain rate in the example shown in figure 3 hiring gradually saturates at 2 5 times the nor mal rate when labor demand is high 2 10 floating goals many formulations respond to some measure of the discrepancy between the de sired and actual state of the system where do the goals come from in some cases goals are exogenous to the decision in the manufacturing example in section 2 4 desired inventory might depend on the firm forecast of demand and a tar get for inventory coverage often however there are no obvious external reference points to determine goals in these cases the desired state of the system is at least partially affected by the state of the system itself the goals float with the ebb and flow of the system itself consider the classical linear negative feedback system s change in stock s net change in stock 63 64 where and s are the desired and actual states of the system and sat is the stock adjustment time the formulation for the net change in the stock assumes decision makers initiate corrective actions in response to any discrepancy between desired and actual in a pure floating goal structure the desired state of the system is itself a variable change in goal net change in goal s 65 66 where gat is the goal adjustment time the goal adjustment rate forms a order linear negative feedback loop that acts to eliminate any discrepancy between the desired and actual states of the system but by eroding the goal rather than changing the world there is ample empirical support for such adaptive aspira tions habituation the tendency to get accustomed to your present circumstances is a form of goal adaptation people find the tension created by unfulfilled goals uncomfortable and often erode their goals to reduce cognitive dissonance tinger see also lant figure 4 shows a causal diagram for the pure floating goal structure the mutual dependence of the state of the system and goal form a positive feedback loop the mutual adaptation of goals and outcomes causes such systems to be path dependent a random shock that lowers the state of the system will generate cor rective actions that raise the state of the system back up but while the actual state is low the goal will be adjusted downward the system will return to equilibrium but not the original equilibrium the degree of goal erosion will depend on how fast the goal adjusts relative to the state of the system chapter modeling decision figure 4 feedback structure created by floating goals stock adjustment time goal adjustment time example recall the student workload management model in section 5 4 figure 5 25 sug gests the student desired grade point average gpa adjusts to the actual desired gpa in desired gpa desired change in desired gpa gpa desired goal adjustment time 67 68 actual achievement is more complex depending on student aptitude compared to the difficulty of the material the student motivation preparation and effort and the support encouragement and expectations of teachers parents and peers nancy roberts develops a model of stu dent teacher parent interactions in which the dynamics depend strongly on the goal formation process for each of these actors part iv tools for modeling dynamic systems 2 what is the behavior of the system when goals are flexible what is the equilibrium gpa when gat is 32 16 and 2 weeks 3 after sketching your intuitive estimates of the behavior simulate the system for the conditions above were your mental simulations correct 4 discuss the implications of flexible goals under what circumstances should goals remain absolute when should goals be adaptive give examples of floating goals from your own experience goals are often partially affected by past performance and partly by various exter nal factors in these cases the goal can be formulated as a weighted average of the various external factors and the traditional performance of the system tra ditional performance adjusts over time to actual performance 69 70 where is the adjustment time for the tradition and is the nth order information delay chapter 3 modeling a goal as a weighted average of past performance and external pres sures is consistent with the common judgmental heuristic known as anchoring and adjustment people often estimate a quantity or make a judgment by anchoring or begin ning with a known reference point then adjusting their judgment to account for factors specific to the case at hand often the adjustments are insufficient leading to bias toward the anchor judgments are often strongly anchored to information people know to be ir relevant northcraft and neale had professional real estate agents appraise the value of various houses agents received packets describing each house identical except for one piece of information the price which should be irrelevant in appraising the true market value of a house nevertheless the appraisals were significantly anchored to the price appraisers told the house was listed at gave an average appraised value of 200 ap praisers told the very same house was listed for 900 gave a mean appraised value of most of the agents denied considering the price in their judgment russo and schoemaker asked mba students for the last three digits of their phone number say xyz then added to the answer students were then asked do you think attila the hun was defeated in europe before or after the year xyz after responding the students were then asked in what year would you guess attila the hun was actually defeated the students know their phone number has nothing to do with the date of defeat yet this irrelevant anchor significantly biased their answers those for whom xyz was between and gave average estimates of the year those for whom xyz was between and gave average estimates of the year attila was actually defeated in the year chapter modeling decision floating goals are pervasive examples include company goals for quality deliv ery time and delivery reliability customer satisfaction and so on organizational norms of all types tend to adjust to past experience including norms such as dress codes the length of the workweek and the degree of civility in the organization your personal goal for your weight may be affected by the official target for your stature calculated by nutritionists but is probably strongly conditioned by your past weight and by the size of the clothes in your closet since those clothes prob ably fit you when you bought them your current weight goal is largely determined by how much you actually weighed when you bought the clothes you are wearing now similarly goals that appear to be exogenous are actually often endogenous a firm may set its goals for service quality delivery time and reliability by its competitors from the point of view of the individual firm these goals are exogenous but when all firms benchmark performance against one an other goals are actually endogenous and determined by past performance an en tire industry can suffer from goal erosion even as each firm seeks external reference points for its aspirations 2 11 nonlinear weighted average a variable is often a compromise or weighted average of several cues near a cer tain operating point but restricted by maximum or minimum limits in extreme con ditions thus requiring a nonlinear weighted average for example the weighting of internal and external factors in goal formation is often nonlinear when the ex ternal inputs to a goal are very different from the current situation people some times discount them as unrealistic suggesting that the goal is anchored nonlinearly to traditional performance effect of external factors on goal effect of external factors on goal 0 figure 5 shows a typical form for the the reference line represents the case external factors fully determine the goal the line the external factors have no role in goal formation the slope of the function is the weight on the external factors the maximum and part iv tools for modeling dynamic systems figure 5 function for a nonlinear weighted average the effect is a weighted average of and when but limited 50 as shown the weight on around the operating point is 0 00 0 50 1 00 1 50 2 00 2 50 sei dimensionless minimum values capture the idea that decision makers are unwilling or unable to set goals too far from traditional norms for performance example jones and repenning studied the dynamics of quality improvement at a ma jor motorcycle producer fieldwork revealed that the workers quality goals were strongly affected by their own experience with the product both as employees and customers many owned one or more of the firm bikes and put many miles on them each year in the the loyalty and knowledge of the employees was a major asset as the company dramatically improved the quality reliability and styling of its bikes the quality improvement contributed to a dramatic surge in de mand and renewed popularity for the firm products by the early a new type of customer became important known as rubbies rich urban bikers these new customers tended to buy the company bikes more for status value than trans portation the rubbies didn t actually ride much and were far more sensitive to mi nor cosmetic issues related to the paint and finish than the company traditional blue collar customers including their own workers as customer standards and warranty claims increased management aggressively boosted goals for finish qual ity some workers initially resisted these new higher standards one manufactur ing manager said sometimes customer standards are too high he s an artist or a banker and expects everything to be perfect an engineer commented i d of like to drop cosmetics as a quality problem that is more of a subjective of quality problem what the customer expects versus what we are producing i don t even want to talk about them because half of the cosmetic defects that will turn up in paint or chrome you won t see another commented we don t build motorcycles we build jewelry the engineers viewed external data such as market research or warranty claims as unreliable or irrelevant data from the customers is too subjective we don t use it to learn about quality improvement instead employee quality goals were strongly anchored on their own experience when asked how he learned about the quality of the product one engineer said by buying them by hanging around in the field i use our product extensively chapter modeling decision making the tendency to discount evidence significantly different from their own stan dards suggests the employees quality goals were flexible only up to a point con sistent with the nonlinear anchoring and adjustment formulation employee quality goal traditional quality effect of customer feedback on quality goal effect of customer feedback on quality goal quality quality 73 74 the traditional quality goal responded to past quality and emphasized mechanical functionality rather than cosmetics customer goals for quality once similar to the employees goals had risen and now emphasized cosmetic concerns the inter view data suggested the effect of customer feedback on quality goal had a slope much less than 1 near the neutral point and saturated for high values of the input boosting the operational goal for quality required changing employees percep tions of what quality should be not merely applying additional pressure for higher quality management used the results of the jones and repenning study along with other work to make significant changes in product development and the manufacturing relationship helping to lift quality goals and quality throughout the organization by the end of the quality was trending up while sales con tinued at record rates 13 2 12 modeling search hill climbing optimization in many situations decision makers strive to optimize a system but lack knowledge of the system structure that might help them identify the optimal operating point for example a firm may seek the price that maximizes profit or the mix of labor and capital that minimizes the cost of production a variant of floating goals en ables the agents in models to find the optimum point in such cases in a behav iorally realistic way since firms for example do not know what the optimal mix of labor and capital is they must find it by sensing whether there is too much labor or too much capital then adjusting the mix in the right direction such a procedure is called hill climbing imagine yourself trying to climb a mountain in a complete whiteout visibility is zero you have no idea which way will lead you to the summit you carefully take one step in each direction to see which way the ground around you slopes then strike out in the direction that leads most steeply uphill every few steps you stop and take a step in each direction to reassess which way is uphill you continue in this fashion stopping only when every direction you can move takes you down hill if the mountain is smooth has only a single peak and you reassess the gradi ent often enough you are guaranteed to make the summit even though you have no idea where the summit 