arrays an array stores a sequence of values that are all of the same type we want not only to store values but also to access each individual value the method that we use to refer to individual values in an array is numbering and then indexing them if we have n values we think of them as being numbered from to n then we can unambiguously specify one of them in java code by using the notation a i to refer to the ith value for any value of i from to n this java construct is known as a one dimensional array creating and initializing an array making an array in a java program involves three distinct steps declare the array name and type create the array initialize the array values to declare the array you need to specify a name and the type of data it will contain to create it you need to specify its length the number of values for example the long form code shown at right makes an array of n numbers of type double all initialized to the first statement is long form double a declaration creation the array declaration it is just like a dec laration of a variable of the correspond ing primitive type except for the square brackets following the type name which specify that we are declaring an array the keyword new in the second state ment is a java directive to create the ar ray the reason that we need to explicitly create arrays at run time is that the java compiler cannot know how much space a new double n for int i i n i a i initialization short form double a new double n initializing declaration int a declaring creating and initializing an array to reserve for the array at compile time as it can for primitive type values the for statement initializes the n array values this code sets all of the array entries to the value when you begin to write code that uses an array you must be sure that your code declares creates and initializes it omitting one of these steps is a common program ming mistake short form for economy in code we often take advantage of java default array ini tialization convention and combine all three steps into a single statement as in the short form code in our example the code to the left of the equal sign constitutes the declaration the code to the right constitutes the creation the for loop is unnecessary in this case because the default initial value of variables of type double in a java array is but it would be required if a nonzero value were desired the default initial value is zero for numeric types and false for type boolean the third option shown for our example is to specify the initialization values at compile time by listing literal values between curly braces separated by commas using an array typical array processing code is shown on page after declaring and creating an array you can refer to any individual value anywhere you would use a variable name in a program by enclosing an integer index in square brackets after the array name once we create an array its size is fixed a program can refer to the length of an array a with the code a length the last element of an array a is always a a length java does automatic bounds checking if you have created an array of size n and use an index whose value is less than or greater than n your pro gram will terminate with an arrayoutofboundsexception runtime exception aliasing note carefully that an array name refers to the whole array if we assign one array name to another then both refer to the same array as illustrated in the following code fragment int a new int n a i int b a b i a i is now this situation is known as aliasing and can lead to subtle bugs if your intent is to make a copy of an array then you need to declare create and initialize a new array and then copy all of the entries in the original array to the new array as in the third example on page two dimensional arrays a two dimensional array in java is an array of one dimen sional arrays a two dimensional array may be ragged its arrays may all be of differing lengths but we most often work with for appropriate parameters m and n m by n two dimensional arrays that are arrays of m rows each an array of length n so it also makes sense to refer to the array as having n columns extending java array constructs to handle two dimensional arrays is straightforward to refer to the entry in row i and column j of a two dimensional array a we use the notation a i j to declare a two dimensional array we add another pair of square brackets and to create the array we specify the number of rows followed by the number of columns after the type name both within square brackets as follows double a new double m n we refer to such an array as an m by n array by convention the first dimension is the number of rows and the second is the number of columns as with one dimensional arrays java initializes all entries in arrays of numeric types to zero and in arrays of boolean values to false default initialization of two dimensional arrays is useful because it masks more code than for one dimensional arrays the following code is equivalent to the single line create and initialize idiom that we just considered double a a new double m n for int i i m i for int j j n j a i j this code is superfluous when initializing to zero but the nested for loops are needed to initialize to other value task implementation code fragment find the maximum of the array values double max a for int i i a length i if a i max max a i compute the average of the array values int n a length double sum for int i i n i sum a i double average sum n copy to another array int n a length double b new double n for int i i n i b i a i reverse the elements within an array int n a length for int i i n i double temp a i a i a n i a n i temp matrix matrix multiplication square matrices a b c int n a length double c new double n n for int i i n i for int j j n j compute dot product of row i and column j for int k k n k c i j a i k b k j typical array processing code static methods every java program in this book is either a data type definition which we describe in detail in section or a library of static methods which we de scribe here static methods are called functions in many programming languages since they can behave like mathematical functions as described next each static method is a sequence of statements that are executed one after the other when the static method is called in the manner described below the modifier static distinguishes these meth ods from instance methods which we discuss in section we use the word method without a modifier when describing characteristics shared by both kinds of methods defining a static method a method encapsulates a computation that is defined as a sequence of statements a method takes arguments values of given data types and computes a return value of some data type that depends upon the arguments such as a value defined by a mathematical function or causes a side effect that depends on the arguments such as printing a value the static method rank in binarysearch signature local variables method body return type method name argument type argument variable is an example of the first main is an ex ample of the second each static method is composed of a signature the keywords public static followed by a return type the method name and a sequence of ar guments each with a declared type and a body a statement block a sequence of statements enclosed in curly braces ex amples of static methods are shown in the table on the facing page return statement call on another method invoking a static method a call on a static anatomy of a static method method is its name followed by expressions that specify argument values in parenthe ses separated by commas when the method call is part of an expression the method computes a value and that value is used in place of the call in the expression for ex ample the call on rank in binarysearch returns an int value a method call followed by a semicolon is a statement that generally causes side effects for example the call arrays sort in main in binarysearch is a call on the system method arrays sort that has the side effect of putting the entries in the array in sorted order when a method is called its argument variables are initialized with the values of the corresponding expressions in the call a return statement terminates a static method returning control to the caller if the static method is to compute a value that value must be specified in a return statement if such a static method can reach the end of its sequence of statements without a return the compiler will report the error task implementation absolute value of an int value public static int abs int x if x return x else return x absolute value of a double value public static double abs double x if x return x else return x public static boolean isprime int n primality test if n return false for int i i i n i if n i return false return true public static double sqrt double c square root newton method if c return double nan double err double t c while math abs t c t err t t c t t return t hypotenuse of a right triangle public static double hypotenuse double a double b return math sqrt a a b b harmonic number see page public static double h int n double sum for int i i n i sum i return sum typical implementations of static methods properties of methods a complete detailed description of the properties of methods is beyond our scope but the following points are worth noting arguments are passed by value you can use argument variables anywhere in the code in the body of the method in the same way you use local variables the only difference between an argument variable and a local variable is that the argument variable is initialized with the argument value provided by the call ing code the method works with the value of its arguments not the arguments themselves one consequence of this approach is that changing the value of an argument variable within a static method has no effect on the calling code gen erally we do not change argument variables in the code in this book the pass by value convention implies that array arguments are aliased see page the method uses the argument variable to refer to the caller array and can change the contents of the array though it cannot change the array itself for example arrays sort certainly changes the contents of the array passed as argument it puts the entries in order method names can be overloaded for example the java math library uses this approach to provide implementations of math abs math min and math max for all primitive numeric types another common use of overload ing is to define two different versions of a function one that takes an argument and another that uses a default value of that argument a method has a single return value but may have multiple return statements a java method can provide only one return value of the type declared in the method signature control goes back to the calling program as soon as the first return statement in a static method is reached you can put return statements wherever you need them even though there may be multiple return statements any static method returns a single value each time it is invoked the value follow ing the first return statement encountered a method can have side effects a method may use the keyword void as its return type to indicate that it has no return value an explicit return is not necessary in a void static method control returns to the caller after the last statement a void static method is said to produce side effects consume input produce output change entries in an array or otherwise change the state of the system for example the main static method in our programs has a void return type because its purpose is to produce output technically void methods do not implement mathematical functions and neither does math random which takes no arguments but does produce a return value the instance methods that are the subject of section share these properties though profound differences surround the issue of side effects recursion a method can call itself if you are not comfortable with this idea known as recursion you are encouraged to work exercises through for ex ample the code at the bottom of this page gives an alternate implementation of the rank method in binarysearch we often use recursive implementations of methods because they can lead to compact elegant code that is easier to understand than a cor responding implementation that does not use recursion for example the comment in the implementation below provides a succinct description of what the code is sup posed to do we can use this comment to convince ourselves that it operates correctly by mathematical induction we will expand on this topic and provide such a proof for binary search in section there are three important rules of thumb in developing recursive programs the recursion has a base case we always include a conditional statement as the first statement in the program that has a return recursive calls must address subproblems that are smaller in some sense so that recursive calls converge to the base case in the code below the difference between the values of the fourth and the third arguments always decreases recursive calls should not address subproblems that overlap in the code below the portions of the array referenced by the two subproblems are disjoint violating any of these guidelines is likely to lead to incorrect results or a spectacularly inefficient program see exercises and adhering to them is likely to lead to a clear and correct program whose performance is easy to understand another reason to use recursive methods is that they lead to mathematical models that we can use to understand performance we address this issue for binary search in section and in several other instances throughout the book basic programming model a library of static methods is a set of static methods that are defined in a java class by creating a file with the keywords public class followed by the class name followed by the static methods enclosed in braces kept in a file with the same name as the class and a java extension a basic model for java programming is to develop a program that addresses a specific computational task by creating a li brary of static methods one of which is named main typing java followed by a class name followed by a sequence of strings leads to a call on main in that class with an array containing those strings as argument after the last statement in main executes the program terminates in this book when we talk of a java program for accomplishing a task we are talking about code developed along these lines possibly also including a data type definition as described in section for example binarysearch is a java program composed of two static methods rank and main that accomplishes the task of printing numbers on an input stream that are not found in a whitelist file given as command line argument modular programming of critical importance in this model is that libraries of stat ic methods enable modular programming where we build libraries of static methods modules and a static method in one library can call static methods defined in other libraries this approach has many important advantages it allows us to work with modules of reasonable size even in program involving a large amount of code share and reuse code without having to reimplement it easily substitute improved implementations develop appropriate abstract models for addressing programming problems localize debugging see the paragraph below on unit testing for example binarysearch makes use of three other independently developed librar ies our stdin and in library and java arrays library each of these libraries in turn makes use of several other libraries unit testing a best practice in java programming is to include a main in every li brary of static methods that tests the methods in the library some other programming languages disallow multiple main methods and thus do not support this approach proper unit testing can be a significant programming challenge in itself at a minimum every module should contain a main method that exercises the code in the module and provides some assurance that it works as a module matures we often refine the main method to be a development client that helps us do more detailed tests as we develop the code or a test client that tests all the code extensively as a client becomes more complicated we might put it in an independent module in this book we use main to help illustrate the purpose of each module and leave test clients for exercises external libraries we use static methods from four different kinds of libraries each requiring slightly differing procedures for code reuse most of these are libraries of static methods but a few are data type definitions that also include some static methods the standard system libraries java lang these include math which contains methods for commonly used mathematical functions integer and double which we use for converting between strings of characters and int and double values string and stringbuilder which we discuss in detail later in this section and in chapter and dozens of other libraries that we do not use imported system libraries such as java util arrays there are thousands of such libraries in a standard java release but we make scant use of them in this book an import statement at the beginning of the program is needed to use such libraries and signal that we are doing so other libraries in this book for example another program can use rank in binarysearch to use such a program down load the source from the booksite into your working directory the standard libraries std that we have developed for use in this book and our introductory book an introduction to programming in java an interdisciplinary approach these libraries are summarized in the following several pages source code and instructions for downloading them are available on the booksite to invoke a method from another library one in the same directory or a specified directory a standard system library or a system library that is named in an import statement before the class definition we prepend the library name to the method name for each call for ex ample the main method in binarysearch calls the sort method in the system library java util arrays the readints method in our library in and the println method in our library stdout standard system libraries math integer double string stringbuilder system imported system libraries java util arrays our standard libraries stdin stdout stddraw stdrandom stdstats in out data type definitions that include some static methods libraries with static methods used in this book libraries of methods implemented by ourselves and by others in a modular programming environment can vastly expand the scope of our programming model beyond all of the libraries available in a standard java release thousands more are avail able on the web for applications of all sorts to limit the scope of our programming model to a manageable size so that we can concentrate on algorithms we use just the libraries listed in the table at right on this page with a subset of their methods listed in apis as described next apis a critical component of modular programming is documentation that explains the operation of library methods that are intended for use by others we will consis tently describe the library methods that we use in this book in application programming interfaces apis that list the library name and the signatures and short descriptions of each of the methods that we use we use the term client to refer to a program that calls a method in another library and the term implementation to describe the java code that implements the methods in an api example the following example the api for commonly used static methods from the standard math library in java lang illustrates our conventions for apis public class math static double abs double a absolute value of a static double max double a double b maximum of a and b static double min double a double b minimum of a and b note abs max and min are defined also for int long and float static double sin double theta sine function static double cos double theta cosine function static double tan double theta tangent function note angles are expressed in radians use todegrees and toradians to convert note use asin acos and atan for inverse functions static double exp double a exponential e a static double log double a natural log loge a or ln a static double pow double a double b raise a to the bth power ab static double random random number in static double sqrt double a square root of a static double e value of e constant static double pi value of constant see booksite for other available functions api for java mathematics library excerpts these methods implement mathematical functions they use their arguments to com pute a value of a specified type except random which does not implement a math ematical function because it does not take an argument since they all operate on double values and compute a double result you can consider them as extending the double data type extensibility of this nature is one of the characteristic features of modern programming languages each method is described by a line in the api that specifies the information you need to know in order to use the method the math li brary also defines the precise constant values pi for and e for e so that you can use those names to refer to those constants in your programs for example the value of math sin math pi is and the value of math log math e is because math sin takes its argument in radians and math log implements the natural logarithm function java libraries extensive online descriptions of thousands of libraries are part of every java release but we excerpt just a few methods that we use in the book in order to clear ly delineate our programming model for example binarysearch uses the sort method from java arrays library which we document as follows public class arrays static void sort int a put the array in increasing order note this method is defined also for other primitive types and object excerpt from java arrays library java util arrays the arrays library is not in java lang so an import statement is needed to use it as in binarysearch actually chapter of this book is devoted to implementations of sort for arrays including the mergesort and quicksort algorithms that are imple mented in arrays sort many of the fundamental algorithms that we consider in this book are implemented in java and in many other programming environments for example arrays also includes an implementation of binary search to avoid confusion we generally use our own implementations although there is nothing wrong with using a finely tuned library implementation of an algorithm that you understand our standard libraries we have developed a number of libraries that provide useful functionality for introductory java programming for scientific applications and for the development study and application of algorithms most of these libraries are for input and output we also make use of the following two libraries to test and analyze our implementations the first extends math random to allow us to draw random values from various distributions the second supports statistical calculations public class stdrandom static void initialize long seed initialize static double random real between and static int uniform int n integer between and n static int uniform int lo int hi integer between lo and hi static double uniform double lo double hi real between lo and hi static boolean bernoulli double p true with probability p static double gaussian normal mean std dev static double gaussian double m double normal mean m std dev static int discrete double a i with probability a i static void shuffle double a randomly shuffle the array a note overloaded implementations of shuffle are included for other primitive types and for object api for our library of static methods for random numbers public class stdstats static double max double a largest value static double min double a smallest value static double mean double a average static double var double a sample variance static double stddev double a sample standard deviation static double median double a median api for our library of static methods for data analysis the initialize method in stdrandom allows us to seed the random number gen erator so that we can reproduce experiments involving random numbers for reference implementations of many of these methods are given on page some of these methods are extremely easy to implement why do we bother including them in a library an swers to this question are standard for well designed libraries they implement a level of abstraction that allow us to focus on implement ing and testing the algorithms in the book not generating random objects or calculating statistics client code that uses such methods is clearer and easier to understand than homegrown code that does the same calculation library implementations test for exceptional conditions cover rarely encoun tered situations and submit to extensive testing so that we can count on them to operate as expected such implementations might involve a significant amount of code for example we often want implementations for various types of data for example java arrays library includes multiple overloaded implementa tions of sort one for each type of data that you might need to sort these are bedrock considerations for modular programming in java but perhaps a bit overstated in this case while the methods in both of these libraries are essentially self documenting and many of them are not difficult to implement some of them represent interesting algorithmic exercises accordingly you are well advised to both study the code in stdrandom java and stdstats java on the booksite and to take advantage of these tried and true implementations the easiest way to use these libraries and to examine the code is to download the source code from the booksite and put them in your working directory various system dependent mechanisms for using them with out making multiple copies are also described on the booksite your own libraries it is worthwhile to consider every program that you write as a li brary implementation for possible reuse in the future write code for the client a top level implementation that breaks the computa tion up into manageable parts articulate an api for a library or multiple apis for multiple libraries of static methods that can address each part develop an implementation of the api with a main that tests the methods independent of the client not only does this approach provide you with valuable software that you can later reuse but also taking advantage of modular programming in this way is a key to suc cessfully addressing a complex programming task intended result implementation random double value in a b public static double uniform double a double b return a stdrandom random b a random int value in n public static int uniform int n return int stdrandom random n random int value in lo hi public static int uniform int lo int hi return lo stdrandom uniform hi lo random int value drawn from discrete distribution i with probability a i public static int discrete double a entries in a must sum to double r stdrandom random double sum for int i i a length i sum sum a i if sum r return i return public static void shuffle double a randomly shuffle the elements in an array of double values see exercise int n a length for int i i n i exchange a i with random element in a i n int r i stdrandom uniform n i double temp a i a i a r a r temp implementations of static methods in stdrandom library the purpose of an api is to separate the client from the implementation the client should know nothing about the implementation other than information given in the api and the implementation should not take properties of any particular client into account apis enable us to separately develop code for various purposes then reuse it widely no java library can contain all the methods that we might need for a given computation so this ability is a crucial step in addressing complex programming ap plications accordingly programmers normally think of the api as a contract between the client and the implementation that is a clear specification of what each method is to do our goal when developing an implementation is to honor the terms of the contract often there are many ways to do so and separating client code from implementation code gives us the freedom to substitute new and improved implementations in the study of algorithms this ability is an important ingredient in our ability to understand the impact of algorithmic improvements that we develop strings a string is a sequence of characters char values a literal string is a sequence of characters within double quotes such as hello world the data type string is a java data type but it is not a primitive type we consider string now be cause it is a fundamental data type that almost every java program uses concatenation java has a built in concatenation operator for string like the built in operators that it has for primitive types justifying the addition of the row in the table below to the primitive type table on page the result of concatenating two string values is a single string value the first string followed by the second typical expressions type set of values typical literals operators expression value string character sequences ab hello concatenate hi bob hi bob java string data type conversion two primary uses of strings are to convert values that we can enter on a keyboard into data type values and to convert data type values to values that we can read on a display java has built in operations for string to facilitate these operations in particular the language includes libraries integer and double that contain static methods to convert between string values and int values and between string values and double values respectively public class integer static int parseint string convert to an int value static string tostring int i convert i to a string value public class double static double parsedouble string convert to a double value static string tostring double x convert x to a string value apis for conversion between numbers and string values automatic conversion we rarely explicitly use the static tostring methods just described because java has a built in mechanism that allows us to convert from any data type value to a string value by using concatenation if one of the arguments of is a string java automatically converts the other argument to a string if it is not already a string beyond usage like the square root of is math sqrt this mechanism enables conversion of any data type value to a string by concatenat ing it with the empty string command line arguments one important use of strings in java programming is to enable a mechanism for passing information from the command line to the program the mechanism is simple when you type the java command followed by a library name followed by a sequence of strings the java system invokes the main method in that library with an array of strings as argument the strings typed after the library name for example the main method in binarysearch takes one command line argument so the system creates an array of size one the program uses that value args to name the file containing the whitelist for use as the argument to in readints an other typical paradigm that we often use in our code is when a command line argu ment is intended to represent a number so we use parseint to convert to an int value or parsedouble to convert to a double value computing with strings is an essential component of modern computing for the moment we make use of string just to convert between external representation of numbers as sequences of characters and internal representation of numeric data type values in section we will see that java supports many many more operations on string values that we use throughout the book in section we will examine the internal representation of string values and in chapter we consider in depth al gorithms that process string data these algorithms are among the most interesting intricate and impactful methods that we consider in this book input and output the primary purpose of our standard libraries for input out put and drawing is to support a simple model for java programs to interact with the outside world these libraries are built upon extensive capabilities that are available in java libraries but are generally much more complicated and much more difficult to learn and use we begin by briefly reviewing the model standard input command line arguments standard output file i o standard drawing a bird eye view of a java program in our model a java program takes input values from command line arguments or from an abstract stream of characters known as the standard input stream and writes to another abstract stream of characters known as the standard output stream necessarily we need to consider the interface between java and the operating system so we need to briefly dis cuss basic mechanisms that are provided by most modern operating systems and program development environ ments you can find more details about your particular system on the booksite by default command line argu ments standard input and standard output are associated with an application supported by either the operating system or the program develop ment environment that takes commands we use the generic term terminal window to refer to the window maintained by this application where we type and read text since early unix systems in the this model has proven to be a convenient and direct way for us to interact with our programs and data we add to the classical model a standard drawing that allows us to create visual representations for data analysis commands and arguments in the terminal window we see a prompt where we type commands to the operating system that may take arguments we use only a few com mands in this book shown in the table below most often we use the java com mand to run our programs as mentioned on page java classes have a main static method that takes a string array args as its argument that array is the sequence of command line arguments that we type provided to java by the operating system by convention both java and the operating system process the arguments as strings if java class file name no extension run java program and command line arguments we intend for an argument to be a number we use a method more any text file name print file contents typical operating system commands such as integer parseint to convert it from string to the appropriate type standard output our stdout library provides sup port for standard output by default the system con nects standard output to the terminal window the print method puts its argument on standard out prompt call the static method main in randomseq java randomseq args put the println method adds a newline and the printf method supports formatted output as de invoke java runtime args args scribed next java provides a similar method in its system out library we use stdout to treat standard input and standard output in a uniform manner and to provide a few technical improvements public class stdout static void print string print anatomy of a command static void println string print followed by newline static void println print a new line static void printf string f formatted print note overloaded implementations are included for primitive types and for object api for our library of static methods for standard output to use these methods download into your working directory stdout java from the booksite and use code such as stdout println hello world to call them a sample client is shown at right formatted output in its simplest form printf takes two arguments the first argument is a format string that describes how the second argu ment is to be converted to a string for output the simplest type of format string begins with and ends with a one letter conversion code the conversion codes that we use most frequently are d for decimal values from java integer types f for floating point values and for string values between the and the conversion code is an integer value that specifies the field width of the converted value the number of characters in the converted output string by default blank spaces are added on the left to make the length of the converted output equal to the field width if we want the spaces on the right we can insert a minus sign before the field width if the converted output string is bigger than the field width the field width is ignored following the width we have the option of including a period followed by the number of digits to put after the decimal point the precision for a double value or the number of characters to take from the beginning of the string for a string value the most important thing to remember about using printf is that the conversion code in the format and the type of the corresponding argument must match that is java must be able to convert from the type of the argument to the type required by the con version code the first argument of printf is a string that may contain characters other than a format string any part of the argument that is not part of a format string passes through to the output with the format string replaced by the argument value converted to a string as specified for example the statement stdout printf pi is approximately n math pi prints the line pi is approximately note that we need to explicitly include the newline character n in the argument in order to print a new line with printf the printf function can take more than two arguments in this case the format string will have a format specifier for each ad ditional argument perhaps separated by other characters to pass through to the out put you can also use the static method string format with arguments exactly as just described for printf to get a formatted string without printing it formatted printing is a convenient mechanism that allows us to develop compact code that can produce tabulated experimental data our primary use in this book type code typical literal sample format strings converted string values for output int d double f e 1680010754388 string hello world 5s hello world hello world hello format conventions for printf see the booksite for many other options standard input our stdin library takes data from the standard input stream that may be empty or may contain a sequence of values sepa rated by whitespace spaces tabs newline characters and the like by default the system connects stan dard output to the terminal win dow what you type is the input stream terminated by ctrl d or ctrl z depending on your termi nal window application each value is a string or a value from one of java primitive types one of the key features of the standard input stream is that your program consumes values when it reads them once your program has read a value it cannot back up and read it again this assumption is restrictive but it reflects physical characteristics of some input devices and simplifies implementing the abstrac tion within the input stream model the static methods in this li brary are largely self documenting described by their signatures public class stdin static boolean isempty true if no more values false otherwise static int readint read a value of type int static double readdouble read a value of type double static float readfloat read a value of type float static long readlong read a value of type long static boolean readboolean read a value of type boolean static char readchar read a value of type char static byte readbyte read a value of type byte static string readstring read a value of type string static boolean hasnextline is there another line in the input stream static string readline read the rest of the line static string readall read the rest of the input stream api for our library of static methods for standard input redirection and piping standard input and output enable us to take advantage of command line extensions supported by many operating systems by adding a simple directive to the command that invokes a program we can redirect its standard output to a file either for permanent storage or for input to another program at a later time java randomseq data txt this command specifies that the standard output stream is not to be printed in the ter minal window but instead is to be written to a text file named data txt each call to stdout print orstdout println redirecting from a file to standard input java average data txt data txt redirecting standard output to a file java randomseq data txt data txt piping the output of one program to the input of another java randomseq java average redirection and piping from the command line appends text at the end of that file in this example the end result is a file that contains random values no out put appears in the terminal window it goes directly into the file named after the symbol thus we can save away information for later retrieval not that we do not have to change randomseq in any way it is using the standard out put abstraction and is unaffected by our use of a different implementation of that abstraction similarly we can redi rect standard input so that stdin reads data from a file instead of the terminal application java average data txt this command reads a sequence of numbers from the file data txt and computes their average value specifi cally the symbol is a directive that tells the operating system to implement the standard input stream by reading from the text file data txt instead of waiting for the user to type something into the terminal window when the program calls stdin readdouble the operating system reads the value from the file combining these to redirect the output of one program to the input of another is known as piping java randomseq java average this command specifies that standard output for randomseq and standard input for average are the same stream the effect is as if randomseq were typing the numbers it generates into the terminal window while average is running this difference is pro found because it removes the limitation on the size of the input and output streams that we can process for example we could replace in our example with even though we might not have the space to save a billion numbers on our computer we do need the time to process them when randomseq calls stdout println a string is added to the end of the stream when average calls stdin readint a string is removed from the beginning of the stream the timing of precisely what happens is up to the operating system it might run randomseq until it produces some output and then run average to consume that output or it might run average until it needs some output and then run randomseq until it produces the needed output the end result is the same but our programs are freed from worrying about such details because they work solely with the standard input and standard output abstractions input and output from a file our in and out libraries provide static methods that implement the abstraction of reading from and writing to a file the contents of an ar ray of values of a primitive type or string we use readints readdoubles and readstrings in the in library and writeints writedoubles and writestrings in the out library the named argument can be a file or a web page for example this ability allows us to use a file and standard input for two different pur poses in the same program as in binarysearch the in and out libraries also imple ment data types with instance methods that allow us the more general ability to treat multiple files as input and output streams and web pages as input streams so we will revisit them in section public class in static int readints string name read int values static double readdoubles string name read double values static string readstrings string name read string values public class out static void write int a string name write int values static void write double a string name write double values static void write string a string name write string values note other primitive types are supported note stdin and stdout are supported omit name argument apis for our static methods for reading and writing arrays standard drawing basic methods up to this point our input output abstractions have focused exclusively on text strings now we introduce an abstraction for producing drawings as output this library is easy to use and allows us to take advantage of a visual medi um to cope with far more information than is possible with just text as with standard input output our stan dard drawing abstraction is implemented in a library stddraw that you can access by downloading the file stddraw java from the booksite into your working directory standard draw is very simple we imagine an abstract drawing device capable of drawing lines and points on a two dimensional canvas the device is ca pable of responding to the commands to draw basic geometric shapes that our programs issue in the form of calls to static methods in stddraw including meth ods for drawing lines points text strings circles rect angles and polygons like the methods for standard input and standard output these methods are nearly self documenting stddraw line draws a straight stddraw point stddraw line stddraw circle x y r stddraw square x y r line segment connecting the point x y with the point whose coordinates are given as arguments stddraw point draws a spot centered on the point x y whose coordinates are given as arguments and so forth as illustrated in the diagrams at right geometric shapes can be filled in black by default the default scale is the unit square all coordinates are between and the standard implementation displays the can vas in a window on your computer screen with black lines and points on a white background double x double y stddraw polygon x y stddraw examples public class stddraw static void line double double double double static void point double x double y static void text double x double y string static void circle double x double y double r static void filledcircle double x double y double r static void ellipse double x double y double rw double rh static void filledellipse double x double y double rw double rh static void square double x double y double r static void filledsquare double x double y double r static void rectangle double x double y double rw double rh static void filledrectangle double x double y double rw double rh static void polygon double x double y static void filledpolygon double x double y api for our library of static methods for standard drawing drawing methods standard drawing control methods the library also includes methods to change the scale and size of the canvas the color and width of the lines the text font and the timing of drawing for use in animation as arguments for setpencolor you can use one of the predefined colors black blue cyan gray green magenta orange pink red white and yellow that are de fined as constants in stddraw so we refer to one of them with code like stddraw red the window also includes a menu option to save your drawing to a file in a format suitable for publishing on the web public class stddraw static void setxscale double double reset x range to static void setyscale double double reset y range to static void setpenradius double r set pen radius to r static void setpencolor color c set pen color to c static void setfont font f set text font to f static void setcanvassize int w int h set canvas to w by h window static void clear color c clear the canvas color it c static void show int dt show all pause dt milliseconds api for our library of static methods for standard drawing control methods in this book we use stddraw for data analysis and for creating visual representations of algorithms in operation the table at on the opposite page indicates some possibli ties we will consider many more examples in the text and the exercises throughout the book the library also supports animation of course this topic is treated primarily on the booksite data plot implementation code fragment result function values int n stddraw setxscale n stddraw setyscale n n stddraw setpenradius for int i i n i stddraw point i i stddraw point i i i stddraw point i i math log i array of random values int n double a new double n for int i i n i a i stdrandom random for int i i n i double x i n double y a i double rw n double rh a i stddraw filledrectangle x y rw rh sorted array of random values int n double a new double n for int i i n i a i stdrandom random arrays sort a for int i i n i double x i n double y a i double rw n double rh a i stddraw filledrectangle x y rw rh stddraw plotting examples binary search the sample java program that we started with shown on the facing page is based on the famous effective and widely used binary search algorithm this example is a prototype of the way in which we will examine new algorithms throughout the book as with all of the programs we consider it is both a precise definition of the method and a complete java implementation that you can download from the booksite binary search we will study the binary search algorithm in detail in section but a brief description is appropriate here the algorithm is implemented in the static successful search for lo mid hi lo mid hi lo mid hi unsuccessful search for lo mid hi lo mid hi 68 lo mid hi 54 68 lo mid hi method rank which takes an integer key and a sorted array of int values as arguments and re turns the index of the key if it is present in the array otherwise it accomplishes this task by maintaining variables lo and hi such that the key is in a lo hi if it is in the array then entering into a loop that tests the middle entry in the in terval at index mid if the key is equal to a mid the return value is mid otherwise the method cuts the interval size about in half looking at the left half if the key is less than a mid and at the right half if the key is greater than a mid the process terminates when the key is found or the interval is empty binary search is effective because it needs to examine just a few ar 54 68 84 hi lo 54 68 84 binary search in an ordered array ray entries relative to the size of the array to find the key or determine that it is not there tinyw txt 84 68 tinyt txt development client for every algorithm implementation we include a development client main that you can use with sample input files provided in the book and on the booksite to learn about the algorithm and to test its performance in this example the client reads integers from the file named on the command line then prints any integers on standard input that do not appear in the file we use small test files such as those shown at right to demonstrate this behavior and as the not in basis for traces and examples such as those at left above we use large test files to model real world applications and to test performance see page small test files for binarysearch test client binary search import java util arrays public class binarysearch public static int rank int key int a array must be sorted int lo int hi a length while lo hi key is in a lo hi or not present int mid lo hi lo if key a mid hi mid else if key a mid lo mid else return mid return public static void main string args int whitelist in readints args arrays sort whitelist while stdin isempty read key print if not in whitelist int key stdin readint if rank key whitelist stdout println key this program takes the name of a whitelist file a sequence of integers as argument and filters any entry that is on the whitelist from standard input leaving only integers that are not on the whitelist on standard output it uses the binary search algorithm implemented in the static method rank to accomplish the task efficiently see sec tion for a full discussion of the binary search algorithm its correctness its per formance analysis and its applications whitelisting when possible our development clients are intended to mirror practical situations and demonstrate the need for the algorithm at hand in this case the process is known as whitelisting specifically imagine a credit card company that needs to check whether customer transactions are for a valid account to do so it can keep customers account numbers in a file which we refer to as a whitelist produce the account number associated with each transaction in the standard input stream use the test client to put onto standard output the numbers that are not associat ed with any customer presumably the company would refuse such transactions it would not be unusual for a big company with millions of customers to have to pro cess millions of transactions or more to model this situation we provide on the book site the files largew txt million integers and larget txt million integers performance a working program is often not sufficient for example a much simpler implementation of rank which does not even require the array to be sorted is to check every entry as follows public static int rank int key int a for int i i a length i if a i key return i return given this simple and easy to understand solution why do we use mergesort and bi nary search if you work exercise you will see that your computer is too slow to run this brute force implementation of rank for large numbers of inputs say million whitelist entries and million transactions solving the whitelist problem for a large number of inputs is not feasible without efficient algorithms such as binary search and mergesort good performance is often of critical importance so we lay the ground work for studying performance in section and analyze the performance character istics of all of our algorithms including binary search in section and mergesort in section in the present context our goal in thoroughly outlining our programming model is to ensure that you can run code like binarysearch on your computer use it on test data like ours and modify it to adapt to various situations such as those described in the exercises at the end of this section in order to best understand its applicability the programming model that we have sketched is designed to facilitate such activities which are crucial to our approach to studying algorithms largew txt larget txt 293674 600579 984875 295754 207807 903531 699418 not in largew txt int values int values int values large files for binarysearch test client perspective in this section we have described a fine and complete programming model that served and still serves many programmers for many decades modern programming however goes one step further this next level is called data abstraction sometimes known as object oriented programming and is the subject of the next sec tion simply put the idea behind data abstraction is to allow a program to define data types sets of values and sets of operations on those values not just static methods that operate on predefined data types object oriented programming has come into widespread use in recent decades and data abstraction is central to modern program development we embrace data abstrac tion in this book for three primary reasons it enables us to expand our ability to reuse code through modular programming for example our sorts in chapter and binary search and other algorithms in chapter allow clients to make use of the same code for any type of data not just integers including one defined by the client it provides a convenient mechanism for building so called linked data structures that provide more flexibility than arrays and are the basis of efficient algorithms in many settings it enables us to precisely define the algorithmic challenges that we face for ex ample our union find algorithms in section our priority queue algorithms in section and our symbol table algorithms in chapter are all oriented toward defining data structures that enable efficient implementations of a set of operations this challenge aligns perfectly with data abstraction despite all of these considerations our focus remains on the study of algorithms in this context we proceed to consider next the essential features of object oriented pro gramming that are relevant to our mission q what is java bytecode a a low level version of your program that runs on the java virtual machine this level of abstraction makes it easier for the developers of java to ensure that our programs run on a broad variety of devices q it seems wrong that java should just let ints overflow and give bad values shouldn t java automatically check for overflow a this issue is a contentious one among programmers the short answer is that the lack of such checking is one reason such types are called primitive data types a little knowledge can go a long way in avoiding such problems we use the int type for small numbers less than ten decimal digits and the long type when values run into the bil lions or more q what is the value of math abs a this strange but true result is a typical example of the effects of integer overflow q how can i initialize a double variable to infinity a java has built in constants available for this purpose double and double q can you compare a double to an int a not without doing a type conversion but remember that java usually does the req uisite type conversion automatically for example if x is an int with the value then the expression x is true java converts x to double because is a double literal before performing the comparison q what happens if i use a variable before initializing it to a value a java will report a compile time error if there is any path through your code that would lead to use of an uninitialized variable q what are the values of and as java expressions a the first generates a runtime exception for division by zero which stops your pro gram because the value is undefined the second has the value infinity q a continued q can you use and to compare string variables a no those operators are defined only for primitive types see page q what is the result of division and remainder for negative integers a the quotient a b rounds toward the remainder a b is defined such that a b b a b is always equal to a for example and are both but is and is q why do we say a b and not a b a the operators and are bitwise logical operations for integer types that do and or and exclusive or respectively on each bit position thus the value of is and the value of is we use these operators rarely but occasionally in this book the operators and are valid only in boolean expressions are included separately because of short circuiting an expression is evaluated left to right and the evaluation stops when the value is known q is ambiguity in nested if statements a problem a yes in java when you write if if stmnta else stmntb it is equivalent to if if stmnta else stmntb even if you might have been thinking if if stmnta else stmntb using explicit braces is a good way to avoid this dangling else pitfall q what is the difference between a for loop and its while formulation a the code in the for loop header is considered to be in the same block as the for loop body in a typical for loop the incrementing variable is not available for use in later statements in the corresponding while loop it is this distinction is often a rea son to use a while instead of a for loop q some java programmers use int a instead of int a to declare arrays what the difference a in java both are legal and equivalent the former is how arrays are declared in c the latter is the preferred style in java since the type of the variable int more clearly indicates that it is an array of integers q why do array indices start at instead of a this convention originated with machine language programming where the ad dress of an array element would be computed by adding the index to the address of the beginning of an array starting indices at would entail either a waste of space at the beginning of the array or a waste of time to subtract the q if a is an array why does stdout println a print out a hexadecimal integer such as instead of the elements of the array a good question it is printing out the memory address of the array which unfortu nately is rarely what you want q why are we not using the standard java libraries for input and graphics a we are using them but we prefer to work with simpler abstract models the java libraries behind stdin and stddraw are built for production programming and the libraries and their apis are a bit unwieldy to get an idea of what they are like look at the code in stdin java and stddraw java q can my program reread data from standard input a no you only get one shot at it in the same way that you cannot undo println q what happens if my program attempts to read after standard input is exhausted a you will get an error stdin isempty allows you to avoid such an error by check ing whether there is more input available q what does this error message mean exception in thread main java lang noclassdeffounderror stdin a you probably forgot to put stdin java in your working directory q can a static method take another static method as an argument in java a no good question since many other languages do support this capability give the value of each of the following expressions a b 100000000 c true false true true give the type and value of each of the following expressions a b c d write a program that takes three integer command line arguments and prints equal if all three are equal and not equal otherwise what if anything is wrong with each of the following statements a if a b then c b if a b c c if a b c d if a b c else b write a code fragment that prints true if the double variables x and y are both strictly between and and false otherwise what does the following program print int f int g for int i i i stdout println f f f g g f g give the value printed by each of the following code fragments a double t while math abs t t t t t stdout printf n t b int sum for int i i i for int j j i j sum stdout println sum c int sum for int i i i for int j j n j sum stdout println sum what do each of the following print a system out println b b system out println b c c system out println char a explain each outcome write a code fragment that puts the binary representation of a positive integer n into a string solution java has a built in method integer tobinarystring n for this job but the point of the exercise is to see how such a method might be implemented here is a particularly concise solution string for int n n n n n exercises continued what is wrong with the following code fragment int a for int i i i a i i i solution it does not allocate memory for a with new this code results in a variable a might not have been initialized compile time error write a code fragment that prints the contents of a two dimensional boolean array using to represent true and a space to represent false include row and column numbers what does the following code fragment print int a new int for int i i i a i i for int i i i a i a a i for int i i i system out println i write a code fragment to print the transposition rows and columns changed of a two dimensional array with m rows and n columns write a static method lg that takes an int value n as argument and returns the largest int not larger than the base logarithm of n do not use math write a static method histogram that takes an array a of int values and an integer m as arguments and returns an array of length m whose ith entry is the num ber of times the integer i appeared in the argument array if the values in a are all between and m the sum of the values in the returned array should be equal to a length give the value of public static string int n if n return return n n n n criticize the following recursive function public static string int n string n n n n if n return return answer the base case will never be reached a call to will result in calls to and so forth until a stackoverflowerror occurs consider the following recursive function public static int mystery int a int b if b return if b return mystery a a b return mystery a a b a what are the values of mystery and mystery given positive integers a and b describe what value mystery a b computes answer the same question but replace with and replace return with return run the following program on your computer public class fibonacci public static long f int n if n return if n return return f n f n public static void main string args for int n n n stdout println n f n exercises continued what is the largest value of n for which this program takes less hour to compute the value of f n develop a better implementation of f n that saves computed values in an array write a recursive static method that computes the value of ln n write a program that reads in lines from standard input with each line contain ing a name and two integers and then uses printf to print a table with a column of the names the integers and the result of dividing the first by the second accurate to three decimal places you could use a program like this to tabulate batting averages for baseball players or grades for students write a version of binarysearch that uses the recursive rank given on page and traces the method calls each time the recursive method is called print the argu ment values lo and hi indented by the depth of the recursion hint add an argument to the recursive method that keeps track of the depth add to the binarysearch test client the ability to respond to a second argu ment to print numbers from standard input that are not in the whitelist to print numbers that are in the whitelist give the sequence of values of p and q that are computed when euclid algo rithm is used to compute the greatest common divisor of and extend the code given on page to develop a program euclid that takes two integers from the command line and computes their greatest common divisor printing out the two arguments for each call on the recursive method use your program to compute the greatest common divisor or and use mathematical induction to prove that euclid algorithm computes the greatest common divisor of any pair of nonnegative integers p and q sorting three numbers suppose that the variables a b c and t are all of the same numeric primitive type show that the following code puts a b and c in ascending order if a b t a a b b t if a c t a a c c t if b c t b b c c t binomial distribution estimate the number of recursive calls that would be used by the code public static double binomial int n int k double p if n k return return p binomial n k p binomial n k to compute binomial develop a better implementation that is based on saving computed values in an array remove duplicates modify the test client in binarysearch to remove any du plicate keys in the whitelist after the sort equal keys add to binarysearch a static method rank that takes a key and a sorted array of int values some of which may be equal as arguments and returns the number of elements that are smaller than the key and a similar method count that returns the number of elements equal to the key note if i and j are the values returned by rank key a and count key a respectively then a i i j are the values in the array that are equal to key array exercise write a code fragment that creates an n by n boolean array a such that a i j is true if i and j are relatively prime have no common fac tors and false otherwise random connections write a program that takes as command line arguments an integer n and a double value p between and plots n equally spaced dots of size on the circumference of a circle and then with probability p for each pair of points draws a gray line connecting them creative problems continued histogram suppose that the standard input stream is a sequence of double values write a program that takes an integer n and two double values l and r from the command line and uses stddraw to plot a histogram of the count of the numbers in the standard input stream that fall in each of the n intervals defined by dividing l r into n equal sized intervals matrix library write a library matrix that implements the following api public class matrix static double dot double x double y vector dot product static double mult double a double b matrix matrix product static double transpose double a transpose static double mult double a double x matrix vector product static double mult double y double a vector matrix product develop a test client that reads values from standard input and tests all the methods filtering which of the following require saving all the values from standard input in an array say and which could be implemented as a filter using only a fixed number of variables and arrays of fixed size not dependent on n for each the input comes from standard input and consists of n real numbers between and print the maximum and minimum numbers print the median of the numbers print the k th smallest value for k less than print the sum of the squares of the numbers print the average of the n numbers print the percentage of numbers greater than the average print the n numbers in increasing order print the n numbers in random order dice simulation the following code computes the exact probability distribu tion for the sum of two dice int sides double dist new double sides for int i i sides i for int j j sides j dist i j for int k k sides k dist k the value dist i is the probability that the dice sum to k run experiments to vali date this calculation simulating n dice throws keeping track of the frequencies of oc currence of each value when you compute the sum of two random integers between and how large does n have to be before your empirical results match the exact results to three decimal places empirical shuffle check run computational experiments to check that our shuffling code on page works as advertised write a program shuffletest that takes command line arguments m and n does n shuffles of an array of size m that is initial ized with a i i before each shuffle and prints an m by m table such that row i gives the number of times i wound up in position j for all j all entries in the array should be close to n m bad shuffling suppose that you choose a random integer between and n in our shuffling code instead of one between i and n show that the resulting order is not equally likely to be one of the n possibilities run the test of the previous exercise for this version binary search versus brute force search write a program bruteforcesearch that uses the brute force search method given on page and compare its running time on your computer with that of binarysearch for largew txt and larget txt experiments continued random matches write a binarysearch client that takes an int value t as command line argument and runs t trials of the following experiment for n and generate two arrays of n randomly generated positive six digit int values and find the number of values that appear in both arrays print a table giving the average value of this quantity over the t trials for each value of n this page intentionally left blank a data type is a set of values and a set of operations on those values so far we have discussed in detail java primitive data types for example the values of the primitive data type int are integers between and the operations of int include and in principle we could write all of our programs using only the built in primitive types but it is much more convenient to write programs at a higher level of abstraction in this section we focus on the process of defining and using data types which is known as data abstraction and supplements the function abstraction style that is the basis of section programming in java is largely based on building data types known as reference types with the familiar java class this style of programming is known as object oriented programming as it revolves around the concept of an object an entity that holds a data type value with java primitive types we are largely confined to programs that operate on numbers but with reference types we can write programs that operate on strings pictures sounds any of hundreds of other abstractions that are available in java stan dard libraries or on our booksite even more significant than libraries of predefined data types is that the range of data types available in java programming is open ended because you can define your own data types to implement any abstraction whatsoever an abstract data type adt is a data type whose representation is hidden from the client implementing an adt as a java class is not very different from implementing a function library as a set of static methods the primary difference is that we associate data with the function implementations and we hide the representation of the data from the client when using an adt we focus on the operations specified in the api and pay no attention to the data representation when implementing an adt we focus on the data then implement operations on that data abstract data types are important because they support encapsulation in program design in this book we use them as a means to precisely specify problems in the form of apis for use by diverse clients describe algorithms and data structures as api implementations our primary reason for studying different algorithms for the same task is that perfor mance characteristics differ abstract data types are an appropriate framework for the study of algorithms because they allow us to put knowledge of algorithm performance to immediate use we can substitute one algorithm for another to improve performance for all clients without changing any client code using abstract data types you do not need to know how a data type is imple mented in order to be able to use it so we begin by describing how to write programs that use a simple data type named counter whose values are a name and a nonnega tive integer and whose operations are create and initialize to zero increment by one and examine the current value this abstraction is useful in many contexts for example it would be reasonable to use such a data type in electronic voting software to ensure that the only thing that a voter can do is increment a chosen candidate tally by one or we might use a counter to keep track of fundamental operations when analyzing the performance of algorithms to use a counter you need to learn our mechanism for specifying the operations defined in the data type and the java language mechanisms for creating and manipulating data type values such mechanisms are critically im portant in modern programming and we use them throughout this book so this first example is worthy of careful attention api for an abstract data type to specify the behavior of an abstract data type we use an application programming interface api which is a list of constructors and instance methods operations with an informal description of the effect of each as in this api for counter public class counter counter string id create a counter named id void increment increment the counter by one int tally number of increments since creation string tostring string representation an api for a counter even though the basis of a data type definition is a set of values the role of the values is not visible from the api only the operations on those values accordingly an adt definition has many similarities with a library of static methods see page both are implemented as a java class instance methods may take zero or more arguments of a specified type sepa rated by commas and enclosed in parentheses they may provide a return value of a specified type or no return value signified by void and there are three significant differences some entries in the api have the same name as the class and lack a return type such entries are known as constructors and play a special role in this case counter has a constructor that takes a string argument instance methods lack the static modifier they are not static methods their purpose is to operate on data type values some instance methods are present so as to adhere to java conventions we refer to such methods as inherited methods and shade them gray in the api as with apis for libraries of static methods an api for an abstract data type is a con tract with all clients and therefore the starting point both for developing any client code and for developing any data type implementation in this case the api tells us that to use counter we have available the counter constructor the increment and tally instance methods and the inherited tostring method inherited methods various java conventions enable a data type to take advantage of built in language mechanisms by including specific methods in the api for example all java data types inherit a tostring method that returns a string representation of the data type values java calls this method when any data type value is to be concat enated with a string value with the operator the default implementation is not par ticularly useful it gives a string representation of the memory address of the data type value so we often provide an implementation that overrides the default and include tostring in the api whenever we do so other examples of such methods include equals compareto and hashcode see page client code as with modular programming based on static methods the api allows us to write client code without knowing details of the implementation and to write implementation code without knowing details of any particular client the mecha nisms introduced on page for organizing programs as independent modules are use ful for all java classes and thus are effective for modular programming with adts as well as for libraries of static methods accordingly we can use an adt in any program provided that the source code is in a java file in the same directory or in the standard java library or accessible through an import statement or through one of the classpath mechanisms described on the booksite all of the benefits of modular programming follow by encapsulating all the code that implements a data type within a single java class we enable the development of client code at a higher level of abstraction to de velop client code you need to be able to declare variables create objects to hold data type values and provide access to the values for instance methods to operate on them these processes are different from the corresponding processes for primitive types though you will notice many similarities objects naturally you can declare that a variable heads is to be associated with data of type counter with the code counter heads but how can you assign values or specify operations the answer to this question in volves a fundamental concept in data abstraction an object is an entity that can take on a data type value objects are characterized by three essential prop erties state identity and behavior the state of an object is a value from its data type the identity of an object distinguishes one object from another it is useful to think of an object identity as the place where its value is stored in memory the behavior of an object is the effect of data type operations the implementation has the sole re sponsibility for maintaining an object identity so that client code can use a data type without regard to the representation of its state by conforming to an api that describes an object behavior an ob ject state might be used to provide information to a client or cause a side effect or be changed by one of its data type operations but the details of the representation of the data type value are not rel evant to client code a reference is a mechanism for accessing an ob ject java nomenclature makes clear the distinction from primitive types where variables are associated with values by using the term reference types for nonprimitive types the details of implementing references vary in java implementations but it is useful to think of a reference as a memory address as shown at right for brevity we use three digit memory addresses in the diagram creating objects each data type value is stored in an object to create or instantiate an individual object we invoke a constructor by using the keyword new followed by the class name followed by or a list of argument values enclosed in parentheses if the con structor takes arguments a constructor has no return type because it always returns a reference to an object of its data type each time that a client uses new the system allocates memory space for the object one counter object heads two counter objects heads tails reference identity details hidden invokes the constructor to initialize its value returns a reference to the object object representation in client code we typically create objects in an initializing declaration that associates a variable with the object as we often do with variables of primitive types unlike primi tive types variables are associated with references to objects not the data type values themselves we can create any num ber of objects from the same class declaration to associate variable with object reference call on constructor to create an object each object has its own identity and may or may not store the same value as another object of the same type for example the code counter heads new counter heads counter tails new counter tails new counter heads creating an object creates two different counter objects in an abstract data type details of the representa tion of the value are hidden from client code you might assume that the value associ ated with each counter object is a string name and an int tally but you cannot write code that depends on any specific representation or even know whether that assumption is true perhaps the tally is a long value invoking instance methods the purpose of an instance method is to operate on data type values so the java language includes a special mechanism to invoke instance meth ods that emphasizes a connection to an object specifically we invoke an instance meth od by writing a variable name that refers to an object counter heads with new constructor heads declaration followed by a period followed by an instance method name followed by or more arguments enclosed in parentheses and separated by commas an instance method might change the data type value or just exam invoke a constructor create an object as a statement void return value ine the data type value instance methods have all of the properties of static methods that we considered on page argumentsarepassedbyvalue methodnames can be overloaded they may have a return value and object name as an expression object name invoke an instance method that changes the object value tails tally invoke an instance method that accesses the object value they may cause side effects but they have an addi tional property that characterizes them each invoca tion is associated with an object for example the code heads increment invokes the instance method increment to operate on the counter object heads in this case the opera tion involves incrementing the tally and the code via automatic type conversion tostring stdout println invoke heads tostring invoking instance methods heads tally tails tally invokes the instance method tally twice first to operate on the counter object heads and then to op erate on the counter object tails in this case the operation involves returning the tally as an int value as these examples illustrate you can use calls on instance methods in client code in the same way as you use calls on stat ic methods as statements void methods or values in expressions methods that re turn a value the primary purpose of stat ic methods is to implement functions the primary purpose of non static instance methods is to implement data type opera tions either type of method may appear in client code but you can easily distinguish between them because a static method call starts with a class name uppercase by parameters reference to object and argument argument convention and a non static method call always starts with an object name lower primary purpose examine or change object value compute return value case by convention these differences are summarized in the table at right instance methods versus static methods using objects declarations give us variable names for objects that we can use in code not just to create objects and invoke instance methods but also in the same way as we use variable names for integers floating point numbers and other primitive types to develop client code for a given data type w declare variables of the type for use in referring to objects use the keyword new to invoke a constructor that creates objects of the type use the object name to invoke instance methods either as statements or within expressions for example the class flips shown at the top of the next page is a counter client that takes a command line argument t and simulates t coin flips it is also a stdrandom cli ent beyond these direct uses we can use variables associated with objects in the same way as we use variables associated with primitive type values in assignment statements to pass or return objects from methods to create and use arrays of object understanding the behavior of each of these types of uses requires thinking in terms of references not values as you will see when we consider them in turn assignment statements an assignment statement with a reference type creates a copy of the reference the assignment statement does not create a new object just another reference to an existing object this situation is known as aliasing both variables refer to the same object the effect of aliasing is a bit unexpected because it is different for variables holding values of a primitive type be sure that you understand the difference if x and y are variables of a primitive type then the as signment x y copies the value of y to x for reference types the reference is copied not the value aliasing is a common source of bugs in java programs as illustrated by the following example counter new counter ones increment counter increment stdout println with a typical tostring implementation this code would print the string ones which may or may not be what was intended and is counterintuitive at first such bugs are common in programs written by people without counter new counter ones increment counter increment references to same object much experience in using objects that may be you so pay attention here changing the state of an object impacts all code involving aliased variables referencing that ob ject we are used to thinking of two different variables of primitive types as being independent but that intuition does not carry over to variables of reference types aliasing reference to ones objects as arguments you can pass objects as arguments to methods this ability typi cally simplifies client code for example when we use a counter as an argument we are essentially passing both a name and a tally but need only specify one variable when we call a method with arguments the effect in java is as if each argument value were to appear on the right hand side of an assignment statement with the corresponding argument name on the left that is java passes a copy of the argument value from the calling program to the method this arrangement is known as pass by value see page one important consequence is that the method cannot change the value of a caller variable for primitive types this policy is what we expect the two variables are inde pendent but each time that we use a reference type as a method argument we create an alias so we must be cautious in other words the convention is to pass the reference by value make a copy of it but to pass the object by reference for example if we pass a reference to an object of type counter the method cannot change the original refer ence make it point to a different counter but it can change the value of the object for example by using the reference to call increment objects as return values naturally you can also use an object as a return value from a method the method might return an object passed to it as an argument as in the example below or it might create an object and return a reference to it this capa bility is important because java methods allow only one return value using objects enables us to write code that in effect returns multiple values arrays are objects in java every value of any nonprimitive type is an object in par ticular arrays are objects as with strings there is special language support for certain operations on arrays declarations initialization and indexing as with any other ob ject when we pass an array to a method or use an array variable on the right hand side of an assignment statement we are making a copy of the array reference not a copy of the array this convention is appropriate for the typical case where we expect the method to be able to modify the array by rearranging its entries as for example in java util arrays sort or the shuffle method that we considered on page arrays of objects array entries can be of any type as we have already seen args in our main implementations is an array of string objects when we create an array of objects we do so in two steps create the array using the bracket syntax for array constructors create each object in the array using a standard constructor for each for example the code below simulates rolling a die using an array of counter objects to keep track of the number of occurrences of each possible value an array of objects in java is an array of references to objects not the objects themselves if the objects are large then we may gain efficiency by not having to move them around just their refer ences if they are small we may lose efficiency by having to follow a reference each time we need to get to some information with this focus on objects writing code that embraces data abstraction defining and using data types with data type values held in objects is widely referred to as object oriented programming the basic concepts that we have just covered are the start ing point for object oriented programming so it is worthwhile to briefly summarize them a data type is a set of values and a set of operations defined on those values we implement data types in independent java class modules and write client programs that use them an object is an entity that can take on a data type value or an instance of a data type objects are characterized by three essential properties state identity and behavior a data type implementation supports clients of the data type as follows client code can create objects establish identity by using the new construct to invoke a constructor that creates an object initializes its instance variables and returns a reference to that object client code can manipulate data type values control an object behavior pos sibly changing its state by using a variable associated with an object to invoke an instance method that operates on that object instance variables client code can manipulate objects by creating arrays of objects and passing them and returning them to methods in the same way as for primitive type values except that variables refer to references to values not the values themselves these capabilities are the foundation of a flexible modern and widely useful program ming style that we will use as the basis for studying algorithms in this book examples of abstract data types the java language has thousands of built in adts and we have defined many other adts to facilitate the study of algorithms in deed every java program that we write is a data type implementation or a library of static methods to control complexity we will specifically cite apis for any adt that we use in this book not many actually in this section we introduce as examples several data types with some examples of client code in some cases we present excerpts of apis that may contain dozens of instance methods or more we articulate these apis to present real world examples to specify the instance methods that we will use in the book and to emphasize that you do not need to know the details of an adt implementation in order to be able to use it for reference the data types that we use and develop in this book are shown on the facing page these fall into several different categories standard system adts in java lang which can be used in any java program java adts in libraries such as java awt java net and java io which can also be used in any java program but need an import statement our i o adts that allow us to work with multiple input output streams similar to stdin and stdout data oriented adts whose primary purpose is to facilitate organizing and pro cessing data by encapsulating the representation we describe several examples for applications in computational geometry and information processing later in this section and use them as examples in client code later on collection adts whose primary purpose is to facilitate manipulation collections of data of the same we describe the basic bag stack and queue types in sec tion pq types in chapter and the st and set types in chapters and opertions oriented adts that we use to analyze algorithms as described in sec tion and section adts for graph algorithms including both data oriented adts that focus on encapsulating representations of various kinds of graphs and operations orient ed adts that focus on providing specifications for graph processing algorithms this list does not include the dozens of types that we consider in exercises which may be found in the index also as described on page we often distinguish multiple imple mentations of various adts with a descriptive prefix as a group the adts that we use demonstrate that organizing and understanding the data types that you use is an important factor in modern programming a typical application might use only five to ten of these adts a prime goal in the development and organization of the adts in this book is to enable programmers to easily take advantage of a relatively small set of them in developing client code standard java system types in java lang integer int wrapper double double wrapper string indexed chars stringbuilder builder for strings other java types java awt color colors java awt font fonts java net url urls java io file files our standard i o types in input stream out output stream draw drawing data oriented types for client examples point in the plane interval interval date date transaction transaction types for the analysis of algorithms counter counter accumulator accumulator visualaccumulator visual version stopwatch stopwatch collection types stack pushdown stack queue fifo queue bag bag minpq maxpq priority queue indexminpq indexminpq priority queue indexed st symbol table set set stringst symbol table string keys data oriented graph types graph graph digraph directed graph edge edge weighted edgeweightedgraph graph weighted directededge edge directed weighted edgeweighteddigraph graph directed weighted operations oriented graph types uf dynamic connectivity depthfirstpaths dfs path searcher cc connected components breadthfirstpaths bfs path search directeddfs dfs digraph path search directedbfs bfs digraph path search transitiveclosure all paths topological topological order depthfirstorder dfs order directedcycle cycle search scc strong components mst minimum spanning tree sp shortest paths selected adts used in this book geometric objects a natural example of object oriented programming is design ing data types for geometric objects for example the apis on the facing page define abstract data types for three familiar geometric objects points in the plane intervals on the line and two dimensional intervals in the plane or axis aligned rectangles as usual the apis are essentially self documenting and lead immediately to easily under stood client code such as the example at left which reads the boundaries of an and an integer t from the command line generates t random points in the unit square and counts the number of points that fall in the interval an estimate of the area of the rectangle for dramatic effect the client also draws the interval and the points that fall outside the inter val this computation is a model for a method that reduces the problem of computing the area and volume test client java hits of geometric shapes to the problem of determining whether a point falls within the shape or not a less difficult but not trivial prob lem of course we can define apis for other geometric ob jects such as line segments triangles polygons circles and so forth though implementing operations on them can be challenging several examples are addressed in the exercises at the end of this section programs that process geometric objects have wide application in computing with models of the natural world in scientific computing video games movies and many other applications the development and study of such pro grams and applications has blossomed into a far reaching field of study known as computational geometry which is a public class double x double y create a point double x x coordinate double y y coordinate double r radius polar coordinates double theta angle polar coordinates double distto that euclidean distance from this point to that void draw draw the point on stddraw an api for points in the plane public class double lo double hi create an interval double length length of the interval boolean contains double x does the interval contain x boolean intersects that does the interval intersect that void draw draw the interval on stddraw an api for intervals on the line public class x y create a interval double area area of the interval boolean contains point p does the interval contain p boolean intersects that does the interval intersect that void draw draw the interval on stddraw an api for two dimensional intervals in the plane fertile area of examples for the application of the algorithms that we address in this book as you will see in examples throughout the book in the present context our interest is to suggest that abstract data types that directly represent geometric abstrac tions are not difficult to define and can lead to simple and clear client code this idea is reinforced in several exercises at the end of this section and on the booksite information processing whether it be a bank processing millions of credit card trans actions or a web analytics company processing billions of touchpad taps or a scien tific research group processing millions of experimental observations a great many applications are centered around processing and organizing information abstract data types provide a natural mechanism for organizing the information without getting into details the two apis on the facing page suggest a typical approach for a commer cial application the idea is to define data types that allow us to keep information in objects that correspond to things in the real world a date is a day a month and a year and a transaction is a customer a date and an amount these two are just examples we might also define data types that can hold detailed information for customers times locations goods and services or whatever each data type consists of constructors that create objects containing the data and methods for use by client code to access it to simplify client code we provide two constructors for each type one that presents the data in its appropriate type and another that parses a string to get the data see exer cise for details as usual there is no reason for client code to know the rep resentation of the data most often the reason to organize the data in this way is to treat the data associated with an object as a single entity we can maintain arrays of transaction values use date values as a argument or a return value for a method and so forth the focus of such data types is on encapsulating the data while at the same time enabling the development of client code that does not depend on the representa tion of the data we do not dwell on organizing information in this way except to take note that doing so and including the inherited methods tostring compareto equals and hashcode allows us to take advantage of algorithm implementations that can process any type of data we will discuss inherited methods in more detail on page for example we have already noted java convention that enables clients to print a string representation of every value if we include tostring implemen tation in a data type we consider conventions corresponding to the other inherited methods in section section section and section using date and transaction as examples section gives classic examples of data types and a java language mechanism known as parameterized types or generics that takes advantage of these conventions and chapter and chapter are also devoted to taking advantage of generic types and inherited methods to develop implementations of sorting and searching algorithms that are effective for any type of data whenever you have data of different types that logically belong together it is worthwhile to contemplate defining an adt as in these examples the ability to do so helps to organize the data can greatly simplify client code in typical applications and is an important step on the road to data abstraction public class date implements comparable date date int month int day int year create a date date string date create a date parse constructor int month month int day day int year year string tostring string representation boolean equals object that is this the same date as that int compareto date that compare this date to that int hashcode hash code public class transaction implements comparable transaction transaction string who date when double amount transaction string transaction create a transaction parse constructor string who customer name date when date double amount amount string tostring string representation boolean equals object that is this the same transaction as that int compareto transaction that compare this transaction to that int hashcode hash code sample apis for commercial applications dates and transactions strings java string is an important and useful adt a string is an indexed se quence of char values string has dozens of instance methods including the following public class string string create an empty string int length length of the string int charat int i ith character int indexof string p first occurrence of p if none int indexof string p int i first occurrence of p after i if none string concat string t this string with t appended string substring int i int j substring of this string ith to j chars string split string delim strings between occurrences of delim int compareto string t string comparison boolean equals string t is this string value the same as t int hashcode hash code java string api partial list of methods string values are similar to arrays of characters but the two are not the same ar rays have built in java language syntax for accessing a character string has instance methods for indexed access length and many other operations on the other hand string has special language support for initialization and concatenation instead of creating and initializing a string with a constructor we can use a string literal instead of invoking the method concat we can use the operator we do not need to con sider the details of the implementation though understanding performance characteristics of some of the methods is important when develop ing string processing algorithms as you will see in chapter why not just use arrays of charac string a now is string b the time string c to call value ters instead of string values the answer to this question is the same as for any adt to simplify and clarify client code with string we can write clear and simple client code that uses numerous convenient instance methods without regard to the way in which strings are represented see fac ing page even this short list contains powerful a length a charat a concat c a indexof is a substring a split a split b equals c i now is to w i now is false operations that require advanced algorithms such examples of string operations task implementation public static boolean ispalindrome string is the string a palindrome int n length for int i i n i if charat i charat n i return false return true extract file name and extension from a command line argument string args int dot rank string base substring dot string extension substring dot length print all lines in standard input that contain a string specified on the command line string query args while stdin isempty string stdin readline if contains query stdout println create an array of the strings on stdin delimited by whitespace string input stdin readall string words input split public boolean issorted string a check whether an array of strings is in alphabetical order for int i i a length i if a i compareto a i return false return true typical string processing code as those considered in chapter for example the argument of split can be a regular expression see section the split example on page uses the argu ment which means one or more tabs spaces newlines or returns input and output revisited a disadvantage of the stdin stdout and stddraw stan dard libraries of section is that they restrict us to working with just one input file one output file and one drawing for any given program with object oriented pro gramming we can define similar mechanisms that allow us to work with multiple input streams output streams and drawings within one program specifically our standard libary includes the data types in out and draw with the apis shown on the facing page when invoked with a constructor having a string argument in and out will first try to find a file in the current directory of your computer that has that name if it cannot do so it will assume the argu ment to be a website name and will try to connect to that web site if no such website exists it will issue a runtime exception in either case the specified file or website becomes the source target of the input output for the stream object thus created and the read and print methods will refer to that file or website if you use the no argu ment constructor then you ob tain the standard streams this arrangement makes it possible for a single program to process multiple files and drawings you also can assign such objects to variables pass them as arguments or re turn values from methods create arrays of them and manipulate them just as you manipulate objects of any type the program cat shown at left is a sample client of in and out that uses multiple input streams to concatenate several input files into a single out put file the in and out classes also contain static methods for reading files containing values that are all int double or string types into an array see page and exercise public class in in create an input stream from standard input in string name create an input stream from a file or website boolean isempty true if no more input false otherwise int readint read a value of type int double readdouble read a value of type double void close close the input stream note all operations supported by stdin are also supported for in objects api for our data type for input streams public class out out create an output stream to standard output out string name create an output stream to a file void print string append to the output stream void println string append and a newline to the output stream void println append a newline to the output stream void printf string f formatted print to the output steam void close close the output stream note all operations supported by stdout are also supported for out objects api for our data type for output streams public class draw draw void line double double double double y1 void point double x double y note all operations supported by stddraw are also supported for draw objects api for our data type for drawings implementing an abstract data type as with libraries of static methods we implement adts with a java class putting the code in a file with the same name as the class followed by the java extension the first statements in the file declare in stance variables that define the data type values following the instance variables are the constructor and the instance methods that implement operations on data type values instance methods may be public specified in the api or private used to organize the computation and not available to clients a data type definition may have multiple constructors and may also include definitions of static methods in particular a unit test client main is normally useful for testing and debugging as a first example we consider an implementation of the counter adt that we defined on page a full annotated implementation is shown on the facing page for reference as we discuss its constituent parts every adt implementation that you will develop has the same basic ingredients as this simple example instance variables to define data type values the state of each object we de instance variable public class counter private final string name private int count clare instance variables in much the same way as we declare local variables there is a critical distinction between instance vari ables and the local variables within a static declarations instance variables in adts are private method or a block that you are accustomed to there is just one value corresponding to each local variable at a given time but there are numerous values corresponding to each instance variable one for each object that is an instance of the data type there is no ambiguity with this arrangement because each time that we access an instance variable we do so with an object name that object is the one whose value we are accessing also each declaration is qualified by a visibility modifier in adt implementations we use private using a java language mechansim to enforce the idea that the representa tion of an adt is to be hidden from the client and also final if the value is not to be changed once it is initialized counter has two instance variables a string value name and an int value count if we were to use public instance variables allowed in java the data type would by definition not be abstract so we do not do so constructors every java class has at least one constructor that establishes an object identity a constructor is like a static method but it can refer directly to instance vari ables and has no return value generally the purpose of a constructor is to initialize the instance variables every constructor creates an object and provides to the client a reference to that object constructors always share the same name as the class we can overload the name and have multiple constructors with different signatures just as with methods if no other constructor is defined a default no argument constructor is public class counter class instance variables constructor private final string name private int count public counter string id name id name instance methods public string tostring return count name instance variable name test client create and initialize objects public static void main string args counter heads new counter heads counter tails new invoke heads increment heads increment tails increment constructor automatically invoke tostring object stdout println heads tails name stdout println heads tally tails tally invoke method anatomy of a class that defines a data type implicit has no arguments and initializes instance values to default values the default values of instance variables are for primitive numeric types false for boolean and null for reference types these defaults may be changed by using initializing declarations for instance variables java automatically invokes a constructor when a client program uses the keyword public class counter private final string name private int count new overloaded constructors are typi cally used to initialize instance variables to client supplied values other than the defaults for example counter has a one argument constructor that initial visibility modifier no return type constructor name same as class name parameter variable signature izes the name instance variable to the value given as argument leaving the count instance variable to be initialized to the default value code to initialize instance variables count initialized to by default anatomy of a constructor instance methods to implement data type instance methods the behavior of each object we implement instance methods with code that is precisely like the code that you learned in section to implement static methods functions each instance method has a return type a signature which specifies its name and the types and names of its parameter variables and a body which consists of a sequence of statements in cluding a return statement that provides a value of the return type back to the cli ent when a client invokes a method the visibility return modifier type method name signature parameter values if any are initialized with client values the statements are ex ecuted until a return value is computed and the value is returned to the client with the same effect as if the method in count instance variable name anatomy of an instance method vocation in the client were replaced with that value all of this action is the same as for static methods but there is one critical distinction for instance methods they can access and perform operations on instance variables how do we specify which object instance variables we want to use if you think about this question for a moment you will see the logical answer a reference to a variable in an instance method refers to the value for the object that was used to invoke the method when we say heads increment the code in increment is referring to the instance variables for heads in other words object oriented programming adds one critically important additional way to use vari ables in a java program to invoke an instance method that operates on the object values the difference from working solely with static methods is semantic see the q a but has reoriented the way that modern programmers think about developing code in many situations as you will see it also dovetails well with the study of algorithms and data structures scope in summary the java code that we write to implement instance methods uses three kinds of variables parameter variables local variables instance variables the first two of these are the same as for static methods parameter variables are spec ified in the method signature and initialized with client values when the method is called and local variables are declared and initialized within the method body the scope of parameter variables is the entire method the scope of local variables is the following statements in the block where they are defined instance variables are com pletely different they hold data type values for objects in a class and their scope is the entire class whenever there is an ambiguity you can use the this prefix to identify in stance variables understanding the distinctions among these three kinds of variables in instance methods is a key to success in object oriented programming public class example instance variable private int var private void local variable int var refers to local variable not instance variable var this var refers to instance variable private void var refers to instance variable scope of instance and local variables in an instance method api clients and implementations these are the basic components that you need to understand to be able to build and use abstract data types in java every adt im plementation that we will consider will be a java class with private instance variables constructors instance methods and a client to fully understand a data type we need the api typical client code and an implementation summarized for counter on the facing page to emphasize the separation of client and implementation we normally present each client as a separate class containing a static method main and reserve test client main in the data type definition for minimal unit testing and develop ment calling each instance method at least once in each data type that we develop we go through the same steps rather than thinking about what action we need to take next to accomplish a computational goal as we did when first learning to program we think about the needs of a client then accommodate them in an adt following these three steps specify an api the purpose of the api is to separate clients from implementa tions to enable modular programming we have two goals when specifying an api first we want to enable clear and correct client code indeed it is a good idea to write some client code before finalizing the api to gain confidence that the specified data type operations are the ones that clients need second we want to be able to implement the operations there is no point specifying opera tions that we have no idea how to implement implement a java class that meets the api specifications first we choose the instance variables then we write constructors and the instance methods develop multiple test clients to validate the design decisions made in the first two steps what operations do clients need to perform and what data type values can best sup port those operations these basic decisions are at the heart of every implementation that we develop api typical client public class counter counter string id create a counter named id void increment increment the counter int tally number of increments since creation string tostring string representation implementation application an abstract data type for a simple counter more adt implementations as with any programming concept the best way to understand the power and utility of adts is to consider carefully more examples and more implementations there will be ample opportunity for you to do so as much of this book is devoted to adt implementations but a few more simple examples will help us lay the groundwork for addressing them date shown on the facing page are two implementations of the date adt that we con sidered on page to reduce clutter we omit the parsing constructor which is described andtheinheritedmethodsequals compareto see page and hashcode see exercise the straightforward implementation on the left maintains the day month and year as instance variables so that the instance methods can just return the appropriate value the more space efficient implementa tion on the right uses only a single int value to represent a date using a mixed radix number that represents the date with day d month m and year y as d one way that a client might notice the difference between these implementations is by violating implicit assumptions the second implementation depends for its correctness on the day being between and the month being between and and the year be ing positive in practice both implementations should check that months are between and days are between and and that dates such as june and february are illegal though that requires a bit more work this example highlights the idea that we rarely fully specify implementation requirements in an api we normally do the best we can and could do better here another way that a client might notice the difference between the two implementations is performance the implementation on the right uses less space to hold data type values at the cost of more time to provide them to the client in the agreed form one or two arithmetic operations are needed such trad eoffs are common one client may prefer one of the implementations and another client might prefer the other so we need to accommodate both indeed one of the recurring themes of this book is that we need to understand the space and time requirements of various implementations and their suitability for use by various clients one of the key advantages of using data abstraction in our implementations is that we can normally change from one implementation to another without changing any client code maintaining multiple implementations multiple implementations of the same api can present maintainence and nomenclature issues in some cases we simply want to replace an old implementation with an improved one in others we may need to main tain two implementations one suitable for some clients the other suitable for others indeed a prime goal of this book is to consider in depth several implementations of each of a number of fundamental adts generally with different performance charac teristics in this book we often compare the performance of a single client using two test client implementation application alternate implementation an abstract data type to encapsulate dates with two implementations different implementations of the same api for this reason we generally adopt an in formal naming convention where we identify different implementations of the same api by prepending a descrip tive modifier for example we might name our date implementations on the previous page basicdate and smalldate and we might wish to develop a smartdate implementation that can validate that dates are legal maintain a reference implementation with no prefix that makes a choice that should be suitable for most clients that is most clients should just use date in a large system this solution is not ideal as it might involve changing client code for example if we were to develop a new implementation extrasmalldate then our only options are to change client code or to make it the reference implementation for use by all clients java has various advanced language mechanisms for maintaining multiple implementations without needing to change client code but we use them sparingly because their use is challenging and even controversial even for experts especially in conjuction with other advanced language features that we do value generics and itera tors these issues are important for example ignoring them led to the celebrated problem at the turn of the millennium because many programs used their own imple mentations of the date abstraction that did not take into account the first two digits of the year but detailed consideration of these issues would take us rather far afield from the study of algorithms accumulator the accumulator api shown on the facing page defines an abstract data type that provides to clients the ability to maintain a running average of data values for example we use this data type frequently in this book to process experimental results see section the implementation is straightforward it maintains a int instance variable counts the number of data values seen so far and a double instance variable that keeps track of the sum of the values seen so far to compute the average it divides the sum by the count note that the implementation does not save the data values it could be used for a huge number of them even on a device that is not capable of holding that many or a huge number of accumulators could be used on a big system this performance characteristic is subtle and might be specified in the api because an implementation that does save the values might cause an application to run out of memory api typical client application implementation an abstract data type for accumulating data values visual accumulator the visual accumulator implementation shown on the facing page extends accumulator to present a useful side effect it draws on stddraw all the data in gray and the running average in red the easiest way to do so is to add a constructor that provides the number of points to be plotted and the maximum value for rescaling the plot visualaccumulator is not technically an imple mentation of the accumulator api its construc tor has a different signature and it causes a differ ent prescribed side effect generally we are careful to fully specify apis and are loath to make any changes in an api once articulated as it might height of nth red dot from the left is the average of the heights of the leftmost n gray dots height of gray dot is the data point value visual accumulator plot involve changing an unknown amount of client and implementation code but add ing a constructor to gain functionality can sometimes be defended because it involves changing the same line in client code that we change when changing a class name in this example if we have developed a client that uses an accumulator and perhaps has many calls to adddatavalue and avg we can enjoy the benefits of visualaccumulator by just changing one line of client code application java testvisualaccumulator mean values api typical client implementation an abstract data type for accumulating data values visual version data type design an abstract data type is a data type whose representation is hid den from the client this idea has had a powerful effect on modern programming the various examples that we have considered give us the vocabulary to address advanced characteristics of adts and their implementation as java classes many of these topics are on the surface tangential to the study of algorithms so it is safe for you to skim this section and refer to it later in the context of specific implementation problems our goal is to put important information related to designing data types in one place for reference and to set the stage for implementations throughout this book encapsulation a hallmark of object oriented programming is that it enables us to encapsulate data types within their implementations to facilitate separate development of clients and data type implementations encapsulation enables modular program ming allowing us to independently develop of client and implementation code substitute improved implementations without affecting clients support programs not yet written the api is a guide for any future client encapsulation also isolates data type operations which leads to the possibility of limiting the potential for error adding consistency checks and other debugging tools in implementations clarifying client code an encapsulated data type can be used by any client so it extends the java language the programming style that we are advocating is predicated on the idea of breaking large programs into small modules that can be developed and debugged independently this approach improves the resiliency of our software by limiting and localizing the ef fects of making changes and it promotes code reuse by making it possible to substitute new implementations of a data type to improve performance accuracy or memory footprint the same idea works in many settings we often reap the benefits of encap sulation when we use system libraries new versions of the java system often include new implementations of various data types or static method libraries but the apis do not change in the context of the study of algorithms and data structures there is strong and constant motivation to develop better algorithms because we can improve perfor mance for all clients by substituting an improved adt implementation without chang ing the code of any client the key to success in modular programming is to maintain independence among modules we do so by insisting on the api being the only point of dependence between client and implementation you do not need to know how a data type is implemented in order to use it and you can assume that a client knows nothing but the api when implementing a data type encapsulation is the key to attaining both of these advantages designing apis one of the most important and most challenging steps in building modern software is designing apis this task takes practice careful deliberation and many iterations but any time spent designing a good api is certain to be repaid in time saved debugging or code reuse articulating an api might seem to be overkill when writing a small program but you should consider writing every program as though you will need to reuse the code someday ideally an api would clearly articulate behavior for all possible inputs including side effects and then we would have software to check that implementations meet the specification unfortunately a fundamental result from theoretical computer science known as the specification problem implies that this goal is actually impossible to achieve briefly such a specification would have to be written in a formal language like a programming language and the problem of determining whether two programs perform the same computation is known mathematically to be undecidable therefore our apis are brief english language descriptions of the set of values in the associated abstract data type along with a list of constructors and instance methods again with brief english language descriptions of their purpose including side effects to validate the design we always include examples of client code in the text surrounding our apis within this broad outline there are numerous pitfalls that every api design is susceptible to an api may be too hard to implement implying implementations that are dif ficult or impossible to develop an api may be too hard to use leading to client code that is more complicated than it would be without the api an api may be too narrow omitting methods that clients need an api may be too wide including a large number of methods not needed by any client this pitfall is perhaps the most common and one of the most difficult to avoid the size of an api tends to grow over time because it is not difficult to add methods to an existing api but it is difficult to remove methods without breaking existing clients an api may be too general providing no useful abstractions an api may be too specific providing abstractions so detailed or so diffuse as to be useless an api may be too dependent on a particular representation therefore not serv ing the purpose of freeing client code from the details of using that representa tion this pitfall is also difficult to avoid because the representation is certainly central to the development of the implementation these considerations are sometimes summarized in yet another motto provide to cli ents the methods they need and no others algorithms and abstract data types data abstraction is naturally suited to the study of algorithms because it helps us provide a framework within which we can precisely specify both what an algorithm needs to accomplish and how a client can make use of an algorithm typically in this book an algorithm is an implementation of an instance method in an abstract data type for example our whitelisting example at the begin ning of the chapter is naturally cast as an adt client based on the following operations construct a set from an array of given values determine whether a given value is in the set these operations are encapsulated in the staticsetofints adt shown on the facing page along with whitelist a typical client staticsetofints is a special case of the more general and more useful symbol table adt that is the focus of chapter binary search is one of several algorithms that we study that is suitable for implementing these adts by comparison with the binarysearch implementation on page this imple mentation leads to clearer and more useful client code for example staticsetofints enforces the idea that the array must be sorted before rank is called with the abstract data type we separate the client from the implementation making it easier for any client to benefit from the ingenuity of the binary search algorithm just by following the api clients of rank in binarysearch have to know to sort the array first whitelisting is one of many clients that can take advantage of binary search every java program is a set of static methods and or a data type implementation in this book we focus primarily on abstract data type implementations such as staticsetofints where the focus is on operations and the representa tion of the data is hidden from the client as this example illustrates data abstraction enables us to application precisely specify what algorithms can provide for clients separate algorithm implementations from the client code develop layers of abstraction where we make use of well understood algorithms to develop other algorithms these are desirable properties of any approach to describing algorithms whether it be an english language description or pseudo code by embracing the java class mecha nism in support of data abstraction we have little to lose and much to gain working code that we can test and use to compare performance for diverse clients api public class staticsetofints staticsetofints int a create a set from the values in a boolean contains int key is key in the set typical client implementation binary search recast as an object oriented program an adt for search in a set of integers interface inheritance java provides language support for defining relationships among objects known as inheritance these mechanisms are widely used by software developers so you will study them in detail if you take a course in software engineer ing the first inheritance mechanism that we consider is known as subtyping which allows us to specify a relationship between otherwise unrelated classes by specifying in an interface a set of common methods that each implementing class must contain an interface is nothing more than a list of instance methods for example instead of using our informal api we might have articulated an interface for date public interface datable int month int day int year and then referred to the interface in our implementation code public class date implements datable implementation code same as before so that the java compiler will check that it matches the interface adding the code implements datable to any class that implements month day and year pro vides a guarantee to any client that an object of that class can invoke those methods this arrangement is known as interface inheritance an implementing class inherits the interface interface inheritance allows us to write client programs that can manipulate objects of any type that implements the interface even a type to be creat ed in the future by invoking meth ods in the interface we might have used interface inheritance in place of our more informal apis but chose not to do so to avoid dependence on specific high level language mecha nisms that are not critical to the understanding of algorithms and to avoid the extra baggage of inter face files but there are a few situa java interfaces used in this book tions where java conventions make it worthwhile for us to take advantage of interfaces we use them for comparison and for iteration as detailed in the table at the bottom of the previous page and will consider them in more detail when we cover those concepts implementation inheritance java also supports another inheritence mechanism known as subclassing which is a powerful technique that enables a programmer to change behavior and add functionality without rewriting an entire class from scratch the idea is to define a new class subclass or derived class that inherits instance meth ods and instance variables from another class superclass or base class the subclass contains more methods than the superclass moreover the subclass can redefine or override methods in the superclass subclassing is widely used by systems programmers to build so called extensible libraries one programmer even you can add methods to a library built by another programmer or perhaps a team of systems programmers effectively reusing the code in a potentially huge library for example this approach is widely used in the development of graphical user interfaces so that the large amount of code required to provide all the facilities that users expect drop down menus cut and paste access to files and so forth can be reused the use of subclassing is controversial among systems and applications programmers its advantages over interface inheri tance are debatable and we avoid it in this book because it generally works against encapsulation certain vestiges of the approach are built in to java and therefore un avoidable specifically every class is a subtype of java object class this structure enables the convention that every class includes an implementation of getclass tostring equals hashcode and several other methods that we do not use in this book actually every class inherits these methods from object through subclassing so any client can use them for any object we usually override tostring equals hashcode in new classes because the default object implementation generally does not lead to the desired behavior we now will consider tostring and equals we discuss hashcode in section method purpose section class getclass what class is this object string tostring string representation of this object boolean equals object that is this object equal to that int hashcode hash code for this object inherited methods from object used in this book string conversion by convention every java type inherits tostring from object so any client can invoke tostring for any object this convention is the basis for ja va automatic conversion of one operand of the concatenation operator to a string whenever the other operand is a string if an object data type does not include an implementation of tostring then the default implementation in object is invoked which is normally not helpful since it typically returns a string representation of the memory address of the object accordingly we generally include implementations of tostring that override the default in every class that we develop as highlighted for date on the facing page as illustrated in this code tostring implementations are often quite simple implicitly through using tostring for each instance variable wrapper types java supplies built in reference types known as wrapper types one for each of the primitive types boolean byte character double float integer long and short correspond to boolean byte char double float int long and short respectively these classes consist primarily of static methods such as parseint but they also include the inherited instance methods tostring compareto equals and hashcode java automatically converts from primitive types to wrapper types when warranted as described on page for example when an int value is concat enated with a string it is converted to an integer that can invoke tostring equality what does it mean for two objects to be equal if we test equality with a b where a and b are reference variables of the same type we are testing whether they have the same identity whether the references are equal typical clients would rather be able to test whether the data type values object state are the same or to implement some type specific rule java gives us a head start by providing implementa tions both for standard types such as integer double and string and for more com plicated types such as file and url when using these types of data you can just use the built in implementation for example if x and y are string values then x equals y is true if and only if x and y have the same length and are identical in each character position when we define our own data types such as date or transaction we need to override equals java convention is that equals must be an equivalence rela tion it must be reflexive x equals x is true symmetric x equals y is true if and only if y equals x transitive if x equals y and y equals z are true then so is x equals z in addition it must take an object as argument and satisfy the following properties consistent multiple invocations of x equals y consistently return the same value provided neither object is modified not null x equals null returns false these are natural definitions but ensuring that these properties hold adhering to java conventions and avoiding unnecessary work in an implementation can be tricky as il lustrated for date below it takes the following step by step approach if the reference to this object is the same as the reference to the argument object return true this test saves the work of doing all the other checks in this case if the argument is null return false to adhere to the convention and to avoid following a null reference in code to follow if the objects are not from the same class return false to determine an object class we use getclass note that we can use to tell us whether two objects of type class are equal because getclass is guaranteed to return the same reference for all objects in any given class cast the argument from object to date this cast must succeed because of the previous test return false if any instance variables do not match for other classes some other definition of equality might be appropriate for example we might regard two counter objects as equal if their count instance variables are equal this implementation is a model that you can use to implement equals for any type that you implement once you have implemented one equals you will not find it difficult to implement another memory management the ability to assign a new value to a reference variable cre ates the possibility that a program may have created an object that can no longer be referenced for example consider the three assignment statements in the figure at left after the third assignment statement not only do a and b refer to the same date object but also there is no longer a reference to the date object that was created and used to initialize b the only reference to that object date a new date date b new date b a was in the variable b and this reference was overwritten by the assignment so there is no way to refer to the object again such an object is said to be orphaned objects are also orphaned when they go out of scope java programs tend to create huge numbers of objects and variables that a b 655 657 812 references to same object orphaned object new year eve new year day hold primitive data type values but only have a need for a small number of them at any given point in time accord ingly programming languages and systems need mecha nisms to allocate memory for data type values during the time they are needed and to free the memory when they are no longer needed for an object sometime after it is orphaned memory management turns out to be easier for primitive types because all of the information needed for memory allocation is known at compile time java and most other systems takes care of reserving space for vari ables when they are declared and freeing that space when they go out of scope memory management for objects is more complicated the system can allocate memory for an object when it is created but cannot know precisely when to free the memory associated with each object because the dynamics of a program in execution determines when an orphaned object objects are orphaned in many languages such as c and c the programmer is responsible for both allocating and freeing memory doing so is tedious and notoriously error prone one of java most significant features is its ability to automatically man age memory the idea is to free the programmers from the responsibility of managing memory by keeping track of orphaned objects and returning the memory they use to a pool of free memory reclaiming memory in this way is known as garbage collection one of java characteristic features is its policy that references cannot be modified this policy enables java to do efficient automatic garbage collection programmers still debate whether the overhead of automatic garbage collection justifies the convenience of not having to worry about memory management immutability an immutable data type such as date has the property that the value of an object never changes once constructed by contrast a mutable data type such as counter or accumulator manipulates object values that are intended to change java language support for helping to enforce immutability is the final modifier when you declare a variable to be final you are promising to assign it a value only once either in an initializer or in the constructor code that could modify the value of a final variable leads to a compile time error in our code we use the modifier final with instance variables whose values never change this policy serves as documentation that the value does not change prevents accidental changes and makes programs easier to debug for example you do not have to include a final value in a trace since you know that its value never changes a data type such as date whose instance variables are all primitive and final is immutable in code that does not use implementation inheritence our convention whether to make a data type immutable is an important design decision and depends on the application at hand for data types such as date the purpose of the abstraction is to encap sulate values that do not change so that we can use them in as signment statements and as arguments and return values from functions in the same way as we use primitive types without hav ing to worry about their values changing a programmer imple menting a date client might reasonably expect to write the code d for two date variables in the same way as for double or int values but if date were mutable and the value of d were to mutable immutable counter date java arrays string mutable immutable examples change after the assignment d then the value of would also change they are both references to the same object on the other hand for data types such as counter and accumulator the very purpose of the abstraction is to encapsulate values as they change you have already encountered this distinction as a client programmer when using java arrays mutable and java string data type immutable when you pass a string to a method you do not worry about that method changing the sequence of characters in the string but when you pass an array to a method the method is free to change the contents of the array string objects are immutable because we generally do not want string values to change and java arrays are mutable because we generally do want array values to change there are also situations where we want to have mutable strings that is the purpose of java stringbuilder class and where we want to have immutable arrays that is the purpose of the vector class that we consider later in this section generally immutable types are easier to use and harder to misuse than muta ble types because the scope of code that can change their values is far smaller it is easier to debug code that uses immutable types because it is easier to guarantee that variables in client code that uses them remain in a consistent state when using mutable types you must always be concerned about where and when their values change the down side of immutability is that a new object must be created for every value this expense is normally manageable because java garbage collectors are typically optimized for such situations another downside of immutability stems from the fact that unfortunately final guarantees immutability only when instance variables are primitive types not reference types if an instance variable of a reference type has the final modifier the value of that instance variable the reference to an object will never change it will always refer to the same object but the value of the object itself can change for ex ample this code does not implement an immutable type public class vector private final double coords public vector double a coords a a client program could create a vector by specifying the entries in an array and then bypassing the api change the elements of the vector after construction double a vector vector new vector a a bypasses the public api the instance variable coords is private and final but vector is mutable because the client holds a reference to the data immutability needs to be taken into account in any data type design and whether a data type is immutable should be specified in the api so that clients know that object values will not change in this book our primary interest in immutability is for use in certifying the correctness of our algorithms for example if the type of data used for a binary search algorithm were mutable then cli ents could invalidate our assumption that the array is sorted for binary search design by contract to conclude we briefly discuss java language mechanisms that enables you to verify assumptions about your program as it is running we use two java language mechanisms for this purpose exceptions which generally handle unforeseen errors outside our control assertions which verify assumptions that we make within code we develop liberal use of both exceptions and assertions is good programming practice we use them sparingly in the book for economy but you will find them throughout the code on the booksite this code aligns with a substantial amount of the surrounding com mentary about each algorithm in the text that has to do with exceptional conditions and with asserted invariants exceptions and errors exceptions and errors are disruptive events that occur while a program is running often to signal an error the action taken is known as throwing an exception or throwing an error we have already encountered exceptions thrown by java system methods in the course of learning basic features of java stackoverflowerror arithmeticexception arrayindexoutofboundsexception outofmemoryerror and nullpointerexception are typical examples you can also create your own ex ceptions the simplest kind is a runtimeexception that terminates execution of the program and prints an error message throw new runtimeexception error message here a general practice known as fail fast programming suggests that an error is more easily pinpointed if an exception is thrown as soon as an error is discovered as opposed to ignoring the error and deferring the exception to sometime in the future assertions an assertion is a boolean expression that you are affirming is true at that point in the program if the expression is false the program will terminate and re port an error message we use assertions both to gain confidence in the correctness of programs and to document intent for example suppose that you have a computed value that you might use to index into an array if this value were negative it would cause an arrayindexoutofboundsexception sometime later but if you write the code assert index you can pinpoint the place where the error occurred you can also add an optional detail message such as assert index negative index in method x to help you locate the bug by default assertions are disabled you can enable them from the command line by using the enableassertions flag ea for short assertions are for debugging your program should not rely on assertions for normal operation since they may be disabled when you take a course in systems programming you will learn to use assertions to ensure that your code never terminates in a system error or goes into an infinite loop one model known as the design by contract model of programming expresses the idea the designer of a data type expresses a precondition the condition that the client promises to satisfy when calling a method a postcondition the condi tion that the implementation promises to achieve when returning from a method and side effects any other change in state that the method could cause during develop ment these conditions can be tested with assertions summary the language mechanisms discussed throughout this section illustrate that effective data type design leads to nontrivial issues that are not easy to resolve ex perts are still debating the best ways to support some of the design ideas that we are discussing why does java not allow functions as arguments why does matlab copy arrays passed as arguments to functions as mentioned early in chapter it is a slip pery slope from complaining about features in a programming language to becoming a programming language designer if you do not plan to do so your best strategy is to use widely available languages most systems have extensive libraries that you cer tainly should use when appropriate but you often can simplify your client code and protect yourself by building abstractions that can easily transport to other languages your main goal is to develop data types so that most of your work is done at a level of abstraction that is appropriate to the problem at hand the table on the facing page summarizes the various kinds of java classes that we have considered kind of class examples characteristics static methods math stdin stdout no instance variables immutable abstract data type date transaction string integer instance variables all private instance variables all final defensive copy for reference types note these are necessary but not sufficient mutable abstract data type counter accumulator instance variables all private not all instance variables final abstract data type with i o side effects visualaccumulator in out draw instance variables all private instance methods do i o java classes data type implementations q why bother with data abstraction a it helps us produce reliable and correct code for example in the presidential election al gore received votes on an electronic voting machine in volusia county florida the tally was clearly not properly encapsulated in the voting machine software q why the distinction between primitive and reference types why not just have refer ence types a performance java provides the reference types integer double and so forth that correspond to primitive types that can be used by programmers who prefer to ignore the distinction primitive types are closer to the types of data that are supported by computer hardware so programs that use them usually run faster than programs that use corresponding reference types q do data types have to be abstract a no java also allows public and protected to allow some clients to refer directly to instance variables as described in the text the advantages of allowing client code to directly refer to data are greatly outweighed by the disadvantages of dependence on a particular representation so all instance variables are private in our code we also oc casionally use private instance methods to share code among public methods q what happens if i forget to use new when creating an object a to java it looks as though you want to call a static method with a return value of the object type since you have not defined such a method the error message is the same as anytime you refer to an undefined symbol if you compile the code counter c counter test you get this error message cannot find symbol symbol method counter string you get the same kind of error message if you provide the wrong number of arguments to a constructor q what happens if i forget to use new when creating an array of objects a you need to use new for each object that you create so when you create an array of n objects you need to use new n times once for the array and once for each of the objects if you forget to create the array counter a a new counter test you get the same error message that you would get when trying to assign a value to any uninitialized variable variable a might not have been initialized a new counter test but if you forget to use new when creating an object within the array and then try to use it to invoke a method counter a new counter a increment you get a nullpointerexception q why not write stdout println x tostring to print objects a that code works fine but java saves us the trouble of writing it by automatically invoking the tostring method for any object since println has a method that takes an object as argument q what is a pointer a good question perhaps that should be nullreferenceexception like a java ref erence you can think of a pointer as a machine address in many programming lan guages the pointer is a primitive data type that programmers can manipulate in many ways but programming with pointers is notoriously error prone so operations pro vided for pointers need to be carefully designed to help programmers avoid errors java takes this point of view to an extreme that is favored by many modern program ming language designers in java there is only one way to create a reference new and only one way to change a reference with an assignment statement that is the only things that a programmer can do with references are to create them and copy them in q a continued programming language jargon java references are known as safe pointers because java can guarantee that each reference points to an object of the specified type and it can determine which objects are not in use for garbage collection programmers used to writing code that directly manipulates pointers think of java as having no pointers at all but people still debate whether it is really desirable to have unsafe pointers q where can i find more details on how java implements references and does garbage collection a one java system might differ completely from another for example one natural scheme is to use a pointer machine address another is to use a handle a pointer to a pointer the former gives faster access to data the latter provides for better garbage collection q what exactly does it mean to import a name a not much it just saves some typing you could type java util arrays instead of arrays everywhere in your code instead of using the import statement q what is the problem with implementation inheritance a subtyping makes modular programming more difficult for two reasons first any change in the superclass affects all subclasses the subclass cannot be developed inde pendently of the superclass indeed it is completely dependent on the superclass this problem is known as the fragile base class problem second the subclass code hav ing access to instance variables can subvert the intention of the superclass code for example the designer of a class like counter for a voting system may take great care to make it so that counter can only increment the tally by one remember al gore problem but a subclass with full access to the instance variable can change it to any value whatever q how do i make a class immutable a to ensure immutability of a data type that includes an instance variable of a mu table type we need to make a local copy known as a defensive copy and that may not be enough making the copy is one challenge ensuring that none of the instance methods change values is another q what is null a it is a literal value that refers to no object invoking a method using the null ref erence is meaningless and results in a nullpointerexception if you get this error message check to make sure that your constructor properly initializes all of its instance variables q can i have a static method in a class that implements a data type a of course for example all of our classes have main also it is natural to consider adding static methods for operations that involve multiple objects where none of them naturally suggests itself as the one that should invoke the method for example we might define a static method like the following within point public static double distance point a point b return a distto b often including such methods can serve to clarify client code q are there other kinds of variables besides parameter local and instance variables a if you include the keyword static in a class declaration outside of any type it creates a completely different type of variable known as a static variable like instance variables static variables are accessible to every method in the class however they are not associated with any object in older programming languages such variables are known as global variables because of their global scope in modern programming we focus on limiting scope and therefore rarely use such variables when we do we will call attention to them q what is a deprecated method a a method that is no longer fully supported but kept in an api to maintain compat ibility for example java once included a method character isspace and pro grammers wrote programs that relied on using that method behavior when the de signers of java later wanted to support additional unicode whitespace characters they could not change the behavior of isspace without breaking client programs so instead they added a new method character iswhitespace and deprecated the old method as time wears on this practice certainly complicates apis sometimes en tire classes are deprecated for example java deprecated its java util date in order to better support internationalization write a client that takes an integer value n from the command line generates n random points in the unit square and computes the distance separating the closest pair of points write an client that takes an int value n as command line argu ment reads n intervals each defined by a pair of double values from standard input and prints all pairs that intersect write an client that takes command line arguments n min and max and generates n random intervals whose width and height are uniformly distributed between min and max in the unit square draw them on stddraw and print the number of pairs of intervals that intersect and the number of intervals that are contained in one another what does the following code fragment print string hello string world stdout println stdout println what does the following code fragment print string hello world touppercase substring stdout println answer hello world string objects are immutable string methods return a new string object with the appropriate value but they do not change the value of the object that was used to invoke them this code ignores the objects returned and just prints the original string to print world use touppercase and substring a string is a circular rotation of a string t if it matches when the characters are circularly shifted by any number of positions e g actgacg is a circular shift of tgacgac and vice versa detecting this condition is important in the study of genomic sequences write a program that checks whether two given strings and t are circular shifts of one another hint the solution is a one liner with indexof length and string concatenation what does the following recursive function return public static string mystery string int n length if n return string a substring n string b substring n n return mystery b mystery a suppose that a and b are each integer arrays consisting of millions of inte gers what does the follow code do is it reasonably efficient int t a a b b t answer it swaps them it could hardly be more efficient because it does so by copying references so that it is not necessary to copy millions of elements instrument binarysearch page to use a counter to count the total number of keys examined during all searches and then print the total after all searches are com plete hint create a counter in main and pass it as an argument to rank develop a class visualcounter that allows both increment and decrement operations take two arguments n and max in the constructor where n specifies the maximum number of operations and max specifies the maximum absolute value for the counter as a side effect create a plot showing the value of the counter each time its tally changes develop an implementation smartdate of our date api that raises an excep tion if the date is not legal add a method dayoftheweek to smartdate that returns a string value monday tuesday wednesday thursday friday saturday or sunday giving the ap propriate day of the week for the date you may assume that the date is in the century exercises continued using ourimplementationof date asamodel page developanimplementa tion of transaction using our implementation of equals in date as a model page develop implementations of equals for transaction file input develop a possible implementation of the static readints meth od from in which we use for various test clients such as binary search on page that is based on the split method in string solution public static int readints string name in in new in name string input stdin readall string words input split int ints new int words length for int i i word length i ints i integer parseint words i return ints we will consider a different implementation in section see page rational numbers implement an immutable data type rational for rational numbers that supports addition subtraction multiplication and division public class rational rational int numerator int denominator rational plus rational b sum of this number and b rational minus rational b difference of this number and b rational times rational b product of this number and b rational divides rational b quotient of this number and b boolean equals rational that is this number equal to that string tostring string representation you do not have to worry about testing for overflow see exercise but use as instance variables two long values that represent the numerator and denominator to limit the possibility of overflow use euclid algorithm see page to ensure that the numerator and denominator never have any common factors include a test client that exercises all of your methods creative problems continued robust implementation of rational numbers use assertions to develop an im plementation of rational see exercise that is immune to overflow variance for accumulator validate that the following code which adds the methods var and stddev to accumulator computes both the mean and variance of the numbers presented as arguments to adddatavalue public class accumulator private double m private double private int n public void adddatavalue double x n n n x m x m m m x m n public double mean return m public double var return n public double stddev return math sqrt this var this implementation is less susceptible to roundoff error than the straightforward im plementation based on saving the sum of the squares of the numbers parsing develop the parse constructors for your date and transaction im plementations of exercise that take a single string argument to specify the initialization values using the formats given in the table below partial solution public date string date string fields date split month integer parseint fields day integer parseint fields year integer parseint fields type format example date integers separated by slashes transaction customer date and amount separated by whitespace formats for parsing turing several fundamental data types involve collections of objects specifically the set of values is a collection of objects and the operations revolve around adding remov ing or examining objects in the collection in this section we consider three such data types known as the bag the queue and the stack they differ in the specification of which object is to be removed or examined next bags queues and stacks are fundamental and broadly useful we use them in imple mentations throughout the book beyond this direct applicability the client and imple mentation code in this section serves as an introduction to our general approach to the development of data structures and algorithms one goal of this section is to emphasize the idea that the way in which we represent the objects in the collection directly impacts the efficiency of the various operations for collections we design data structures for representing the collection of objects that can support efficient implementation of the requisite operations a second goal of this section is to introduce generics and iteration basic java con structs that substantially simplify client code these are advanced programming lan guage mechanisms that are not necessarily essential to the understanding of algorithms but their use allows us to develop client code and implementations of algorithms that is more clear compact and elegant than would otherwise be possible a third goal of this section is to introduce and show the importance of linked data structures in particular a classic data structure known as the linked list enables im plementation of bags queues and stacks that achieve efficiencies not otherwise pos sible understanding linked lists is a key first step to the study of algorithms and data structures for each of the three types we consider apis and sample client programs then look at possible representations of the data type values and implementations of the data type operations this scenario repeats with more complicated data structures throughout this book the implementations here are models of implementations later in the book and worthy of careful study apis as usual we begin our discussion of abstract data types for collections by de fining their apis shown below each contains a no argument constructor a method to add an item to the collection a method to test whether the collection is empty and a method that returns the size of the collection stack and queue each have a method to remove a particular item from the collection beyond these basics these apis reflect two java features that we will describe on the next few pages generics and iterable collections bag public class bag item implements iterable item bag create an empty bag void add item item add an item boolean isempty is the bag empty int size number of items in the bag fifo queue public class queue item implements iterable item queue create an empty queue void enqueue item item add an item item dequeue remove the least recently added item boolean isempty is the queue empty int size number of items in the queue pushdown lifo stack public class stack item implements iterable item stack create an empty stack void push item item add an item item pop remove the most recently added item boolean isempty is the stack empty int size number of items in the stack apis for fundamental generic iterable collections generics an essential characteristic of collection adts is that we should be able to use them for any type of data a specific java mechanism known as generics also known as parameterized types enables this capability the impact of generics on the program ming language is sufficiently deep that they are not found in many languages including early versions of java but our use of them in the present context involves just a small bit of extra java syntax and is easy to understand the notation item after the class name in each of our apis defines the name item as a type parameter a symbolic place holder for some concrete type to be used by the client you can read stack item as stack of items when implementing stack we do not know the concrete type of item but a client can use our stack for any type of data including one defined long after we develop our implementation the client code provides a concrete type when the stack is created we can replace item with the name of any reference data type consistently everywhere it appears this provides exactly the capability that we need for example you can write code such as stack string stack new stack string stack push test string next stack pop to use a stack for string objects and code such as queue date queue new queue date queue enqueue new date date next queue dequeue to use a queue for date objects if you try to add a date or data of any other type than string to stack or a string or data of any other type than date to queue you will get a compile time error without generics we would have to define and implement different apis for each type of data we might need to collect with generics we can use one api and one implementation for all types of data even types that are imple mented in the future as you will soon see generic types lead to clear client code that is easy to understand and debug so we use them throughout this book autoboxing type parameters have to be instantiated as reference types so java has special mechanisms to allow generic code to be used with primitive types recall that java wrapper types are reference types that correspond to primitive types boolean byte character double float integer long and short correspond to boolean byte char double float int long and short respectively java automatically con verts between these reference types and the corresponding primitive types in assign ments method arguments and arithmetic logic expressions in the present context this conversion is helpful because it enables us to use generics with primitive types as in the following code stack integer stack new stack integer stack push auto boxing int integer int i stack pop auto unboxing integer int automatically casting a primitive type to a wrapper type is known as autoboxing and automatically casting a wrapper type to a primitive type is known as auto unboxing in this example java automatically casts autoboxes the primitive value to be of type integer when we pass it to the push method the pop method returns an integer which java casts auto unboxes to an int before assigning it to the variable i iterable collections for many applications the client requirement is just to process each of the items in some way or to iterate through the items in the collection this paradigm is so important that it has achieved first class status in java and many other modern languages the programming language itself has specific mechanisms to sup port it not just the libraries with it we can write clear and compact code that is free from dependence on the details of a collection implementation for example suppose that a client maintains a collection of transactions in a queue as follows queue transaction collection new queue transaction if the collection is iterable the client can print a transaction list with a single statement for transaction t collection stdout println t this construct is known as the foreach statement you can read the for statement as for each transaction t in the collection execute the following block of code this client code does not need to know anything about the representation or the implementation of the collection it just wants to process each of the items in the collection the same for loop would work with a bag of transactions or any other iterable collection we could hardly imagine client code that is more clear and compact as you will see supporting this capability requires extra effort in the implementation but this effort is well worthwhile it is interesting to note that the only differences between the apis for stack and queue are their names and the names of the methods this observation highlights the idea that we cannot easily specify all of the characteristics of a data type in a list of method signatures in this case the true specification has to do with the english lan guage descriptions that specify the rules by which an item is chosen to be removed or to be processed next in the foreach statement differences in these rules are profound part of the api and certainly of critical importance in developing client code bags a bag is a collection where removing items is not supported its purpose is to provide clients with the ability to collect items and then to iterate through the collected items the client can also test if a bag is empty and find its number of items the order of iteration is unspecified and should be immaterial to the client to appreciate the con cept consider the idea of an avid marble collector who might put marbles in a bag one at a time and periodically process all the marbles to look for one having some particular characteristic with our bag api a client can add items to a bag and process them all with a foreach statement whenever needed such a cli ent could use a stack or a queue but one way to emphasize that the order in which items are processed is immaterial is to use a bag the class stats at right illustrates a typi cal bag client the task is simply to compute the average and the sample standard deviation of the double values on standard input if there are n numbers on standard in put their average is computed by adding the numbers and dividing by n their sample standard deviation is comput ed by adding the squares of the difference between each number and the average dividing by n and taking the square root the order in which the numbers are consid ered is not relevant for either of these calculations so we save them in a bag and use the foreach construct to com pute each sum note it is possible to compute the standard deviation without saving all the numbers as we did for the average in accumulator see exercise keeping the all numbers in a bag is required for more complicated statistics a bag of marbles add add for marble m bag process each marble m in any order operations on a bag typical bag client application fifo queues a fifo queue or just a queue is a collection that is based on the first in first out fifo policy the policy of doing tasks in the same order that they arrive is one that we encounter frequently in everyday life enqueue enqueue dequeue dequeue server first in line leaves queue next in line leaves queue queue of customers new arrival at the end new arrival at the end from people waiting in line at a theater to cars wait ing in line at a toll booth to tasks waiting to be ser viced by an application on your computer one bed rock principle of any service policy is the perception of fairness the first idea that comes to mind when most people think about fairness is that whoever has been waiting the longest should be served first that is precisely the fifo discipline queues are a natural model for many everyday phenomena and they play a central role in numerous applications when a client iterates through the items in a queue with the foreach construct the items are processed in the order they were added to the queue a typi cal reason to use a queue in an application is to save items in a collection while at the same time preserv ing their relative order they come out in the same order in which they were put in for example the client below is a possible implementation of the readdoubles static method from our in class a typical fifo queue the problem that this method solves for the client is that the client can get numbers from a file into an array without knowing the file size ahead of time we enqueue the numbers from the file use the size method from queue to find the size needed for the array create the ar ray and then dequeue the num bers to move them to the array a queue is appropriate because it puts the numbers into the ar ray in the order in which they appear in the file we might use a bag if that order is immateri al this code uses autoboxing and auto unboxing to convert between the client double primitive type and and the queue double wrapper type pushdown stacks a pushdown stack or just a stack is a collection that is based on the last in first out lifo policy when you keep your mail in a pile on your desk you are using a stack you pile pieces of new mail on the top when they arrive and take each piece of mail from the top when you are ready to read it people do not process as many papers as they did in the past but the same organizing principle underlies several of the ap plications that you use regularly on your computer for example many people organize their email as a stack they push messages on the top when they are received and pop them from the top when they read them with most recently received first last in first out the ad vantage of this strategy is that we see interesting email as soon as possible the disadvantage is that some old email might never get read if we never empty the stack you have likely encountered another common example of a stack when surfing the web when you click a hyperlink your browser displays the new page and pushes onto a stack you can keep clicking on hyperlinks to visit new pages but you can always revisit the previous page by clicking the back button popping it from the stack a stack of documents push push pop pop new gray one goes on top new black one goes on top remove the black one from the top remove the gray one from the top the lifo policy offered by a stack provides just the be havior that you expect when a client iterates through the items in a stack with the foreach construct the items are processed in the reverse of the order in which they were added a typical reason to use a stack iterator in an application is to save items in a collection while at the same time reversing their relative order for example the client reverse at right reverses the or der of the integers on standard input again without having to know ahead of time how many there are the importance of stacks in computing is fundamental and profound as indicated in the detailed example that we consider next operations on a pushdown stack arithmetic expression evaluation as another example of a stack client we consider a classic example that also demonstrates the utility of generics some of the first pro grams that we considered in section involved computing the value of arithmetic expressions like this one if you multiply by add to multiply the result and then add you get the value but how does the java system do this calculation without going into the details of how the java system is built we can address the essential ideas by writing a java program that can take a string as input the expression and produce the number represented by the expression as output for simplicity we begin with the following explicit recursive definition an arithmetic expression is either a number or a left parenthesis followed by an arithmetic expression followed by an operator followed by another arithmetic ex pression followed by a right parenthesis for simplicity this definition is for fully paren thesized arithmetic expressions which specify precisely which operators apply to which operands you are a bit more familiar with expressions such as where we often rely on precedence rules instead of parentheses the same basic mechanisms that we consider can handle precedence rules but we avoid that complication for speci ficity we support the familiar binary operators and as well as a square root operator sqrt that takes just one argument we could easily allow more operators and more kinds of operators to embrace a large class of familiar mathematical expressions involving trigonometric exponential and logarithmic functions our focus is on un derstanding how to interpret the string of parentheses operators and numbers to en able performing in the proper order the low level arithmetic operations that are avail able on any computer precisely how can we convert an arithmetic expression a string of characters to the value that it represents a remarkably simple algorithm that was developed by e w dijkstra in the uses two stacks one for operands and one for operators to do this job an expression consists of parentheses operators and oper ands numbers proceeding from left to right and taking these entities one at a time we manipulate the stacks according to four possible cases as follows push operands onto the operand stack push operators onto the operator stack ignore left parentheses on encountering a right parenthesis pop an operator pop the requisite number of operands and push onto the operand stack the result of applying that opera tor to those operands after the final right parenthesis has been processed there is one value on the stack which is the value of the expression this method may seem mysterious at first but it dijkstra two stack algorithm for expression evaluation public class evaluate public static void main string args stack string ops new stack string stack double vals new stack double while stdin isempty read token push if operator string stdin readstring if equals else if equals ops push else if equals ops push else if equals ops push else if equals ops push else if equals sqrt ops push else if equals pop evaluate and push result if token is string op ops pop double v vals pop if op equals v vals pop v else if op equals v vals pop v else if op equals v vals pop v else if op equals v vals pop v else if op equals sqrt v math sqrt v vals push v token not operator or paren push double value else vals push double parsedouble stdout println vals pop this stack client uses two stacks to evaluate arithmetic expressions illustrating an essential compu tational process interpreting a string as a program and executing that program to compute the de sired result with generics we can use the code in a single stack implementation to implement one stack of string values and another stack of double values for simplicity this code assumes that the expres sion is fully parenthesized with numbers and characters separated by whitespace java evaluate java evaluate sqrt is easy to convince yourself that it computes the proper value any time the algorithm encounters a subexpression consisting of two operands separated by an operator all surrounded by parentheses it leaves the result of performing that operation on those operands on the operand stack the result is the same as if that value had appeared in the input instead of the subexpression so we can think of replacing the subexpression by the value to get an expression that would yield the same result we can apply this argument again and again until we get a single value for example the algorithm com putes the same value for all of these expressions evaluate on the previous page is an implementation of this algorithm this code is a simple example of an interpreter a program that interprets the computation specified by a given string and performs the computation to arrive at the result operand stack operator stack left parenthesis ignore operand push onto operand stack operator push onto operator stack right parenthesis pop operator and operands and push result trace of dijkstra two stack arithmetic expression evaluation algorithm implementing collections to address the issue of implementing bag stack and queue we begin with a simple classic implementation then address improvements that lead us to implementations of the apis articulated on page fixed capacity stack as a strawman we consider an abstract data type for a fixed capacity stack of strings shown on the opposite page the api differs from our stack api it works only for string values it requires the client to specify a capacity and it does not support iteration the primary choice in developing an api implementation is to choose a representation for the data for fixedcapacitystackofstrings an obvious choice is to use an array of string values pursuing this choice leads to the implemen tation shown at the bottom on the opposite page which could hardly be simpler each method is a one liner the instance variables are an array a that holds the items in the stack and an integer n that counts the number of items in the stack to remove an item we decrement n and then return a n to insert a new item we set a n equal to the new item and then increment n these operations preserve the following properties the items in the array are in their insertion order the stack is empty when n is the top of the stack if it is nonempty is at a n as usual thinking in terms of invariants of this sort is the easiest way to verify that an implementation operates as intended be sure that you fully understand this implemen tation the best way to do so is to examine a trace of the stack contents for a sequence of operations as illustrated at left for the test client which reads strings from standard input and push es each string onto a stack unless it is when it pops the stack and prints the result the primary performance characteristic of this implementation is that the push and pop operations take time inde pendent of the stack size for many applications it is the method of choice because of its simplicity but it has several drawbacks that limit its potential applicability as a general purpose tool which we now address with a moderate amount of effort and some help from java language mechanisms we can develop an implementation that is broadly useful this effort is worthwhile because the im plementations that we develop serve as a model for implementations of other more powerful abstract trace of fixedcapacitystackofstrings test client data types throughout the book api public class fixedcapacitystackofstrings fixedcapacitystackofstrings int cap create an empty stack of capacity cap void push string item add a string string pop remove the most recently added string boolean isempty is the stack empty int size number of strings on the stack test client application implementation an abstract data type for a fixed capacity stack of strings generics the first drawback of fixedcapacitystackofstrings is that it works only for string objects if we want a stack of double values we would need to develop another class with similar code essentially replacing string with double everywhere this is easy enough but becomes burdensome when we consider building a stack of transaction values or a queue of date values and so forth as discussed on page java parameterized types generics are specifically designed to address this situation and we saw several examples of client code on pages and but how do we implement a generic stack the code on the facing page shows the details it imple ments a class fixedcapacitystack that differs from fixedcapacitystackofstrings only in the code highlighted in red we replace every occurrence of string with item with one exception discussed below and declare the class with the following first line of code public class fixedcapacitystack item the name item is a type parameter a symbolic placeholder for some concrete type to be used by the client you can read fixedcapacitystack item as stack of items which is precisely what we want when implementing fixedcapacitystack we do not know the actual type of item but a client can use our stack for any type of data by providing a concrete type when the stack is created concrete types must be reference types but cli ents can depend on autoboxing to convert primitive types to their corresponding wrap per types java uses the type parameter item to check for type mismatch errors even though no concrete type is yet known variables of type item must be assigned values of type item and so forth but there is one significant hitch in this story we would like to implement the constructor in fixedcapacitystack with the code a new item cap which calls for creation of a generic array for historical and technical reasons beyond our scope generic array creation is disallowed in java instead we need to use a cast a item new object cap this code produces the desired effect though the java compiler gives a warning which we can safely ignore and we use this idiom throughout the book the java system li brary implementations of similar abstract data types use the same idiom api public class fixedcapacitystack item fixedcapacitystack int cap create an empty stack of capacity cap void push item item add an item item pop remove the most recently added item boolean isempty is the stack empty int size number of items on the stack test client application implementation an abstract data type for a fixed capacity generic stack array resizing choosing an array to represent the stack contents implies that clients must estimate the maximum size of the stack ahead of time in java we cannot change the size of an array once created so the stack always uses space proportional to that maximum a client that chooses a large capacity risks wasting a large amount of mem ory at times when the collection is empty or nearly empty for example a transaction system might involve billions of items and thousands of collections of them such a client would have to allow for the possibility that each of those collections could hold all of those items even though a typical constraint in such systems is that each item can appear in only one collection moreover every client risks overflow if the collection grows larger than the array for this reason push needs code to test for a full stack and we should have an isfull method in the api to allow clients to test for that condition we omit that code because our desire is to relieve the client from having to deal with the concept of a full stack as articulated in our original stack api instead we modify the array implementation to dynamically adjust the size of the array a so that it is both sufficiently large to hold all of the items and not so large as to waste an excessive amount of space achieving these goals turns out to be remarkably easy first we implement a method that moves a stack into an array of a different size private void resize int max move stack of size n max to a new array of size max item temp item new object max for int i i n i temp i a i a temp now in push we check whether the array is too small in particular we check wheth er there is room for the new item in the array by checking whether the stack size n is equal to the array size a length if there is no room we double the size of the array then we simply insert the new item with the code a n item as before public void push string item add item to top of stack if n a length resize a length a n item similarly in pop we begin by deleting the item then we halve the array size if it is too large if you think a bit about the situation you will see that the appropriate test is whether the stack size is less than one fourth the array size after the array is halved it will be about half full and can accommodate a substantial number of push and pop operations before having to change the size of the array again public string pop remove item from top of stack string item a n a n null avoid loitering see text if n n a length resize a length return item with this implementation the stack never overflows and never becomes less than one quarter full unless the stack is empty when the array size is we will address the performance analysis of this approach in more detail in section loitering java garbage collection policy is to reclaim the memory associated with any objects that can no longer be accessed in our pop implementations the refer ence to the popped item remains in the array the item is effectively an orphan it will be never be accessed again but the java garbage collector has no way to know this until it is overwritten even when the client is done with the item the reference in the array may keep it alive this condition holding a reference to an item that is no longer needed is known as loitering in this case loitering is easy to avoid by setting the array entry corresponding to the popped item to null thus overwriting the unused refer ence and making it possible for the system to reclaim the memory associated with the popped item when the client is finished with it a push pop n a length null to be or not to to to to be be be or or null not to to be or not to null null null to to be or not null null null null be to be or not be null null null be to be or not null null null null not to be or null null null null null that to be or that null null null null that to be or null null null null null or to be null null be to null is to is trace of array resizing during a sequence of push and pop operations iteration as mentioned earlier in this section one of the fundamental operations on collections is to process each item by iterating through the collection using java foreach statement this paradigm leads to clear and compact code that is free from dependence on the details of a collection implementation to consider the task of implementing iteration we start with a snippet of client code that prints all of the items in a collection of strings one per line stack string collection new stack string for string collection stdout println now this foreach statement is shorthand for a while construct just like the for state ment itself it is essentially equivalent to the following while statement iterator string i collection iterator while i hasnext string i next stdout println this code exposes the ingredients that we need to implement in any iterable collection the collection must implement an iterator method that returns an iterator object the iterator class must include two methods hasnext which returns a boolean value and next which returns a generic item from the collection in java we use the interface mechanism to express the idea that a class implements a specific method see page for iterable collections the necessary interfaces are al ready defined for us in java to make a class iterable the first step is to add the phrase implements iterable item to its declaration matching the interface public interface iterable item iterator item iterator which is in java lang iterable and to add a method iterator to the class that returns an iterator item iterators are generic so we can use our parameterized type item to allow clients to iterate through objects of whatever type is provided by our client for the array representation that we have been using we need to iterate through an array in reverse order so we name the iterator reversearrayiterator and add this method public iterator item iterator return new reversearrayiterator what is an iterator an object from a class that implements the methods hasnext and next as defined in the following interface which is in java util iterator public interface iterator item boolean hasnext item next void remove although the interface specifies a remove method we always use an empty method for remove in this book because interleaving iteration with operations that modify the data structure is best avoided for reversearrayiterator these methods are all one liners implemented in a nested class within our stack class private class reversearrayiterator implements iterator item private int i n public boolean hasnext return i public item next return a i public void remove note that this nested class can access the instance variables of the enclosing class in this case a and n this ability is the main reason we use nested classes for iterators technically to conform to the iterator specification we should throw exceptions in two cases an unsupportedoperationexception if a client calls remove and a nosuchelementexception if a client calls next when i is since we only use itera tors in the foreach construction where these conditions do not arise we omit this code one crucial detail remains we have to include import java util iterator at the beginning of the program because for historical reasons iterator is not part of java lang even though iterable is part of java lang now a client using the foreach statement for this class will get behavior equivalent to the common for loop for arrays but does not need to be aware of the array representation an implementation detail this arrangement is of critical importance for implementations of fundamen tal data types like the collections that we consider in this book and those included in java libraries for example it frees us to switch to a totally different representation without having to change any client code more important taking the client point of view it allows clients to use iteration without having to know any details of the class implementation algorithm is an implementation of our stack api that resizes the array allows clients to make stacks for any type of data and supports client use of foreach to iterate through the stack items in lifo order this implementation is based on java language nuances involving iterator and iterable but there is no need to study those nuances in detail as the code itself is not complicated and can be used as a template for other collection implementations for example we can implement the queue api by maintaining two indices as in stance variables a variable head for the beginning of the queue and a variable tail for the end of the queue to remove an item use head to access it and then increment head to insert an item use tail to store it and then increment tail if incrementing an index brings it past the end of the array reset it to developing the details of checking when the queue is empty and when the array is full and needs resizing is an interesting and worthwhile programming exercise see exercise stdin stdout a enqueue dequeue n head tail to be or not to to to be or not to be to be or not to be be to be or not to be or to be or that to be trace of resizingarrayqueue test client in the context of the study of algorithms algorithm is significant because it almost but not quite achieves optimum performance goals for any collection implementation each operation should require time independent of the collection size the space used should always be within a constant factor of the collection size the flaw in resizingarraystack is that some push and pop operations require resiz ing this takes time proportional to the size of the stack next we consider a way to cor rect this flaw using a fundamentally different way to structure data algorithm pushdown lifo stack resizing array implementation import java util iterator public class resizingarraystack item implements iterable item private item a item new object stack items private int n number of items public boolean isempty return n public int size return n private void resize int max move stack to a new array of size max item temp item new object max for int i i n i temp i a i a temp public void push item item add item to top of stack if n a length resize a length a n item public item pop remove item from top of stack item item a n a n null avoid loitering see text if n n a length resize a length return item public iterator item iterator return new reversearrayiterator private class reversearrayiterator implements iterator item support lifo iteration private int i n public boolean hasnext return i public item next return a i public void remove this generic iterable implementation of our stack api is a model for collection adts that keep items in an array it resizes the array to keep the array size within a constant factor of the stack size linked lists now we consider the use of a fundamental data structure that is an ap propriate choice for representing the data in a collection adt implementation this is our first example of building a data structure that is not directly supported by the java language our implementation serves as a model for the code that we use for building more complex data structures throughout the book so you should read this section carefully even if you have experience working with linked lists the node in this definition is an abstract entity that might hold any kind of data in ad dition to the node reference that characterizes its role in building linked lists as with a recursive program the concept of a recursive data structure can be a bit mindbending at first but is of great value because of its simplicity node record with object oriented programming implementing linked lists is not dif ficult we start with a nested class that defines the node abstraction private class node item item node next a node has two instance variables an item a parameterized type and a node we define node within the class where we want to use it and make it private because it is not for use by clients as with any data type we create an object of type node by in voking the no argument constructor with new node the result is a reference to a node object whose instance variables are both initialized to the value null the item is a placeholder for any data that we might want to structure with a linked list we will use java generic mechanism so that it can represent any reference type the instance vari able of type node characterizes the linked nature of the data structure to emphasize that we are just using the node class to structure the data we define no methods and we refer directly to the instance variables in code if first is a variable associated with an object of type node we can refer to the instance variables with the code first item and first next classes of this kind are sometimes called records they do not imple ment abstract data types because we refer directly to instance variables however node and its client code are in the same class in all of our implementations and not accessible by clients of that class so we still enjoy the benefits of data abstraction building a linked list now from the recursive definition we can represent a linked list with a variable of type node simply by ensuring that its value is either null or a ref erence to a node whose next field is a reference to a linked list for example to build a linked list that contains the items to be and or we create a node for each item node first new node node second new node node third new node and set the item field in each of the nodes to the desired value for simplicity these examples assume that item is string first item to second item be third item or and set the next fields to build the linked list node first new node first item to first node second new node second item be first next second first next second second next third note that third next remains null the value it was initialized to at the time of creation as a result first second third is a linked list it is a reference to a node that has a reference to null which is the null reference to an empty linked list and second is a linked list node third new node third item or second next third it is a reference to a node that has a reference to third which is a linked list and first is a linked list it is a reference to a node that has a reference to second which is a linked list the code that we will examine does these assignment statements in a dif first second third ferent order depicted in the diagram on this page linking together a list a linked list represents a sequence of items in the example just considered first represents the sequence to be or we can also use an array to represent a sequence of items for example we could use string to be or to represent the same sequence of strings the difference is that it is easier to insert items into the sequence and to remove items from the sequence with linked lists next we consider code to accomplish these tasks when tracing code that uses linked lists and other linked structures we use a visual representation where we draw a rectangle to represent each object we put the values of instance variables within the rectangle we use arrows that point to the referenced objects to depict references this visual representation captures the essential characteristic of linked lists for econ omy we use the term links to refer to node references for simplicity when item values are strings as in our examples we put the string within the object rectangle rather than the more accurate rendition depicting the string object and the character array that we discussed in section this visual representation allows us to focus on the links insert at the beginning first suppose that you want to insert a new node into a linked list the easiest place to do so is at the beginning of the list for example to insert the string not at the beginning of a given linked list whose first node is first we save first in oldfirst assign to first a new node and assign its item field to not and its next field to oldfirst this code for inserting a node at the beginning of a linked list involves just a few assignment statements so the amount of time that it takes is inde pendent of the length of the list save a link to the list node oldfirst first oldfirst first create a new node for the beginning first new node oldfirst first set the instance variables in the new node first item not first next oldfirst first inserting a new node at the beginning of a linked list remove from the beginning next suppose that you want to remove the first node from a list this op eration is even easier simply assign to first the value first next normally you would retrieve the value of the item by assigning it to some variable of type item before doing this assignment because once you change the value of first you may not have any access to the node to which it was referring typically the node ob ject becomes an orphan and the java memory manage ment system eventually reclaims the memory it occupies first first next first first removing the first node in a linked list again this operation just involves one assignment statement so its running time is independent of the length of the list insert at the end how do we add a node to the end of a linked list to do so we need a link to the last node in the list because that node link has to be changed to refer ence a new node containing the item to be inserted maintaining an extra link is not something that should be taken lightly in linked list code because every method that modifies the list needs code to check whether that variable needs to be modified and to make the necessary modifications for example the code that we just examined for removing the first node in the list might in volve changing the reference to the last node in the list since when there is only one node in the list it is both the first one and the last one also this code does not work it follows a null link in the case that the list is empty details like these make linked list code noto riously difficult to debug save a link to the last node node oldlast last oldlast first create a new node for the end node last new node last item not last insert remove at other positions in sum mary we have shown that we can implement the following operations on linked lists with just a few instructions provided that we have access to both a link first to the first ele ment in the list and a link last to the last element in the list insert at the beginning remove from the beginning first link the new node to the end of the list oldlast next last first oldlast oldlast last last insert at the end inserting a new node at the end of a linked list other operations such as the following are not so easily handled remove a given node insert a new node before a given node for example how can we remove the last node from a list the link last is no help because we need to set the link in the previous node in the list the one with the same value as last to null in the absence of any other information the only solution is to traverse the entire list looking for the node that links to last see below and exercise such a solution is undesirable because it takes time proportional to the length of the list the standard solution to enable arbitrary insertions and deletions is to use a doubly linked list where each node has two links one in each direction we leave the code for these operations as an exercise see exercise we do not need doubly linked lists for any of our implementations traversal to examine every item in an array we use familiar code like the following loop for processing the items in an array a for int i i n i process a i there is a corresponding idiom for examining the items in a linked list we initialize a loop index variable x to reference the first node of the linked list then we find the item associated with x by accessing x item and then update x to refer to the next node in the linked list assigning to it the value of x next and repeating this process until x is null which indicates that we have reached the end of the linked list this process is known as traversing the list and is succinctly expressed in code like the following loop for pro cessing the items in a linked list whose first item is associated with the variable first for node x first x null x x next process x item this idiom is as natural as the standard idiom for iterating through the items in an ar ray in our implementations we use it as the basis for iterators for providing client code the capability of iterating through the items without having to know the details of the linked list implementation stack implementation given these preliminaries developing an implementation for our stack api is straightforward as shown in algorithm on page it maintains the stack as a linked list with the top of the stack at the beginning referenced by an instance variable first thus to push an item we add it to the beginning of the list using the code discussed on page and to pop an item we remove it from the beginningof the list usingthecodediscussedonpage to implement size wekeep track of the number of items in an instance variable n incrementing n when we push and decrementing n when we pop to implement isempty we check whether first is null alternatively we could check whether n is the implementation uses the generic type item you can think of the code item after the class name as meaning that any occurrence of item in the implementation will be replaced by a client supplied data type name see page for now we omit the code to support iteration which we consider on page a trace for the test client that we have been using is shown on the next page this use of linked lists achieves our optimum design goals it can be used for any type of data the space required is always proportional to the size of the collection the time per operation is always independent of the size of the collection this implementation is a prototype for many algorithm implementations that we con sider it defines the linked list data structure and implements the client methods push and pop that achieve the specified effect with just a few lines of code the algorithms and data structure go hand in hand in this case the code for the algorithm implemen tations is quite simple but the properties of the data structure are not at all elemen tary requiring explanations on the past several pages this interaction between data structure definition and algorithm implementation is typical and is our focus in adt implementations throughout this book stdin stdout to be or not to be that is to be not that or be trace of stack development client algorithm pushdown stack linked list implementation public class stack item implements iterable item private node first top of stack most recently added node private int n number of items private class node nested class to define nodes item item node next public boolean isempty return first null or n public int size return n public void push item item add item to top of stack node oldfirst first first new node first item item first next oldfirst n public item pop remove item from top of stack item item first item first first next n return item see page for iterator implementation see page for test client main this generic stack implementation is based on a linked list data structure it can be used to create stacks containing any type of data to support iteration add the highlighted code described for bag on page queue implementation an implementation of our queue api based on the linked list data structure is also straightforward as shown in algorithm on the facing page it maintains the queue as a linked list in order from least recently to most recently added items with the beginning of the queue referenced by an instance variable first and the end of the queue referenced by an instance variable last thus to enqueue an item we add it to the end of the list using the code discussed on page augmented to set both first and last to refer to the new node when the list is empty and to dequeue an item we remove it from the beginning of the list using the same code as for pop in stack augmented to update last when the list becomes empty the implementations of size and isempty are the same as for stack as with stack the implementation uses the generic type parameter item and we omit the code to support iteration which we consider in our bag implementation on page a develop ment client similar to the one we used for stack is shown below and the trace for this client is shown on the following page this implementation uses the same data struc ture as does stack a linked list but it implements different algorithms for adding and removing items which make the difference between lifo and fifo for the client again the use of linked lists achieves our optimum design goals it can be used for any type of data the space required is proportional to the number of items in the collection and the time required per operation is always independent of the size of the collection algorithm fifo queue public class queue item implements iterable item private node first link to least recently added node private node last link to most recently added node private int n number of items on the queue private class node nested class to define nodes item item node next public boolean isempty return first null or n public int size return n public void enqueue item item add item to the end of the list node oldlast last last new node last item item last next null if isempty first last else oldlast next last n public item dequeue remove item from the beginning of the list item item first item first first next if isempty last null n return item see page for iterator implementation see page for test client main this generic queue implementation is based on a linked list data structure it can be used to create queues containing any type of data to support iteration add the highlighted code described for bag on page stdin stdout to be or not to be that is to be or not to be trace of queue development client linked lists are a fundamental alternative to arrays for structuring a collection of data from a historical perspective this alternative has been available to program mers for many decades indeed a landmark in the history of programming languages was the development of lisp by john mccarthy in the where linked lists are the primary structure for programs and data programming with linked lists presents all sorts of challenges and is notoriously difficult to debug as you can see in the exercises in modern code the use of safe pointers automatic garbage collection see page and adts allows us to encapsulate list processing code in just a few classes such as the ones presented here bag implementation implementing our bag api using a linked list data structure is simply a matter of changing the name of push in stack to add and removing the implementation of pop as shown in algorithm on the facing page doing the same for queue would also be effective but requires a bit more code this implemen tation also highlights the code needed to make stack queue and bag all iterable by traversing the list for stack the list is in lifo order for queue it is in fifo order and for bag it happens to be in lifo order but the order is not relevant as detailed in the highlighted code in algorithm to implement iteration in a collection the first step is to include import java util iterator so that our code can refer to java iterator interface the second step is to add implements iterable item to the class declaration a promise to provide an iterator method the iterator method itself simply returns an object from a class that implements the iterator interface public iterator item iterator return new listiterator this code is a promise to implement a class that implements the hasnext next and remove methods that are called when a client uses the foreach construct to implement these methods the nested class listiterator in algorithm maintains an instance variable current that keeps track of the current node on the list then the hasnext method tests if current is null and the next method saves a reference to the current item updates current to refer to the next node on the list and returns the saved reference algorithm bag import java util iterator public class bag item implements iterable item private node first first node in list private class node item item node next public void add item item same as push in stack node oldfirst first first new node first item item first next oldfirst public iterator item iterator return new listiterator private class listiterator implements iterator item private node current first public boolean hasnext return current null public void remove public item next item item current item current current next return item this bag implementation maintains a linked list of the items provided in calls to add code for isempty and size is the same as in stack and is omitted the iterator traverses the list main taining the current node in current we can make stack and queue iterable by adding the code highlighted in red to algorithms and because they use the same underlying data structure and stack and queue maintain the list in lifo and fifo order respectively overview the implementations of bags queues and stacks that support generics and iteration that we have considered in this section provide a level of abstraction that allows us to write compact client programs that manipulate collections of objects de tailed understanding of these adts is important as an introduction to the study of al gorithms and data structures for three reasons first we use these data types as building blocks in higher level data structures throughout this book second they illustrate the interplay between data structures and algorithms and the challenge of simultaneously achieving natural performance goals that may conflict third the focus of several of our implementations is on adts that support more powerful operations on collections of objects and we use the implementations here as starting points data structures we now have two ways to represent collections of objects arrays and linked lists arrays are built in to java linked lists are easy to build with standard java records these two alternatives often referred to as sequential allocation and linked al location are fundamental later in the book we develop adt implementations that combine and extend these basic structures in numerous ways one important exten data structure advantage disadvantage sion is to data structures with multiple index provides array immediate access to any item uses space proportional to size need to know size on initialization need reference to access an item links for example our focus in sections and is on data structures known as binary trees that are built from nodes that each have two links another important extension is to compose data structures fundamental data structures we can have a bag of stacks a queue of ar rays and so forth for example our focus in chapter is on graphs which we rep resent as arrays of bags it is very easy to define data structures of arbitrary complexity in this way one important reason for our focus on abstract data types is an attempt to control such complexity our treatment of bags queues and stacks in this section is a prototypical ex ample of the approach that we use throughout this book to describe data structures and algorithms in approaching a new applications domain we identify computational challenges and use data abstraction to address them proceeding as follows specify an api develop client code with reference to specific applications describe a data structure representation of the set of values that can serve as the basis for the instance variables in a class that will implement an adt that meets the specification in the api describe algorithms approaches to implementing the set of operations that can serve as the basis for implementing the instance methods in the class analyze the performance characteristics of the algorithms in the next section we consider this last step in detail as it often dictates which algo rithms and implementations can be most useful in addressing real world applications hash table hash table linearprobinghashst two arrays of objects graph adjacency lists graph array of bag objects trie triest node with array of links ternary search trie tst three links per node examples of data structures developed in this book q not all programming languages have generics even early versions of java what are the alternatives a one alternative is to maintain a different implementation for each type of data as mentioned in the text another is to build a stack of object values then cast to the desired type in client code for pop the problem with this approach is that type mis match errors cannot be detected until run time but with generics if you write code to push an object of the wrong type on the stack like this stack apple stack new stack apple apple a new apple orange b new orange stack push a stack push b compile time error you will get a compile time error push apple in stack apple cannot be applied to orange this ability to discover such errors at compile time is reason enough to use generics q why does java disallow generic arrays a experts still debate this point you might need to become one to understand it for starters learn about covariant arrays and type erasure q how do i create an array of stacks of strings a use a cast such as the following stack string a stack string new stack n warning this cast in client code is different from the one described on page you might have expected to use object instead of stack when using generics java checks for type safety at compile time but throws away that information at run time so it is left with stack object or just stack for short which we must cast to stack string q what happens if my program calls pop for an empty stack a it dependsontheimplementation forourimplementationonpage youwillgeta nullpointerexception in our implementations on the booksite we throw a runtime exception to help users pinpoint the error generally including as many such checks as possible is wise in code that is likely to be used by many people q why do we care about resizing arrays when we have linked lists a we will see several examples of adt implementations that need to use ar rays to perform other operations that are not easily supported with linked lists resizingarraystack is a model for keeping their memory usage under control q why declare node as a nested class why private a by declaring the nested class node to be private we restrict access to methods and instance variables within the enclosing class one characteristic of a private nested class is that its instance variables can be directly accessed from within the enclosing class but nowhere else so there is no need to declare the instance variables public or private note for experts a nested class that is not static is known as an inner class so technically our node classes are inner classes though the ones that are not generic could be static q when i type javac stack java to run algorithm and similar programs i find stack class and a file stack node class what is the purpose of that second one a that file is for the inner class node java naming convention is to use to separate the name of the outer class from the inner class q are there java libraries for stacks and queues a yes and no java has a built in library called java util stack but you should avoid using it when you want a stack it has several additional operations that are not normally associated with a stack e g getting the ith element it also allows adding an element to the bottom of the stack instead of the top so it can implement a queue although having such extra operations may appear to be a bonus it is actually a curse we use data types not just as libraries of all the operations we can imagine but also as a mechanism to precisely specify the operations we need the prime benefit of doing so is that the system can prevent us from performing operations that we do not actually q a continued want the java util stack api is an example of a wide interface which we generally strive to avoid q should a client be allowed to insert null items onto a stack or queue a this question arises frequently when implementing collections in java our imple mentation and java stack and queue libraries do permit the insertion of null values q what should the stack iterator do if the client calls push or pop during iterator a throw a java util concurrentmodificationexception to make it a fail fast it erator see q can i use a foreach loop with arrays a yes even though arrays do not implement the iterable interface the following one liner prints out the command line arguments public static void main string args for string args stdout println q can i use a foreach loop with strings a no string does not implement iterable q why not have a single collection data type that implements methods to add items remove the most recently inserted remove the least recently inserted remove random iterate return the number of items in the collection and whatever other operations we might desire then we could get them all implemented in a single class that could be used by many clients a again this is an example of a wide interface java has such implementations in its java util arraylist and java util linkedlist classes one reason to avoid them is that it there is no assurance that all operations are implemented efficiently through out this book we use apis as starting points for designing efficient algorithms and data structures which is certainly easier to do for interfaces with just a few operations as opposed to an interface with many operations another reason to insist on narrow in terfaces is that they enforce a certain discipline on client programs which makes client code much easier to understand if one client uses stack string and another uses queue transaction we have a good idea that the lifo discipline is important to the first and the fifo discipline is important to the second add a method isfull to fixedcapacitystackofstrings give the output printed by java stack for the input it was the best of times it was the suppose that a client performs an intermixed sequence of stack push and pop operations the push operations put the integers through in order onto the stack the pop operations print out the return values which of the following sequence could not occur a b c d e f g h write a stack client parentheses that reads in a text stream from standard input and uses a stack to determine whether its parentheses are properly balanced for ex ample your program should print true for and false for what does the following code fragment print when n is give a high level description of what it does when presented with a positive integer n stack integer stack new stack integer while n stack push n n n for int d stack stdout print d stdout println answer prints the binary representation of n when n is exercises continued what does the following code fragment do to the queue q stack string stack new stack string while q isempty stack push q dequeue while stack isempty q enqueue stack pop add a method peek to stack that returns the most recently inserted item on the stack without popping it give the contents and size of the array for doublingstackofstrings with the input it was the best of times it was the write a program that takes from standard input an expression without left pa rentheses and prints the equivalent infix expression with the parentheses inserted for example given the input your program should print write a filter infixtopostfix that converts an arithmetic expression from in fix to postfix write a program evaluatepostfix that takes a postfix expression from stan dard input evaluates it and prints the value piping the output of your program from the previous exercise to this program gives equivalent behavior to evaluate write an iterable stack client that has a static method copy that takes a stack of strings as argument and returns a copy of the stack note this ability is a prime example of the value of having an iterator because it allows development of such func tionality without changing the basic api suppose that a client performs an intermixed sequence of queue enqueue and dequeue operations the enqueue operations put the integers through in order onto the queue the dequeue operations print out the return value which of the following sequence could not occur a b c d develop a class resizingarrayqueueofstrings that implements the queue abstraction with a fixed size array and then extend your implementation to use array resizing to remove the size restriction write a queue client that takes a command line argument k and prints the kth from the last string found on standard input assuming that standard input has k or more strings using readints onpage asamodel writeastaticmethod readdates for date that reads dates from standard input in the format specified in the table on page and returns an array containing them do exercise for transaction this list of exercises is intended to give you experience in working with linked lists sugges tion make drawings using the visual representation described in the text suppose x is a linked list node and not the last node on the list what is the ef fect of the following code fragment x next x next next answer deletes from the list the node immediately following x give a code fragment that removes the last node in a linked list whose first node is first write a method delete that takes an int argument k and deletes the kth ele ment in a linked list if it exists write a method find that takes a linked list and a string key as arguments and returns true if some node in the list has key as its item field false otherwise suppose that x is a linked list node what does the following code fragment do t next x next x next t answer inserts node t immediately after node x why does the following code fragment not do the same thing as in the previous question x next t t next x next answer when it comes time to update t next x next is no longer the original node following x but is instead t itself write a method removeafter that takes a linked list node as argument and removes the node following the given one and does nothing if the argument or the next field in the argument node is null write a method insertafter that takes two linked list node arguments and inserts the second after the first on its list and does nothing if either argument is null write a method remove that takes a linked list and a string key as arguments and removes all of the nodes in the list that have key as its item field write a method max that takes a reference to the first node in a linked list as argument and returns the value of the maximum key in the list assume that all keys are positive integers and return if the list is empty develop a recursive solution to the previous question write a queue implementation that uses a circular linked list which is the same as a linked list except that no links are null and the value of last next is first when ever the list is not empty keep only one node instance variable last write a function that takes the first node in a linked list as argument and de structively reverses the list returning the first node in the result iterative solution to accomplish this task we maintain references to three consecutive nodes in the linked list reverse first and second at each iteration we extract the node first from the original linked list and insert it at the beginning of the reversed list we maintain the invariant that first is the first node of what left of the original list second is the second node of what left of the original list and reverse is the first node of the resulting reversed list public node reverse node x node first x node reverse null while first null node second first next first next reverse reverse first first second return reverse when writing code involving linked lists we must always be careful to properly handle the exceptional cases when the linked list is empty when the list has only one or two linked list exercises continued nodes and the boundary cases dealing with the first or last items this is usually much trickier than handling the normal cases recursive solution assuming the linked list has n nodes we recursively reverse the last n nodes and then carefully append the first node to the end public node reverse node first if first null return null if first next null return first node second first next node rest reverse second second next first first next null return rest implement a nested class doublenode for building doubly linked lists where each node contains a reference to the item preceding it and the item following it in the list null if there is no such item then implement static methods for the following tasks insert at the beginning insert at the end remove from the beginning remove from the end insert before a given node insert after a given node and remove a given node steque a stack ended queue or steque is a data type that supports push pop and enqueue articulate an api for this adt develop a linked list based implementation deque a double ended queue or deque pronounced deck is like a stack or a queue but supports adding and removing items at both ends a deque stores a collec tion of items and supports the following api public class deque item implements iterable item deque create an empty deque boolean isempty is the deque empty int size number of items in the deque void pushleft item item add an item to the left end void pushright item item add an item to the right end item popleft remove an item from the left end item popright remove an item from the right end api for a generic double ended queue write a class deque that uses a doubly linked list to implement this api and a class resizingarraydeque that uses a resizing array random bag a random bag stores a collection of items and supports the fol lowing api public class randombag item implements iterable item randombag create an empty random bag boolean isempty is the bag empty int size number of items in the bag void add item item add an item api for a generic random bag write a class randombag that implements this api note that this api is the same as for bag except for the adjective random which indicates that the iteration should provide creative problems continued the items in random order all n permutations equally likely for each iterator hint put the items in an array and randomize their order in the iterator constructor random queue a random queue stores a collection of items and supports the following api public class randomqueue item randomqueue create an empty random queue boolean isempty is the queue empty void enqueue item item add an item item dequeue remove and return a random item sample without replacement item sample return a random item but do not remove sample with replacement api for a generic random queue write a class randomqueue that implements this api hint use an array representation with resizing to remove an item swap one at a random position indexed through n with the one at the last position index n then delete and return the last ob ject as in resizingarraystack write a client that deals bridge hands cards each using randomqueue card random iterator write an iterator for randomqueue item from the previous exercise that returns the items in random order josephus problem in the josephus problem from antiquity n people are in dire straits and agree to the following strategy to reduce the population they arrange them selves in a circle at positions numbered from to n and proceed around the circle eliminating every mth person until only one person is left legend has it that josephus figured out where to sit to avoid being eliminated write a queue client josephus that takes n and m from the command line and prints out the order in which people are eliminated and thus would show josephus where to sit in the circle java josephus delete kth element implement a class that supports the following api public class generalizedqueue item generalizedqueue create an empty queue boolean isempty is the queue empty void insert item x add an item item delete int k delete and return the kth least recently inserted item api for a generic generalized queue first develop an implementation that uses an array implementation and then develop one that uses a linked list implementation note the algorithms and data structures that we introduce in chapter make it possible to develop an implementation that can guarantee that both insert and delete take time prortional to the logarithm of the number of items in the queue see exercise ring buffer a ring buffer or circular queue is a fifo data structure of a fixed size n it is useful for transferring data between asynchronous processes or for storing log files when the buffer is empty the consumer waits until data is deposited when the buffer is full the producer waits to deposit data develop an api for a ringbuffer and an implementation that uses an array representation with circular wrap around move to front read in a sequence of characters from standard input and maintain the characters in a linked list with no duplicates when you read in a previ ously unseen character insert it at the front of the list when you read in a duplicate character delete it from the list and reinsert it at the beginning name your program movetofront it implements the well known move to front strategy which is useful for caching data compression and many other applications where items that have been recently accessed are more likely to be reaccessed copy a queue create a new constructor so that queue item r new queue item q makes r a reference to a new and independent copy of the queue q you should be able to push and pop from either q or r without influencing the other hint delete all of the elements from q and add these elements to both q and r creative problems continued copy a stack create a new constructor for the linked list implementation of stack so that stack item t new stack item makes t a reference to a new and independent copy of the stack listing files a folder is a list of files and folders write a program that takes the name of a folder as a command line argument and prints out all of the files contained in that folder with the contents of each folder recursively listed indented under that folder name hint use a queue and see java io file text editor buffer develop a data type for a buffer in a text editor that imple ments the following api public class buffer buffer create an empty buffer void insert char c insert c at the cursor position char delete delete and return the character at the cursor void left int k move the cursor k positions to the left void right int k move the cursor k positions to the right int size number of characters in the buffer api for a text buffer hint use two stacks stack generability suppose that we have a sequence of intermixed push and pop operations as with our test stack client where the integers n in that order push directives are intermixed with n minus signs pop directives devise an algorithm that determines whether the intermixed sequence causes the stack to under flow you may use only an amount of space independent of n you cannot store the integers in a data structure devise a linear time algorithm that determines whether a given permutation can be generated as output by our test client depending on where the pop directives occur solution the stack does not overflow unless there exists an integer k such that the first k pop operations occur before the first k push operations if a given permutation can be generated it is uniquely generated as follows if the next integer in the output permuta tion is in the top of the stack pop it otherwise push it onto the stack forbidden triple for stack generability prove that a permutation can be gener ated by a stack as in the previous question if and only if it has no forbidden triple a b c such that a b c with c first a second and b third possibly with other intervening integers between c and a and between a and b partial solution suppose that there is a forbidden triple a b c item c is popped before a and b but a and b are pushed before c thus when c is pushed both a and b are on the stack therefore a cannot be popped before b catenable queues stacks or steques add an extra operation catenation that de structively concatenates two queues stacks or steques see exercise hint use a circular linked list maintaining a pointer to the last item two stacks with a deque implement two stacks with a single deque so that each operation takes a constant number of deque operations see exercise 49 queue with three stacks implement a queue with three stacks so that each queue operation takes a constant worst case number of stack operations warning high degree of difficulty fail fast iterator modify the iterator code in stack to immediately throw a java util concurrentmodificationexception if the client modifies the collection via push or pop during iteration b solution maintain a counter that counts the number of push and pop operations when creating an iterator store this value as an iterator instance variable before each call to hasnext and next check that this value has not changed since con struction of the iterator if it has throw the exception as people gain experience using computers they use them to solve difficult prob lems or to process large amounts of data and are invariably led to questions like these how long will my program take why does my program run out of memory you certainly have asked yourself these questions perhaps when rebuilding a music or photo library installing a new application working with a large document or work ing with a large amount of experimental data the questions are much too vague to be answered precisely the answers depend on many factors such as properties of the particular computer being used the particular data being processed and the particular program that is doing the job which implements some algorithm all of these factors leave us with a daunting amount of information to analyze despite these challenges the path to developing useful answers to these basic ques tions is often remarkably straightforward as you will see in this section this process is based on the scientific method the commonly accepted body of techniques used by sci entists to develop knowledge about the natural world we apply mathematical analysis to develop concise models of costs and do experimental studies to validate these models scientific method the very same approach that scientists use to understand the natural world is effective for studying the running time of programs observe some feature of the natural world generally with precise measurements hypothesize a model that is consistent with the observations predict events using the hypothesis verify the predictions by making further observations validate by repeating until the hypothesis and observations agree one of the key tenets of the scientific method is that the experiments we design must be reproducible so that others can convince themselves of the validity of the hypothesis hypotheses must also be falsifiable so that we can know for sure when a given hypoth esis is wrong and thus needs revision as einstein famously is reported to have said no amount of experimentation can ever prove me right a single experiment can prove me wrong we can never know for sure that any hypothesis is absolutely correct we can only validate that it is consistent with our observations observations our first challenge is to determine how to make quantitative mea surements of the running time of our programs this task is far easier than in the natu ral sciences we do not have to send a rocket to mars or kill laboratory animals or split an atom we can simply run the program indeed every time you run a program you are performing a scientific experiment that relates the program to the natural world and answers one of our core questions how long will my program take our first qualitative observation about most programs is that there is a problem size that characterizes the difficulty of the computational task normally the problem size is either the size of the input or the value of a command line argument intuitively the running time should increase with problem size but the question of by how much it increases naturally comes up every time we develop and run a program another qualitative observation for many programs is that the running time is rela tively insensitive to the input itself it depends primarily on the problem size if this relationship does not hold we need to take steps to better understand and perhaps better control the running time sensitivity to the input but it does often hold so we now focus on the goal of better quantifying the relationship between problem size and running time example as a running example we will work with the program threesum shown here which counts the number of triples in a file of n integers that sum to assum ing that overflow plays no role this computation may seem contrived to you but it is deeply related to numerous fun damental computational tasks for exam ple see exercise as a test input consider the file txt from the booksite which contains million ran domly generated int values the second eighth and tenth entries in txt sum to how many more such triples are there in the file threesum can tell us but can it do so in a reasonable amount of time what is the relationship between the problem size n and running time for threesum as a first experiment try running threesum on your computer for the files txt txt txt and txt on the booksite that contain the first and integers from txt respectively you can quickly determine that there are triples that sum to in txt and that there are triples that sum to in txt the program takes substantially more time to determine that there are triples that sum to in txt and as you wait for the program to finish for txt you will find yourself asking the question how long will my program take as you will see answering this question for this program turns out to be easy in deed you can often come up with a fairly accurate prediction while the program is running stopwatch reliably measuring the exact running time of a given program can be difficult fortu nately we are usually happy with estimates we want to be able to distinguish programs that will finish in a few seconds or a few minutes from those that might require a few days or a few months or more and we want to know when one program is twice as fast as another for the same task still we need accurate measurements to generate experimental data that we can use to formulate and to check the validity of hypotheses about the relationship between running time and problem size for this purpose we use the stopwatch data type shown on the facing page its elapsedtime method returns the elapsed time since it was created in seconds the implementation is based on using the java system currenttimemillis method which gives the current time in milliseconds to save the time when the constructor is invoked then uses it again to compute the elapsed time when elapsedtime is invoked java threesum txt tick tick tick java threesum txt tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick java threesum txt tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick tick observing the running time of a program api public class stopwatch stopwatch double elapsedtime create a stopwatch return elapsed time since creation typical client application implementation an abstract data type for a stopwatch analysis of experimental data the program doublingtest on the facing page is a more sophisticated stopwatch client that produces experimental data for threesum it generates a sequence of random input arrays doubling the array size at each step and prints the running times of threesum count for each input size these experiments are certainly reproducible you can also run them on your own computer as many times as you like when you run doublingtest you will find yourself in a prediction verification cycle it prints several lines very quickly but then slows down considerably each time it prints a line you find yourself wondering how long it will be until it prints the next line of course since you have a different computer from ours the actual run ning times that you get are likely to be different from those shown for our computer indeed if your computer is twice as fast as ours your running times will be about half ours which leads immediately to the well founded hypothesis that running times on different computers are likely to differ by a constant factor still you will find yourself asking the more detailed question how long will my program take as a function of the input size to help answer this question we plot the data the diagrams at the bottom of the facing page show the result of plotting the data both on a normal and on a log log scale with the problem size n on the x axis and the running time t n on the y axis the log log plot immediately leads to a hypothesis about the running time the data fits a straight line of slope on the log log plot the equation of such a line is lg t n lg n lg a where a is a constant which is equivalent to t n a n the running time as a function of the input size as desired we can use one of our data points to solve for a for example t a so a and then use the equation t n n to predict running times for large n informally we are checking the hypothesis that the data points on the log log plot fall close to this line statistical methods are available for doing a more careful analysis to find estimates of a and the exponent b but our quick calculations suffice to estimate running time for most purposes for example we can estimate the running time on our computer for n to be about seconds or about minutes the actual time was seconds while waiting for your computer to print the line for n in doublingtest you might use this method to predict when it will finish then check the result by waiting to see if your prediction is true program to perform experiments results of experiments public class doublingtest java doublingtest public static double timetrial int n time threesum count for n random digit ints int max int a new int n for int i i n i a i stdrandom uniform max max stopwatch timer new stopwatch int cnt threesum count a return timer elapsedtime public static void main string args print table of running times for int n true n n print time for problem size n double time timetrial n stdout printf n n time standard plot log log plot problem size n lg n analysis of experimental data the running time of threesum count so far this process mirrors the process scientists use when trying to understand properties of the real world a straight line in a log log plot is equivalent to the hy pothesis that the data fits the equation t n a nb such a fit is known as a power law a great many natural and synthetic phenomena are described by power laws and it is reasonable to hypothesize that the running time of a program does as well indeed for the analysis of algorithms we have mathematical models that strongly support this and similar hypotheses to which we now turn mathematical models in the early days of computer science d e knuth postu lated that despite all of the complicating factors in understanding the running times of our programs it is possible in principle to build a mathematical model to describe the running time of any program knuth basic insight is simple the total running time of a program is determined by two primary factors the cost of executing each statement the frequency of execution of each statement the former is a property of the computer the java compiler and the operating system the latter is a property of the program and the input if we know both for all instruc tions in the program we can multiply them together and sum for all instructions in the program to get the running time the primary challenge is to determine the frequency of execution of the statements some statements are easy to analyze for example the statement that sets cnt to in threesum count is executed exactly once others require higher level reasoning for example the if statement in threesum count is executed precisely n n n times the number of ways to pick three different numbers from the input array see exercise others depend on the input data for example the number of times the instruction cnt in threesum count is executed is precisely the number of triples that sum to in the input which could range from of them to all of them in the case of doublingtest where we generate the numbers randomly it is possible to do a prob abilistic analysis to determine the expected value of this quantity see exercise tilde approximations frequency analyses of this sort can lead to complicated and lengthy mathematical expressions for example consider the count just considered of the number of times the if statement in threesum is executed n n n n n n as is typical in such expressions the terms after the leading term are relatively small for exam ple when n the value of n n is certainly insignificant by com parison with n to allow us to ignore insignificant terms and therefore sub stantially simplify the mathematical formulas that we work with we often use a mathemati cal device known as the tilde notation this n n notation allows us to work with tilde approxi mations where we throw away low order terms that complicate formulas and represent a negli gible contribution to values of interest leading term approximation function tilde order typical tilde approximations order of growth description function constant logarithmic log n linear n linearithmic n log n quadratic n cubic n exponential n commonly encountered order of growth functions for example we use the approximation n to de scribe the number of times the if statement in threesum is executed since n n n di vided by n approaches as n grows most of ten we work with tilde approximations of the form g n af n where f n nb log n c with a b and c constants and refer to f n as the order of growth of g n when using the logarithm in the order of growth we gener ally do not specify the base since the constant a can absorb that detail this usage covers the relatively few functions that are commonly encountered in studying the order of growth of a program running time shown in the table at left with the exception of the exponential which we defer to context we will describe these functions in more de tail and briefly discuss why they appear in the analysis of algorithms after we complete our treatment of threesum approximate running time to follow through on knuth approach to develop a mathematical expression for the total running time of a java program we can in prin ciple study our java compiler to find the number of machine instructions correspond ing to each java instruction and study our machine specifications to find the time of execution of each of the machine instructions to produce a grand total this process for threesum is briefly summarized on the facing page we classify blocks of java state ments by their frequency of execution develop leading term approximations for the frequencies determine the cost of each statement and then compute a total note that some frequencies may depend on the input in this case the number of times cnt is executed certainly depends on the input it is the number of triples that sum to and could range from to n we stop short of exhibiting the details values of the constants for any particular system except to highlight that by using constant values for the time taken by the blocks of statements we are assuming that each block of java statements corresponds to machine instructions that require a specified fixed amount of time a key observation from this exercise is to note that only the instruc tions that are executed the most frequently play a role in the final total we refer to these instructions as the inner loop of the program for threesum the inner loop is the statements that increment k and test that it is less than n and the statements that test whether the sum of three given numbers is and possibly the statement that imple ments the count depending on the input this behavior is typical the running times of a great many programs depend only on a small subset of their instructions order of growth hypothesis in summary the experiments on page and the math ematical model on page both support the following hypothesis throughout this book we use the term property to refer to a hypothesis that needs to be validated through experimentation the end result of our mathematical analysis is precisely the same as the end result of our experimental analysis the running time of threesum is a n for a machine dependent constant a this match validates both the experiments and the mathematical model and also exhibits more insight about the public class threesum public static int count int a a int n a length int cnt for int i i n i b for int j i j n j c for int k j k n k d if a i a j a k e return cnt public static void main string args int a in readints args stdout println count a n n n x inner loop anatomy of a program statement execution frequencies statement time in frequency total time grand total n n t3 n x tilde approximation n assuming x is small order of growth n analyzing the running time of a program example program because it does not require experimentation to determine the exponent with some effort we could validate the value of a on a particular system as well though that activity is generally reserved for experts in situations where performance is critical analysis of algorithms hypotheses such as property a are significant because they relate the abstract world of a java program to the real world of a computer running it working with the order of growth allows us to take one further step to separate a pro gram from the algorithm it implements the idea that the order of growth of the run ning time of threesum is n does not depend on the fact that it is implemented in java or that it is running on your laptop or someone else cellphone or a supercomputer it depends primarily on the fact that it examines all the different triples of numbers in the input the algorithm that you are using and sometimes the input model determines the order of growth separating the algorithm from the implementation on a particular computer is a powerful concept because it allows us to develop knowledge about the performance of algorithms and then apply that knowledge to any computer for ex ample we might say that threesum is an implementation of the brute force algorithm compute the sum of all different triples counting those that sum to we expect that an implementation of this algorithm in any programming language on any computer will lead to a running time that is proportional to n in fact much of the knowledge about the performance of classic algorithms was developed decades ago but that knowledge is still relevant to today computers cost model we focus attention on properties of al gorithms by articulating a cost model that defines the basic operations used by the algorithms we are study ing to solve the problem at hand for example an ap propriate cost model for the sum problem shown at right is the number of times we access an array entry with this cost model we can make precise mathematical statements about prop erties of an algorithm not just a particular implementation as follows we use the term proposition to refer to mathematical truths about algorithms in terms of a cost model throughout this book we study the algorithms that we consider within the framework of a specific cost model our intent is to articulate cost models such that the order of growth of the running time for a given implementation is the same as the order of growth of the cost of the underlying algorithm in other words the cost model should include operations that fall within the inner loop we seek precise mathemati cal results about algorithms propositions and also hypotheses about performance of implementations properties that you can check through experimentation in this case proposition b is a mathematical truth that supports the hypothesis stated in property a which we have validated with experiments in accordance with the scien tific method summary for many programs developing a mathematical model of running time reduces to the following steps develop an input model including a definition of the problem size identify the inner loop define a cost model that includes operations in the inner loop determine the frequency of execution of those operations for the given input doing so might require mathematical analysis we will consider some examples in the context of specific fundamental algorithms later in the book if a program is defined in terms of multiple methods we normally consider the methods separately as an example consider our example program of section binarysearch binary search the input model is the array a of size n the inner loop is the statements in the single while loop the cost model is the compare operation compare the values of two array entries and the analysis discussed in section and given in full detail in proposition b in section shows that the num ber of compares is at most lg n whitelist the input model is the n numbers in the whitelist and the m numbers on standard input where we assume m n the inner loop is the statements in the single while loop the cost model is the compare operation inherited from binary search and the analysis is immediate given the analysis of binary search the number of compares is at most m lg n thus we draw the conclusion that the order of growth of the running time of the whitelist computation is at most m lg n subject to the following considerations if n is small the input output cost might dominate the number of compares depends on the input it lies between m and m lg n depending on how many of the numbers on standard input are in the whitelist and on how long the binary search takes to find the ones that are typi cally it is m lg n we are assuming that the cost of arrays sort is small compared to m lg n arrays sort implements the mergesort algorithm and in section we will see that the order of growth of the running time of mergesort is n log n see proposition g in chapter so this assumption is justified thus the model supports our hypothesis from section that the binary search algo rithm makes the computation feasible when m and n are large if we double the length of the standard input stream then we can expect the running time to double if we double the size of the whitelist then we can expect the running time to increase only slightly developing mathematical models for the analysis of algorithms is a fruitful area of research that is somewhat beyond the scope of this book still as you will see with binary search mergesort and many other algorithms understanding certain math ematical models is critical to understanding the efficiency of fundamental algorithms so we often present details and or quote the results of classic studies when doing so we encounter various functions and approximations that are widely used in mathemati cal analysis for reference we summarize some of this information in the tables below description notation definition floor x largest integer not greater than x ceiling x smallest integer not smaller than x natural logarithm ln n log e n x such that e x n binary logarithm lg n log n x such that n integer lg n binary logarithm largest integer not greater than lg n bits in binary representation of n harmonic numbers hn n factorial n x x x x x n commonly encountered functions in the analysis of algorithms description approximation harmonic sum hn n ln n triangular sum n n geometric sum n when n stirling approximation binomial coefficients lg n lg lg lg lg lg n n lg n n nk k when k is a small constant exponential x x e useful approximations for the analysis of algorithms order of growth classifications we use just a few structural primitives state ments conditionals loops nesting and method calls to implement algorithms so very often the order of growth of the cost is one of just a few functions of the problem size n these functions are summarized in the table on the facing page along with the names that we use to refer to them typical code that leads to each function and examples constant a program whose running time order of growth is constant executes a fixed number of operations to finish its job consequently its running time does not depend on n most java operations take constant time logarithmic a program whose running time order of growth is logarithmic is barely slower than a constant time program the classic example of a program whose running time is logarithmic in the problem size is binary search see binarysearch on page the base of the logarithm is not relevant with respect to the order of growth since all logarithms with a constant base are related by a constant factor so we use log n when referring to order of growth linear programs that spend a constant amount of time processing each piece of input data or that are based on a single for loop are quite common the order of growth of such a program is said to be linear its running time is proportional to n linearithmic we use the term linearithmic to describe programs whose running time for a problem of size n has order of growth n log n again the base of the logarithm is not relevant with respect to the order of growth the prototypical examples of lin earithmic algorithms are merge sort see algorithm and quick sort see algorithm quadratic a typical program whose running time has order of growth n has two nested for loops used for some calculation involving all pairs of n elements the elementary sorting algorithms selection sort see algorithm and insertion sort see algorithm are prototypes of the programs in this classification cubic a typical program whose running time has order of growth n has three nested for loops used for some calculation involving all triples of n elements our example for this section threesum is a prototype exponential in chapter but not until then we will consider programs whose running times are proportional to or higher generally we use the term exponential to refer to algorithms whose order of growth is b n for any constant b even though different values of b lead to vastly different running times exponential algorithms are extremely slow you will never run one of them to completion for a large problem still exponential algorithms play a critical role in the theory of algorithms because description order of typical code framework description example constant a b c statement add two numbers logarithmic log n see page divide in half binary search linear n double max a for int i i n i if a i max max a i loop find the maximum linearithmic n log n see algorithm divide and mergesort quadratic n for int i i n i for int j i j n j if a i a j cnt double loop check all pairs cubic n for int i i n i for int j i j n j for int k j k n k if a i a j a k cnt triple loop check all triples exponential n see chapter exhasutive check all subsets summary of common order of growth hypotheses there exists a large class of problems for which it seems that an exponential algorithm is the best possible choice these classifications are the most common but certainly not a complete set the order of growth of an algorithm cost might be n log n or n or some similar func tion indeed the detailed analysis of algorithms standard plot log log plot 4t t 500k problem size constant 8k problem size typical orders of growth can require the full gamut of mathematical tools that have been developed over the centuries a great many of the algorithms that we con sider have straightforward performance charac teristics that can be accurately described by one of the orders of growth that we have considered accordingly we can usually work with specific propositions with a cost model such as mergesort uses between n lg n and n lg n compares that immediately imply hypotheses properties such as the order of growth of mergesort running time is linearithmic for economy we abbreviate such a statement to just say mergesort is linearithmic the plots at left indicate the importance of the order of growth in practice the x axis is the problem size the y axis is the running time these charts make plain that quadratic and cubic algorithms are not feasible for use on large prob lems as it turns out several important prob lems have natural solutions that are quadratic but clever algorithms that are linearithmic such algorithms including mergesort are critically important in practice because they enable us to address problem sizes far larger than could be addressed with quadratic solutions naturally we therefore focus in this book on developing loga rithmic linear and linearithmic algorithms for fundamental problems designing faster algorithms one of the primary reasons to study the order of growth of a program is to help design a faster algorithm to solve the same problem to illustrate this point we consider next a faster algorithm for the sum problem how can we devise a faster algorithm before even embarking on the study of algorithms the answer to this question is that we have discussed and used two classic algorithms mergesort and binary search have introduced the facts that the mergesort is linearith mic and binary search is logarithmic how can we take advantage of these algorithms to solve the sum problem warmup sum consider the easier problem of determining the number of pairs of integers in an input file that sum to to simplify the discussion assume also that the integers are distinct this problem is easily solved in quadratic time by deleting the k loop and a k from threesum count leaving a double loop that examines all pairs as shown in the quadratic entry in the table on page we refer to such an implementa tion as twosum the implementation below shows how mergesort and binary search see page can serve as a basis for a linearithmic solution to the sum problem the improved algorithm is based on the fact that an entry a i is one of a pair that sums to if and only if the value a i is in the array and a i is not zero to solve the prob lem we sort the array to enable binary search and then for every entry a i in the ar ray do a binary search for a i with rank in binarysearch if the result is an index j with j i we increment the count this succinct test covers three cases an unsuccessful binary search re turns so we do not increment the count if the binary search re turns j i we have a i a j so we incre ment the count if the binary search returns j between and i we also have a i a j but do not increment the count to avoid double counting the result of the computation is precise ly the same as the result of the quadratic algorithm but it takes much less time the running time of the mergesort is proportional to n log n and the n binary searches each take time proportional to log n so the running time of the whole algorithm is proportional to n log n developing a faster algorithm like this is not merely an academic exercise the faster algorithm enables us to address much larger problems for example you are likely to be able to solve the sum problem for million integers txt in a reasonable amount of time on your computer but you would have to wait quite a long time to do it with the quadratic algorithm see exercise fast algorithm for sum the very same idea is effective for the sum problem again assume also that the integers are distinct a pair a i and a j is part of a triple that sums to if and only if the value a i a j is in the array and not a i or a j the code below sorts the array then does n n binary searches that each take time proportional to log n for a total running time proportional to n log n note that in this case the cost of the sort is insignificant again this solution enables us to ad dress much larger problems see exercise the plots in the figure at the bottom of the next page show the disparity in costs among these four algorithms for problem sizes in the range we have considered such differences certainly motivate the search for faster algorithms lower bounds the table on page summarizes the discussion of this section an in teresting question immediately arises can we find algorithms for the sum and sum problems that are substantially faster than twosumfast and threesumfast is there a linear algorithm for sum or a linea rithmic algorithm for sum the answer to this question is no for sum under a model that counts and allows only compari sons of linear or quadratic func tions of the numbers and no one knows for sum though experts believe that the best possible al gorithm for sum is quadratic the idea of a lower bound on the order of growth of the worst case running time for all possible al gorithms to solve a problem is a very powerful one which we will revisit in detail in section in the context of sorting non trivial lower bounds are difficult to establish but very helpful in guiding our search for efficient algorithms algorithm order of growth of running time the examples in this section set the stage for our treat ment of algorithms in this book throughout the book our strategy for addressing new problems is the following implement and analyze a straighforward solution to the problem we usually refer to such solutions like threesum and twosum as the brute force solution examine algorithmic improvements usually designed twosum n twosumfast n log n threesum n threesumfast n log n summary of running times to reduce the order of growth of the running time such as twosumfast and threesumfast run experiments to validate the hypotheses that the new algorithms are faster in many cases we examine several algorithms for the same problem because running time is only one consideration when choosing an algorithm for a practical problem we will develop this idea in detail in the context of fundamental problems throughout the book lgn 600 200 8k problem size n 4k 8k problem size n costs of algorithms to solve the sum and sum problems doubling ratio experiments the following is a simple and effective shortcut for predicting performance and for determining the approximate order of growth of the running time of any program develop an input generator that produces inputs that model the inputs expected in practice such as the random integers in timetrial in doublingtest run the program doublingratio given below a modification of doublingtest that calculates the ratio of each running time with the previous run until the ratios approach a limit this test is not effective if the ratios do not approach a limiting value but they do for many many programs implying the following conclusions the order of growth of the running time is approximately nb to predict running times multiply the last observed running time by and double n continuing as long as desired if you want to predict for an input size that is not a power of times n you can adjust ratios accordingly see exercise as illustrated below the ratio for threesum is about and we can predict the running times for n to be seconds respectively just by successively multiplying the last time for 51 by program to perform experiments results of experiments 2000 51 predictions 408 26163 this test is roughly equivalent to the process described on page run experiments plot values on a log log plot to develop the hypothesis that the running time is anb determine the value of b from the slope of the line then solve for a but it is sim pler to apply indeed you can accurately predict preformance by hand when you run doublingratio as the ratio approaches a limit just multiply by that ratio to fill in later values in the table your approximate model of the order of growth is a power law with the binary logarithm of that ratio as the power why does the ratio approach a constant a simple mathematical calculation shows that to be the case for all of the common orders of growth just discussed except exponential generally the logarithmic factor cannot be ignored when developing a mathematical model but it plays a less important role in predicting performance with a doubling hypothesis you should consider running doubling ratio experiments for every program that you write where performance matters doing so is a very simple way to estimate the order of growth of the running time perhaps revealing a performance bug where a program may turn out to be not as efficient as you might think more generally we can use hypotheses about the order of growth of the running time of programs to predict performance in one of the following ways estimating the feasibility of solving large problems you need to be able to answer this basic question for every program that you write will the program be able to process this given input data in a reasonable amount of time to address such questions for a large amount of data we extrapolate by a much larger factor than for doubling say as shown in the fourth column in the table at the bottom of the next page whether it is an investment banker running daily financial models or a scientist running a program to analyze experimental data or an engineer running simulations to test a design it is not unusual for people to regularly run programs that take several hours to complete so the table focuses on that situation knowing the order of growth of the running time of an algorithm provides precisely the information that you need to understand limita tions on the size of the problems that you can solve developing such understanding is the most important reason to study performance without it you are likely have no idea how much time a program will consume with it you can make a back of the envelope calculation to estimate costs and proceed accordingly estimating the value of using a faster computer you also may be faced with this basic question periodically how much faster can i solve the problem if i get a faster computer generally if the new computer is x times faster than the old one you can improve your running time by a factor of x but it is usually the case that you can address larger prob lems with your new computer how will that change affect the running time again the order of growth is precisely the information needed to answer that question a famous rule of thumb known as moore law implies that you can expect to have a computer with about double the speed and double the memory months from now or a computer with about times the speed and times the memory in about years the table below demonstrates that you cannot keep pace with moore law if you are using a quadratic or a cubic algorithm and you can quickly determine whether that is the case by doing a doubling ratio test and checking that the ratio of running times as the input size doubles approaches not or order of growth of time for a program that takes a few hours for input of size n predicted time description function factor factor predicted time for on a faster computer linear n a day a few hours linearithmic n log n a day a few hours quadratic n a few weeks a day cubic n several months a few weeks exponential n n never never predictions on the basis of order of growth function caveats there are many reasons that you might get inconsistent or misleading re sults when trying to analyze program performance in detail all of them have to do with the idea that one or more of the basic assumptions underlying our hypotheses might be not quite correct we can develop new hypotheses based on new assumptions but the more details that we need to take into account the more care is required in the analysis large constants with leading term approximations we ignore constant coefficients in lower order terms which may not be justifed for example when we approximate the function n c n by n we are assuming that c is small if that is not the case suppose that c is or the approximation is misleading thus we have to be sensitive to the possibility of large constants nondominant inner loop the assumption that the inner loop dominates may not always be correct the cost model might miss the true inner loop or the problem size n might not be sufficiently large to make the leading term in the mathematical descrip tion of the frequency of execution of instructions in the inner loop so much larger than lower order terms that we can ignore them some programs have a significant amount of code outside the inner loop that needs to be taken into consideration in other words the cost model may need to be refined instruction time the assumption that each instruction always takes the same amount of time is not always correct for example most modern computer systems use a tech nique known as caching to organize memory in which case accessing elements in huge arrays can take much longer if they are not close together in the array you might ob serve the effect of caching for threesum by letting doublingtest run for a while after seeming to converge to the ratio of running times may jump to a larger value for large arrays because of caching system considerations typically there are many many things going on in your com puter java is one application of many competing for resources and java itself has many options and controls that significantly affect performance a garbage collector or a just in time compiler or a download from the internet might drastically affect the results of experiments such considerations can interfere with the bedrock principle of the scientific method that experiments should be reproducible since what is happening at this moment in your computer will never be reproduced again whatever else is going on in your system should in principle be negligible or possible to control too close to call often when we compare two different programs for the same task one might be faster in some situations and slower in others one or more of the consid erations just mentioned could make the difference there is a natural tendency among some programmers and some students to devote an extreme amount of energy run ning races to find the best implementation but such work is best left for experts strong dependence on inputs one of the first assumptions that we made in order to determine the order of growth of the program running time of a program was that the running time should be relatively insensitive to the inputs when that is not the case we may get inconsistent results or be unable to validate our hypotheses for example sup pose that we modify threesum to answer the question does the input have a triple that sums to by changing it to return a boolean value replacing cnt by return true and adding return false as the last statement the order of growth of the running time of this program is constant if the first three integers sum to and cubic if there are no such triples in the input multiple problem parameters we have been focusing on measuring performance as a function of a single parameter generally the value of a command line argument or the size of the input however it is not unusual to have several parameters a typical ex ample arises when an algorithm involves building a data structure and then performing a sequence of operations that use that data structure both the size of the data structure and the number of operations are parameters for such applications we have already seen an example of this in our analysis of the problem of whitelisting using binary search where we have n numbers in the whitelist and m numbers on standard input and a typical running time proportional to m log n despite all these caveats understanding the order of growth of the running time of each program is valuable knowledge for any programmer and the methods that we have described are powerful and broadly applicable knuth insight was that we can carry these methods through to the last detail in principle to make detailed accurate predictions typical computer systems are extremely complex and close analysis is best left for experts but the same methods are effective for developing approximate esti mates of the running time of any program a rocket scientist needs to have some idea of whether a test flight will land in the ocean or in a city a medical researcher needs to know whether a drug trial will kill or cure all the subjects and any scientist or engineer using a computer program needs to have some idea of whether it will run for a second or for a year coping with dependence on inputs for many problems one of the most sig nificant of the caveats just mentioned is the dependence on inputs because running times can vary widely the running time of the modification of threesum mentioned on the facing page ranges from constant to cubic depending on the input so a closer analysis is required if we want to predict performance we briefly consider here some of the approaches that are effective and that we will consider for specific algorithms later in the book input models one approach is to more carefully model the kind of input to be pro cessed in the problems that we need to solve for example we might assume that the numbers in the input to threesum are random int values this approach is challenging for two reasons the model may be unrealistic the analysis may be extremely difficult requiring mathematical skills quite be yond those of the typical student or programmer the first of these is the more significant often because the goal of a computation is to discover characteristics of the input for example if we are writing a program to process a genome how can we estimate its performance on a different genome a good model describing the genomes found in nature is precisely what scientists seek so estimating the running time of our programs on data found in nature actually amounts to con tributing to that model the second challenge leads to a focus on mathematical results only for our most important algorithms we will see several examples where a simple and tractable input model in conjunction with classical mathematical analysis helps us predict performance worst case performance guarantees some applications demand that the running time of a program be less than a certain bound no matter what the input to provide such performance guarantees theoreticians take an extremely pessimistic view of the performance of algorithms what would the running time be in the worst case for example such a conservative approach might be appropriate for the software that runs a nuclear reactor or a pacemaker or the brakes in your car we want to guarantee that such software completes its job within the bounds that we set because the result could be catastrophic if it does not scientists normally do not contemplate the worst case when studying the natural world in biology the worst case might be the extinction of the human race in physics the worst case might be the end of the universe but the worst case can be a very real concern in computer systems where the input may be generated by another potentially malicious user rather than by nature for example websites that do not use algorithms with performance guarantees are subject to denial of service attacks where hackers flood them with pathological requests that make them run much more slowly than planned accordingly many of our algorithms are designed to provide performance guarantees such as the following randomized algorithms one important way to provide a performance guarantee is to introduce randomness for example the quicksort algorithm for sorting that we study in section perhaps the most widely used sorting algorithm is quadratic in the worst case but randomly ordering the input gives a probabilistic guarantee that its running time is linearithmic every time you run the algorithm it will take a different amount of time but the chance that the time will not be linearithmic is so small as to be negligible similarly the hashing algorithms for symbol tables that we study in section again perhaps the most widely used approach are linear time in the worst case but constant time under a probabilistic guarantee these guarantees are not absolute but the chance that they are invalid is less than the chance your computer will be struck by lightning thus such guarantees are as useful in practice as worst case guarantees sequences of operations for many applications the algorithm input might be not just data but the sequence of operations performed by the client for example a pushdown stack where the client pushes n values then pops them all may have quite different performance characteristics from one where the client issues an alternating sequence n of push and pop operations our analysis has to take both situations into account or to include a reasonable model of the sequence of operations amortized analysis accordingly another way to provide a performance guarantee is to amortize the cost by keeping track of the total cost of all operations divided by the number of operations in this setting we can allow some expensive operations while keeping the average cost of operations low the prototypical example of this type of analysis is the study of the resizing array data structure for stack that we considered in section algorithm on page for simplicity suppose that n is a power of starting with an empty structure how many array entries are accessed for n consecu tive calls to push this quantity is easy to calculate the number of array accesses is n the first term accounts for the array access within each of the n calls to push the sub sequent terms account for the array accesses to initialize the data structure each time it doubles in size thus the average number of array access es per operation is constant even though the last operation takes linear time this is known as an amortized analysis because we spread the cost of the few expensive operations by assigning a number of add operations amortized cost of adding to a randombag portion of it to each of a large number of inexpensive operations visualaccumulator provides an easy way to illustrate the process shown above this kind of analysis is widely applicable in particular we use resizing arrays as the underlying data structure for several algorithms that we consider later in this book it is the task of the algorithm analyst to discover as much relevant information about an algorithm as possible and it is the task of the applications programmer to apply that knowledge to develop programs that effectively solve the problems at hand ideally we want algorithms that lead to clear and compact code that provides both a good guarantee and good performance on input values of interest many of the classic algorithms that we consider in this chapter are important for a broad variety of ap plications precisely because they have these properties using them as models you can develop good solutions yourself for typical problems that you face while programming memory as with running time a program memory usage connects directly to the physical world a substantial amount of your computer circuitry enables your pro gram to store values and later retrieve them the more values you need to have stored at any given instant the more circuitry you need you probably are aware of limits on memory usage on your computer even more so than for time because you probably have paid extra money to get more memory memory usage is well defined for java on your computer every value requires pre cisely the same amount of memory each time that you run your program but java is implemented on a very wide range of computational devices and memory consump tion is implementation dependent for economy we use the word typical to signal that values are subject to machine dependencies one of java most significant features is its memory allocation system type bytes boolean byte char int float long double typical memory requirements for primitive types which is supposed to relieve you from having to worry about memory certainly you are well advised to take advantage of this feature when ap propriate still it is your responsibility to know at least approximately when a program memory requirements will prevent you from solving a given problem analyzing memory usage is much easier than analyzing running time primarily because not as many program statements are involved just dec larations and because the analysis reduces complex objects to the primi tive types whose memory usage is well defined and simple to understand we can count up the number of variables and weight them by the number of bytes according to their type for example since the java int data type is the set of integer values between 648 and 647 a grand total of different values typical java implementations use bits to represent int values similar considerations hold for other primitive types typical java implementations use bit bytes representing each char value with bytes bits each int value with bytes bits each double and each long value with bytes bits and each boolean value with byte since computers typically access memory one byte at a time combined with knowledge of the amount of memory available you can calculate limitations from these values for example if you have of memory on your computer billion bytes you cannot fit more than about mil lion int values or million double values in memory at any one time on the other hand analyzing memory usage is subject to various differences in ma chine hardware and in java implementations so you should consider the specific ex amples that we give as indicative of how you might go about determining memory usage when warranted not the final word for your computer for example many data structures involve representation of machine addresses and the amount of memory needed for a machine address varies from machine to machine for consistency we assume that bytes are needed to represent addresses as is typical for bit architectures that are now widely used recognizing that many older machines use a bit architecture that would involve just bytes per machine address objects to determine the memory usage of an object we add the amount of memory used by each instance variable to the overhead associated with each object typically bytes the overhead includes a reference to the object class garbage collection information and synchronization information moreover the memory usage is typically padded to be a multiple of bytes machine words on a bit machine for example an integer object uses bytes bytes of overhead bytes for its int instance variable and bytes of padding similarly a date page object also uses bytes bytes of overhead bytes for each of its three int instance variables and bytes of padding a ref erence to an object typically is a memory address and thus uses bytes of memory for example a counter page object uses bytes bytes of overhead bytes for its string instance variable a reference bytes for its int instance variable and bytes of pad ding when we account for the memory for a reference we account separately for the memory for the object itself so this total does not count the memory for the string value integer wrapper object public class integer private int x date object public class date private int day private int month private int year counter object public class counter private string name private int count node object inner class public class node private item item private node next bytes bytes object overhead day month year padding bytes object overhead name count padding bytes object overhead extra overhead item next int value int values string reference int value references linked lists a nested non static inner class such as our node class page requires an extra bytes of typical object memory requirements overhead for a reference to the enclosing instance thus a node object uses bytes bytes of object overhead bytes each for the references to the item and node ob jects and bytes for the extra overhead thus since an integer object uses bytes a stack with n integers built with a linked list representation algorithm uses bytes the usual for object overhead for stack for its reference instance vari able for its int instance variable for padding and for each entry for a node and for an integer arrays typical memory requirements for various types of arrays in java are summa rized in the diagrams on the facing page arrays in java are implemented as objects typically with extra overhead for the length an array of primitive type values typically requires bytes of header information bytes of object overhead bytes for the length and bytes of padding plus the memory needed to store the values for ex ample an array of n int values uses bytes rounded up to be a multiple of and an array of n double values uses bytes an array of objects is an array of references to the objects so we need to add the space for the references to the space required for the objects for example an array of n date objects page uses bytes array overhead plus bytes references plus bytes for each object and bytes of padding for a grand total of bytes a two dimensional array is an array of ar rays each array is an object for example a two dimensional m by n array of double values uses bytes overhead for the array of arrays plus m bytes references to the row arrays plus m times bytes overhead from the row arrays plus m times n times bytes for the n double values in each of the m rows for a grand total of bytes when array entries are objects a similar accounting leads to a total of bytes for the array of arrays filled with references to objects plus the memory for the objects themselves string objects we account for memory in java string objects in the same way as for any other object except that aliasing is common for strings the standard string implementation has four instance variables a reference to a character array bytes and three int values bytes each the first int value is an offset into the character ar ray the second is a count the string length in terms of the instance variable names in the drawing on the facing page the string that is represented consists of the characters value offset through value offset count the third int value in string objects is a hash code that saves recomputation in certain circumstances that need not concern us now therefore each string object uses a total of bytes bytes for object overhead plus bytes for each of the three int instance variables plus bytes for the array reference plus bytes of padding this space requirement is in addition to the space needed for the characters themselves which are in the array the space needed for the characters is accounted for separately because the char array is often shared among strings since string objects are immutable this arrangement allows the imple mentation to save memory when string objects have the same underlying value string values and substrings a string of length n typically uses bytes for the string object plus bytes for the array that contains the characters for a total of bytes but it is typical in string processing to work with substrings and java representation is meant to allow us to do so without having to make copies of array of int values array of double values int a new int n double c new double n a c bytes bytes int value bytes total n even n int values bytes int value bytes total n double values bytes bytes bytes bytes array of objects double date double 8nm bytes object overhead day month year padding array of arrays two dimensional array double t t new double m n t bytes int value bytes m references bytes total mx 32m n double values bytes typical memory requirements for arrays of int values double values objects and arrays string object java library public class string bytes the string characters when you use the substring method you create a new string object bytes private char value private int offset private int count private int hash substring example string genome cgcctggcgtctgtac reference int values but reuse the same value array so a substring of an existing string takes just bytes the character array containing the original string is aliased in the object for the substring the offset and length fields identify the substring in other words a substring takes constant ex tra memory and forming a substring takes constant time even when the lengths of the string and the substring are huge a naive representation that requires copying string codon genome substring genome r es a string and a substring characters to make substrings would take linear time and space the ability to create a substring using space and time independent of its length is the key to effi ciency in many basic string processing algorithms these basic mechanisms are effective for esti mating the memory usage of a great many programs but there are numerous complicating factors that can make the task significantly more difficult we have already noted the potential effect of aliasing more over memory consumption is a complicated dynamic process when function calls are involved because the system memory allocation mechanism plays a more important role with more system dependencies for example when your program calls a method the sys tem allocates the memory needed for the method for its local variables from a special area of memory called the stack a system pushdown stack and when the method returns to the caller the memory is returned to the stack for this reason creating arrays or other large objects in recursive programs is dangerous since each recursive call implies significant memory usage when you create an object with new the system allocates the memory needed for the object from another special area of memory known as the heap not the same as the binary heap data structure we consider in section and you must remember that every object lives until no references to it remain at which point a system process known as garbage collection reclaims its memory for the heap such dynamics can make the task of pre cisely estimating memory usage of a program challenging perspective good performance is important an impossibly slow program is al most as useless as an incorrect one so it is certainly worthwhile to pay attention to the cost at the outset to have some idea of which kinds of problems you might feasibly address in particular it is always wise to have some idea of which code constitutes the inner loop of your programs perhaps the most common mistake made in programming is to pay too much at tention to performance characteristics your first priority is to make your code clear and correct modifying a program for the sole purpose of speeding it up is best left for experts indeed doing so is often counterproductive as it tends to create code that is complicated and difficult to understand c a r hoare the inventor of quicksort and a leading proponent of writing clear and correct code once summarized this idea by saying that premature optimization is the root of all evil to which knuth added the qualifier or at least most of it in programming beyond that improving the running time is not worthwhile if the available cost benefits are insignificant for example im proving the running time of a program by a factor of is inconsequential if the run ning time is only an instant even when a program takes a few minutes to run the total time required to implement and debug an improved algorithm might be substantially more than the time required simply to run a slightly slower one you may as well let the computer do the work worse you might spend a considerable amount of time and effort implementing ideas that should in theory improve a program but do not do so in practice perhaps the second most common mistake made in programming is to ignore per formance characteristics faster algorithms are often more complicated than brute force ones so you might be tempted to accept a slower algorithm to avoid having to deal with more complicated code however you can sometimes reap huge savings with just a few lines of good code users of a surprising number of computer systems lose substantial time unknowingly waiting for brute force quadratic algorithms to finish solving a problem when linear or linearithmic algorithms are available that could solve the problem in a fraction of the time when we are dealing with huge problem sizes we often have no choice but to seek better algorithms we generally take as implicit the methodology described in this section to estimate memory usage and to develop an order of growth hypothesis of the running time from a tilde approximation resulting from a mathematical analysis within a cost model and to check those hypotheses with experiments improving a program to make it more clear efficient and elegant should be your goal every time that you work on it if you pay attention to the cost all the way through the development of a program you will reap the benefits every time you use it q why not use stdrandom to generate random values instead of maintaining the file txt a it is easier to debug code in development and to reproduce experiments stdrandom produces different values each time it is called so running a program after fixing a bug may not test the fix you could use the initialize method in stdrandom to address this problem but a reference file such as txt makes it easier to add test cases while debugging also different programmers can compare performance on different computers without worrying about the input model once you have debugged a pro gram and have a good idea of how it performs it is certainly worthwhile to test it on random data for example doublingtest and doublingratio take this approach q i ran doublingratio on my computer but the results were not as consistent as in the book some of the ratios were not close to why a thatiswhywediscussed caveats onpage mostlikely yourcomputer soperating system decided to do something else during the experiment one way to mitigate such problems is to invest more time in more experiments for example you could change doublingtest to run the experiments 000 times for each n giving a much more ac curate estimate for the running time for each size see exercise q what exactly does as n grows mean in the definition of the tilde notation a the formal definition of f n g n is limn f n g n q i ve seen other notations for describing order of growth what the story a the big oh notation is widely used we say that f n is o g n if there exist constants c and such that f n c g n for all n this notation is very use ful in providing asymptotic upper bounds on the performance of algorithms which is important in the theory of algorithms but it is not useful for predicting performance or for comparing algorithms q why not a the primary reason is that it describes only an upper bound on the running time actual performance might be much better the running time of an algorithm might be both o n and a n log n as a result it cannot be used to justify tests like our doubling ratio test see proposition c on page q so why is the big oh notation so widely used a it facilitates development of bounds on the order of growth even for complicated algorithms for which more precise analysis might not be feasible moreover it is com patible with the big omega and big theta notations that theoretical computer sci entists use to classify algorithms by bounding their worst case performance we say that f n is d g n if there exist constants c and such that f n c g n for n and if f n is o g n and d g n we say that f n is g n the big omega notation is typically used to describe a lower bound on the worst case and the big theta notation is typically used to describe the performance of algorithms that are optimal in the sense that no algorithm can have better asymptotic worst case order of growth optimal algorithms are certainly worth considering in practical applica tions but there are many other considerations as you will see q aren t upper bounds on asymptotic performance important a yes but we prefer to discuss precise results in terms of frequency of statement ex ceution with respect to cost models because they provide more information about algorithm performance and because deriving such results is feasible for the algorithms that we discuss for example we say threesum uses n array accesses and the number of times cnt is executed in threesum is n in the worst case which is a bit more verbose but much more informative than the statement the running time of threesum is o n q when the order of growth of the running time of an algorithm is n log n the dou bling test will lead to the hypothesis that the running time is a n for a constant a isn t that a problem a we have to be careful not to try to infer that the experimental data implies a par ticular mathematical model but when we are just predicting performance this is not really a problem for example when n is between 000 and 000 the plots of and n lg n are very close to one another the data fits both curves as n increases the curves become closer together it actually requires some care to experimentally check the hypothesis that an algorithm running time is linearithmic but not linear q does int a new int n count as n array accesses to initialize entries to a most likely yes so we make that assumption in this book though a sophisticated compiler implementation might try to avoid this cost for huge sparse arrays show that the number of different triples that can be chosen from n items is precisely n n n hint use mathematical induction modify threesum to work properly even when the int values are so large that adding two of them might cause overflow modify doublingtest to use stddraw to produce plots like the standard and log log plots in the text rescaling as necessary so that the plot always fills a substantial portion of the window develop a table like the one on page for twosum give tilde approximations for the following quantities a n b n c n n d n n e lg lg n f lg n lg n g n give the order of growth as a function of n of the running times of each of the following code fragments a int sum for int n n n n for int i i n i sum b int sum for int i i n i for int j j i j sum c int sum for int i i n i for int j j n j sum analyze threesum under a cost model that counts arithmetic operations and comparisons involving the input numbers write a program to determine the number pairs of values in an input file that are equal if your first try is quadratic think again and use arrays sort to develop a linearithmic solution give a formula to predict the running time of a program for a problem of size n when doubling experiments have shown that the doubling factor is and the running time for problems of size is t modify binary search so that it always returns the element with the smallest index that matches the search element and still guarantees logarithmic running time add aninstancemethod howmany to staticsetofints page thatfindsthe number of occurrences of a given key in time proportional to log n in the worst case write a program that given two sorted arrays of n int values prints all ele ments that appear in both arrays in sorted order the running time of your program should be proportional to n in the worst case using the assumptions developed in the text give the amount of memory need ed to represent an object of each of the following types a accumulator b transaction c fixedcapacitystackofstrings with capacity c and n entries d e f g double sum develop an algorithm for the sum problem faster sum as a warmup develop an implementation twosumfaster that uses a linear algorithm to count the pairs that sum to zero after the array is sorted in stead of the binary search based linearithmic algorithm then apply a similar idea to develop a quadratic algorithm for the sum problem closest pair in one dimension write a program that given an array a of n double values finds a closest pair two values whose difference is no greater than the the difference of any other pair in absolute value the running time of your program should be linearithmic in the worst case farthest pair in one dimension write a program that given an array a of n double values finds a farthest pair two values whose difference is no smaller than the the difference of any other pair in absolute value the running time of your program should be linear in the worst case local minimum of an array write a program that given an array a of n dis tinct integers finds a local minimum an index i such that a i a i a i your program should use n compares in the worst case answer examine the middle value a n and its two neighbors a n and a n if a n is a local minimum stop otherwise search in the half with the smaller neighbor local minimum of a matrix given an n by n array a of n distinct inte gers design an algorithm that runs in time proportional to n to find a local minimum a pair of indices i and j such that a i j a i j a i j a i j a i j a i j and a i j a i j the running time of your pro gram should be proportional to n in the worst case bitonic search an array is bitonic if it is comprised of an increasing sequence of integers followed immediately by a decreasing sequence of integers write a program that given a bitonic array of n distinct int values determines whether a given integer is in the array your program should use n compares in the worst case binary search on distinct values develop an implementation of binary search for staticsetofints seepage wheretherunningtimeof contains isguaranteed to be lg r where r is the number of different integers in the array given as argument to the constructor binary search with only addition and subtraction mihai patrascu write a program that given an array of n distinct int values in ascending order determines whether a given integer is in the array you may use only additions and subtractions and a constant amount of extra memory the running time of your program should be proportional to log n in the worst case answer instead of searching based on powers of two binary search use fibonacci numbers which also grow exponentially maintain the current search range to be the interval i i fk and keep fk and fk in two variables at each step compute fk via subtraction check element i fk and update the current range to either i i fk or i fk i fk fk binary search for a fraction devise a method that uses a logarithmic number of queries of the form is the number less than x to find a rational number p q such that p q n hint two fractions with denominators less than n cannot differ by more than n throwing eggs from a building suppose that you have an n story building and plenty of eggs suppose also that an egg is broken if it is thrown off floor f or higher and unhurt otherwise first devise a strategy to determine the value of f such that the number of broken eggs is lg n when using lg n throws then find a way to reduce the cost to f throwing two eggs from a building consider the previous question but now suppose you only have two eggs and your cost model is the number of throws devise a strategy to determine f such that the number of throws is at most n then find a way to reduce the cost to c f this is analogous to a situation where search hits egg intact are much cheaper than misses egg broken collinearity suppose that you have an algorithm that takes as input n dis tinct points in the plane and can return the number of triples that fall on the same line show that you can use this algorithm to solve the sum problem strong hint use algebra to show that a b and c are collinear if and only if a b c queue with two stacks implement a queue with two stacks so that each queue creative problems continued operation takes a constant amortized number of stack operations hint if you push elements onto a stack and then pop them all they appear in reverse order if you repeat this process they re now back in order stack with a queue implement a stack with a single queue so that each stack operations takes a linear number of queue operations hint to delete an item get all of the elements on the queue one at a time and put them at the end except for the last one which you should delete and return this solution is admittedly very inefficient steque with two stacks implement a steque with two stacks so that each steque operation see exercise takes a constant amortized number of stack operations deque with a stack and a steque implement a deque with a stack and a steque see exercise so that each deque operation takes a constant amortized number of stack and steque operations deque with three stacks implement a deque with three stacks so that each deque operation takes a constant amortized number of stack operations amortized analysis prove that starting from an empty stack the number of ar ray accesses used by any sequence of m operations in the resizing array implementation of stack is proportional to m memory requirements on a bit machine give the memory requirements for integer date counter int double double string node and stack linked list representation for a bit machine assume that references are bytes object overhead is bytes and padding is to a multiple of bytes hot or cold your goal is to guess a secret integer between and n you repeat edly guess integers between and n after each guess you learn if your guess equals the secret integer and the game stops otherwise you learn if the guess is hotter closer to or colder farther from the secret number than your previous guess design an algo rithm that finds the secret number in at most lg n guesses then design an algorithm that finds the secret number in at most lg n guesses time costs for pushdown stacks justify the entries in the table below which shows typical time costs for various pushdown stack implementations using a cost model that counts both data references references to data pushed onto the stack either an array reference or a reference to an object instance variable and objects created data structure item type cost to push n int values linked list time costs for pushdown stacks various implementations space usage for pushdown stacks justify the entries in the table below which shows typical space usage for various pushdown stack implementations use a static nested class for linked list nodes to avoid the non static nested class overhead data structure item type space usage for n int values bytes int n linked list resizing array integer n int between n and n integer between n and n space usage in pushdown stacks various implementations autoboxing performance penalty run experiments to determine the perfor mance penalty on your machine for using autoboxing and auto unboxing develop an implementation fixedcapacitystackofints and use a client such as doublingratio to compare its performance with the generic fixedcapacitystack integer for a large number of push and pop operations naive sum implementation run experiments to evaluate the following im plementation of the inner loop of threesum for int i i n i for int j j n j for int k k n k if i j j k if a i a j a k cnt do so by developing a version of doublingtest that computes the ratio of the running times of this program and threesum improved accuracy for doubling test modify doublingratio to take a second command line argument that specifies the number of calls to make to timetrial for each value of n run your program for and 000 trials and comment on the precision of the results sum for random values formulate and validate a hypothesis describing the number of triples of n random int values that sum to if you are skilled in math ematical analysis develop an appropriate mathematical model for this problem where the values are uniformly distributed between m and m where m is not small running times estimate the amount of time it would take to run twosumfast twosum threesumfast and threesum on your computer to solve the problems for a file of million numbers use doublingratio to do so problem sizes estimate the size of the largest value of p for which you can run twosumfast twosum threesumfast and threesum on your computer to solve the problems for a file of thousand numbers use doublingratio to do so 43 resizing arrays versus linked lists run experiments to validate the hypothesis that resizing arrays are faster than linked lists for stacks see exercise and exer cise do so by developing a version of doublingratio that computes the ratio of the running times of the two programs birthday problem write a program that takes an integer n from the command line and uses stdrandom uniform to generate a random sequence of integers be tween and n run experiments to validate the hypothesis that the number of integers generated before the first repeated value is found is coupon collector problem generating random integers as in the previous exer cise run experiments to validate the hypothesis that the number of integers generated before all possible values are generated is n hn to illustrate our basic approach to developing and analyzing algorithms we now consider a detailed example our purpose is to emphasize the following themes good algorithms can make the difference between being able to solve a practical problem and not being able to address it at all an efficient algorithm can be as simple to code as an inefficient one understanding the performance characteristics of an implementation can be an interesting and satisfying intellectual challenge the scientific method is an important tool in helping us choose among different methods for solving the same problem an iterative refinement process can lead to increasingly efficient algorithms these themes are reinforced throughout the book this prototypical example sets the stage for our use of the same general methodology for many other problems the problem that we consider is not a toy problem it is a fundamental compu tational task and the solution that we develop is of use in a variety of applications from percolation in physical chemistry to connectivity in communications networks we start with a simple solution then seek to understand that solution performance characteristics which help us to see how to improve the algorithm dynamic connectivity we start with the following problem specification the input is a sequence of pairs of integers where each integer represents an object of some type and we are to interpret the pair p q as meaning p is connected to q we assume that is connected to is an equivalence relation which means that it is reflexive p is connected to p symmetric if p is connected to q then q is connected to p transitive if p is connected to q and q is connected to r then p is connected to r an equivalence relation partitions the objects into equivalence classes in this case two objects are in the same equivalence class if and only if they are connected our goal is to write a program to filter out extraneous pairs pairs where both objects are in the same equivalence class from the sequence in other words when the program reads a pair p q from the input it should write the pair to the output only if the pairs it has seen to that point do not imply that p is connected to q if the previous pairs do imply that p is connected to q then the program should ignore the pair p q and proceed to read in the next pair the figure on the facing page gives an example of this process to achieve the desired goal we need to devise a data structure that can remember sufficient information about the pairs it has seen to be able to decide whether or not a new pair of objects is connected informally we refer to the task of designing such a method as the dynamic connectivity problem this problem arises applications such as the following networks the integers might represent computers in a large network and the pairs might represent connections in the network then our program determines whether we need to establish a new direct connection for p and q to be able to communicate or whether we can use existing connections to set up a communications path or the integers might represent contact sites in an electrical circuit and the pairs might represent wires connecting the sites or the integers might represent people in a social network and the pairs might represent friendships in such applications we might need to process millions of objects and billions of connections variable name equivalence in certain programming environ ments it is possible to declare two variable names as being equiv alent references to the same object after a sequence of such dec larations the system needs to be able to determine whether two given names are equivalent this application is an early one for the fortran programming language that motivated the devel opment of the algorithms that we are about to consider mathematical sets on a more abstract level you can think of the integers as belonging to mathematical sets when we process a pair p q we are asking whether they belong to the same set if not we unite p set and q set putting them in the same set to fix ideas we will use networking terminology for the rest of this section and refer to the objects as sites the pairs as connec tions and the equivalence classes as connected components or just components for short for simplicity we assume that we have n components don t print pairs that are already connected sites with integer names from to n we do so without loss of generality because we shall be considering a host of algorithms in chapter that can associate arbitrary names with such integer identifiers in an efficient manner dynamic connectivity example a larger example that gives some indication of the difficulty of the connectivity problem is depicted in the figure at the top of the next page you can quickly identify the component consisting of a single site in the left middle of the diagram and the connected component medium connectivity example sites edges connected components component consisting of five sites at the bottom left but you might have difficulty veri fying that all of the other sites are connected to one another for a program the task is even more difficult because it has to work just with site names and connections and has no access to the geometric placement of sites in the diagram how can we tell quickly whether or not any given two sites in such a network are connected the first task that we face in developing an algorithm is to specify the problem in a precise manner the more we require of an algorithm the more time and space we may expect it to need to finish the job it is impossible to quantify this relationship a priori and we often modify a problem specification on finding that it is difficult or expensive to solve or in happy circumstances on finding that an algorithm can provide informa tion more useful than what was called for in the original specification for example our connectivity problem specification requires only that our program be able to determine whether or not any given pair p q is connected and not that it be able to demonstrate a set of connections that connect that pair such a requirement makes the problem more difficult and leads us to a different family of algorithms which we consider in section to specify the problem we develop an api that encapsulates the basic operations that we need initialize add a connection between two sites identify the component containing a site determine whether two sites are in the same component and count the number of components thus we articulate the following api the union operation merges two components if the two sites are in different com ponents the find operation returns an integer component identifier for a given site the connected operation determines whether two sites are in the same component and the count method returns the number of components we start with n compo nents and each union that merges two different components decrements the num ber of components by as we shall soon see the development of an algorithmic solution for dynamic con nectivity thus reduces to the task of developing an implementation of this api every implementation has to define a data structure to represent the known connections develop efficient union find connected and count implementa tions that are based on that data structure as usual the nature of the data structure has a direct impact on the efficiency of the algorithms so data structure and algorithm design go hand in hand the api already specifies the convention that both sites and components will be identified by int val ues between and n so it makes sense to use a site indexed array id as our basic data structure to represent the components we always use the name of one of the sites in a component as the component identifier so you can think of each component as being represented by one of its sites initially we start with n components each site in its own component so we initialize id i to i for all i from to n for each site i we keep the information needed by find to determine the component contain ing i in id i using various algorithm dependent strategies all of our implementa tions use a one line implementation of connected that returns the boolean value find p find q in summary our starting point is algorithm on the facing page we maintain two instance variables the count of components and the array id implementations of find and union are the topic of the remainder of this section to test the utility of the api and to provide a basis for develop ment we include a client in main that uses it to solve the dy namic connectivity problem it reads the value of n followed by a sequence of pairs of integers each in the range to n calling find for each pair if the two sites in the pair are already con nected it moves on to the next pair if they are not it calls union and prints the pair before considering implementations we also prepare test data the file tinyuf txt contains the connections among sites used in the small example illustrated on page the file mediumuf txt contains the connections among sites illustrated on page and the file largeuf txt is an example with million connections among millions sites our goal is to be able to handle inputs such as largeuf txt in a reasonable amount of time to analyze the algorithms we focus on the number of times each algorithm accesses an array entry by doing so we are implicitly for mulating the hypothesis that the running times of the algorithms on a particular machine are within a constant factor of this quantity this hypothesis is immediate from the code is not difficult to validate through ex perimentation and provides a useful starting point for comparing algorithms as we will see algorithm union find implementation public class uf private int id access to component id site indexed private int count number of components public uf int n initialize component id array count n id new int n for int i i n i id i i public int count return count public boolean connected int p int q return find p find q public int find int p public void union int p int q see page quick find page quick union andpage weighted public static void main string args solve dynamic connectivity problem on stdin int n stdin readint read number of sites uf uf new uf n initialize n components while stdin isempty int p stdin readint int q stdin readint read pair to connect if uf connected p q continue ignore if connected uf union p q combine components stdout println p q and print connection stdout println uf count components our uf implementations are based on this code which maintains an array of integers id such that the find method returns the same integer for every site in each connected component the union method must maintain this invariant implementations we shall consider three different implementations all based on using the site indexed id array to determine whether two sites are in the same con nected component quick find one approach is to maintain the invariant that p and q are connected if and only if id p is equal to id q in other words all sites in a component must have the same value in id this method is called quick find because find p just returns id p which immediately implies that connected p q reduces to just the test id p id q and returns true if and only if p and q are in the same component to maintain the invariant for the call union p q we first check whether they are already in the same component in which case there is nothing to do otherwise we are faced with the situation that all of the id entries corresponding to sites in the same component as p have one value and all of the id entries corre sponding to sites in the same component as q have another value to combine the two components into one we have to make all of the id entries cor responding to both sets of sites the same value as find examines id and id p q union has to change all to p q quick find overview shown in the example at right to do so we go through the array changing all the entries with values equal to id p to the value id q we could have decided to change all the entries equal to id q to the value id p the choice between these two alternatives is arbitrary the code for find and union based on these descriptions given at left is straightforward a full trace for our development client with our sample test data tinyuf txt is shown on the next page quick find analysis the find operation is certainly quick as it only accesses the id array once in order to complete the operation but quick find is typically not use ful for large problems because union needs to scan through the whole id array for each input pair id p q id p and id q differ so union changes entries equal to id p to id q in red id p and id q match so no change quick find trace in particular suppose that we use quick find for the dynamic connectivity problem and wind up with a single component this requires at least n calls to union and consequently at least n n n array accesses we are led immediately to the hy pothesis that dynamic connectivity with quick find can be a quadratic time process this analysis gener alizes to say that quick find is quadratic for typical applications where we end up with a small number of components you can easily validate this hypothesis on your computer with a doubling test see exercise for an instructive example modern comput ers can execute hundreds of millions or billions of in structions per second so this cost is not noticeable if n is small but we also might find ourselves with mil lions or billions of sites and connections to process in a modern application as represented by our test file largeuf txt if you are still not convinced and feel that you have a particularly fast computer try using quick find to determine the number of components implied by the pairs in largeuf txt the inescap able conclusion is that we cannot feasibly solve such a problem using the quick find algorithm so we seek better algorithms quick union the next algorithm that we consider is a complementary method that concentrates on speeding up the union operation it is based on the same data structure the site indexed id ar ray but we interpret the values dif ferently to define more complicated structures specifically the id entry for each site is the name of another site in the same component possibly itself we refer to this connection as a link to implement find we start at the given site follow its link to an other site follow that site link to yet another site and so forth following links until reaching a root a site that has a link to itself which is guaran teed to happen as you will see two sites are in the same component if and only if this process leads them to the same root to validate this process we need union p q to maintain this invariant which is easily arranged we follow links to find the roots associated with p and q then rename one of the components by linking one of these roots to the other hence the name quick union again we have an arbitrary choice of whether to rename the com ponent containing p or the component containing q the implementation above re names the one containing p the id is parent link representation of a forest of trees root find has to follow links to the root p q figure on the next page shows a trace of the quick union algo rithm for tinyuf txt this trace is best understood in terms of the becomes parent of find is id id id union changes just one link find is id id graphical representation depict ed at left which we consider next p q quick union overview forest of trees representation the code for quick union is compact but a bit opaque representing sites as nodes labeled circles and links as arrows from one node to an other gives a graphical representation of the data structure that makes it relatively easy to understand the operation of the algorithm the resulting structures are trees in technical terms our id array is a parent link representation of a forest set of trees to sim plify the diagrams we often omit both the arrowheads in the links because they all point upwards and the self links in the roots of the trees the forests corre sponding to the id array for tinyuf txt are shown at right when we start at the node cor responding to any site and follow links we eventually end up at the root of the tree containing that node we can prove this prop erty to be true by induction it is true after the array is initialized to have every node link to itself and if it is true before a given union operation it is certainly true afterward thus the find method on page returns the name of the site at the root so that connected checks wheth er two sites are in the same tree this representation is useful for this problem because the nodes corresponding to two sites are in the same tree if and only if the sites are in the same component id p q 8 8 8 8 8 8 8 8 8 8 8 8 8 8 quick union trace with corresponding forests of trees moreover the trees are not difficult to build the union implementation on page combines two trees into one in a single statement by making the root of one the parent of the other quick union analysis the quick union algorithm would seem to be faster than the quick find algorithm because it does not have to go through the entire array for each input pair but how much faster is it analyzing the id cost of quick union is more difficult than it was for quick find because the cost is more dependent on the nature of the input in the best case find just needs one array access to find the identifier associ ated with a site as in quick find in the worst case it needs array accesses as for in the example at left this count is conservative since compiled code will typically not do an array access for the second reference to id p in the while loop ac cordingly it is not difficult to construct a best case input for which the running time of our dynamic connectivity client is linear on the other hand it is also not difficult to construct a worst case input for which the running time is quadratic see the dia depth quick union worst case gram at left and proposition g below fortunate ly we do not need to face the problem of analyzing quick union and we will not dwell on comparative performance of quick find and quick union be cause we will next examine another variant that is far more efficient than either for the moment you can regard quick union as an improvement over quick find because it removes quick find main liability that union always takes linear time this differ ence certainly represents an improvement for typical data but quick union still has the liability that we cannot guarantee it to be substantially faster than quick find in every case for certain input data quick union is no faster than quick find again suppose that we use quick union for the dynamic connectivity problem and wind up with a single component an immediate implication of proposition g is that the running time is quadratic in the worst case suppose that the input pairs come in the order then then and so forth after n such pairs we have n sites all in the same set and the tree that is formed by the quick union algorithm has height n with linking to which links to which links to and so forth see the diagram on the facing page by proposition g the number of array accesses for the union operation for the pair i is exactly site is at depth i and site i at depth thus the total number of array accesses for the find operations for these n pairs is n n weighted quick union fortunately there is an easy modification to quick union that allows us to guarantee that bad cases such as this one do not occur rather than arbitrarily connecting the quick union p q smaller tree might put the smaller tree larger tree second tree to the first for union we keep track of the size of each tree and always connect the smaller tree to the larger this change requires slightly more code and another array to hold the node counts as shown on page but it leads to substantial improvements in efficiency we refer to this algorithm as the weighted quick union al weighted larger tree p larger tree larger tree lower always chooses the better alternative q smaller tree smaller tree larger tree gorithm the forest of trees constructed by this algorithm for tinyuf txt is shown in the figure weighted quick union at left on the top of page even for this small example the tree height is substantially smaller than the height for the unweighted version weighted quick union analysis the figure at right on the top of page illustrates the worst case for weighted quick union when the sizes of the trees to be merged by union are always equal and a power of these tree structures look complex but they have the simple property that the height of a tree of nodes is n fur thermore when we merge two trees of nodes we get a tree of nodes and we increase the height of the tree to n this observation generalizes to provide a proof that the weighted algorithm can guarantee logarithmic performance algorithm continued union find implementation weighted quick union public class weightedquickunionuf private int id parent link site indexed private int sz size of component for roots site indexed private int count number of components public weightedquickunionuf int n count n id new int n for int i i n i id i i sz new int n for int i i n i sz i public int count return count public boolean connected int p int q return find p find q private int find int p follow links to find a root while p id p p id p return p public void union int p int q int i find p int j find q if i j return make smaller root point to larger one if sz i sz j id i j sz j sz i else id j i sz i sz j count this code is best understood in terms of the forest of trees representation described in the text we add a site indexed array sz as an instance variable so that union can link the root of the smaller tree to the root of the larger tree this addition makes it feasible to address large problems reference input worst case input p q p q 8 8 weighted quick union traces forests of trees quick union weighted quick union and weighted quick union sites union operations for dynamic connectivity the practical implication of proposition h and its corollary is that weighted quick union is the only one of the three algorithms that can feasibly be used for huge practical problems the weighted quick union algorithm uses at most c m lg n array accesses to process m connections among n sites for a small constant c this result is in stark contrast to our finding that quick find always and quick union sometimes uses at least mn array accesses thus with weighted quick union we can guarantee that we can solve huge practical dynamic connectivity problems in a reason able amount of time for the price of a few extra lines of code we get a program that can be millions of times faster than the simpler algorithms for the huge dynamic con nectivity problems that we might encounter in practical applications a site example is shown on the top of this page it is evident from this diagram that relatively few nodes fall far from the root with weighted quick union indeed it is frequently the case that a node tree is merged with a larger tree which puts the node just one link from the root empirical studies on huge problems tell us that weighted quick union typically solves practical problems in constant time per operation we could hardly expect to find a more efficient algorithm algorithm order of growth for n sites worst case constructor union find quick find n n quick union n tree height tree height weighted quick union n lg n lg n weighted quick union with path compresson very very nearly but not quite amortized see exercise impossible n performance characteristics of union find algorithms optimal algorithms can we find an algorithm that has guaranteed constant time per operation performance this question is an extremely difficult one that plagued researchers for many years in pursuit of an answer a number of variations of quick union and weighted quick union have been studied for example the following meth od known as path compression is easy to implement ideally we would like every node to link directly to the root of its tree but we do not want to pay the price of changing a large number of links as we did in the quick find algorithm we can approach the ideal simply by making all the nodes that we do examine directly link to the root this step seems drastic at first blush but it is easy to implement and there is nothing sacrosanct about the structure of these trees if we can modify them to make the algorithm more efficient we should do so to implement path compression we just add another loop to find that sets the id entry corresponding to each node encountered along the way to link directly to the root the net result is to flatten the trees almost completely ap proximating the ideal achieved by the quick find algorithm the method is simple and effective but you are not likely to be able to discern any improvement over weighted quick union in a practical situation see exercise theoretical results about the situation are extremely complicated and quite remarkable weighted quick union with path compression is optimal but not quite constant time per operation that is not only is weighted quick find with path compression not constant time per operation in the worst case amortized but also there exists no algorithm that can guarantee to perform each union find operation in amortized constant time under the very general cell probe model of computation weighted quick union with path compression is very close to the best that we can do for this problem quick find amortized cost plots as with any data type implementation it is worthwhile to run experiments to test the validity of our performance hypotheses for typical clients as dis cussion in section the figure at left shows details of the performance of the algorithms for our dynamic connectivity development client when solving our site connectivity example mediumuf txt such diagrams are easy to pro union operations use at least references one gray dot for each connection processed by client duce see exercise 5 for the i th connec tion processed we maintain a variable cost that counts the number of array accesses to id or sz and a variable total that is the sum of the total number of array accesses so far then we plot a gray dot at i cost and a red dot at i total i the red dots are the average cost per operation or amortized cost these red dots give cumulative average connected operations use exactly array accesses 458 plots provide good insights into algorithm be havior for quick find every union opera tion uses at least accesses plus for each component merged up to another and every connected operation uses accesses initially most of the connections lead to a call on union so the cumulative average hovers around later most connections are calls to quick union number of connections find operations become expensive connected that cause the call to union to be skipped so the cumulative average decreas es but still remains relatively high inputs that lead to a large number of connected calls that cause union to be skipped will exhibit signifi cantly better performance see exercise 5 for an example for quick union all operations weighted quick union no expensive operations initially require only a few array accesses eventu 8 ally the height of the trees becomes a significant factor and the amortized cost grows noticably cost of all operations sites for weighted quick union the tree height stays small none of the operations are expensive and the amortized cost is low these experiments validate our conclusion that weighted quick union is certainly worth implementing and that there is not much further room for improvement for practical problems perspective each of the uf implementations that we considered is an improvement over the previous in some intuitive sense but the process is artificially smooth because we have the benefit of hindsight in looking over the development of the algorithms as they were studied by researchers over the years the implementations are simple and the problem is well specified so we can evaluate the various algorithms directly by run ning empirical studies furthermore we can use these studies to validate mathematical results that quantify the performance of these algorithms when possible we follow the same basic steps for fundamental problems throughout the book that we have taken for union find algorithms in this section some of which are highlighted in this list decide on a complete and specific problem statement including identifying fundamental abstract operations that are intrinsic to the problem and an api carefully develop a succinct implementation for a straightforward algorithm using a well thought out development client and realistic input data know when an implementation could not possibly be used to solve problems on the scale contemplated and must be improved or abandoned develop improved implementations through a process of stepwise refinement validating the efficacy of ideas for improvement through empirical analysis mathematical analysis or both find high level abstract representations of data structures or algorithms in op eration that enable effective high level design of improved versions strive for worst case performance guarantees when possible but accept good performance on typical data when available know when to leave further improvements for detailed in depth study to skilled researchers and move on to the next problem the potential for spectacular performance improvements for practical problems such as those that we saw for union find makes algorithm design a compelling field of study what other design activities hold the potential to reap savings factors of millions or billions or more developing an efficient algorithm is an intellectually satisfying activity that can have direct practical payoff as the dynamic connectivity problem indicates a simply stated problem can lead us to study numerous algorithms that are not only both useful and interesting but also intricate and challenging to understand we shall encounter many ingenious algorithms that have been developed over the years for a host of practical problems as the scope of applicability of computational solutions to scientific and commercial problems widens so also grows the importance of being able to use ef ficient algorithms to solve known problems and of being able to develop efficient solu tions to new problems q i d like to add a delete method to the api that allows clients to delete connec tions any advice on how to proceed a no one has devised an algorithm as simple and efficient as the ones in this section that can handle deletions this theme recurs throughout this book several of the data structures that we consider have the property that deleting something is much more difficult than adding something q what is the cell probe model a a model of computation where we only count accesses to a random access memory large enough to hold the input and consider all other operations to be free 5 show the contents of the id array and the number of times the ar ray is accessed for each input pair when you use quick find for the sequence 5 8 7 5 7 5 do exercise 5 but use quick union page in addition draw the forest of trees represented by the id array after each input pair is processed 5 do exercise 5 but use weighted quick union page 5 show the contents of the sz and id arrays and the number of array accesses for each input pair corresponding to the weighted quick union examples in the text both the reference input and the worst case input 5 5 estimate the minimum amount of time in days that would be required for quick find to solve a dynamic connectivity problem with sites and input pairs on a computer capable of executing instructions per second assume that each itera tion of the inner for loop requires machine instructions 5 repeat exercise 5 5 for weighted quick union 5 7 develop classes quickunionuf and quickfinduf that implement quick union and quick find respectively 5 8 give a counterexample that shows why this intuitive implementation of union for quick find is not correct public void union int p int q if connected p q return rename p component to q name for int i i id length i if id i id p id i id q count 5 draw the tree corresponding to the id array depicted at right can this be the result of running weighted quick union explain why this is impossible or give a sequence of operations that results in this array i 5 7 8 9 id i 5 5 exercises continued 5 in the weighted quick union algorithm suppose that we set id find p to q instead of to id find q would the resulting algorithm be correct answer yes but it would increase the tree height so the performance guarantee would be invalid 5 implement weighted quick find where you always change the id entries of the smaller component to the identifier of the larger component how does this change affect performance 5 quick union with path compression modify quick union page to include path compression by adding a loop to union that links every site on the paths from p and q to the roots of their trees to the root of the new tree give a sequence of input pairs that causes this method to produce a path of length note the amortized cost per operation for this algorithm is known to be logarithmic 5 weighted quick union with path compression modify weighted quick union algorithm 5 to implement path compression as described in exercise 5 give a sequence of input pairs that causes this method to produce a tree of height note the amortized cost per operation for this algorithm is known to be bounded by a function known as the inverse ackermann function and is less than 5 for any conceivable practical value of n 5 weighted quick union by height develop a uf implementation that uses the same basic strategy as weighted quick union but keeps track of tree height and always links the shorter tree to the taller one prove a logarithmic upper bound on the height of the trees for n sites with your algorithm 5 binomial trees show that the number of nodes at each level in the worst case trees for weighted quick union are binomial coefficients compute the average depth of a node in a worst case tree with n nodes 5 amortized costs plots instrument your implementations from exercise 5 7 to make amortized costs plots like those in the text 5 random connections develop a uf client erdosrenyi that takes an integer value n from the command line generates random pairs of integers between and n calling connected to determine if they are connected and then union if not as in our development client looping until all sites are connected and printing the number of connections generated package your program as a static method count that takes n as argument and returns the number of connections and a main that takes n from the command line calls count and prints the returned value 5 random grid generator write a program randomgrid that takes an int value n from the command line generates all the connections in an n by n grid puts them in random order randomly orients them so that p q and q p are equally likely to oc cur and prints the result to standard output to randomly order the connections use a randombag see exercise on page to encapsulate p and q in a single object creative problems continued use the connection nested class shown below package your program as two static methods generate which takes n as argument and returns an array of connec tions and main which takes n from the command line calls generate and iterates through the returned array to print the connections 5 animation write a randomgrid client see exercise 5 that uses unionfind as in our development client to check connectivity and uses stddraw to draw the connections as they are processed 5 dynamic growth using linked lists or a resizing array develop a weighted quick union implementation that removes the restriction on needing the number of objects ahead of time add a method newsite to the api which returns an int identifier 5 erds renyi model use your client from exercise 5 to test the hypothesis that the number of pairs generated to get one component is ln n 5 doubling test for erds renyi model develop a performance testing client that takes an int value t from the command line and performs t trials of the following ex periment use your client from exercise 5 to generate random connections using unionfind to determine connectivity as in our development client looping until all sites are connected for each n print the value of n the average number of connections processed and the ratio of the running time to the previous use your program to vali date the hypotheses in the text that the running times for quick find and quick union are quadratic and weighted quick union is near linear 5 compare quick find with quick union for erds renyi model develop a perfor mance testing client that takes an int value t from the command line and performs t trials of the following experiment use your client from exercise 5 to generate random connections save the connections so that you can use both quick union and quick find to determine connectivity as in our development client looping until all sites are connected for each n print the value of n and the ratio of the two running times 5 fast algorithms for erds renyi model add weighted quick union and weight ed quick union with path compression to your tests from exercise 5 can you discern a difference between these two algorithms 5 doubling test for random grids develop a performance testing client that takes an int value t from the command line and performs t trials of the following experie ment use your client from exercise 5 to generate the connections in an n by n square grid randomly oriented and in random order then use unionfind to determine connectivity as in our development client looping until all sites are connected for each n print the value of n the average number of connections processed and the ratio of the running time to the previous use your program to validate the hypotheses in the text that the running times for quick find and quick union are quadratic and weighted quick union is near linear note as n doubles the number of sites in the grid increases by a factor of so expect a doubling factor of for quadratic and for linear experiments continued 5 amortized plot for erds renyi develop a client that takes an int value n from the command line and does an amortized plot of the cost of all operations in the style of the plots in the text for the process of generating random pairs of integers between and n calling connected to determine if they are connected and then union if not as in our development client looping until all sites are connected this page intentionally left blank two sorting elementary sorts mergesort quicksort priority queues 5 applications orting is the process of rearranging a sequence of objects so as to put them in some logical order for example your credit card bill presents transactions in order by date they were likely put into that order by a sorting algorithm in the early days of computing the common wisdom was that up to percent of all com puting cycles was spent sorting if that fraction is lower today one likely reason is that sorting algorithms are relatively efficient not that sorting has diminished in relative importance indeed the ubiquity of computer usage has put us awash in data and the first step to organizing data is often to sort it all computer systems have implementa tions of sorting algorithms for use by the system and by users there are three practical reasons for you to study sorting algorithms even though you might just use a system sort analyzing sorting algorithms is a thorough introduction to the approach that we use to compare algorithm performance throughout the book similar techniques are effective in addressing other problems we often use sorting algorithms as a starting point to solve other problems more important than these practical reasons is that the algorithms are elegant classic and effective sorting plays a major role in commercial data processing and in modern scientific computing applications abound in transaction processing combinatorial optimiza tion astrophysics molecular dynamics linguistics genomics weather prediction and many other fields indeed a sorting algorithm quicksort in section was named as one of the top ten algorithms for science and engineering of the century in this chapter we consider several classical sorting methods and an efficient imple mentation of a fundamental data type known as the priority queue we discuss the theoretical basis for comparing sorting algorithms and conclude the chapter with a survey of applications of sorting and priority queues for our first excursion into the area of sorting algorithms we shall study two ele mentary sorting methods and a variation of one of them among the reasons for study ing these relatively simple algorithms in detail are the following first they provide context in which we can learn terminology and basic mechanisms second these simple algorithms are more effective in some applications than the sophisticated algorithms that we shall discuss later third they are useful in improving the efficiency of more sophisticated algorithms as we will see rules of the game our primary concern is algorithms for rearranging arrays of items where each item contains a key the objective of the sorting algorithm is to rear range the items such that their keys are ordered according to some well defined order ing rule usually numerical or alphabetical order we want to rearrange the array so that each entry key is no smaller than the key in each entry with a lower index and no larger than the key in each entry with a larger index specific characteristics of the keys and the items can vary widely across applications in java items are just objects and the abstract notion of a key is captured in a built in mechanism the comparable interface that is described on page the class example on the facing page illustrates the conventions that we shall use we put our sort code in a sort method within a single class along with private helper functions less and exch and perhaps some others and a sample client main example also illustrates code that might be useful for initial debugging its test client main sorts strings from standard input using the private method show to print the contents of the array later in this chapter we will examine various test clients for com paring algorithms and for studying their performance to differentiate sorting meth ods we give our various sort classes different names clients can call different imple mentations by name insertion sort merge sort quick sort and so forth with but a few exceptions our sort code refers to the data only through two opera tions the method less that compares items and the method exch that exchanges them the exch method is easy to implement and the comparable interface makes it easy to implement less restricting data access to these two operations makes our code readable and portable and makes it easier for us certify that algorithms are cor rect to study performance and to compare algorithms before proceeding to consider sort implementations we discuss a number of important issues that need to be care fully considered for every sort template for sort classes public class example public static void sort comparable a see algorithms 5 or 7 private static boolean less comparable v comparable w return v compareto w private static void exch comparable a int i int j comparable t a i a i a j a j t private static void show comparable a print the array on a single line for int i i a length i stdout print a i stdout println public static boolean issorted comparable a test whether the array entries are in order for int i i a length i if less a i a i return false return true public static void main string args read strings from standard input sort them and print string a in readstrings sort a assert issorted a show a this class illustrates our conventions for imple menting array sorts for each sorting algorithm that we consider we present a sort method for a class like this with example changed to a name that corresponds to the algorithm the test client sorts strings taken from standard input but with this code our sort methods are effective for any type of data that implements comparable certification does the sort implementation always put the array in order no mat ter what the initial order as a conservative practice we include the statement assert issorted a in our test client to certify that array entries are in order after the sort it is reasonable to include this statement in every sort implementation even though we normally test our code and develop mathematical arguments that our al gorithms are correct note that this test is sufficient only if we use exch exclusively to change array entries when we use code that stores values into the array directly we do not have full assurance for example code that destroys the original input array by setting all values to be the same would pass this test running time we also test algorithm performance we start by proving facts about the number of basic operations compares and exchanges or perhaps the number of times the array is ac cessed for read or write that the various sorting algorithms per form for various natural input models then we use these facts to develop hypotheses about the comparative performance of the algorithms and present tools that you can use to experimentally check the validity of such hypotheses we use a consistent coding style to facilitate the development of valid hypotheses about per formance that will hold true for typical implementations extra memory the amount of extra memory used by a sorting algorithm is often as important a factor as running time the sorting algorithms divide into two basic types those that sort in place and use no extra memory except perhaps for a small function call stack or a constant number of instance variables and those that need enough extra memory to hold another copy of the array to be sorted types of data our sort code is effective for any item type that implements the comparable interface adhering to java convention in this way is convenient be cause many of the types of data that you might want to sort implement comparable for example java numeric wrapper types such as integer and double implement comparable as do string and various advanced types such as file or url thus you can just call one of our sort methods with an array of any of these types as argu ment for example the code at right uses quicksort see section to sort n random double values when we create types of our own we can enable client code to sort that type of data by implementing the comparable in terface to do so we just need to implement a compareto method that defines an ordering on objects of that type known as the natural order for that type as shown here for our date data type see page java convention is that the call v compareto w returns an integer that is negative zero or positive usually or when v w v w or v w respectively for economy we use standard notation like v w as short hand for code like v compareto w for the remainder of this paragraph by convention v compareto w throws an exception if v and w are incompatible types or either is null furthermore compareto must implement a total order it must be reflexive for all v v v antisymmetric for all v and w if v w then w v and if v w then w v transitive for all v w and x if v w and w x then v x these rules are intuitive and standard in mathematics you will have little difficulty adhering to them in short compareto implements our key ab straction it defines the ordering of the items objects to be sorted which can be any type of data that implements comparable note that compareto need not use all of the instance variables indeed the key might be a small part of each item for the remainder of this chapter we shall address numerous algorithms for sort ing arrays of objects having a natural order to compare and contrast the algorithms we shall examine a number of their properties including the number of compares and exchanges that they use for various types of inputs and the amount of extra memory that they use these properties lead to the development of hypotheses about perfor mance properties many of which have been validated on countless computers over the past several decades specific implementations always need to be checked so we also consider tools for doing so after considering the classic selection sort insertion sort shellsort mergesort quicksort and heapsort algorithms we will consider practical is sues and applications in section 5 selection sort one of the simplest sorting algorithms works as follows first find the smallest item in the array and exchange it with the first entry itself if the first entry is already the smallest then find the next smallest item and exchange it with the sec ond entry continue in this way until the entire array is sorted this method is called selection sort because it works by repeatedly selecting the smallest remaining item as you can see from the implementation in algorithm the inner loop of selec tion sort is just a compare to test a current item against the smallest item found so far plus the code necessary to increment the current index and to check that it does not exceed the array bounds it could hardly be simpler the work of moving the items around falls outside the inner loop each exchange puts an item into its final position so the number of exchanges is n thus the running time is dominated by the number of compares in summary selection sort is a simple sorting method that is easy to understand and to implement and is characterized by the following two signature properties running time is insensitive to input the process of finding the smallest item on one pass through the array does not give much information about where the smallest item might be on the next pass this property can be disadvantageous in some situations for example the person using the sort client might be surprised to realize that it takes about as long to run selection sort for an array that is already in order or for an array with all keys equal as it does for a randomly ordered array as we shall see other algo rithms are better able to take advantage of initial order in the input data movement is minimal each of the n exchanges changes the value of two array entries so selection sort uses n exchanges the number of array accesses is a linear function of the array size none of the other sorting algorithms that we consider have this property most involve linearithmic or quadratic growth algorithm selection sort public class selection public static void sort comparable a sort a into increasing order int n a length array length for int i i n i exchange a i with smallest entry in a i n int min i index of minimal entr for int j i j n j if less a j a min min j exch a i min see page for less exch issorted and main for each i this implementation puts the ith smallest item in a i the entries to the left of position i are the i smallest items in the array and are not examined again a i min 3 5 6 7 8 9 entries in black are examined to find s o r t e x a m p l e 6 s o r t e x a m p l e a o r t e x s m p l e a e r t o x s m p l e 3 9 a e e t o x s m p l r 7 a e e l o x s m p t r 5 7 a e e l m x s o p t r 6 8 a e e l m o s x p t r 7 a e e l m o p x s t r 8 8 a e e l m o p r s t x 9 9 a e e l m o p r s t x a e e l m o p r s t x a e e l m o p r s t x the minimum entries in red are a min entries in gray are in final position trace of selection sort array contents just after each exchange insertion sort the algorithm that people often use to sort bridge hands is to con sider the cards one at a time inserting each into its proper place among those already considered keeping them sorted in a computer implementation we need to make space to insert the current item by moving larger items one position to the right before inserting the current item into the vacated position algorithm is an implementa tion of this method which is called insertion sort as in selection sort the items to the left of the current index are in sorted order dur ing the sort but they are not in their final position as they may have to be moved to make room for smaller items encountered later the array is however fully sorted when the index reaches the right end unlike that of selection sort the running time of insertion sort depends on the ini tial order of the items in the input for example if the array is large and its entries are already in order or nearly in order then insertion sort is much much faster than if the entries are randomly ordered or in reverse order insertion sort works well for certain types of nonrandom arrays that often arise in practice even if they are huge for example as just mentioned consider what happens when you use insertion sort on an array that is already sorted each item is immediately determined to be in its proper place in the array and the total running time is linear the running time of selection sort is quadratic for such an array the same is true for arrays whose keys are all equal hence the condition in proposition b that the keys must be distinct algorithm insertion sort public class insertion public static void sort comparable a sort a into increasing order int n a length for int i i n i insert a i among a i a i a i 3 for int j i j less a j a j j exch a j j see page for less exch issorted and main for each i from to n exchange a i with the entries that are smaller in a through a i as the index i travels from left to right the entries to its left are in sorted order in the array so the array is fully sorted when i reaches the right end trace of insertion sort array contents just after each insertion entries in gray do not move entry in red is a j entries in black moved one position right for insertion more generally we consider the concept of a partially sorted array as follows an in version is a pair of entries that are out of order in the array for instance e x a m p l e has inversions e a x a x m x p x l x e m l m e p l p e and l e if the number of inversions in an array is less than a constant multiple of the array size we say that the array is partially sorted typical examples of partially sorted arrays are the following an array where each entry is not far from its final position a small array appended to a large sorted array an array with only a few entries that are not in place insertion sort is an efficient method for such arrays selection sort is not indeed when the number of inversions is low insertion sort is likely to be faster than any sorting method that we consider in this chapter it is not difficult to speed up insertion sort substantially by shortening its inner loop to move the larger entries to the right one position rather than doing full exchanges thus cutting the number of array accesses in half we leave this improvement for an exercise see exercise in summary insertion sort is an excellent method for partially sorted arrays and is also a fine method for tiny arrays these facts are important not just because such arrays frequently arise in practice but also because both types of arrays arise in intermediate stages of advanced sorting algorithms so we will be considering insertion sort again in relation to such algorithms visualizing sorting algorithms throughout this chapter we will be using a simple visual representation to help describe the properties of sorting algorithms rather than tracing the progress of a sort with key values such as letters numbers or words we use vertical bars to be sorted by their heights the advantage of such a representation is that it can give insights into the behavior of a sort ing method for example you can see at a glance on the visual traces at right that insertion sort does not touch entries to the right of the scan pointer and selec tion sort does not touch entries to the left of the scan pointer moreover it is clear from the visual traces that since insertion sort also does not touch entries smaller than the inserted item it uses about half the number of compares as selection sort on the average with our stddraw library developing a visual trace is not much more difficult than doing a stan dard trace we sort double values instrument the algorithm to call show as appropriate just as we do for a standard trace and develop a version of show that uses stddraw to draw the bars instead of printing the results the most complicated task is setting the scale for the y axis so that the lines of the trace appear in the expected order you are en couraged to work exercise in order to gain a better appreciation of the value of visual traces and gray entries are untouched black entries are involved in compares the ease of creating them an even simpler task is to animate the trace so that you can see the array dynamically evolve to the sorted result developing an animated trace in insertion sort selection sort visual traces of elementary sorting algorithms volves essentially the same process described in the previous paragraph but without having to worry about the y axis just clear the window and redraw the bars each time though we cannot make the case on the printed page such animated representations are also effective in gaining insight into how an algorithm works you are also encour aged to work exercise to see for yourself comparing two sorting algorithms now that we have two implementations we are naturally interested in knowing which one is faster selection sort algorithm or insertion sort algorithm questions like this arise again and again and again in the study of algorithms and are a major focus throughout this book we have discussed some fundamental ideas in chapter but we use this first case in point to illustrate our basic approach to answering such questions generally following the ap proach introduced in section we compare algorithms by implementing and debugging them analyzing their basic properties formulating a hypothesis about comparative performance running experiments to validate the hypothesis these steps are nothing more than the time honored scientific method applied to the study of algorithms in the present context algorithm and algorithm are evidence of the first step propositions a b and c constitute the second step property d on page constitutes the third step and the class sortcompare on page enables the fourth step these activities are all interrelated our brief descriptions mask a substantial amount of effort that is required to prop erly implement analyze and test algorithms every programmer knows that such code is the product of a long round of debugging and refinement every mathematician knows that proper analysis can be very difficult and every scientist knows that formu lating hypotheses and designing and executing experiments to validate them require great care full development of such results is reserved for experts studying our most important algorithms but every programmer using an algorithm should be aware of the scientific context underlying its performance properties having developed implementations our next choice is to settle on an appropriate model for the input for sorting a natural model which we have used for proposi tions a b and c is to assume that the arrays are randomly ordered and that the key values are distinct in applications where significant numbers of equal key values are present we will need a more complicated model how do we formulate a hypothesis about the running times of insertion sort and selection sort for randomly ordered arrays examining algorithms and and propositions a and b it follows immediately that the running time of both algorithms should be quadratic for randomly ordered arrays that is the running time of insertion sort for such an input is proportional to some small constant times n and the running time of selection sort is proportional to some other small constant times n the values of the two constants depend on the cost of compares and exchanges on the particular computer being used for many types of data and for typical computers it is reasonable to assume that these costs are similar though we will see a few significant exceptions the following hypothesis follows directly to validate this hypothesis we use sortcompare see page to perform the experi ments as usual we use stopwatch to compute the running time the implementation of time shown here does the job for the basic sorts in this chapter the randomly or dered input model is embedded in the timerandominput method in sortcompare which generates random double values sorts them and returns the total measured time of the sort for the given number of trials using ran dom double values between and is much simpler than the alternative of us ing a library function such as stdrandom shuffle and is effective because equal key values are very unlikely see exercise 5 as discussed in chapter the number of trials is taken as an argument both to take advan tage of the law of large numbers the more trials the total running time divided by the number of trials is a more accurate estimate of the true average running time and to help damp out system effects you are encouraged to experiment with sortcompare comparing two sorting algorithms public class sortcompare public static double time string alg double a see text public static double timerandominput string alg int n int t use alg to sort t random arrays of length n double total double a new double n for int t t t t perform one experiment generate and sort an array for int i i n i a i stdrandom uniform total time alg a return total public static void main string args string args string args int n integer parseint args int t integer parseint args 3 double timerandominput n t total for double timerandominput n t total for stdout printf for d random doubles n is n stdout printf times faster than n t1 this client runs the two sorts named in the first two command line arguments on arrays of n the third command line argument random double values between and repeating the experi ment t the fourth command line argument times then prints the ratio of the total running times on your computer to learn the extent to which its conclusion about insertion sort and selection sort is robust property d is intentionally a bit vague the value of the small constant factor is left unstated and the assumption that the costs of compares and exchanges are similar is left unstated so that it can apply in a broad variety of situations when possible we try to capture essential aspects of the performance of each of the algorithms that we study in statements like this as discussed in chapter each property that we consider needs to be tested scientifically in a given situation perhaps supplemented with a more refined hypothesis based upon a related proposition mathematical truth for practical applications there is one further step which is crucial run experiments to validate the hypothesis on the data at hand we defer consideration of this step to section 5 and the exercises in this case if your sort keys are not distinct and or not randomly ordered property d might not hold you can randomly order an array with stdrandom shuffle but applications with significant numbers of equal keys involve more careful analysis our discussions of the analyses of algorithms are intended to be starting points not final conclusions if some other question about performance of the algorithms comes to mind you can study it with a tool like sortcompare many opportunities to do so are presented in the exercises we do not dwell further on the comparative performance of insertion sort and selec tion sort because we are much more interested in algorithms that can run a hundred or a thousand or a million times faster than either still understanding these elementary algorithms is worthwhile for several reasons they help us work out the ground rules they provide performance benchmarks they often are the method of choice in some specialized situations they can serve as the basis for developing better algorithms for these reasons we will use the same basic approach and consider elementary algo rithms for every problem that we study throughout this book not just sorting pro grams like sortcompare play a critical role in this incremental approach to algorithm development at every step along the way we can use such a program to help evaluate whether a new algorithm or an improved version of a known algorithm provides the performance gains that we expect h shellsort to exhibit the value of knowing properties of elementary sorts we next consider a fast algorithm based on insertion sort insertion sort is slow for large un ordered arrays because the only exchanges it does involve adjacent entries so items can move through the array only one place at a time for example if the item with the smallest key happens to be at the end of the array n exchanges are needed to get that one item where it belongs shellsort is a simple extension of insertion sort that gains speed by allowing exchanges of array entries that are far apart to produce partially sorted arrays that can be efficiently sorted eventually by insertion sort the idea is to rearrange the array to give it the property that taking every hth entry starting anywhere yields a sorted subsequence such an array is said to be h sorted put another way an h sorted array is h inde pendent sorted subsequences interleaved l e e a m h l e p s o l t s x r l m p t e h s s e l o x a e l r an h sorted sequence is h interleaved sorted subsequences together by h sorting for some large val ues of h we can move items in the array long distances and thus make it easier to h sort for smaller values of h using such a procedure for any sequence of values of h that ends in will produce a sorted ar ray that is shellsort the implementation in algorithm 3 on the facing page uses the sequence of decreasing values starting at the largest increment less than n 3 and decreasing to we refer to such a sequence as an increment sequence algorithm 3 computes its increment sequence another alternative is to store an increment sequence in an array one way to implement shellsort would be for each h to use insertion sort indepen dently on each of the h subsequences because the subsequences are independent we can use an even simpler approach when h sorting the array we insert each item among the previous items in its h subsequence by exchanging it with those that have larger keys moving them each one position to the right in the subsequence we accomplish this task by using the insertion sort code but modified to decrement by h instead of when moving through the array this observation reduces the shellsort implementa tion to an insertion sort like pass through the array for each increment shellsort gains efficiency by making a tradeoff between size and partial order in the subsequences at the beginning the subsequences are short later in the sort the subse quences are partially sorted in both cases insertion sort is the method of choice the extent to which the subsequences are partially sorted is a variable factor that depends strongly on the increment sequence understanding shellsort performance is a chal lenge indeed algorithm 3 is the only sorting method we consider whose perfor mance on randomly ordered arrays has not been precisely characterized algorithm 3 shellsort public class shell public static void sort comparable a sort a into increasing order int n a length int h while h n 3 h 3 h 121 while h h sort the array for int i h i n i insert a i among a i h a i h a i 3 h for int j i j h less a j a j h j h exch a j j h h h 3 see page for less exch issorted and main if we modify insertion sort algorithm to h sort the array and add an outer loop to decrease h through a sequence of increments starting at an increment as large as a constant fraction of the ar ray length and ending at we are led to this compact shellsort implementation input s h e l l s o r t e x a m p l e sort p h e l l s o r t e x a m s l e sort l e e a m h l e p s o l t s x r sort a e e e h l l l m o p r s s t x shellsort trace array contents after each pass how do we decide what increment sequence to use in general this question is a dif ficult one to answer the performance of the algorithm depends not just on the num ber of increments but also on arithmetical interactions among the increments such as the size of their common divi input s h e l l s o r t e x a m p l e detailed trace of shellsort insertions sors and other properties many different increment sequences have been studied in the lit erature but no provably best sequence has been found the increment sequence that is used in algorithm 3 is easy to compute and use and performs nearly as well as more sophisti cated increment sequences that have been discovered that have provably better worst case per formance increment sequences that are substantially better still may be waiting to be discovered shellsort is useful even for large arrays particularly by contrast with selection sort and insertion sort it also performs well on arrays that are in arbi trary order not necessarily ran dom indeed constructing an array for which shellsort runs slowly for a particular incre ment sequence is usually a chal lenging exercise as you can learn with sortcompare shellsort is much faster than insertion sort and selection sort and its speed ad vantage increases with the array size before reading further try using sortcompare to compare shellsort with insertion sort and selection sort for array sizes that are increasing powers of on your computer see exercise you will see that shellsort makes it possible to address sorting input sorted sorted sorted result visual trace of shellsort problems that could not be addressed with the more elementary algorithms this ex ample is our first practical illustration of an important principle that pervades this book achieving speedups that enable the solution of problems that could not otherwise be solved is one of the prime reasons to study algorithm performance and design the study of the performance characteristics of shellsort requires mathematical ar guments that are beyond the scope of this book if you want to be convinced start by thinking about how you would prove the following fact when an h sorted array is k sorted it remains h sorted as for the performance of algorithm 3 the most im portant result in the present context is the knowledge that the running time of shellsort is not necessarily quadratic for example it is known that the worst case number of compares for algorithm 3 is proportional to n 3 that such a simple modification can break the quadratic running time barrier is quite interesting as doing so is a prime goal for many algorithm design problems no mathematical results are available about the average case number of compares for shellsort for randomly ordered input increment sequences have been devised that drive the asymptotic growth of the worst case number of compares down to n 4 3 n 5 4 n 6 5 but many of these results are primarily of academic interest because these functions are hard to distinguish from one another and from a constant factor of n for practical values of n in practice you can safely take advantage of the past scientific study of shellsort just by using the increment sequence in algorithm 3 or one of the increment sequences in the exercises at the end of this section which may improve performance by to percent moreover you can easily validate the following hypothesis experienced programmers sometimes choose shellsort because it has acceptable running time even for moderately large arrays it requires a small amount of code and it uses no extra space in the next few sections we shall see methods that are more ef ficient but they are perhaps only twice as fast if that much except for very large n and they are more complicated if you need a solution to a sorting problem and are work ing in a situation where a system sort may not be available for example code destined for hardware or an embedded system you can safely use shellsort then determine sometime later whether it will be worthwhile to replace it with a more sophisticated method q sorting seems like a toy problem aren t many of the other things that we do with computers much more interesting a perhaps but many of those interesting things are made possible by fast sorting al gorithms you will find many examples in section 5 and throughout the rest of the book sorting is worth studying now because the problem is easy to understand and you can appreciate the ingenuity behind the faster algorithms q why so many sorting algorithms a one reason is that the performance of many algorithms depends on the input val ues so different algorithms might be appropriate for different applications having dif ferent kinds of input for example insertion sort is the method of choice for partially sorted or tiny arrays other constraints such as space and treatment of equal keys also come into play we will revisit this question in section 5 q why bother using the tiny helper methods less and exch a they are basic abstract operations needed by any sort algorithm and the code is easier to understand in terms of these abstractions moreover they make the code di rectly portable to other settings for example much of the code in algorithms and is legal code in several other programming languages even in java we can use this code as the basis for sorting primitive types which are not comparable simply implement less with the code v w q when i run sortcompare i get different values each time that i run it and those are different from the values in the book why a for starters you have a different computer from the one we used not to mention a different operating system java runtime and so forth all of these differences might lead to slight differences in the machine code for the algorithms differences each time that you run it on your computer might be due to other applications that you are run ning or various other conditions running a very large number of trials should dampen the effect the lesson is that small differences in algorithm performance are difficult to notice nowadays that is a primary reason that we focus on large ones show in the style of the example trace with algorithm how selection sort sorts the array e a s y q u e s t i o n what is the maximum number of exchanges involving any particular element during selection sort what is the average number of exchanges involving an element 3 give an example of an array of n items that maximizes the number of times the test a j a min fails and therefore min gets updated during the operation of selection sort algorithm 4 show in the style of the example trace with algorithm how insertion sort sorts the array e a s y q u e s t i o n 5 for each of the two conditions in the inner for loop in insertion sort algo rithm describe an array of n items where that condition is always false when the loop terminates 6 which method runs faster for an array with all keys identical selection sort or insertion sort 7 which method runs faster for an array in reverse order selection sort or inser tion sort 8 suppose that we use insertion sort on a randomly ordered array where elements have only one of three values is the running time linear quadratic or something in between 9 show in the style of the example trace with algorithm 3 how shellsort sorts the array e a s y s h e l l s o r t q u e s t i o n why not use selection sort for h sorting in shellsort implement a version of shellsort that keeps the increment sequence in an array rather than computing it instrument shellsort to print the number of compares divided by the array size for each increment write a test client that tests the hypothesis that this number is a small constant by sorting arrays of random double values using array sizes that are increasing powers of starting at deck sort explain how you would put a deck of cards in order by suit in the order spades hearts clubs diamonds and by rank within each suit with the restriction that the cards must be laid out face down in a row and the only allowed operations are to check the values of two cards and to exchange two cards keeping them face down dequeue sort explain how you would sort a deck of cards with the restric tion that the only allowed operations are to look at the values of the top two cards to exchange the top two cards and to move the top card to the bottom of the deck expensive exchange a clerk at a shipping company is charged with the task of rearranging a number of large crates in order of the time they are to be shipped out thus the cost of compares is very low just look at the labels relative to the cost of ex changes move the crates the warehouse is nearly full there is extra space sufficient to hold any one of the crates but not two what sorting method should the clerk use certification write a check method that calls sort for a given array and returns true if sort puts the array in order and leaves the same set of objects in the array as were there initially false otherwise do not assume that sort is restricted to move data only with exch you may use arrays sort and assume that it is correct animation add code to insertion and selection to make them draw the array contents as vertical bars like the visual traces in this section redrawing the bars after each pass to produce an animated effect ending in a sorted picture where the bars appear in order of their height hint use a client like the one in the text that gener ates random double values insert calls to show as appropriate in the sort code and implement a show method that clears the canvas and draws the bars visual trace modify your solution to the previous exercise to make insertion and selection produce visual traces such as those depicted in this section hint judi cious use of setyscale makes this problem easy extra credit add the code necessary to produce red and gray color accents such as those in our figures shellsort worst case construct an array of elements containing the num bers through for which shellsort with the increments 4 uses as large a number of compares as you can find shellsort best case what is the best case for shellsort justify your answer creative problems continued comparable transactions using our code for date page as a model ex pand your implementation of transaction exercise so that it implements comparable such that transactions are kept in order by amount solution public class transaction implements comparable transaction private final double amount public int compareto transaction that if this amount that amount return if this amount that amount return return transaction sort test client write a class sorttransactions that consists of a static method main that reads a sequence of transactions from standard input sorts them and prints the result on standard output see exercise 3 solution public class sorttransactions public static transaction readtransactions see exercise 3 public static void main string args transaction transactions readtransactions shell sort transactions for transaction t transactions stdout println t deck sort ask a few friends to sort a deck of cards see exercise ob serve them carefully and write down the method that they use insertion sort with sentinel develop an implementation of insertion sort that eliminates the j test in the inner loop by first putting the smallest item into position use sortcompare to evaluate the effectiveness of doing so note it is often possible to avoid an index out of bounds test in this way the element that enables the test to be eliminated is known as a sentinel insertion sort without exchanges develop an implementation of insertion sort that moves larger elements to the right one position with one array access per entry rather than using exch use sortcompare to evaluate the effectiveness of doing so primitive types develop a version of insertion sort that sorts arrays of int values and compare its performance with the implementation given in the text which sorts integer values and implicitly uses autoboxing and auto unboxing to convert shellsort is subquadratic use sortcompare to compare shellsort with insertion sort and selection sort on your computer use array sizes that are increasing powers of starting at equal keys formulate and validate hypotheses about the running time of in sertion sort and selection sort for arrays that contain just two key values assuming that the values are equally likely to occur shellsort increments run experiments to compare the increment sequence in algorithm 3 with the sequence 5 209 3905 64769 which is formed by merging together the se quences 9 9 and 3 see exercise geometric increments run experiments to determine a value of t that leads to the lowest running time of shellsort for random arrays for the increment sequence t t t 3 t 4 for n give the values of t and the increment sequences for the best three values that you find experiments continued the following exercises describe various clients for helping to evaluate sorting methods they are intended as starting points for helping to understand performance properties using ran dom data in all of them use time as in sortcompare so that you can get more accurate results by specifying more trials in the second command line argument we refer back to these exercises in later sections when evaluating more sophisticated methods 31 doubling test write a client that performs a doubling test for sort algorithms start at n equal to and print n the predicted number of seconds the actual num ber of seconds and the ratio as n doubles use your program to validate that insertion sort and selection sort are quadratic for random inputs and formulate and test a hy pothesis for shellsort plot running times write a client that uses stddraw to plot the average running times of the algorithm for random inputs and various values of the array size you may add one or two more command line arguments strive to design a useful tool 33 distribution write a client that enters into an infinite loop running sort on arrays of the size given as the third command line argument measures the time taken for each run and uses stddraw to plot the average running times a picture of the dis tribution of the running times should emerge corner cases write a client that runs sort on difficult or pathological cases that might turn up in practical applications examples include arrays that are already in order arrays in reverse order arrays where all keys are the same arrays consisting of only two distinct values and arrays of size or 1 35 nonuniform distributions write a client that generates test data by randomly ordering objects using other distributions than uniform including the following gaussian poisson geometric discrete see exercise 1 for a special case develop and test hypotheses about the effect of such input on the performance of the algorithms in this section 1 nonuniform data write a client that generates test data that is not uniform including the following half the data is half half the data is half the remainder is half the remainder is and so forth half the data is half random int values develop and test hypotheses about the effect of such input on the performance of the algorithms in this section 1 partially sorted write a client that generates partially sorted arrays including the following percent sorted last percent random values all entries within positions of their final place in the array sorted except for 5 percent of the entries randomly dispersed throughout the array develop and test hypotheses about the effect of such input on the performance of the algorithms in this section 1 various types of items write a client that generates arrays of items of various types with random key values including the following string key at least ten characters one double value double key ten string values all at least ten characters int key one int value develop and test hypotheses about the effect of such input on the performance of the algorithms in this section the algorithms that we consider in this section are based on a simple operation known as merging combining two ordered arrays to make one larger ordered array this operation immediately leads to a simple recursive sort method known as merge sort to sort an array divide it into two halves sort the two halves recursively and then merge the results as you will see one of mergesort most attractive properties is that it guarantees to sort any array of n items in time proportional to n log n its prime disadvantage is that it uses extra space proportional to n input m e r g e s o r t e x a m p l e sort left half e e g m o r r s t e x a m p l e sort right half e e g m o r r s a e e l m p t x merge results a e e e e g l m m o p r r s t x mergesort overview abstract in place merge the straightforward approach to implementing merg ing is to design a method that merges two disjoint ordered arrays of comparable ob jects into a third array this strategy is easy to implement create an output array of the requisite size and then choose successively the smallest remaining item from the two input arrays to be the next item added to the output array however when we mergesort a large array we are doing a huge number of merges so the cost of creating a new array to hold the output every time that we do a merge is problematic it would be much more desirable to have an in place method so that we could sort the first half of the array in place then sort the second half of the array in place then do the merge of the two halves by moving the items around within the ar ray without using a significant amount of other extra space it is worthwhile to pause momentarily to consider how you might do that at first blush this problem seems to be one that must be simple to solve but solutions that are known are quite complicated especially by comparison to alternatives that use extra space still the abstraction of an in place merge is useful accordingly we use the method signature merge a lo mid hi to specify a merge method that puts the result of merging the subarrays a lo mid with a mid 1 hi into a single ordered array leaving the result in a lo hi the code on the next page implements this merge method in just a few lines by copying everything to an auxiliary array and then merging back to the original another approach is described in exercise abstract in place merge public static void merge comparable a int lo int mid int hi merge a lo mid with a mid 1 hi int i lo j mid 1 for int k lo k hi k copy a lo hi to aux lo hi aux k a k for int k lo k hi k merge back to a lo hi if i mid a k aux j else if j hi a k aux i else if less aux j aux i a k aux j else a k aux i this method merges by first copying into the auxiliary array aux then merging back to a in the merge the second for loop there are four conditions left half exhausted take from the right right half exhausted take from the left current key on right less than current key on left take from the right and current key on right greater than or equal to current key on left take from the left a aux k 1 3 4 5 6 7 8 9 i j 1 3 4 5 6 7 8 9 input copy e e e e g g m m r r a a c c e e r r t t e e g m r a c e r t 5 a 6 e e g m r a c e r t 1 a c 7 e e g m r c e r t a c e 1 7 e e g m r e r t 3 a c e e 7 e g m r e r t 4 a c e e e 8 g m r e r t 5 a c e e e g 3 8 g m r r t 6 a c e e e g m 4 8 m r r t 7 a c e e e g m r 5 8 r r t 8 a c e e e g m r r 5 9 r t 9 a c e e e g m r r t 6 t merged result a c e e e g m r r t abstract in place merge trace top down mergesort algorithm 4 is a recur sive mergesort implementation based on this abstract in sort sort a sort a 7 place merge it is one of the best known examples of the utility of the divide and conquer paradigm for efficient algorithm design this recursive code is the basis for an inductive proof that the algorithm sorts the array if it sorts the two subarrays it sorts the whole array by merg ing together the subarrays to understand mergesort it is worthwhile to consider carefully the dynamics of the method calls shown in the trace at right to sort a the sort method calls itself to sort a 7 then calls itself to sort a 3 and a 1 before finally doing the first merge of a with a 1 after calling itself to sort a and then a 1 for brevity we omit the calls for the base case 1 entry sorts in the trace then the next merge is a with a 3 and then a 1 with a 3 and so forth from this trace we see that the sort code simply provides an orga nized way to sequence the calls to the merge method this insight will be useful later in this section the recursive code also provides us with the basis for analyzing mergesort running time because mergesort is a prototype of the divide and conquer algorithm de sign paradigm we will consider this analysis in detail left half sort right half merge results sort a 3 sort a 1 merge a 1 sort a 3 merge a 3 merge a 1 3 sort a 4 7 sort a 4 5 merge a 4 4 5 sort a 6 7 merge a 6 6 7 merge a 4 5 7 merge a 3 7 sort a 8 sort a 8 sort a 8 9 merge a 8 8 9 sort a merge a merge a 8 9 sort a sort a merge a sort a merge a merge a merge a 8 merge a 7 top down mergesort call trace algorithm 4 top down mergesort public class merge private static comparable aux auxiliary array for merges public static void sort comparable a aux new comparable a length allocate space just once sort a a length 1 private static void sort comparable a int lo int hi sort a lo hi if hi lo return int mid lo hi lo sort a lo mid sort left half sort a mid 1 hi sort right half merge a lo mid hi merge results code on page to sort a subarray a lo hi we divide it into two parts a lo mid and a mid 1 hi sort them independently via recursive calls and merge the resulting ordered subarrays to produce the result lo hi a 1 3 4 5 6 7 8 9 m e r g e s o r t e x a m p l e merge a 0 1 e m r g e s o r t e x a m p l e merge a 3 e m g r e s o r t e x a m p l e merge a 0 1 3 e g m r e s o r t e x a m p l e merge a 4 4 5 e g m r e s o r t e x a m p l e merge a 6 6 7 e g m r e s o r t e x a m p l e merge a 4 5 7 e g m r e o r s t e x a m p l e merge a 0 3 7 e e g m o r r s t e x a m p l e merge a 8 8 9 e e g m o r r s e t x a m p l e merge a e e g m o r r s e t a x m p l e merge a 8 9 e e g m o r r s a e t x m p l e merge a e e g m o r r s a e t x m p l e merge a e e g m o r r s a e t x m p e l merge a e e g m o r r s a e t x e l m p merge a 8 e e g m o r r s a e e l m p t x merge a 0 7 a e e e e g l m m o p r r s t x trace of merge results for top down mergesort term is the number of compares for the merge the lower bound c n c n c n n follows because the number of compares for the merge is at least n we derive an exact solution to the recurrence when equality holds and n is a power of say n first since n n 1 we have c 1 dividing both sides by gives c c 1 1 1 applying the same equation to the first term on the right we have c c 1 1 repeating the previous step n 1 additional times gives c c n which after multiplying both sides by leaves us with the solution c n c n n lg n exact solutions for general n are more complicated but it is not difficult to apply the same argument to the inequalities describing the bounds on the number of compares to prove the stated result for all values of n this proof is valid no matter what the input values are and no matter in what order they appear another way to understand proposition f is to examine the tree drawn below where each node depicts a subarray for which sort does a merge the tree has precisely n levels for k from 0 to n 1 the kth level from the top depicts subarrays each of length k each of which thus requires at most k compares for the merge thus we have k total cost for each of the n levels for a total of n 2n n lgn mergesort subarray dependence tree for n propositions f and g tell us that we can expect the time required by mergesort to be proportional to n log n that fact brings us to a different level from the elementary methods in section 1 because it tells us that we can sort huge arrays using just a logarithmic factor more time than it takes to examine every entry you can sort millions of items or more with mergesort but not with insertion sort or selection sort the primary drawback of mergesort is that it requires extra space proportional to n for the auxiliary array for merging if space is at a premium we need to consider another method on the other hand we can cut the running time of mergesort substantially with some carefully considered modifications to the implementation use insertion sort for small subarrays we can improve most recursive algorithms by handling small cases differently because the recursion guarantees that the method will be used often for small cases so improvements in handling them lead to improvements in the whole algorithm in the case of sorting we know that insertion sort or selection sort is simple and therefore likely to be faster than mergesort for tiny subarrays as usual a visual trace provides insight into the operation of mergesort the visual trace on the facing page shows the operation of a mergesort implementation with a cutoff for small subarrays switching to insertion sort for small subarrays length or less say will improve the running time of a typical mergesort implementation by to percent see exercise 23 test whether the array is already in order we can reduce the running time to be linear for arrays that are already in order by adding a test to skip the call to merge if a mid is less than or equal to a mid 1 with this change we still do all the recursive calls but the running time for any sorted subarray is linear see exercise 8 eliminate the copy to the auxiliary array it is possible to eliminate the time but not the space taken to copy to the auxiliary array used for merging to do so we use two invocations of the sort method one takes its input from the given array and puts the sorted output in the auxiliary array the other takes its input from the auxiliary array and puts the sorted output in the given array with this approach in a bit of recursive trickery we can arrange the recursive calls such that the computation switches the roles of the input array and the auxiliary array at each level see exercise first subarray second subarray first merge first half sorted l l1 i i ii l ii i i i 1 i i ii iiiii i iiii ii i ii i 1 11iiiii i iiiii i i 1 l 1 ii 1l1 1l1 ll111i l 11111111 i 1 i i ii ii i ii ii i ii i 1 i i ii 1 i11iii ii i ii ii i i 1 1i111i i i ii iiiii i iiii ii i 1 11111111111111111111111111111111111i i i iii i i 111111 1 1i 111 i i 111111ll 1 ll 1 l l l i i i i 1 i1i second half sorted result llllll l l l l ii ii ii ii ii 1 iiiiilil i ii i iil o o i i i i i i i i i i i i ii ii i i i i i i i i i i iiii 1111111111111111111111111111111111111111111111111111111111111111111111111111111111111 visual trace of top down mergesort with cutoff for small subarrays it is appropriate to repeat here a point raised in chapter 1 that is easily forgotten and needs reemphasis locally we treat each algorithm in this book as if it were critical in some application globally we try to reach general conclusions about which approach to recommend our discussion of such improvements is not necessarily a recommen dation to always implement them rather a warning not to draw absolute conclusions about performance from initial implementations when addressing a new problem your best bet is to use the simplest implementation with which you are comfortable and then refine it if it becomes a bottleneck addressing improvements that decrease running time just by a constant factor may not otherwise be worthwhile you need to test the effectiveness of specific improvements by running experiments as we indicate in exercises throughout in the case of mergesort the three improvements just listed are simple to implement and are of interest when mergesort is the method of choice for example in situations discussed at the end of this chapter bottom up mergesort the recursive implementation of mergesort is prototypi cal of the divide and conquer algorithm design paradigm where we solve a large prob lem by dividing it into pieces solving the subproblems then using the solutions for the pieces to solve the whole problem even though we are thinking in terms of merging together two large subarrays the fact is that most merges are merging together tiny subarrays another way to implement mergesort is to organize the merges so that we do all the merges of tiny subarrays on one pass then do a second pass to merge those sub arrays in pairs and so forth continuing until we do a merge that encompasses the whole array this method requires even less code than the standard recursive implementation we start by doing a pass of 1 by 1 merges considering individual items as subarrays of size 1 then a pass of by merges merge subarrays of size to make subarrays of size 4 then 4 by 4 merges and so forth the sec ond subarray may be smaller than the first in the last merge on each pass which is no problem for merge but otherwise all merges involve subar rays of equal size doubling the sorted subarray size for the next pass sz 1 4 8 visual trace of bottom up mergesort bottom up mergesort public class mergebu private static comparable aux auxiliary array for merges see page for merge code public static void sort comparable a do lg n passes of pairwise merges int n a length aux new comparable n for int sz 1 sz n sz sz sz sz subarray size for int lo 0 lo n sz lo sz sz lo subarray index merge a lo lo sz 1 math min lo sz sz 1 n 1 bottom up mergesort consists of a sequence of passes over the whole array doing sz by sz merges starting with sz equal to 1 and doubling sz on each pass the final subarray is of size sz only when the array size is an even multiple of sz otherwise it is less than sz a i 0 1 3 4 5 6 7 8 9 sz 1 m e r g e s o r t e x a m p l e merge a 0 0 1 e m r g e s o r t e x a m p l e merge a 3 e m g r e s o r t e x a m p l e merge a 4 4 5 e m g r e s o r t e x a m p l e merge a 6 6 7 e m g r e s o r t e x a m p l e merge a 8 8 9 e m g r e s o r e t x a m p l e merge a 10 e m g r e s o r e t a x m p l e merge a e m g r e s o r e t a x m p l e merge a e m g r e s o r e t a x m p e l sz merge a 0 1 3 e g m r e s o r e t a x m p e l merge a 4 5 7 e g m r e o r s e t a x m p e l merge a 8 9 11 e g m r e o r s a e t x m p e l merge a e g m r e o r s a e t x e l m p sz 4 merge a 0 3 7 e e g m o r r s a e t x e l m p merge a 8 11 e e g m o r r s a e e l m p t x sz 8 merge a 0 7 a e e e e g l m m o p r r s t x trace of merge results for bottom up mergesort when the array length is a power of top down and bottom up mergesort per form precisely the same compares and array accesses just in a different order when the array length is not a power of the sequence of compares and array accesses for the two algorithms will be different see exercise 5 a version of bottom up mergesort is the method of choice for sorting data orga nized in a linked list consider the list to be sorted sublists of size 1 then pass through to make sorted subarrays of size linked together then size 4 and so forth this method rearranges the links to sort the list in place without creating any new list nodes both the top down and bottom up approaches to implementing a divide and conquer algorithm are intuitive the lesson that you can take from mergesort is this whenever you encounter an algorithm based on one of these approaches it is worth considering the other do you want to solve the problem by breaking it up into smaller problems and solving them recursively as in merge sort or by building small solu tions into larger ones as in mergebu sort the complexity of sorting one important reason to know about mergesort is that we use it as the basis for proving a fundamental result in the field of computational complexity that helps us understand the intrinsic difficulty of sorting in general com putational complexity plays an important role in the design of algorithms and this result in particular is directly relevant to the design of sorting algorithms so we next consider it in detail the first step in a study of complexity is to establish a model of computation gen erally researchers strive to understand the simplest model relevant to a problem for sorting we study the class of compare based algorithms that make their decisions about items only on the basis of comparing keys a compare based algorithm can do an ar bitrary amount of computation between compares but cannot get any information about a key except by comparing it with another one because of our restriction to the comparable api all of the algorithms in this chapter are in this class note that we are ignoring the cost of array accesses as are many algorithms that we might imagine in chapter 5 we consider algorithms that are not restricted to comparable items proposition i no compare based sorting algorithm can guarantee to sort n items with fewer than lg n n lg n compares proof first we assume that the keys are all distinct since any algorithm must be able to sort such inputs now we use a binary tree to describe the sequence of com pares each node in the tree is either a leaf in 1 that indicates that the sort is complete and has discovered that the original inputs were in the order a a a in 1 or an internal node i j that corresponds to a com pare operation between a i and a j with a left subtree corresponding to the sequence of compares in the case that a i is less than a j and a right subtree corresponding to what happens if a i is greater than a j each path from the root to a leaf corresponds to the sequence of compares that the algorithm uses to establish the ordering given in the leaf for example here is a compare tree for n 3 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 we never explicitly construct such a tree it is a mathematical device for describ ing the compares used by any algorithm the first key observation in the proof is that the tree must have at least n leaves because there are n different permutations of n distinct keys if there are fewer than n leaves then some permutation is missing from the leaves and the algo rithm would fail for that permutation the number of internal nodes on a path from the root to a leaf in the tree is the number of compares used by the algorithm for some input we are interested in the length of the longest such path in the tree known as the tree height since it mea sures the worst case number of compares used by the algorithm now it is a basic combinatorial property of binary trees that a tree of height h has no more than leaves the tree of height h with the maximum number of leaves is perfectly bal anced or complete an example for h 4 is diagrammed on the next page this result serves as a guide for us to know when designing a sorting algorithm how well we can expect to do for example without such a result one might set out to try to design a compare based sorting algorithm that uses half as many compares as does mergesort in the worst case the lower bound in proposition i says that such an effort is futile no such algorithm exists it is an extremely strong statement that applies to any conceivable compare based algorithm proposition h asserts that the number of compares used by mergesort in the worst case is n lg n this result is an upper bound on the difficulty of the sorting problem in the sense that a better algorithm would have to guarantee to use a smaller number of compares proposition i asserts that no sorting algorithm can guarantee to use fewer than n lg n compares it is a lower bound on the difficulty of the sorting problem in the sense that even the best possible algorithm must use at least that many compares in the worst case together they imply it is important to note that like the model of computation we need to precisely define what we mean by an optimal algorithm for example we might tighten the definition of optimality and insist that an optimal algorithm for sorting is one that uses precisely lg n compares we do not do so because we could not notice the difference between such an algorithm and for example mergesort for large n or we might broaden the definition of optimality to include any sorting algorithm whose worst case number of compares is within a constant factor of n lg n we do not do so because we might very well notice the difference between such an algorithm and mergesort for large n computational complexity may seem rather abstract but fundamental re search on the intrinsic difficulty of solving computational problems hardly needs jus tification moreover when it does apply it is emphatically the case that computational complexity affects the development of good software first good upper bounds allow software engineers to provide performance guarantees there are many documented instances where poor performance has been traced to someone using a quadratic sort instead of a linearithmic one second good lower bounds spare us the effort of search ing for performance improvements that are not attainable but the optimality of mergesort is not the end of the story and should not be mis used to indicate that we need not consider other methods for practical applications that is not the case because the theory in this section has a number of limitations for example mergesort is not optimal with respect to space usage the worst case may not be likely in practice operations other than compares such as array accesses may be important one can sort certain data without using any compares thus we shall be considering several other sorting methods in this book q is mergesort faster than shellsort a in practice their running times are within a small constant factor of one another when shellsort is using a well tested increment sequence like the one in algorithm 3 so comparative performance depends on the implementations java sortcompare merge shell for random double values merge is 1 times faster than shell in theory no one has been able to prove that shellsort is linearithmic for random data so there remains the possibility that the asymptotic growth of the average case perfor mance of shellsort is higher such a gap has been proven for worst case performance but it is not relevant in practice q why not make the aux array local to merge a to avoid the overhead of creating an array for every merge even the tiny ones this cost would dominate the running time of mergesort see exercise a more proper solution which we avoid in the text to reduce clutter in the code is to make aux local to sort and pass it as an argument to merge see exercise 9 q how does mergesort fare when there are duplicate values in the array a if all the items have the same value the running time is linear with the extra test to skip the merge when the array is sorted but if there is more than one duplicate value this performance gain is not necessarily realized for example suppose that the input array consists of n items with one value in odd positions and n items with another value in even positions the running time is linearithmic for such an array it satisfies the same recurrence as for items with distinct values not linear 1 give a trace in the style of the trace given at the beginning of this section show ing how the keys a e q s u y e i n o s t are merged with the abstract in place merge method give traces in the style of the trace given with algorithm 4 showing how the keys e a s y q u e s t i o n are sorted with top down mergesort 3 answer exercise for bottom up mergesort 4 does the abstract in place merge produce proper output if and only if the two input subarrays are in sorted order prove your answer or provide a counterexample 5 give the sequence of subarray sizes in the merges performed by both the top down and the bottom up mergesort algorithms for n 6 write a program to compute the exact value of the number of array accesses used by top down mergesort and by bottom up mergesort use your program to plot the val ues for n from 1 to and to compare the exact values with the upper bound lg n 7 show that the number of compares used by mergesort is monotonically increas ing c n 1 c n for all n 0 8 suppose that algorithm 4 is modified to skip the call on merge whenever a mid a mid 1 prove that the number of compares used to mergesort a sorted array is linear 9 use of a static array like aux is inadvisable in library software because multiple clients might use the class concurrently give an implementation of merge that does not use a static array do not make aux local to merge see the q a for this section hint pass the auxiliary array as an argument to the recursive sort 10 faster merge implement a version of merge that copies the second half of a to aux in decreasing order and then does the merge back to a this change al lows you to remove the code to test that each of the halves has been exhausted from the inner loop note the resulting sort is not stable see page 11 improvements implement the three improvements to mergesort that are de scribed in the text on page add a cutoff for small subarrays test whether the array is already in order and avoid the copy by switching arguments in the recursive code 2 12 sublinear extra space develop a merge implementation that reduces the extra space requirement to max m n m based on the following idea divide the array into n m blocks of size m for simplicity in this description assume that n is a multiple of m then i considering the blocks as items with their first key as the sort key sort them using selection sort and ii run through the array merging the first block with the second then the second block with the third and so forth 2 2 13 lower bound for average case prove that the expected number of compares used by any compare based sorting algorithm must be at least n lg n assuming that all possible orderings of the input are equally likely hint the expected number of com pares is at least the external path length of the compare tree the sum of the lengths of the paths from the root to all leaves which is minimized when it is balanced 2 2 14 merging sorted queues develop a static method that takes two queues of sorted items as arguments and returns a queue that results from merging the queues into sorted order 2 2 bottom up queue mergesort develop a bottom up mergesort implementation based on the following approach given n items create n queues each containing one of the items create a queue of the n queues then repeatedly apply the merging opera tion of exercise 2 2 14 to the first two queues and reinsert the merged queue at the end repeat until the queue of queues contains only one queue 2 2 natural mergesort write a version of bottom up mergesort that takes advan tage of order in the array by proceeding as follows each time it needs to find two arrays to merge find a sorted subarray by incrementing a pointer until finding an entry that is smaller than its predecessor in the array then find the next then merge them ana lyze the running time of this algorithm in terms of the array size and the number of creative problems continued maximal increasing sequences in the array 2 2 linked list sort implement a natural mergesort for linked lists this is the method of choice for sorting linked lists because it uses no extra space and is guaranteed to be linearithmic 2 2 shuffling a linked list develop and implement a divide and conquer algo rithm that randomly shuffles a linked list in linearithmic time and logarithmic extra space 2 2 inversions develop and implement a linearithmic algorithm for computing the number of inversions in a given array the number of exchanges that would be performed by insertion sort for that array see section 2 1 this quantity is related to the kendall tau distance see section 2 5 2 2 20 indirect sort develop and implement a version of mergesort that does not re arrange the array but returns an int array perm such that perm i is the index of the i th smallest entry in the array 2 2 triplicates given three lists of n names each devise a linearithmic algorithm to determine if there is any name common to all three lists and if so return the first such name 2 2 22 3 way mergesort suppose instead of dividing in half at each step you divide into thirds sort each third and combine using a 3 way merge what is the order of growth of the overall running time of this algorithm 2 2 23 improvements run empirical studies to evaluate the effectiveness of each of the three improvements to mergesort that are described in the text see exercise 2 2 11 also compare the performance of the merge implementation given in the text with the merge described in exercise 2 2 10 in particular empirically determine the best value of the parameter that decides when to switch to insertion sort for small subarrays 2 2 sort test improvement run empirical studies for large randomly ordered ar rays to study the effectiveness of the modification described in exercise 2 2 8 for ran dom data in particular develop a hypothesis about the average number of times the test whether an array is sorted succeeds as a function of n the original array size for the sort 2 2 multiway mergesort develop a mergesort implementation based on the idea of doing k way merges rather than 2 way merges analyze your algorithm develop a hy pothesis regarding the best value of k and run experiments to validate your hypothesis 2 2 array creation use sortcompare to get a rough idea of the effect on perfor mance on your machine of creating aux in merge rather than in sort 2 2 subarray lengths run mergesort for large random arrays and make an empiri cal determination of the average length of the other subarray when the first subarray exhausts as a function of n the sum of the two subarray sizes for a given merge 2 2 top down versus bottom up use sortcompare to compare top down and bot tom up mergesort for n and 2 2 natural mergesort determine empirically the number of passes needed in a natural mergesort see exercise 2 2 16 for random long keys with n and hint you do not need to implement a sort or even generate full bit keys to complete this exercise the subject of this section is the sorting algorithm that is probably used more widely than any other quicksort quicksort is popular because it is not difficult to implement works well for a variety of different kinds of input data and is substantially faster than any other sorting method in typical applications the quicksort algorithm desirable features are that it is in place uses only a small auxiliary stack and that it requires time proportional to n log n on the average to sort an array of length n none of the algorithms that we have so far considered combine these two properties furthermore quicksort has a shorter inner loop than most other sorting algorithms which means that it is fast in practice as well as in theory its primary drawback is that it is fragile in the sense that some care is involved in the implementation to be sure to avoid bad performance numerous examples of mistakes leading to quadratic perfor mance in practice are documented in the literature fortunately the lessons learned from these mistakes have led to various improvements to the algorithm that make it of even broader utility as we shall see the basic algorithm quicksort is a divide and conquer method for sorting it works by partitioning an array into two subarrays then sorting the subarrays indepen dently quicksort is complementary to mergesort for mergesort we break the array into two subarrays to be sorted and then combine the ordered subarrays to make the whole ordered array for quicksort we rearrange the array such that when the two subarrays are sorted the whole array is ordered in the first instance we do the two recursive calls before working on the whole array in the second instance we do the two recursive calls after working on the whole array for mergesort the array is divided in half for quicksort the position of the partition depends on the contents of the array this code partitions on the item v in a lo the main loop exits when the scan indices i and j cross within the loop we increment i while a i is less than v and decrement j while a j is greater than v then do an exchange to maintain the invariant property that no entries to the left of i are greater than v and no entries to the right of j are smaller than v once the indices meet we complete the partitioning by exchanging a lo with a j thus leaving the partitioning value in a j partitioning trace array contents before and after each exchange partitioning in place if we use an extra array partitioning is easy to implement but not so much easier that it is worth the extra cost of copying the partitioned version back into the original a novice java programmer might even create a new spare array within the recursive method for each partition which would drastically slow down the sort staying in bounds if the smallest item or the largest item in the array is the partition ing item we have to take care that the pointers do not run off the left or right ends of the array respectively our partition implementation has explicit tests to guard against this circumstance the test j lo is redundant since the partitioning item is at a lo and not less than itself with a similar technique on the right it is not dif ficult to eliminate both tests see exercise 2 3 preserving randomness the random shuffle puts the array in random order since it treats all items in the subarrays uniformly algorithm 2 5 has the property that its two subarrays are also in random order this fact is crucial to the predictability of the algo rithm s running time an alternate way to preserve randomness is to choose a random item for partitioning within partition terminating the loop experienced programmers know to take special care to ensure that any loop must always terminate and the partitioning loop for quicksort is no ex ception properly testing whether the pointers have crossed is a bit trickier than it might seem at first glance a common error is to fail to take into account that the array might contain other items with the same key value as the partitioning item handling items with keys equal to the partitioning item s key it is best to stop the left scan for items with keys greater than or equal to the partitioning item s key and the right scan for items with key less than or equal to the partitioning item s key as in algorithm 2 5 even though this policy might seem to create unnecessary exchanges involving items with keys equal to the partitioning item s key it is crucial to avoiding quadratic running time in certain typical applications see exercise 2 3 11 later we discuss a better strategy for the case when the array contains a large number of items with equal keys terminating the recursion experienced programmers also know to take special care to ensure that any recursive method must always terminate and quicksort is again no exception for instance a common mistake in implementing quicksort involves not ensuring that one item is always put into position then falling into an infinite recursive loop when the partitioning item happens to be the largest or smallest item in the array performance characteristics quicksort has been subjected to very thorough mathematical analysis so that we can make precise statements about its performance the analysis has been validated through extensive empirical experience and is a useful tool in tuning the algorithm for optimum performance the inner loop of quicksort in the partitioning method increments an index and compares an array entry against a fixed value this simplicity is one factor that makes quicksort quick it is hard to envision a shorter inner loop in a sorting algorithm for example mergesort and shellshort are typically slower than quicksort because they also do data movement within their inner loops the second factor that makes quicksort quick is that it uses few compares ulti mately the efficiency of the sort depends on how well the partitioning divides the array which in turn depends on the value of the partitioning item s key partitioning divides a large randomly ordered array into two smaller randomly ordered subarrays but the actual split is equally likely for distinct keys to be anywhere in the array next we consider the analysis of the algorithm which allows us to see how this choice compares to the ideal choice the best case for quicksort is when each partitioning stage divides the array exactly in half this circumstance would make the number of compares used by quicksort satisfy the divide and conquer recurrence cn 2 n the 2 term covers the cost of sorting the two subarrays the n is the cost of examining each entry using one partitioning index or the other as in the proof of proposition f for mergesort we know that this recurrence has the solution cn n lg n although things do not always go this well it is true that the partition falls in the middle on the average taking into account the precise probability of each partition position makes the recurrence more complicated and more difficult to solve but the final result is similar the proof of this result is the basis for our confidence in quicksort if you are not mathematically inclined you may wish to skip and trust it if you are mathematically inclined you may find it intriguing warmup interactive proofs with a deterministic verifier let us consider what happens when we introduce interaction into the np scenario that is we d have an interrogation style proof system where rather than the prover send a written proof to the verifier the prover and verifier interact with the verifier asking questions and the prover responding where at the end the verifier decides whether or not to accept the input of course both verifier and prover can keep state during the interaction or equivalently the message a party sends at any warmup interactive proofs with a deterministic verifier point in the interaction can be a function of all messages sent and received so far formally we make the following definition definition interaction of deterministic functions let f g be functions a k round interaction of f and g on input x denoted by f g x is the sequence of the following strings ak defined as follows f x g x f x g x a2i where we consider a suitable encoding of i tuples of strings to strings the output of f resp g at the end of the interaction denoted outf f g x resp outg f g x is defined to be f x ak resp g x ak definition deterministic proof systems we say that a language l has a k round deterministic interactive proof system if there a deter ministic tm v that on input x ai runs in time polynomial in x satisfying completeness x l p outv v p x soundness x l p outv v p x the class dip contains all languages with a k n round deterministic interactive proof systems with k n polynomial in n it turns out this actually does not change the class of languages we can prove theorem dip np proof clearly every np language has a round proof system now we prove that if a l has an interactive proof system of this type then l np the certificate for membership is just the transcript ak causing the verifier to accept to verify this transcript check that indeed v x v x and v x ak if x l then there indeed exists such a transcript if there exists such a transcript ak then we can define a prover function p to satisfy p x p x etc we see that outv v p x and hence x l draft web draft the class ip the class ip in order to realize the full potential of interaction we need to let the verifier be probabilistic the idea is that similar to probabilistic algorithms the verifier will be allowed to come to a wrong conclusion e g accept a proof for a wrong statement with some small probability however as in the case of probabilistic algorithms this probability is over the verifier coins and the verifier will reject proofs for a wrong statement with good probability regardless of the strategy the prover uses it turns out that the combination of interaction and randomization has a huge effect as we will see the set of languages which have interactive proof systems now jumps from np to pspace example as an example for a probabilistic interactive proof system consider the following scenario marla claims to arthur that she can distinguish between the taste of coke coca cola and pepsi to verify this statement marla and arthur repeat the following experiment times marla turns her back to arthur as he places coke in one unmarked cup and pepsi in another choosing randomly whether coke will be in the cup on the left or on the right then marla tastes both cups and states which one contained which drinks while regardless of her tasting abilities marla can answer correctly with probability by a random guess if she manages to answer correctly for all the repetitions arthur can indeed be convinced that she can tell apart pepsi and coke to formally define this we extend the notion of interaction to probabilistic functions actually we only need to do so for the verifier to model an interaction between f and g where f is probabilistic we add an additional m bit input r to the function f in that is having f x r f x r etc the interaction f g x is now a random variable over r r m similarly the output outf f g x is also a random variable definition ip let k n n be some function with k n computable in poly n time a language l is in ip k if there is a turing machine v such that on inputs x r ai v runs in time polynomial in x and such that completeness x l p pr outv v p x soundness x l p pr outv v p x we define ip c nc remark the following observations on the class ip are left as an exercise exercise allowing the prover to be probabilistic i e the answer function ai depends upon some random string used by the prover does not change the class ip the reason is that for any language l if a probabilistic prover p results in making verifier v accept with some probability then averaging implies there is a deterministic prover which makes v accept with the same probability web draft proving that graphs are not isomorphic figure unavailable in pdf file figure two isomorphic graphs since the prover can use an arbitrary function it can in principle use unbounded computa tional power or even compute undecidable functions however one can show that given any verifier v we can compute the optimum prover which given x maximizes the verifier ac ceptance probability using poly x space and hence x time thus ip pspace the probabilities of correctly classifying an input can be made arbitrarily close to by using the same boosting technique we used for bpp see section to replace by exp m sequentially repeat the protocol m times and take the majority answer in fact using a more complicated proof it can be shown that we can decrease the probability without increasing the number of rounds using parallel repetition i e the prover and verifier will run m executions of the protocol in parallel we note that the proof is easier for the case of public coin proofs which will be defined below replacing the constant in the completeness requirement by does not change the class ip this is a nontrivial fact it was originally proved in a complicated way but today can be proved using our characterization of ip later in section in contrast replacing the constant by in the soundness condition is equivalent to having a deterministic verifier and hence reduces the class ip to np we emphasize that the prover functions do not depend upon the verifier random strings but only on the messages questions the verifier sends in other words the verifier random string is private often these are called private coin interactive proofs later we will also consider the model where all the verifier questions are simply obtained by tossing coins and revealing them to the prover this is known as public coins or arthur merlin proofs proving that graphs are not isomorphic we ll now see an example of a language in ip that is not known to be in np recall that the usual ways of representing graphs adjacency lists adjacency matrices involve a numbering of the vertices we say two graphs and are isomorphic if they are the same up to a renumbering of vertices in other words if there is a permutation  of the labels of the nodes of such that  the graphs in figure for example are isomorphic with  that is and are mapped to each other to to to and to if and are isomorphic we write the gi problem is the following given two graphs say in adjacency matrix representation decide if they are isomorphic note that clearly gi np since a certificate is simply the description of the permutation  the graph isomorphism problem is important in a variety of fields and has a rich history see along with the factoring problem it is the most famous np problem that is not known to be draft web draft public coins and am either in p or np complete the results of this section show that gi is unlikely to be np complete unless the polynomial hierarchy collapses this will follow from the existence of the following proof system for the complement of gi the problem gni of deciding whether two given graphs are not isomorphic to see that definition is satisfied by the above protocol note that if then there exists a prover such that pr v accepts because if the graphs are non isomorphic an all powerful prover can certainly tell which one of the two is isomorphic to h on the other hand if the best any prover can do is to randomly guess because a random permutation of looks exactly like a random permutation of thus in this case for every prover pr v accepts this probability can be reduced to by sequential or parallel repetition public coins and am allowing the prover full access to the verifier random string leads to the model of interactive proofs with public coins definition am ma for every k we denote by am k the class of languages that can be decided by a k round interactive proof in which each verifier message consists of sending a random string of polynomial length and these messages comprise of all the coins tossed by the verifier a proof of this form is called a public coin proof it is sometimes also known an arthur merlin proof we define by am the class am that is am is the class of languages with an interactive proof that consist of the verifier sending a random string the prover responding with a message and where the decision to accept is obtained by applying a deterministic polynomial time function to the transcript the class ma denotes the class of languages with round public coins interactive proof with the prover sending the first message that is l ma if there a proof system for l that consists of the prover first sending a message and then the verifier tossing coins and applying a polynomial time predicate to the input the prover message and the coins was a famous king of medieval england and merlin was his court magician babai named these classes by drawing an analogy between the prover infinite power and merlin magic one justification for this model is that while merlin cannot predict the coins that arthur will toss in the future arthur has no way of hiding from merlin magic the results of the coins he tossed in the past that am am while ip ip poly while this is indeed somewhat inconsistent this is the standard notation used in the literature we note that some sources denote the class am by ama the class am by amam etc web draft 21 public coins and am note that clearly for every k am k ip k the interactive proof for gni seemed to crucially depend upon the fact that p cannot see the random bits of v if p knew those bits p would know i and so could trivially always guess correctly thus it may seem that allowing the verifier to keep its coins private adds significant power to interactive proofs and so the following result should be quite surprising theorem for every k n n with k n computable in poly n ip k am k the central idea of the proof of theorem can be gleaned from the proof for the special case of gni theorem gni am k for some constant k the key idea in the proof of theorem is to look at graph nonisomorphism in a different more quantitative way aside this is a good example of how nontrivial interactive proofs can be designed by recasting the problem consider the set s h h or h note that it is easy to prove that a graph h is a member of s by providing the permutation mapping either or to h the size of this set depends on whether is isomorphic to an n vertex graph g has at most n equivalent graphs if and have each exactly n equivalent graphs this will happen if for i there no non identity permutation  such that  gi gi we ll have that if then s if then s n to handle the general case that or may have less than n equivalent graphs we actually change the definition of s to s h  h or h and  aut h where  aut h if  h h it is clearly easy to prove membership in the set s and it can be verified that s satisfies and thus to convince the verifier that the prover has to convince the verifier that case holds rather than this is done by using a set lower bound protocol set lower bound protocol in a set lower bound protocol the prover proves to the verifier that a given set s where membership in s is efficiently verifiable has cardinality at least k up to accuracy of say factor of that is if s k then the prover can cause the verifier to accept with high probability while if s k then the verifier will reject with high probability no matter what the prover does by the observations above such a protocol suffices to complete the proof of theorem draft web draft 21 public coins and am tool pairwise independent hash functions the main tool we use for the set lower bound protocol is a pairwise independent hash function collection this is a simple but incredibly useful tool that has found numerous applications in complexity theory and computer science at large see note note that an equivalent formulation is that for every two distinct strings x xi n the random variable h x h xi for h chosen at random from hn k is distributed according to the uniform distribution on k k recall that we can identify the elements of n with the finite field see section a in the appendix denoted gf containing elements whose addition and multiplication operations satisfy the usual commutative and distributive laws where and every element x has an additive inverse denoted by x and if nonzero a multiplicative inverse denoted by x the following theorem provides a construction of an efficiently computable pairwise independent hash functions see also exercise for a different construction theorem efficient pairwise independent hash functions for every n define the collection hn n to be ha b a b gf where for every a b gf the func tion ha b gf gf maps x to ax b then hn n is a collection of pairwise independent hash functions remark theorem implies the existence of an efficiently computable pairwise independent hash functions n k for every n k if k n we can use the collection k k and reduce the size of the input to n by padding it with zeros if k n then we can use the collection n n and truncate the last n k bits of the output proof for every x xi gf and y yi gf ha b x y and ha b xi yi iff a b satisfy the equations a x b y a xi b yi these imply a x xi y yi or a y yi x xi since b y a x the pair a b is completely determined by these equations and so the probability that this happens over the choice of a b is exactly one over the number of possible pairs which indeed equals web draft 21 public coins and am note the hashing paradigm a hash function collection is a collection of functions mapping a large uni verse say n to a smaller universe say k for k n typically we require of such a collection that it maps its input in a fairly uniform way to the output range for example if s is a subset of n then we wish that if h is chosen at random from the collection then most elements of k have roughly s k preimages in s which is the expected number if h was a completely random function in particular if s has size roughly then we expect the mapping to be one to one or almost one to one and so there should be a relatively small number of collisions pairs x xi s such that h x h xi therefore the image of s under h should look like this n h s k k in databases hash functions are used to maintain very efficient databases that allow fast membership queries to a subset s n of size re quiring only as opposed to bits of storage in theoretical computer science hash functions have a variety of uses an example is lemma of the next chapter that shows that if the collection is pairwise independent and s n has size roughly then with good probability the value will have exactly one preimage in s in all these cases it is important that the hash function is chosen at random from some collection independently of the choice of set s it is easy to see that if k is small enough e g k n then for every h n k there is a set s n of size that is very bad for h in the sense that all the members of s map to the same element under h pairwise independent hash functions are but one example of a hash func tion collection several types of such collections are known in the literature featuring various tradeoffs between efficiency and uniformity of output draft web draft 21 public coins and am the lower bound protocol the lower bound protocol is as follows k let p k if s k then clearly h s and so the verifier will accept with probability p at most the main challenge is to show that if s k then the verifier will accept with probability noticeably larger than p the gap between the probabilities can then be amplified using repetition that is it suffices to prove claim let s m satisfy s then h r pr hm k y r x sh x y s proof for every y m we ll prove the claim by showing that pr h rhm k x sh x y p where p s indeed for every x s define the event ex to hold if h x y then pr x sh x y pr x sex but by the inclusion exclusion principle this is at least x s x x pr ex exi however by pairwise independence if x xi then pr ex k and pr ex exi and so this probability is at least s s s s p web draft 21 public coins and am figure unavailable in pdf file figure am k looks like tlp with the quantifier replaced by probabilitic choice proving theorem the public coin interactive proof system for gni consists of the verifier and prover running several iterations of the set lower bound protocol for the set s as defined above where the verifier accepts iff the fraction of accepting iterations was at least note that both parties can compute p using the chernoff bound theorem a it can be easily seen that a constant number of iteration will suffices to ensure completeness probability at least and soundness error at most remark how does this protocol relate to the private coin protocol of section the set s roughly corresponds to the set of possible messages sent by the verifier in the protocol where the verifier message is a random element in s if the two graphs are isomorphic then the verifier message completely hides its choice of a random i r while if they re not then it completely reveals it at least to a prover that has unbounded computation time thus roughly speaking in the former case the mapping from the verifier coins to the message is to while in the latter case it is to resulting in a set that is twice as large indeed we can view the prover in the public coin protocol as convincing the verifier that its probability of convincing the private coin verifier is large while there are several additional intricacies to handle this is the idea behind the generalization of this proof to show that ip k am k remark note that unlike the private coins protocol the public coins protocol of theorem does not enjoy perfect completeness since the set lowerbound protocol does not satisfy this property however we can construct a perfectly complete public coins set lowerbound protocol see exercise thus implying a perfectly complete public coins proof for gni again this can be generalized to show that any private coins proof system even one not satisfying perfect completeness can be transformed into a perfectly complete public coins system with a similar number of rounds some properties of ip and am we state the following properties of ip and am without proof exercise am bp np where bp np is the class in definition in particular it follows thatam p exercise for constants k we have am k am this collapse is somewhat surprising because am k at first glance seems similar to ph with the quantifiers changed to probabilistic quantifiers where most of the branches lead to acceptance see figure it is open whether there is any nice characterization of am  n where  n is a suitably slow growing function of n such as log log n draft web draft 21 ip pspace can gi be np complete we now prove that if gi is np complete then the polynomial hierarchy collapses theorem if gi is np complete then proof if gi is np complete then gni is conp complete which implies that there exists a function f such that for every n variable formula  y y holds iff f  gni let  x n y n  x y be a formula we have that  is equivalent to x n g x gni where g x f using remark and the comments of section we have that gni has a two round am proof with perfect completeness and after appropriate amplification soundness error less than n let v be the verifier algorithm for this proof system and denote by m the length of the verifier random tape and by mi the length of the prover message and we claim that  is equivalent to  r m x n a m v g x r a indeed by perfect completeness if  is satisfiable then  is satisfiable if  is not satisfiable then by the fact that the soundness error is at most n we have that there exists a single string r m such that for every x with g x gni there no a such that v g x r a and so  is not satisfiable since  can easily be reduced to a formula we get that implying since that ip pspace in this section we show a surprising characterization of the set of languages that have interactive proofs theorem lfkn shamir ip pspace note that this is indeed quite surprising we already saw that interaction alone does not increase the languages we can prove beyond np and we tend to think of randomization as not adding significant power to computation e g we ll see in chapter that under reasonable conjectures bpp p as noted in section we even know that languages with constant round interactive proofs have a two round public coins proof and are in particular contained in the polynomial hierarchy which is believed to be a proper subset of pspace nonetheless it turns out that the combination of sufficient interaction and randomness is quite powerful web draft 21 ip pspace by our earlier remark we need only show the direction pspace ip to do so we ll show that tqbf ip poly n this is sufficient because every l pspace is polytime reducible to tqbf we note that our protocol for tqbf will use public coins and also has the property that if the input is in tqbf then there is a prover which makes the verifier accept with probability rather than tackle the job of designing a protocol for tqbf right away let us first think about how to design one for how can the prover convince the verifier than a given formula has no satisfying assignment we show how to prove something even more general the prover can prove to the verifier what the number of satisfying assignments is in other words we will design a prover for sat the idea of arithmetization introduced in this proof will also prove useful in our protocol for tqbf arithmetization the key idea will be to take an algebraic view of boolean formulae by representing them as polyno mials note that can be thought of both as truth values and as elements of some finite field f thus we have the following correspondence between formulas and polynomials when the variables take values x y x y x x x y x y x y z x y z given any formula  xn with m clauses we can write such a degree polyno mial for each clause multiplying these polynomials we obtain a degree multivariate polynomial p xn that evaluates to for satisfying assignments and evaluates to for unsatis fying assignments note we represent such a polynomial as a multiplication of all the degree polynomials without opening up the parenthesis and so p xn has a representation of size o m this conversion of  to p is called arithmetization once we have written such a polynomial nothing stops us from going ahead and and evaluating the polynomial when the variables take arbitrary values from the field f instead of just as we will see this gives the verifier unexpected power over the prover interactive protocol for satd to design a protocol for we give a protocol for satd which is a decision version of the counting problem sat we saw in chapter satd  k k is the number of satisfying assignments of  and  is a formula of n variables and m clauses theorem satd ip draft web draft 21 ip pspace proof given input  k we construct by arithmetization p the number of satisfying as signments  of  is  p bn to start the prover sends to the verifier a prime p in the interval the verifier can check that p is prime using a probabilistic or deterministic primality testing algorithm all computations described below are done in the field f fp of numbers modulo p note that since the sum in is between and this equation is true over the integers iff it is true modulo p thus from now on we consider as an equation in the field fp we ll prove the theorem by showing a general protocol sumcheck for verifying equations such as sumcheck protocol given a degree d polynomial g xn an integer k and a prime p we present an interactive proof for the claim k g xn where all computations are modulo p to execute the protocol v will need to be able to evaluate the polynomial g for any setting of values to the variables note that this clearly holds in the case g p for each sequence of values bn to x3 xn note that g bn is a univariate degree d polynomial in the variable thus the following is also a univariate degree d polynomial h g bn bn if claim is true then we have h h k consider the following protocol protocol sumcheck protocol to check claim v if n check that g g k if so accept otherwise reject if n ask p to send h as defined above p sends some polynomial if the prover is not cheating then we ll have h v reject if k otherwise pick a random a recursively use the same protocol to check that a b web draft 21 bn g a bn ip pspace if claim is true the prover that always returns the correct polynomial will always convince v if is false then we prove that v rejects with high probability pr v rejects k g n p with our choice of p the right hand side is about dn p which is very close to since d and p assume that is false we prove by induction on n for n v simply evaluates g g and rejects with probability if their sum is not k assume the hypothesis is true for degree d polynomials in n variables in the first round the prover p is supposed to return the polynomial h if it indeed returns h then since h h k by assumption v will immediately reject i e with probability so assume that the prover returns some different from h since the degree d nonzero polynomial h has at most d roots there are at most d values a such that a h a thus when v picks a random a pr a h a d if a h a then the prover is left with an incorrect claim to prove in the recursive step by the induction hypothesis the prover fails to prove this false claim with probability at least d n thus we have pr v rejects this finishes the induction d n p n p protocol for tqbf proof of theorem we use a very similar idea to obtain a protocol for tqbf given a quantified boolean formula  xn xn we use arithmetization to construct the polynomial p we have that  tqbf if and only if fl fl p bn a first thought is that we could use the same protocol as in the satd case except check that k when you have a but alas multiplication unlike addition increases the degree of the polynomial after k steps the degree could be such polynomials may have coefficients and cannot even be transmitted in polynomial time if k log n the solution is to look more closely at the polynomials that are are transmitted and their relation to the original formula we ll change  into a logically equivalent formula whose arithmetization draft web draft 21 the power of the prover does not cause the degrees of the polynomials to be so large the idea is similar to the way circuits are reduced to formulas in the cook levin theorem we ll add auxiliary variables specifically we ll change  to an equivalent formula i that is not in prenex form in the following way work from right to left and whenever encountering a quantifier on a variable xi that is when considering a postfix of the form xi  xi where  may contain quantifiers over additional variables xi xn ensure that the variables xi never appear to the right of another quantifier in  by changing the postfix to xi xii xii xi  xn continuing this way we ll obtain the formula i which will have o variables and will be at most o larger than  it can be seen that the natural arithmetization for i will lead to the polynomials transmitted in the sumcheck protocol never having degree more than note that the prover needs to prove that the arithmetization of i leads to a number k different than but because of the multiplications this number can be as large as nevertheless the prover can find a prime p between and such that k mod p in fact as we saw in chapter a random prime will do this finishes the proof of theorem remark an alternative way to obtain the same result or more accurately an alternative way to describe the same protocol is to notice that for x xk x for all k thus in principle we can convert any polynomial p xn into a multilinear polynomial q xn i e the degree of q in any variable xi is at most one that agrees with p on all xn specifically for any polynomial p let li p be the polynomial defined as follows li p xn xip xi xi xn xi p xi xi xn then ln p is such a multilinear polynomial agreeing with p on all values in we can thus use o invocations operator to convert into an equivalent form where all the intermediate polynomials sent in the sumcheck protocol are multilinear we ll use this equivalent form to run the sumcheck protocol where in addition to having round for a or operator we ll also have a round for each application of the operator l in such rounds the prover will send a polynomial of degree at most the power of the prover a curious feature of many known interactive proof systems is that in order to prove membership in language l the prover needs to do more powerful computation than just deciding membership in l we give some examples the public coin system for graph nonisomorphism in theorem requires the prover to produce for some randomly chosen hash function h and a random element y in the range of h a graph h such that h h is isomorphic to either or and h x y this seems harder than just solving graph non isomorphism web draft 21 162 program checking the interactive proof for a language in conp requires the prover to do p compu tations doing summations of exponentially many terms recall that all of ph is in p p in both cases it is an open problem whether the protocol can be redesigned to use a weaker prover note that the protocol for tqbf is different in that the prover replies can be computed in pspace as well this observation underlies the following result which is in the same spirit as the karp lipton results described in chapter except the conclusion is stronger since ma is contained in indeed a perfectly complete ma proof system for l trivially implies that l theorem if pspace p poly then pspace ma proof if pspace p poly then the prover in our tqbf protocol can be replaced by a circuit of polynomial size merlin the prover can just give this circuit to arthur the verifier in round who then runs the interactive proof using this prover no more interaction is needed note that there is no need for arthur to put blind trust in merlin circuit since the correctness proof of the tqbf protocol shows that if the formula is not true then no prover can make arthur accept with high probability in fact using the karp lipton theorem one can prove a stronger statement see lemma below program checking the discovery of the interactive protocol for the permanent problem was triggered by a field called program checking blum and kannan motivation for introducing this field was the fact that program verification deciding whether or not a given program solves a certain computational task is undecidable they observed that in many cases we can guarantee a weaker guarantee of the program correctness on an instance by instance basis this is encapsulated in the notion of a program checker a checker c for a program p is itself another program that may run p as a subroutine whenever p is run on an input x c job is to detect if p answer is incorrect buggy on that particular instance x to do this the checker may also compute p answer on some other inputs program checking is sometimes also called instance checking perhaps a more accurate name since the fact that the checker did not detect a bug does not mean that p is a correct program in general but only that p answer on x is correct definition 21 let p be a claimed program for computational task t a checker for t is a probabilistic polynomial time tm c that given any x has the following behavior if p is a correct program for t i e y p y t y then p cp accepts p x if p x t x then p cp accepts p x draft web draft 21 program checking note that in the case that p is correct on x i e p x c x but the program p is not correct everywhere there is no guarantee on the output of the checker surprisingly for many problems checking seems easier than actually computing the problem blum and kannan suggestion was to build checkers into the software whenever this is true the overhead introduced by the checker would be negligible example checker for graph non isomorphism the input for the problem of graph non isomorphism is a pair of labelled graphs and the problem is to decide whether as noted we do not know of an efficient algorithm for this problem but it has an efficient checker there are two types of inputs depending upon whether or not the program claims if it claims that then one can change the graph little by little and use the program to actually obtain the permutation  we now show how to check the claim that using our earlier interactive proof of graph non isomorphism recall the ip for graph non isomorphism in case prover admits repeat k times choose i r permute gi randomly into h ask the prover h h and check to see if the prover first answer is consistent given a computer program that supposedly computes graph isomorphism p how would we check its correctness the program checking approach suggests to use an ip while regarding the program as the prover let c be a program that performs the above protocol with p as the prover then theorem if p is a correct program for graph non isomorphism then c outputs correct always otherwise if p is incorrect then p c outputs correct k moreover c runs in polynomial time languages that have checkers whenever a language l has an interactive proof system where the prover can be implemented using oracle access to l this implies that l has a checker thus the following theorem is a direct consequence of the interactive proofs we have seen theorem the problems graph isomorphism gi permanent perm and true quantified boolean formulae tqbf have checkers using the fact that p complete languages are reducible to each other via nc reductions it suffices to show a checker in nc for one p complete language as was shown by blum kannan to obtain the following interesting fact web draft 21 164 multiprover interactive proofs mip theorem for any p complete language there exists a program checker in nc since we believe that p complete languages cannot be computed in nc this provides additional evidence that checking is easier than actual computation multiprover interactive proofs mip it is also possible to define interactive proofs that involve more than one prover the important assumption is that the provers do not communicate with each other during the protocol they may communicate before the protocol starts and in particular agree upon a shared strategy for answering questions the analogy often given is that of the police interrogating two suspects in separate rooms the suspects may be accomplices who have decided upon a common story to tell the police but since they are interrogated separately they may inadvertently reveal an inconsistency in the story the set of languages with multiprover interactive provers is call mip the formal definition is analogous to definition we assume there are two provers though one can also study the case of polynomially many provers see the exercises and in each round the verifier sends a query to each of them the two queries need not be the same each prover sends a response in each round clearly ip mip since we can always simply ignore one prover however it turns out that mip is probably strictly larger than ip unless pspace nexp that is we have theorem nexp mip we will outline a proof of this theorem in chapter one thing that we can do using two rounds is to force non adaptivity that is consider the interactive proof as an interrogation where the verifier asks questions and gets back answers from the prover if the verifier wants to ensure that the answer of a prover to the question q is a function only of q and does not depend on the previous questions the prover heard the prover can ask the second prover the question q and accept only if both answers agree with one another this technique was used to show that multi prover interactive proofs can be used to implement and in fact are equivalent to a model of a probabilistically checkable proof in the sky in this model we go back to an np like notion of a proof as a static string but this string may be huge and so is best thought of as a huge table consisting of the prover answers to all the possible verifier questions the verifier checks the proof by looking at only a few entries in this table that are chosen randomly from some distribution if we let the class pcp r q be the set of languages that can be proven using a table of size and q queries to this table then theorem can be restated as theorem theorem restated nexp pcp poly poly cpcp nc nc it turns out theorem can be scaled down to to obtain np pcp polylog polylog in fact with a lot of work the following is known draft web draft 21 multiprover interactive proofs mip theorem the pcp theorem alm np pcp o log n o this theorem which will be proven in chapter has had many applications in complexity and in particular establishing that for many np complete optimization problems obtaining an approximately optimal solution is as hard as coming up with the optimal solution itself thus it seems that complexity theory has gone a full circle with interactive proofs by adding interaction randomization and multiple provers and getting to classes as high as nexp we have gained new and fundamental insights on the class np the represents static deterministic proofs or equivalently efficiently verifiable search problems chapter notes and history interactive proofs were defined in by goldwasser micali rackoff for cryptographic applications and independently and using the public coin definition by babai and moran the private coins interactive proof for graph non isomorphism was given by goldreich micali and wigderson simulations of private coins by public coins we given by goldwasser and sipser the general feeling at the time was that interactive proofs are only a slight extension of np and that not even has interactive proofs the result ip pspace was a big surprise and the story of its discovery is very interesting in the late blum and kannan introduced the notion of program checking around the same time manuscripts of beaver and feigenbaum and lipton appeared in spired by some of these developments nisan proved in december that sat has multiprover interactive proofs he announced his proof in an email to several colleagues and then left on va cation to south america this email motivated a flurry of activity in research groups around the world lund fortnow karloff showed that sat is in ip they added nisan as a coauthor and the final paper is then shamir showed that ip pspace and babai fortnow and lund showed mip nexp the entire story as well as related developments are described in babai entertaining survey vadhan explores some questions related to the power of the prover web draft 21 166 multiprover interactive proofs mip the result that approximating the shortest vector is probably not np hard as mentioned in the introduction is due to goldreich and goldwasser exercises prove the assertions in remark that is prove a let ipi denote the class obtained by allowing the prover to be probabilistic in defini tion that is the prover strategy can be chosen at random from some distribution on functions prove that ipi ip b prove that ip pspace c let ipi denote the class obtained by changing the constant in and to x prove that ipi ip d let ipi denote the class obtained by changing the constant in to prove that ipi ip e let ipi denote the class obtained by changing the constant in to prove that ipi np we say integer y is a quadratic residue modulo m if there is an integer x such that y mod m show that the following language is in ip qnr y m y is not a quadratic residue modulo m prove that there exists a perfectly complete am o protocol for the proving a lowerbound on set size prove that for every constant k am k am k show that am bp np show that if exp p poly then exp ma draft web draft 21 a interactive proof for the permanent 21 show that the problem gi is downward self reducible that is prove that given two graphs on n vertices and access to a subroutine p that solves the gi problem on graphs with up to n vertices we can decide whether or not and are isomorphic in polynomial time prove that in the case that and are isomorphic we can obtain the permutation  mapping to using the procedure of the above exercise use this to complete the proof in example and show that graph isomorphism has a checker specifically you have to show that if the program claims that then we can do some further investigation including calling the programs on other inputs and with high probability conclude that either a conclude that the program was right on this input or b the program is wrong on some input and hence is not a correct program for graph isomorphism define a language l to be downward self reducible there a polynomial time algorithm r that for any n and x n rln x l x where by lk we denote an oracle that solves l on inputs of size at most k prove that if l is downward self reducible than l pspace show that mip nexp show that if we redefine multiprover interactive proofs to allow instead of two provers as many as m n poly n provers on inputs of size n then the class mip is unchanged a interactive proof for the permanent the permanent is defined as follows definition let a f n n be a matrix over the field f the permanent of a is n perm a ai  i  sn i the problem of calculating the permanent is clearly in pspace in chapter we will see that if the permanent can be computed in polynomial time then p np and hence this problem likely does not have a polynomial time algorithm although the existence of an interactive proof for the permanent follows from that for sat and tqbf we describe a specialized protocol as well this is both for historical context this protocol was discovered before the other two protocols and also because this protocol may be helpful for further research one example will appear in a later chapter web draft 21 168 a interactive proof for the permanent we use the following observation n f xn perm n is a degree n polynomial since xn xn xn n n f xn xi  i  sn i we now show two properties of the permanent problem the first is random self reducibility earlier encountered in section theorem lipton there is a randomized algorithm that given an oracle that can compute the permanent on fraction of the inputs in f n n where the finite field f has size can compute the permanent on all inputs correctly with high probability proof let a be some input matrix pick a random matrix r r f n n and let b x a x r for a variable x notice that f x perm b is a degree n univariate polynomial for any fixed b b b is a random matrix hence the probability that oracle computes perm b b correctly is at least now the algorithm for computing the permanent of a is straightforward query oracle on all matrices b i i n according to the union bound with probability of at least n the oracle will compute the permanent correctly on all matrices recall the fact see section in appendix a that given n point value pairs ai bi i n there exists a unique a degree n polynomial p that satisfies i p ai bi therefore given that the values b i are correct the algorithm can interpolate the polynomial b x and compute b a note the above theorem can be strengthened to be based on the assumption that the oracle can compute the permanent on a fraction of  for any constant  of the inputs the observation is that not all values of the polynomial must be correct for unique interpolation see chapter another property of the permanent problem is downward self reducibility encountered earlier in context of sat n perm a i i where i is a n n sub matrix of a obtained by removing the st row and i th column of a recall the analogous formula for the determinant uses alternating signs draft web draft 21 a interactive proof for the permanent 23 definition define a n n matrix da x such that each entry contains a degree n polynomial this polynomial is uniquely defined by the values of the matrices i i n that is i n da i i where da i is the matrix da x with i substituted for x notice that these equalities force n points and values on them for each polynomial at a certain entry of da x and hence according to the previously mentioned fact determine this polynomial uniquely observation perm da x is a degree n n polynomial in x a the protocol we now show an interactive proof for the permanent the decision problem is whether perm a k for some value k round prover sends to verifier a polynomial g x of degree n n which is supposedly perm da x round verifier checks whether m k ig i i if not rejects at once otherwise verifier picks a random element of the field r f and asks the prover to prove that g perm da this reduces the matrix dimension to n n round n prover sends to verifier a polynomial of degree which is supposedly the permanent of a matrix round n verifier is left with a matrix and calculates the permanent of this matrix and decides appropriately claim 32 the above protocol is indeed an interactive proof for perm proof if perm a k then there exists a prover that makes the verifier accept with probability this prover just returns the correct values of the polynomials according to definition on the other hand suppose that perm a k if on the first round the polynomial g x sent is the correct polynomial da x then k ig i perm a web draft 21 24 a interactive proof for the permanent and the verifier would reject hence g x da x according to the fact on polynomials stated above these polynomials can agree on at most n n points hence the probability that they would agree on the randomly chosen point is at most n n the same considerations apply to all subsequent rounds if exist and the overall probability that the verifier will not accepts is thus assuming f and sufficiently large n p r n n n n f n n f f f draft web draft 21 chapter complexity of counting it is an empirical fact that for many combinatorial problems the detection of the existence of a solution is easy yet no computationally efficient method is known for counting their number for a variety of problems this phenomenon can be explained l valiant the class np captures the difficulty of finding certificates however in many contexts one is interested not just in a single certificate but actually counting the number of certificates this chapter studies p pronounced sharp p a complexity class that captures this notion counting problems arise in diverse fields often in situations having to do with estimations of probability examples include statistical estimation statistical physics network design and more counting problems are also studied in a field of mathematics called enumerative combinatorics which tries to obtain closed form mathematical expressions for counting problems to give an example in the century kirchoff showed how to count the number of spanning trees in a graph using a simple determinant computation results in this chapter will show that for many natural counting problems such efficiently computable expressions are unlikely to exist here is an example that suggests how counting problems can arise in estimations of probability example in the graphreliability problem we are given a directed graph on n nodes suppose we are told that each node can fail with probability and want to compute the probability that node has a path to n a moment thought shows that under this simple edge failure model the remaining graph is uniformly chosen at random from all subgraphs of the original graph thus the correct answer is number of subgraphs in which node has a path to n we can view this as a counting version of the path problem draft the class p in the rest of the chapter we study the complexity class p a class containing the graphreliability problem and many other interesting counting problems we will show that it has a natural and important complete problem namely the problem of computing the permanent of a given matrix we also show a surprising connection between ph and p called toda theorem along the way we encounter related complexity classes such as pp and p the class p we now define the class p note that it contains functions whose output is a natural number and not just remark as in the case of np we can also define p using non deterministic tms that is p consists of all functions f such that f x is equal to the number of paths from the initial configuration to an accepting configuration in the configuration graph gm x of a polynomial time ndtm m the big open question regarding p is whether all problems in this class are efficiently solvable in other words whether p fp recall that fp is the analog of the class p for functions with more than one bit of output that is fp is the set of functions from to computable by a deterministic polynomial time turing machine thinking of the output as the binary representation of an integer we can identify such functions with functions from to n since computing the number of certificates is at least as hard as finding out whether a certificate exists if p fp then np p we do not know whether the other direction also holds whether np p implies that p fp we do know that if pspace p then p fp since counting the number of certificates can be done in polynomial space here are two more examples for problems in p sat is the problem of computing given a boolean formula  the number of satisfying assignments for  cycle is the problem of computing given a directed graph g the number of simple cycles in g a simple cycle is one that does not visit any vertex twice clearly if sat fp then sat p and so p np thus presumably sat fp how about cycle the corresponding decision problem given a directed graph decide if it has a draft web draft 21 the class p cycle can be solved in linear time by breadth first search the next theorem suggests that the counting problem may be much harder u v m figure reducing ham to cycle by replacing every edge in g with the above gadget to obtain g every simple cycle of length in g becomes simple cycles in g theorem if cycle fp then p np proof we show that if cycle can be computed in polynomial time then ham p where ham is the np complete problem of deciding whether or not a given digraph has a hamiltonian cycle i e a simple cycle that visits all the vertices in the graph given a graph g with n vertices we construct a graph gi such that g has a hamiltonian cycle iff gi has at least cycles to obtain gi replace each edge u v in g by the gadget shown in figure the gadget has m n log n levels it is an acyclic digraph so cycles in gi correspond to cycles in g furthermore there are directed paths from u to v in the gadget so a simple cycle of length in g yields c simple cycles in gi notice if g has a hamiltonian cycle then gi has at least n cycles if g has no hamiltonian cycle then the longest cycle in g has length at most n the number of cycles is bounded above by nn so gi can have at most n nn nn cycles the class pp decision problem analog for p similar to the case of search problems even when studying counting complexity we can often restrict our attention to decision problems the reason is that there exists a class of decision problems pp such that pp p p fp intuitively pp corresponds to computing the most significant bit of functions in p that is l is in pp if there exists a polynomial time tm m and a polynomial p n n such that for every x x l p x m x y x you are asked to prove the non trivial direction of in exercise it is instructive to compare the class pp which we believe contains problem requiring exponential time to solve with the class bpp which although it has a seemingly similar definition can in fact be solved efficiently using probabilistic algorithms and perhaps even also using deterministic algorithms see chapter note that we do not know whether this holds also for the class of decision problems corresponding to the least significant bit of p namely p see definition below web draft 21 p completeness p completeness now we define p completeness loosely speaking a function f is p complete if it is in p and a polynomial time algorithm for f implies that p fp to formally define p completeness we use the notion of oracle tms as defined in section recall that a tm m has oracle access to a language o if it can make queries of the form is q o in one computational step we generalize this to non boolean functions by saying that m has oracle access to a function f if it is given access to the language o x i f x i we use the same notation for functions mapping to n identifying numbers with their binary representation as strings for a function f we define fpf to be the set of functions that are computable by polynomial time tms that have access to an oracle for f definition a function f is p complete if it is in p and every g p is in fpf if f fp then fpf fp thus the following is immediate proposition if f is p complete and f fp then fp p counting versions of many np complete languages such as ham and clique naturally lead to p complete problems we demonstrate this with sat theorem sat is p complete proof consider the cook levin reduction from any l in np to sat we saw in section this is a polynomial time computable function f such that for every x x l f x sat however the proof that the reduction works actually gave us more information than that it provided a levin reduction by which we mean the proof showed a way to transform a certificate that x is in l into a certificate i e satisfying assignment showing that f x sat and also vice versa transforming a satisfying assignment for f x into a witness that x l in particular it means that the mapping from the certificates of x to the assignments of f x was invertible and hence one to one thus the number of satisfying assignments for f x is equal to the number of certificates for x as shown below there are p complete problems for which the corresponding decision problems are in fact in p permanent and valiant theorem now we study another problem the permanent of an n n matrix a is defined as perm a fl ai  i draft  sn i web draft 21 p completeness where sn denotes the set of all permutations of n elements recall that the expression for the determinant is similar det a sgn  fl ai i except for an additional sign term this similarity does not translate into computational equiv alence the determinant can be computed in polynomial time whereas computing the permanent seems much harder as we see below the permanent function can also be interpreted combinatorially first suppose the matrix a has each entry in it may be viewed as the adjacency matrix of a bipartite graph g x y e with x xn y yn and xi yj e iff ai j then the term tln ai i is iff  is a perfect matching which is a set of n edges such that every node is in exactly one edge thus if a is a matrix then perm a is simply the number of perfect matchings in the corresponding g and in r perm a is p if a is a matrix sat oracle to compute perm a in fact one can show for general integer matrices that computing the permanent is in fp sat see exercise the next theorem came as a surprise to researchers in the since it implies that if perm fp then p np thus unless p np computing the permanent is much more difficult then computing the determinant before proving theorem we introduce yet another way to look at the permanent consider matrix a as the the adjacency matrix of a weighted n node digraph with possible self loops then the expression tln ai  i is nonzero iff  is a cycle cover of a a cycle cover is a subgraph in which each node has in degree and out degree such a subgraph must be composed of cycles we define the weight of the cycle cover to be the product of the weights of the edges in it thus perm a is equal to the sum of weights of all possible cycle covers example consider the graph in figure even without knowing what the subgraph gi is we show that the permanent of the whole graph is for each cycle cover in gi of weight w there are exactly two cycle covers for the three nodes one with weight w and one with weight w any non zero weight cycle cover of the whole graph is composed of a cycle cover for gi and one of these two cycle covers thus the sum of the weights of all cycle covers of g is is known that every permutation  sn can be represented as a composition of transpositions where a transposition is a permutation that only switches between two elements in n and leaves the other elements intact one proof for this statement is the bubblesort algorithm if m is a sequence of transpositions such that their composition equals  then the sign of  is equal to if m is even and if m is odd it can be shown that the sign is well defined in the sense that it does not depend on the representation of  as a composition of transpositions web draft 21 p completeness figure the above graph g has cycle cover weight zero regardless of the choice of g since for every cycle cover of weight w in g there exist two covers of weight w and w in the graph g unmarked edges have weight we follow this convention through out this chapter proof of valiant theorem theorem we reduce the p complete problem to perm given a boolean formula  with n variables and m clauses first we shall show how to construct an integer matrix ai with negative entries such that perm ai   stands for the number of satisfying assignments of  later we shall show how to to get a matrix a from ai such that knowing perm a allows us to compute perm ai the main idea is that our construction will result in two kinds of cycle covers in the digraph gi associated with ai those that correspond to satisfying assignments we will make this precise and those that don t we will use negative weights to ensure that the contribution of the cycle covers that do not correspond to satisfying assignments cancels out this is similar reasoning to the one used in example on the other hand we will show that each satisfying assignment contributes to perm ai and so perm ai  to construct gi from  we combine the following three kinds of gadgets shown in figure variable gadget the variable gadget has two possible cycle covers corresponding to an assign ment of or to that variable assigning corresponds to a single cycle taking all the external edges true edges and assigning correspond to taking all the self loops and taking the false edge each external edge of a variable is associated with a clause in which the variable appears clause gadget the clause gadget is such that the only possible cycle covers exclude at least one external edge also for a given proper subset of external edges used there is a unique cycle cover of weight each external edge is associated with a variable appearing in the clause xor gadget we also use a graph called the xor gadget whose purpose is to ensure that for some pair of edges u ui and v vi exactly one of these edges is present in any cycle cover that counts towards the final sum suppose that we replace a pair of edges u ui and v vi in some graph g with the xor gadget as described in figure count fig valiantgad to obtain some graph gi then via similar reasoning to example every cycle cover of g of weight w that uses exactly one of the edges u ui and draft web draft 21 p completeness gadget symbolic description variable gadget false edge external true edges one per clause variable gadget external edges clause gadget external edges one per variable external edges clause gadget xor gadget u u u u v v v v the overall construction variable gadget for every variable clause gadget for every clause connect via xor external edges of gadgets for variables that appear in clauses figure the gadgets used in the proof of valiant theorem web draft 21 178 p completeness v vi is mapped to a set of cycle covers in gi whose total weight is i e the set of covers that enter the gadget at u and exit at ui or enter it at v and exit it at vi while all the other cycle covers of gi have total weight exercise for this reason whenever we replace edges u ui and v vi with a xor gadget we can consider in the analysis only cycle covers that use exactly one of these edges as the other covers do not contribute anything to the total sum the xor gadgets are used to connect the variable gadgets to the corresponding clause gadgets so that only cycle covers corresponding to a satisfying assignment will be counted towards the total number of cycle covers consider a clause and a variable appearing in it each has an external edge corresponding to the other connected by an xor gadget if the external edge in the clause is not taken then by the analysis of the xor gadget the external edge in the variable must be taken and hence the variable is true since at least one external edge of each clause gadget has to be omitted each cycle cover that is counted towards the sum corresponds to a satisfying assignment conversely for each satisfying assignment there is a a set of cycle covers with total weight since they passes through the xor gadget exactly times so perm gi  reducing to the case matrices finally we have to reduce finding perm gi to finding perm g where g is an unweighted graph or equivalently its adjacency matrix has only entries we start by reducing to the case that all edges have weights in first note that replacing an edge of weight k by k parallel edges of weight does not change the permanent parallel edges are not allowed but we can make edges non parallel by cutting each edge u v in two and inserting a new node w with an edge from u to w w to v and a self loop at w to get rid of the negative weights note that the permanent of an n vertex graph with edge weights in is a number x in n n and hence this permanent can be computed from y x mod where m is sufficiently large e g m will do but to compute y it is enough to compute the permanent of the graph where all weight edges are replaced with edges of weight such edges can be converted to m edges of weight in series which again can be transformed to parallel edges of weight as above approximate solutions to p problems since computing exact solutions to p complete problems is presumably difficult a natural ques tion is whether we can approximate the number of certificates in the sense of the following definition definition let f n and  an algorithm a is an  approximation for f if for every x f x a x f x  not all p problems behave identically with respect to this notion approximating certain problems within any constant factor  is np hard see exercise for other problems such as permanent there is a fully polynomial randomized approximation scheme fpras which is an algorithm which for any e  approximates the function within a factor e its answer may be incorrect with probability  in time poly n log  log e such approximation of counting problems is sufficient for many applications in particular those where counting is needed to obtain draft web draft 21 toda s theorem ph p sat estimates for the probabilities of certain events e g see our discussion of the graph reliability problem the approximation algorithm for the permanent as well as other similar algorithms for a host of p complete problems use the monte carlo markov chain technique the result that spurred this development is due to valiant and vazirani and it shows that under fairly general conditions approximately counting the number of elements in a set membership in which is testable in polynomial time is equivalent in the sense that the problems are interreducible via polynomial time randomized reductions to the problem of generating a random sample from the set we will not discuss this interesting area any further interestingly if p np then every p problem has an fpras and in fact an fptas i e a deterministic polynomial time approximation scheme see exercise toda theorem ph p sat an important question in the was the relative power of the polynomial hierarchy ph and the class of counting problems p both are natural generalizations of np but it seemed that their features alternation and the ability to count certificates respectively are not directly comparable to each other thus it came as big surprise when in toda showed that is we can solve any problem in the polynomial hierarchy given an oracle to a p complete problem remark note that we already know even without toda theorem that if p fp then np p and so ph p however this does not imply that any problem in ph can be computed in polynomial time using an oracle to sat for example one implication of toda theorem is that a subexponential i e time algorithm for sat will imply such an algorithm for any problem in ph such an implication is not known to hold from a time algorithm for sat the class p and hardness of satisfiability with unique solutions the following complexity class will be used in the proof definition a language l in the class p pronounced parity p iff there exists a polynomial time ntm m such that x l iff the number of accepting paths of m on input x is odd thus p can be considered as the class of decision problems corresponding to the least sig nificant bit of a p problem as in the proof of theorem the fact that the standard np completeness reduction is parsimonious implies the following problem sat is p complete under many to one karp reductions web draft 21 180 toda s theorem ph p sat definition define the quantifier as follows for every boolean formula  on n variables x n  x is true if the number of such that is true is odd the language consists of all the true quantified boolean formula of the form x n  x where  is an unquantified boolean formula not necessarily in cnf form unlike the class p it is not known that a polynomial time algorithm for p implies that np p however such an algorithm does imply that np rp since np can be probabilistically reduced to sat to prove theorem we use the following lemma on pairwise independent hash functions lemma valiant vazirani lemma let hn k be a pairwise independent hash function collection from n to k and s n such that s then pr h rhn k s h x proof for every x s let p k be the probability that h x when h r hn k note that for every x xi pr h x h xi let n be the random variable denoting the number of x s satisfying h x note that e n s p by the inclusion exclusion principle pr n pr h x pr h x h xi s p s x s x x s and by the union bound we get that pr n s thus pr n pr n pr n s p s s p s where the last inequality is obtained using the fact that s p that if we identify true with and with false then n  x n  x mod also note that x n  x xn  xn x x web draft 21 toda s theorem ph p sat proof of theorem we now use lemma to prove theorem given a formula  on n variables our probabilistic algorithm a chooses k at random from n and a random hash function h r n k it then uses the cook levin reduction to compute a formula  on variables x n y m for m poly n such that h x if and only if there exists a unique y such that  x y the output of a if the formula  x n y m  x  x y it is equivalent to the statement x n  x h x if  is unsatisfiable then  is false since we ll have no x satisfying the inner formula and zero is an even number if  is satisfiable we let s be the set of its satisfying assignments with probability n k satisfies s conditioned on which with probability there is a unique x such that  x h x since one happens to be an odd number this implies that  is true remark hardness of unique satisfiability the proof of theorem implies the following stronger statement the existence of an algorithm to distinguish between an unsatisfiable boolean formula and a formula with exactly one satisfying assignment implies the existence of a probabilistic polynomial time algorithm for all of np thus the guarantee that a particular search problem has either no solutions or a unique solution does not necessarily make the problem easier to solve step randomized reduction from ph to p we now go beyond np that is to say the valiant vazirani theorem and show that we can actually reduce any language in the polynomial hierarchy to sat lemma let c n be some constant there exists a probabilistic polynomial time algorithm a such that for every  a quantified boolean formula with c levels of alternations  is true pr a  sat  is false pr a  sat before proving the lemma let us make a few notations and observations for a boolean formula  on n variables let  denote the number of satisfying assignments of  we consider also formulae  that are partially quantified that is in addition to the n variables  takes as input some implementations of hash functions such as the one described in exercise one can construct directly without going through the cook levin reduction such a formula  that does not use the y variables web draft 21 182 toda s theorem ph p sat it may also have other variables that are bound by a or quantifiers for example  can be of the form  xn y n  xn y where  is say a boolean formula given two possibly partially quantified formulae   on variables x n y m we can construct in polynomial time an n m variable formula   and a max n m variable formula   such that     and     indeed take   x y  x  y   z  zn  zm for a formula  we use the notation  to denote the formula   where  is some canonical formula with a single satisfying assignment since the product of numbers is even iff one of the numbers is even and since adding one to a number flips the parity for every two formulae   as above  x  y   x y  x  x z  x  y   x y z proof of lemma recall that membership in a ph language can be reduced to deciding the truth of a quantified boolean formula with a constant number of alternating quantifiers the idea behind the proof is to replace one by one each quantifiers with a quantifier let  be a formula with c levels of alternating quantifiers possibly with an initial quantifier we transform  in probabilistic polynomial time to a formula i such that i has only c levels of alternating quantifiers an initial quantifier satisfying if  is false then so is i and if  is true then with probability at least i is true as well the lemma follows by repeating this step c times for ease of notation we demonstrate the proof for the case that  has a single quantifier and two additional quantifiers we can assume without loss of generality that  is of the form  z x n w k  z x w as otherwise we can use the identities xp x x p x and to transform  into this form the proof of theorem provides for every n a probabilistic algorithm that outputs a for mula  on variables x n and y m such that for every nonempty set s n pr x n y m  x y run this algorithm t log n times to obtain the for mulae t then for every nonempty set s n the probability that there does not exist i t such that x n y m  x y is true is less than c we claim that this implies that with probability at least the following formula is equivalent to  where z  z  z t x n y m w k i x y  x z w web draft 21 toda s theorem ph p sat indeed for every z c define sz jx n k  x z w l then  is equivalent w to z sz is nonempty but by the union bound with probability at least it holds that for every z such that sz is nonempty there exists i satisfying x yi x y this means that for every such z  z is true on the other hand if sz is empty then certainly  z is false implying that indeed  is equivalent to by applying the identity we can transform into an equivalent formula of the desired form z x y w wi x y z w for some unquantified polynomial size formula i step making the reduction deterministic to complete the proof of toda theorem theorem we prove the following lemma lemma there is a deterministic polynomial time transformation t that for every formula  that is an input for sat t  is an unquantified boolean formula and  sat  mod  sat  mod proof of theorem using lemmas and let l p h we show that we can decide whether an input x l by asking a single question to a sat oracle for every x n lemmas and 19 together imply there exists a polynomial time tm m such that x l pr r m x r mod x l r r m m x r mod m where m is the polynomial in n number of random bits used by the procedure described in that lemma furthermore even in the case x l we are guaranteed that for every r m m x r mod consider the function that maps two strings r u into the evaluation of the formula m x r on the assignment u since this function is computable in polynomial time the cook levin transformation implies that we can obtain in polynomial time a cnf formula x on variables r u y such that for every r u m x r is satisfied by u if and only if there exist a unique y such that x r u y is true let fx r be the number of u y such that x r u y is true then x r m fx r but if x l then fx r mod for every r and hence x mod on the other hand if x l then fx r mod for between and values of r and is web draft 21 184 open problems equal to on the other values and hence x mod we see that deciding whether x l can be done by computing x proof of lemma 19 for every pair of formulae   recall that we defined formulas   and   satisfying     and     and note that these formulae are of size at most a constant factor larger than   consider the formula where  for example is shorthand for    one can easily check that  mod 3 mod  mod 3 mod let  and i let  ilog m l repeated use of equations shows that if  is odd then  mod and if  is even then  mod also the size of  is only polynomially larger than size of  open problems what is the exact power of sat and sat what is the average case complexity of n n permanent modulo small prime say or note that for a prime p n random self reducibility of permanent implies that if permanent is hard to compute on at least one input then it is hard to compute on o p n fraction of inputs i e hard to compute on average see theorem draft web draft 21 open problems chapter notes and history the definition of p as well as several interesting examples of p problems appeared in valiant seminal paper the p completeness of the permanent is from his other paper toda theorem is proved in the proof given here follows the proof of although we use formulas where they used circuits for an introduction to fpras for computing approximations to many counting problems see the relevant chapter in vazirani an excellent resource on approximation algorithms in general exercises let f p show a polynomial time algorithm to compute f given access to an oracle for some language l pp see remark show that computing the permanent for matrices with integer entries is in fp sat complete the analysis of the xor gadget in the proof of theorem let g be any weighted graph containing a pair of edges u ui and v vi and let gi be the graph obtained by replacing these edges with the xor gadget prove that every cycle cover of g of weight w that uses exactly one of the edges u ui is mapped to a set of cycle covers in gi whose total weight is and all the other cycle covers of gi have total weight let k n prove that the following family hn k is a collection of pairwise independent functions from n to k identify with the field gf for every k n matrix a with entries in gf and k length vector b gf n hn k contains the function ha b gf n gf k defined as follows ha b x ax b show that if there is a polynomial time algorithm that approximates cycle within a factor then p np show that if np p then for every f p and there is a polynomial time algorithm that approximates f within a factor of can you show the same for a factor of e for arbitrarily small constant e can you make these algorithms deterministic web draft 21 186 open problems note that we do not know whether p np implies that exact computation of functions in p can be done in polynomial time show that every for every language in there is a depth circuit of npoly log n size that decides it on poly n fraction of inputs and looks as follows it has a single gate at the top and the other gates are of fanin at most poly log n draft web draft 21 chapter cryptography from times immemorial humanity has gotten frequent often cruel reminders that many things are easier to do than to reverse l levin lev somewhat rough still the importance of cryptography in today online world needs no introduction here we focus on the complexity issues that underlie this field the traditional task of cryptography was to allow two parties to encrypt their messages so that eavesdroppers gain no information about the message see figure various encryption techniques have been invented throughout history with one common characteristic sooner or later they were broken figure unavailable in pdf file figure people sending messages over a public channel e g the internet wish to use encryption so that eavesdroppers learn nothing in the post np completeness era a crucial new idea was presented the code breaker should be thought of as a resource bounded computational device hence the security of encryption schemes ought to be proved by reducing the task of breaking the scheme into the task of solving some computationally intractable problem say requiring exponential time complexity or circuit size thus one could hope to design encryption schemes that are efficient enough to be used in practice but whose breaking will require say millions of years of computation time early researchers tried to base the security of encyption methods upon the presumed in tractability of np complete problems this effort has not succeeded to date seemingly because np completeness concerns the intractability of problems in the worst case whereas cryptography seems to need problems that are intractable on most instances after all when we encrypt email we require that decryption should be difficult for an eavesdropper for all or almost all messages not just for a few messages thus the concept most useful in this chapter will be average case we will see a class of functions called one way functions that are easy to compute problem average case and worst case complexities can differ radically for instance is np complete hard on average problems and one way functions but hard to invert for most inputs they are alluded to in levin quote above such functions exist under a variety of assumptions including the famous assumption that factoring integers re quires time super polynomial time in the integer bit length to solve in the average case e g for a product of two random primes furthermore in the past two decades cryptographers have taken on tasks above and beyond the basic task of encryption from implementing digital cash to maintaining the privacy of individuals in public databases we survey some applications in section surprisingly many of these tasks can be achieved using the same computational assumptions used for encryption a crucial ingredient in these developments turns out to be an answer to the question what is a random string and how can we generate one the complexity theoretic answer to this question leads to the notion of a pseudorandom generator which is a central object see section this notion is very useful in itself and is also a template for several other key definitions in cryptography including that of encryption see section private key versus public key solutions to the encryption problem today come in two distinct flavors in private key cryptography one assumes that the two or more parties participating in the protocol share a private key namely a statistically random string of modest size that is not known to the in a public key encryption system a concept introduced by diffie and hellman in we drop this assumption instead a party p picks a pair of keys an encryption key and decryption key both chosen at random from some correlated distribution the encryption key will be used to encrypt messages to p and is considered public i e published and known to everybody including the eavesdropper the decryption key is kept secret by p and is used to decrypt messages a famous public key encryption scheme is based upon the rsa function of example at the moment we do not know how to base public key encryption on the sole assumption that one way functions exist and current constructions require the assumption that there exist one way functions with some special structure such as rsa factoring based and lattice based one way functions most topics described in this chapter are traditionally labeled private key cryptography hard on average problems and one way functions a basic cryptographic primitive is a one way function roughly speaking this is a function f that is easy to compute but hard to invert notice that if f is not one to one then the inverse f x may not be unique in such cases inverting means that given f x the algorithm is able to produce some preimage namely any element of f f x we say that the function is one way function if inversion is difficult for the average or many x now we define this formally a discussion of this definition appears below in section a function family gn is a family of functions where gn takes n bit inputs it is polynomial time computable if there is a polynomial time tm that given an input x computes g x x on general graphs but on most n node graphs is solvable in quadratic time or less a deeper study of average case complexity appears in chapter 2practically this could be ensured with a face to face meeting that might occur long before the transmission of messages web draft 21 hard on average problems and one way functions definition one way function a family of functions fn n m n is e n one way with security n if it is polynomial time computable and furthermore for every algorithm a that runs in time n prx n a inverts fn x e n now we give a few examples and discuss the evidence that they are hard to invert on average inputs example the first example is motivated by the fact that finding the prime factors of a given integer is the famous factoring problem for which the best current algorithm has running time about and even that bounds relies on the truth of some unproven conjectures in number theory the hardest inputs for current algorithms appear to be of the type x y where x y are random primes of roughly equal size here is a first attempt to define a one way function using this observation let fn be a family of functions where fn n n is defined as fn x y x y if x and y are primes which by the prime number theorem happens with probability  when x y are random n bit integers then fn seems hard to invert it is widely believed that there are f c f such that family fn is nc one way with security parameter an even harder version of the above function is obtained by using the existence of a randomized polynomial time algorithm a which we do not describe that given generates a random n bit prime number suppose a uses m random bits where m poly n then a may be seen as a deterministic mapping from m bit strings to n bit primes now let function f m map to a a where a a are the primes output by a using random strings respectively this function seems hard to invert for almost all note that any inverse for f m allows us to factor the integer a a since unique factorization implies that the prime pair a a must be the same as a a it is widely conjecture that there are c f such that f n is nc one way with security parameter f the factoring problem a mainstay of modern cryptography is of course the inverse of multiplication who would have thought that the humble multiplication taught to children in second grade could be the source of such power the next two examples also rely on elementary mathematical operations such as exponentiation albeit with modular arithmetic example let be a sequence of primes where pi has i bits let gi be the generator of the group zp i the set of numbers that are nonzero mod pi then for every y pi there is a unique x p such that gx y mod pi web draft 21 hard on average problems and one way functions then x gx mod pi is a permutation on pi and is conjectured to be one way the inversion problem is called the discrete log problem we show below using random self reducibility that if it is hard on worst case inputs then it is hard on average we list some more conjectured one way functions example rsa function let m pq where p q are large random primes and e be a random number coprime to  m p q let zm be the set of integers in m coprime to m then the function is defined to be fp q e x xe mod m this function is used in the famous rsa public key cryptosystem rabin function for a composite number m define fm x mod m if we can invert this function on a poly log m fraction of inputs then we can factor m in poly log m time see exercises both the rsa and rabin functions are useful in public key cryptography they are examples of trapdoor one way functions if the factors of m the trapdoor information are given as well then it is easy to invert the above functions trapdoor functions are fascinating objects but will not be studied further here random subset sum let m let the inputs to f be n positive m bit integers an and a subset s of n its output is an i s ai note that f maps n m bit inputs to nm m bits when the inputs are randomly chosen this function seems hard to invert it is conjectured that there is c d such that this function is nc one way with security discussion of the definition of one way function we will always assume that the the one way function under consideration is such that the security parameter n is superpolynomial i e larger than nk for every k the functions described earlier are actually believed to be one way with a larger security parameter for some fixed e of greater interest is the error parameter e n since it determines the fraction of inputs for which inversion is easy clearly a continuum of values is possible but two important cases to consider are i e n nc for some fixed c in other words the function is difficult to invert on at least nc fraction of inputs such a function is often called a weak one way function the simple one way function fn of example is conjectured to be of this type ii e n nk for every k such a function is called a strong one way function yao showed that if weak one way functions exist then so do strong one way functions we will prove this surprising theorem actually something close to it in chapter we will not use it in this chapter except as a justification for our intuition that strong one way functions exist draft web draft 21 what is a random enough string another justification is of course the empirical observation that the candidate one way functions mentioned above do seem appear difficult to invert on most inputs random self reducibility roughly speaking a problem is random self reducible if solving the problem on any input x reduces to solving the problem on a sequence of random inputs where each yi is uniformly distributed among all inputs to put it more intuitively the worst case can be reduced to the average case hence the problem is either easy on all inputs or hard on most inputs in other words we can exclude the possibility that problem is easy on almost all the inputs but not all if a function is one way and also randomly self reducible then it must be a strong one way function this is best illustrated with an example theorem suppose a is an algorithm with running time t n that given a prime p a generator g for z p and an input gx mod p manages to find x for  fraction of x z p then there is a randomized algorithm ai with running time o t n poly n that solves discrete log on every input with probability at least e proof suppose we are given y gx mod p and we are trying to find x repeat the following trial o  log e times randomly pick r p and use a to try to compute the logarithm of y gr modp suppose a outputs z check if gz r modp is y and if so output z r mod p as the answer the main observation is that if r is randomly chosen then y gr mod p is randomly distributed in z p and hence the hypothesis implies that a has a  chance of finding its discrete log after o  log e trials the probability that a failed every time is at most e corollary if for any infinite sequence of primes discrete log mod pi is hard on worst case x z pi then it is hard for almost all x later as part of the proof of theorem we give another example of random self reducibility linear functions over gf what is a random enough string cryptography often becomes much easier if we have an abundant supply of random bits here is an example example one time pad suppose the message sender and receiver share a long string r of random bits that is not available to eavesdroppers then secure communication is easy to encode message m n take the first n bits of r say the string interpret both strings as vectors in gf n and encrypt m by the vector m the receiver decrypts this message by adding to it note that in gf n web draft 21 what is a random enough string if is statistically random then so is m hence the eavesdropper provably cannot obtain even a single bit of information about m regardless of how much computational power he expends note that reusing is a strict no no hence the name one time pad if the sender ever reuses to encrypt another message mi then the eavesdropper can add the two vectors to obtain m mi m mi which is some nontrivial information about the two messages of course the one time pad is just a modern version of the old idea of using codebooks with a new key prescribed for each day one time pads are conceptually simple but impractical to use because the users need to agree in advance on a secret pad that is large enough to be used for all their future communications it is also hard to generate because sources of quality random bits e g those based upon quantum phenomena are often too slow cryptography suggested solution to such problems is to use a pseudorandom generator this is a deterministically computable function g n nc for some c such that if x n is randomly chosen then g x looks random thus so long as users have been provided a common n bit random string they can use the generator to produce nc random looking bits which can be used to encrypt nc messages of length n in cryptography this is called a stream cipher clearly at this point we need an answer to the question posed in the section title philosophers and statisticians have long struggled with this question example what is a random enough string here is kolmogorov definition a string of length n is random if no turing machine whose description length is say outputs this string when started on an empty tape this definition is the right definition in some philosophical and technical sense which we will not get into here but is not very useful in the complexity setting because checking if a string is random according to this definition is undecidable statisticians have also attempted definitions which boil down to checking if the string has the right number of patterns that one would expect by the laws of statistics e g the number of times appears as a substring see knuth volume for a comprehensive discussion it turns out that such definitions are too weak in the cryptographic setting one can find a distribution that passes these statistical tests but still will be completely insecure if used to generate the pad for the one time pad encryption scheme blum micali and yao definitions now we introduce two complexity theoretic definitions of pseudorandomness due to blum micali and yao in the early for a string y n and s n we let y s denote the projection of y to the coordinates of s in particular y i denotes the first i bits of y draft web draft 21 what is a random enough string the blum micali definition is motivated by the observation that one property in fact the defin ing property of a statistically random sequence of bits y is that given y i we cannot predict yi with odds better than regardless of the computational power available to us thus one could define a pseudorandom string by considering predictors that have limited computational resources and to show that they cannot achieve odds much better than in predicting yi from y i of course this definition has the shortcoming that any single finite string would be predictable for a trivial reason it could be hardwired into the program of the predictor turing machine to get around this difficulty the blum micali definition and also yao definition below defines pseudorandomness for distributions of strings rather than for individual strings further more the definition concerns an infinite sequence of distributions one for each input size definition blum micali let gn be a polynomial time computable family of functions where gn n m and m m n n we say the family is e n t n unpredictable if for every probabilistic polynomial time algorithm a that runs in time t n and every large enough input size n pr a g x i g x i e n where the probability is over the choice of x n i n and the randomness used by a if for every fixed k the family gn is nc nk unpredictable for every c then we say in short that it is unpredictable by polynomial time algorithms remark allowing the tester to be an arbitrary polynomial time machine makes perfect sense in a crypto graphic setting where we wish to assume nothing about the adversary except an upperbound on her computational power pseudorandom generators proposed in the pre complexity era such as the popular linear or quadtratic congruential generators do not satisfy the blum micali definition because bit prediction can in fact be done in polynomial time yao gave an alternative definition in which the tester machine is given access to the entire string at once this definition implicitly sets up a test of randomness analogous to the more famous turing test for intelligence see figure the tester machine a is given a string y nc that is produced in one of two ways it is either drawn from the uniform distribution on nc or generated by taking a random string x n and stretching it using a deterministic function g n nc the tester is asked to output if the string looks random to it and otherwise we say that g is a pseudorandom generator if no polynomial time tester machine a has a great chance of being able to determine which of the two distributions the string came from definition let gn be a polynomial time computable family of functions where gn n m and m m n n we say it is a  n n pseudorandom generator if for every probabilistic algorithm a running in time n and for all large enough n pry nc a y prx n a gn x  n web draft 21 194 what is a random enough string we call  n the distinguishing probability and n the security parameter if for every ci k the family is nc nk pseudorandom then we say in short that it is a pseudorandom generator figure unavailable in pdf file figure yao definition if c then g n nc is a pseudorandom generator if no polynomial time tester has a good chance of distinguishing between truly random strings of length n and strings generated by applying g on random n bit strings equivalence of the two definitions yao showed that the above two definitions are equivalent up to minor changes in the security parameter a family is a pseudorandom generator iff it is bitwise unpredictable the hybrid argument used in this proof has become a central idea of cryptography and complexity theory the nontrivial direction of the equivalence is to show that pseudorandomness of the blum micali type implies pseudorandomness of the yao type not surprisingly this direction is also more important in a practical sense designing pseudorandom generators seems easier for the blum micali definition as illustrated by the goldreich levin construction below whereas yao definition seems more powerful for applications since it allows the adversary unrestricted access to the pseudorandom string thus yao theorem provides a bridge between what we can prove and what we need proof the converse part is trivial since a bit prediction algorithm can in particular be used to distinguish g x from random strings of the same length it is left to the reader let n be shorthand for n n suppose g is not e n t n pseudorandom and a is a distin guishing algorithm that runs in t n time and satisfies prn a g x pr a y e n n b y by considering either a or the algorithm that is a with the answer flipped we can assume that the can be removed and in fact pr a g x pr draft a y e n n what is a random enough string consider b the following bit prediction algorithm let its input be g x i where x n and i n are chosen uniformly at random b program is pick bits ui ui un randomly and run a on the input g x iui un if a outputs output ui else output ui clearly b runs in time less than t n o n n n to complete the proof we show that b predicts g x i correctly with probability at least e n n consider a sequence of n distributions through dn defined as follows in all cases x n and un are assumed to be chosen randomly un g x un di g x iui un dn g x x g x n furthermore we denote by di the distribution obtained from di by flipping the ith bit i e replacing g x i by g x i if d is any of these n distributions then we denote pry d a y by q d with this notation we rewrite as q dn q e n furthermore in di the i th bit is equally likely to be g x i and g x i so q di q di q di now we analyze the probability that b predicts g x i correctly since i is picked randomly we have pr b is correct n pr b guess for g x i is correct ui g x i i x n i x u pr b guess for g x i is correct ui g x i x u since b guess is ui iff a outputs this is n i i q d i n web draft 21 one way functions and pseudorandom number generators from q di q di q di q di so this becomes n n q dn q this finishes our proof e n n one way functions and pseudorandom number generators do pseudorandom generators exist surprisingly the answer though we will not prove it in full generality is that they do if and only if one way functions exist theorem one way functions exist iff pseudorandom generators do since we had several plausible candidates for one way functions in section this result helps us design pseudorandom generators using those candidate one way functions if the pseudorandom generators are ever proved to be insecure then the candidate one way functions were in fact not one way and so we would obtain among other things efficient algorithms for factoring and discrete log the if direction of theorem is trivial if g is a pseudorandom generator then it must also be a one way function since otherwise the algorithm that inverts g would be able to distinguish its outputs from random strings the only if direction is more difficult and involves using a one way function to explicitly construct a pseudorandom generator we will do this only for the special case of one way functions that are permutations namely they map n to n in a one to one and onto fashion as a first step we describe the goldreich levin theorem which gives an easy way to produce one pseudorandom bit and then describe how to produce nc pseudorandom bits goldreich levin hardcore bit let fn be a one way permutation where fn n n clearly the function g n n defined as g x r f x r is also a one way permutation goldreich and levin showed that given f x r it is difficult for a polynomial time algorithm to predict x r the scalar product of x and r mod thus even though the string f x r in principle contains all the information required to extract x r it is computationally difficult to extract even the single bit x r this bit is called a hardcore bit for the permutation prior to the goldreich levin result we knew of hardcore bits for some specific conjectured one way permutations not all draft web draft 21 one way functions and pseudorandom number proof sup pose that some algorithm a can predict x r with probability  in time t n we show how to invert fn x for o  fraction of the inputs in o n time from which the theorem follows claim suppose that prx r n a fn x r x r  then for at least  fraction of x  prr n a fn x r x r proof we use an averaging argument suppose that p is the fraction of x satisfying we have p p   solving this with respect to p we obtain  p   we design an inversion algorithm that given fn x where x r n will try to recover x it succeeds with high probability if x is such that holds in other words for at least  fraction of x note that the algorithm can always check the correctness of its answer since it has fn x available to it and it can apply fn to its answer and see if fn x is obtained warmup reconstruction when the probability in is  let p be any program that computes some unknown linear function over gf n but errs on some inputs specifically there is an unknown vector x gf n such that pr p r x r  r then we show to add a simple correction procedure to turn p into a probabilistic program p i such that r pr p i r x r once we know how to compute x r for every r with high probability it is easy to recover x bit by bit using the observation that if ei is the n bit vector that is in the ith position and zero elsewhere then x ei ai the ith bit of a web draft 21 one way functions and pseudorandom number generators on input r repeat the following trial o log n times pick y randomly from gf n and compute the bit p r y p y at the end output the majority value the main observation is that when y is randomly picked from gf n then r y and y are both randomly distributed in gf n and hence the probability that p r y a r y or p y a y is at most  thus with probability at least each trial produces the correct bit then chernoff bounds imply that probability is at least that the final majority is correct general case the idea for the general case is very similar the only difference being that this time we want to pick rm so that we already know x ri the preceding statement may appear ridiculous since knowing the inner product of x with m n random vectors is with high probability enough to reconstruct x see exercises the explanation is that the ri will not be completely random instead they will be pairwise independent recall the following construction of a set of pairwise independent vectors pick k random vectors tk gf n and for each nonempty s k ys i s ti s s ys ys are independent of each other now let us describe the observation at the heart of the proof suppose and our random strings rm are ys from the previous paragraph then x ys x i s ti actually know x ti for i k since x is unknown and the ti are random vectors but we can just try all possibilities for the vector x ti i k and run the rest of the algorithm for each of them whenever our guess for these innerproducts is correct the algorithm succeeds in producing x and this answer can be checked by applying fn on it as already noted thus the guessing multiplies the running time by a factor which is only m this is why we can assume that we know x ys for each subset s the details of the rest of the algorithm are similar to before pick m pairwise independent vectors ys such that as described above we know x ys for all s for each i n and each s run a on the input fn x ys ei where ys ei is ys with its ith entry flipped compute the majority value of a fn x ys ei x ys among all s and use it as your guess for xi suppose x gf n satisfies we will show that this algorithm produces all n bits of x with probability at least fix i for each i the guess for xi is a majority of m bits the expected number of bits among these that agree with xi is m  so for the majority vote to result in the incorrect answer it must be the case that the number of incorrect values deviates from its expectation by more than m now we can bound the variance of this random variable and apply chebyshev inequality lemma a in appendix a to conclude that the probability m here is the calculation using chebyshev inequality let s denote the event that a produces the correct answer on fn x ys ei since x satisfies and ys ei is randomly distributed over gf e s  v ar s e s e s  s s number of correct answers on a sample of size m by linearity of expectation e  m  furthermore the ys are pairwise independent which implies that the same is true for the outputs s produced by the algorithm a on them hence by pairwise independence v ar  m now by draft web draft 21 applications chebyshev inequality the probability that the majority vote is incorrect is at most ar  finally setting m n2 the probability of guessing the ith bit incorrectly is at most by the union bound the probability of guessing the whole word incorrectly is at most hence for every x satisfying we can find the preimage of f x with probability at least which makes the overall probability of inversion at least  the running time is about running time of a which is t n as we had claimed pseudorandom number generation we saw that if f is a one way permutation then g x r f x r x r is a pseudorandom generator that stretches bits to bits stretching to even more bits is easy too as we now show let fi x denote the i th iterate of f on x i e f f f f x where f is applied i times theorem if f is a one way permutation then gn x r r x r f x r f x r f n x r is a pseudorandom generator for n n for any constant c proof since any distinguishing machine could just reverse the string as a first step it clearly suffices to show that the string r f n x r f n x r f x r x r looks pseudorandom by yao theorem theorem it suffices to show the difficulty of bit prediction for contra diction sake assume there is a ppt machine a such that when x r n and i n are randomly chosen pr a predicts fi x r given r f n x r f n x r fi x r e we describe an algorithm b that given f z r where z r n are randomly chosen predicts the hardcore bit z r with reasonable probability which contradicts theorem algorithm b picks i n randomly let x n be such that fi x z there is of course no efficient way for b to find x but for any l b can efficiently compute fi l x fl f z so it produces the string r f n x r f n x r fi x r and uses it as input to a by assumption a predicts fi x r z r with good odds thus we have derived a contradiction to theorem applications now we give some applications of the ideas introduced in the chapter pseudorandom functions pseudorandom functions are a natural generalization of and are easily constructed using pseudo random generators this is a function g m n m for each k m we denote by g k the function from n to m defined by g k x g k x thus the family contains functions from n to m one for each k web draft 21 applications we say g is a pseudorandom function generator if it passes a turing test of randomness analogous to that in yao definition of a pseudorandom generator definition recall that the set of all functions from n to m denoted n m has cardinality the ppt machine is presented with an oracle for a function from n to n the function is one of two types either a function chosen randomly from n m or a function f k where k m is randomly chosen the ppt machine is allowed to query the oracle in any points of its choosing we say f k is a pseudorandom function generator if for all c the ppt has probability less than n c of detecting which of the two cases holds a completely formal definition would resemble definition and talk about a family of generators one for each n then m is some function of n figure unavailable in pdf file figure constructing a pseudorandom function from n to m using a random key k m and a length doubling pseudorandom generator g now we describe a construction of a pseudorandom function generator g from a length doubling pseudorandom generator f m for any k m let tk be a complete binary tree of depth n whose each node is labelled with an m bit string the root is labelled k if a node in the tree has label y then its left child is labelled with the first m bits of f y and the right child is labelled with the last m bits of f y now we define g k x for any x n interpret x as a label for a path from root to leaf in tk in the obvious way and output the label at the leaf see figure we leave it as an exercise to prove that this construction is correct a pseudorandom function generator is a way to turn a random string k into an implicit de scription of an exponentially larger random looking string namely the table of all values of the function g k this has proved a powerful primitive in cryptography see the next section further more pseudorandom function generators have also figured in a very interesting explanation of why current lowerbound techniques have been unable to separate p from np see chapter private key encryption definition of security we hinted at a technique for private key encryption in our discussion of a one time pad including the pseudorandom version at the start of section but that discussion completely omitted what the design goals of the encryption scheme were this is an important point design of insecure systems often traces to a misunderstanding about the type of security ensured or not ensured by an underlying protocol the most basic type of security that a private key encryption should ensure is semantic security informally speaking this means that whatever can be computed from the encrypted message is also computable without access to the encrypted message and knowing only the length of the message the formal definition is omitted here but it has to emphasize the facts that we are talking about an ensemble of encryption functions one for each message size as in definition and that the encryption and decryption is done by probabilistic algorithms that use a shared private key and draft web draft 21 applications that for every message the guarantee of security holds with high probability with respect to the choice of this private key now we describe an encryption scheme that is semantically secure let f n n n be a pseudorandom function generator the two parties share a secret random key k when one of them wishes to send a message x to the other she picks a random string r n and transmits r x fk r to decrypt the other party computes fk r and then xors this string with the last n bits in the received text we leave it as an exercise to show that this scheme is semantically secure derandomization the existence of pseudorandom generators implies subexponential deterministic algorithms for bpp this is usually referred to as derandomization of bpp in this case the derandomization is only partial since it results in a subexponential deterministic algorithm stronger complexity assumptions imply a full derandomization of bpp as we will see in chapter theorem if for every c there is a pseudorandom generator that is secure against circuits of size nc then bpp  dtime proof let us fix an  and show that bpp dtime suppose that m is a bpp machine running in nk time we can build another probabilistic machine m i that takes n random bits streches them to nk bits using the pseudorandom generator and then simulates m using this nk bits as a random string obviously m i can be simulated by going over all binary strings n running m i on each of them and taking the majority vote it remains to prove that m and m i accept the same language suppose otherwise then there exists an infinite sequence of inputs xn on which m distinguishes a truly random string from a pseudorandom string with a high probability because for m and m i to produce different results the probability of acceptance should drop from to below hence we can build a distinguisher similar to the one described in the previous theorem by hardwiring these inputs into a circuit family the above theorem shows that the existence of hard problems implies that we can reduce the randomness requirement of algorithms this hardness versus randomness tradeoff is studied more deeply in chapter remark there is an interesting connection to discrepancy theory a field of mathematics let s be a set of subsets of n subset a n has discrepancy e with respect to s if for every s s our earlier result that bpp p poly showed the existence of polynomial size sets a that have low discrepancy for all sets defined by polynomial time turing machines we only described dis crepancy for the universe n but one can define it for all input sizes using lim sup the goal of derandomization is to explicitly construct such sets see chapter web draft 21 applications tossing coins over the phone and bit commitment how can two parties a and b toss a fair random coin over the phone many cryptographic protocols require this basic primitive if only one of them actually tosses a coin there is nothing to prevent him from lying about the result the following fix suggests itself both players toss a coin and they take the xor as the shared coin even if b does not trust a to use a fair coin he knows that as long as his bit is random the xor is also random unfortunately this idea also does not work because the player who reveals his bit first is at a disadvantage the other player could just adjust his answer to get the desired final coin toss this problem is addressed by the following scheme which assumes that a and b are polynomial time turing machines that cannot invert one way permutations the protocol itself is called bit commitment first a chooses two strings xa and ra of length n and sends a message fn xa ra where fn is a one way permutation this way a commits the string xa without revealing it now b selects a random bit b and conveys it then a reveals xa and they agree to use the xor of b and xa ra as their coin toss note that b can verify that xa is the same as in the first message by applying fn therefore a cannot change her mind after learning b bit on the other hand by the goldreich levin theorem b cannot predict xa ra from a first message so this scheme is secure secure multiparty computations this concerns a vast generalization of the setting in section there are k parties and the ith party holds a string xi n they wish to compute f xk where f nk is a polynomial time computable function known to all of them the setting in section is a subcase whereby each xi is a bit randomly chosen as it happens and f is xor clearly the parties can just exchange their inputs suitably encrypted if need be so that unauthorized eavesdroppers learn nothing and then each of them can compute f on his her own however this leads to all of them knowing each other input which may not be desirable in many situations for instance we may wish to compute statistics such as the average on the combination of several medical databases that are held by different hospitals strict privacy and nondisclosure laws may forbid hospitals from sharing information about individual patients the original example yao gave in introducing the problem was of k people who wish to compute the average of their salaries without revealing their salaries to each other we say that a multiparty protocol for computing f is secure if at the end no party learns anything new apart from the value of f xk the formal definition is inspired by the definition of a pseudorandom generator and states that for each i the bits received by party i during the protocol should be computationally indistinguishable from completely random it is completely nonobvious why such protocols must exist yao proved existence for k and goldreich micali wigderson proved existence for general k we will not to our medical database example we see that the hospitals can indeed compute statistics on their combined databases without revealing any information to each other at least any information that can be extracted feasibly nevetheless it is unclear if current privacy laws allow hospitals to perform such secure multiparty protocols using patient data an example of the law lagging behind scientific progress web draft 21 recent developments 17 describe this protocol in any detail here except to mention that it involves scrambling the circuit that computes f lowerbounds for machine learning in machine learning the goal is to learn a succinct function f n from a sequence of type f f where the xi are randomly chosen inputs clearly this is impossible in general since a random function has no succinct description but suppose f has a succinct description e g as a small circuit can we learn f in that case the existence of pseudorandom functions implies that even though a function may be polynomial time computable there is no way to learn it from examples in polynomial time in fact it is possible to extend this impossibility result though we do not attempt it to more restricted function families such as see kearns and valiant recent developments the earliest cryptosystems were designed using the subset sum problem they were all shown to be insecure by the early in the last few years interest in such problems and also the related problems of computing approximate solutions to the shortest and nearest lattice vector problems has revived thanks to a one way function described in ajtai and a public key cryptosystem described in ajtai and dwork and improved on since then by other researchers these constructions are secure on most instances iff they are secure on worst case instances the idea used is a variant of random self reducibility also there has been a lot of exploration of the exact notion of security that one needs for various cryptographic tasks for instance the notion of semantic security in section may seem quite strong but researchers subsequently realized that it leaves open the possibility of some other kinds of attacks including chosen ciphertext attacks or attacks based upon concurrent execution of several copies of the protocol achieving security against such exotic attacks calls for many ideas most notably zero knowledge a brief introduction to this concept appears in section chapter notes and history in the shannon speculated about topics reminiscent of complexity based cryptography the first concrete proposal was made by diffie and hellman though their cryptosystem was later broken the invention of the rsa cryptosystem named after its inventors ron rivest adi shamir and len adleman brought enormous attention to this topic in shamir suggested the idea of replacing a one time pad by a pseudorandom string he also exhibited a weak pseudorandom generator assuming the average case intractability of the rsa function the more famous papers of blum and micali and then yao laid the intellectual foundations of private key cryptography the hybrid argument used by yao is a stronger version of one in an earlier important manuscript of goldwasser and micali that proposed probabilistic encryption schemes the construction of pseudorandom functions in section is due to goldreich goldwasser and micali the question about tossing coins over a telephone web draft 21 recent developments was raised in an influential paper of blum today complexity based cryptography is a vast field with several dedicated conferences goldreich two volume book gives a definitive account a scholarly exposition of number theoretic algorithms including generating random primes and factoring integers appears in victor shoup recent book and the book of bach and shal lit theorem and its very technical proof is in ha stad et al the relevant conference publications are a decade older our proof of the goldreich levin theorem is usually attributed to rackoff unpublished exercises show that if p np then one way functions and pseudorandom generators do not exist requires just a little number theory prove that if some algorithm inverts the rabin func tion fm x mod m on a poly log m fraction of inputs then we can factor m in poly log m time show that if f is a one way permutation then so is fk namely f f f f x where f is applied k times where k nc for some fixed c assuming one way functions exist show that the above fails for one way functions suppose a gf m is an unknown vector let rm gf m be randomly chosen and a ri revealed to us for all i m describe a deterministic algorithm to reconstruct a from this information and show that the probability over the choice of the ri is at least that it works this shows that the trick in goldreich levin proof is necessary suppose somebody holds an unknown n bit vector a whenever you present a randomly chosen subset of indices s n then with probability at least e she tells you the parity of the all the bits in a indexed by s describe a guessing strategy that allows you to guess a an n bit string with probability at least e c for some constant c suppose g n n is any pseudorandom generator then use g to describe a pseudorandom generator that stretches n bits to nk for any constant k show the correctness of the pseudorandom function generator in section draft web draft 21 hint first show that for all message pairs x y their encryptions are indistinguishable by polynomial time algorithms why does this suffice hint use a hybrid argument which replaces the labels on the first k levels of the tree by completely random strings note that the random labels do not need to be assigned ahead of time this would take at least time but can be assigned on the fly whenever they are needed by the distinguishing algorithm recent developments draft web draft 21 part ii lowerbounds for concrete computational models web draft 21 21 complexity theory a modern approach sanjeev arora and boaz barak references and attributions are still incomplete draft 23 in the next few chapters the topic will be concrete complexity the study of lowerbounds on models of computation such as decision trees communication games circuits etc algorithms or devices considered in this lecture take inputs of a fixed size n and we study the complexity of these devices as a function of n web draft 21 210 draft web draft 21 chapter decision trees a decision tree is a model of computation used to study the number of bits of an input that need to be examined in order to compute some function on this input consider a function f n a decision tree for f is a tree for which each node is labelled with some xi and has two outgoing edges labelled and each tree leaf is labelled with an output value or the computation on input x xn proceeds at each node by inspecting the input bit xi indicated by the node label if xi the computation continues in the subtree reached by taking the edge the edge is taken if the bit is thus input x follows a path through the tree the output value at the leaf is f x an example of a simple decision tree for the majority function is given in figure figure unavailable in pdf file figure a decision tree for computing the majority function m aj on three bits outputs if at least two input bits are else outputs recall the use of decision trees in the proof of the lower bound for comparison based sorting algorithms that study can be recast in the above framework by thinking of the input which consisted of n numbers as consisting of n bits each giving the outcome of a pairwise comparison between two numbers we can now define two useful decision tree metrics definition the cost of tree t on input x cost t x is the number of bits of x examined by t definition the decision tree complexity of function f d f is defined as follows where t below refers to the set of decision trees that decide f d f min max cost t x t t x n the decision tree complexity of a function is the number of bits examined by the most efficient decision tree on the worst case input to that tree we are now ready to consider several examples draft example graph connectivity given a graph g as input in adjacency matrix form we would like to know how many bits of the adjacency matrix a decision tree algorithm might have to inspect in order to determine whether g is connected we have the following result theorem let f be a function that computes the connectivity of input graphs with m vertices then d f m the idea of the proof of this theorem is to imagine an adversary that constructs a graph edge by edge in response to the queries of a decision tree for every decision tree that decides connectivity the strategy implicitly produces an input graph which requires the decision tree to inspect each of the m possible edges in a graph of m vertices adversary strategy whenever the decision tree algorithm asks about edge ei answer no unless this would force the graph to be disconnected after i queries let ni be the set of edges for which the adversary has replied no yi the set of edges for which the adversary has replied yes and ei the set of edges not yet queried the adversary strategy maintains the invariant that yi is a disconnected forest for i m and yi ei is connected this ensures that the decision tree will not know whether the graph is connected until it queries every edge example or function let f xn n xi here we can use an adversary argument to show that d f n for any decision tree query of an input bit xi the adversary responds that xi equals for the first n queries since f is the or function the decision tree will be in suspense until the value of the nth bit is revealed thus d f is n example consider the and or function with n we define fk as follows fk xn fk fk x2k if k and is odd xi if k a diagram of a circuit that computes the and or function is shown in figure it is left as an exercise to prove using induction that d fk draft web draft 21 certificate complexity figure unavailable in pdf file figure a circuit showing the computation of the and or function the circuit has k layers of alternating gates where n certificate complexity we now introduce the notion of certificate complexity which in a manner analogous to decision tree complexity above tells us the minimum amount of information needed to be convinced of the value of a function f on input x definition consider a function f n if f x then a certificate for x is a sequence of bits in x that proves f x if f x then a certificate is a sequence of bits in x that proves f x definition the certificate complexity c f of f is defined as follows c f max x input number of bits in the smallest or certificate for x example if f is a function that decides connectivity of a graph a certificate for an input must prove that some cut in the graph has no edges hence it has to contain all the possible edges of a cut of the graph when these edges do not exist the graph is disconnected similarly a certificate is the edges of a spanning tree thus for those inputs that represent a connected graph the minimum size of a certificate is the number of edges in a spanning tree n for those that represent a disconnected graph a certificate is the set of edges in a cut the size of a certificate is at most n and there are graphs such as the graph consisting of two disjoint cliques of size n in which no smaller certificate exists thus c f example we show that the certificate complexity of the and or function fk of example is recall that fk is defined using a circuit of k layers each layer contains only or gates or only and gates and the layers have alternative gate types the bottom layer receives the bits of input x as input and the single top layer gate outputs the answer fk x if f x we can construct a certificate as follows for every and gate in the tree of gates we have to prove that both its children evaluate to whereas for every or gate we only need to prove that some child evaluates to thus the certificate is a subtree in which the and gates have two children but the or gates only have one each thus the subtree only needs to involve input bits if f x a similar web draft 21 214 certificate complexity argument applies but the role of or gates and and gates and values and are reversed the result is that the certificate complexity of fk is or about n the following is a rough way to think about these concepts in analogy to turing machine complexity as we have studied it low decision tree complexity p low certificate complexity np low certificate complexity conp the following result shows however that the analogy may not be exact since in the decision tree world p np conp it should be noted that the result is tight for example for the and or function theorem for function f d f c f proof let be the set of minimal certificates and certificates respectively for f let k c f so each certificate has at most k bits remark note that every certificate must share a bit position with every certificate and furthermore assign this bit differently if this were not the case then it would be possible for both a certificate and certificate to be asserted at the same time which is impossible the following decision tree algorithm then determines the value of f in at most queries algorithm repeat until the value of f is determined choose a remaining certificate from and query all the bits in it if the bits are the values that prove the f to be then stop otherwise we can prune the set of remaining certificates as follows since all certificates must intersect the chosen certificate for any one bit in must have been queried here eliminate from consideration if the certifying value of at at location is different from the actual value found otherwise we only need to consider the remaining k bits of this algorithm can repeat at most k times for each iteration the unfixed lengths of the uneliminated certificates decreases by one this is because once some values of the input have been fixed due to queries for any certificate it remains true that all certificates must intersect it in at least one location that has not been fixed otherwise it would be possible for both a certificate and a certificate to be asserted with at most k queries for at most k iterations a total of queries is used draft web draft 21 randomized decision trees randomized decision trees there are two equivalent ways to look at randomized decision trees we can consider decision trees in which the branch taken at each node is determined by the query value and by a random coin flip we can also consider probability distributions over deterministic decision trees the analysis that follows uses the latter model we will call a probability distribution over a set of decision trees that compute a particular function is then the probability that tree t is chosen from the distribution for a particular input x then we define c x tin t cost t x c x is thus the expected number of queries a tree chosen from will make on input x we can then characterize how well randomized decision trees can operate on a particular problem definition the randomized decision tree complexity r f of f is defined as follows r f min max c p x p x the randomized decision tree complexity thus expresses how well the best possible probability distribution of trees will do against the worst possible input for a particular probability distribution of trees we can observe immediately that f c f this is because c f is a minimum value of cost t x since f is just an expected value for a particular probability distribution of these cost values the minimum such value can be no greater than the expected value example consider the majority function f m aj it is straightforward to see that d f we show that f let be a uniform distribution over the six ways of ordering the queries of the three input bits now if all three bits are the same then regardless of the order chosen the decision tree will produce the correct answer after two queries for such x c x if two of the bits are the same and the third is different then there is a probability that the chosen decision tree will choose the two similar bits to query first and thus a probability that the cost will be there thus remains a probability that all three bits will need to be inspected for such x then c p x therefore r f is at most how can we prove lowerbounds on randomized complexity for this we need another concept lowerbounds on randomized complexity needs cleanup now to prove lowerbounds on randomized complexity it suffices by yao lemma see section to prove lowerbounds on distributional complexity where randomized complexity explores distribu tions over the space of decision trees for a problem distributional complexity considers probability distributions on inputs it is under such considerations that we can speak of average case analysis web draft 21 216 lowerbounds on randomized complexity let be a probability distribution over the space of input strings of length n then if a is a deterministic algorithm such as a decision tree for a function then we define the distributional complexity of a on a function f with inputs distributed according to as the expected cost for algorithm a to compute f where the expectation is over the distribution of inputs definition 15 the distributional complexity d a of algorithm a given inputs distributed according to is defined as d a d x input d x cost a x ex d cost a x from this we can characterize distributional complexity as a function of a single function f itself definition the distributional decision tree complexity f of function f is defined as f max min d a d d a where a above runs over the set of decision trees that are deciders for f so the distributional decision tree complexity measures the expected efficiency of the most efficient decision tree algorithm works given the worst case distribution of inputs the following theorem follows from yao lemma theorem 17 r f f so in order to find a lower bound on some randomized algorithm it suffices to find a lower bound on f such a lower bound can be found by postulating an input distribution and seeing whether every algorithm has expected cost at least equal to the desired lower bound example 18 we return to considering the majority function and we seek to find a lower bound on f consider a distribution over inputs such that inputs in which all three bits match namely and occur with probability all other inputs occur with probability for any decision tree that is for any order in which the three bits are examined there is exactly a probability that the first two bits examined will be the same value and thus there is a probability that the cost is there is then a probability that the cost is thus the overall expected cost for this distribution is this implies that f and in turn that r f so f r f draft web draft 21 some techniques for decision tree lowerbounds some techniques for decision tree lowerbounds definition 19 sensitivity if f n is a function and x n then the sensitivity of f on x denoted sx f is the number of bit positions i such that f x f xi where xi is x with its ith bit flipped the sensitivity of f denoted f is maxx sx f the block sensitivity of f on x denoted bsx f is the maximum number b such that there are disjoint blocks of bit positions bb such that f x f xbi where xbi is x with all its bits flipped in block bi the block sensitivity of f denoted bs f is maxx bsx f it is conjectured that there is a constant c as low as such that bs f o f c for all f but this is wide open the following easy observation is left as an exercise lemma for any function f bs f d f theorem 21 nisan c f f bs f proof for any input x n we describe a certificate for x of size f bs f this certificate is obtained by considering the largest number of disjoint blocks of variables bb that achieve b bsx f bs f we claim that setting these variables according to x constitutes a certificate for x suppose not and let xi be an input that is consistent with the above certificate let bb be a block of variables such that xi xbb then bb must be disjoint from bb which contradicts b bsx f note that each of bb has size at most f by definition of f and hence the size of the certificate we have exhibited is at most f bs f recent work on decision tree lowerbounds has used polynomial representations of boolean func tions recall that a multilinear polynomial is a polynomial whose degree in each variable is definition an n variate polynomial p xn represents f n if p x f x for all x the degree of f denoted deg f is the degree of the multilinear polynomial that represents f the exercises ask you to show that the multilinear polynomial representation is unique so deg f is well defined example 23 the and of n variables xn is represented by the multilinear polynomial n xi and web draft 21 comparison trees and sorting lowerbounds the degree of and and or is n and so is their decision tree complexity there is a similar connection for other problems too but it is not as tight the first part of the next theorem is an easy exercise the second part is nontrivial theorem 24 deg f d f nisan smolensky d f deg f f o deg f comparison trees and sorting lowerbounds to be written yao minmax lemma this section presents yao minmax lemma which is used in a variety of settings to prove lower bounds on randomized algorithms therefore we present it in a very general setting let x be a finite set of inputs and a be a finite set of algorithms that solve some computational problem on these inputs for x x a a we denote by cost a x the cost incurred by algorithm a on input x a randomized algorithm is a probability distribution r on a the cost of r on input x denoted cost r x is ea r cost a x the randomized complexity of the problem is min max cost r x r x x let d be a distribution on inputs for any deterministic algorithm a the cost incurred by it on d denoted cost a d is ex d cost a x the distributional complexity of the problem is max min cost a d d a a yao lemma says that these two quantitities are the same it is easily derived from von neu mann minmax theorem for zero sum games or with a little more work from linear programming duality yao lemma is typically used to lowerbound randomized complexity to do so one defines using some insight and some luck a suitable distribution on the inputs then one proves that every deterministic algorithm incurs high cost say c on this distribution by yao lemma it follows that the randomized complexity then is at least c exercises suppose f is any function that depends on all its bits in other words for each bit position i there is an input x such that f x f xi show that f  log n consider an defined as follows the bit input is partitioned into blocks of size about n the function is iff there is at least one block in which two consecutive bits are and the remaining bits in the block are estimate f bs f c f d f for this function draft web draft 21 yao s minmax lemma show that there is a unique multilinear polynomial that represents f n use this fact to find the multilinear representation of the parity of n variables show that deg f d f chapter notes and history the result that the decision tree complexity of connectivity and many other problems is n has motivated the following conjecture atributed variously to anderaa karp yao n here monotone means that adding edges to the graph cannot make it go from having the property to not having the property e g connectivity graph property means that the property does not depend upon the vertex indices e g the property that vertex and vertex have an edge between them this conjecture is known to be true up to a o factor the proof uses topology and is excellently described in du and ko a more ambitious conjecture is that even the randomized decision tree complexity of monotone graph properties is  but here the best lowerbound is close to the polynomial method for decision tree lowerbounds is surveyed in buhrman and de wolf the method can be used to lowerbound randomized decision tree complexity and more recently quantum decision tree complexity but then one needs to consider polynomials that approximately represent the function web draft 21 yao s minmax lemma draft web draft 21 chapter communication complexity communication complexity concerns the following scenario there are two players with unlimited computational power each of whom holds an n bit input say x and y neither knows the other input and they wish to collaboratively compute f x y where function f n n is known to both furthermore they had foreseen this situation e g one of the parties could be a spacecraft and the other could be the base station on earth so they had already before they knew their inputs x y agreed upon a protocol for the cost of this protocol is the number of bits communicated by the players for the worst case choice of x y researchers have studied many modifications of the above basic scenario including randomized protocols nondeterministic protocols average case protocols where x y are assumed to come from a distribution multiparty protocols etc truly this is a self contained mini world within com plexity theory furthermore lowerbounds on communication complexity have uses in a variety of areas including lowerbounds for parallel and vlsi computation circuit lowerbounds polyhedral theory data structure lowerbounds etc we give a very rudimentary introduction to this area an excellent and detailed treatment can be found in the book by kushilevitz and nisan definition now we formalize the informal description of communication complexity given above a t round communication protocol for f is a sequence of function pairs st ct the input of si is the communication pattern of the first i rounds and the output is from indicating which player will communicate in the ith round the input of ci is the input string of this selected player as well as the communication pattern of the first i rounds the output of ci is the bit that this player will communicate in the ith round finally are valued func tions that the players apply at the end of the protocol to their inputs as well as the communication pattern in the t rounds in order to compute the output these two outputs must be f x y the not confuse this situation with information theory where an algorithm is given messages that have to be transmitted over a noisy channel and the goal is to transmit them robustly while minimizing the amount of com munication in communication complexity the channel is not noisy and the players determine what messages to send web draft 21 221 complexity theory a modern approach sanjeev arora and boaz barak references and attributions are still incomplete lowerbound methods communication complexity of f is c f min protocols p max x y number of bits exchanged by p on x y notice c f n since the trivial protocol is for one player to communicate his entire input whereupon the second player computes f x y and communicates that single bit to the first can they manage with less communication example parity suppose the function f x y is the parity of all the bits in x y we claim that c f clearly c f since the function depends nontrivially on each input so each player must transmit at least one bit next c f since it suffices for each player to transmit the parity of all the bits in his possession then both know the parity of all the bits remark sometimes students ask whether a player can communicate by not saying anything after all they have three options send a or or not say anything in that round we can regard such protocols as communicating with a ternary not binary alphabet and analyze them analogously lowerbound methods now we discuss methods for proving lowerbounds on communication complexity as a running example in this chapter we will use the equality function eq x y if x y otherwise we will see that c eq n fooling set we show c eq n for contradiction sake suppose a protocol exists whose complexity is at most n then there are only communication patterns possible between the players consider the set of all pairs x x using the pigeonhole principle we conclude there exist two pairs x x and xi xi on which the communication pattern is the same of course thus far we have nothing to object to since the answers eq x x and eq xi xi on both pairs are however now imagine giving one player x and the other player xi as inputs a moment thought shows that the communication pattern will be the same as the one on x x and xi xi formally this can be shown by induction if player communicates a bit in the first round then clearly this bit is the same whether his input is x or xi if player communicates in the round then his bit must also be the same on both inputs since he receives the same bit from player and so on draft web draft 21 lowerbound methods hence the player answer on x x must agree with their answer on x xi but then the protocol must be incorrect since eq x xi eq x x the lowerbound argument above is called a fooling set argument it is formalized as follows definition a fooling set for f n n is a set s n n and a value b for every x y s f x y b for every two distinct pairs s either f b or f b lemma if f has a fooling set with m pairs then c f m example disjointness let x y be interpreted as characteristic vectors of subsets of n let disj x y if these two subsets are disjoint otherwise disj x y then c disj n since the following pairs constitute a fooling set s a a a n the tiling lowerbound the tiling lowerbound takes a more global view of f consider the matrix of f denoted m f which is a matrix whose x y th entry is f x y see figure we visualize the figure unavailable in pdf file figure matrix m f for the equality function when the inputs to the players have bits the numbers in the matrix are values of f communication protocol in terms of this matrix a combinatorial rectangle or just rectangle in the matrix is a submatrix corresponding to a b where a n b n if the protocol begins with the first player sending a bit then m f partitions into two rectangles of the type n bn where ab is the subset of strings for which the first player communicates bit b notice n if the next bit is sent by the second player then each of the two rectangles above is further partitioned into two smaller rectangles depending upon what this bit was if the protocol continues for k steps the matrix gets partitioned into rectangles note that each rectangle in the partition corresponds to a subset of input pairs for which the communication sequence thus far has been identical see figure for an example web draft 21 224 lowerbound methods figure unavailable in pdf file figure two way communication matrix after two steps the large number labels are the concatenation of the bit sent by the first player with the bit sent by the second player if the protocol stops then the value of f is determined within each rectangle and thus must be the same for all pairs x y in that rectangle thus the set of all communication patterns must lead to a partition of the matrix into monochromatic rectangles a rectangle a b is monochromatic if for all x in a and y in b f x y is the same definition a monochromatic tiling of m f is a partition of m f into disjoint monochromatic rectangles we denote by  f the minimum number of rectangles in any monochromatic tiling of m f the following theorem is immediate from our discussion above theorem if f has communication complexity c then it has a monochromatic tiling with at most rectangles consequently c  f the following observation shows that the tiling bound subsumes the fooling set bound lemma if f has a fooling set with m pairs then  f m proof if and are two of the pairs in the fooling set then they cannot be in a monochromatic rectangle since not all of y2 y2 have the same f value rank lowerbound now we introduce an algebraic method to lowerbound  f and hence communication complexity recall the high school notion of rank of a square matrix it is the size of the largest subset of rows colums that are independent the following is another definition definition if a matrix has entries from a field f then the rank of an n n matrix m is the minimum value of l such that m can be expressed as l m ibi i where i f and each bi is an n n matrix of rank note that are elements of every field so we can compute the rank over any field we like the choice of field can be crucial see problem in the exercises the following theorem is trivial since each monochromatic rectangle can be viewed by filling out entries outside the rectangle with as a matrix of rank at most theorem for every function f  f rank m f draft web draft 21 lowerbound methods discrepancy the discrepancy of a rectangle a b in m f is number of in a b number of in a b the discrepancy of the matrix m f denote disc f is the largest discrepancy among all rectangles the following lemma relates it to  f lemma  f disc f proof for a monochromatic rectangle the discrepancy is its size divided by the total number of entries in the matrix is the bound follows example lemma 11 can be very loose for the eq function the discrepancy is at least n namely the discrepancy of the entire matrix which would only give a lowerbound of for  f however  f is at least as already noted now we describe a method to upperbound the discrepancy using eigenvalues lemma eigenvalue bound for any matrix m the discrepancy of a rectangle a b is at most max m a b where max m is the magnitude of the largest eigenvalue of m proof let rn denote the characteristic vectors of a b then i a a the discrepancy of the rectangle a b is 1t m  m  m j a b a b max a b max explain this example the mod inner product function defined as f x y x y i xiyi has been encoun tered a few times in this book to bound its discrepancy we consider the matrix f this transformation makes the range of the function and will be useful again later let this new web draft 21 226 lowerbound methods matrix be denoted n it is easily checked that every two distinct rows columns of n are orthog onal every row has norm and that n t n thus we conclude that n 2ni where i is that the discrepancy of a rectangle a b is at most at most since a b a b and the overall discrepancy is a technique for upperbounding the discrepancy now we describe an upperbound technique for the discrepancy that will later be useful in the multiparty setting section for ease of notation in this section we change the range of f to by replacing in m f with and replacing with note that now disc f max f a b definition 15 e f i j f ai bj note that e f can be computed like the rank in polynomial time given the m f as input lemma proof the proof follows in two steps disc f e f claim for every function h n n h ea b f a b we will use the cauchy schwartz inequality specifically the version according to which e e z for every random variable z e h b2 fl fl h ai bj i j eb h b h b eb h b h b cauchy schwartz ea b h a b repeating prev two steps claim for every function f there is a function h such that f h and ea b h a b disc f draft web draft 21 lowerbound methods first we note that for every two functions n if we define h f h a b f a b a b then e f e h the reason is that for all b2 fl fl h ai bj b1 b2 fl fl f ai bj i j and the square of any value of is i j now we prove claim using the probabilistic method define two random functions n as follows g a if a a ra ra is randomly chosen g b if b b sb sb is randomly chosen let h f and therefore e h e f furthermore ea b h a b ea b f a b a b a a b b f a b disc f where the second line follows from the fact that a b for a a and b b thus in particular there exist such that ea b h a b disc f comparison of the lowerbound methods as already noted discrepancy upperbounds imply lowerbounds on  f of the other three meth ods the tiling argument is the strongest since it subsumes the other two the rank method is the weakest since the rank lowerbound always implies a tiling lowerbound and a fooling set lowerbound the latter follows from problem in the exercises also we can separate the power of these lowerbound arguments for instance we know functions for which there is a significant gap between log  f and log rank m f however the following conjecture we only state one form of it says that all three methods except discrepancy which as already noted can be arbitrarily far from  f give the same bound up to a polynomial factor conjecture 17 log rank conjecture there is a constant c such that c f o log rank m f c for all f and all input sizes n web draft 21 multiparty communication complexity multiparty communication complexity there is more than one way to generalize communication complexity to a multiplayer setting the most interesting model is the number on the forehead model often encountered in math puzzles that involve people in a room each person having a bit on their head which everybody else can see but they cannot more formally there is some function f n k and the input is xk where each xi n the ith player can see all the xj such that j i as in the player case the k players have an agreed upon protocol for communication and all this communication is posted on a public blackboard at the end of the protocol all parties must know f xk example 18 consider computing the function n f maj i in the party model where are n bit strings the communication complexity of this function is each player counts the number of i such that she can determine the majority of by examining the bits available to her she writes the parity of this number on the blackboard and the final answer is the parity of the players bits this protocol is correct because the majority for each row is known by either or players and both are odd numbers example 19 generalized inner product the generalized inner product function gipk n maps nk bits to bit as follows n k f xk xij i j notice for k this reduces to the mod inner product of example in the party model we introduced the notion of a monochromatic rectangle in order to prove lower bounds for the k party case we will use cylinder intersections a cylinder in dimen sion i is a subset s of the inputs such that if xk s then for all xii we have that xi xii xi xk s also a cylinder intersection is k ti where ti is a cylinder in dimension i as noted in the party case a communication protocol can be viewed as a way of partitioning the matrix m f here m f is a k dimensional cube and player i communication does not depend upon xi thus we conclude that if f has a multiparty protocol that communicates c bits then its matrix has a tiling using at most monochromatic cylinder intersections draft web draft 21 multiparty communication complexity lemma if every partition of m f into monochromatic cylinder intersections requires at least r cylinder intersections then the k party communication complexity isat least r discrepancy based lowerbound in this section we will assume as in our earlier discussion of discrepancy that the range of the function f is we define the k party discrepancy of f by analogy to the party case disc f max f a a a where t ranges over all cylinder intersections to upperbound the discrepancy we introduce the k party analogue of e f let a cube be a set d in nk of points of the form a2 a2 k a2 k where each ai j n e f ed r f a a d notice that the definition of for the party case is recovered when k the next lemma is also an easy generalization lemma 21 disc f e f proof the proof is analogous to lemma 16 and left as an exercise the only difference is that instead of defining random functions we need to define k random functions g2 gk nk where gi depends on every one of the k coordinates except the ith now we can prove a lowerbound for the generalized inner product function note that since we changed the range to it is now defined as gipk n xk i n tlj k xij theorem the function gipk n has k party communication complexity  n as n grows larger proof we use induction on k for k let k be defined using and k k we claim that e gipk n n web draft 21 probabilistic communication complexity assuming truth for k we prove for k a random cube d in nk is picked by picking n and then picking a random cube di in k n e gipk n ed a d gipk n a 11 the proof proceeds by considering the number of coordinates where strings and are identical examining the expression for gipk n in we see that these coordinates contribute nothing once we multiply all the terms in the cube since their contributions get squared and thus become the coordinates that contribute are to be completed probabilistic communication complexity will define the model give the protocol for eq and describe the discrepancy based lowerbound overview of other communication models we outline some of the alternative settings in which communication complexity has been studied nondeterministic protocols these are defined by analogy to np in a nondeterministic pro tocol the players are both provided an additional third input z nondeterministic guess apart from this guess the protocol is deterministic the cost incurred on x y is min z z number of bits exchanged by protocol when guess is z the nondeterministic communication complexity of f is the minimum k such that there is a nondeterministic protocol whose cost for all input pairs is at most k in general one can consider communication protocols analogous to np conp ph etc randomized protocols these are defined by analogy to rp bpp the players are provided with an additional input r that is chosen uniformly at random from m bit strings for some m randomization can significantly reduce the need for communication for instance we can use fingerprinting with random primes explored in chapter to compute the equality function by exchanging o log n bits the players just pick a random prime p of o log n bits and exchange x mod p and y mod p average case protocols just as we can study average case complexity in the turing machine model we can study communication complexity when the inputs are chosen from a distribu tion d this is defined as cd f min pr x y d number of bits exchanged by p on x y protocols draft web draft 21 applications of communication complexity 11 computing a non boolean function here the function output is not just but an m bit number for some m we discuss one example in the exercises asymmetric communication the cost of communication is asymmetric there is some b such that it costs the first player b times as much to transmit a bit than it does the second player the goal is to minimize the total cost multiparty settings the most obvious generalization to multiparty settings is whereby f has k arguments xk and player i gets xi at the end all players must know f xk this is not as interesting as the so called number of the forehead where player i can see all of the input except for xi we discuss it in section together with some applications computing a relation there is a relation r n n m and given x y bn the players seek to agree on any b m such that x y b r see section these and many other settings are discussed in applications of communication complexity we briefly discussed parallel computation in chapter yao invented communication com plexity as a way to lowerbound the running time of parallel computers for certain tasks the idea is that the input is distributed among many processors and if we partition these processors into two halves we may lowerbound the computation time by considering the amount of communication that must necessarily happen between the two halves a similar idea is used to prove time space lowerbounds for vlsi circuits for instance in a vlsi chip that is an m m grid if the communi cation complexity for a function is greater than c then the time required to compute it is at least c m communication complexity is also useful in time space lowerbounds for turing machines see problem in exercises and circuit lowerbounds see chapter data structures such as heaps sorted arrays lists etc are basic objects in algorithm design often algorithm designers wish to determine if the data structure they have designed is the best possible communication complexity lowerbounds can be used to establish such results see yannakakis has shown how to use communication complexity lowerbounds to prove lowerbounds on the size of polytopes representing np complete problems solving the open prob lem mentioned in problem in the exercises would prove a lowerbound for the polytope representing vertex cover exercises if s n n show that a space s n tm takes at least  n s n steps to decide the language x x x show that the high school definition of rank the size of the largest set of independent rows or columns is equivalent to that in definition web draft 21 applications of communication complexity give a fooling set argument that proves that c f ilog rank m f l show that c f rank m f consider x y as vectors over gf n and let f x y be their inner product mod prove that the communication complexity is n what field should you use to compute the rank does it matter let f n n be such that all rows of m f are distinct show that c f log n aho ullman yannakakis show that c f o  f 8 for any graph g with n vertices consider the following communication problem player receives a clique c in g and player receives an independent set i they have to com municate in order to determine c i note that this number is either or prove an o n upperbound on the communication complexity can you improve your upperbound or prove a lower bound better than  log n open question prove lemma 21 using the hint given there karchmer wigderson consider the following problem about computing a relation associate the following communication problem with any function f n player gets any input x such that f x and player gets any input y such that f y they have to communicate in order to determine a bit position i such that xi yi show that the communication complexity of this problem is exactly the minixmum depth of any circuit that computes f the maximum fanin of each gate is 11 use the previous question to show that computing the parity of n bits requires depth at least log n show that the following computational problem is in exp given the matrix m f of a boolean function and a number k decide if c f k open since yao can you show this problem is complete for some complexity class draft web draft 21 applications of communication complexity chapter notes and history communication complexity was first defined by yao other early papers that founded the field were papadimitriou and sipser mehlhorn and schmidt who introduced the rank lowerbound and aho ullman and yannakakis the original log rank conjecture was that c f o rank m f but this was disproved by raz and spieker the book by nisan and kushilevitz is highly recommended web draft 21 applications of communication complexity draft web draft 21 chapter circuit lowerbounds complexity theory waterloo we believe that np does not have polynomial sized circuits we ve seen that if true this implies that np p in the and many researchers came to believe that the route to resolving p versus np should go via circuit lowerbounds since circuits seem easier to reason about than turing machines the success in this endeavor was mixed progress on general circuits has been almost nonexistent a lowerbound of n is trivial for any function that depends on all its input bits we are unable to prove even a superlinear circuit lowerbound for any np problem the best we can do after years of effort is 5n o n to make life comparatively easier researchers focussed on restricted circuit classes and were successful in proving some decent lowerbounds we prove some of the major results of this area and indicate where researchers are currently stuck in chapter we ll explain some of the inherent obstacles that need to be overcome to make further progress and h astad switching lemma as we saw in chapter is the class of languages computable by circuit families of constant depth polynomial size and whose gates have unbounded fanin constant depth circuits with fanin can only compute functions depending on a constant number of input bits the burning question in the late was whether problems like clique and tsp have circuits however in furst saxe and sipser and independently ajtai proved a lowerbound for a much simpler function theorem let be the parity function that is for every x n xn n xi mod often courses in digital logic design teach students how to do circuit minimization using karnaugh maps note that circuits talked about in those courses are depth circuits i e cnf or dnf indeed it is easy to show using for example the karnaugh map technique studied in logic and h astad s switching lemma design that the parity function requires exponentially many gates if the depth is two however those simple ideas do not seem to generalize to even depth circuits the main tool in the proof of theorem is the concept of random restrictions let f be a function computable by a depth d circuit and suppose that we choose at random a vast majority i e n ne for some constant e depending on d of the input variables and assign to each such variable either or at random we ll prove that with positive probability the function f subject to this restriction is constant i e either always zero or always one since the parity function cannot be made a constant by fixing values to a subset of the variables it follows that it cannot be computed by a constant depth circuit the switching lemma now we prove the main lemma about how a circuit simplifies under a random restriction a k dnf resp k cnf formula is an or of and resp and or or where each and resp or involves at most k variables lemma h astad switching lemma suppose f is expressible as a k dnf and let  denote a random restriction that assigns random values to t randomly selected input bits then for every n t where f  denotes the function f restricted to the partial assignment  we ll typically use this lemma with k constant and t n n in which case the guaranteed bound on the probability will be n c for some constant c note that by applying the lemma to the function f we can get the same result with the terms dnf and cnf interchanged proving theorem from lemma now we show how h astad lemma implies that parity is not in we start with any circuit and assume that the circuit has been simplified as follows the simplifications are straightforward to do and are left as exercises and a all fanouts are the circuit is a tree b all not gates to the input level of the circuit equivalently the circuit has input wires with the last n of them being the negations of the first n c and gates alternate at worst this assumption doubles the depth of the circuit d the bottom level has gates of fanin we randomly restrict more and more variables where each step with high probability will reduce the depth of the circuit by and will keep the bottom level at a constant fanin specifically letting ni stand for the number of unrestricted variables after step i we restrict ni ni variables at step i i since n we have ni let nb denote an upper bound on the number of gates in the circuit and let ki we ll show that with high probability after the ith restriction we re left with a depth d i circuit with at most ki fanin in the bottom level indeed suppose that the bottom level contains gates and the level above it contains gates the function each such gate computes is a k dnf and hence by lemma with probability k10 ki which draft web draft 21 and h astad s switching lemma is at least for large enough n the function such a gate computes will be expressible as a ki cnf we can then merge this cnf with the gate above it reducing the depth of the circuit by one see figures and the symmetric reasoning applies in the case the bottom level consists of gates in this case we use the lemma to transform the ki cnf of the level above it into a ki dnf note that we apply the lemma at most once per each of the at most nb gates of the original circuit by the union bound with probability if we continue this process for d steps we ll get a depth two circuit with fanin k kd at bottom level i e a k cnf or k dnf formula if we then choose to restrict each variable with probability half i e restrict about half of the variables to a random value this circuit will be reduced to a constant function with probability at least k since the parity function is not constant under any restriction of less than n variables this proves theorem figure unavailable in pdf file figure circuit before h astad switching transformation figure unavailable in pdf file figure circuit after h astad switching transformation notice that the new layer of gates can be collapsed with the single parent gate to reduce the number of levels by one proof of the switching lemma lemma now we prove the switching lemma the original proof was more complicated this one is due to razborov let f be expressible as a k dnf on n variables let t be as in the lemma and let denote the set of all restrictions to variables note we can assume we have that rt n let kt denote the set of restrictions  such that f  is not a cnf we need to bound kt rt by the right hand side of to prove the lemma we ll do that by showing a one to one function mapping kt into the set z s where z is the set of restrictions of at least t variables i e z t t srt and s is some set of size this will prove the lemma since at he range ti n n n and hence z will be of size bounded by roughly n t r we leave verifying the exact bound as exercise mapping kt into z s let  kt be a restriction fixing t variables such that f  is not an cnf we need to map  in a one to one way into some restriction  of at least t variables and some additional element in a set s of size at most special case each term has at most one live variable to get some intuition for the proof consider first the case that for each term t in the k dnf formula for f  either fixed t to the value or left a single unassigned variable in t in which case we say that tis value is  can t fix a term to the value since we assume f  is not constant we denote by xs denote the web draft 21 238 and h astad s switching lemma first such unassigned variables according to some canonical ordering of the terms for the k dnf formula of f there are more than since otherwise f  would be expressible as an cnf for each such variable xi let termi be the valued term in which xi appears let ri be the operation of setting xi to the value that ensures termi is true we ll map  to rs that is apply rs to  then apply rk to  then apply to  the crucial insight is that given one can deduce this is the first term that is true in f one might think that the second term that is true in f is but that not necessarily the case since the variable may have appeared several times and so setting it to may have set other terms to true it could not have set other terms to false since this would imply that f  includes an or of xi and xi and hence is the constant one function we thus supply as part of the mapping a string that tells us the assignment of the k variables of in rs given that information we can undo and move from to now in is the first satisfied term continuing on this way we see that from which is an assignment of at least t variables and strings ws that are defined as above we can recover  implying that we have a one to one mapping that takes  into an assignment of at least t variables and a sequence in ks the general case we now consider the general case where some terms might have more than one unassigned variable in them we let be the first valued term in f  and let be the first unassigned variable in once again we have an operation that will make true although this time we think of as assigning to all the k variables in the unique value that makes the term true we also have an operation assigning a value to such that f cannot be expressed by an cnf indeed if for both possible assignments to we get an cnf then f  is an cnf we note that it not necessarily the case that value under is different from its value under but it is the case that value is either or false under since otherwise f would be constant we let be the first valued term in f note that and let be the first unassigned variable in once again we have an operation such that is the first true term in f and operation such that f is not a cnf continuing in this way we come up with operations ls rs such that if we let i be the assignment li with  then for i termi is the first valued term in f i termi is the first true valued term in f rii li agrees with i on all variables assigned a value by i ri agrees with i on all variables assigned a value by i for i define i to be riri rss and define s s we have that termi is the first true term in f i indeed all the operations in i do not change variables assigned values by i and there termi is the first valued term thus i cannot make any earlier term true however since the last operation applied is ri termi is true in f i let zs and ws be strings in defined as follows zi describes the values assigned to the k variables appearing in termi by i and wi describes the value assigned to termi variables by i clearly from termi zi and the assignment i one can compute i and draft web draft 21 circuits with counters acc from termi wi and the assignment i one can compute i we ll map  to and the sequence zs ws note that does assign values to at least variables not assigned by  and that from we can find as this is the first true term in f and then using recover and continue in this way until we recover the original assignment  thus this mapping is a one to one map from tt to z circuits with counters acc one way to extend the lowerbounds of the previous section was to define a more general class of circuits what if we allow more general gates the simplest example is a parity gate clearly an circuit provided with parity gates can can compute the parity function but are there still other functions that it cannot compute razborov proved the first such lowerbound using his method of approximations smolensky later extended this work and clarified this method for the circuit class considered here normally we think of a modular computation as working with numbers rather than bit but it is sufficient to consider modular gates whose output is always definition modular gates for any integer m the m odm gate outputs if the sum of its inputs is modulo m and otherwise definition acc for integers mk we say a language l is in mk if there exists a circuit family cn with constant depth and polynomial size and unbounded fan in consisting of and m m odmk gates accepting l the class contains every language that is in mk for some k and mk good lowerbounds are known only when the circuit has one kind of modular gate theorem razborov smolensky for distinct primes p and q the function m odp is not in q we exhibit the main idea of this result by proving that the parity function cannot be computed by an circuit proof the proof proceeds in two steps step in the first step we show using induction on h that for any depth h m circuit on n inputs and size s there is a polynomial of degree h which agrees with the circuit on s fraction of the inputs if our circuit c has depth d then we set to obtain a degree n polynomial that agrees with c on s fraction of inputs step we show that no polynomial of degree n agrees with m on more than fraction of inputs web draft 21 240 circuits with counters acc together the two steps imply that s for any depth d circuit computing m od thus proving the theorem now we give details step consider a node g in the circuit at a depth h the input is assumed to have depth if g xn is the function computed at this node we desire a polynomial g xn over gf with degree h such that g xn g xn for most xn we will also ensure that on every input in n gf polynomial g takes a value in this is without loss of generality since we can just square the polynomial recall that the elements of gf are and and we construct the approximator polynomial by induction when h the gate is an input wire xi which is exactly represented by the degree polynomial xi suppose we have constructed approximators for all nodes up to height h and g is a gate at height h if g is a not gate then g for some other gate that is at height h or less the inductive hypothesis gives an approximator f for then we use g f as the approximator polynomial for g this has the same degree as f whenever f then g g so we introduced no new error if g is a m gate with inputs fk we use the approximation g k f i the degree increases to at most h h since and we introduced no new error if g is an and or an or gate we need to be more careful suppose g k fi the naive approach would be to replace g with the polynomial i if i for an or gate g k fi de morgan law gives a similar naive approximator tl i these multiply the degree by k the fanin of the gate which could greatly exceed the correct solution involves introducing some error we give the solution for or de mor gan law allows and gates to be handled similarly if g k fi then g if and only if at least one of the fi furthermore by the random subsum principle see section in appendix a if any of the fi then the sum over gf of a random subset of fi is nonzero with probability at least randomly pick l subsets sl of k compute the l polynomials j si fj each of which has degree at most twice that of the largest input polynomial compute the or of these l terms using the naive approach we get a polynomial of degree at most h 2l h for any x the probability over the choice of subsets that this polynomial differs from or f f k is at most so by the probabilistic method there exists a choice for the l subsets such that the probability over the choice of x that this polynomial differs from or f f k is at most we use this choice of the subsets to construct the approximator applying the above procedure for each gate gives an approximator for the output gate of degree 2l d where d is depth of the entire circuit each operation of replacing the gate by its approximator polynomial introduces error on at most 2l fraction of all inputs so the overall fraction of erroneous inputs for the approximator is at most s 2l note that errors at different gates may affect each other error introduced at one gate may be cancelled out by errors at another gate higher up we draft web draft 21 circuits with counters acc are being pessimistic in applying the union bound to upperbound the probability that any of the approximator polynomials anywhere in the circuit miscomputes step suppose that a polynomial f agrees with the m function for all inputs in a set gi if the degree of f is bounded by n then we show gi consider the change of variables yi xi mod thus and then gi becomes some subset of n and becomes some other polynomial say which still has degree n moreover m xn n i n i yi yi thus g y2 yn a degree n polynomial agrees with n yi on g this is decidedly odd and we show that any such g must be small specifically let fg be the set of all functions s g clearly fg g and we will show fg whence step 2 follows lemma for every s fg there exists a polynomial gs which is a sum of monomials ai tli i yi where proof let s gf 3 n gf 3 be any function which agrees with s on g then s can be written as a polynomial in the variables yi however we are only interested in its values on y2 yn n when y2 and so every monomial i iyri has without loss of generality ri thus s is a polynomial of degree at most n now consider any of its monomial terms i iyi of degree i n 2 we can rewrite it as i iyi n yi i i yi 3 which takes the same values as g y2 yn i i yi over n thus every monomial in s 2 to conclude we bound the number of polynomials whose every monomial with a degree at most n n clearly this number is polynomials 3 monomials and monomials n n n n n i i 2 n using knowledge of the tails of a binomial distribution or alternatively direct calculation n 50 2 web draft 21 8 242 3 lowerbounds for monotone circuits 3 lowerbounds for monotone circuits a boolean circuit is monotone if it contains only and and or gates and no not gates such a circuit can only compute monotone functions defined as follows definition for x y n we denote x y if every bit that is in x is also in y a function f n is monotone if f x f y for every x y remark 8 an alternative characterization is that f is monotone if for every input x changing a bit in x from to cannot change the value of the function from to it is easy to check that every monotone circuit computes a monotone function and every mono tone function can be computed by a sufficiently large monotone circuit clique is a monotone function since adding an edge to the graph cannot destroy any clique that existed in it in this section we show that the clique function can not be computed by polynomial and in fact even subexponential sized monotone circuits theorem ab87 denote by clique k n n be the function that on input an adjacency matrix of an n vertex graph g outputs iff g contains a k vertex clique there exists some constant e such that for every k there no monotone circuit of size less than k that computes cliquek n we believe clique does not have polynomial size circuits even allowing not gates i e that np r p poly in fact a seemingly plausible approach to proving this might be to show that for every monotone function f the monotone circuit complexity of f is polynomially related to the general non monotone circuit complexity alas this conjecture was refuted by razborov see also 3 proving theorem clique indicators to get some intuition why this theorem might be true lets show that cliquek n can t be computed or even approximated by subexponential monotone circuits of a very special form for every s n let cs denote the function on that outputs on a graph g iff the set s is a clique n in g we call cs the clique indicator of s note that cliquek n cs we ll now prove that cliquek n can t be computed by an or of less than n k clique indicators let y be the following distribution on n vertex graphs choose a set k n with k k at random and output the graph that has a clique on k and no other edges let n be the following distribution on n vertex graphs choose a function c n k at random and place an edge between u and v iff c u c v with probability one cliquen k y and cliquen k n the fact that cliquen k requires an or of at least n clique indicators follows immediately from the following lemma draft web draft 21 3 lowerbounds for monotone circuits lemma let n be sufficiently large k and s n then either pr cs n or pr cs y n k proof let k if s then by the birthday bound we expect a random f s k to have less than collisions and hence by markov the probability f is one to one is at least this implies that pr cs n 99 if s then pr cs y is equal to the probability that s k for a random k n of size k this probability is equal to n c n which is at most n n which by the formula for the binomial coefficients is less than 2 k c n c n k for sufficiently large n approximation by clique indicators together with lemma 10 the following lemma implies theorem lemma 11 let c be a monotone circuit of size let m n k such that k 10 then there exist sets sm with prg ry csi g c g 9 prg rn csi g c g 9 8 i 9 proof set k 10 p 10 k log n and m p c note that m n k we can think of the circuit c as the sequence of monotone functions f f from n to where each function fk is either the and or or of two functions fk fk for ki kii k or is the value of an input variable xu v for u v n i e fk c u v the function that c computes is fs we ll show a sequence of functions f such that each function f k is an or of at most m clique indicators cs cs with si and 2 f k approximates fk in the sense of and 8 we call a function f k satisfying an m function the result will follow by considering the function f we construct the functions f f by induction for k if fk is an input variable then we let f k fk if fk fk fk then we let f k lj f k and if fk fk fk then we let f k n f k where the operations lj n will be defined below we ll prove that for every f g n a if f and g are m functions then so is f lj g resp f n g and b prg ry f ljg g f g g resp prg ry f g g f g g and prg rn f g g f g g resp prg f g g f g g the lemma will then follow by showing using the union bound that with probability 9 the equations of condition b hold for all f f we ll now describe the two operations condition a will follow from the definition of the operations while condition b will require a proof web draft 21 10 244 3 lowerbounds for monotone circuits the operation f lj g let f g be two m functions that is f vm cs and g vm ct i j if f or g is the or of less than m clique indicators we can add duplicate sets to make the number m consider the function h where zi si and zm j tj for i j m the function h is not an m function since it is the or of clique indicators we make it into an m function in the following way as long as there are more than m distinct sets find p subsets zip that are in a sunflower formation that is there exists a set z n such that for every j ji p zij zi j z replace the functions czi czip in the function h with the function cz once we obtain an m function hi we define f g to be hi we won t get stuck because of the following lemma whose proof we defer lemma 12 sunflower lemma let z be a collection of distinct sets each of cardinality at most if z p c then there exist p sets zp z and set z such that zi zj z for every i j p the operation f n g let f g be two m functions that is f vm cs and g vm ct let be the function v i i j j function cz for z 2 reduce the number of functions to m by applying the sunflower lemma as above proving condition b to complete the proof of the lemma we prove the following four equations prg ry f ljg g f g g 10s if z zp then for every i czi g implies that cz g and hence the operation f g can t introduce any false negatives prg rn f ljg g f g g 10s we can introduce a false positive on a graph g only if when we replace the clique indicators for a sunflower zp with the clique indicator for the common intersection z it is the case that cz g holds even though czi g is false for every i recall that we choose g r n by choosing a random function c n k and adding an edge for every two vertices u v with c u c v thus we get a false positive if c is one to one on z we denote this event by b but not one to one on zi for every i p we denote these events by ap we ll show that the intersection of b and ap happens with probability at most 2 p which by the choice of p is less than 10m2s since we apply the reduction step at most m times the equation will follow since k 10 for every i pr ai b 2 the probability that there ll be a collision on the at most elements of zi z is less than half conditioned on b the events ap are independent since they depend on the values of c on disjoint sets and hence we have that pr ap b pr ap b tlp pr ap b 2 p prg ry f ng g f g g 10s by the distributive law f g i j csi ctj a graph g in the support of y consists of a clique over some set k for such a graph csi ctj holds iff si tj k and thus csi ctj holds draft web draft 21 circuit complexity the frontier 11 iff csi tj holds we can introduce a false negative when we discard functions of the form cz for z but by lemma 10 for such sets z pr cz n k the equation follows since we discard at most m2 such sets prg rn f ng g f g g 10s since implies both and we can t introduce false positives by moving from to i j csi tj we can t introduce false positives by discarding functions from the or thus the only place where we can introduce false positives is where we replace the clique indicators of a sunflower with the clique indicator of the common intersection we bound this probability in the same way as this was done for the lj operator proof of the sunflower lemma lemma 12 the proof is by induction on the case is trivial since distinct sets of size must be disjoint for let m be a maximal subcollection of z containing only disjoint sets because of m maximality for every z z there exists x m mm such that x z if p we re done since such a collection is already a sunflower otherwise since p by averaging there an x that appears in at least a fraction of the sets in let z z be the sets containing x and note that c p thus by induction there are sets among the sized sets x zk x that form a sunflower adding back x we get the desired sunflower among the original sets note that the statement and proof assume nothing about the size of the universe the sets in z live in circuit complexity the frontier now we sketch the frontier of circuit lowerbounds namely the dividing line between what we can prove and what we cannot along the way we also define multi party communication since it may prove useful for proving some new circuit lowerbounds 4 circuit lowerbounds using diagonalization we already mentioned that the best lowerbound on circuit size for an np problem is 4 5n o n for ph better lowerbounds are known one of the exercises in chapter 6 asked you to show that some for every k some language in ph in fact in p requires circuits of size  nk the latter lowerbound uses diagonalization and one imagines that classes higher up than ph should have even harder languages frontier does nexp have languages that require super polynomial size circuits if we go a little above nexp we can actually prove a super polynomial lowerbound we know that maexp r p poly where maexp is the set of languages accepted by a one round proof with an all powerful prover and an exponential time probabilistic verifier this follows from the fact web draft 21 12 4 circuit complexity the frontier figure unavailable in pdf file figure 3 the depth 2 circuit with a symmetric output gate from theorem that if maexp p poly then in particular pspace p poly however by ip pspace theorem 8 17 we have that in this case pspace ma the prover can send in one round the circuit for computing the prover strategy in the interactive proof however by simple padding this implies that maexp equals the class of languages in exponential space which can be directly shown to not contain p poly using diagonalization interestingly this lower bound does not relativize i e there an oracle under which manexp p poly 4 2 status of acc versus p the result that parity is not in separates from the next logical step would be to separate from less ambitiously we would like to show even a function in p or np that is not in the razborov smolenksy method seems to fail when we allow the circuit even two types of modular gates say m and m in fact if we allow the bounded depth circuit modular gates that do arithmetic mod q when q is not a prime a prime power to be exact we reach the limits of our knowledge the exercises ask you to figure out why the proof of theorem does not seem to apply when the modulus is a composite number to give one example it it is consistent with current knowledge that the majority of n bits can be computed by linear size circuits of constant depth consisting entirely of m gates the problem seems to be that low degree polynomials modulo m where m is composite are surprisingly expressive frontier 2 show clique is not in 6 or even less ambitiously frontier 2 exhibit a language in nexp that is not in 6 it is worth noting that thus far we are talking about nonuniform circuits to which theorem also applies stronger lower bounds are known for uniform circuits allender and gore have shown that a decision version of the permanent and hence the permanent itself requires exponential size dlogtime uniform circuits a circuit family cn is dlogtime uniform if there exists a deterministic turing machine m that given a triple n g h determines in linear time i e o log n time when g h poly n what types of gates g and h are and whether g is h parent in cn but going back to nonuniform we wish to mention an alternative representation of circuits that may be useful in further lowerbounds let a symmetric gate be a gate whose output depends only on the number of inputs that are for example majority and mod gates are symmetric yao has shown that circuits can be simplified to give an equivalent depth 2 circuits with a symmetric gate at the output figure beigel and tarui subsequently improved yao result draft web draft 21 4 circuit complexity the frontier theorem yao beigel and tarui if f then f can be computed by a depth 2 circuit c with a symmetric gate with quasipolynomial i e n fan in at the output level and gates with polylogarithmic fan in at the input level we will revisit this theorem below in section 4 3 linear circuits with logarithmic depth when we restrict circuits to have bounded fanin we necessarily need to allow them to have non constant in fact  log n depth to have any reasonable power with this in mind the simplest interesting circuit class seems to be one of circuits wth linear size and logarithmic depth frontier 3 find an explicit function that cannot be computed by circuits of linear size and logarithmic depth note that by counting one can easily show that some function on n bits requires superpoly nomial size circuits and hence bounded fan in circuits with more than logarithmic depth see the exercises on the chapter on circuits hence we want to show this for an explicit function e g clique valiant thought about this problem in the his initial candidates for lowerbounds boiled down to showing that a certain graph called a superconcentrator needed to have superlinear size he failed to prove thisand instead ended up proving that such superconcentrators do exist another sideproduct of valiant investigations was the following important lemma concerning depth reduction for such circuits lemma valiant in any circuit with m edges and depth d there are km log d edges whose removal leaves a circuit with depth at most d this lemma can be applied as follows suppose we have a circuit c of depth c log n with n inputs xn and n outputs yn and suppose c e where e is arbitrarily small removing o n log log n edges from c then results in a circuit with depth at most e log n but then since c has bounded fan in we must have that each output yi is connected to at most log n ne inputs so each output yi in c is completely determined by ne inputs and the values of the omitted edges so we have a dense encoding for the function fi xn yi we do not expect this to be the case for any reasonably difficult function 4 4 branching programs just as circuits are used to investigate time requirements of turing machines branching programs are used to investigate space complexity a branching program on n input variables xn is a directed acyclic graph all of whose nodes of nonzero outdegree are labeled with a variable xi it has two nodes of outdegree zero that are labeled with an output value accept or reject the edges are labeled by or one of the nodes is designated the start node a setting of the input variables determines a way to walk web draft 21 14 approaches using communication complexity on the directed graph from the start node to an output node at any step if the current node has label xi then we take an edge going out of the node whose label agrees with the value of xi the branching program is deterministic if every nonoutput node has exactly one edge and one edge leaving it otherwise it is nondeterministic the size of the branching program is the number of nodes in it the branching program complexity of a language is defined analogously with circuit complexity sometimes one may also require the branching program to be leveled whereby nodes are arranged into a sequence of levels with edges going only from one level to the next then the width is the size of the largest level theorem 15 if s n log n and l space s n then l has branching program complexity at most cs n for some constant c proof essentially mimics our proof of theorem that space s n dtime s n the nodes of the branching program correspond to the configurations of the space bounded tm and it is labeled with variable xi if the configuration shows the tm reading the ith bit in the input of course a similar theorem is true about ndtms and nondeterministic branching program complexity frontier 4 describe a problem in p or even np that requires branching programs of size greater than e for some constant e there is some evidence that branching programs are more powerful than one may imagine for instance branching programs of constant width reminiscent of a tm with o bits of memory seem inherently weak thus the next result is unexpected theorem 16 barrington a language has polynomial size width branching programs iff it is in approaches using communication complexity here we outline a concrete approach rather a setting in which better lowerbounds may lead to a resolution of some of the questions above it relates to generalizations of communication complexity introduced earlier mostly we will use multiparty communication complexity though section 4 will use communication complexity of a relation 5 connection to circuits suppose f xk has a depth 2 circuit with a symmetric gate with fan in n at the output and gates with fan in k at the input level figure 2 the claim is that f k party communication complexity is at most k log n this observation is due to razborov and wigderson to see the claim first partition the gates amongst the players each bit is not known to exactly one player so the input bits of each gate are known to at least one player assign the gate to such a player with the lowest index players then broadcast how many of their gates output since this number has at most log n bits the claim follows draft web draft 21 5 approaches using communication complexity 15 figure unavailable in pdf file figure 4 if f is computed by the above circuit then f has a k party protocol of complexity k log n our hope is to employ this connection with communication complexity in conjunction with theorem to obtain lower bounds on circuits for example note that the function in example above cannot have k log n 4 however this is not enough to obtain a lower bound on circuits since we need to show that k is not polylogarithmic to employ theorem thus a strengthening of the babai nisan szegedy lowerbound to  n poly k for say the clique function would close frontier 2 5 2 connection to linear size logarithmic depth circuits suppose that f n log n n has bounded fan in circuits of linear size and logarithmic depth if f x j i denotes the ith bit of f x j then valiant lemma implies that f x j i has a simultaneous 3 party protocol that is a protocol where all parties speak only once and write simultaneously on the blackboard i e non adaptively where x j player sends n log log n bits x i player sends ne bits and i j player sends o log n bits so if we can show that a function does not have such a protocol then we would have a lower bound for the function on linear size logarithmic depth circuits with bounded fan in conjecture the function f x j i xj i where j i is the bitwise xor is conjectured to be hard i e f should not have a compact representation 5 3 connection to branching programs the notion of multiparty communication complexity at least the number on the forehead model discussed here was invented by chandra furst and lipton for proving lowerbounds on branching programs especially constant width branching programs discussed in section 5 4 karchmer wigderson communication games and depth lowerbounds the result that parity is not in separates from the next step would be to separate from of course ignoring for the moment the issue of separating from karchmer and wigderson described how communication complexity can be used to prove lowerbounds on the minimum depth required to compute a function they showed the following result about monotone circuits which we will not prove this result theorem 17 detecting whether a graph has a perfect matching is impossible with monotone circuits of depth o log n web draft 21 16 13 5 approaches using communication complexity however we do describe the basic karchmer wigderson game used to prove the above result since it is relevant for nonmonotone circuits as well for a function f n 1 this game is defined as follows there are two players zero and one player zero receives an input x such that f x and player one receives an input y such that f y 1 they communicate bits to each other until they can agree on an i 1 2 n such that xi yi the mechanism of communication is defined similarly as in chapter 12 there is a protocol that the players agree on in advance before receiving the input note that the key difference from the scenario in chapter 12 is that the final answer is not a single bit and furthermore the final answer is not unique the number of acceptable answers is equal to the number of bits that x y differ on sometimes this is described as computing a relation the relation in this case consists of all triples x y i such that f x f y 1 and xi yi we define ckw f as the communication complexity of the above game namely the maximum over all x f 1 y f 1 1 of the number of bits exchanged in computing an answer for x y the next theorem shows that this parameter has a suprising alternative characterization it assumes that circuits don t have not gates and instead the not gates are pushed down to the inputs using de morgan law in other words the inputs may be viewed as xn xn furthermore and and or gates have fanin 2 none of these assumptions is crucial and affects the theorem only marginally theorem 13 18 ckw f is exactly the minimum depth among all circuits that compute f proof first we show that if there is a circuit of depth k that computes f then ckw f k each player has a copy of and evaluates this circuit on the input given to him of course it ealuates to for player zero and to 1 for player one suppose the top gate is an or then at least one of the two incoming wires to this gate must be 1 and in the first round player one sends one bit communicating which of these wires it was note that this wire is for player zero in the next round the players focus on the gate that produced the value on this wire if the top gate is an and on the other hand then in the first round player zero speaks conveying which of the two incoming wires was this wire will be 1 for player one this goes on and the players go deeper down the circuit always maintaining the invariant that the current gate has value 1 for player one and for player zero finally after at most k steps they arrive at an input bit according to the invariant being maintained this bit must be 1 for player one and for player zero thus they both know an index i that is a valid answer for the reverse direction we have to show that if ckw f k then there is a circuit of depth at most k that computes f we prove a more general result for any two disjoint nonempty subsets a f 1 and b f 1 1 let ckw a b be the communication complexity of the karchmer wigderson game when x always lies in a and y in b we show that there is a circuit of depth ckw a b that outputs on every input from a and 1 on every input from b such a circuit is called a distinguisher for sets a b the proof is by induction on k ckw a b the base case k is trivial since this means the players do not have to communicate at all to agree on an answer say i hence xi yi for all x a y b which implies that either a xi for draft web draft 21 13 5 approaches using communication complexity 17 every x a and yi for every y b or b xi 1 for every x a and yi 1 for every y b in case a we can use the depth circuit xi and in case b we can use the circuit xi to distinguish a b for the inductive step suppose ckw a b k and at the first round player zero speaks then a is the disjoint union of two sets where ab is the set of inputs in a for which player zero sends bit b then ckw ab b k 1 for each b and the inductive hypothesis gives a circuit cb of depth at most k 1 that distinguishes ab b we claim that distinguishes a b note that it has depth at most k the reason is that y y 1 for every y b whereas for every x a x x since if x ab then cb x thus we have the following frontier frontier 5 show that some function f in p or even nexp has ckw f  log n log log n karchmer raz and wigderson describe a candidate function that may work it uses the fact a function on k bits has a truth table of size and that most functions on k bits are hard e g require circuit size  k circuit depth  k etc they define the function by assuming that part of the n bit input encodes a very hard function and this hard function is applied to the remaining input in a tree fashion for any function g 1 k 1 and 1 define g 1 k 1 as follows if 1 then g g otherwise express the input x 1 ks as x x x x 1 and define 1 2 3 xk where each i g xk g g 1 g 1 g 1 xk clearly if g can be computed in depth d then g can be computed in depth sd furthermore if one fails to see how one could reduce the depth for an arbitrary function now we describe the krw candidate function f 1 n 1 let k ilog n l and be the largest integer such that ks n 2 thus  log n for any n bit input x let gx be the function whose truth table is the first bits of x let x 2 be the string of the last ks bits of x then f x g s x 2 according to our earlier intuition when the first bits of x represent a really hard function as they must for many choices of the input then g s x should require depth  sk  n x 2 of course proving this seems difficult log log n this type of complexity questions whereby we are asking whether s instances of a problem are s times as hard as a single instance are called direct sum questions similar questions have been studied in a variety of computational models and sometimes counterintuitive results have been proven for them one example is that by a counting argument there exists an n n matrix a over n 1 such that the smallest circuit computing the linear function v 1 av for v 1 is of size  however computing this function on n instances vn can be done significantly faster than steps using fast matrix multiplication the current record is roughly o 38 web draft 21 18 13 5 approaches using communication complexity chapter notes and history shannon defined circuit complexity including monotone circuit complexity in the topic was studied in russia since the see trakhtenbrot for some references savage was the first to observe the close relationship between time required to decide a language on a tm and its circuit complexity and to suggest circuit lowerbounds as a way to separate complexity classes a burst of results in the such as the separation of p from and razborov s separation of monotone np from monotone p poly raised hopes that a resolution of p versus np might be near these hopes were dashed by razborov himself when he showed that his method of approximations was unlikely to apply to nonmonotone circuits later razborov and rudich formalized what they called natural proofs to show that all lines of attack considered up to that point were unlikely to work see chapter 22 our presentation in sections 13 2 and 13 3 closely follows that in boppana and sipser s excellent survey of circuit complexity which is still useful and current 15 years later it omits discussion of lowerbounds on algebraic circuits see for a recent result h astad s switching lemma is a stronger form of results from ajt83 the razborov smolensky method of using approximator polynomials is from strength ened in valiant s observations about superlinear circuit lowerbounds are from a paper and an unpublished manuscript lack of progress on this basic problem gets more embarrassing by the day the 4 5n o n lowerbound on general circuits is from lachish raz exercises 1 suppose that f is computable by an ac circuit c of depth d and size s prove that f is computable by an ac circuit ci of size 10s and depth d that does not contain not gates but instead has n additional inputs that are negations of the original n inputs 2 suppose that f is computable by an ac circuit c of depth d and size s prove that f is computable by an ci circuit of size 10s d and depth d where each gate has fanout 1 3 prove that for t n 2 n n n k use this to complete the proof of lemma 13 2 4 show that 5 identify reasons why the razborov smolensky method does not work when the circuit has modm gates where m is a composite number 6 show that representing the or of n variables x2 xn exactly with a polynomial over gf q where q is prime requires degree exactly n draft web draft 21 13 5 approaches using communication complexity 19 7 the karchmer wigderson game can be used to prove upperbounds and not just lowerbounds show using this game that parity and majority are in 8 show that if a language is computed by a polynomial size branching program of width 5 then it is in 9 prove valiant s lemma lemma 13 14 web draft 21 20 13 5 approaches using communication complexity draft web draft 21 chapter 14 algebraic computation models the turing machine model captures computations on bits equivalently integers but it does not always capture the spirit of algorithms which operate on say the real numbers r or complex num bers c such algorithms arise in a variety of applications such as numerical analysis computational geometry robotics and symbolic algebra a simple example is newton s method for finding roots of a given real valued function function f it iteratively produces a sequence of candidate solutions x2 r where xi 1 xi f xi f i xi under appropriate conditions this sequence can be shown to converge to a root of f of course a perfectly defensible position to take is that even the behavior of such algorithms should be studied using tms since they will be run on real life computers which represent real numbers using finite precision in this chapter though we take a different approach and study models which do allow arithmetic operations on real numbers or numbers from fields other than r such an idealized model may not be implementable strictly speaking but it provides a useful approximation to the asymptotic behavior as computers are allowed to use more and more precision in their computations furthermore one may be able to prove nontrivial lowerbounds for these models using techniques from well developed areas of mathematics such as algebraic geometry and topology by contrast boolean circuit lowerbounds have proven very difficult however coming up with a meaningful well behaved model of algebraic computation is not an easy task as the following example suggests example 14 1 pitfalls awaiting designers of such models a real number can encode infinite amount of information for example a single real number is enough to encode the answer to every instance of sat or any other language in general thus a model that can store any real number with infinite precision may not be realistic shamir has shown how to factor any integer n in poly log n time on a computer that can do real arithmetic with arbitrary precision the usual way to avoid this pitfall is to restrict the algorithms ability to access individual bits e g the machine may require more than polynomial time to extract a particular digit from 2 14 1 algebraic circuits a real number or sometimes as in case of algebraic computation trees it is ok to consider unrealistically powerful models since the goal is to prove nontrivial lowerbounds say superlinear or quadratic rather than arbitrary polynomial lowerbounds after all lowerbounds for unrealis tically powerful models will apply to more realistic and weaker models as well this chapter is a sketchy introduction to algebraic complexity it introduces three algebraic computation models algebraic circuits algebraic computation trees and algebraic turing ma chines the algebraic tm is closely related to the standard turing machine model and allows us to study similar questions for arbitrary fields including decidability and complexity that we earlier studied for strings over 1 we introduce an undecidable problem namely deciding membership in the mandelbrot set and and an np complete problem decision version of hilbert s nullstellensatz in this model 14 1 algebraic circuits an algebraic circuit over a field f is defined by analogy with a boolean circuit it consists of a directed acyclic graph the leaves are called input nodes and labeled x2 xn except these take values in a field f rather than boolean variables there are also special input nodes labeled with the constants 1 and 1 which are field elements each internal node called a gate is labeled with one of the arithmetic operations rather than with the boolean operations used in boolean circuits there is only output node we restrict indegree of each gate to 2 the size of the circuit is the number of gates in it one can also consider algebraic circuits that allow division at the gates one can also study circuits that have access to constants other than 1 though typically one assumes that this set is fixed and independent of the input size n finally as in the boolean case if each gate has outdegree 1 we call it an arithmetic formula a gate s operation consists of performing the operation it is labeled with on the numbers present on the incoming wires and then passing this output to all its outgoing wires after each gate has performed its operation an output appears on the circuit s lone output node thus the circuit may be viewed as a computing a function f x2 xn of the input variables and simple induction shows that this output function is a multivariate polynomial in x2 xn if we allow gates to also be labeled with the division operation denoted then the function is a rational function of xn in other words functions of the type x2 xn x1 xn where f2 are polynomials of course if the inputs come from a field such as r then rational functions can be used to approximate via taylor series expansion all smooth real valued functions as usual we are interested in the asymptotic size as a function of n of the smallest family of algebraic circuits that computes a family of polynomials fn where fn is a polynomial in n variables the exercises ask you to show that circuits over gf 2 with no are equivalent to boolean circuits and the same is true for circuits over any finite field so the case when f is infinite is usually of greatest interest course description cmpt machines and algorithms class time and location tuesday and thursday in arts brief learning objectives to learn the major methods for designing algorithms and the major mathematical models of computation student evaluation grading scheme assignments midterm final examination note all students must be properly registered in order to attend lectures and receive credit for this course final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text algorithm design by kleinberg and tardos recommended texts elements of the theory of computation papadimitriou lecture topics i algorithms o graph algorithms o greedy algorithms 7 o divide and conquer 5 5 o dynamic programming 6 6 o np completeness and computational intractability 8 o handling hard problems ii machines o finite automata o nondeterministic finite automata o regular languages o context free languages o pushdown automata o turing machines policies missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http www arts usask ca students transition tips php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar university of saskatchewan calendar academic courses policy academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behavior that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of complaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals section of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to register with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss course description cmpt advanced algorithms class time and location mwf 30 in geology brief learning objectives to learn and be able to apply network work flow algorithms to learn and be able to create computational geometry algorithms to learn and be able to apply and create some advanced graph algorithms to learn and be able to apply some parallel algorithms student evaluation grading scheme assignments project cmpt final examination cmpt 20 note all students must be properly registered in order to attend lectures and receive credit for this course final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information textbook computational geometry algorithms and applications ed de berg cheong van kreveld overmars this book is available as an online book through the library website outline augmenting algorithms o network flow algorithms o matching computational geometry o convex hulls o line segment intersection o triangulations o linear programming o orthogonal range searching o point location o voronoi diagrams o arrangements and duality o delaunay triangulations o geometric data structures graph algorithms o algorithms on special classes of graphs parallel algorithms student presentations policies missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam 2 final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term 2 and full year courses http artsandscience usask ca undergraduate advising strategies php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar university of saskatchewan calendar academic courses policy academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behavior that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of complaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals section of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to register with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss computational geometry introduction imagine you are walking on the campus of a university and suddenly you realize you have to make an urgent phone call there are many public phones on campus and of course you want to go to the nearest one but which one is the nearest it would be helpful to have a map on which you could look up the nearest public phone wherever on campus you are the map should show a subdivision of the campus into regions and for each region indicate the nearest public phone what would these regions look like and how could we compute them even though this is not such a terribly important issue it describes the basics of a fundamental geometric concept which plays a role in many applications the subdivision of the campus is a so called voronoi diagram and it will be studied in chapter in this book it can be used to model trading areas of different cities to guide robots and even to describe and simulate the growth of crystals computing a geometric structure like a voronoi diagram requires geometric algorithms such algorithms form the topic of this book a second example assume you located the closest public phone with a campus map in hand you will probably have little problem in getting to the phone along a reasonably short path without hitting walls and other objects but programming a robot to perform the same task is a lot more difcult again the heart of the problem is geometric given a collection of geometric obstacles we have to nd a short connection between two points avoiding collisions with the obstacles solving this so called motion planning problem is of crucial importance in robotics chapters and deal with geometric algorithms required for motion planning a third example assume you don t have one map but two one with a description of the various buildings including the public phones and one indicating the roads on the campus to plan a motion to the public phone we have to overlay these maps that is we have to combine the information in the two maps overlaying maps is one of the basic operations of geographic information systems it involves locating the position of objects from one map in the other computing the intersection of various features and so on chapter deals with this problem chapter computational geometry convex not convex these are just three examples of geometric problems requiring carefully de signed geometric algorithms for their solution in the the eld of compu tational geometry emerged dealing with such geometric problems it can be dened as the systematic study of algorithms and data structures for geometric objects with a focus on exact algorithms that are asymptotically fast many researchers were attracted by the challenges posed by the geometric problems the road from problem formulation to efcient and elegant solutions has often been long with many difcult and sub optimal intermediate results today there is a rich collection of geometric algorithms that are efcient and relatively easy to understand and implement this book describes the most important notions techniques algorithms and data structures from computational geometry in a way that we hope will be attractive to readers who are interested in applying results from computational geometry each chapter is motivated with a real computational problem that requires geometric algorithms for its solution to show the wide applicability of computational geometry the problems were taken from various application areas robotics computer graphics cad cam and geographic information systems you should not expect ready to implement software solutions for major problems in the application areas every chapter deals with a single concept in computational geometry the applications only serve to introduce and motivate the concepts they also illustrate the process of modeling an engineering problem and nding an exact solution an example convex hulls good solutions to algorithmic problems of a geometric nature are mostly based on two ingredients one is a thorough understanding of the geometric properties of the problem the other is a proper application of algorithmic techniques and data structures if you don t understand the geometry of the problem all the algorithms of the world won t help you to solve it efciently on the other hand even if you perfectly understand the geometry of the problem it is hard to solve it effectively if you don t know the right algorithmic techniques this book will give you a thorough understanding of the most important geometric concepts and algorithmic techniques to illustrate the issues that arise in developing a geometric algorithm this section deals with one of the rst problems that was studied in computational geometry the computation of planar convex hulls we ll skip the motivation for this problem here if you are interested you can read the introduction to chapter where we study convex hulls in dimensional space a subset s of the plane is called convex if and only if for any pair of points p q s the line segment pq is completely contained in s the convex hull ch s of a set s is the smallest convex set that contains s to be more precise it is the intersection of all convex sets that contain s we will study the problem of computing the convex hull of a nite set p of n points in the plane we can visualize what the convex hull looks like by a thought experiment imagine that the points are nails sticking out of the plane take an elastic rubber band hold it around the nails and let it go it will snap around the nails minimizing its length the area enclosed by the rubber band is the convex hull of p this leads to an alternative denition of the convex hull of a nite set p of points in the plane it is the unique convex polygon whose vertices are points from p and that contains all points of p of course we should prove rigorously that this is well dened that is that the polygon is unique and that the denition is equivalent to the one given earlier but let skip that in this introductory chapter how do we compute the convex hull before we can answer this question we must ask another question what does it mean to compute the convex hull as we have seen the convex hull of p is a convex polygon a natural way to represent a polygon is by listing its vertices in clockwise order starting with an arbitrary one so the problem we want to solve is this given a set p pn of points in the plane compute a list that contains those points from p that are the vertices of ch p listed in clockwise order section an example convex hulls input set of points output representation of the convex hull p5 p5 figure computing a convex hull the rst denition of convex hulls is of little help when we want to design an algorithm to compute the convex hull it talks about the intersection of all convex sets containing p of which there are innitely many the observation that ch p is a convex polygon is more useful let see what the edges of ch p are both endpoints p and q of such an edge are points of p and if we direct the line through p and q such that ch p lies to the right then all the points of p must lie to the right of this line the reverse is also true if all points of p p q lie to the right of the directed line through p and q then pq is an edge of ch p now that we understand the geometry of the problem a little bit better we can develop an algorithm we will describe it in a style of pseudocode we will use throughout this book algorithm slowconvexhull p input a set p of points in the plane output a list l containing the vertices of ch p in clockwise order e for all ordered pairs p q p p with p not equal to q do valid true chapter computational geometry for all points r p not equal to p or q do if r lies to the left of the directed line from p to q then valid false destination of origin of if valid then add the directed edge pq to e from the set e of edges construct a list l of vertices of ch p sorted in clockwise order two steps in the algorithm are perhaps not entirely clear the rst one is line how do we test whether a point lies to the left or to the right of a directed line this is one of the primitive operations required in most geometric algorithms throughout this book we assume that such operations are available it is clear that they can be performed in constant time so the actual implementation will not affect the asymptotic running time in order of magnitude this is not to say that such primitive operations are unimportant or trivial they are not easy to implement correctly and their implementation will affect the actual running time of the algorithm fortunately software libraries containing such primitive operations are nowadays available we conclude that we don t have to worry about the test in line we may assume that we have a function available performing the test for us in constant time the other step of the algorithm that requires some explanation is the last one in the loop of lines we determine the set e of convex hull edges from e we can construct the list l as follows the edges in e are directed so we can speak about the origin and the destination of an edge because the edges are directed such that the other points lie to their right the destination of an edge comes after its origin when the vertices are listed in clockwise order now remove an arbitrary edge from e put the origin of as the rst point into l and the destination as the second point find the edge in e whose origin is the destination of remove it from e and append its destination to l next nd the edge whose origin is the destination of remove it from e and append its destination to l we continue in this manner until there is only one edge left in e then we are done the destination of the remaining edge is necessarily the origin of which is already the rst point in l a simple implementation of this procedure takes o time this can easily be improved to o n log n but the time required for the rest of the algorithm dominates the total running time anyway analyzing the time complexity of slowconvexhull is easy we check n pairs of points for each pair we look at the n other points to see whether they all lie on the right side this will take o time in total the nal step takes o time so the total running time is o an algorithm with a cubic running time is too slow to be of practical use for anything but the smallest input sets the problem is that we did not use any clever algorithmic design techniques we just translated the geometric insight into an algorithm in a brute force manner but before we try to do better it is useful to make several observations about this algorithm we have been a bit careless when deriving the criterion of when a pair p q denes an edge of ch p a point r does not always lie to the right or to the left of the line through p and q it can also happen that it lies on this line what should we do then this is what we call a degenerate case ora degeneracy for short we prefer to ignore such situations when we rst think about a problem so that we don t get confused when we try to gure out the geometric properties of a problem however these situations do arise in practice for instance if we create the points by clicking on a screen with a mouse all points will have small integer coordinates and it is quite likely that we will create three points on a line to make the algorithm correct in the presence of degeneracies we must reformulate the criterion above as follows a directed edge p q is an edge of ch p if and only if all other points r p lie either strictly to the right of the directed line through p and q or they lie on the open line segment pq we assume that there are no coinciding points in p so we have to replace line of the algorithm by this more complicated test we have been ignoring another important issue that can inuence the correctness of the result of our algorithm we implicitly assumed that we can somehow test exactly whether a point lies to the right or to the left of a given line this is not necessarily true if the points are given in oating point coordinates and the computations are done using oating point arithmetic then there will be rounding errors that may distort the outcome of tests imagine that there are three points p q and r that are nearly collinear and that all other points lie far to the right of them our algorithm tests the pairs p q r q and p r since these points are nearly collinear it is possible that the rounding errors lead us to decide that r lies to the right of the line from p to q that p lies to the right of the line from r to q and that q lies to the right of the line from p to r of course this is geometrically impossible but the oating point arithmetic doesn t know that in this case the algorithm will accept all three edges even worse all three tests could give the opposite answer in which case the algorithm rejects all three edges leading to a gap in the boundary of the convex hull and this leads to a serious problem when we try to construct the sorted list of convex hull vertices in the last step of our algorithm this step assumes that there is exactly one edge starting in every convex hull vertex and exactly one edge ending there due to the rounding errors there can suddenly be two or no edges starting in vertex p this can cause the program implementing our simple algorithm to crash since the last step has not been designed to deal with such inconsistent data although we have proven the algorithm to be correct and to handle all special cases it is not robust small errors in the computations can make it fail in completely unexpected ways the problem is that we have proven the correctness assuming that we can compute exactly with real numbers section an example convex hulls q r p q r p we have designed our rst geometric algorithm it computes the convex hull of a set of points in the plane however it is quite slow its running time is o it deals with degenerate cases in an awkward way and it is not robust we should try to do better chapter computational geometry upper hull pn lower hull i points deleted to this end we apply a standard algorithmic design technique we will develop an incremental algorithm this means that we will add the points in p one by one updating our solution after each addition we give this incremental approach a geometric avor by adding the points from left to right so we rst sort the points by x coordinate obtaining a sorted sequence pn and then we add them in that order because we are working from left to right it would be convenient if the convex hull vertices were also ordered from left to right as they occur along the boundary but this is not the case therefore we rst compute only those convex hull vertices that lie on the upper hull which is the part of the convex hull running from the leftmost point to the rightmost point pn when the vertices are listed in clockwise order in other words the upper hull contains the convex hull edges bounding the convex hull from above in a second scan which is performed from right to left we compute the remaining part of the convex hull the lower hull the basic step in the incremental algorithm is the update of the upper hull after adding a point pi in other words given the upper hull of the points pi we have to compute the upper hull of pi this can be done as follows when we walk around the boundary of a polygon in clockwise order we make a turn at every vertex for an arbitrary polygon this can be both a right turn and a left turn but for a convex polygon every turn must be a right turn this suggests handling the addition of pi in the following way let lupper be a list that stores the upper vertices in left to right order we rst append pi to lupper this is correct because pi is the rightmost point of the ones added so far so it must be on the upper hull next we check whether the last three points in lupper make a right turn if this is the case there is nothing more to do lupper contains the vertices of the upper hull of pi and we can proceed to the next point pi but if the last three points make a left turn we have to delete the middle one from the upper hull in this case we are not nished yet it could be that the new last three points still do not make a right turn in which case we again have to delete the middle one we continue in this manner until the last three points make a right turn or until there are only two points left we now give the algorithm in pseudocode the pseudocode computes both the upper hull and the lower hull the latter is done by treating the points from right to left analogous to the computation of the upper hull algorithm convexhull p input a set p of points in the plane output a list containing the vertices of ch p in clockwise order sort the points by x coordinate resulting in a sequence pn put the points and in a list lupper with as the rst point for i to n do append pi to lupper while lupper contains more than two points and the last three points in lupper do not make a right turn do delete the middle of the last three points from lupper put the points pn and pn in a list llower with pn as the rst point for i n downto do append pi to llower while llower contains more than points and the last three points in llower do not make a right turn do delete the middle of the last three points from llower remove the rst and the last point from llower to avoid duplication of the points where the upper and lower hull meet append llower to lupper and call the resulting list l return l once again when we look closer we realize that the above algorithm is not correct without mentioning it we made the assumption that no two points have the same x coordinate if this assumption is not valid the order on x coordinate is not well dened fortunately this turns out not to be a serious problem we only have to generalize the ordering in a suitable way rather than using only the x coordinate of the points to dene the order we use the lexicographic order this means that we rst sort by x coordinate and if points have the same x coordinate we sort them by y coordinate another special case we have ignored is that the three points for which we have to determine whether they make a left or a right turn lie on a straight line in this case the middle point should not occur on the convex hull so collinear points must be treated as if they make a left turn in other words we should use a test that returns true if the three points make a right turn and false otherwise note that this is simpler than the test required in the previous algorithm when there were collinear points with these modications the algorithm correctly computes the convex hull the rst scan computes the upper hull which is now dened as the part of the convex hull running from the lexicographically smallest vertex to the lexico graphically largest vertex and the second scan computes the remaining part of the convex hull what does our algorithm do in the presence of rounding errors in the oating point arithmetic when such errors occur it can happen that a point is removed from the convex hull although it should be there or that a point inside the real convex hull is not removed but the structural integrity of the algorithm is unharmed it will compute a closed polygonal chain after all the output is a list of points that we can interpret as the clockwise listing of the vertices of a polygon and any three consecutive points form a right turn or because of the rounding errors they almost form a right turn moreover no point in p can be far outside the computed hull the only problem that can still occur is that when three points lie very close together a turn that is actually a sharp left turn can be interpretated as a right turn this might result in a dent in the resulting polygon a way out of this is to make sure that points in the input that are very close together are considered as being the same point for example by rounding hence although the result need not be exactly correct but then we cannot hope for an exact result if we use inexact arithmetic it does make sense for many applications this is good enough still it is wise to be careful section an example convex hulls not a right turn in the implementation of the basic test to avoid errors as much as possible chapter computational geometry pi we conclude with the following theorem theorem the convex hull of a set of n points in the plane can be computed in o n log n time proof we will prove the correctness of the computation of the upper hull the lower hull computation can be proved correct using similar arguments the proof is by induction on the number of point treated before the for loop starts the list lupper contains the points and which trivially form the upper hull of now suppose that lupper contains the upper hull vertices of pi and consider the addition of pi after the execution of the while loop and because of the induction hypothesis we know that the points in lupper form a chain that only makes right turns moreover the chain starts at the lexicographically smallest point of pi and ends at the lexicographically largest point namely pi if we can show that all points of pi that are not in lupper are below the chain then lupper contains the correct points by induction we know there is no point above the chain that we had before pi was added since the old chain lies below the new chain the only possibility for a point to lie above the new chain is if it lies in the vertical slab between pi and pi but this is not possible since such a point would be in between pi and pi in the lexicographical order you should verify that a similar argument holds if pi and pi or any other points have the same x coordinate to prove the time bound we note that sorting the points lexicographically can be done in o n log n time now consider the computation of the upper hull the for loop is executed a linear number of times the question that remains is how often the while loop inside it is executed for each execution of the for loop the while loop is executed at least once for any extra execution a point is deleted from the current hull as each point can be deleted only once during the construction of the upper hull the total number of extra executions over all for loops is bounded by n similarly the computation of the lower hull takes o n time due to the sorting step the total time required for computing the convex hull is o n log n the nal convex hull algorithm is simple to describe and easy to implement it only requires lexicographic sorting and a test whether three consecutive points make a right turn from the original denition of the problem it was far from obvious that such an easy and efcient solution would exist degeneracies and robustness as we have seen in the previous section the development of a geometric algorithm often goes through three phases in the rst phase we try to ignore everything that will clutter our understanding of the geometric concepts we are dealing with sometimes collinear points are a nuisance sometimes vertical line segments are when rst trying to design or understand an algorithm it is often helpful to ignore these degenerate cases in the second phase we have to adjust the algorithm designed in the rst phase to be correct in the presence of degenerate cases beginners tend to do this by adding a huge number of case distinctions to their algorithms in many situations there is a better way by considering the geometry of the problem again one can often integrate special cases with the general case for example in the convex hull algorithm we only had to use the lexicographical order instead of the order on x coordinate to deal with points with equal x coordinate for most algorithms in this book we have tried to take this integrated approach to deal with special cases still it is easier not to think about such cases upon rst reading only after understanding how the algorithm works in the general case should you think about degeneracies if you study the computational geometry literature you will nd that many authors ignore special cases often by formulating specic assumptions on the input for example in the convex hull problem we could have ignored special cases by simply stating that we assume that the input is such that no three points are collinear and no two points have the same x coordinate from a theoretical point of view such assumptions are usually justied the goal is then to establish the computational complexity of a problem and although it is tedious to work out the details degenerate cases can almost always be handled without increasing the asymptotic complexity of the algorithm but special cases denitely increase the complexity of the implementations most researchers in computational geometry today are aware that their general position assumptions are not satised in practical applications and that an integrated treatment of the special cases is normally the best way to handle them furthermore there are general techniques so called symbolic perturbation schemes that allow one to ignore special cases during the design and implementation and still have an algorithm that is correct in the presence of degeneracies the third phase is the actual implementation now one needs to think about the primitive operations like testing whether a point lies to the left to the right or on a directed line if you are lucky you have a geometric software library available that contains the operations you need otherwise you must implement them yourself another issue that arises in the implementation phase is that the assumption of doing exact arithmetic with real numbers breaks down and it is necessary to understand the consequences robustness problems are often a cause of frustration when implementing geometric algorithms solving robustness prob lems is not easy one solution is to use a package providing exact arithmetic using integers rationals or even algebraic numbers depending on the type of problem but this will be slow alternatively one can adapt the algorithm to detect inconsistencies and take appropriate actions to avoid crashing the program in this case it is not guaranteed that the algorithm produces the correct output and it is important to establish the exact properties that the output has this is what we did in the previous section when we developed the convex hull algorithm the result might not be a convex polygon but we know that the structure of the output is correct and that the output polygon is very close to the section degeneracies and robustness convex hull finally it is possible to predict based on the input the precision in chapter computational geometry the number representation required to solve the problem correctly which approach is best depends on the application if speed is not an issue exact arithmetic is preferred in other cases it is not so important that the result of the algorithm is precise for example when displaying the convex hull of a set of points it is most likely not noticeable when the polygon deviates slightly from the true convex hull in this case we can use a careful implementation based on oating point arithmetic in the rest of this book we focus on the design phase of geometric algorithms we won t say much about the problems that arise in the implementation phase application domains as indicated before we have chosen a motivating example application for every geometric concept algorithm or data structure introduced in this book most of the applications stem from the areas of computer graphics robotics geographic information systems and cad cam for those not familiar with these elds we give a brief description of the areas and indicate some of the geometric problems that arise in them computer graphics computer graphics is concerned with creating images of modeled scenes for display on a computer screen a printer or other output device the scenes vary from simple two dimensional drawings consisting of lines polygons and other primitive objects to realistic looking dimensional scenes including light sources textures and so on the latter type of scene can easily contain over a million polygons or curved surface patches because scenes consist of geometric objects geometric algorithms play an important role in computer graphics for dimensional graphics typical questions involve the intersection of certain primitives determining the primitive pointed to with the mouse or deter mining the subset of primitives that lie within a particular region chapters and describe techniques useful for some of these problems when dealing with dimensional problems the geometric questions be come more complex a crucial step in displaying a dimensional scene is hidden surface removal determine the part of a scene visible from a particular viewpoint or in other words discard the parts that lie behind other objects in chapter we study one approach to this problem to create realistic looking scenes we have to take light into account this creates many new problems such as the computation of shadows hence realistic image synthesis requires complicated display techniques like ray tracing and radiosity when dealing with moving objects and in virtual reality applications it is important to detect collisions between objects all these situations involve geometric problems robotics the eld of robotics studies the design and use of robots as robots are geometric objects that operate in a dimensional space the real world it is obvious that geometric problems arise at many places at the beginning of this chapter we already introduced the motion planning problem where a robot has to nd a path in an environment with obstacles in chapters and we study some simple cases of motion planning motion planning is one aspect of the more general problem of task planning one would like to give a robot high level tasks vacuum the room and let the robot gure out the best way to execute the task this involves planning motions planning the order in which to perform subtasks and so on other geometric problems occur in the design of robots and work cells in which the robot has to operate most industrial robots are robot arms with a xed base the parts operated on by the robot arm have to be supplied in such a way that the robot can easily grasp them some of the parts may have to be immobilized so that the robot can work on them they may also have to be turned to a known orientation before the robot can work on them these are all geometric problems sometimes with a kinematic component some of the algorithms described in this book are applicable in such problems for example the smallest enclosing disc problem treated in section can be used for optimal placement of robot arms section application domains geographic information systems a geographic information system or gis for short stores geographical data like the shape of countries the height of mountains the course of rivers the type of vegetation at different locations population density or rainfall they can also store human made structures such as cities roads railways electricity lines or gas pipes a gis can be used to extract information about certain regions and in particular to obtain information about the relation between different types of data for example a biologist may want to relate the average rainfall to the existence of certain plants and a civil engineer may need to query a gis to determine whether there are any gas pipes underneath a lot where excavation works are to be performed as most geographic information concerns properties of points and regions on the earth surface geometric problems occur in abundance here moreover the amount of data is so large that efcient algorithms are a must below we mention the gis related problems treated in this book a rst question is how to store geographic data suppose that we want to develop a car guidance system which shows the driver at any moment where she is this requires storing a huge map of roads and other data at every moment we have to be able to determine the position of the car on the map and to quickly select a small portion of the map for display on the on board computer efcient data structures are needed for these operations chapters and describe computational geometry solutions to these problems the information about the height in some mountainous terrain is usually only available at certain sample points for other positions we have to obtain the heights by interpolating between nearby sample points but which sample points should we choose chapter deals with this problem the combination of different types of data is one of the most important operations in a gis for example we may want to check which houses lie in chapter computational geometry caffeine a forest locate all bridges by checking where roads cross rivers or determine a good location for a new golf course by nding a slightly hilly rather cheap area not too far from a particular town a gis usually stores different types of data in separate maps to combine the data we have to overlay different maps chapter deals with a problem arising when we want to compute the overlay finally we mention the same example we gave at the beginning of this chapter the location of the nearest public phone or hospital or any other facility this requires the computation of a voronoi diagram a structure studied in detail in chapter cad cam computer aided design cad concerns itself with the design of products with a computer the products can vary from printed circuit boards machine parts or furniture to complete buildings in all cases the resulting product is a geometric entity and hence it is to be expected that all sorts of geometric problems appear indeed cad packages have to deal with intersec tions and unions of objects with decomposing objects and object boundaries into simpler shapes and with visualizing the designed products to decide whether a design meets the specications certain tests are needed often one does not need to build a prototype for these tests and a simulation sufces chapter deals with a problem arising in the simulation of heat emission by a printed circuit board once an object has been designed and tested it has to be manufactured computer aided manufacturing cam packages can be of assistance here cam involves many geometric problems chapter studies one of them a recent trend is design for assembly where assembly decisions are already taken into account during the design stage a cad system supporting this would allow designers to test their design for feasibility answering questions like can the product be built easily using a certain manufacturing process many of these questions require geometric algorithms to be answered other applications domains there are many more application domains where geometric problems occur and geometric algorithms and data structures can be used to solve them for example in molecular modeling molecules are often represented by collections of intersecting balls in space one ball for each atom typical questions are to compute the union of the atom balls to obtain the molecule surface or to compute where two molecules can touch each other another area is pattern recognition consider for example an optical char acter recognition system such a system scans a paper with text on it with the goal of recognizing the text characters a basic step is to match the image of a character against a collection of stored characters to nd the one that best ts it this leads to a geometric problem given two geometric objects determine how well they resemble each other even certain areas that at rst sight do not seem to be geometric can ben et from geometric algorithms because it is often possible to formulate non geometric problem in geometric terms in chapter for instance we will see how records in a database can be interpreted as points in a higher dimensional space and we will present a geometric data structure such that certain queries on the records can be answered efciently we hope that the above collection of geometric problems makes it clear that computational geometry plays a role in many different areas of computer sci ence the algorithms data structures and techniques described in this book will provide you with the tools needed to attack such geometric problems successfully section notes and comments notes and comments every chapter of this book ends with a section entitled notes and comments these sections indicate where the results described in the chapter came from indicate generalizations and improvements and provide references they can be skipped but do contain useful material for those who want to know more about the topic of the chapter more information can also be found in the handbook of computational geometry and the handbook of discrete and computational geometry in this chapter the geometric problem treated in detail was the computation of the convex hull of a set of points in the plane this is a classic topic in computational geometry and the amount of literature about it is huge the algorithm described in this chapter is commonly known as graham scan and is based on a modication by andrew of one of the earliest algorithms by graham this is only one of the many o n log n algorithms available for solving the problem a divide and conquer approach was given by preparata and hong also an incremental method exists that inserts the points one by one in o log n time per insertion overmars and van leeuwen generalized this to a method in which points could be both inserted and deleted in o n time other results on dynamic convex hulls were obtained by hershberger and suri chan and brodal and jacob even though an  n log n lower bound is known for the problem many authors have tried to improve the result this makes sense because in many applications the number of points that appear on the convex hull is relatively small while the lower bound result assumes that almost all points show up on the convex hull hence it is useful to look at algorithms whose running time depends on the complexity of the convex hull jarvis introduced a wrapping technique often referred to as jarvis march that computes the convex hull in o h n time where h is the complexity of the convex hull the same worst case performance is achieved by the algorithm of overmars and van leeuwen based on earlier work by bykat eddy and green and silverman this algorithm has the advantage that its expected running time is linear for many distributions of points finally kirkpatrick and seidel improved the result to o n log h and recently chan discovered a much simpler algorithm to achieve the same result chapter computational geometry the convex hull can be dened in any dimension convex hulls in dimensional space can still be computed in o n log n time as we will see in chapter for dimensions higher than however the complexity of the convex hull is no longer linear in the number of points see the notes and comments of chapter for more details in the past years a number of general methods for handling special cases have been suggested these symbolic perturbation schemes perturb the input in such a way that all degeneracies disappear however the perturbation is only done symbolically this technique was introduced by edelsbrunner and mu cke and later rened by yap and emiris and canny symbolic perturbation relieves the programmer of the burden of degeneracies but it has some drawbacks the use of a symbolic perturbation library slows down the algorithm and sometimes one needs to recover the real result from the perturbed result which is not always easy these drawbacks led burnikel et al to claim that it is both simpler in terms of programming effort and more efcient in terms of running time to deal directly with degenerate inputs robustness in geometric algorithms is a topic that has recently received a lot of interest most geometric comparisons can be formulated as computing the sign of some determinant a possible way to deal with the inexactness in oating point arithmetic when evaluating this sign is to choose a small threshold value  and to say that the determinant is zero when the outcome of the oating point computation is less than  when implemented naively this can lead to inconsistencies for instance for three points a b c we may decide that a b and b c but a c that cause the program to fail guibas et al showed that combining such an approach with interval arithmetic and backwards error analysis can give robust algorithms another option is to use exact arithmetic here one computes as many bits of the determinant as are needed to determine its sign this will slow down the computation but techniques have been developed to keep the performance penalty relatively small besides these general approaches there have been a number papers dealing with robust computation in specic problems 181 we gave a brief overview of the application domains from which we took our examples which serve to show the motivation behind the various geometric notions and algorithms studied in this book below are some references to textbooks you can consult if you want to know more about the application domains of course there are many more good books about these domains than the few we mention there is a large number of books on computer graphics the book by foley et al is very extensive and generally considered one of the best books on the topic other good books are the ones by shirley et al and watt an extensive overview of robotics and the motion planning problem can be found in the book of choset et al and in the somewhat older books of latombe and hopcroft schwartz and sharir more information on geometric aspects of robotics is provided by the book of selig there is a large collection of books about geographic information systems but most of them do not consider algorithmic issues in much detail some general textbooks are the ones by demers longley et al and worboys and duckham data structures for spatial data are described extensively in the book of samet the books by faux and pratt mortenson and hoffmann are good introductory texts on cad cam and geometric modeling exercises the convex hull of a set s is dened to be the intersection of all convex sets that contain s for the convex hull of a set of points it was indicated that the convex hull is the convex set with smallest perimeter we want to show that these are equivalent denitions a prove that the intersection of two convex sets is again convex this implies that the intersection of a nite family of convex sets is convex as well b prove that the smallest perimeter polygon p containing a set of points p is convex c prove that any convex set containing the set of points p contains the smallest perimeter polygon p let p be a set of points in the plane let p be the convex polygon whose vertices are points from p and that contains all points in p prove that this polygon p is uniquely dened and that it is the intersection of all convex sets containing p let e be an unsorted set of n segments that are the edges of a convex polygon describe an o n log n algorithm that computes from e a list containing all vertices of the polygon sorted in clockwise order for the convex hull algorithm we have to be able to test whether a point r lies left or right of the directed line through two points p and q let p px py q qx qy and r rx ry a show that the sign of the determinant section exercises px py d qx qy rx ry determines whether r lies left or right of the line b show that d in fact is twice the surface of the triangle determined by p q and r c why is this an attractive way to implement the basic test in algorithm convexhull give an argument for both integer and oating point coordinates chapter computational geometry verify that the algorithm convexhull with the indicated modications correctly computes the convex hull also of degenerate sets of points consider for example such nasty cases as a set of points that all lie on one vertical line in many situations we need to compute convex hulls of objects other than points a let s be a set of n line segments in the plane prove that the convex hull of s is exactly the same as the convex hull of the endpoints of the segments b let p be a non convex polygon describe an algorithm that computes the convex hull of p in o n time hint use a variant of algorithm convexhull where the vertices are not treated in lexicographical order but in some other order consider the following alternative approach to computing the convex hull of a set of points in the plane we start with the rightmost point this is the rst point of the convex hull now imagine that we start with a vertical line and rotate it clockwise until it hits another point this is the second point on the convex hull we continue rotating the line but this time around until we hit a point in this way we continue until we reach again a give pseudocode for this algorithm b what degenerate cases can occur and how can we deal with them c prove that the algorithm correctly computes the convex hull d prove that the algorithm can be implemented to run in time o n h where h is the complexity of the convex hull e what problems might occur when we deal with inexact oating point arithmetic the o n log n algorithm to compute the convex hull of a set of n points in the plane that was described in this chapter is based on the paradigm of incremental construction add the points one by one and update the convex hull after each addition in this exercise we shall develop an algorithm based on another paradigm namely divide and conquer a let and be two disjoint convex polygons with n vertices in total give an o n time algorithm that computes the convex hull of b use the algorithm from part a to develop an o n log n time divide and conquer algorithm to compute the convex hull of a set of n points in the plane suppose that we have a subroutine convexhull available for comput ing the convex hull of a set of points in the plane its output is a list of con vex hull vertices sorted in clockwise order now let s xn be a set of n numbers show that s can be sorted in o n time plus the time needed for one call to convexhull since the sorting problem has an  n log n lower bound this implies that the convex hull problem has an  n log n lower bound as well hence the algorithm presented in this chapter is asymptotically optimal let s be a set of n possibly intersecting unit circles in the plane we want to compute the convex hull of s a show that the boundary of the convex hull of s consists of straight line segments and pieces of circles in s b show that each circle can occur at most once on the boundary of the convex hull c let sl be the set of points that are the centers of the circles in s show that a circle in s appears on the boundary of the convex hull if and only if the center of the circle lies on the convex hull of sl d give an o n log n algorithm for computing the convex hull of s e give an o n log n algorithm for the case in which the circles in s have different radii section exercises line segment intersection thematic map overlay when you are visiting a country maps are an invaluable source of information they tell you where tourist attractions are located they indicate the roads and railway lines to get there they show small lakes and so on unfortunately they can also be a source of frustration as it is often difcult to nd the right information even when you know the approximate position of a small town it can still be difcult to spot it on the map to make maps more readable geographic information systems split them into several layers each layer is a thematic map that is it stores only one type of information thus there will figure cities rivers railroads and their overlay in western canada be a layer storing the roads a layer storing the cities a layer storing the rivers chapter line segment intersection grizzly bear and so on the theme of a layer can also be more abstract for instance there could be a layer for the population density for average precipitation habitat of the grizzly bear or for vegetation the type of geometric information stored in a layer can be very different the layer for a road map could store the roads as collections of line segments or curves perhaps the layer for cities could contain points labeled with city names and the layer for vegetation could store a subdivision of the map into regions labeled with the type of vegetation users of a geographic information system can select one of the thematic maps for display to nd a small town you would select the layer storing cities and you would not be distracted by information such as the names of rivers and lakes after you have spotted the town you probably want to know how to get there to this end geographic information systems allow users to view an overlay of several maps see figure using an overlay of the road map and the map storing cities you can now gure out how to get to the town when two or more thematic map layers are shown together intersections in the overlay are positions of special interest for example when viewing the overlay of the layer for the roads and the layer for the rivers it would be useful if the intersections were clearly marked in this example the two maps are basically networks and the intersections are points in other cases one is interested in the intersection of complete regions for instance geographers studying the climate could be interested in nding regions where there is pine forest and the annual precipitation is between mm and mm these regions are the intersections of the regions labeled pine forest in the vegetation map and the regions labeled in the precipitation map line segment intersection we rst study the simplest form of the map overlay problem where the two map layers are networks represented as collections of line segments for example a map layer storing roads railroads or rivers at a small scale note that curves can be approximated by a number of small segments at rst we won t be interested in the regions induced by these line segments later we shall look at the more complex situation where the maps are not just networks but subdivisions of the plane into regions that have an explicit meaning to solve the network overlay problem we rst have to state it in a geometric setting for the overlay of two networks the geometric situation is the following given two sets of line segments compute all intersections between a segment from one set and a segment from the other this problem specication is not quite precise enough yet as we didn t dene when two segments intersect in particular do two segments intersect when an endpoint of one of them lies on the other in other words we have to specify whether the input segments are open or closed to make this decision we should go back to the application the network overlay problem roads in a road map and rivers in a river map are represented by chains of segments so a crossing of a road and a river corresponds to the interior of one chain intersecting the interior of another chain this does not mean that there is an intersection between the interior of two segments the intersection point could happen to coincide with an endpoint of a segment of a chain in fact this situation is not uncommon because windy rivers are represented by many small segments and coordinates of endpoints may have been rounded when maps are digitized we conclude that we should dene the segments to be closed so that an endpoint of one segment lying on another segment counts as an intersection to simplify the description somewhat we shall put the segments from the two sets into one set and compute all intersections among the segments in that set this way we certainly nd all the intersections we want we may also nd intersections between segments from the same set actually we certainly will because in our application the segments from one set form a number of chains and we count coinciding endpoints as intersections these other intersections can be ltered out afterwards by simply checking for each reported intersection whether the two segments involved belong to the same set so our problem specication is as follows given a set s of n closed segments in the plane report all intersection points among the segments in s this doesn t seem like a challenging problem we can simply take each pair of segments compute whether they intersect and if so report their intersection point this brute force algorithm clearly requires o time in a sense this is optimal when each pair of segments intersects any algorithm must take  time because it has to report all intersections a similar example can be given when the overlay of two networks is considered in practical situations however most segments intersect no or only a few other segments so the total number of intersection points is much smaller than quadratic it would be nice to have an algorithm that is faster in such situations in other words we want an algorithm whose running time depends not only on the number of segments in the input but also on the number of intersection points such an algorithm is called an output sensitive algorithm the running time of the algorithm is sensitive to the size of the output we could also call such an algorithm intersection sensitive since the number of intersections is what determines the size of the output how can we avoid testing all pairs of segments for intersection here we must make use of the geometry of the situation segments that are close together are candidates for intersection unlike segments that are far apart below we shall see how we can use this observation to obtain an output sensitive algorithm for the line segment intersection problem let s sn be the set of segments for which we want to compute all intersections we want to avoid testing pairs of segments that are far apart but how can we do this let rst try to rule out an easy case dene the y interval of a segment to be its orthogonal projection onto the y axis when the y intervals of a pair of segments do not overlap we could say that they are far apart in the y direction then they cannot intersect hence we only need to test pairs of segments whose y intervals overlap that is pairs for which there exists a horizontal line that intersects both segments to nd these pairs we imagine section line segment intersection y x sweeping a line f downwards over the plane starting from a position above all chapter line segment intersection event point new neighbors segments while we sweep the imaginary line we keep track of all segments intersecting it the details of this will be explained later so that we can nd the pairs we need this type of algorithm is called a plane sweep algorithm and the line f is called the sweep line the status of the sweep line is the set of segments intersecting it the status changes while the sweep line moves downwards but not continuously only at particular points is an update of the status required we call these points the event points of the plane sweep algorithm in this algorithm the event points are the endpoints of the segments the moments at which the sweep line reaches an event point are the only moments when the algorithm actually does something it updates the status of the sweep line and performs some intersection tests in particular if the event point is the upper endpoint of a segment then a new segment starts intersecting the sweep line and must be added to the status this segment is tested for intersection against the ones already intersecting the sweep line if the event point is a lower endpoint a segment stops intersecting the sweep line and must be deleted from the status this way we only test pairs of segments for which there is a horizontal line that intersects both segments unfortunately this is not enough there are still situations where we test a quadratic number of pairs whereas there is only a small number of intersection points a simple example is a set of vertical segments that all intersect the x axis so the algorithm is not output sensitive the problem is that two segments that intersect the sweep line can still be far apart in the horizontal direction let order the segments from left to right as they intersect the sweep line to include the idea of being close in the horizontal direction we shall only test segments when they are adjacent in the horizontal ordering this means that we only test any new segment against two segments namely the ones immediately left and right of the upper endpoint later when the sweep line has moved downwards to another position a segment can become adjacent to other segments against which it will be tested our new strategy should be reected in the status of our algorithm the status now corresponds to the ordered sequence of segments intersecting the sweep line the new status not only changes at endpoints of segments it also changes at intersection points where the order of the intersected segments changes when this happens we must test the two segments that change position against their new neighbors this is a new type of event point before trying to turn these ideas into an efcient algorithm we should convince ourselves that the approach is correct we have reduced the number of pairs to be tested but do we still nd all intersections in other words if two segments si and j intersect is there always a position of the sweep line f where si and j are adjacent along f let rst ignore some nasty cases assume that no segment is horizontal that any two segments intersect in at most one point they do not overlap and that no three segments meet in a common point later we shall see that these cases are easy to handle but for now it is convenient to forget about them the intersections where an endpoint of a segment lies on another segment can easily be detected when the sweep line reaches the endpoint so the only question is whether intersections between the interiors of segments are always detected lemma let si and j be two non horizontal segments whose interiors intersect in a single point p and assume there is no third segment passing through p then there is an event point above p where si and j become adjacent and are tested for intersection proof let f be a horizontal line slightly above p if f is close enough to p then si and j must be adjacent along f to be precise we should take f such that there is no event point on f nor in between f and the horizontal line through p in other words there is a position of the sweep line where si and j are adjacent on the other hand si and j are not yet adjacent when the algorithm starts because the sweep line starts above all line segments and the status is empty hence there must be an event point q where si and j become adjacent and are tested for intersection section line segment intersection si j f p so our approach is correct at least when we forget about the nasty cases mentioned earlier now we can proceed with the development of the plane sweep algorithm let briey recap the overall approach we imagine moving a horizontal sweep line f downwards over the plane the sweep line halts at certain event points in our case these are the endpoints of the segments which we know beforehand and the intersection points which are computed on the y while the sweep line moves we maintain the ordered sequence of segments intersected by it when the sweep line halts at an event point the sequence of segments changes and depending on the type of event point we have to take several actions to update the status and detect intersections when the event point is the upper endpoint of a segment there is a new segment intersecting the sweep line this segment must be tested for intersection against its two neighbors along the sweep line only intersection points below the sweep line are important the ones above the sweep line have been detected already for example if segments si and sk are adjacent on the sweep line and a new upper endpoint of a segment j appears in between then we have to test j for intersection with si and sk if we nd an intersection below the sweep line we have found a new event point after the upper endpoint is handled we continue to the next event point when the event point is an intersection the two segments that intersect change their order each of them gets at most one new neighbor against which it is tested for intersection again only intersections below the sweep line are still interesting suppose that four segments j sk sl and sm appear in this order on the sweep line when the intersection point of sk and sl is reached then sk and sl switch position and we must test sl and j for intersection below the sweep line and also sk and sm the new intersections that we nd are of course also event points for the algorithm note however that it is possible that these events have already been detected earlier namely if a pair becoming adjacent si sk j f intersection detected has been adjacent before chapter line segment intersection when the event point is the lower endpoint of a segment its two neighbors now become adjacent and must be tested for intersection if they intersect below the sweep line then their intersection point is an event point again this event could have been detected already assume three segments sk sl and sm appear in this order on the sweep line when the lower endpoint of sl is encountered then sk and sm will become adjacent and we test them for intersection after we have swept the whole plane more precisely after we have treated the last event point we have computed all intersection points this is guaran teed by the following invariant which holds at any time during the plane sweep all intersection points above the sweep line have been computed correctly after this sketch of the algorithm it time to go into more detail it also time to look at the degenerate cases that can arise like three or more segments meeting in a point we should rst specify what we expect from the algorithm in these cases we could require the algorithm to simply report each intersection point once but it seems more useful if it reports for each intersection point a list of segments that pass through it or have it as an endpoint there is another special case for which we should dene the required output more carefully namely that of two partially overlapping segments but for simplicity we shall ignore this case in the rest of this section we start by describing the data structures the algorithm uses first of all we need a data structure called the event queue that stores the events we denote the event queue by q we need an operation that removes the next event that will occur from q and returns it so that it can be treated this event is the highest event below the sweep line if two event points have the same y coordinate then the one with smaller x coordinate will be returned in other words event points on the same horizontal line are treated from left to right this implies that we should consider the left endpoint of a horizontal segment to be its upper endpoint and its right endpoint to be its lower endpoint you can also think about our convention as follows instead of having a horizontal sweep line imagine it is sloping just a tiny bit upward as a result the sweep f line reaches the left endpoint of a horizontal segment just before reaching the right endpoint the event queue must allow insertions because new events will be computed on the y notice that two event points can coincide for example the upper endpoints of two distinct segments may coincide it is convenient to treat this as one event point hence an insertion must be able to check whether an event is already present in q we implement the event queue as follows dene an order on the event points that represents the order in which they will be handled hence if p and q are two event points then we have p q if and only if py qy holds or py qy and px qx holds we store the event points in a balanced binary search tree ordered according to with each event point p in q we will store the segments starting at p that is the segments whose upper endpoint is p this information will be needed to handle the event both operations fetching the next event and inserting an event take o log m time where m is the number of events in q we do not use a heap to implement the event queue because we have to be able to test whether a given event is already present in q second we need to maintain the status of the algorithm this is the ordered sequence of segments intersecting the sweep line the status structure denoted by t is used to access the neighbors of a given segment so that they can be tested for intersection with the status structure must be dynamic as segments start or stop to intersect the sweep line they must be inserted into or deleted from the structure because there is a well dened order on the segments in the status structure we can use a balanced binary search tree as status structure when you are only used to binary search trees that store numbers this may be surprising but binary search trees can store any set of elements as long as there is an order on the elements in more detail we store the segments intersecting the sweep line ordered in the leaves of a balanced binary search tree t the left to right order of the segments along the sweep line corresponds to the left to right order of the leaves in t we must also store information in the internal nodes to guide the search down the tree to the leaves at each internal node we store the segment from the rightmost leaf in its left subtree alternatively we could store the segments only in interior nodes this will save some storage however it is conceptually simpler to think about the segments in interior nodes as values to guide the search not as data items storing the segments in the leaves also makes some algorithms simpler to describe suppose we search in t for the segment immediately to the left of some point p that lies on the sweep line at each internal node  we test whether p lies left or right of the segment stored at  depending on the outcome we descend to the left or right subtree of  eventually ending up in a leaf either this leaf or the leaf immediately to the left of it stores the segment we are searching for in a similar way we can nd the segment immediately to the right of p or the segments containing p it follows that each update and neighbor search operation takes o log n time the event queue q and the status structure t are the only two data structures we need the global algorithm can now be described as follows algorithm findintersections s input a set s of line segments in the plane output the set of intersection points among the segments in s with for each intersection point the segments that contain it initialize an empty event queue q next insert the segment endpoints into q when an upper endpoint is inserted the corresponding segment should be stored with it initialize an empty status structure t while q is not empty do determine the next event point p in q and delete it handleeventpoint p we have already seen how events are handled at endpoints of segments we have to insert or delete segments from the status structure t and at intersection points we have to change the order of two segments in both cases we also section line segment intersection have to do intersection tests between segments that become neighbors after the chapter line segment intersection event in degenerate cases where several segments are involved in one event point the details are a little bit more tricky the next procedure describes how to handle event points correctly it is illustrated in figure figure an event point and the changes in the status structure handleeventpoint p let u p be the set of segments whose upper endpoint is p these segments are stored with the event point p for horizontal segments the upper endpoint is by denition the left endpoint find all segments stored in t that contain p they are adjacent in t let l p denote the subset of segments found whose lower endpoint is p and let c p denote the subset of segments found that contain p in their interior if l p u p c p contains more than one segment then report p as an intersection together with l p u p and c p delete the segments in l p c p from t insert the segments in u p c p into t the order of the segments in t should correspond to the order in which they are intersected by a sweep line just below p if there is a horizontal segment it comes last among all segments containing p deleting and re inserting the segments of c p reverses their order if u p c p then let sl and sr be the left and right neighbors of p in t findnewevent sl sr p else let be the leftmost segment of u p c p in t let sl be the left neighbor of in t findnewevent sl p let be the rightmost segment of u p c p in t let sr be the right neighbor of in t findnewevent sr p note that in lines we assume that sl and sr actually exist if they do not exist the corresponding steps should obviously not be performed the procedures for nding the new intersections are easy they simply test two segments for intersection the only thing we need to be careful about is when we nd an intersection whether this intersection has already been handled earlier or not when there are no horizontal segments then the intersection has not been handled yet when the intersection point lies below the sweep line but how should we deal with horizontal segments recall our convention that events with the same y coordinate are treated from left to right this implies that we are still interested in intersection points lying to the right of the current event point hence the procedure findnewevent is dened as follows findnewevent sl sr p if sl and sr intersect below the sweep line or on it and to the right of the current event point p and the intersection is not yet present as an event in q then insert the intersection point as an event into q section line segment intersection what about the correctness of our algorithm it is clear that findintersec tions only reports true intersection points but does it nd all of them the next lemma states that this is indeed the case lemma algorithm findintersections computes all intersection points and the segments that contain it correctly proof recall that the priority of an event is given by its y coordinate and that when two events have the same y coordinate the one with smaller x coordinate is given higher priority we shall prove the lemma by induction on the priority of the event points let p be an intersection point and assume that all intersection points q with a higher priority have been computed correctly we shall prove that p and the segments that contain p are computed correctly let u p be the set of segments that have p as their upper endpoint or for horizontal segments their left endpoint let l p be the set of segments having p as their lower endpoint or for horizontal segments their right endpoint and let c p be the set of segments having p in their interior first assume that p is an endpoint of one or more of the segments in that case p is stored in the event queue q at the start of the algorithm the segments from u p are stored with p so they will be found the segments from l p and c p are stored in t when p is handled so they will be found in line of handleeventpoint hence p and all the segments involved are determined correctly when p is an endpoint of one or more of the segments now assume that p is not an endpoint of a segment all we need to show is that p will be inserted into q at some moment note that all segments that are involved have p in their interior order these segments by angle around p and let si and j be two neighboring segments following the proof of lemma we see that there is an event point with a higher priority than p such that si and j become adjacent when q is passed in lemma we assumed for simplicity that si and j are non horizontal but it is straightforward to adapt the proof for chapter line segment intersection horizontal segments by induction the event point q was handled correctly which means that p is detected and stored into q so we have a correct algorithm but did we succeed in developing an output sensitive algorithm the answer is yes the running time of the algorithm is o n k log n where k is the size of the output the following lemma states an even stronger result the running time is o n i log n where i is the number of intersections this is stronger because for one intersection point the output can consist of a large number of segments namely in the case where many segments intersect in a common point lemma the running time of algorithm findintersections for a set s of n line segments in the plane is o n log n i log n where i is the number of intersection points of segments in s proof the algorithm starts by constructing the event queue on the segment endpoints because we implemented the event queue as a balanced binary search tree this takes o n log n time initializing the status structure takes constant time then the plane sweep starts and all the events are handled to handle an event we perform three operations on the event queue q the event itself is deleted from q in line of findintersections and there can be one or two calls to findnewevent which may cause at most two new events to be inserted into q deletions and insertions on q take o log n time each we also perform operations insertions deletions and neighbor nding on the status structure t which take o log n time each the number of operations is linear in the number m p card l p u p c p of segments that are involved in the event if we denote the sum of all m p over all event points p by m the running time of the algorithm is o m log n it is clear that m o n k where k is the size of the output after all whenever m p we report all segments involved in the event and the only events involving one segment are the endpoints of segments but we want to prove that m o n i where i is the number of intersection points to show this we will interpret the set of segments as a planar graph embedded in the plane if you are not familiar with planar graph terminology you should read the rst paragraphs of section rst its vertices are the endpoints of segments and intersection points of segments and its edges are the pieces of the segments connecting vertices consider an event point p it is a vertex of the graph and m p is bounded by the degree of the vertex consequently m is bounded by the sum of the degrees of all vertices of our graph every edge of the graph contributes one to the degree of exactly two vertices its endpoints so m is bounded by where ne is the number of edges of the graph let bound ne in terms of n and i by denition nv the number of vertices is at most i it is well known that in planar graphs ne o nv which proves our claim but for completeness let us give the argument here every face of the planar graph is bounded by at least three edges provided that there are at least three segments and an edge can bound at most two different faces therefore n f the number of faces is at most we now use euler formula which states that for any planar graph with nv vertices ne edges and n f faces the following relation holds nv ne n f section the doubly connected edge list equality holds if and only if the graph is connected plugging the bounds on nv and n f into this formula we get i ne i ne so ne and m and the bound on the running time follows we still have to analyze the other complexity aspect the amount of storage used by the algorithm the tree t stores a segment at most once so it uses o n storage the size of q can be larger however the algorithm inserts intersection points in q when they are detected and it removes them when they are handled when it takes a long time before intersections are handled it could happen that q gets very large of course its size is always bounded by o n i but it would be better if the working storage were always linear there is a relatively simple way to achieve this only store intersection points of pairs of segments that are currently adjacent on the sweep line the algorithm given above also stores intersection points of segments that have been horizontally adjacent but aren t anymore by storing only intersections among adjacent segments the number of event points in q is never more than linear the modication required in the algorithm is that the intersection point of two segments must be deleted when they stop being adjacent these segments must become adjacent again before the intersection point is reached so the intersection point will still be reported correctly the total time taken by the algorithm remains o n log n i log n we obtain the following theorem theorem let s be a set of n line segments in the plane all intersection points in s with for each intersection point the segments involved in it can be reported in o n log n i log n time and o n space where i is the number of intersection points the doubly connected edge list we have solved the easiest case of the map overlay problem where the two maps are networks represented as collections of line segments in general maps have a more complicated structure they are subdivisions of the plane into labeled regions a thematic map of forests in canada for instance would be a subdivision of canada into regions with labels such as pine deciduous birch and mixed before we can give an algorithm for computing the overlay of two subdivi sions we must develop a suitable representation for a subdivision storing a subdivision as a collection of line segments is not such a good idea operations like reporting the boundary of a region would be rather complicated it is better chapter line segment intersection figure types of forest in canada edge vertex face disconnected subdivision to incorporate structural topological information which segments bound a given region which regions are adjacent and so on the maps we consider are planar subdivisions induced by planar embeddings of graphs such a subdivision is connected if the underlying graph is connected the embedding of a node of the graph is called a vertex and the embedding of an arc is called an edge we only consider embeddings where every edge is a straight line segment in principle edges in a subdivision need not be straight a subdivision need not even be a planar embedding of a graph as it may have unbounded edges in this section however we don t consider such more general subdivisions we consider an edge to be open that is its endpoints which are vertices of the subdivision are not part of it a face of the subdivision is a maximal connected subset of the plane that doesn t contain a point on an edge or a vertex thus a face is an open polygonal region whose boundary is formed by edges and vertices from the subdivision the complexity of a subdivision is dened as the sum of the number of vertices the number of edges and the number of faces it consists of if a vertex is the endpoint of an edge then we say that the vertex and the edge are incident similarly a face and an edge on its boundary are incident and a face and a vertex of its boundary are incident what should we require from a representation of a subdivision an opera tion one could ask for is to determine the face containing a given point this is denitely useful in some applications indeed in a later chapter we shall design a data structure for this but it is a bit too much to ask from a basic representation the things we can ask for should be more local for example it is reasonable to require that we can walk around the boundary of a given face or that we can access one face from an adjacent one if we are given a common edge another operation that could be useful is to visit all the edges around a given vertex the representation that we shall discuss supports these operations it is called the doubly connected edge list a doubly connected edge list contains a record for each face edge and vertex of the subdivision besides the geometric and topological information to be described shortly each record may also store additional information for instance if the subdivision represents a thematic map for vegetation the doubly connected edge list would store in each face record the type of vegetation of the corresponding region the additional information is also called attribute information the geometric and topological information stored in the doubly connected edge list should enable us to perform the basic operations mentioned earlier to be able to walk around a face in counterclockwise order we store a pointer from each edge to the next it can also come in handy to walk around a face the other way so we also store a pointer to the previous edge an edge usually bounds two faces so we need two pairs of pointers for it it is convenient to view the different sides of an edge as two distinct half edges so that we have a unique next half edge and previous half edge for every half edge this also means that a half edge bounds only one face the two half edges we get for a given edge are called twins dening the next half edge of a given half edge with respect to a counterclockwise traversal of a face induces an orientation on each half edge it is oriented such that the face that it bounds lies to its left for an observer walking along the edge because half edges are oriented we can speak of the origin and the destination of a half edge if a half edge e has v as its origin and w as its destination then its twin twin e has w as its origin and v as its destination to reach the boundary of a face we just need to store one pointer in the face record to an arbitrary half edge bounding the face starting from that half edge we can step from each half edge to the next and walk around the face what we just said does not quite hold for the boundaries of holes in a face if they are traversed in counterclockwise order then the face lies to the right it will be convenient to orient half edges such that their face always lies to the same side so we change the direction of traversal for the boundary of a hole to clockwise now a face always lies to the left of any half edge on its boundary another consequence is that twin half edges always have opposite orientations the presence of holes in a face also means that one pointer from the face to an arbitrary half edge on its boundary is not enough to visit the whole boundary we need a pointer to a half edge in every boundary component if a face has isolated vertices that don t have any incident edge we can store pointers to them as well for simplicity we ll ignore this case let summarize the doubly connected edge list consists of three collections of records one for the vertices one for the faces and one for the half edges these records store the following geometric and topological information the vertex record of a vertex v stores the coordinates of v in a eld called coordinates v it also stores a pointer incidentedge v to an arbitrary half edge that has v as its origin the face record of a face f stores a pointer outercomponent f to some half edge on its outer boundary for the unbounded face this pointer is nil it also stores a list innercomponents f which contains for each hole in section the doubly connected edge list v twin e the face a pointer to some half edge on the boundary of the hole chapter line segment intersection origin e incidentface e the half edge record of a half edge e stores a pointer origin e to its origin a pointer twin e to its twin half edge and a pointer incidentface e to the face that it bounds we don t need to store the destination of an edge because it is equal to origin twin e the origin is chosen such that incidentface e lies to the left of e when it is traversed from origin to destination the half edge record also stores pointers next e and prev e to the next and previous edge on the boundary of incidentface e thus next e is the unique half edge on the boundary of incidentface e that has the destination of e as its origin and prev e is the unique half edge on the boundary of incidentface e that has origin e as its destination a constant amount of information is used for each vertex and edge a face may require more storage since the list innercomponents f has as many elements as there are holes in the face because any half edge is pointed to at most once from all innercomponents f lists together we conclude that the amount of storage is linear in the complexity of the subdivision an example of a doubly connected edge list for a simple subdivision is given below the two half edges corresponding to an edge ei are labeled ei and ei vertex coordinates incidentedge face outercomponent innercomponents nil nil half edge origin twin incidentface next prev e3 e4 the information stored in the doubly connected edge list is enough to perform the basic operations for example we can walk around the outer boundary of a given face f by following next e pointers starting from the half edge outercomponent f we can also visit all edges incident to a vertex v it is a good exercise to gure out for yourself how to do this we described a fairly general version of the doubly connected edge list in applications where the vertices carry no attribute information we could store their coordinates directly in the origin eld of the edge there is no strict need for a separate type of vertex record even more important is to realize that in many applications the faces of the subdivision carry no interesting meaning think of the network of rivers or roads that we looked at before if that is the case we can completely forget about the face records and the incidentface eld of half edges as we will see the algorithm of the next section doesn t need these elds and is actually simpler to implement if we don t need to update them some implementations of doubly connected edge lists may also insist that the graph formed by the vertices and edges of the subdivision be connected this can always be achieved by introducing dummy edges and has two advantages firstly a simple graph transversal can be used to visit all half edges and secondly the innercomponents list for faces is not necessary computing the overlay of two subdivisions now that we have designed a good representation of a subdivision we can tackle the general map overlay problem we dene the overlay of two subdivisions and to be the subdivision o such that there is a face f in o if and only if there are faces in and in such that f is a maximal connected subset of this sounds more complicated than it is what it means is that the overlay is the subdivision of the plane induced by the edges from and figure illustrates this the general map overlay problem is to compute a doubly connected edge list for o given the doubly connected edge lists of and we require that each face in o be labeled with the labels of the faces in and that contain it this way we have access to the attribute information stored for these faces in an overlay of a vegetation map and a precipitation map this would mean that we know for each region in the overlay the type of vegetation and the amount of precipitation section computing the overlay of two subdivisions figure overlaying two subdivisions let rst see how much information from the doubly connected edge lists for and we can re use in the doubly connected edge list for o consider the network of edges and vertices of this network is cut into pieces by the edges of these pieces are for a large part re usable only the edges that have been cut by the edges of should be renewed but does this also chapter line segment intersection hold for the half edge records in the doubly connected edge list that correspond to the pieces if the orientation of a half edge would change we would still have to change the information in these records fortunately this is not the case the half edges are oriented such that the face that they bound lies to the left the shape of the face may change in the overlay but it will remain to the same side of the half edge hence we can re use half edge records corresponding to edges that are not intersected by edges from the other map stated differently the only half edge records in the doubly connected edge list for o that we cannot borrow from or are the ones that are incident to an intersection between edges from different maps this suggests the following approach first copy the doubly connected edge lists of and into one new doubly connected edge list the new doubly connected edge list is not a valid doubly connected edge list of course in the sense that it does not yet represent a planar subdivision this is the task of the overlay algorithm it must transform the doubly connected edge list into a valid doubly connected edge list for o by computing the intersections between the two networks of edges and linking together the appropriate parts of the two doubly connected edge lists we did not talk about the new face records yet the information for these records is more difcult to compute so we leave this for later we rst describe in a little more detail how the vertex and half edge records of the doubly connected edge list for o are computed our algorithm is based on the plane sweep algorithm of section for com puting the intersections in a set of line segments we run this algorithm on the set of segments that is the union of the sets of edges of the two subdivisions and here we consider the edges to be closed recall that the algorithm is supported by two data structures an event queue q which stores the event points and the status structure t which is a balanced binary search tree storing the segments intersecting the sweep line ordered from left to right we now also maintain a doubly connected edge list d initially d contains a copy of the doubly connected edge list for and a copy of the doubly connected edge list for during the plane sweep we shall transform d to a correct doubly connected edge list for o that is to say as far as the vertex and half edge records are concerned the face information will be computed later we keep cross pointers between the edges in the status structure t and the half edge records in d that correspond to them this way we can access the part of d that needs to be changed when we encounter an intersection point the invariant that we maintain is that at any time during the sweep the part of the overlay above the sweep line has been computed correctly now let consider what we must do when we reach an event point first of all we update t and q as in the line segment intersection algorithm if the event involves only edges from one of the two subdivisions this is all the event point is a vertex that can be re used if the event involves edges from both subdivisions we must make local changes to d to link the doubly connected edge lists of the two original subdivisions at the intersection point this is tedious but not difcult the geometric situation and the two doubly connected edge lists before handling the intersection the doubly connected edge list after handling the intersection section computing the overlay of two subdivisions we describe the details for one of the possible cases namely when an edge e of passes through a vertex v of see figure the edge e must be replaced by two edges denoted e and e in the doubly connected edge list the two half edges for e must become four we create two new half edge records both with v as the origin the two existing half edges for e keep the endpoints of e as their origin as shown in figure then we pair up the existing half edges with the new half edges by setting their twin pointers so e is represented by one new and one existing half edge and the same holds for e now we must set a number of prev and next pointers we rst deal with the situation around the endpoints of e later we ll worry about the situation around v the next pointers of the two new half edges each copy the next pointer of the old half edge that is not its twin the half edges to which these pointers point must also update their prev pointer and set it to the new half edges the correctness of this step can be veried best by looking at a gure it remains to correct the situation around vertex v we must set the next and prev pointers of the four half edges representing e and e and of the four half edges incident from to v we locate these four half edges from by testing where e and e should be in the cyclic order of the edges around vertex v there are four pairs of half edges that become linked by a next pointer from the one and a prev pointer from the other consider the half edge for e that has v as its destination it must be linked to the rst half edge seen clockwise from e with v as its origin the half edge for e with v as its origin must be linked to the rst counterclockwise half edge with v as its destination the same statements hold for e most of the steps in the description above take only constant time only locating where e and e appear in the cyclic order around v may take longer it will take time linear in the degree of v the other cases that can arise crossings of two edges from different maps and coinciding vertices are not more difcult than the case we just discussed these cases also take time o m where m is the number of edges incident to the event point this means that updating d does not increase the running time of the line segment intersection algorithm asymptotically notice that every intersection that we nd is a vertex of the overlay it follows that the vertex records and the half edge records of the doubly connected edge list for o can be computed in o n log n k log n time where n denotes the sum of the complexities of and and k is the figure an edge of one subdivision passing through a vertex of the other rst clockwise half edge from e with v as its origin complexity of the overlay chapter line segment intersection after the elds involving vertex and half edge records have been set it remains to compute the information about the faces of o more precisely we have to create a face record for each face f in o we have to make outercomponent f point to a half edge on the outer boundary of f and we have to make a list innercomponents f of pointers to half edges on the bound aries of the holes inside f furthermore we must set the incidentface elds of the half edges on the boundary of f so that they point to the face record of f finally each of the new faces must be labeled with the names of the faces in the old subdivisions that contain it how many face records will there be well except for the unbounded face every face has a unique outer boundary so the number of face records we have to create is equal to the number of outer boundaries plus one from the part of the doubly connected edge list we have constructed so far we can easily extract all boundary cycles but how do we know whether a cycle is an outer boundary or the boundary of a hole in a face this can be decided by looking at the leftmost vertex v of the cycle or in case of ties at the lowest of the leftmost ones recall that half edges are directed in such a way that their incident face locally lies to the left consider the two half edges of the cycle that are incident to v because we know that the incident face lies to the left we can compute the angle these two half edges make inside the incident face if this angle is smaller than then the cycle is an outer boundary and otherwise it is the boundary of a hole this property holds for the leftmost vertex of a cycle but not necessarily for other vertices of that cycle to decide which boundary cycles bound the same face we construct a graph g for every boundary cycle inner and outer there is a node in g there is also one node for the imaginary outer boundary of the unbounded face there is an arc between two cycles if and only if one of the cycles is the boundary of a hole and the other cycle has a half edge immediately to the left of the leftmost vertex of that hole cycle if there is no half edge to the left of the leftmost vertex of a cycle then the node representing the cycle is linked to the node of the unbounded face figure gives an example the dotted segments in the gure indicate the linking of the hole cycles to other cycles the graph corresponding to the subdivision is also shown in the gure the hole cycles are shown as single circles and the outer boundary cycles are shown as double circles observe that and are in the same connected component as this indicates that and are hole cycles in the face whose outer boundary is if there is only one hole in a face f then the graph g links the boundary cycle of the hole to the outer boundary of f in general this need not be the case a hole can also be linked to another hole as you can see in figure this hole which lies in the same face f may be linked to the outer boundary of f or it may be linked to yet another hole but eventually we must end up linking a hole to the outer boundary as the next lemma shows lemma each connected component of the graph g corresponds exactly to the set of cycles incident to one face proof consider a cycle c bounding a hole in a face f because f lies locally to the left of the leftmost vertex of c c must be linked to another cycle that also section computing the overlay of two subdivisions g c4 figure a subdivision and the corresponding graph g bounds f it follows that cycles in the same connected component of g bound the same face to nish the proof we show that every cycle bounding a hole in f is in the same connected component as the outer boundary of f suppose there is a cycle for which this is not the case let c be the leftmost such cycle that is the one whose the leftmost vertex is leftmost by denition there is an arc between the c and another cycle c that lies partly to the left of the leftmost vertex of c hence c is in the same connected component as c which is not the component of the outer boundary of f this contradicts the denition of c lemma shows that once we have the graph g we can create a face record for every component then we can set the incidentface pointers of the half edges that bound each face f and we can construct the list innercomponents f and the set outercomponent f how can we construct g recall that in the plane sweep algorithm for line segment intersection we always looked for the segments immediately to the left of an event point they had to be tested for intersection against the leftmost edge through the event point hence the information we need to construct g is determined during the plane sweep so to construct g we rst make a node for every cycle to nd the arcs of g we consider the leftmost vertex v of every cycle bounding a hole if e is the half edge immediately left of v then we add an arc between the two nodes in g representing the cycle containing e and the hole cycle of which v is the leftmost vertex to nd these nodes in g efciently we need pointers from every half edge record to the node in g representing the cycle it is in so the face information of the doubly connected edge list can be set in o n k additional new arc time after the plane sweep chapter line segment intersection one thing remains each face f in the overlay must be labeled with the names of the faces in the old subdivisions that contained it to nd these faces consider an arbitrary vertex v of f if v is the intersection of an edge from and an edge from then we can decide which faces of and contain f by looking at the incidentface pointer of the appropriate half edges corresponding to and if v is not an intersection but a vertex of say then we only know the face of containing f to nd the face of containing f we have to do some more work we have to determine the face of that contains v in other words if we knew for each vertex of in which face of it lay and vice versa then we could label the faces of o correctly how can we compute this information the solution is to apply the paradigm that has been introduced in this chapter plane sweep once more however we won t explain this nal step here it is a good exercise to test your understanding of the plane sweep approach to design the algorithm yourself in fact it is not necessary to compute this information in a separate plane sweep it can also be done in the sweep that computes the intersections putting everything together we get the following algorithm algorithm mapoverlay input two planar subdivisions and stored in doubly connected edge lists output the overlay of and stored in a doubly connected edge list d copy the doubly connected edge lists for and to a new doubly connected edge list d compute all intersections between edges from and with the plane sweep algorithm of section in addition to the actions on t and q required at the event points do the following update d as explained above if the event involves edges of both and this was explained for the case where an edge of passes through a vertex of store the half edge immediately to the left of the event point at the vertex in d representing it now d is the doubly connected edge list for o except that the information about the faces has not been computed yet determine the boundary cycles in o by traversing d construct the graph g whose nodes correspond to boundary cycles and whose arcs connect each hole cycle to the cycle to the left of its leftmost ver tex and compute its connected components the information to determine the arcs of g has been computed in line second item for each connected component in g do let c be the unique outer boundary cycle in the component and let f denote the face bounded by the cycle create a face record for f set outercomponent f to some half edge of c and construct the list innercomponents f consisting of pointers to one half edge in each hole cycle in the component let the incidentface pointers of all half edges in the cycles point to the face record of f label each face of o with the names of the faces of and containing it as explained above theorem let be a planar subdivision of complexity let be a subdivision of complexity and let n the overlay of and can be constructed in o n log n k log n time where k is the complexity of the overlay proof copying the doubly connected edge lists in line takes o n time and the plane sweep of line takes o n log n k log n time by lemma steps where we ll in the face records takes time linear in the complexity of o the connected components of a graph can be determined in linear time by a simple depth rst search finally labeling each face in the resulting subdivision with the faces of the original subdivisions that contain it can be done in o n log n k log n time section boolean operations boolean operations the map overlay algorithm is a powerful instrument that can be used for various other applications one particular useful one is performing the boolean opera tions union intersection and difference on two polygons and see figure for an example note that the output of the operations might no longer be a polygon it can consist of a number of polygonal regions some with holes union intersection difference to perform the boolean operation we regard the polygons as planar maps whose bounded faces are labeled and respectively we compute the overlay of these maps and we extract the faces in the overlay whose labels correspond to the particular boolean operation we want to perform if we want to compute the intersection we extract the faces in the overlay that are figure the boolean operations union intersection and difference on two polygons and labeled with and if we want to compute the union we extract the chapter line segment intersection faces in the overlay that are labeled with or and if we want to compute the difference we extract the faces in the overlay that are labeled with and not with because every intersection point of an edge of and an edge of is a vertex of the running time of the algorithm is o n log n k log n where n is the total number of vertices in and and k is the complexity of the same holds for the other boolean operations every intersection of two edges is a vertex of the nal result no matter which operation we want to perform we immediately get the following result corollary let be a polygon with vertices and a polygon with vertices and let n then and can each be computed in o n log n k log n time where k is the complexity of the output notes and comments the line segment intersection problem is one of the most fundamental problems in computational geometry the o n log n k log n solution presented in this chapter was given by bentley and ottmann in a few years earlier shamos and hoey had solved the detection problem where one is only interested in deciding whether there is at least one intersection in o n log n time the method for reducing the working storage from o n k to o n described in this chapter is taken from pach and sharir who also show that the event list can have size  n log n before this improvement brown describes an alternative method to achieve the reduction the lower bound for the problem of reporting all line segment intersections is  n log n k so the plane sweep algorithm described in this chapter is not optimal when k is large a rst step towards an optimal algorithm was taken by chazelle who gave an algorithm with o n n loglog n k running time in chazelle and edelsbrunner presented the rst o n log n k time algorithm unfortunately it requires o n k storage later clarkson and shor and mulmuley gave randomized incremental algorithms whose expected running time is also o n log n k see chapter for an explanation of randomized algorithms the working storage of these algorithms is o n and o n k respectively unlike the algorithm of chazelle and edelsbrunner these randomized algorithms also work for computing inter sections in a set of curves balaban gave the rst deterministic algorithm for the segment intersection problem that works in o n log n k time and o n space it also works for curves there are cases of the line segment intersection problem that are easier than the general case one such case is where we have two sets of segments say red segments and blue segments such that no two segments from the same set intersect each other this is in fact exactly the network overlay problem in the solution described in this chapter however the fact that the segments came from two sets of non intersecting segments was not used this so called red blue line segment intersection problem was solved in o n log n k time and o n storage by mairson and stol before the general problem was solved optimally other optimal red blue intersection algorithms were given by chazelle et al and by palazzi and snoeyink if the two sets of segments form connected subdivisions then the situation is even better in this case the overlay can be computed in o n k time as has been shown by finke and hinrichs their result generalizes and improves previous results on map overlay by nievergelt and preparata guibas and seidel and mairson and stol the line segment intersection counting problem is to determine the number of intersection points in a set of n line segments since the output is a single integer a term with k in the time bound no longer refers to the output size which is constant but only to the number of intersections algorithms that do not depend on the number of intersections take o logc n time for some small constant c a running time close to o n log n is not known to exist plane sweep is one of the most important paradigms for designing geometric algorithms the rst algorithms in computational geometry based on this paradigm are by shamos and hoey lee and preparata and bentley and ottmann plane sweep algorithms are especially suited for nding intersections in sets of objects but they can also be used for solving many other problems in chapter plane sweep solves part of the polygon triangulation problem and in chapter we will see a plane sweep algorithm to compute the so called voronoi diagram of a set of points the algorithm presented in the current chapter sweeps a horizontal line downwards over the plane for some problems it is more convenient to sweep the plane in another way for instance we can sweep the plane with a rotating line see chapter for an example or with a pseudo line a line that need not be straight but otherwise behaves more or less as a line the plane sweep technique can also be used in higher dimensions here we sweep the space with a hyperplane such algorithms are called space sweep algorithms in this chapter we described a data structure for storing subdivisions the doubly connected edge list this structure or in fact a variant of it was described by muller and preparata there are also other data structures for storing subdivisions such as the winged edge structure by baumgart and the quad edge structure by guibas and stol the difference between all these structures is small they all have more or less the same functionality but some save a few bytes of storage per edge section exercises exercises let s be a set of n disjoint line segments whose upper endpoints lie on the line y and whose lower endpoints lie on the line y these segments partition the horizontal strip into n regions give an o n log n time algorithm to build a binary search tree on the segments chapter line segment intersection in s such that the region containing a query point can be determined in o log n time also describe the query algorithm in detail the intersection detection problem for a set s of n line segments is to determine whether there exists a pair of segments in s that intersect give a plane sweep algorithm that solves the intersection detection problem in o n log n time change the code of algorithm findintersections and of the pro cedures that it calls such that the working storage is o n instead of o n k let s be a set of n line segments in the plane that may partly overlap each other for example s could contain the segments and we want to compute all intersections in s more precisely we want to compute each proper intersection of two segments in s that is each intersection of two non parallel segments and for each end point of a segment all segments containing the point adapt algorithm findintersections to this end which of the following equalities are always true twin twin e e next prev e e twin prev twin e next e incidentface e incidentface next e give an example of a doubly connected edge list where for an edge e the faces incidentface e and incidentface twin e are the same given a doubly connected edge list representation of a subdivision where twin e next e holds for every half edge e how many faces can the subdivision have at most give pseudocode for an algorithm that lists all vertices adjacent to a given vertex v in a doubly connected edge list also give pseudocode for an algorithm that lists all edges that bound a face in a not necessarily connected subdivision suppose that a doubly connected edge list of a connected subdivision is given give pseudocode for an algorithm that lists all faces with vertices that appear on the outer boundary let s be a subdivision of complexity n and let p be a set of m points give a plane sweep algorithm that computes for every point in p in which face of s it is contained show that your algorithm runs in o n m log n m time let s be a set of n circles in the plane describe a plane sweep algorithm to compute all intersection points between the circles because we deal with circles not discs two circles do not intersect if one lies entirely inside the other your algorithm should run in o n k log n time where k is the number of intersection points let s be a set of n triangles in the plane the boundaries of the triangles are disjoint but it is possible that a triangle lies completely inside another triangle let p be a set of n points in the plane give an o n log n algorithm that reports each point in p lying outside all triangles let s be a set of n disjoint triangles in the plane we want to nd a set of n segments with the following properties each segment connects a point on the boundary of one triangle to a point on the boundary of another triangle the interiors of the segments are pairwise disjoint and they are disjoint from the triangles together they connect all triangles to each other that is by walking along the segments and the triangle boundaries it must be possible to walk from a triangle to any other triangle develop a plane sweep algorithm for this problem that runs in o n log n time state the events and the data structures that you use explicitly and describe the cases that arise and the actions required for each of them also state the sweep invariant let s be a set of n disjoint line segments in the plane and let p be a point not on any of the line segments of s we wish to determine all line segments of s that p can see that is all line segments of s that contain some point q so that the open segment pq doesn t intersect any line segment of s give an o n log n time algorithm for this problem that uses a rotating half line with its endpoint at p section exercises not visible polygon triangulation guarding an art gallery works of famous painters are not only popular among art lovers but also among criminals they are very valuable easy to transport and apparently not so difcult to sell art galleries therefore have to guard their collections carefully figure an art gallery during the day the attendants can keep a look out but at night this has to be done by video cameras these cameras are usually hung from the ceiling and they rotate about a vertical axis the images from the cameras are sent to tv screens in the ofce of the night watch because it is easier to keep an eye on few tv screens rather than on many the number of cameras should be as small as possible an additional advantage of a small number of cameras is that the cost of the security system will be lower on the other hand we cannot have too few cameras because every part of the gallery must be visible to at least one of them so we should place the cameras at strategic positions such that each of them guards a large part of the gallery this gives rise to what is usually referred to as the art gallery problem how many cameras do we need to guard a given gallery and how do we decide where to place them chapter polygon triangulation guarding and triangulations if we want to dene the art gallery problem more precisely we should rst formalize the notion of gallery a gallery is of course a dimensional space but a oor plan gives us enough information to place the cameras therefore we model a gallery as a polygonal region in the plane we further restrict ourselves to regions that are simple polygons that is regions enclosed by a single closed polygonal chain that does not intersect itself thus we do not allow regions with holes a camera position in the gallery corresponds to a point in the polygon a camera sees those points in the polygon to which it can be connected with an open segment that lies in the interior of the polygon how many cameras do we need to guard a simple polygon this clearly depends on the polygon at hand the more complex the polygon the more cameras are required we shall therefore express the bound on the number of cameras needed in terms of n the number of vertices of the polygon but even when two polygons have the same number of vertices one can be easier to guard than the other a convex polygon for example can always be guarded with one camera to be on the safe side we shall look at the worst case scenario that is we shall give a bound that is good for any simple polygon with n vertices it would be nice if we could nd the minimum number of cameras for the specic polygon we are given not just a worst case bound unfortunately the problem of nding the minimum number of cameras for a given polygon is np hard let p be a simple polygon with n vertices because p may be a complicated shape it seems difcult to say anything about the number of cameras we need to guard p hence we rst decompose p into pieces that are easy to guard namely triangles we do this by drawing diagonals between pairs of vertices figure a simple polygon and a possible triangulation of it a diagonal is an open line segment that connects two vertices of p and lies in the interior of p a decomposition of a polygon into triangles by a maximal set of non intersecting diagonals is called a triangulation of the polygon see figure we require that the set of non intersecting diagonals be maximal to ensure that no triangle has a polygon vertex in the interior of one of its edges this could happen if the polygon has three consecutive collinear vertices triangulations are usually not unique the polygon in figure for example can be triangulated in many different ways we can guard p by placing a camera in every triangle of a triangulation tp of p but does a triangulation always exist and how many triangles can there be in a triangulation the following theorem answers these questions theorem every simple polygon admits a triangulation and any triangula tion of a simple polygon with n vertices consists of exactly n triangles proof we prove this theorem by induction on n when n the polygon itself is a triangle and the theorem is trivially true let n and assume that the theorem is true for all m n let p be a polygon with n vertices we rst prove the existence of a diagonal in p let v be the leftmost vertex of p in case of ties we take the lowest leftmost vertex let u and w be the two neighboring section guarding and triangulations vertices of v on the boundary of p if the open segment uw lies in the interior of v p we have found a diagonal otherwise there are one or more vertices inside the triangle dened by u v and w or on the diagonal uw of those vertices let vi be the one farthest from the line through u and w the segment connecting vi to v cannot intersect an edge of p because such an edge would have an endpoint inside the triangle that is farther from the line through u and w contradicting the denition of vi hence vvi is a diagonal so a diagonal exists any diagonal cuts p into two simple subpolygons and let be the number of vertices of and the number of vertices of both and must be smaller than n so by induction and can v be triangulated hence p can be triangulated as well it remains to prove that any triangulation of p consists of n triangles to this end consider an arbitrary diagonal in some triangulation tp this diagonal cuts p into two subpolygons with and vertices respectively every vertex of p occurs in exactly one of the two subpolygons except for the vertices dening the diagonal which occur in both subpolygons hence n by induction any triangulation of pi consists of mi triangles which implies that tp consists of n triangles theorem implies that any simple polygon with n vertices can be guarded with n cameras but placing a camera inside every triangle seems overkill a camera placed on a diagonal for example will guard two triangles so by placing the cameras on well chosen diagonals we might be able to reduce the number of cameras to roughly n placing cameras at vertices seems even better because a vertex can be incident to many triangles and a camera at that vertex guards all of them this suggests the following approach let tp be a triangulation of p select a subset of the vertices of p such that any triangle in tp has at least one selected vertex and place the cameras at the selected vertices to nd such a subset we assign each vertex of p a color white gray or black the coloring will be such that any two vertices connected by an edge or a diagonal have different colors this is called a coloring of a triangulated polygon in a coloring of a triangulated polygon every triangle has a white a gray and a black vertex hence if we place cameras at all gray vertices say we have guarded the whole polygon by choosing the smallest color class to place the cameras we can guard p using at most n cameras but does a coloring always exist the answer is yes to see this we look at what is called the dual graph of tp this graph g tp has a node for every triangle in tp we denote the triangle corresponding to a node  by t  there is an arc between two nodes  and  if t  and t  share a diagonal the arcs chapter polygon triangulation in g tp correspond to diagonals in tp because any diagonal cuts p into two the removal of an edge from g tp splits the graph into two hence g tp is a tree notice that this is not true for a polygon with holes this means that we can nd a coloring using a simple graph traversal such as depth rst search next we describe how to do this while we do the depth rst search we maintain the following invariant all vertices of the already encountered triangles have been colored white gray or black and no two connected vertices have received the same color the invariant implies that we have computed a valid coloring when all triangles have been encountered the depth rst search can be started from any node of g tp the three vertices of the corresponding triangle are colored white gray and black now suppose that we reach a node  in g coming from node  hence t  and t  share a diagonal since the vertices of t  have already been colored only one vertex of t  remains to be colored there is one color left for this vertex namely the color that is not used for the vertices of the diagonal between t  and t  because g tp is a tree the other nodes adjacent to  have not been visited yet and we still have the freedom to give the vertex the remaining color we conclude that a triangulated simple polygon can always be colored as a result any simple polygon can be guarded with n cameras but perhaps we can do even better after all a camera placed at a vertex may guard more than just the incident triangles unfortunately for any n there are simple polygons that require n cameras an example is a comb shaped polygon with a long horizontal base edge and n prongs made of two edges each the prongs are connected by horizontal edges the construction can be made such that there is no position in the polygon from which a camera can look into two prongs of the comb simultaneously so we cannot hope for a strategy that always produces less than n cameras in other words the coloring approach is optimal in the worst case we just proved the art gallery theorem a classical result from combinato rial geometry theorem art gallery theorem for a simple polygon with n vertices n cameras are occasionally necessary and always sufcient to have every point in the polygon visible from at least one of the cameras now we know that n cameras are always sufcient but we don t have an efcient algorithm to compute the camera positions yet what we need is a fast algorithm for triangulating a simple polygon the algorithm should deliver a suitable representation of the triangulation a doubly connected edge list for instance so that we can step in constant time from a triangle to its neighbors given such a representation we can compute a set of at most n camera positions in linear time with the method described above use depth rst search on the dual graph to compute a coloring and take the smallest color class to place the cameras in the coming sections we describe how to compute a triangulation in o n log n time anticipating this we already state the nal result about guarding a polygon theorem let p be a simple polygon with n vertices a set of n camera positions in p such that any point inside p is visible from at least one of the cameras can be computed in o n log n time partitioning a polygon into monotone pieces let p be a simple polygon with n vertices we saw in theorem that a triangulation of p always exists the proof of that theorem is constructive and leads to a recursive triangulation algorithm nd a diagonal and triangulate the two resulting subpolygons recursively to nd the diagonal we take the leftmost vertex of p and try to connect its two neighbors u and w if this fails we connect v to the vertex farthest from uw inside the triangle dened by u v and w this way it takes linear time to nd a diagonal this diagonal may split p into a triangle and a polygon with n vertices indeed if we succeed to connect u and w this will always be the case as a consequence the triangulation algorithm will take quadratic time in the worst case can we do better for some classes of polygons we surely can convex polygons for instance are easy pick one vertex of the polygon and draw diagonals from this vertex to all other vertices except its neighbors this takes only linear time so a possible approach to triangulate a non convex polygon would be to rst decompose p into convex pieces and then triangulate the pieces unfortunately it is as difcult to partition a polygon into convex pieces as it is to triangulate it therefore we shall decompose p into so called monotone pieces which turns out to be a lot easier a simple polygon is called monotone with respect to a line f if for any line fi perpendicular to f the intersection of the polygon with fi is connected in other words the intersection should be a line segment a point or empty a polygon that is monotone with respect to the y axis is called y monotone the following property is characteristic for y monotone polygons if we walk from a topmost to a bottommost vertex along the left or the right boundary chain then we always move downwards or horizontally never upwards section partitioning a polygon into monotone pieces our strategy to triangulate the polygon p is to rst partition p into y monotone pieces and then triangulate the pieces we can partition a polygon into mono tone pieces as follows imagine walking from the topmost vertex of p to the bottommost vertex on the left or right boundary chain a vertex where the direction in which we walk switches from downward to upward or from upward to downward is called a turn vertex to partition p into y monotone pieces we should get rid of these turn vertices this can be done by adding diagonals if at a turn vertex v both incident edges go down and the interior of the polygon locally lies above v then we must choose a diagonal that goes up from v the diagonal splits the polygon into two the vertex v will appear in both pieces moreover in both pieces v has an edge going down namely on original edge of p and an edge going up the diagonal hence v cannot be a turn vertex anymore in either of them if both incident edges of a turn vertex go up and chapter polygon triangulation the interior locally lies below it we have to choose a diagonal that goes down apparently there are different types of turn vertices let make this more precise if we want to dene the different types of turn vertices carefully we should pay special attention to vertices with equal y coordinate we do this by dening the notions of below and above as follows a point p is below another point q if py qy or py qy and px qx and p is above q if py qy or py qy and px qx you can imagine rotating the plane slightly in clockwise direction with respect to the coordinate system such that no two points have the same y coordinate the above below relation we just dened is the same as the above below relation in this slightly rotated plane e4 e3 start vertex end vertex figure v12 regular vertex split vertex merge vertex five types of vertices we distinguish ve types of vertices in p see figure four of these types are turn vertices start vertices split vertices end vertices and merge vertices they are dened as follows a vertex v is a start vertex if its two neighbors lie below it and the interior angle at v is less than  if the interior angle is greater than  then v is a split vertex if both neighbors lie below v then the interior angle cannot be exactly  a vertex is an end vertex if its two neighbors lie above it and the interior angle at v is less than  if the interior angle is greater than  then v is a merge vertex the vertices that are not turn vertices are regular vertices thus a regular vertex has one of its neighbors above it and the other neighbor below it these names have been chosen because the algorithm will use a downward plane sweep maintaining the intersection of the sweep line with the polygon when the sweep line reaches a split vertex a component of the intersection splits when it reaches a merge vertex two components merge and so on the split and merge vertices are sources of local non monotonicity the following stronger statement is even true lemma a polygon is y monotone if it has no split vertices or merge vertices proof suppose p is not y monotone we have to prove that p contains a split or a merge vertex since p is not monotone there is a horizontal line f that intersects p in more than one connected component we can choose f such that the leftmost component is a segment not a single point let p be the left endpoint of this segment and let q be the right endpoint starting at q we follow the boundary of p such that p lies to the left of the boundary this means that we go up from q at some point let call it r the boundary will intersect f again if r p as in figure a then the highest vertex we encountered while going from q to r must be a split vertex and we are done section partitioning a polygon into monotone pieces b pp p r q ri f merge vertex figure two cases in the proof of lemma if r p as in figure b we again follow the boundary of p starting at q but this time in the other direction as before the boundary will intersect f let ri be the point where this happens we cannot have ri p because that would mean that the boundary of p intersects f only twice contradicting that f intersects p in more than one component so we have ri p implying that the lowest vertex we have encountered while going from q to ri must be a merge vertex lemma implies that p has been partitioned into y monotone pieces once we get rid of its split and merge vertices we do this by adding a diagonal going upward from each split vertex and a diagonal going downward from each merge vertex these diagonals should not intersect each other of course once we have done this p has been partitioned into y monotone pieces let rst see how we can add the diagonals for the split vertices we use a plane sweep method for this let vn be a counterclockwise enumeration of the vertices of p let en be the set of edges of p where ei vivi for i n and en the plane sweep algorithm moves an imaginary sweep line f downward over the plane the sweep line halts at certain event points in our case these will be the vertices of p no new event points will be created during the sweep the event points are stored in a event queue q the event queue is a priority queue where the priority of a vertex is its y coordinate if two vertices have the same y coordinate then the leftmost one has higher priority this way the next event to be handled can be found in o log n time because no new events are generated during the sweep we could also sort the vertices on y coordinate before the sweep and then use the sorted list to nd the next event in o time chapter polygon triangulation vi ek e j vm diagonal will be added when the sweep line reaches vm the goal of the sweep is to add diagonals from each split vertex to a vertex lying above it suppose that the sweep line reaches a split vertex vi to which vertex should we connect vi a good candidate is a vertex close to vi because we can probably connect vi to this vertex without intersecting any edge of p let make this more precise let ej be the edge immediately to the left of vi on the sweep line and let ek be the edge immediately to the right of vi on the sweep line then we can always connect vi to the lowest vertex in between ej and ek and above vi if there is no such vertex then we can connect vi to the upper endpoint of ej or to the upper endpoint of ek we call this vertex the helper of ej and denote it by helper ej formally helper ej is dened as the lowest vertex above the sweep line such that the horizontal segment connecting the vertex to ej lies inside p note that helper ej can be the upper endpoint of ej itself now we know how to get rid of split vertices connect them to the helper of the edge to their left what about merge vertices they seem more difcult to get rid of because they need a diagonal to a vertex that is lower than they are since the part of p below the sweep line has not been explored yet we cannot add such a diagonal when we encounter a merge vertex fortunately this problem is easier than it seems at rst sight suppose the sweep line reaches a merge vertex vi let ej and ek be the edges immediately to the right and to the left of vi on the sweep line respectively observe that vi becomes the new helper of ej when we reach it we would like to connect vi to the highest vertex below the sweep line in between ej and ek this is exactly the opposite of what we did for split vertices which we connected to the lowest vertex above the sweep line in between ej and ek this is not surprising merge vertices are split vertices upside down of course we don t know the highest vertex below the sweep line when we reach vi but it is easy to nd later on when we reach a vertex vm that replaces vi as the helper of ej then this is the vertex we are looking for so whenever we replace the helper of some edge we check whether the old helper is a merge vertex and if so we add the diagonal between the old helper and the new one this diagonal is always added when the new helper is a split vertex to get rid of the split vertex if the old helper was a merge vertex we thus get rid of a split vertex and a merge vertex with the same diagonal it can also happen that the helper of ej is not replaced anymore below vi in this case we can connect vi to the lower endpoint of ej in the approach above we need to nd the edge to the left of each vertex therefore we store the edges of p intersecting the sweep line in the leaves of a dynamic binary search tree t the left to right order of the leaves of t corresponds to the left to right order of the edges because we are only interested in edges to the left of split and merge vertices we only need to store edges in t that have the interior of p to their right with each edge in t we store its helper the tree t and the helpers stored with the edges form the status of the sweep line algorithm the status changes as the sweep line moves edges start or stop intersecting the sweep line and the helper of an edge may be replaced the algorithm partitions p into subpolygons that have to be processed further in a later stage to have easy access to these subpolygons we shall store the subdivision induced by p and the added diagonals in a doubly connected edge list d we assume that p is initially specied as a doubly connected edge list if p is given in another form by a counterclockwise list of its vertices for example we rst construct a doubly connected edge list for p the diagonals computed for the split and merge vertices are added to the doubly connected edge list to access the doubly connected edge list we use cross pointers between the edges in the status structure and the corresponding edges in the doubly connected edge list adding a diagonal can then be done in constant time with some simple pointer manipulations the global algorithm is now as follows algorithm makemonotone p input a simple polygon p stored in a doubly connected edge list d output a partitioning of p into monotone subpolygons stored in d construct a priority queue q on the vertices of p using their y coordinates as priority if two points have the same y coordinate the one with smaller x coordinate has higher priority initialize an empty binary search tree t while q is not empty do remove the vertex vi with the highest priority from q call the appropriate procedure to handle the vertex depending on its type we next describe more precisely how to handle the event points you should rst read these algorithms without thinking about degenerate cases and check only later that they are also correct in degenerate cases to this end you should give an appropriate meaning to directly left of in line of handlesplitvertex and line of handlemergevertex there are always two things we must do when we handle a vertex first we must check whether we have to add a diagonal this is always the case for a split vertex and also when we replace the helper of an edge and the previous helper was a merge vertex second we must update the information in the status structure t the precise algorithms for each type of event are given below you can use the example gure on the next page to see what happens in each of the different cases handlestartvertex vi insert ei in t and set helper ei to vi at the start vertex in the example gure for instance we insert into the tree t handleendvertex vi if helper ei is a merge vertex then insert the diagonal connecting vi to helper ei in d delete ei from t in the running example when we reach end vertex the helper of the edge section partitioning a polygon into monotone pieces is is not a merge vertex so we don t need to insert a diagonal chapter polygon triangulation handlesplitvertex vi search in t to nd the edge ej directly left of vi insert the diagonal connecting vi to helper ej in d helper ej vi e5 e4 e3 insert ei in t and set helper ei to vi for split vertex in our example is the edge to the left its helper is so we add a diagonal from to e v1 handlemergevertex vi e10 54 v12 if helper ei is a merge vertex then insert the diagonal connecting vi to helper ei in d delete ei from t search in t to nd the edge ej directly left of vi if helper ej is a merge vertex then insert the diagonal connecting vi to helper ej in d helper ej vi for the merge vertex in our example the helper of edge is a merge vertex so we add a diagonal from to the only routine that remains to be described is the one to handle a regular vertex the actions we must take at a regular vertex depend on whether p lies locally to its left or to its right handleregularvertex vi if the interior of p lies to the right of vi then if helper ei is a merge vertex then insert the diagonal connecting vi to helper ei in d delete ei from t insert ei in t and set helper ei to vi else search in t to nd the edge ej directly left of vi if helper ej is a merge vertex then insert the diagonal connecting vi to helper ej in d helper ej vi for instance at the regular vertex in our example we add a diagonal from to it remains to prove that makemonotone correctly partitions p into monotone pieces lemma algorithm makemonotone adds a set of non intersecting diag onals that partitions p into monotone subpolygons proof it is easy to see that the pieces into which p is partitioned contain no split or merge vertices hence they are monotone by lemma it remains to prove that the added segments are valid diagonals that is that they don t intersect the edges of p and that they don t intersect each other to this end we will show that when a segment is added it intersects neither an edge of p nor any of the previously added segments we shall prove this for the segment added in handlesplitvertex the proof for the segments added inhandleendvertex handleregularvertex and handlemerge vertex is similar we assume that no two vertices have the same y coordinate the extension to the general case is fairly straightforward consider a segment vmvi that is added by handlesplitvertex when vi is reached let ej be the edge to the left of vi and let ek be the edge to the right of vi thus helper ej vm when we reach vi we rst argue that vmvi does not intersect an edge of p to see this consider the quadrilateral q bounded by the horizontal lines through vm and vi and by ej and ek there are no vertices of p inside q otherwise vm would not be the helper of ej now suppose there would be an edge of p intersecting vmvi since the edge cannot have an endpoint inside q and polygon edges do not intersect each other it would have to intersect the horizontal segment connecting vm to ej or the horizontal segment connecting vi to ej both are impossible since for both vm and vi the edge ej lies immediately to the left hence no edge of p can intersect vmvi now consider a previously added diagonal since there are no vertices of p inside q and any previously added diagonal must have both of its endpoints above vi it cannot intersect vmvi we now analyze the running time of the algorithm constructing the priority queue q takes linear time and initializing t takes constant time to handle an event during the sweep we perform one operation on q at most one query one insertion and one deletion on t and we insert at most two diagonals into d priority queues and balanced search trees allow for queries and updates in o log n time and the insertion of a diagonal into d takes o time hence handling an event takes o log n time and the total algorithm runs in o n log n time the amount of storage used by the algorithm is clearly linear every vertex is stored at most once in q and every edge is stored at most once in t together with lemma this implies the following theorem theorem a simple polygon with n vertices can be partitioned into y monotone polygons in o n log n time with an algorithm that uses o n storage triangulating a monotone polygon we have just seen how to partition a simple polygon into y monotone pieces in o n log n time in itself this is not very interesting but in this section we show that monotone polygons can be triangulated in linear time together these results imply that any simple polygon can be triangulated in o n log n time a nice improvement over the quadratic time algorithm that we sketched at the beginning of the previous section section triangulating a monotone polygon let p be a y monotone polygon with n vertices for the moment we assume that p is strictly y monotone that is we assume that p is y monotone and does pushed v j chapter polygon triangulation popped e popped and pushed not contain horizontal edges thus we always go down when we walk on the left or right boundary chain of p from the highest vertex of p to the lowest one this is the property that makes triangulating a monotone polygon easy we can work our way through p from top to bottom on both chains adding diagonals whenever this is possible next we describe the details of this greedy triangulation algorithm the algorithm handles the vertices in order of decreasing y coordinate if two vertices have the same y coordinate then the leftmost one is handled rst the algorithm requires a stack s as auxiliary data structure initially the stack is empty later it contains the vertices of p that have been encountered but may still need more diagonals when we handle a vertex we add as many diagonals from this vertex to vertices on the stack as possible these diagonals split off triangles from p the vertices that have been handled but not split off the vertices on the stack are on the boundary of the part of p that still needs to be triangulated the lowest of these vertices which is the one encountered last is on top of the stack the second lowest is second on the stack and so on the part of p that still needs to be triangulated and lies above the last vertex that has been encountered so far has a particular shape it looks like a funnel turned upside down one boundary of the funnel consists of a part of a single edge of p and the other boundary is a chain consisting of reex vertices that is the interior angle at these vertices is at least only the highest vertex which is at the bottom of the stack is convex this property remains true after we have handled the next vertex hence it is an invariant of the algorithm now let see which diagonals we can add when we handle the next vertex we distinguish two cases v j the next vertex to be handled lies on the same chain as the reex vertices on the stack or it lies on the opposite chain if v j lies on the opposite chain it must be the lower endpoint of the single edge e bounding the funnel due to the shape of the funnel we can add diagonals from v j to all vertices currently on the stack except for the last one that is the one at the bottom of the stack the last vertex on the stack is the upper vertex of e so it is already connected to v j all these vertices are popped from the stack the untriangulated part of the polygon above v j is bounded by the diagonal that connects v j to the vertex previously on top of the stack and the edge of p extending downward from this vertex so it looks like a funnel and the invariant is preserved this vertex and v j remain part of the not yet triangulated polygon so they are pushed onto the stack the other case is when v j is on the same chain as the reex vertices on the stack this time we may not be able to draw diagonals from v j to all vertices on the stack nevertheless the ones to which we can connect v j are all consecutive and they are on top of the stack so we can proceed as follows first pop one vertex from the stack this vertex is already connected to v j by an edge of p next pop vertices from the stack and connect them to v j until we encounter one where this is not possible checking whether a diagonal can be drawn from v j to a vertex vk on the stack can be done by looking at v j vk and the previous vertex that was popped when we nd a vertex to which we cannot connect v j we push the last vertex that has been popped back onto the stack this is either the last vertex to which a diagonal was added or if no diagonals have been added it is the neighbor of v j on the boundary of p see figure after this popped and pushed section triangulating a monotone polygon popped v j pushed v j figure two cases when the next vertex is on the same side as the reex vertices on the stack has been done we push v j onto the stack in both cases the invariant is restored one side of the funnel is bounded by a part of a single edge and the other side is bounded by a chain of reex vertices we get the following algorithm the algorithm is actually similar to the convex hull algorithm of chapter algorithm triangulatemonotonepolygon p input a strictly y monotone polygon p stored in a doubly connected edge list d output a triangulation of p stored in the doubly connected edge list d merge the vertices on the left chain and the vertices on the right chain of p into one sequence sorted on decreasing y coordinate if two vertices have the same y coordinate then the leftmost one comes rst let un denote the sorted sequence initialize an empty stack s and push and onto it for j to n do if u j and the vertex on top of s are on different chains then pop all vertices from s insert into d a diagonal from u j to each popped vertex except the last one push u j and u j onto s else pop one vertex from s pop the other vertices from s as long as the diagonals from u j to them are inside p insert these diagonals into d push the last vertex that has been popped back onto s push u j onto s add diagonals from un to all stack vertices except the rst and the last one how much time does the algorithm take step takes linear time and step takes constant time the for loop is executed n times and one execution may take linear time but at every execution of the for loop at most two vertices are pushed hence the total number of pushes including the two in step is bounded by because the number of pops cannot exceed the number of pushes the total time for all executions of the for loop is o n the last step of chapter polygon triangulation the algorithm also takes at most linear time so the total algorithm runs in o n time theorem a strictly y monotone polygon with n vertices can be triangulated in linear time we wanted a triangulation algorithm for monotone polygons as a subroutine for triangulating arbitrary simple polygons the idea was to rst decompose a polygon into monotone pieces and then to triangulate these pieces it seems that we have all the ingredients we need there is one problem however in this section we have assumed that the input is a strictly y monotone polygon whereas the algorithm of the previous section may produce monotone pieces with horizontal edges recall that in the previous section we treated vertices with the same y coordinates from left to right this had the same effect as a slight rotation of the plane in clockwise direction such that no two vertices are on a horizontal line it follows that the monotone subpolygons produced by the algorithm of the previous section are strictly monotone in this slightly rotated plane hence the triangulation algorithm of the current section operates correctly if we treat vertices with the same y coordinate from left to right which corresponds to working in the rotated plane so we can combine the two algorithms to obtain a triangulation algorithm that works for any simple polygon how much time does the triangulation algorithm take decomposing the polygon into monotone pieces takes o n log n time by theorem in the second stage we triangulate each of the monotone pieces with the linear time algorithm of this section since the sum of the number of vertices of the pieces is o n the second stage takes o n time in total we get the following result theorem a simple polygon with n vertices can be triangulated in o n log n time with an algorithm that uses o n storage we have seen how to triangulate simple polygons but what about polygons with holes can they also be triangulated easily the answer is yes in fact the algorithm we have seen also works for polygons with holes nowhere in the algorithm for splitting a polygon into monotone pieces did we use the fact that the polygon was simple it even works in a more general setting suppose we have a planar subdivision s and we want to triangulate that subdivision more precisely if b is a bounding box containing all edges of s in its interior we want to nd a maximal set of non intersecting diagonals line segments connecting vertices of s or b that do not intersect the edges of s that partitions b into triangles figure shows a triangulated subdivision the edges of the subdivisions and of the bounding box are shown bold to compute such a triangulation we can use the algorithm of this chapter rst split the subdivision into monotone pieces and then triangulate the pieces this leads to the following theorem theorem a planar subdivision with n vertices in total can be triangulated in o n log n time with an algorithm that uses o n storage section notes and comments notes and comments the art gallery problem was posed in by victor klee in a conversation with vasek chva tal in chva tal gave the rst proof that n cameras are always sufcient and sometimes necessary a result that became known as the art gallery theorem or the watchman theorem chva tal proof is quite complicated the much simpler proof presented in this chapter was discovered by fisk his proof is based on the two ears theorem by meisters from which the colorability of the graph that is a triangulation of a simple polygon follows easily the algorithmic problem of nding the minimum number of guards for a given simple polygon was shown to be np hard by aggarwal and lee and lin the book by o rourke and the overview by shermer contain an extensive treatment of the art gallery problem and numerous variations figure a triangulated subdivision a decomposition of a polygon or any other region into simple pieces is useful in many problems often the simple pieces are triangles in which case we call the decomposition a triangulation but sometimes other shapes such as quadrilaterals or trapezoids are used see also chapters and we only discuss the results on triangulating polygons here the linear time algorithm to triangulate a monotone polygon described in this chapter was given by garey et al and the plane sweep algorithm to partition a polygon into monotone pieces is due to lee and preparata avis and toussaint and chazelle described other algorithms for triangulating a simple polygon in o n log n time for a long time one of the main open problems in computational geome try was whether simple polygons can be triangulated in o n log n time for triangulating subdivisions with holes there is an  n log n lower bound in this chapter we have seen that this is indeed the case for monotone polygons linear time triangulation algorithms were also found for other special classes of polygons 184 but the problem for general simple polygons remained open for a number of years in tarjan and van wyk broke the o n log n barrier by presenting an o n loglog n algorithm their algorithm was later simplied by kirkpatrick et al randomization an approach used in chapters and proved to be a good tool in developing even chapter polygon triangulation faster algorithms clarkson et al devillers and seidel pre sented algorithms with o n log n running time where log n is the iterated logarithm of n being the number of times you can take the logarithm before the result is smaller than these algorithms are not only slightly faster than the o n loglog n algorithm but also simpler seidel algorithm is closely related to the algorithm for constructing a trapezoidal decomposition of a planar subdi vision described in chapter however the question whether a simple polygon can be triangulated in linear time was still open in this problem was nally settled by chazelle who gave a quite complicated deterministic linear time algorithm a randomized linear time algorithm was developed later by amato et al the dimensional equivalent to the polygon triangulation problem is this de compose a given polytope into non overlapping tetrahedra where the vertices of the tetrahedra must be vertices of the original polytope such a decomposition is called a tetrahedralization of the polytope this problem is much more difcult than the two dimensional version in fact it is not always possible to decompose a polytope into tetrahedra without using additional vertices chazelle has shown that for a simple polytope with n vertices  additional vertices may be needed and are always sufcient to obtain a decomposition into tetrahedra this bound was rened by chazelle and palios to  n where r is the number of reex edges of the polytope the algorithm to compute the decompo sition runs in o nr log r time deciding whether a given simple polytope can be tetrahedralized without additional vertices is np complete exercises prove that any polygon admits a triangulation even if it has holes can you say anything about the number of triangles in the triangulation a rectilinear polygon is a simple polygon of which all edges are horizontal or vertical let p be a rectilinear polygon with n vertices give an example to show that ln cameras are sometimes necessary to guard it prove or disprove the dual graph of the triangulation of a monotone polygon is always a chain that is any node in this graph has degree at most two suppose that a simple polygon p with n vertices is given together with a set of diagonals that partitions p into convex quadrilaterals how many cameras are sufcient to guard p why doesn t this contradict the art gallery theorem give the pseudo code of the algorithm to compute a coloring of a triangulated simple polygon the algorithm should run in linear time give an algorithm that computes in o n log n time a diagonal that splits a simple polygon with n vertices into two simple polygons each with at most vertices hint use the dual graph of a triangulation let p be a simple polygon with n vertices which has been partitioned into monotone pieces prove that the sum of the number of vertices of the pieces is o n the algorithm given in this chapter to partition a simple polygon into monotone pieces constructs a doubly connected edge list for the parti tioned polygon during the algorithm new edges are added to the dcel namely diagonals to get rid of split and merge vertices in general adding an edge to a dcel cannot be done in constant time discuss why adding an edge may take more than constant time and argue that in the polygon partitioning algorithm we can add a diagonal in o time nevertheless show that if a polygon has o turn vertices then the algorithm given in this chapter can be made to run in o n time can the algorithm of this chapter also be used to triangulate a set of n points if so explain how to do this efciently give an efcient algorithm to determine whether a polygon p with n vertices is monotone with respect to some line not necessarily a horizontal or vertical one the pockets of a simple polygon are the areas outside the polygon but inside its convex hull let be a simple polygon with m vertices and assume that a triangulation of as well as its pockets is given let be a convex polygon with n vertices show that the intersection p2 can be computed in o m n time the stabbing number of a triangulated simple polygon p is the maximum number of diagonals intersected by any line segment interior to p give an algorithm that computes a triangulation of a convex polygon that has stabbing number o log n 14 given a simple polygon p with n vertices and a point p inside it show how to compute the region inside p that is visible from p section exercises pockets linear programming manufacturing with molds most objects we see around us today from car bodies to plastic cups and cutlery are made using some form of automated manufacturing computers play an important role in this process both in the design phase and in the construction phase cad cam facilities are a vital part of any modern factory the construction process used to manufacture a specic object depends on factors such as the material the object should be made of the shape of the object and whether the object will be mass produced in this chapter we study some geometric aspects of manufacturing with molds a commonly used process for plastic or metal objects for metal objects this process is often referred to as casting figure the casting process figure illustrates the casting process liquid metal is poured into a mold it solidies and then the object is removed from the mold the last step is not always as easy as it seems the object could be stuck in the mold so that it cannot be removed without breaking the mold sometimes we can get around this problem by using a different mold there are also objects however for which no good mold exists a sphere is an example this is the problem we shall study in this chapter given an object is there a mold for it from which it can be removed we shall conne ourselves to the following situation first of all we assume that the object to be constructed is polyhedral secondly we only consider top facet chapter linear programming molds of one piece not molds consisting of two or more pieces using molds consisting of two pieces it is possible to manufacture objects such as spheres which cannot be manufactured using a mold of a single piece finally we only allow the object to be removed from the mold by a single translation this means that we will not be able to remove a screw from its mold fortunately translational motions sufce for many objects the geometry of casting if we want to determine whether an object can be manufactured by casting we have to nd a suitable mold for it the shape of the cavity in the mold is determined by the shape of the object but different orientations of the object give rise to different molds choosing the orientation can be crucial some orientations may give rise to molds from which the object cannot be removed while other orientations allow removal of the object one obvious restriction on the orientation is that the object must have a horizontal top facet this facet will be the only one not in contact with the mold hence there are as many potential orientations or equivalently possible molds as the object has facets we call an object castable if it can be removed from its mold for at least one of these orientations in the following we shall concentrate on determining whether an object is removable by a translation from a specic given mold to decide on the castability of the object we then simply try every potential orientation let p be a dimensional polyhedron that is a dimensional solid bounded by planar facets with a designated top facet we shall not try to give a precise formal denition of a polyhedron giving such a denition is tricky and not necessary in this context we assume that the mold is a rectangular block with a cavity that corresponds exactly to p when the polyhedron is placed in the mold its top facet should be coplanar with the topmost facet of the mold which we assume to be parallel to the xy plane this means that the mold has no unnecessary parts sticking out on the top that might prevent p from being removed we call a facet of p that is not the top facet an ordinary facet every ordinary facet f has a corresponding facet in the mold which we denote by f we want to decide whether p can be removed from its mold by a single transla tion in other words we want to decide whether a direction d exists such that p can be translated to innity in direction d without intersecting the interior of the mold during the translation note that we allow p to slide along the mold because the facet of p not touching the mold is its top facet the removal direction has to be upward that is it must have a positive z component this is only a necessary condition on the removal direction we need more constraints to be sure that a direction is valid let f be an ordinary facet of p this facet must move away from or slide along its corresponding facet f of the mold to make this constraint precise we need to dene the angle of two vectors in space we do this as follows take the plane spanned by the vectors we assume both vectors are rooted at the origin the angle of the vectors is the smaller of the two angles measured in this plane now f blocks any translation in a direction making an angle of less than with  f the outward normal of f so a necessary condition on d is that it makes an angle of at least with the outward normal of every ordinary facet of p the next lemma shows that this condition is also sufcient lemma the polyhedron p can be removed from its mold by a translation in direction d if and only if d makes an angle of at least with the outward normal of all ordinary facets of p proof the only if part is easy if d made an angle less than with some outward normal  f then any point q in the interior of f collides with the mold when translated in direction d to prove the if part suppose that at some moment p collides with the mold when translated in direction d we have to show that there must be an outward normal making an angle of less than with d let p be a point of p that collides with a facet f of the mold this means that p is about to move into the interior of the mold so  f the outward normal of f must make an angle greater than with d but then d makes an angle less than with the outward normal of the ordinary facet f of p that corresponds to f lemma has an interesting consequence if p can be removed by a sequence of small translations then it can be removed by a single translation so allowing for more than one translation does not help in removing the object from its mold we are left with the task of nding a direction d that makes an angle of at least with the outward normal of each ordinary facet of p a direction in dimensional space can be represented by a vector rooted at the origin we already know that we can restrict our attention to directions with a positive z component we can represent all such directions as points in the plane z where the point x y represents the direction of the vector x y this way every point in the plane z represents a unique direction and every direction with a positive z value is represented by a unique point in that plane lemma gives necessary and sufcient conditions on the removal direc tion d how do these conditions translate into our plane of directions let  x y z be the outward normal of an ordinary facet the direction d dx dy makes an angle at least with  if and only if the dot product of d and  is non positive hence an ordinary facet induces a constraint of the form section the geometry of casting xdx ydy z this inequality describes a half plane on the plane z that is the area left or the area right of a line on the plane this last statement is not true for horizontal facets which have x y in this case the constraint is either impossible to satisfy or always satised which is easy to test hence every non horizontal facet of p denes a closed half plane on the plane z and any point in the chapter linear programming common intersection of these half planes corresponds to a direction in which p can be removed the common intersection of these half planes may be empty in this case p cannot be removed from the given mold we have transformed our manufacturing problem to a purely geometric problem in the plane given a set of half planes nd a point in their common intersection or decide that the common intersection is empty if the polyhedron to be manufactured has n facets then the planar problem has at most n half planes the top facet does not induce a half plane in the next sections we will see that the planar problem just stated can be solved in expected linear time see section where also the meaning of expected is explained recall that the geometric problem corresponds to testing whether p can be removed from a given mold if this is impossible there can still be other molds corresponding to different choices of the top facet from which p is removable in order to test whether p is castable we try all its facets as top facets this leads to the following result theorem let p be a polyhedron with n facets in o expected time and using o n storage it can be decided whether p is castable moreover if p is castable a mold and a valid direction for removing p from it can be computed in the same amount of time half plane intersection let h hn be a set of linear constraints in two variables that is constraints of the form aix biy ci where ai bi and ci are constants such that at least one of ai and bi is non zero geometrically we can interpret such a constraint as a closed half plane in bounded by the line aix biy ci the problem we consider in this section is to nd the set of all points x y that satisfy all n constraints at the same time in other words we want to nd all the points lying in the common intersection of the half planes in h in the previous section we reduced the casting problem to nding some point in the intersection of a set of half planes the problem we study now is more general the shape of the intersection of a set of half planes is easy to determine a half plane is convex and the intersection of convex sets is again a convex set so the intersection of a set of half planes is a convex region in the plane every point on the intersection boundary must lie on the bounding line of some half plane hence the boundary of the region consists of edges contained in these bounding lines since the intersection is convex every bounding line can contribute at most one edge it follows that the intersection of n half planes is a convex polygonal region bounded by at most n edges figure shows a few examples of intersections of half planes to which side of its bounding line a half plane lies is indicated by dark shading in the gure the common intersection is shaded lightly as you can see in figures ii and iii the i ii iii section half plane intersection v intersection does not have to be bounded the intersection can also degenerate to a line segment or a point as in iv or it can be empty as in v we give a rather straightforward divide and conquer algorithm to compute the intersection of a set of n half planes it is based on a routine intersectcon vexregions to compute the intersection of two convex polygonal regions we rst give the overall algorithm algorithm intersecthalfplanes h input a set h of n half planes in the plane output the convex polygonal region c h h h if card h then c the unique half plane h h else split h into sets and of size in and ln intersecthalfplanes intersecthalfplanes c intersectconvexregions what remains is to describe the procedure intersectconvexregions but wait didn t we see this problem before in chapter indeed corollary states that we can compute the intersection of two polygons in o n log n k log n time where n is the total number of vertices in the two polygons we must be a bit careful in applying this result to our problem because the regions we have can be unbounded or degenerate to a segment or a point hence the regions are not necessarily polygons but it is not difcult to modify the algorithm from chapter so that it still works let analyze this approach assume we have already computed the two regions and by recursion since they are both dened by at most n half planes they both have at most n edges the algorithm from chapter computes their overlay in time o n k log n where k is the number of figure examples of the intersection of half planes intersection points between edges of and edges of what is k look chapter linear programming right boundary left boundary lleft c lright c at an intersection point v between an edge e1 of and an edge of no matter how e1 and intersect v must be a vertex of but is the intersection of n half planes and therefore has at most n edges and vertices it follows that k n so the computation of the intersection of and takes o n log n time this gives the following recurrence for the total running time t n o if n o n log n n if n this recurrence solves to t n o n n to obtain this result we used a subroutine for computing the intersection of two arbitrary polygons the polygonal regions we deal with in intersect halfplanes are always convex can we use this to develop a more efcient algorithm the answer is yes as we show next we will assume that the regions we want to intersect are dimensional the case where one or both of them is a segment or a point is easier and left as an exercise first let specify more precisely how we represent a convex polygonal region c we will store the left and the right boundary of c separately as sorted lists of half planes the lists are sorted in the order in which the bounding lines of the half planes occur when the left or right boundary is traversed from top to bottom we denote the left boundary list by lleft c and the right boundary list by lright c vertices are not stored explicitly they can be computed by intersecting consecutive bounding lines to simplify the description of the algorithm we shall assume that there are no horizontal edges to adapt the algorithm to deal with horizontal edges one can dene such edges to belong to the left boundary if they bound c from above and to the right boundary if they bound c from below with this convention only a few adaptations are needed to the algorithm stated below the new algorithm is a plane sweep algorithm like the one in chapter we move a sweep line downward over the plane and we maintain the edges of and intersecting the sweep line since and are convex there are at most four such edges hence there is no need to store these edges in a complicated data structure instead we simply have pointers left edge right edge left edge and right edge to them if the sweep line does not intersect the right or left boundary of a region then the corresponding pointer is nil figure illustrates the denitions how are these pointers initialized let be the y coordinate of the topmost vertex of if has an unbounded edge extending upward to innity then we dene dene similarly for and let ystart min to compute the intersection of and we can restrict our attention to the part of the plane with y coordinate less than or equal to ystart hence we let the sweep line start at ystart and we initialize the edges left edge right edge left edge and right edge as the ones intersecting the line y ystart left edge right edge nil section half plane intersection right edge left edge in a plane sweep algorithm one normally also needs a queue to store the events in our case the events are the points where edges of or of start or stop to intersect the sweep line this implies that the next event point which determines the next edge to be handled is the highest of the lower endpoints of the edges intersecting the sweep line endpoints with the same y coordinate are handled from left to right if two endpoints coincide then the leftmost edge is treated rst hence we don t need an event queue the next event can be found in constant time using the pointers left edge right edge left edge and right edge at each event point some new edge e appears on the boundary to handle the edge e we rst check whether e belongs to or to and whether it is on the left or the right boundary and then call the appropriate procedure we shall only describe the procedure that is called when e is on the left boundary of the other procedures are similar let p be the upper endpoint of e the procedure that handles e will discover three possible edges that c might have the edge with p as upper endpoint the edge with e left edge as upper endpoint and the edge with e right edge as upper endpoint it performs the following actions first we test whether p lies in between left edge and right edge if this is the case then e contributes an edge to c starting at p we then add the half plane whose bounding line contains e to the list lleft c next we test whether e intersects right edge if this is the case then the intersection point is a vertex of c either both edges contribute an edge to c starting at the intersection point this happens when p lies to the right of right edge as in figure i or both edges contribute an edge ending there this happens when p lies to the left of right edge as in figure ii if both edges contribute an edge starting at the intersection point then we have to add the half plane dening e to lleft c and the half plane dening right edge to lright c if they contribute an edge ending at the intersection point we do nothing these edges have already been discovered in some other way figure the edges maintained by the sweep line algorithm finally we test whether e intersects left edge if this is the case then the chapter linear programming i ii p edge e right edge figure the two possibilities when e intersects right edge p e left edge intersection point is a vertex of c the edge of c starting at that vertex is either a part of e or it is a part of left edge we can decide between these possibilities in constant time if p lies to the left of left edge then it is a part of e otherwise it is a part of left edge after we decided whether e or left edge contributes the edge to c we add the appropriate half plane to lleft c notice that we may add two half planes to lleft c the half plane bounding e and the half plane bounding left edge in which order should we add them we add left edge only if it denes an edge of c starting at the intersection point of left edge and e if we also decide to add the half plane of e it must be because e denes an edge of c starting at its upper endpoint or at its intersection point with right edge in both cases we should add the half plane bounding e rst which is guaranteed by the order of the tests given above we conclude that it takes constant time to handle an edge so the intersection of two convex polygons can be computed in time o n to show that the algorithm is correct we have to prove that it adds the half planes dening the edges of c in the right order consider an edge of c and let p be its upper endpoint then p is either an upper endpoint of an edge in or or it is the intersection of two edges e and el of and respectively in the former case we discover the edge of c when p is reached and in the latter case when the lower of the upper endpoints of e and el is reached hence all half planes dening the edges of c are added it is not difcult to prove that they are added in the correct order we get the following result theorem the intersection of two convex polygonal regions in the plane can be computed in o n time this theorem shows that we can do the merge step in intersecthalf planes in linear time hence the recurrence for the running time of the algorithm becomes t n o if n o n n if n leading to the following result corollary the common intersection of a set of n half planes in the plane can be computed in o n log n time and linear storage the problem of computing the intersection of half planes is intimately related to the computation of convex hulls and an alternative algorithm can be given that is almost identical to algorithm convexhull from chapter the relationship between convex hulls and intersections of half planes is discussed in detail in sections and those sections are independent of the rest of their chapters so if you are curious you can already have a look section incremental linear programming incremental linear programming in the previous section we showed how to compute the intersection of a set of n half planes in other words we computed all solutions to a set of n linear constraints the running time of our algorithm was o n log n one can prove that this is optimal as for the sorting problem any algorithm that solves the half plane intersection problem must take  n log n time in the worst case in our application to the casting problem however we don t need to know all solutions to the set of linear constraints just one solution will do ne it turns out that this allows for a faster algorithm finding a solution to a set of linear constraints is closely related to a well known problem in operations research called linear optimization or linear programming this term was coined before programming came to mean giving instructions to a computer the only difference is that linear program ming involves nding one specic solution to the set of constraints namely the one that maximizes a given linear function of the variables more precisely a linear optimization problem is described as follows maximize cdxd subject to dxd a2 dxd an an dxd bn where the ci and ai j and bi are real numbers which form the input to the problem the function to be maximized is called the objective function and the set of constraints together with the objective function is a linear program the number of variables d is the dimension of the linear program we already saw that linear constraints can be viewed as half spaces in rd the intersection of these half spaces which is the set of points satisfying all constraints is called the feasible region of the linear program points solutions in this region are called feasible points outside are infeasible recall from figure that the feasible region can be unbounded and that it can be empty in the latter case the linear program is called infeasible the objective function can be viewed as a direction in rd maximizing cdxd means nding chapter linear programming feasible region figure different types of solutions to a linear program a point xd that is extreme in the direction c cd hence the solution to the linear program is a point in the feasible region that is extreme in direction c we let f c denote the objective function dened by a direction vector c many problems in operations research can be described by linear programs and a lot of work has been dedicated to linear optimization this has resulted in many different linear programming algorithms several of which the famous simplex algorithm for instance perform well in practice let go back to our problem we have n linear constraints in two variables and we want to nd one solution to the set of constraints we can do this by taking an arbitrary objective function and then solving the linear program dened by the objective function and the linear constraints for the latter step we can use the simplex algorithm or any other linear programming algorithm developed in operations research however this particular linear program is quite different from the ones usually studied in operations research both the number of constraints and the number of variables are large but in our case the number of variables is only two the traditional linear programming methods are not very efcient in such low dimensional linear programming problems methods developed in computational geometry like the one described below do better we denote the set of n linear constraints in our dimensional linear program ming problem by h the vector dening the objective function is c cx cy thus the objective function is f c p cx px cy py our goal is to nd a point p such that p h and f c p is maximized we denote the linear program by h c and we use c to denote its feasible region we can distinguish four cases for the solution of a linear program h c the four cases are illustrated in figure the vector dening the objective function is vertically downward in the examples i ii iii iv i the linear program is infeasible that is there is no solution to the set of constraints ii the feasible region is unbounded in direction c in this case there is a ray  completely contained in the feasible region c such that the function f c takes arbitrarily large values along  the solution we require in this case is the description of such a ray iii the feasible region has an edge e whose outward normal points in the direction c in this case there is a solution to the linear program but it is not unique any point on e is a feasible point that maximizes f c p iv if none of the preceding three cases applies then there is a unique solution which is the vertex v of c that is extreme in the direction c our algorithm for dimensional linear programming is incremental it adds the constraints one by one and maintains the optimal solution to the intermediate linear programs it requires however that the solution to each intermediate problem is well dened and unique in other words it assumes that each intermediate feasible region has a unique optimal vertex as in case iv above to fulll this requirement we add to our linear program two additional constraints that will guarantee that the linear program is bounded for example if cx and cy we add the contraints px m and py m for some large m r the idea is that m should be chosen so large that the additional constraints do not inuence the optimal solution if the original linear program was bounded in many practical applications of linear programming a bound of this form is actually a natural restriction in our application to the casting problem for instance mechanical limitations will not allow us to remove the polyhedron in a direction that is nearly horizontal for instance we may not be able to remove the polyhedron in a direction whose angle with the xy plane is less than degree this constraint immediately gives a bound on the absolute value of px py we will discuss in section how we can correctly recognize unbounded linear programs and how we can solve bounded ones without enforcing articial constraints on the solution for preciseness let give a name to the two new constraints px m if cx px m otherwise section incremental linear programming and py m if cy py m otherwise note that are chosen as a function of c only they do not depend on the half planes h the feasible region is an orthogonal wedge another simple convention now allows us to say that case iii also has a unique solution if there are several optimal points then we want the lexico graphically smallest one conceptually this convention is equivalent to rotating c a little such that it is no longer normal to any half plane we have to be careful when doing this as even a bounded linear program may not have a lexicographically smallest solution see exercise our choice of the two constraints and is such that this cannot happen with these two conventions any linear program that is feasible has a unique solution which is a vertex of the feasible region we call this vertex the optimal vertex let h c be a linear program we number the half planes hn let hi be the set of the rst i constraints together with the special constraints and and let ci be the feasible region dened by these constraints hi hi ci hi chapter linear programming by our choice of each feasible region ci has a unique optimal vertex denoted vi clearly we have c2 cn c this implies that if ci for some i then cj for all j i and the linear program is infeasible so our algorithm can stop once the linear program becomes infeasible the next lemma investigates how the optimal vertex changes when we add a half plane hi it is the basis of our algorithm lemma let i n and let ci and vi be dened as above then we have i if vi hi then vi vi ii if vi hi then either ci or vi fi where fi is the line bounding hi proof i let vi hi because ci ci hi and vi ci this means that vi ci furthermore the optimal point in ci cannot be better than the optimal point in ci since ci ci hence vi is the optimal vertex in ci as well ii let vi hi suppose for a contradiction that ci is not empty and that vi does not lie on fi consider the line segment vi we have vi ci and since ci ci also vi ci together with the convexity of ci this implies that the segment vi is contained in ci since vi is the optimal point in ci and the objective function f c is linear it follows that f c p increases monotonically along vi as p moves from vi to vi now consider the intersection point q of vi and fi this intersection point exists because vi hi and vi ci since vi is contained in ci the point q must be in ci but the value of the objective function increases along vi so f c q f c vi this contradicts the denition of vi figure illustrates the two cases that arise when adding a half plane in figure i the optimal vertex that we have after adding the rst four half planes is contained in the next half plane that we add therefore the optimal vertex remains the same the optimal vertex is not contained in however so when we add we must nd a new optimal vertex according i ii c figure adding a half plane to lemma this vertex is contained in the line bounding as is shown in figure ii but lemma does not tell us how to nd the new optimal vertex fortunately this is not so difcult as we show next assume that the current optimal vertex vi is not contained in the next half plane hi the problem we have to solve can be stated as follows find the point p on fi that maximizes f c p subject to the con straints p h for h hi to simplify the terminology we assume that fi is not vertical and so we can parameterize it by x coordinate we can then dene a function f c r r such that f c p f c px for points p fi for a half plane h let  h fi be the x coordinate of the intersection point of fi and the bounding line of h if there is no intersection then either the constraint h is satised by any point on fi or by no point on fi in the former case we can ignore the constraint in the latter case we can report the linear program infeasible depending on whether fi h is bounded to the left or to the right we get a constraint on the x coordinate of the solution of the form x  h fi or of the form x  h fi we can thus restate our problem as follows maximize f c x subject to x  h fi h hi and fi h is bounded to the left x  h fi h hi and fi h is bounded to the right this is a dimensional linear program solving it is very easy let xleft max  h fi fi h is bounded to the left h hi section incremental linear programming and xright min  h fi fi h is bounded to the right hi the interval xleft xright is the feasible region of the dimensional linear x  h fi program hence the linear program is infeasible if xleft xright and otherwise the optimal point is the point on fi at either xleft or xright depending on the objective function note that the dimensional linear program cannot be unbounded due to the constraints and we get the following lemma lemma a dimensional linear program can be solved in linear time hence if case ii of lemma arises then we can compute the new optimal vertex vi or decide that the linear program is infeasible in o i time  h fi fi we can now describe the linear programming algorithm in more detail as above we use fi to denote the line that bounds the half plane hi algorithm h c input a linear program h c where h is a set of n half planes c and bound the solution output if h c is infeasible then this fact is reported otherwise the lexicographically smallest point p that maximizes f c p is reported chapter linear programming let be the corner of let hn be the half planes of h for i to n do if vi hi then vi vi else vi the point p on fi that maximizes f c p subject to the constraints in hi if p does not exist then report that the linear program is infeasible and quit return vn we now analyze the performance of our algorithm lemma algorithm computes the solution to a bounded linear program with n constraints and two variables in o time and linear storage proof to prove that the algorithm correctly nds the solution we have to show that after every stage whenever we have added a new half plane hi the point vi is still the optimum point for ci this follows immediately from lemma if the dimensional linear program on fi is infeasible then ci is empty and consequently c cn ci is empty which means that the linear program is infeasible it is easy to see that the algorithm requires only linear storage we add the half planes one by one in n stages the time spent in stage i is dominated by the time to solve a dimensional linear program in line which is o i hence the total time needed is bounded by n o i o i although our linear programming algorithm is nice and simple its running time is disappointing the algorithm is much slower than the previous algorithm c v3 vn hn v5 v4 h4 which computed the whole feasible region is our analysis too crude we bounded the cost of every stage i by o i this is not always a tight bound stage i takes  i time only when vi hi when vi hi then stage i takes constant time so if we could bound the number of times the optimal vertex changes we might be able to prove a better running time unfortunately the optimum vertex can change n times there are orders for some congurations where every new half plane makes the previous optimum illegal the gure in the margin shows such an example this means that the algorithm will really spend  time how can we avoid this nasty situation randomized linear programming if we have a second look at the example where the optimum changes n times we see that the problem is not so much that the set of half planes is bad if we had added them in the order hn hn then the optimal vertex would not change anymore after the addition of hn in this case the running time would be o n is this a general phenomenon is it true that for any set h of half planes there is a good order to treat them the answer to this question is yes but that doesn t seem to help us much even if such a good order exists there seems to be no easy way to actually nd it remember that we have to nd the order at the beginning of the algorithm when we don t know anything about the intersection of the half planes yet we now meet a quite intriguing phenomenon although we have no way to determine an ordering of h that is guaranteed to lead to a good running time we have a very simple way out of our problem we simply pick a random ordering of h of course we could have bad luck and pick an order that leads to a quadratic running time but with some luck we pick an order that makes it run much faster indeed we shall prove below that most orders lead to a fast algorithm for completeness we rst repeat the algorithm algorithm h c input a linear program h c where h is a set of n half planes c and bound the solution output if h c is infeasible then this fact is reported otherwise the lexicographically smallest point p that maximizes f c p is reported let be the corner of compute a random permutation hn of the half planes by calling randompermutation h n for i to n do if vi hi then vi vi else vi the point p on fi that maximizes f c p subject to the constraints in hi if p does not exist then report that the linear program is infeasible and quit 9 return vn the only difference from the previous algorithm is in line where we put the half planes in random order before we start adding them one by one to be able to do this we assume that we have a random number generator random k which has an integer k as input and generates a random integer between and k in constant time computing a random permutation can then be done with the following linear time algorithm algorithm randompermutation a input an array a n output the array a n with the same elements but rearranged into a random permutation for k n downto do rndindex random k section randomized linear programming exchange a k and a rndindex chapter linear programming the new linear programming algorithm is called a randomized algorithm its running time depends on certain random choices made by the algorithm in the linear programming algorithm these random choices were made in the subroutine randompermutation what is the running time of this randomized version of our incremental linear programming algorithm there is no easy answer to that it all depends on the order that is computed in line consider a xed set h of n half planes treats them depending on the permutation cho sen in line since there are n possible permutations of n objects there are n possible ways in which the algorithm can proceed each with its own run ning time because the permutation is random each of these running times is equally likely so what we do is analyze the expected running time of the algorithm which is the average running time over all n possible permutations the lemma below states that the expected running time of our randomized linear programming algorithm is o n it is important to realize that we do not make any assumptions about the input the expectancy is with respect to the random order in which the half planes are treated and holds for any set of half planes lemma the dimensional linear programming problem with n constraints can be solved in o n randomized expected time using worst case linear storage proof as we observed before the storage needed by the algorithm is linear the running time randompermutation is o n so what remains is to analyze the time needed to add the half planes hn adding a half plane takes constant time when the optimal vertex does not change when the optimal vertex does change we need to solve a dimensional linear program we now bound the time needed for all these dimensional linear programs let xi be a random variable which is if vi hi and otherwise recall that a dimensional linear program on i constraints can be solved in o i time the total time spent in line over all half planes hn is therefore n o i xi i to bound the expected value of this sum we will use linearity of expectation the expected value of a sum of random variables is the sum of the expected values of the random variables this holds even if the random variables are dependent hence the expected time for solving all dimensional linear programs is c n n e o i xi o i e xi i i half planes dening vn but what is e xi it is exactly the probability that vi hi let analyze this probability we will do this with a technique called backwards analysis we look at the algorithm backwards assume that it has already nished and that it has computed the optimum vertex vn since vn is a vertex of cn it is dened by at least two of the half planes now we make one step backwards in time and look at cn note that cn is obtained from cn by removing the half plane hn when does the optimum point change this happens exactly if vn is not a vertex of cn that is extreme in the direction c which is only possible if hn is one of the half planes that dene vn but the half planes are added in random order so hn is a random element of hn hence the probability that hn is one of the half planes dening vn is at most n why do we say at most first it is possible that the boundaries of more than two half planes pass through vn in that case removing one of the two half planes containing the edges incident to vn may fail to change vn furthermore vn may be dened by or which are not among the n candidates for the random choice of hn in both cases the probability is less than n the same argument works in general to bound e xi we x the subset of the rst i half planes this determines ci to analyze what happened in the last step when we added hi we think backwards the probability that we had to compute a new optimal vertex when adding hi is the same as the probability that the optimal vertex changes when we remove a half plane from ci the latter event only takes place for at most two half planes of our xed set hi since the half planes are added in random order the probability that hi is one of the special half planes is at most i we derived this probability under the condition that the rst i half planes are some xed subset of h but since the derived bound holds for any xed subset it holds unconditionally hence e xi i we can now bound the expected total time for solving all dimensional linear programs by section unbounded linear programs n o i o n i we already noted that the time spent in the rest of the algorithm is o n as well note again that the expectancy here is solely with respect to the random choices made by the algorithm we do not average over possible choices for the input for any input set of n half planes the expected running time of the algorithm is o n there are no bad inputs unbounded linear programs in the preceding sections we avoided handling the case of an unbounded linear program by adding two additional articial constraints this is not always a suitable solution even if the linear program is bounded we may not know a large enough bound m furthermore unbounded linear programs do occur in practice and we have to solve them correctly let rst see how we can recognize whether a given linear program h c is unbounded as we saw before that means that there is a ray  completely chapter linear programming contained in the feasible region c such that the function f c takes arbitrarily large values along  if we denote the ray starting point as p and its direction vector as d we can parameterize  as follows  p  d  the function f c takes arbitrarily large values if and only if d c on the other hand if  h is the normal vector of a half plane h h oriented towards the feasible side of h bounding line we have d  h the next lemma shows that these two necessary conditions on d are sufcient to test whether a linear program is unbounded lemma 9 a linear program h c is unbounded if and only if there is a vector d with d c such that d  h for all h h and the linear program hl c is feasible where hl h h  h d proof the only if direction follows from the argument above so it remains to show the if direction we consider a linear program h c and a vector d with the conditions of the lemma since hl c is feasible there is a point h hl h consider now the ray  d  since d  h for h hl the ray is completely contained in each h hl furthermore since d c the objective function f c takes arbitrarily large values along for a half plane h h hl we have d  h this implies that there is a parameter h such that  d h for all  h let  l maxh h hl h and p  ld it follows that the ray  p  d  is completely contained in each half plane h h and so h c is unbounded we can now test whether a given dimensional linear program h c is unbounded by proceeding similarly to section and solving a dimensional linear program let rst rotate the coordinate system so that c is the upward vertical direction c any direction vector d dx dy with d c can be normalized to the form d dx and be represented by the point dx on the line y given a normal vector  h x y the inequality d  h dxx y translates to the inequality dxx y we thus obtain a system of n linear inequalities or in other words a dimensional linear program h this is actually an abuse of the terminology since a linear program consists of constraints and an objective function but since at this point we are only interested in feasibility it is convenient to ignore the objective function if h has a feasible solution dx we identify the set hl h of half planes h for which the solution is tight that is where dx x y we still need to verify that the system hl is feasible are we again left with a dimensional linear programming problem yes but a very special one for each h hl the normal  h is orthogonal to d dx and that means that the bounding line of h is parallel to d in other words all half planes in hl are bounded by parallel lines and by intersecting them with the x axis we have again a dimensional linear program hl if hl is feasible then the original linear program is unbounded and we can construct a feasible ray  in time o n as in the lemma above if hl is infeasible then so is hl and therefore h if h does not have a feasible solution by the lemma above the original linear program h c is bounded can we extract some more information in this case recall the solution for dimensional linear programs h is infeasible if and only if the maximum boundary of a half line bounded to the left is larger than the minimum boundary of a half line bounded to the right these two half lines and have an empty intersection if and are the original half planes that correspond to these two constraints then this is equivalent to saying that c is bounded we can call and certicates they prove that h c is really bounded how useful certicates are becomes clear with the following observation after nding the two certicates and we can use them like and in that means that we no longer need to make an articial restriction on the range in which we allow the solution to lie again we must be careful it can happen that the linear program c is bounded but has no lexicographically smallest solution this is the case when the dimensional linear program is infeasible due to a single constraint namely when  c in that case we scan the remaining list of half planes for a half plane with x if we are successful and are certicates that guarantee a unique lexicographically smallest solution if no such exists the linear program is either infeasible or it has no lexicographically smallest solution we can solve it by solving the dimensional linear program formed by all half planes h with x h if it is feasible we can return a ray  in direction such that all points on  are feasible optimal solutions we can now give a general algorithm for the dimensional linear program ming problem algorithm h c input a linear program h c where h is a set of n half planes and c output if h c is unbounded a ray is reported if it is infeasible then two or three certicate half planes are reported otherwise the lexicographically smallest point p that maximizes f c p is reported determine whether there is a direction vector d such that d c and d  h for all h h if d exists then compute hl and determine whether hl is feasible section 5 unbounded linear programs if hl is feasible chapter linear programming 5 then report a ray proving that h c is unbounded and quit else report that h c is infeasible and quit let h be certicates proving that h c is bounded and has a unique lexicographically smallest solution let be the intersection of and f2 9 let h4 hn be a random permutation of the remaining half planes in h for i to n do if vi hi then vi vi else vi the point p on fi that maximizes f c p subject to the constraints in hi 14 if p does not exist then let h j hk with j k i be the certicates possibly h j hk with h j hk fi report that the linear program is infeasible with hi h j hk as certicates and quit return vn we summarize our results so far in the following theorem theorem a dimensional linear programming problem with n constraints can be solved in o n randomized expected time using worst case linear storage linear programming in higher dimensions the linear programming algorithm presented in the previous sections can be generalized to higher dimensions when the dimension is not too high then the resulting algorithm compares favorably with traditional algorithms such as the simplex algorithm let h be a set of n closed half spaces in rd given a vector c cd we want to nd the point p pd rd that maximizes the linear function f c p cd pd subject to the constraint that p lies in h for all h h to make sure that the solution is unique when the linear program is bounded we agree to look for the lexicographically smallest point that maximizes f c p as in the planar version we maintain the optimal solution while incremen tally adding the half space constraints one by one for this to work we again need to make sure that there is a unique optimal solution at each step we do this as in the previous section we rst determine whether the linear program is unbounded if not we obtain a set of d certicates hd h that guar antee that the solution is bounded and that there is a unique lexicographically smallest solution we ll look at the details of nding these certicates later and concentrate on the main algorithm for the moment let hd be the d certicate half spaces obtained by checking that the linear program is bounded and let hd hd hn be a random permuta tion of the remaining half spaces in h furthermore dene ci to be the feasible region when the rst i half spaces have been added for d i n ci hi let vi denote the optimal vertex of ci that is the vertex that maximizes f c lemma 5 gave us an easy way to maintain the optimal vertex in the dimensional case either the optimal vertex doesn t change or the new optimal vertex is contained in the line that bounds the half plane hi that we are adding the following lemma generalizes this result to higher dimensions its proof is a straightforward generalization of the proof of lemma 5 lemma let i n and let ci and vi be dened as above then we have i if vi hi then vi vi ii if vi hi then either ci or vi gi where gi is the hyperplane that bounds hi if we denote the hyperplane that bounds the half space hi by gi the optimal vertex vi of ci can be found by nding the optimal vertex of the intersection gi ci but how do we nd the optimal vertex of gi ci in two dimensions this was easy to do in linear time because everything was restricted to a line let look at the dimensional case in three dimensions gi is a plane and gi ci is a dimensional convex polygonal region what do we have to do to nd the optimum in gi ci we have to solve a dimensional linear program the linear function f c dened in induces a linear function in gi and we need to nd the point in gi ci that maximizes this function in case c is orthogonal to gi all points on gi are equally good following our rule we then need to nd the lexicographically smallest solution we achieve this by choosing the objective function correctly for instance when gi is not orthogonal to the axis we obtain the vector c by projecting the vector onto gi so in the dimensional case we nd the optimal vertex of gi ci as follows we compute the intersection of all i half spaces with gi and project the vectors section linear programming in higher dimensions c on gi until a projection is non zero this results in a linear program in two dimensions which we solve using algorithm by now you can probably guess how we will attack the general d dimensional case there gi is a hyperplane a d dimensional subspace and we have to nd the point in the intersection ci gi that maximizes f c this is a linear program in d dimensions and so we will solve it by making a recursive call to the d dimensional version of our algorithm the recursion bottoms out when we get to a dimensional linear program which can be solved directly in linear time we still need to determine whether the linear program is unbounded and to nd suitable certicates if that is not the case we rst verify that lemma 9 chapter linear programming holds in arbitrary dimensions the lemma and its proof need no change the lemma implies that the d dimensional linear program h c is bounded if and only if a certain d dimensional linear program is infeasible we will solve this d dimensional linear program by a recursive call if the d dimensional linear program is feasible we obtain a direction vector d the d dimensional linear program is then either unbounded in di rection d or infeasible this can be determined by verifying whether hl c is feasible where hl is as dened in lemma 9 the boundaries of all the half spaces in hl are parallel to d and so this can be decided by solving a second d dimensional program with a second recursive call if the d dimensional linear program is infeasible its solution will give us k certicate half spaces hk h with k d that prove that h c is bounded if k d then the set of optimal solutions to hk c is unbounded in that case these optimal solutions form a d k dimensional subspace we determine whether the linear program restricted to this subspace is bounded with respect to the lexicographical order if not we can report the solution otherwise we can repeat the process until we obtain a set of d certicates with a unique solution the global algorithm now looks as follows again we use gi to denote the hyperplane that bounds the half space hi algorithm randomizedlp h c input a linear program h c where h is a set of n half spaces in rd and c rd output if h c is unbounded a ray is reported if it is infeasible then at most d certicate half planes are reported otherwise the lexicographically smallest point p that maximizes f c p is reported determine whether a direction vector d exists such that d c and d  h for all h h if d exists then compute hl and determine whether hl is feasible if hl is feasible 5 then report a ray proving that h c is unbounded and quit 6 else report that h c is infeasible provide certicates and quit let hd be certicates proving that h c is bounded let vd be the intersection of gd 9 compute a random permutation hd hn of the remaining half spaces in h 10 for i d to n 11 do if vi hi then vi vi else vi the point p on gi that maximizes f c p subject to the constraints hi 14 if p does not exist then let h be the at most d certicates for the infeasi bility of the d dimensional program report that the linear program is infeasible with h hi as certicates and quit return vn the following theorem states the performance of randomizedlp although we consider d a constant which means we can state an o n bound on the running time it is useful to have a close look at the dependency of the running time on d see the end of the proof the following theorem theorem 12 for each xed dimension d a d dimensional linear programming problem with n constraints can be solved in o n expected time proof we must prove that there is a constant cd such that the algorithm takes at most cdn expected time we proceed by induction on the dimension d for two dimensions the result follows from theorem 10 so let assume d the induction step is basically identical to the proof of the dimensional cases we start by solving at most d linear programs of dimension d by the induction assumption this takes time o dn dcd the algorithm spends o d time to compute vd testing whether vi hi takes o d time the running time is therefore o dn as long as we do not count the time spent in line in line 13 we need to project c on gi in time o d and to intersect i half spaces with gi in time o di furthermore we make a recursive call with dimension d and i half spaces dene a random variable xi which is if vi hi and otherwise the total expected time spent by the algorithm is bounded by section 6 linear programming in higher dimensions o dn dcd n o di cd i e xi i d to bound e xi we apply backwards analysis consider the situation after adding hi the optimum point is a vertex vi of ci so it is dened by d of the half spaces now we make one step backwards in time the optimum point changes only if we remove one of the half spaces dening vi since hd hi is a random permutation the probability that this happens is at most d i d consequently we get the following bound for the expected running time of the algorithm o dn dc n n o di c i d d i d d i d this can be bounded by cdn with cd o cd so cd o cdd for a constant c indpendent on the dimension when d is a constant it is correct to say that the algorithm runs in linear time still that would be quite misleading the constant factor cd grows so fast as a function of d that this algorithm is useful only for rather small dimensions chapter linear programming smallest enclosing discs the simple randomized technique we used above turns out to be surprisingly powerful it can be applied not only to linear programming but to a variety of other optimization problems as well in this section we shall look at one such problem consider a robot arm whose base is xed to the work oor the arm has to pick up items at various points and place them at other points what would be a good position for the base of the arm this would be somewhere in the middle of the points it must be able to reach more precisely a good position is at the center of the smallest disc that encloses all the points this point minimizes the maximum distance between the base of the arm and any point it has to reach we arrive at the following problem given a set p of n points in the plane the points on the work oor that the arm must be able to reach nd the smallest enclosing disc for p that is the smallest disc that contains all the points of p this smallest enclosing disc is unique see lemma 14 i below which is a generalization of this statement pi pi di di as in the previous sections we will give a randomized incremental algorithm for the problem first we generate a random permutation pn of the points in p let pi pi we add the points one by one while we maintain di the smallest enclosing disc of pi in the case of linear programming there was a nice fact that helped us to maintain the optimal vertex when the current optimal vertex is contained in the next half plane then it does not change and otherwise the new optimal vertex lies on the boundary of the half plane is a similar statement true for smallest enclosing discs the answer is yes lemma 13 let i n and let pi and di be dened as above then we have di i if pi di then di di ii if pi di then pi lies on the boundary of di we shall prove this lemma later after we have seen how we can use it to design a randomized incremental algorithm that is quite similar to the linear programming algorithm algorithm minidisc p input a set p of n points in the plane output the smallest enclosing disc for p compute a random permutation pn of p let be the smallest enclosing disc for for i to n do if pi di 5 then di di 6 else di minidiscwithpoint pi pi 7 return dn the critical step occurs when pi di we need a subroutine that nds the smallest disc enclosing pi using the knowledge that pi must lie on the boundary of that disc how do we implement this routine let q pi we use the same framework once more we add the points of pi in random order and maintain the smallest enclosing disc of pi q under the extra constraint that it should have q on its boundary the addition of a point p j will be facilitated by the following fact when p j is contained in the currently smallest enclosing disc then this disc remains the same and otherwise it must have p j on its boundary so in the latter case the disc has both q and p j and its boundary we get the following subroutine minidiscwithpoint p q input a set p of n points in the plane and a point q such that there exists an enclosing disc for p with q on its boundary output the smallest enclosing disc for p with q on its boundary compute a random permutation pn of p let be the smallest disc with q and on its boundary for j to n do if p j dj 5 then dj dj 6 else dj p j p j q 7 return dn section 7 smallest enclosing discs how do we nd the smallest enclosing disc for a set under the restriction that two given points and are on its boundary we simply apply the same approach one more time thus we add the points in random order and maintain the optimal disc when the point pk we add is inside the current disc we don t have to do anything and when pk is not inside the current disc it must be on the boundary of the new disc in the latter case we have three points on the disc boundary and pk this means there is only one disc left the unique disc with and pk on its boundary this following routine describes this in more detail p input a set p of n points in the plane and two points and such that there exists an enclosing disc for p with and on its boundary output the smallest enclosing disc for p with and on its boundary let be the smallest disc with and on its boundary for k to n 3 do if pk dk then dk dk 5 else dk the disc with and pk on its boundary 6 return dn this nally completes the algorithm for computing the smallest enclosing disc of a set of points before we analyze it we must validate its correctness by proving some facts that we used in the algorithms for instance we used the chapter linear programming z x x fact that when we added a new point and this point was outside the current optimal disc then the new optimal disc must have this point on its boundary lemma 14 let p be a set of points in the plane let r be a possibly empty set of points with p r and let p p then the following holds i if there is a disc that encloses p and has all points of r on its boundary then the smallest such disc is unique we denote it by md p r ii if p md p p r then md p r md p p r iii if p md p p r then md p r md p p r p proof i assume that there are two distinct enclosing discs and with centers and respectively and with the same radius clearly all points of p must lie in the intersection we dene a continuous family d   of discs as follows let z be an intersection point of and the boundaries of and the center of d  is the point x   and the radius of d  is r  d x  z x  d  we have d  for all  with  and in particular for  2 hence since both and enclose all points of p so must d 2 moreover d 2 passes through the intersection points of and because r this implies that r d 2 in other words d 2 is an enclosing disc for p with r on its boundary but the radius of d 2 is strictly less than the radii of and so whenever there are two distinct enclosing discs of the same radius with r on their boundary then there is a smaller enclosing disc with r on its boundary hence the smallest enclosing disc md p r is unique ii let d md p p r if p d then d contains p and has r on its boundary there cannot be any smaller disc containing p with r on its boundary because such a disc would also be a containing disc for p p with r on its boundary contradicting the denition of d it follows that d md p r iii let md p p r and let md p r consider the family d  of discs dened above note that d and d so the family denes a continous deformation of to by assumption we have p we also have p so by continuity there must be some  such that p lies on the boundary of d  as in the proof of i we have p d  and r d  since the radius of any d  with  is strictly less than the radius of and is by denition the smallest enclosing disc for p we must have  in other words has p on its boundary lemma 14 implies that minidisc correctly computes the smallest enclos ing disc of a set of points the analysis of the running time is given in the proof of the following theorem theorem 15 the smallest enclosing disc for a set of n points in the plane can be computed in o n expected time using worst case linear storage proof runs in o n time because every iteration of the loop takes constant time and it uses linear storage minidiscwithpoint and minidisc also need linear storage so what remains is to analyze their expected running time the running time of minidiscwithpoint is o n as long as we don t count the time spent in calls to what is the prob ability of having to make such a call again we use backwards analysis to bound this probability fix a subset pi and let di be the smallest disc enclosing pi and having q on its boundary imagine that we remove one of the points pi when does the smallest enclosing circle change that happens only when we remove one of the three points on the boundary one of the points on the boundary is q so there are at most two points that cause the smallest enclosing circle to shrink the probability that pi is one of those points is 2 i when there are more than three points on the boundary then the probability that the smallest enclosing circle changes can only get smaller so we can bound the total expected running time of minidiscwithpoint by section 8 notes and comments q di points that together with n 2 q dene di o n o i o n i 2 applying the same argument once more we nd that the expected running time of minidisc is o n as well algorithm minidisc can be improved in various ways first of all it is not necessary to use a fresh random permutation in every instance of subroutine minidiscwithpoint instead one can compute a permutation once at the start of minidisc and pass the permutation to minidiscwithpoint furthermore instead of writing three different routines one could write a single algorithm minidiscwithpoints p r that computes md p r as dened in lemma 4 14 cs algorithm implementation https people cs pitt edu bill home cs algorithm implementation section d spring home policy lectures programs writing general information course description this course will cover a broad range of the most commonly used algorithms including algorithms for sorting searching encryption compression and local cs algorithm implementation https people cs pitt edu bill home search students will learn to implement algorithms and analyze their performance textbook robert sedgewick and kevin wayne algorithms edition addison wesley isbn x grading non writing section first exam feb second exam apr programming assignments recitation attendance and quizzes grading writing section first exam feb second exam apr programming assignments writing assignments recitation attendance and quizzes cs coe cs pitt edu bill introduction these notes are intended for use by students in cs coe at the university of pittsburgh they are provided free of charge and may not be sold in any shape or form these notes are not a substitute for material covered during course lectures if you miss a lecture it is your responsibility to obtain written notes from a classmate who attended the lecture material from these notes is obtained from various sources including but not limited to the following algorithms in c by robert sedgewick algorithms edition by robert sedgewick and kevin wayne introduction to algorithms by cormen leiserson and rivest various java and c textbooks various online resources see notes for specifics dr william garrison bill cs pitt edu office sennott square no recitations this friday prefix all email subjects with address all emails to both the instructor and the ta be sure to mention the section of the class you are in section d and writing non writing website cs pitt edu bill review the course information and policies assignments will not be accepted after the deadline no late assignment submissions if you do not submit an assignment by the deadline you will receive a for that assignment up until now your classes have focused on how you could solve a problem here we will start to look at how you should solve a problem first some definitions offline problem we provide the computer with some input and after some time receive some acceptable output algorithm a step by step procedure for solving a problem or accomplishing some end program an algorithm expressed in a language the computer can understand an algorithm solves a problem if it produces an acceptable output on every input to learn to convert non trivial algorithms into programs many seemingly simple algorithms can become much more complicated as they are converted into programs algorithms can also be very complex to begin with and their implementation must be considered carefully various issues will always pop up during implementation such as pseudocode for dynamic programming algorithm for relational query optimization the optimizer portion of the postgresql codebase is over lines of code i e not counting blank comment lines to see and understand differences in algorithms and how they affect the run times of the associated programs different algorithms can be used to solve the same problem different solutions can be compared using many metrics run time is a big one better run times can make an algorithm more desirable better run times can sometimes make a problem solution feasible where it was not feasible before there are other metrics though implement it and measure performance any problems with this approach algorithm analysis determine resource usage as a function of input size measure asymptotic performance performance as input size increases to infinity problem given a set of arbitrary integers could be negative find out how many distinct triples sum to exactly zero simple solution triple for loops public static int count int a int n a length int count for int i i n i for int j i j n j for int k j k n k if a i a j a k count return count big o upper bound on asymptotic performance as we go to infinity function representing resource consumption will not exceed specified function e g saying runtime is o means that as input size n approaches infinity actual runtime will not exceed assuming that definition is threesum o what about o what about o if all of these are true why was o what we jumped to to start big omega lower bound on asymptotic performance theta upper and lower bound on asymptotic performance exact bound f x is o g x if constants c and exist such that f x c g x x f x is  g x if constants c and exist such that f x c g x x if f x is o g x and  g x then f x is  g x and exist such that g x f x g x x may also see f x o g x or f x o g x used to mean that f x is o g x same for  and  runtime primarily determined by two factors cost of executing each statement determined by machine used environment running on the machine frequency of execution of each statement determined by program and input public static int count int a int n a length int count for int i i n i for int j i j n j for int k j k n k if a i a j a k count return count ignore multiplicative constants and lower terms use standard measures for comparison constant logarithmic log n linear n linearithmic n log n quadratic cubic exponential factorial n threesum order of growth upper bound o lower bound  and hence  tilde approximations introduced in section of the text in this case how can we ignore lower order terms and multiplicative constants remember this is asymptotic analysis why do we need to bother with big o and big omega is there a better way to solve the problem what if we sorted the array first pick two numbers then binary search for the third one that will make a sum of zero a i a j binary search for still have two for loops but we replace the third with a binary search runtime now what if the input data isn t sorted see threesumfast java sorting searching hashing compression heaps and priority queues graph algorithms cryptography p vs np heuristic approximation dynamic programming cs coe cs pitt edu bill sorting given a list of n items place the items in a given order ascending or descending numerical alphabetical etc boolean less comparable v comparable w return v compareto w void swap object a int i int j object temp a i a i a j a j temp iterate through the array comparing adjacent pairs of items swap them if they are out of relative order repeat until you make it through the array with swaps void bubblesort comparable a swapped void bubblesort comparable a runtime o a lthough the techniques used in the calculations to analyze the bubble sort are instructive the results are disappointing since they tell us that the bubble sort isn t really very good at all donald knuth the art of computer programming what is the most efficient way to sort a million bit integers i think the bubble sort would be the wrong way to go divide and conquer void sort comparable a comparable aux int lo int hi if hi lo return int mid lo hi lo sort a aux lo mid sort a aux mid hi merge a aux lo mid hi merge comparable a comparable aux int lo int mid int hi for int k lo k hi k aux k a k runtime o n log n so what the catch now we need o n space available for the aux array sort does not occur in place choose a pivot value place the pivot in the array such that all items at lower indices are less than pivot and all higher indices are greater recurse for lesser indices and greater indices void sort comparable a int lo int hi if hi lo return int j partition a lo hi sort a lo j sort a j hi int partition comparable a int lo int hi int i lo j hi comparable v a lo while true while less a i v if i hi break while less v a j if j lo break if i j break swap a i j swap a lo j return j lo v hi i j runtime in place stable sorting maintains the relative ordering of tied values comparison sort runtime of o n log n is optimal the problem of sorting cannot be solved using comparisons with less than n log n time complexity see proposition i in chapter of the text consider the following approach look at the least significant digit group numbers with the same digit maintain relative order place groups back in array together i e all the all the all the etc repeat for increasingly significant digits runtime n length of items in collection we ll say nk how can we compare this to the n log n runtime that is optimal for comparison based sorts also why is it called radix sort in place stable bit integers don t take up a whole lot of space mb what if we needed to sort of numbers won t all fit in memory we had been assuming we were performing internal sorts everything in memory we now need to consider external sorting where we need to write to disk read in amount of data that will fit in memory sort it in place i e via quick sort write sorted chunk of data to disk repeat until all data is stored in sorted chunks merge chunks together should we merge all chunks together at once means fewer disk read writes each merge pass reads writes every value but also more disk seeks can we do parallel reads writes to multiple disks can we use multiple cpus cores to speed up processing what about when you have of data in google sorted trillion byte records on computers in hours minutes hard drives were involved at least disk failed during each run of the sort cs coe cs pitt edu bill brute force search find the solution to a problem by considering all potential solutions and selecting the correct one run time is bounded by the number of potential solutions potential solutions means cubic run time potential solutions means exponential run time brute force password attacks depend on the length of the password hence the insecurity of short passwords we can view the series of guesses we make as a tree each path from root to leaf is an attempted solution root 001 this tree will enumerate different pins n is the length of the pin so for our case different pins note that this is for a computer tiny what would be a long password for a computer say bits long different passwords assuming a supercomputer can check passwords per second and we ll on average find the correct password after guessing half the possibilities we should be able to crack a bit password on our supercomputer in x years using brute force what if we have background knowledge that the pin we re trying to crack doesn t have more than one removes entire subtrees of our search space exploration when we can use it it makes our algorithm practical for much larger values of n does not however affect the asymptotic performance of an algorithm still exponential time requirement for our pin example for the pin example a whole bunch of for loops would do in general exhaustive search trees can be easily traversed via recursion and backtracking recurse until its apparent no solution can be achieved along the current path undo the path to the point that you can start to move forward again place queens on a chessboard such that no queen can take another queens can move horizontally vertically and diagonally how many ways can you places pieces on a chess board c meaning total queen placements do we really need to look through all of these options solutions only have one queen per column still possible combinations solutions only have one queen per row combining these two observations only looking quite feasible finally prune subtrees with queens on the same diagonal basic idea recurse over columns of the board each recursive call iterates through the rows of the board check rows diagonals are they currently safe place a queen in the current row col if you are at the end of the board you ve found a solution otherwise try recursive call for the next column if they are not currently safe continue to the next row in the current recursive call a b c d e f g h words at least adjacent letters long must be assembled from a grid adjacent letters are horizontally vertically or diagonally neighboring any cube in the grid can only be used once per word have different options from each cube from b i j b i j b i j b i j b i j b i j b i j b i j b i j naively the runtime here would be 000 constructing the words over the course of recursion will mean building up and tearing down strings moving forward adds a new character to the current word string backtracking removes the most recent character basically pushing popping to from a string stack push pop stack operations are generally  unless you need to resize but that cost can be amortized java strings however are immutable new string here is a basic string this operation allocates and initializes all over again becomes essentially a  n operation where n is the length of the string append and deletecharat can be used to push and pop back to  still need to account for resizing though stringbuffer can also be used for this purpose differences cs coe cs pitt edu bill searching given a collection of keys c how to we search for the value associated with a given key k store collection in an array unsorted sorted linked list unsorted sorted binary search tree differences runtimes abstract structures that link keys to values key is used to search the data structure for a value described as a class in the text but probably more accurate to think of the concept of a symbol table in general as an interface key functions put contains get follows similarly binarysearchst java and bst java present symbol tables based on sorted arrays and binary search trees respectively can we do better than these both methods depend on comparisons against other keys i e k is compared against other keys in the data structure options at each node in a bst node ref is null k not found k is equal to the current node key k is found k is less than current key continue to left child k is greater than the current key continue to right child instead of looking at less than greater than let go left right based on the bits of the key so we again have options node ref is null k not found k is equal to the current node key k is found current bit of k is continue to left child current bit of k is continue to right child search runtime we end up doing many comparisons against the full key can we improve on this trie as in retrieve pronounced the same as try instead of storing keys as nodes in the tree we store them implicitly as paths down the tree interior nodes of the tree only serve to direct us according to the bitstring of the key values can then be stored at the end of key bit string path search runtime would this structure work as well for other key data types characters strings in our binary based radix search trie we considered one bit at a time what if we applied the same method to characters in a string what would like this new structure look like let try inserting the following strings into an trie she sells sea shells by the sea shore b t y e h h a l e o e l l r l e runtime miss times require an average of logr n nodes to be examined where r is the size of the alphabet being considered proof in proposition h of section of the text average of checks with keys in an rst with keys in an r way trie assuming bit ascii see triest java implements an r way trie basic node object where r is the branching factor private static class node private object val private node next new node r non null val means we have traversed to a valid key again note that keys are not directly stored in the trie at all val next a b c d e f g h i j k l m n o p q r s t u v w x y z val next a b c d e f g h i j k l m n o p q r s t u v w x y z space considering bit ascii each node contains references this is especially problematic as in many cases a lot of this space is wasted common paths or prefixes for example e g if all keys begin with key that wasted references at the lower levels of the trie most keys have probably been separated out and reference lists will be sparse replace the next array of the r way trie with a linked list dlb example how does dlb performance differ from r way tries which should you use so far we ve continually assumed each search would only look for the presence of a whole key what about prefix search as was needed for boggle this lecture does not present an exhaustive look at search trees tries just the sampling that we re going to focus on many variations on these techniques exist and perform quite well in different circumstances red black bsts ternary search tries r way tries without way branching see the table at the end of section of the text cs coe cs pitt edu bill b trees the variant of b trees presented here differs slightly from that presented in the book b trees are not discussed in the book we ve discussed several approaches to search through a set of keys and retrieve a value several implementations of a symbol table all of them assumed we were storing the keys values the symbol table in memory what if data needs to be stored on disk what should we do differently you re writing software that will be used to store records of online store transactions each with a unique id e g vinyl album sales you ll want to store these records on disk you expect a large volume of transaction records you want the transaction records stored in non volatile memory how can you still efficiently search for a given transaction by its id data stored on disk is grouped into blocks typically of size i o to the disk is performed at the block level to read a file from disk the os will fetch all of the blocks that store some portion of that file and read the data from each block operates similarly to a binary search tree but not limited to a branching factor of the order of a b tree determines the max branching factor invariants for an order m b tree nodes have a max of m children interior nodes have at min of m children nodes that are not the root or leaves corollary all interior nodes must be at least half full root has at least two children if it is not a leaf node non leaf nodes with k children have k keys stored all leaves appear on the same level start with a single node add keys until the node fills i e contains m keys has m children in adding the mth key split the node in two pull one key up to the parent node potentially creating a new parent node ok so how does this help us store transaction records see how to store ids as keys but what about full records of a sale transaction id customer info price item purchased how many purchased etc runtime search insert to maintain invariants tree must be self balancing find and delete the key if the key is not in a leaf node you need to find a replacement rebalance the tree is there a sibling node with more than minimum keys if so rotate right left accordingly if not need to merge with the left or right sibling how long will it take us to find all the disk blocks containing records is there a better way maintain a copy of all keys in the leaves of the tree create a linked list out of the leaf nodes of the tree defining order here m is the max number of children elsewhere could be the min number of keys min was the original notation but is ambiguous where to go to follow keys some implementations have left link point to keys and right point to keys strictly others have left point to keys strictly and right point to keys the variant of b trees presented here differs slightly from that presented in the book b trees are not discussed in the book typically you ll store such records in a database but how does the database store records ibm informix microsoft sql server oracle sybase ase and sqlite all use b trees to store tables indexes other applications ntfs reiserfs nss xfs jfs refs and bfs all use b trees for metadata indexing cs coe cs pitt edu bill hashing search through a collection could be accomplished in  with relatively small memory needs let try this assume we have an array of length m call it ht assume we have a function h x that maps from our key space to m e g h  m integer keys let also assume h x is efficient to compute this is the basic premise of hash tables insert i h x ht i x search i h x if ht i x return true else return false this is a very general simple approach to a hash table implementation where will it run into problems called a collision company has employees stores records using a hashmap with entries employee ssns are hashed to store records in the hashmap keys are ssns so keyspace specifically what keys are needed can t be known in advance due to employee turnover what if one employee with ssn x is fired and replacement has an ssn of y can we design a hash function that guarantees h y does not collide with the other employees hashed ssns can we ever guarantee collisions will not occur yes if the our keyspace is smaller than our hashmap if keyspace m perfect hashing can be used i e a hash function that maps every key to a distinct integer m note it can also be used if n m and the keys to be inserted are known in advance e g hashing the keywords of a programming language during compilation if keyspace m collisions cannot be avoided can we reduce the number of collisions using a good hash function is a start what makes a good hash function utilize the entire key exploit differences between keys uniform distribution of hash values should be produced hash list of classmates by phone number bad use first digits better consider it a single int take that value modulo m hash words bad add up the ascii values better use horner method to do modular hashing again see section of the text horner method base 102 base 22 base 161 ascii strings b e e f 70 overall a good simple general approach to implement a hash map basic formula h x c x mod m where c x converts x into a possibly large integer generally want m to be a prime number consider m only the least significant digits matter h h h by choosing a good hash function we can reduce the number of collisions but we still need to deal with those we cannot prevent collision resolution two main approaches open addressing closed addressing i e if a pigeon hole is taken it has to find another if h x h y i and x is stored at index i in an example hash table if we want to insert y we must try alternative indices this means y will not be stored at ht h y we must select alternatives in a consistent and predictable way so that they can be located later insert if we cannot store a key at index i due to collision attempt to insert the key at index i then i and so on mod m until an open space is found search if another key is stored at index i check i i i until key is found empty location is found we circle through the buffer back to i h x x mod insert how would deletes be handled what happens if key is removed well not quite consider the load factor  n m as  increases what happens to hash table performance consider an empty table using a good hash function what is the probability that a key x will be inserted into any one of the indices in the hash table consider a table that has a cluster of c consecutive indices occupied what is the probability that a key x will be inserted into the index directly after the cluster we must make sure that even after a collision all of the indices of the hash table are possible for a key probability of filled locations need to be distributed throughout the table after a collision instead of attempting to place the key x in i mod m look at i x mod m is a second different hash function should still follow the same general rules as h to be considered good but needs to be different from h h x h y and x y should be very unlikely hence it should be unlikely for two keys to use the same increment h x x mod x x mod insert 16 insert second hash function cannot map a value to you should try all indices once before trying one twice were either of these issues for linear probing as  meaning n approaches m both linear probing and double hashing degrade to  n how multiple collisions will occur in both schemes consider inserts and misses both continue until an empty index is found with few indices available close to m probes will need to be performed  m n is approaching m so this turns out to be  n must keep a portion of the table empty to maintain respectable performance for linear probing is a good rule of thumb can go higher with double hashing most commonly done with separate chaining i e if a pigeon hole is taken it lives with a roommate create a linked list of keys at each index in the table as with dlbs performance depends on chain length which is determined by  and the quality of the hash function closed addressing hash tables are fast and efficient for a large number of applications cs coe cs pitt edu bill compression represent the same data using less storage space can get more use out a disk of a given size can get more use out of memory e g free up memory by compressing inactive sections faster than paging built in to mac os x and later can reduce the amount data transmitted faster file transfers cut power usage on mobile devices two main approaches to compression d c d information is permanently lost in the compression process examples jpeg with audio video files this typically isn t a huge problem as human users might not be able to perceive the difference cuts out portions of audio that are considered beyond what most people are capable of hearing jpeg d c d input can be recovered from compressed data exactly examples zip files flac png gif works on arbitrary bit strings but pretty easily explained using characters consider the ascii character set essentially blocks of codes in general to fit r potential characters in a block you need lg r bits of storage per block consequently n bit storage blocks can represent characters each bit code block represents one of possible characters in ascii easy to encode decode what if we used variable length codewords instead of the constant could we store the same info in less space different characters are represented using codes of different bit lengths if all characters in the alphabet have the same usage frequency we can t beat block storage on a character by character basis what about different usage frequencies between characters in english r s t l n e are used much more than q or x decoding was easy for block codes grab the next bits in the bitstring how can we decode a bitstring that is made up of variable length code words bad example of variable length encoding a t k u r c n variable length encoding for lossless compression codes must be prefix free no code can be a prefix of any other in the scheme using this we can achieve compression by using fewer bits to represent more common characters using longer codes to represent less common characters huffman encoding assume we have k characters that are used in the file to be compressed and each has a weight its frequency of use create a forest f of k single node trees one for each character with the single node storing that char weight while f select f that have the smallest weights in f create a new tree node n whose weight is the sum of and weights add and as children subtrees of n remove and from f add the new tree rooted by n to f build a tree for abracadabra to encode decode we ll need to read in characters and output codes read in codes and output characters sounds like we ll need a symbol table what implementation would be best same for encoding and decoding note that this means we need access to the trie to expand a compressed file need to efficiently be able to select lowest weight trees to merge when constructing the trie can accomplish this using a priority queue need to be able to read write bitstrings unless we pick multiples of bits for our codewords we will need to read write fractions of bytes for our codewords we re not actually going to do i o on fraction of bytes we ll maintain a buffer of bytes and perform bit processing on this buffer see binarystdin java and binarystdout java private static void writebit boolean bit add bit to buffer buffer if bit buffer if buffer is full bits write out as a single byte n if n clearbuffer writebit true writebit false writebit true writebit false writebit false writebit false writebit false writebit true buffer n binary i o private static void writetrie node x if x isleaf binarystdout write true binarystdout write x ch return binarystdout write false writetrie x left writetrie x right private static node readtrie if binarystdin readboolean return new node binarystdin readchar null null return new node readtrie readtrie encoding approach read input compute frequencies build trie codeword table write out trie as a bitstring to compressed file write out character count of input use table to write out the codeword for each input character decoding approach read trie read character count use trie to decode bitstring of compressed file option preprocess the file to be compressed upside ensure that huffman algorithm will produce the best output for the given file downsides requires two passes over the input one to analyze frequencies build the trie build the code lookup table and another to compress the file trie must be stored with the compressed file reducing the quality of the compression this especially hurts small files generally large files are more amenable to huffman compression just because a file is large however does not mean that it will compress well option use a static trie analyze multiple sample files build a single tree that will be used for all compressions expansions saves on trie storage overhead but in general not a very good approach different character frequency characteristics of different files means that a code set trie that works well for one file could work very poorly for another could even cause an increase in file size after compression option adaptive huffman coding single pass over the data to construct the codes and compress a file with no background knowledge of the source distribution not going to really focus on adaptive huffman in the class just pointing out that it exists ascii requires bits to store m characters for a file containing c different characters given huffman codes h c and frequencies f c sum from to c hi fi total storage depends on the differences in frequencies the bigger the differences the better the potential for compression huffman is optimal for character by character prefix free encodings proof in propositions t and u of section of the text where does huffman fall short what about repeated patterns of multiple characters consider a file containing a b of every ascii character will this compress at all with huffman encoding nope but it seems like it should be compressible could represent the previously mentioned string as etc assuming we use bits to represent the number of repeats and bits to represent the character bits needed to store run length encoded file vs bits for input file huge savings note that this incredible compression performance is based on a very specific scenario run length encoding is not generally effective for most files as they often lack long runs of repeated characters patterns are compressible need a general approach huffman used variable length codewords to represent fixed length portions of the input let try another approach that uses fixed length codewords to represent variable length portions of the input idea the more characters can be represented in a single codeword the better the compression consider the bits in ascii representing the with a single bit codeword cuts the used space in half similarly representing longer strings with a bit codeword would mean even better savings need to avoid the same problems as the use of a static trie for huffman encoding so use an adaptive algorithm and build up our patterns and codewords as we go through the file initialize codebook to all single characters e g character maps to its ascii value while eof match longest prefix in codebook output codeword take this longest prefix add the next character in the file and add the result to the dictionary with a new codeword compress using bit codewords tobeornottobeortobeornot initialize codebook to all single characters e g ascii value maps to its character while eof read next codeword from file lookup corresponding pattern in the codebook output that pattern add the previous pattern the first character of the current pattern to the codebook note this means no codebook addition after first pattern output both compression and expansion construct the same codebook compression stores character string codeword expansion stores codeword character string they contain the same pairs in the same order hence the codebook doesn t need to be stored with the compressed file saving space expansion can sometimes be a step ahead of compression if during compression the pattern codeword that was just added to the dictionary is immediately used in the next step the decompression algorithm will not yet know the codeword this is easily detected and dealt with however compress using bit codewords aaaaaa aa aaa expansion how to represent store during compression expansion considerations what operations are needed how many of these operations are going to be performed discuss even further implementation issues codebook size what happens when we run out of codewords only possible codewords for n bit codes two primary options stop adding new keywords use the codebook as it stands maintains long already established patterns but if the file changes it will not be compressed as effectively throw out the codebook and start over from single characters allows new patterns to be compressed until new patterns are built up though compression will be minimal how long should codewords be use fewer bits gives better compression earlier on but leaves fewer codewords available which will hamper compression later on use more bits delays actual compression until longer patterns are found due to large codeword size more codewords available means that greater compression gains can be made later on in the process this sounds eerily like variable length codewords exactly what we set out to avoid here we re talking about a different technique example start out using bit codewords when codeword is inserted into the codebook switch to outputting grabbing bit codewords when codeword is inserted into the codebook switch to outputting grabbing bit codewords etc huffman vs lzw in general lzw will give better compression also better for compression archived directories of files why very long patterns can be built up leading to better compression different files don t hurt each other as they did in huffman remember our thoughts on using static tries well gifs use it and pdfs most dedicated compression applications use other algorithms deflate combination of and huffman used by pkzip and gzip most common zip algorithms burrows wheeler transforms used by lzma used by brotli introduced by google in sept based around a combination of a modern variant of the algorithm huffman coding and order context modeling how much can they compress a file better question how much can a file be compressed by any algorithm no algorithm can compress every bitstream assume we have such an algorithm we can use to compress its own output and we could keep compressing its output until our compressed file is bits clearly this can t work proofs in proposition s of section of the text yes using shannon entropy founded by claude shannon in his paper a mathematical theory of communication entropy is a key measure in information theory slightly different from thermodynamic entropy a measure of the unpredictability of information content by losslessly compressing data we represent the same information in less space hence bits of uncompressed text has less entropy than bits of compressed data translating a language into binary the entropy is the average number of bits required to store a letter of the language entropy of a message length of message amount of information contained in that message on average a lossless compression scheme cannot compress a message to have more than bit of information per bit of compressed message uncompressed english has between and bits of entropy per character of the message weissman scores are a made up metric for silicon valley tv cs coe cs pitt edu bill string pattern matching have a pattern string p of length m have a text string a of length n can we find an index i of string a such that each of the m characters in the substring of a starting at i matches each character in p example can we find the pattern fox in the text the quick brown fox jumps over the lazy dog yes at index of the text string brute force start at the beginning of both pattern and text compare characters left to right mismatch start again at the character of the text and the beginning of the pattern public static int string pat string txt int m pat length int n txt length for int i i n m i int j for j j m j if txt charat i j pat charat j break if j m return i found at offset i return n not found runtime what does the worst case look like a xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxy p xxxxy m n m  nm if n m is the average case runtime any better assume we mostly miss on the first pattern character  n m  n if n m improve worst case theoretically very interesting practically doesn t come up that often for human language improve average case much more practically helpful especially if we anticipate searching through large files first improving the worst case discovered the same algorithm independently knuth morris pratt worked together jointly published in knuth morris pratt algorithm kmp goal avoid backing up in the text string on a mismatch main idea in checking the pattern we learned something about the characters in the text take advantage of this knowledge to avoid backing up build a deterministic finite state automata dfa storing information about the pattern from a given state in searching through the pattern if you encounter a mismatch how many characters currently match from the beginning of the pattern pattern ababac a b c d a a b a b a b a c c d b c d c d b c d d dfa can be represented as a array dfa storage needed mr public int string pat string txt int m pat length int n txt length int i j for i j i n j m i j dfa txt charat i j if j m return i m found return n not found runtime what if we compare starting at the end of the pattern a abcdvabcdwabcdxabcdyabcdz p abcde v does not match e further v is nowhere in the pattern so skip ahead m positions with comparison runtime in the best case n m when searching through text with a large alphabet will often come across characters not in the pattern one of boyer moore heuristics takes advantage of this fact mismatched character heuristic how well it works depends on the pattern and text at hand what do we do in the general case after a mismatch consider a xyxyxyzxxxxxxxxxxxxxx p xyxyz if mismatched character does appear in p need to slide to the right to the next occurrence of that character in p requires us to pre process the pattern create a right array for all i right i for all j from to m right p charat j j text a b c d x a b c d c a b c d y a e c d e a b c d e a b c d e a b c d e a b c d e a b c d e pattern a b c d e right a b c d e a b c d e a b c d e what does the worst case look like runtime  nm same as brute force this is why missed character is only one of boyer moore heuristics the galil rule works similarly to kmp see boyermoore java hashing was cool let try using that public static int string pat string txt int m pat length int n txt length int h pat for int i i n m i if h txt substring i i m return i found return n not found is it efficient nope practically worse than brute force instead of nm character comparisons we perform n hashes of m character strings can we make an efficient pattern matching algorithm based on hashing horner method brought up during the hashing lecture public long string key int m long h for int j j m j h r h key charat j q return h abcd a b c r d mod q bcde b c d r e mod q cdef c d e r f mod q text abcdefg pattern defg this is rabin karp note that we re not storing any values in a hash table so increasing q doesn t affect memory utilization make q really big and the chance of a collision becomes really small but not ok so do a character by character comparison on a collision just to be sure worst case runtime back to brute force esque runtime two options do a character by character comparison after collision guaranteed correct probably fast las vegas assume a hash match means a substring match guaranteed fast probably correct monte carlo cs coe cs pitt edu bill priority queues we mentioned priority queues in building huffman tries primary operations they needed insert find item with highest priority e g findmin or findmax remove an item with highest priority e g removemin or removemax how do we implement these operations simplest approach arrays insert add new item to the end of the array  find search for the highest priority item e g min or max  n remove search for the highest priority item and delete  n total runtime for n inserts and deletes  insert add new item in appropriate sorted order  n find return the item at the end of the array  remove return and delete the item at the end of the array  total runtime for n inserts and deletes  what about a binary search tree insert average case of  lg n but worst case of  n find average case of  lg n but worst case of  n remove average case of  lg n but worst case of  n ok so in the average case all operations are  lg n no constant time operations worst case is  n for all operations our find and remove operations only need the highest priority item not to find remove any item can we take advantage of this to improve our runtime yes a heap is complete binary tree such that for each node t in the tree t item is of a higher priority than t item t item is of a higher priority than t item it does not matter how t item relates to t item this is a relaxation of the approach needed by a bst the heap property find is easy simply the root of the tree  remove and insert are not quite so trivial the tree is modified and the heap property must be maintained add a new node at the next available leaf push the new node up the tree until it is supporting the heap property insert tricky to delete root so let simply overwrite the root with the item from the last leaf and delete the last leaf but then the root is violating the heap property so we push the root down the tree until it is supporting the heap property no find  insert and remove height of a complete binary tree is lg n at most upheap and downheap operations traverse the height of the tree hence insert and remove are  lg n simply implement tree nodes like for bst this requires overhead for dynamic node allocation also must follow chains of parent child relations to traverse the tree note that a heap will be a complete binary tree we can easily represent a complete binary tree using an array number nodes row wise starting at use these numbers as indices in the array now for node at index i parent i i i i for arrays indexed from heapify the numbers max heap to sort ascending min heap to sort descending remove the root don t actually delete the leaf node consider the heap to be from length repeat runtime worst case n log n in place yes stable no what if we want to update an object what is the runtime to find an arbitrary item in a heap  n hence updating an item in the heap is  n can we improve of this back the pq with something other than a heap develop a clever workaround maintain a second data structure that maps item ids to each item current position in the heap this creates an indexable pq indirection example setup let say i m shopping for a new video card and want to build a heap to help me keep track of the lowest price available from different stores keep objects of the following type in the heap class cardprice implements comparable cardprice public string store public double price public cardprice string double p public int compareto cardprice o if price o price return else if price o price return else return indirection example n new cardprice ne a new cardprice amzn x new cardprice ncix b new cardprice bb update price for ne update price for ncix update price for bb indirection cs coe cs pitt edu bill graphs a graph g v e where v is a set of vertices e is a set of edges connecting vertex pairs example v e can be used to model many different scenarios undirected graph edges are unordered pairs a b b a directed graph edges are ordered pairs a b b a adjacent vertices or neighbors vertices connected by an edge let v v and e e given v what are the minimum maximum sizes of e minimum value of e definition doesn t necessitate that there are any edges so maximum of e depends are self edges allowed directed graph or undirected graph in this class we ll assume directed graphs have self edges while undirected graphs do not a graph is considered sparse if e v lg v a graph is considered dense as it approaches the maximum number of edges i e e max  a complete graph has the maximum number of edges or trivially graphs can be represented as list of vertices list of edges performance assume we re going to be analyzing static graphs i e no insert and remove so what operations should we consider rows columns are vertex labels m i j if i j e m i j if i j e runtime space array of neighbor lists a i contains a list of the neighbors of vertex i runtime space where would we want to use adjacency lists vs adjacency matrices what about the list of nodes list of edges approach path a sequence of adjacent vertices simple path a path in which no vertices are repeated simple cycle a simple path with the same first and last vertex connected graph a graph in which a path exists between all vertex pairs connected component connected subgraph of a graph acyclic graph a graph with no cycles tree a connected acyclic graph has exactly v edges what is the best order to traverse a graph two primary approaches depth first search dfs dive as deep as possible into the graph first branch when necessary breadth first search bfs search all directions evenly i e from i visit all of i neighbors then all of their neighbors etc already seen and used this throughout the term for tries for huffman encoding can be easily implemented recursively for each node visit first unseen neighbor backtrack at dead ends i e nodes with no unseen neighbors try next unseen neighbor after backtracking can be easily implemented using a queue for each node visited add all of its neighbors to the queue vertices that have been seen but not yet visited are said to be the fringe pop head of the queue to be the next visited vertex see example bfs example q bfs traversals can further be used to determine the shortest path between two vertices at a high level dfs and bfs have the same runtime each node must be seen and then visited but the order will differ between these two approaches how will the representation of the graph affect the runtimes of of these traversal algorithms dfs and bfs would be called from a wrapper function if the graph is connected dfs bfs is called only once and returns a spanning tree else a loop in the wrapper function will have to continually call dfs bfs while there are still unseen vertices each call will yield a spanning tree for a connected component of the graph a biconnected graph has at least distinct paths no common edges or vertices between all vertex pairs any graph that is not biconnected has one or more articulation points vertices that if removed will separate the graph any graph that has no articulation points is biconnected thus we can determine that a graph is biconnected if we look for but do not find any articulation points variation on dfs consider building up the spanning tree have it be directed create back edges when considering a node that has already been visited in constructing the spanning tree label each vertex v with with two numbers num v pre order traversal order low v lowest numbered vertex reachable from v using or more spanning tree edges and then at most one back edge min of num v lowest num w of all back edges v w lowest low w of all spanning tree edges v w finding articulation points example num low e a b c d f so where are the articulation points if any non root vertex v has some child w such that low w num v v is an articulation point what about if we start at an articulation point if the root of the spanning tree has more than one child it is an articulation point cs coe cs pitt edu bill weighted graphs last time we said spatial layouts of graphs were irrelevant we define graphs as sets of vertices and edges however we ll certainly want to be able to reason about bandwidth distance capacity etc of the real world things our graph represents whether a link is gigabit or megabit will drastically affect our analysis of traffic flowing through a network having a road between two cities that is a lane country road is very different from having a lane highway if two airports are miles apart the number of flights going in and out between them will be drastically different from airports miles apart we can represent such information with edge weights how do we store edge weights adjacency matrix adjacency list do we need a whole new graph representation how do weights affect finding spanning trees shortest paths the weighted variants of these problems are called finding the minimum spanning tree and the weighted shortest path graphs can potentially have multiple spanning trees mst is the spanning tree that has the minimum sum of the weights of its edges initialize t to contain the starting vertex t will eventually become the mst while there are vertices not in t find minimum edge weight edge that connects a vertex in t to a vertex not yet in t add the edge with its vertex to t at each step check all possible edges for a complete graph first iteration v possible edges next iteration v possibilities each vertex in t shared v edges with other vertices but the edges they shared with each other already in t next v possibilities runtime i to v i v i evaluates to  do we need to keep track of all possible edges no we only need the best edge for possible for each vertex parent best edge let assume we use an adjacency matrix takes  v to check the neighbors of a given vertex for every vertex we add to t we ll need to check all of its neighbors to check for edges to add to t next during each neighbor check maintain a parent and list while t v new t t new for j to v if m new j j t m new j j parent j new j m new j sounds like a job for a priority queue priority queues can remove the min value stored in them in  lg n also  lg n to add to the priority queue what does our algorithm look like now visit a vertex add edges coming out of it to a pq while there are unvisited vertices pop from the pq for the next vertex to visit and repeat pq have to insert all e edges into the priority queue in the worst case we ll also have to remove all e edges so we have e  lg e e  lg e  e lg e  e lg e this algorithm is known as lazy prim do we really need to maintain e items in the pq i suppose we could be less lazy just like with the adjacency matrix implementation we only need the best edge for each vertex pq will need to be indexable this is the idea of eager prim runtime is  e lg v adjacency matrix prim runtime  space  v lazy prim runtime  e lg space  e requires a pq eager prim runtime  e lg v space  v requires an indexable pq how do these compare dijkstra algorithm set a distance value of for all vertices but start set cur start while destination is not visited for each unvisited neighbor of cur compute tentative distance from start to the unvisited neighbor through cur update any vertices for which a lesser distance is computed mark cur as visited let cur be the unvisited vertex with the smallest tentative distance from start distance via how to implement best path parent array runtime pq turns out to be very similar to eager prim storing paths instead of edges runtime kruskal mst insert all edges into a pq grab the min edge from the pq that does not create a cycle in the mst remove it from the pq and add it to the mst pq 4 6 2 4 6 4 instead of building up the mst starting from a single vertex we build it up using edges all over the graph how do we efficiently implement cycle detection cs coe cs pitt edu bill union find for a given graph g can we determine whether or not two vertices are connected in g can also be viewed as checking subset membership important for many practical applications we will solve this problem using a union find data structure have an id array simply store the component id for each item in the union find structure find simply returns its id what about union u u u u u u u id runtime for find  for union  n initialize with n items numbered to n uf int n void union int p int q int find int p boolean connected int p int q int count connect p with q return id of the connected component that p is in true if p and q are connected number of connected components public int count return count public boolean connected int p int q return find p find q public uf int n count n id new int n for int i i n i id i i public int find int p return id p public void union int p int q int pid find p qid find q if pid qid return for int i i id length i if id i pid id i qid count what if we store our connected components as a forest of trees each tree representing a different connected component every time a new connection is made we simply make one tree the child of another 2 public int find int p while p id p p id p return p public void union int p int q int i find p int j find q if i j return id i j count runtime find bound by the height of the tree union bound by the height of the tree what is the max height of the tree can we modify our approach to cap its max height 2 public uf int n count n id new int n sz new int n for int i i n i id i i sz i public void union int p int q int i find p j find q if i j return if sz i sz j id i j sz j sz i else id j i sz i sz j count runtime find  log n union  log n can we do any better with this knowledge of union find how exactly can it be used as a part of kruskal algorithm what is the runtime of kruskal algorithm cs coe cs pitt edu bill network flow consider a directed weighted graph g v e weights are applied to edges to state their capacity c u w is the capacity of edge u w if there is no edge from u to w c u w consider two nodes a source and a sink t let determine the maximum flow that can run from to t in the graph g let the f u w be the amount of flow being carried along the edge u w some rules on the flow running through an edge u w e f u w c u w u v t w v f w u w v f u w let all edges in g have an allocated flow of while there is path p from to t in g t all edges in p have some residual capacity i e u w p f u w c u w such a path is called an augmenting path compute the residual capacity of each edge in p residual capacity of edge u w is c u w f u w find the edge with the minimum residual capacity in p we ll call this residual capacity increment the flow on all edges in p by to find the max flow we will have need to consider re routing flow we had previously allocated this means when finding an augmenting path we will need to look not only at the edges of g but also at backwards edges that allow such re routing for each edge u w e a backwards edge w u must be considered during pathfinding if f u w the capacity of a backwards edge w u is equal to f u w we will perform searches for an augmenting path not on g but on a residual graph built using the current state of flow allocation on g the residual graph is made up of v an edge for each u w e where f u w c u w u w mirror in the residual graph will have flow and a capacity of c u w f u w a backwards edge for each u w e where f u w u w backwards edge has a capacity of f u w all backwards edges have flow residual graph example a b t 5 a t b how the augmenting path is chosen affects the performance of the search for max flow edmonds and karp proposed a shortest path heuristic for ford fulkerson use bfs to find augmenting paths a t b 1000 1000 edmonds karp only uses bfs used to find spanning trees and shortest paths for unweighted graphs why do we not use some measure of priority to find augmenting paths representing the graph similar to a directed graph can store an adjacency list of directed edges actually more than simply directed edges flow edges for each edge we need to store start point the from vertex end point the to vertex capacity flow residual capacities for forwards and backwards edges public class flowedge private final int v from private final int w to private final double capacity capacity private double flow flow public double residualcapacityto int vertex if vertex v return flow else if vertex w return capacity flow else throw new illegalargumentexception illegal endpoint bfs search for an augmenting path pseudocode edgeto v marked v queue q q enqueue marked true while q isempty v q dequeue each flowedge object is stored in the adjacency list twice once for its forward edge once for its backwards edge for each v w in adjlist v if residualcapacity v w if marked w edgeto w e marked w true q enqueue w a c t 5 b an st cut on g is a set of edges in g that if removed will partition the vertices of g into two disjoint sets one contains one contains t may be many st cuts for a given graph let s focus on finding the minimum st cut the st cut with the smallest capacity may not be unique we could examine residual graphs specifically try and allocate flow in the graph until we get to a residual graph with no existing augmenting paths the set of saturated edges makes up a minimum st cut a s c t 5 7 b a special case of duality i e you can look at an optimization problem from two angles in this case to find the maximum flow or minimum cut in general dual problems do not have to have equal solutions the differences in solutions to the two ways of looking at the problem is referred to as the duality gap if the duality gap strong duality holds max flow min cut uphold strong duality if the duality gap weak duality holds first run ford fulkerson to produce a residual graph with no further augmenting paths the last attempt to find an augmenting path will visit every node reachable from s edges with only one endpoint in this set comprise a minimum st cut a 7 1 1 s c t 5 5 7 9 b min cut is it possible how would we measure the max flow min cut what would an algorithm to solve this problem look like a f s b e t c d cs midterm greedy algorithms fall consider thefollowingproblem theinput consists of n skiers withheights pn and n skies withheights sn theproblemistoassign each skieraski to to minimizethe averagedi erencebetweentheheight of a skier andhis her assigned ski that is if the ith skier is given the i thski then you want to minimize x n pi i n i so for example if and then the matching and which matchesthefrstpersontothe third ski the second person to the second ski and the third person to the frst ski would have average di erence a consider the following greedy algorithm find the skier and ski whose height di erence is minimized assign this skier this ski repeat the process until every skier has a ski prove of disprove that this algorithm is correct b considerthefollowinggreedy algorithm givethe shortest skierthe shortest ski givethesecond shortest skierthesecond shortest ski givethethird shortest skier the third shortest ski etc prove of disprove that this algorithm is correct cs midterm fall points we consider the problem of computing the longest common subsequence of two sequences a am and b bn let t i j be thelengthof thelongest common subsequence of ai and bj a write a recursive function to compute t i j in the naive way don t forget the base case b showthatifyouimplementthis recursiondirectly in say thecprogramming language that the program could use time that is exponential in n c write iterative array based code to compute t m n that runs in o n time d write code to actually fnd the longest common subsequence from your array points consider the problem where the input is a collection of n train trips within germany for the ith trip ti you are given the date di of that trip and the non discounted fare fi for that trip for simplicity we will assume that dates are nonnegative integers the german railway system sells a class a bahncard for y eurosthat entitlesyouto fare reduction on all traintravel withingermany within a days ofpurchase thegerman railway system also sells a class b bahncard for m euros that entitles you to a fare reduction on all train travel within germany within b days of purchase you can apply at most one bahncarddiscountto aparticulartrip theproblemistodeterminetheleastyoucan spend on your travel in this paragraph we give an example assume that y a and m and b further assume euros euros euros euros euros and and euros then you might buy a class b bahncard on day and a class a bahncard on day this results in a total cost of 100 euros give an o n time algorithm for problem signifcant partial credit will be given for any polynomial time algorithm points this is an actual problem that arises in video processing called temporally consistent assignment you are given two videos each represented as a sequence of im ages both videos were taken o of the same scene at roughly the same time period let x xm be one sequence of images and y yn be the other sequence of images the cameras are not synchronized and may run at signifcantly di erent speeds the objective is to assign each image of the x video sequence withits most similarimagein the y video sequence subjecttotheconstraintthattheassignmentsareconsistentintime in order to determine how similar two images are as part of the input you may assume that you are given a table d i j which contains a numeric dissimilarity value between these two images xi and yj the lower d i j is the more similar the images are do not worry how d i j is computed an assignment of sequence x to sequence y is a sequence of m indices a jm meaning that for i m image xi is assigned to image yji the cost of an assignment is a sum of the dissimilarities between the assigned images that is p m cost a d i ji an assignmenta is temporally consistent if ji ji for i m i in other words if xi is assigned to some image yji then the next image in the sequence xi must be assigned to an image appearing no earlier that yji in the y image sequence this makes sense as time runs forward for both cameras we allow two images of x to be as signed to the same image of y the problem is given the video sequences x y and the cost table d i j compute the minimum cost temporally consistent assignment of x to y consider the following table for m and n d i j j j j j j j i i i is and is andx3 is assigned to thenthe cost ofthis assignmentis d d d give ano n time algorithmto compute thelowest achievable cost signifcantpartialcredit willbegivenfor any algorithm withpolynomial running time cs midterm fall points consider the problem input a undirected graph g and an integer k output if g has three vertex disjoint cliques of size k and otherwise show that this problem is np hard use the fact that the clique problem in np complete the input to the clique problem is an undirected graph h and an integer j the output should be if h contains a clique of size j and otherwise note that a clique is a mutually adjacent collection of vertices three cliques are disjoint if no vertex is in more than one clique points show that the vertex cover problem is self reducible the decision problem is to take a graph g and an integer k and decide if g has a vertex cover of size k or not the optimizationproblem takes agraph g and returnsasmallest vertexcoverin g soyou must show that if the decision problem has a polynomial time algorithm then the optimization problem also has a polynomial time algorithm recall that a vertex cover is a collection s of vertices with the property that every edge is incident to a vertex in s points consider the following bahncard problem where the input is a collection of n train trips withingermany for the ith trip ti you aregiventhedatedi of that trip and the non discounted fare fi forthattrip forsimplicity wewill assumethatdatesare nonnegative integers the german railway system sells a bahncard for y euros that entitles you to a fare reduction on all train travel within germany within a days of purchase you can apply at most one bahncard discount to a particular trip the problem is to determine the least you can spend on your travel in this paragraph we give an example assume that y a 100 further assume euros euros euros 100 euros euros and and euros then you might buy a bahncard on day and day this results in a total cost of 200 100 200 give an o n time algorithm for problem signifcant partial credit will be given for any polynomial time algorithm cs midterm fall points a what is the most important reason that it is standard practice to ignore multiplicative constants when computing running times of algorithms programs hint your answer should explain why we are not more precise for example includ ing multiplicative constants and ignoring low order terms or less precise for example ignoringpoly log factors b explain how to compute the minimum of n numbers xn in o logn time with n logn processorson aerewpram thatisyouwant t n p n logn o logn c whatisthe eciency oftheerew algorithminpart a start with thedefnition of eciency d explain how to compute the minimum of n numbers xn in o time with nprocessors on a common crcw pram points the input to this problem is a string c of n integers the problem is to fnd the largest k k n such that c c c k c n k c n c n that is k is the length of the longest prefx that is also a sux so for example if c then k give a erew parallel algorithm that runs in o logn time with a polynomial number of processors points design a parallel algorithm that fnds the maximum number in a sequence xn of not necessarilydistinct integersin the to n your algorithm should run in constant time on a crcw pram with n processors note that it is important here that the xi have restricted range adversary arguments searching section sorting section questions maximum finding subsection maximum and minimum finding subsection maximum and maximum finding subsection no title http people cs pitt edu kirk exams html nex t u p previou next about this document cs final exam summer points consider the following problem the input consists of the lengths and access probabilities for n files the problem is to order these files on a tape so as to minimize the expected access time if the files are placed in the order then the expected access time is don t let this formula throw you the term is the probability that you access the ith file times the length of the first i files for each of the below algorithms either give a proof that the algorithm is correct or a proof that the algorithm is incorrect a order the files from shortest to longest on the tape that is implies that i j b order the files from most likely to be accessed to least likely to be accessed that is pi pj implies that i j c order the the files from smallest ratio of length over access probability to largest ratio of length over access probability that is implies that i j points give an algorithm for the following problem whose running time is polynomial in n l k input positive integers k and l output a solution if one exists to where each xi is an integer satisfying no title http people cs pitt edu kirk exams 98summer html points give a polynomial time algorithm that takes three strings a b and c as input and returns the longest sequence s that is a subsequence of a b and c points show that the following problem is nphard input a graph g let n be the number of vertices in g output if g contains a simple cycle with edges and otherwise use the fact the the following problem is nphard input a graph g output if g contains a simple cycle that spans g and otherwise note that a cycle is simple if it doesn t visit any vertex more than once a cycle spans g if every vertex is included in the cycle points show that the vertex cover problem is selfreducible the decision problem is to take a graph g and an integer k and decide if g has a vertex cover of size k or not the optimization problem takes a graph g and returns a smallest vertex cover in g so you must show that if the decision problem has a polynomial time algorithm then the optimization problem also has a polynomial time algorithm recall that a vertex cover is a collection s of vertices with the property that every edge is incident to a vertex in s points give an algorithm that given an integer n computes n that is n factorial in time on an erew pram with n processors make the unrealistic assumption that a word of memory can store arbitrarily large integers points give an algorithm for the minimum edit distance problem that runs in polylog time on a crew pram with with a polynomial number of processors here polylog means where n is the input size and k is some constant independent of the input size recall that the input to this problem is a pair of strings and the goal is to convert a into b as cheaply as possible the rules are as follows for a cost of you can delete any letter for a cost of you can insert a letter in any position for a cost of you can replace any letter by any other letter about this document nex t u p previou next about this document cs adversarial lower bound arguments in a simplified form of the game mastermind there is a hidden sequence h ck of k colored pegs there are c different possible colors colors can be repeated in the hidden sequence the game consistes of repeated rounds to start a round the guesser gives the hider a sequece g gk of k colors the hider then tells the guesser how many of the guesses were correct that is the number of indices j such that hj gj this ends a round compute a lower bound as a function of k and c on the number of the number rounds required by the guesser to guarantee that he she will solve the puzzle a typical values of k and c what do you get for your lower bound in this case in the game mastermind there is a hidden sequence h ck of k colored pegs there are c different possible colors colors can be repeated in the hidden sequence the game consistes of repeated rounds to start a round the guesser gives the hider a sequece g gk of k colors the hider then tells the guesser how many of the guesses were correct that is the number of indices j such that hj gj in addition the hider tells the guess how many colors are correct but are in the wrong position think of h and g as being multi sets and the hider tells the guesser the cardinality of the multi set intersection of h and g this ends a round compute a lower bound as a function of k and c on the number of the number rounds required by the guesser to guarantee that he she will solve the puzzle a typical values of k and c what do you get for your lower bound in this case prove that computing the or of n bits requires  log n steps on a erew pram independent of the number of processors used consider the following situation you have two workstations a and b connected by a communication line the workstation a is initially given an n bit integer x the workstation b is initially given an n bit integer y the goal of the two workstations is to communicate over the line in such a way that they both know the number of bits that are in x plus the number of bits that are in y show that the number of bits sent across the line must be  log n consider the following situation you have two workstations a and b connected by a communication line the workstation a is initially given an n bit integer x the workstation b is initially given an n bit integer y the goal of the two workstations is to communicate over the line in such a way that they both know the bit wise exclusive or of x and y show that the number of bits sent across the line must be  n you goal is to find a counterfeit coin among a group of coins counterfeit coins are lighter than real coins you know that exactly one of the coins is counterfeit to help you decide which coin is legitimate you have a pan balance a pan balance functions in the following manner you can give the pan balance any two disjoint subcollections say and of the coins let and be the cumulative weight of the coins in and respectively the pan balance then determines whether or show how to solve this problem in two weighings on the pan hint this is not a lower bound problem it purpose is to build you intution you goal is to find a counterfeit coin among a group of n coins counterfeit coins are lighter than real coins you know that exactly one of the n coins is counterfeit to help you decide which coin is legitimate you have a pan balance a pan balance functions in the following manner you can give the pan balance any two disjoint subcollections say and of the coins let and be the cumulative weight of the coins in and respectively the pan balance then determines whether or show that solving this problem requires at least n weighings on the pan balance show that there is no comparison based sorting algorithm who running time is linear for at least half of the n inputs of length n show that comparisons are necessary in the worst case to merge two sorted lists of length n in the broadcast gossip problem there are n workstations there workstations may be initially pro grammed in any way that you like after they are programmed an arbitary k of them are given an integer assume time is divided into unit slots and that a workstation can transmit its integer in a unit slot workstations can only communicate via broadcast if more than one workstation tries to broadcast in a time slot then all the workstations detect interference that is they don t get the message but they can tell that more than two workstations attempted to broadcast if only one workstation broadcasts in a time slot then it succeeds and all workstations hear the broadcast if only no workstation broadcasts in a time slot then this can be detected by all workstations here assume that the workstations goal is for all workstations to successfully transmit their messages explain how to solve this problem in time n hint this is not a lower bound question it is just a warm up question to understand the problem consider the broadcast problem where the goal is for at least one workstation to successfully transmit its message show how to solve this problem in time o log n hint this is not a lower bound question it is just a warm up question to understand the problem consider the broadcast problem where the goal is for all workstations to successfully transmit their messages show how to solve this problem in time o k log n hint this is not a lower bound question it is just a warm up question to understand the problem consider the broadcast problem where the goal is for all workstations to successfully transmit their messages and all workstations know a priori that k that is the workstations can be programmed under the assumption that k show to solve this problem requires time  log n consider the broadcast problem where the goal is for all workstations to successfully transmit their messages show to solve thie problem requires time  k log n hin t the math gets a bit involved here the first fact you need is that x can be approximated well by x x where e is the base of the natural logarithm the second fact you need is that x y y can be approximated well by ex when y is large consider the following problem you start with one zyglit in a petri dish zyglit reproduce asexually by splitting into two zyglits the original zyglit disappears in this process the genetic material of the two resulting zyglits is subject to mutation and are not identical to either siblings or parents you are given a petri dish with n zyglits that are the leaves of the family tree your goal is to reproduce the family tree to help you you can perform a dna check on any three zyglits say zi zj and zk that will determine which pair of zyglits among the three possible pairs zi and zj zi and zk and zj and zk have the lowest common ancestor in the tree show that to recompute the tree requires  n log n dna checks you are given a collection of n vlsi chips up to of the chips may not be reliable there is jig that allows you to test pairs of chips say you are testing a pair x and y of chips if x is good then x will accurately report on whether y is good or not if x is bad x may say that y is good or x may say that y is bad independent of whether y is good or bad show that it is not possible to identify a good chip with surity under this setting consider the element uniqueness problem the input is n reals xn the question is does there exist an i and j with i j with that xi xj that is are there two numbers in the input that are identical show that the information theoretic lower bound on the worst case time complexity for this problem is  n log n hint think of the input as a point xn as a point in n dimensional space rn let u be the portion of rn where no pair of coordinates are the same show that u consists of n disjoint regions perhaps its easiest to first consider n then n and then you should see it assume that the algorithm a has made k comparisons let ak be the region of rn consistent with the answers that a has received to date for example if n assume that in its first k comparisons a learned that and then ak would be exactly thost points where and and show that it must always be the case the ak is convex that is if and t are points in ak then the line segment from to t is in ak conclude that at least  n log n comparisons are needed by any algorithm for element uniqueness let xn be a collection of reals let  be the permutation that sorts this collection that is x x x n then the maximum gap is defined as the i i n that maximizes x i x i that is this is the maximum gap between any two consecutive numbers in the sorted list show that the information theoretic lower bound for computing the maximum gap is  n log n show how to compute maximum gap in time o n given that you can in constant time take the floor of a real number you have n workstations on the internet each workstation knows the indentify of the other work stations and knows how to send messages to each of the other workstations you know a priori that infilitrated by some hacker and may function in arbitrarily malicious ways the goal of the remaining of some bit the value of the bit doesn t matter all we need is that all of the good workstations agree on the value note that a workstation knows whether or not it is good but it has no idea which other workstations are good assume that all sent message are eventually delivered although they may be delayed arbitrarily long show that there is no algorithm that will allow the good workstations to agree on a bit hint as the rating suggests this should not be attempted by the faint of heart this is a very hard problem cs approximiation algorithms problems consider the vertex cover problem that is given a graph g find a minimal cardinality collection s of vertices with the property that every edge in g is incident to a vertex in s consider the following algorithm pick an arbitrary edge e v w from g add v and w to s remove v and w and all incident edges from g go to step a show that the performance ratio of this algorithm is at most hint first consider why the size of any matching a collection of edges such that no pair are incident on a common vertex is a lower bound on the size of the vertex cover give an example instance of the vertex cover problem where the above algorithm does not give an optimal solution in this problem you have n boxes bn that you wish to load onto k trucks you know the integral weight in kilograms wi of each box bi you goal is to minimize the weight on more heavily loaded truck consider the obvious greedy algorithm that considers the boxes in an arbitrary order and always places the box under consideration into the least heavily loaded truck show that this algorithm guarantees that the weight on the more heavily loaded truck is at most times optimal that is this algorithm has performance ratio of at most in this problem you have n boxes bn that you wish to load onto two trucks you know the integral weight in kilograms wi of each box bi you goal is to minimize the weight on more heavily loaded truck consider the obvious greedy algorithm that considers the boxes in an arbitrary order and always places the box under consideration into the least heavily loaded truck show that this algorithms guarantees that the weight on the more heavily loaded truck is at most times optimal that is this algorithm has performance ratio give an example of an instance to the above problem where the load on the more heavily loaded truck is fifty percent larger than the load on the more heavily loaded truck in the optimal solution in this problem you have n boxes bn that you wish to load onto two trucks you know the integral weight in kilograms wi of each box bi you goal is to minimize the weight on more heavily loaded truck consider generalizations of the obvious greedy algorithm that considers the boxes in an arbitrary order and always places the box under consideration into one of the trucks based on some rule that relies only on the load of the two trucks at that time show no such algorithm can have performance ratio strictly better than in this problem you have n chickens with weights bn grams that you wish to pack into packages each package must contain at least l grams of chicken so you don t get sued for false advertising the goal is to do this by maximizie the number of packages that you fill to l or more grams consider the obvious greedy algorithm that considers the chickens in an arbitrary order and adds the chickens to the same package until that package is full show that the performance ratio of this algorithm is that is that this algorithm fills at least as many bags as optimal basic definitions a optimization problem has the following form output a best solution s satisfying some property p best usually means least cost a minimization problem or most benefit a maximization problem a best solution is called an optimal solution note that for many problems there may be many dif ferent optimal solutions a feasible solution is a solution that satisfies the property p most of the problems that we consider can be viewed as opti mization problems an algorithm a for a minimization problem p has performance ratio c if for every input i the cost of the output of a on input i has cost at most c time the cost of the least cost feasible solution to i an algorithm a for a maximization problem p has performance ratio c if for every input i the benefit of most beneficial feasible solution to i is at most c times the benefit of the output of a on input i mst doubling algorithm for tsp with tri angle inequality see section christofides algorithm for tsp with trian gle inequality see section without triangle inequality there is no al gorithm with constant performance ratio theorem for all c if there is a polynomial time algorithm for tsp with a constant performance ratio c then every np complete problem has a polyno mial time algorithm proof we use a many to one reduction from the hamiltonian cycle problem i e the problem of deciding whether a graph contains a simple spanning cycle let h a weighted complete graph constructed from an unweighted graph g with n vertices in the folloring manner each edge in g is given weight and a edge x y with weight cn is added between each pair of nonadjacent vertices in g note that if g has a hamiltonian cycle then h has a tour of weight n and if g does not have a hamiltonian cycle then every tour in h has weight greater than cn hence if the approximiation algorithm for tsp finds a tour with length at most cn then we may be sure the g has a hamiltonian cycle and if the approximiation algorithm for tsp finds a tour with length more than cn then we may be sure the g does not have a hamiltonian cycle analysis of first fit for bin packing analysis of first fit decreasing for bin packing see section dynamic programming take we examine problems that can be solved by dynamic programming perhaps the single most useful algorithmic design technique dynamic programming can be explained many ways rather than explain what a dynamic program ming algorithm is we explain how one might develop one find a recursive algorithm for the problem it often helps to first find a recursive algorithm to count the number of feasible solutions spot redundancy in the calculation eliminate the redundancy this can often be best accomplished by per forming the calculations bottom up instead of top down fibonacci numbers and binomial coeffi cients see section and section from the text longest common subsequence problem the input to this problem is two sequences a am and b bn the problem is to find the longest sequence that is a subsequence of both a and b for example if a ccdedcdec and b cdedcdec then cddec is subsequence of length of both sequences let t i j be the length of the longest common subsequence of ai and bj then one can see that if ai bj then t i j t i j otherwise one can see that t i j max t i j t i j note that there are only nm possible subproblems since there are only m choices for i and n choices for j hence by treating the t i j as array entries instead of recursive calls and updating the table in the appropriate way we can get the following o time algorithm for i to m do t i for j to n do t j for i to m do for j to n do if a i b j then t i j t i j else t i j max t i j t i j computing number of binary search trees and optimal binary search trees section from the text chained matrix multiplication section from the text maximum weight independent set in a tree the input to this problem is a tree t with weights on the vertices the goal is to find the independent set in t with maximum aggregate weight an independent set is a collection of mutually nonadjacent vertices root the tree at an arbitrary node r and process the tree in postorder we generalize the induction hypothesis consider an arbitrary node v with branches to k descendants wk we can create an independent set for the subtree rooted at v in essentially two ways depending on whether or not we include v in the independent set if we decide not to include v consider an arbitrary root v with branches to k descendants wk we can create an independent set for the subtree rooted at v in essentially two ways depending on whether or not we include v in the independent set if we decide not to include v we can combine any independent sets for the subtrees rooted at wk to create an independent set for the subtree rooted at v since there are no edges between the subtrees on the other hand if we do include v in the independent set we can only use independent sets for the subtrees that do not include their respective root wk otherwise we would have both v and some wj in the set and it would not be an independent set anymore therefore for each node v the algorithm computes the following information big v the maximum weight of an independent set for the subtree rooted at v and bignotroot v the maximum weight of an independent set for the subtree rooted at v that does not include v at node v the algorithm first recursively computes big wi and bignotroot wi for each descendant subtree wk it then computes bignotroot v and big v using the following recurrence relations that correspond to the two cases identified above k bignotroot v big wi i k big v max bignotroot v weight v bignotroot wi i if v is a leaf then bignotroot v and big v weight v longest increasing subsequence the longest increasing subsequence lis problem is defined as follows input a sequence x xn of integers output the longest increasing subsequence of x a sequence is increasing if each number in the sequence is larger than the previous number let us try to develop a recursive algorithm for this problem let lis k be the longest increasing subsequence of the first k integers in x consider the following first stab at an induction hypothesis induction hypothesis we know how to compute lis k assume that we have computed lis k and we now want to compute lis k the following sequence shows that we can t just know any longest sequence before we consider we need to know the lis not the lis the obvious solution is to remember the lis that ends in the smallest number hence we change the definition of lis as follows let lis k be the the longest increasing subsequence of the first k integers in x that ends in the smallest number induction hypothesis we know how to compute lis k the following sequence shows that knowing the lis that ends in the smallest number is not sufficient information notice that in this example the length of the lis doesn t change but a new one is formed with a smaller last number the most obvious way to fix this problem is to remember the best the one that ends in the smallest number sequence of length one less than the lis let k be the the increasing subsequence of the first k integers in x that has length one less than the length of lis k and that ends in the smallest number induction hypothesis we know how to compute lis k and k the following sequence shows that there is not sufficient inductive information to compute k we need to know the sequence of the first k integers in x of length two shorter than lis k that ends in the smallest number one can see that eventually we have to inductively know the sequence of each length that ends in the smallest number hence we define lis k as the the smallest last number of any subsequence of length from among the first k numbers of x this then leads us to the following algorithm for k to n do for to n do lis k plus infinity for k to n do lis k minus infinity for k to n do for to n do if lis k x k lis k then lis k x k else lis k lis k this is an example of where when trying to compute something recursively inductive it is sometimes easier to compute more information than you need this is sometimes called strengthening the inductive hypothesis this is not para doxical because the strengthening of the inductive hypothesis also gives you more information to work with when the recursion returns from the smaller subproblem the most common mistake made here is to use the additional information returned from the recursion to solve the original problem not the more generalized problem one point that i want you to draw from this example is that its not always easy to determine how one should strengthen the inductive hypothesis dynamic programming take in this section we discuss another method for developing dynamic program ming algorithms that avoids having to develop a recursive algorithm which is almost always the most difficult part in the previously outlined method this method is particularly applicable when the feasible solutions are subsets or subsequences the steps to developing a dynamic programming algorithm using this method are as follows determine how to generate all possible feasible solutions once again it might be easier to first determine how to count the number of feasible solutions enumeration usually follows easily from counting develop a pruning rule to eliminate partial feasible solutions that either are redundant or that can not be extended to an optimal solution transform the algorithm to an iterative table based algorithm subset sum example the subset sum problem is defined as follows input a collection xn of positive integers and a positive integer l output a subset of the xi that sum to l or a statement that no such subset exists we can generate the subsets inductively using a binary tree as follows the nodes of depth i are all subsets of xi the children of a node s at depth i are s and s xi thus all the subsets can be found at the leaves the depth of this tree is n the two pruning rules are eliminate the subtree rooted at any subset with sum greater than l if there are two subsets s and t at the same depth with the same aggre gate sum then one can arbitrary select one of the two nodes and eliminate the subtree rooted at that node note that these two pruning rules mean that there are at most l nodes left unpruned at any level hence this gives us an algorithm with running time is polynomial in n and l we now wish to derive an iterative algorithm let sum k s be true if there is a subset of the first k numbers that sums to s we compute sum k s as follows sum true for s to l do sum s false for k to n do for s to l do sum k s sum k s or sum k s x k knapsack example the knapsack problem can be defined as follows input a collection of objects on with positive integer weights wn and positive integer values vn a positive integer l output the highest valued subset of the objects with weight at most l we can generate the subsets inductively using a binary tree as in the subset sum problem the two pruning rules are eliminate the subtree rooted at any subset with sum greater than l if there are two subsets s and t at the same depth with the same total weight then eliminate the one of least value note that these two pruning rules mean that there are at most l nodes left unpruned at any level let v alue k s be highest value one can obtain from a subset of the first k numbers with aggregate weight s we compute v alue k s as follows value for s to l do sum s minus infinity for k to n do for s to l do value k s max value k s value k s w k v k longest increasing subsequence problem take we now apply this method to the longest increasing subsequence problem we can generate feasible solutions subsequences as in the subset sum prob lem one obvious the pruning rule is if you have two subsequences s and t at the same depth that have the same length prune the one that ends in the larger number this pruning rule will give you at most n unpruned nodes at any level if you turn this into an iterative code you will get the same code as we got before another possible pruning rule would be if two sequences s and t at the same depth have the same last number prune the shorter sequence this pruning rule will also give you at most n unpruned nodes at any depth and an time algorithm note how much easier it was to develop an algorithm for the lis problem this way as opposed to trying recursion and generalizing the induction hypothesis the single source shortest path problem the input to this problem is a directed positive edge weighted graph with a designated vertex the problem is to find the shortest simple path from to each other vertex for more information see section of the text here feasible solutions are simple paths that start from the source vertex the nodes in level k of the tree represent all paths from of k or less hops note that by simplicity we we need only consider the tree to depth n the number of vertices the pruning rule is that we need only remember the shortest path to a particular vertex we thus get the following code here d k i is the shortest path from to i of k or less hops for k to n do for i to n do d k i min d k i d k i for each edge e i j do d k j min d k j d k i the length of e this is bellman ford shortest path algorithm and has running time o v e the shortest path problem with negative edge weights the input to this problem is a directed edge weighted graph with a designated vertex the edge weights may be positive or negative the problem is to find the shortest simple path from to each other vertex for more information see section of the text here feasible solutions are simple paths that start from the source vertex the nodes in level k of the tree represent all paths from of k or less hops note that by simplicity we we need only consider the tree to depth n the number of vertices the pruning rule is that if two paths end at the same vertex and contain the same vertices then we may prune the shorter one make sure you understand why the pruning rule that we used for positive weights does not work here we thus get the following code here d k s i is the shortest path from to i of k or less hops that visits exactly the vertices in s for k to n do for i to n do for s to n do d k s i min d k s i d k s i for each edge e i j do if j is not in s then d k s j j min d k s j j d k s i the length of e the running time of this code is o v where v is the number of vertices and e is the number of edges the traveling salesman problem see section from the text the input to this problem is a directed edge weighted graph with a designated vertex the edge weights may be positive or negative the problem is to find the shortest simple path from that visits all of the vertices here feasible solutions are simple paths that start from the source vertex the nodes in level k of the tree represent all paths from of exactly k note that by simplicity we we need only consider the tree to depth n the number of vertices the pruning rule is that if two paths end at the same vertex and contain the same vertices then we may prune the shorter one we thus get the following code here d k s i is the shortest path from to i of k or less hops that visits exactly the vertices in s for k to n do for i to n do for s to n do for each edge e i j do if j is not in s then d k s j j min d k s j j d k s i the length of e the running time of this code is o v where v is the number of vertices and e is the number of edges cs dynamic programming homework problems points consider the recurrence relation t t and for n n t n t i t i i we consider the problem of computing t n from n show that if you implement this recursion directly in say the c programming language that the program would use exponentially in n many arithmetic operations explain how by not recomputing the same t i value twice one can obtain an algorithm for this problem that only uses o arithmetic operations give an algorithm for this problem that only uses o n arithmetic operations points give a polynomial time algorithm that takes three strings a b and c as input and returns the longest sequence s that is a subsequence of a b and c points give an efficient algorithm for finding the shortest common super sequence of two strings a and b c is a super sequence of a if and only if a is a subsequence of c hint obviously this problem is very similar to the problem of finding the longest common sub sequence you should try to first figure out how to compute the length of the shortest common super sequence show the table that your algorithm constructs for the inputs a zxyyzz and b zzyxzy explain how to find the length of the shortest common super sequence in your table explain how to compute the actual shortest common super sequence from your table by tracing back from the table entry that gives the length of the shortest common super sequence points the input to this problem is a pair of strings a am and b bn the goal is to convert a into b as cheaply as possible the rules are as follows for a cost of you can delete any letter for a cost of you can insert a letter in any position for a cost of you can replace any letter by any other letter for example you can convert a abcabc to b abacab via the following sequence abcabc at a cost of can be converted to abaabc which at cost of can be converted to ababc which at cost of can be converted to abac which at cost of can be converted to abacb which at cost of can be converted to abacab thus the total cost for this conversion would be this is almost surely not the cheapest possible conversion give a polynomial time dynamic programming algorithm note the unix diff command essentially solves this problem points find the optimal binary search tree for keys where the access probabilities weights are respectively using the algorithm discussed in class and in the notes construct one table showing the optimal expected access time for all subtrees considered in the algorithm and another showing the roots of the optimal subtrees computed in the other table show how to use the table of roots to recompute the tree points give an efficient algorithm for the following problem the input is an n sided convex polygon assume that the polygon is specified by the cartesian coordinates of its vertices the output should be the triangulation of the polygon into n triangles that minimizes the sums of the perimeters of the into triangles note that this is equivalent to minimizing the length of the cuts required to create the triangle hint this is very similar to the matrix multiplication problem and the problem of finding the optimal binary search tree you need to figure out how to represent all feasible solutions as binary trees hint hint fix a side of the polygon and call it e now consider the triangles that might contain e think of these triangles as the root of a binary tree think of the two resulting polygons that you get by removing this triangle as the two subtrees of the root points the input to this problem is a sequence s of integers not necessarily positive the problem is to find the consecutive subsequence of s with maximum sum consecutive means that you are not allowed to skip numbers for example if the input was the output would be give a linear time dynamic programming algorithm for this problem in your write up first explain why a naive recursive solution is not possible that is figure out why knowing the nth number and the maximum consecutive sum of the first n numbers is not sufficient information to compute the maximum consecutive sum of the first n numbers these examples which show you need to strengthen the inductive hypothesis should give you a big hint how to strengthen the inductive hypothesis to compute two different different consecutive subsequences the maximum consecutive sum subsequence and one other one points the input to this problem is a tree t with integer weights on the edges the weights may be negative zero or positive a positive edge weight represents a payment that one receives when traveling over the edge a negative edge weight represents a payment that one much make when traveling over the edge give a linear time algorithm to find the most profit that one can make from traversing a simple path in t note that you do not have to necessarily find the actual path the length of a path is the sum of the weights of the edges in the path a path is simple if no vertex is repeated note that the endpoints of the path are unconstrained this means that if the tree has n vertices then there are possible simple paths that feasibly might be the shortest simple path depending on the weights as there is a one to one correspondence between pairs of vertices and simple paths in a tree note also that the maximum profit is always non negative since one can make zero profit by picking the start and endpoint to be the same points the input for this problem consists of n keys kn with kn and associated probabilities pn the problem is to find the avl tree for these keys that minimizes the expected depth of a key an avl tree is a binary search tree with the property that every node has balance factor or the balance factor or a node is the height of its right subtree minus the height of its left subtree give a polynomial time algorithm for this problem hint you will have to strengthen the inductive hypothesis obviously the height of a subtree is relevant points the input consists of n intervals over the real line the output should be a collection c of non overlapping intervals such the sum of the lengths of the intervals in c is maximized give a polynomial time algorithm for this problem develop a dynamic programming algorithm by first developing a recursive algorithm hint to get a recursive algorithm you will need to strengthen the induction hypothesis consider the intervals by increasing order of their left endpoints just like in the longest increasing subsequence lis problem there are two pieces of information that are relevant about a partial solution one is obviously the length of the intervals what is the other think about what the answer was in the lis problem develop a dynamic programming algorithm by enumerating the possible subsets of intervals and then pruning the tree hint consider the intervals by increasing order of their left endpoints points consider the code for the knapsack program given in the class notes explain how one can actually find the highest valued subset of objects subject to the weight constraint from the value table computed by this code so you need to explain how to backtrack from the bottom of the table back to the top of the table to actually construct the subset of objects in the knapsack explain how to solve the knapsack problem using only o l memory space and o nl time you need only find the value and weight of the optimal solution not the actual collection of objects points our goal is now to consider the knapsack problem and develop a method for computing the actual items to be taken in o l space and o nl time consider the following problem the input is the same as for the knapsack problem a collection of n items in with weights wn and values vn and a weight limit l the output is in two parts first you want to compute the maximum value of a subset s of the n items that has weight at most l as well as the weight of this subset let us call this value and weight va and wa secondly for this subset s you want to compute the weight and value of the items in in that are in s let use call this value and weight vb and wb so your output will be two weights and two values give an algorithm for this problem that uses space o l and time o nl explain how to use the algorithm from the previous subproblem to get a divide and conquer algorithm for finding the items in the knapsack problem a and uses space o l and time o nl hint first call the algorithm for the previous subproblem what recursive call do you need to make to find the items in the final answer from the items in in what recursive call do you need to make to find the items in the final answer from the items in in in hint hint the solution to the recurrence relation t k t a t b k is o k if a b k points give an algorithm for the following problem whose running time is polynomial in n l input positive integers vn with l n vi output a solution if one exists to n xi vi where each xi is either or hint very similar to subset sum algorithm points give an algorithm for the following problem whose running time is polynomial in n log l input positive integers vn and l output a solution if one exists to n xivi mod n l mod n where each xi is either or here x mod y means the remainder when x is divided by y hint this is like the subset sum problem but you will have to be more severe in your pruning since you can t afford to have l partial solutions anymore points give an algorithm for the following problem whose running time is polynomial in n w input positive integers wn vn and w output the maximum possible value of n xivi subject to n xiwi w and each xi is a nonnegative integer hint the tree of feasible solutions will be different than the one in the knapsack subset sum problem since you may include an item in the final sum more than once each node in the tree of feasible solutions may now have up to w children points give an algorithm for the following problem whose running time is polynomial in n l where l max n n vi input positive integers vn output a subset s of the integers such that vi s vi s vi points the input to this problem is a set of n gems each gem has a value in dollars and is either a ruby or an emerald let the sum of the values of the gems be l the problem is to determine if it is possible to partition of the gems into two parts p and q such that each part has the same value the number of rubies in p is equal to the number of rubies in q and the number of emeralds in p is equal to the number of emeralds in q note that a partition means that every gem must be in exactly one of p or q you algorithm should run in time polynomial in n l hint start as in the subset sum example your pruning rule will have to be less severe that is first ask yourself why you may not be able to prune two potential solutions that have the same aggregate value points the input to this problem consists of an ordered list of n words the length of the ith word is wi that is the ith word takes up wi spaces for simplicity assume that there are no spaces between words the goal is to break this ordered list of words into lines this is called a layout note that you can not reorder the words the length of a line is the sum of the lengths of the words on that line the ideal line length is l no line may be longer than l although it may be shorter the penalty for having a line of length k is l k the total penalty is the maximum of the line penalties the problem is to find a layout that minimizes the total penalty give a polynomial time algorithm for this problem hint consider whether how many layouts of the first m words which have k letters on the last line you need to remember points the input to this problem is two sequences t tn and p pk such that k n and a positive integer cost ci associated with each ti the problem is to find a subsequence of of t that matches p with maximum aggregate cost that is find the sequence ik such that for all j j k we have tij pj and k cij is maximized so for example if n t xy xxy k p xy and then the optimal solution is to pick the second x in t and the second y in t for a cost of give a recursive algorithm to solve this problem then explain how to turn this recursive algorithm into a dynamic program give a dynamic programming algorithm based on enumerating subsequences of t and using the pruning method give a dynamic programming algorithm based on enumerating subsequences of p and using the pruning method points the input to this problem is a sequence of n points pn in the euclidean plane you are to find the shortest routes for two taxis to service these requests in order let us be more specific the two taxis start at the origin if a taxi visits a point pi before pj then it must be the case that i j stop and think about what this last sentence means each point must be visited by at least one of the two taxis the cost of a routing is just the total distance traveled by the first taxi plus the total distance traveled by the second taxi design an o time algorithm to find the minimum cost routing hint use dynamic programming consider exhaustively enumerating the possible tours one point at a time so after the ith stage you would consider all ways to visit the points pi then find a pruning rule that will reduce the number of tours we need to remember down to a polynomial number an o time algorithm is worth points points the input to this problem is n points xn on a line a good path p has the property that one endpoint of p is the origin and every xi is covered by p note that p need not be simple that is it can backtrack over territory that it has already covered assume a vehicle moves along this path from the origin at unit speed the response time ri for each xi is the time until the vehicle first reaches xi the problem is to find the good path that minimizes n ri n the average response time for example if the points are and and the path visited the points in the order the average response time for this path would be give a polynomial time algorithm for this problem points consider the problem where the input is a collection of n train trips within germany for the ith trip ti you are given the date di of that trip and the non discounted fare fi for that trip the german railway system sells a bahncard for b marks that entitles you to a fare reduction on all train travel within germany within l days of purchase the problem is to determine when to buy a bahncard to minimize the total cost of your travel for example if the input was january marks february 200 marks january 200 marks march 100 marks february 200 marks and january 600 marks b and l then you might buy a bahncard on february and february resulting in a total cost of marks give a polynomial time algorithm for this problem the running time of you algorithm should be independent of b and l points give a polynomial time algorithm for the following problem the input consists of a sequence r rn of non negative integers and an integer k the number ri represents the number of users requesting some particular piece of information at time i say from a www server if the server broadcasts this information at some time t the the requests of all the users who requested the information strictly before time t are satisfied the server can broadcast this information at most k times the goal is to pick the k times to broadcast in order to minimize the total time over all requests that requests users have to wait in order to have their requests satisfied as an example assume that the input was r so n and k then one possible solution there is no claim that this is the optimal solution would be to broadcast at times and note that it is obvious that in every optimal schedule that there is a broadcast at time n if rn the requests at time would then have to wait time unit the requests at time would then have to wait time units the requests at time would then have to wait time units the requests at time would then have to wait time units the requests at time would then have to wait time units thus the total waiting time for this solution would be points assume that you are given a collection bn of boxes you are told that the weight in kilograms of each box is an integer between and some constant l inclusive however you do not know the specific weight of any box and you do not know the specific value of l you are also given a pan balance a pan balance functions in the following manner you can give the pan balance any two disjoint sub collections say and of the boxes let and be the cumulative weight of the boxes in and respectively the pan balance then determines whether or you have nothing else at your disposal other than these n boxes and the pan balance the problem is to determine if one can partition the boxes into two disjoint sub collections of equal weight give an algorithm for this problem that makes at most o l uses of the pan balance for partial credit find an algorithm where the number of uses is polynomial in n and l points give a algorithm that takes a positive integer n as input and computes the number of possible orderings of n objects under the relations and for example if n the possible orderings are as follows a b c a b c a b c a b c a c b a c b b a c b a c b c a b c a c a b c a b and c b a your algorithm should run in time polynomial in n points give a polynomial time dynamic programming algorithm for the following problem the input consists of a two dimensional array r of non negative integers and an integer k the value rt p gives the number of of users requesting page p at time t say from a www server at each integer time the server can broadcast an arbitrary collection of pages if the server broadcasts the j pages pj at time t then the requests of all the users who requested pages pj strictly before time t are satisfied this counts at j broadcasts the server can make at most k broadcasts in total over all times the goal is to pick the times to broadcast and the pages to broadcast at those times in order to minimize the total time over all requests that requests users have to wait in order to have their requests satisfied subject to the constraint that there are at most k broadcasts for example if k and the array r was one schedule which is presumably optimal is to broadcast pages a and b at time and page b again at time this gives k total broadcasts the waiting time for the page requests to page a at time would be the page request to b at time would wait the page requests to b at time would wait and the page requests to b at time would wait this gives total waiting time so problem is a special case of this problem where there is only one page points informally in this problem we consider how to divide the chapters in a story into volumes think the volumes of harry potter or however many game of thrones volumes there will be so as to equalize the size of the volumes the input consists of positive integers xn and k here xi is the number of pages in chapter i and k is the desired number of volumes the problem is determine which chapters go into each of the k volumes so as to minimize the difference between the most number of pages in any volumes and the least number of pages in any of the volumes of course you can not reorder the story give an algorithm whose running time is bounded by a polynomial in n you running time should not depend on the number of pages in the chapters cs greedy homework problems points consider the following problem input a set s xi yi i n of intervals over the real line output a maximum cardinality subset si of s such that no pair of intervals in si overlap consider the following algorithm repeat until s is empty select the interval i that overlaps the least number of other intervals add i to final solution set si remove all intervals from s that overlap with i prove or disprove that this algorithm solves the problem points consider the following interval coloring problem input a set s xi yi i n of intervals over the real line think of interval xi yi as being a request for a room for a class that meets from time xi to time yi output find an assignment of classes to rooms that uses the fewest number of rooms note that every room request must be honored and that no two classes can use a room at the same time consider the following iterative algorithm assign as many classes as possible to the first room we can do this using the greedy algorithm discussed in class and in the class notes then assign as many classes as possible to the second room then assign as many classes as possible to the third room etc does this algorithm solve the interval coloring problem justify your answer consider the following algorithm process the classes in increasing order of start times assume that you are processing class c if there is a room r such that r has been assigned to an earlier class and c can be assigned to r without overlapping previously assigned classes then assign c to r otherwise put c in a new room does this algorithm solve the interval coloring problem justify your answer hint let be the maximum number of intervals that overlap at one particular point in time obvi ously you need at least rooms therefore any algorithm that uses only rooms is obviously optimal this lower bound on the number of rooms required allows you to prove optimality without using an exchange argument points we consider two change making problems consider the change problem in pre euro austria the input to this problem is an integer l the output should be the minimum cardinality collection of coins required to make l shillings of change that is you want to use as few coins as possible in austria the coins are worth shillings assume that you have an unlimited number of coins of each type formally prove or disprove that the greedy algorithm that takes as many coins as possible from the highest denominations correctly solves the change problem so for example to make change for shillings the greedy algorithms would take four 50 shilling coins one shilling coin one shilling coin and four shilling coins consider the change problem in binaryland the input to this problem is an integer l the output should be the minimum cardinality collection of coins required to make l nibbles of change that is you want to use as few coins as possible in binaryland the coins are worth nibbles assume that you have an unlimited number of coins of each type prove or disprove that the greedy algorithm that takes as many coins of the highest value as possible solves the change problem in binaryland hint the greedy algorithm is correct for one of the above two subproblems and is incorrect for the other for the problem where greedy is correct use the following proof strategy assume to reach a contradiction that there is an input i on which greedy is is not correct let op t i be a solution for input i that is better than the greedy output g i show that the existence of such an optimal solution op t i that is different than greedy is a contradiction so what you can conclude from this is that for every input the output of the greedy algorithm is the unique optimal correct solution points you wish to drive from point a to point b along a highway minimizing the time that you are stopped for gas you are told beforehand the capacity c of you gas tank in liters your rate f of fuel consumption in liters kilometer the rate r in liters minute at which you can fill your tank at a gas station and the locations a b xn of the gas stations along the highway so if you stop to fill your tank from liters to liters you would have to stop for r minutes consider the following two algorithms stop at every gas station and fill the tank with just enough gas to make it to the next gas station stop if and only if you don t have enough gas to make it to the next gas station and if you stop fill the tank up all the way for each algorithm either prove or disprove that this algorithm correctly solves the problem your proof of correctness must use an exchange argument points consider the following problem the input is a collection a an of n points on the real line the problem is to find a minimum cardinality collection s of unit intervals that cover every point in a another way to think about this same problem is the following you know a collection of times a that trains will arrive at a station when a train arrives there must be someone manning the station due to union rules each employee can work at most one hour at the station the problem is to find a scheduling of employees that covers all the times in a and uses the fewest number of employees prove or disprove that the following algorithm correctly solves this problem let i be the interval that covers the most number of points in a add i to the solution set s then recursively continue on the points in a not covered by i prove or disprove that the following algorithm correctly solves this problem let aj be the smallest leftmost point in a add the interval i aj aj to the solution set s then recursively continue on the points in a not covered by i hint one of the above greedy algorithms is correct and one is incorrect for the other the proof of correctness must use an exchange argument points we consider a greedy algorithm for two related problems the input to this problem consists of an ordered list of n words the length of the ith word is wi that is the ith word takes up wi spaces for simplicity assume that there are no spaces between words the goal is to break this ordered list of words into lines this is called a layout note that you can not reorder the words the length of a line is the sum of the lengths of the words on that line the ideal line length is l no line may be longer than l although it may be shorter the penalty for having a line of length k is l k the total penalty is the sum of the line penalties the problem is to find a layout that minimizes the total penalty prove of disprove that the following greedy algorithm correctly solves this problem for i to n place the ith word on the current line if it fits else place the ith word on a new line the input to this problem consists of an ordered list of n words the length of the ith word is wi that is the ith word takes up wi spaces for simplicity assume that there are no spaces between words the goal is to break this ordered list of words into lines this is called a layout note that you can not reorder the words the length of a line is the sum of the lengths of the words on that line the ideal line length is l no line may be longer than l although it may be shorter the penalty for having a line of length k is l k the total penalty is the maximum of the line penalties the problem is to find a layout that minimizes the total penalty prove of disprove that the following greedy algorithm correctly solves this problem for i to n place the ith word on the current line if it fits else place the ith word on a new line hint the greedy algorithm is correct for one of the above two problems and is incorrect for the other the proof of correctness must be done using an exchange argument points the setting for this problem is storage system with a fast memory consisting of k pages and a slow memory consisting of n pages at any time the fast memory can hold copies of up to k of the pages in slow memory the input consists of a sequence of pages from slow memory think of these as being accesses to memory if an accessed page is not in fast memory then it must be swapped into fast memory and if the fast memory was full some page must selected to be evicted from fast memory the goal is to determine the pages to evict so as to minimize the total number of evictions consider for example that k n pages are named a b c and d and the access sequence is a b c a then after the first two pages the fast memory contains a and b when c is accessed then either a or b must be evicted if b is evicted then no further evictions are necessary and the total number of evictions is if a was evicted then either b or c must be evicted when a is accessed again and the total number of evictions would be give a greedy for this problem and prove that it is correct using an exchange argument points consider the following problem the input consists of the lengths n and access probabilities pn for n files fn the problem is to order these files on a tape so as to minimize the expected access time if the files are placed in the order fs fs n then the expected access time is n i ps i j don t let this formula throw you the term ps i i j is the probability that you access the ith file times the length of the first i files for each of the below algorithms either give a proof that the algorithm is correct using an exchange argument or a proof that the algorithm is incorrect order the files from shortest to longest on the tape that is i j implies that i j order the files from most likely to be accessed to least likely to be accessed that is pi pj implies that i j order the the files from smallest ratio of length over access probability to largest ratio of length over access probability that is i j implies that i j pi pj points consider the following problem the input consists of n skiers with heights pn and n skies with heights sn the problem is to assign each skier a ski to to minimize the average difference between the height of a skier and his her assigned ski that is if the ith skier is given the  i th ski then you want to minimize n pi n i s i consider the following greedy algorithm find the skier and ski whose height difference is mini mized assign this skier this ski repeat the process until every skier has a ski prove of disprove that this algorithm is correct consider the following greedy algorithm give the shortest skier the shortest ski give the second shortest skier the second shortest ski give the third shortest skier the third shortest ski etc prove of disprove that this algorithm is correct hint one of the above greedy algorithms is correct and one is incorrect for the other the proof of correctness must be done using an exchange argument points we consider the following scheduling problem input a collection of jobs jn where the ith job is a tuple ri xi of non negative integers specifying the release time and size of the job output a preemptive feasible schedule for these jobs on one processor that minimizes the total completion time n ci a schedule specifies for each unit time interval the unique job that that is run during that time interval in a feasible schedule every job ji has to be run for exactly xi time units after time ri the completion time ci for job ji is the earliest time when ji has been run for xi time units examples of these basic definitions can be found below we consider two greedy algorithms for solving this problem that schedule times in an online fashion that is the algorithms are of the following form t while there are jobs left not completely scheduled among those jobs ji such that ri t and that have previously been scheduled for less than xi time units pick a job jm to schedule at time t according to some rule increment t one can get different greedy algorithms depending on the rule for selecting jm for each of the following greedy algorithms prove or disprove that the algorithm is correct proofs of correctness must use an exchange argument hint the most obvious exchange argument does not work if you think that the first thing that you tried worked you might want to reevaluate sjf pick jm to be the job with minimal size xi ties may be broken arbitrarily srpt let yi t be the total time that job ji has been run before time t pick jm to be a job that has minimal remaining processing time that it that has minimal xi yi t ties may be broken arbitrarily as an example of sjf and srpt consider the following instance and both sjf and srpt schedule job between time and time and job between time and time when job completes and job again between time and time at time sjf schedules job because its original size is less than job original size at time srpt schedules job because its remaining processing time is less than job remaining processing time both sjf and srpt schedule job between time and when completes and then job from time until time which job completes thus for both sjf and srpt on this instance and and thus both sjf and srpt have total completion time points we consider the following scheduling problem input a collection of jobs jn the size of job ji is xi which is a nonnegative integer an integer m output a nonpreemptive feasible schedule for these jobs on m processor that minimizes the total completion time n ci a schedule specifies for each unit time interval and for each processor the unique job that that is run during that time interval on that processor in a feasible schedule every job ji has to be run for exactly xi time units after time in a nonpreemptive schedule once a job starts running on a particular processor it has to be run to completion on that particular processor the completion time ci for job ji is the earliest time when ji has been run for xi time units so for example if m jobs of size are run in that order on the first processor and jobs of size are run on the second processor in that order then the total completion time would be give a greedy algorithm for this problem and prove that it is correct points consider the following problem input positive integers rn and cn output an n by n matrix a with entries such that for all i the sum of the ith row in a is ri and the sum of the ith column in a is ci if such a matrix exists think of the problem this way you want to put pawns on an n by n chessboard so that the ith row has ri pawns and the ith column has ci pawns consider the following greedy algorithm that constructs a row by row assume that the first i rows have been constructed let aj be the number of in the jth column in the first i rows now the ri columns with with maximum cj aj are assigned in row i and the rest of the columns are assigned that is the columns that still needs the most are given formally prove that this algorithm is correct using an exchange argument points we consider two problem where the input contains n jobs where each job j has an integer release time rj and an integer deadline dj in a feasible schedule each job j must be run for one unit of time not starting before rj and not ending after dj note that no two jobs may be run at the same time assume all release times are nonnegative and let t maxj dj in the warmup problem the input also specifies for each integer t t whether a particular machine is turned on during the time interval t t the problem is to determine if each job j can feasibly be run for one unit of time when the machine is turned on give a greedy algorithm and prove that it is correct using an exchange argument in the real problem the input also contains a positive integer l the problem is to determine the minimum number k of time intervals of length l such all jobs can feasibly be run for one unit of time during one of these k intervals another way to interpret this problem it costs a dollar to turn the machine on once the machine is on it will stay on for l units of time the question is to figure out the least number of times to turn the machine on so that one can finish all the jobs which each have to be run for one unit of time give a greedy algorithm and prove that it is correct using an exchange argument points you have n heterosexual men and n heterosexual women each man ranks the women in order of preference each woman ranks the men in order of preference consider the following incredibly stereotypical courting algorithm on stage i each man goes to pitch woo on the porch of the woman that he prefers most among all women that have not rejected him yet at the end of the stage the woman rejects all the men on her porch but the one that she favors most note that a women may not reject a man in some stage but later end up rejecting that man if a better prospect arrives on her porch if it should ever happen that there is exactly one man on each porch the algorithm terminates and each woman marries the man on her porch you may be interested to know that medical schools really use this algorithm to fill intern positions give an upper bound as a function of n of the number of stages in this algorithm a marriage assignment is stable if there does not exist a man x and a woman y such that x prefers y to his assigned mate and y prefers x to her assigned mate clearly adultery is a risk if a marriage assignment is not stable prove that this algorithm leads to a stable marriage a stable marriage m is man optimal if for every man x m is the best possible stable marriage that is in every stable marriage other than m x ends up with a woman no more preferable to him than the woman he is married to in m prove or disprove the above algorithm produces a man optimal stable marriage a stable marriage m is woman optimal if for every woman y m is the best possible stable marriage that is in every stable marriage other than m y ends up with a man no more preferable to her than the man she is married to in m prove or disprove the above algorithm produces a woman optimal stable marriage a stable marriage m is man pessimal if for every man x m is the worst possible stable marriage that is in every stable marriage other than m x ends up with a woman no less preferable to her that the woman he is married to in m prove or disprove the above algorithm produces a man pessimal stable marriage a stable marriage m is woman pessimal if for every woman y m is the worst possible stable marriage that is in every stable marriage other than m y ends up with a man no less preferable to her list than the man she is married to in m prove or disprove the above algorithm produces a woman pessimal stable marriage points the setting for this problem is a line network model by a line graph there are n nodes in this graph and each node is connected to a left neighbor and a right neighbor except the leftmost node has no left neighbor and the rightmost node has no right neighbor the input consists of k packets where packet p consists of an integer release time rp when the packet p arrives in the system a source node sp at which the packet p arrives and a destination node tp which must be to the right of sp to which the packet p must reach between consecutive integer times one packet may be forwarded between each pair of adjacent routers once a packet p reaches its destination tp it leaves the system we want all packets to reach their destinations for example if the input consisted of the following triples of the form name rp sp tp a b c then one way to have each packet reach its destination is between time and pack a is forwarded from node to node and packet b is forwarded from node to node between time and time packet a is forwarded from node to node leaving the system at time and packet b is forwarded from node to node leaving the system at time we could have forward packet c instead of packet a but it is not possible to forward both at this time between time and time packet c is forward from node to node leaving the system at time assume that the objective is to minimize the maximum time that a packet leaves the network that is we want to clear the network of packets as quickly as possible give a greedy algorithm for this problem and prove that it is correct assume that n so that the network consists of two nodes and one edge further assume that the objective is to minimize the maximum waiting time for any packet the waiting time for a packet is difference between the time that the packet reaches its destination and the release time for that packet that is we want the packet that waits the longest to wait as little time as possible give a greedy algorithm for this problem and prove that it is correct assume all the release times are zero further assume that the objective is to minimize the sum of the waiting times of the packets this is equivalent to minimizing the average waiting times of the packets give a greedy algorithm for this problem and prove that it is correct this problem is quite nontrivial points consider the following bridge crossing problem where n people with speeds sn wish to cross the bridge as quickly as possible the rules remain it is nighttime and you only have one flashlight a maximum of two people can cross at any one time any party who crosses either or people must have the flashlight with them the flashlight must be walked back and forth it cannot be thrown etc a pair must walk together at the rate of the slower person pace give an efficient algorithm to find the fastest way to get a group of people across the bridge you must have a proof of correctness for your method points a heterosexual couple is divorcing and need to partition n individual goods gn each of the husband and wife has a valuation for each item but you do not know this valuation however based on their valuations each of the husband and wife does provide you with an ordered list specifying the order that they value the items your goal is to determine whether the lists contain enough information to determine with a certainty that it is possible to partition the goods so that each of the husband and wife think that the total value of the goods that they receive is more than half of the total valuation of the goods according to their evaluation let us call this a fair partition if so you should give a fair partition note that the husband and wife may value goods very differently so for example if there were goods the husband list from most to least desirable was and the wife list was then an fair partition is possible by giving the husband and and the wife and in contrast if there were goods the husband list from most to least desirable was and the wife list was g4 then there is not enough information to determine whether a fair partition is possible give a greedy algorithm for this problem and prove that it is correct we consider the following problem input a collection of jobs jn where the ith job is a tuple ri xi di of non negative integers output if there is a preemptive feasible schedule for these jobs on one processor and otherwise a schedule is feasible if every job job ji is run for xi time units between its release time ri and its deadline di we consider greedy algorithms for solving this problem that schedule times in an online fashion that is the algorithms are of the following form t while there are jobs left not completely scheduled pick a job jm to schedule at time t increment t one can get different greedy algorithms depending on how job jm is selected for each of the follow ing methods of selecting jm prove or disprove the that resulting greedy algorithms produce feasible schedules if they exist for the jobs being considered your proof of correctness must use an exchange argument among those jobs ji such that ri t and that have not been scheduled for enough time units pick jm to be the job i whose size xi is smallest ties may be broken arbitrarily among those jobs ji such that ri t and that have not been scheduled for enough time units pick jm to be the job i whose remaining size xi t is smallest ties may be broken arbitrarily the remaining size xi t of a job i at a time t is the size xi minus the amount of time that job i has been run before time t so if a job had size and was run for units of time before t its remaining size would be among those jobs such that ri t and that have not been scheduled for enough time units pick jm to be the job i whose laxity i t is the smallest ties may be broken arbitrarily the laxity of a job i at time t is di t xi t that is the deadline minus the current time minus the remaining size problem this algorithm does not solve the problem of finding a maxi mum cardinality set of non overlapping intervals consider the following intervals e a b c d obviously the optimal solution is a b c d however the interval that overlaps with the fewest others is e and the algorithm will select e first which precludes it from picking intervals b and c problem this algorithm does not solve the interval coloring problem con sider the following intervals a b c d e f g the optimal solution is to put a in one room b c d in another and e f g in another for a total of rooms however max imizing the number of classes in the first room results in having b c f g in one room and classes a d and g each in their own rooms for a total of this algorithm does solve the interval coloring problem note that if the greedy algorithm creates a new room for the current class ci then because it examines classes in order of start times ci start point must intersect with the last class in all of the current rooms thus when greedy creates the last room n it is because the start time of the current class intersects with n other classes but we know that for any single point in any class it can only intersect with at most other class it must be then that n as is a lower bound on the total number needed and greedy is feasible it is thus also optimal problem the greedy algorithm is not optimal for the problem of making change with the minimum number of coins when the denominations are and in order to make shillings the greedy algorithm would use three coins of and shillings the opti mal solution is to use two shilling coins the greedy algorithm is optimal for the problem of making change with the minimum number of coins when the denominations are suc cessive powers of some integer p we prove this by contradiction assume there is some integer d such that the greedy algorithm is not optimal for the set of denominations dm when mak ing change for x for i n let gi denote the number of coins of denomination di picked by the greedy algorithm and let ti denote the number of denomination di picked in an optimal solution we have n n gidi tidi x i i starting from n let k be the first index for which gk tk the greedy algorithm takes as many coins worth dk as it can so it must be true that gk tk furthermore since both solutions have the same total value x the total value of tk tk must make up for the lost value of at least one dk this can never happen note that k k k tidi d di di di dk dk i i i this says that if tmdm x then one of the ti for i k must be greater than d however if there are ever more than d of any denomination of coin except the largest than d of those coins can be replaced by one coin of the next largest denomination this contradicts our assumption that the t form an optimal solution since if any ti differs from gi then there must be at least d coins of some smaller denomination among the ti problem this greedy algorithm is optimal we prove by contradiction as sume greedy is not optimal for input i we pick the optimal solution op t that is identical to greedy for the most consectutive gas sta tions consider the first gas station where the greedy solution g and op t differ call it station k say g adds gk gas and op t adds ok gas we now create a new solution op t as follows op t is iden tical to op t at every station except k and k call the amount of gas op t adds at station k ok at station k op t only adds gk gas to the tank and at station k op t adds ok ok gk clearly op t is identical to g for one more station namely k we claim that op t is feasible and spends no more time filling the tank than op t prior to station k op t is identical to g thus be cause g makes it to k op t must make it to k by the fact that greedy adds the minimal amount of gas required to get from k to k and g and op t differ at k it must be that ok gk thus ok ok gk meaning op t adds a valid amount of gas at k futher because gk ok ok gk ok ok op t has the same amount of gas in the tank as op t after filing up at k namely ok ok gk because op t is identical to op t after k op t never runs out of gas after k finally because the total gas put in the tank by op t over k and k is gk ok ok gk ok ok op t and op t add the same amount of gas in total over the two stations in which they dif fer making their total time spent filling the same thus we have an optimal solution that is identical to greedy for one more station a contradiction this greedy algorithm is not optimal without loss of generality we can assume the car starts at a with an empty tank consider the input of futher assume that c f and r are such that a full tank of gas takes you the greedy algorithm will fill the tank twice but filling the tank only at then adding just enough at to go will give a lower total time filling up problem this algorithm is not optimal for the problem of covering points with unit intervals let the points to be covered be a b c d e and f the algorithm that tries to maximize the number of points covered by the first interval will cover b c d e with the first interval which forces it to use at least intervals total the points can however be covered with two intervals and this algorithm is optimal for the problem of covering points with unit intervals assume there is a set of points a an such that the solution obtained by the greedy algorithm is not optimal call the greedy solution g gn and the optimal solution t tn assume the intervals are numbered in increasing order of left endpoint starting at the leftmost interval in g compare g and t let k be the number of the first interval for which gk tk by the definition of the greedy algorithm it must be the case that gk tk meaning that gk begins further to the right than tk create solution t by replacing interval tk with gk since for i k gi ti solution t will continue to cover all the points in a if gk overlaps any other interval tj in t shift tj to the right until it no longer overlaps gk continue shifting intervals in t to the right until there are no more overlaps note that t continues to cover all points in a by repeating the above process we can make t g contradicting our assumption that g is not an optimal solution problem this algorithm is incorrect for the problem of minimizing the average difference between the heights of skiers and their skis let and the algorithm would pair with and with for a total cost of pairing with and with yields a total cost of the algorithm is correct for the problem of minimizing the aver age difference between the heights of skiers and their skis the proof is by contradiction assume the people and skis are num bered in increasing order by height if the greedy algorithm is not optimal then there is some input pn sn for which it does not produce an optimal solution let the optimal solution be t s pn s n and note the output of the greedy algorithm will be g pn sn beginning with compare t and g let pi be the first person who is assigned differ ent skis in g than in t let sj be the pair of skis assigned to pi in t create solution t by switching the ski assignments of pi and pk where pk is the person who was assigned si in t note that by the definition of the greedy algorithm si sj also note that by def of pi pi pk the total cost of t is given by cost t cost t p n i sj pk si pi si pk sj there are six cases to be considered for each case one needs to show that pi sj pk si pi si pk sj case pi pk si sj pi sj pk si pi si pk sj sj pi si pk si pi sj pk case pi si pk sj case pi si sj pk case si sj pi pk pi sj pk si pi si pk sj pi sj pk si pi si pk sj case si pi sj pk case si pi pk sj problem the algorithm is correct for the problem of building an n n matrix with zeros and ones such that the sum of all ones in the ith row is ri and the sum of all ones in the ith column is ci for all i n the proof is by contradiction assume there is some input rn cn for which the greedy algorithm does not give the correct solution call any correct matrix t and the matrix generated by the greedy algorithm g let i and j be two numbers such that gij tij let gij this implies that tij by the definition of the problem there must be a number k j such that gik and tik create matrix t by making tij gij t is not a feasible solution column j has too many ones and column k has too few since the greedy algorithm placed a in gij and a in gik it must be true that cj ck therefore the number of ones in column k of t is at most c and the number in column j is exactly cj there must be at least one number l i such that glj t glk and t create a new matrix t by making g lj t t lk lj lj and glk lk columns j and k now have the correct number of ones matrix t is now a feasible solution that is closer to g that t contradiction the case where gij and tij is nearly identical problem the following greedy algorithm selects the optimal output for all inputs at each step in the sequence where a page in fast memory needs to be replaced by one in slow memory replace the page whose next use is at the latest point in the sequence proof let g be the greedy algorithm described by the theorem sup pose g is non optimal for some input sequence let opt be the optimal algorithm having the fewest number of swaps that agrees the most with g let k be the first swap where opt and g disagree on which page to swap into fast memory let label the page swapped out of fast memory by g at step k as a the page swapped out of fast memory by opt as b and the page that needs to be swapped in by both as c we know that by definition of the greedy algorithm b will appear sooner than a after step k let call the step where b next occurs step i and the step where a next occurs step j where i occurs before j let call the steps between step k and step i region x and the steps between step k and step j region y where y includes step i and region x ideally there should be a picture illustrating these definitions suppose that there exists a solution opt that is identical to opt ex cept for at step k opt makes the same decision as g and selects a to be replaced is opt still an optimal solution in order for opt to be optimal it must have the same number of swaps as opt meaning that the change of the decision at k did not affect the number of swaps that had to occur for the algorithm to be feasible after step k the fast memory of opt contains at least pages a and c having swapped out page b we assume the problem applies to fast mem ories of size or greater since a page fast memory would only have one feasible solution of swapping at every non repeated page this means that opt makes at least swap by step i where b needs to be swapped into fast memory opt instead must swap by step j where a needs to be swapped into fast memory it must be the case to meet requirements for optimality that no extra swaps were incurred by the decision of opt to replace b at step k at step k the fast memories of opt and opt are identical except for the page which contains either a or b with this information we might try to infer that any swaps not involving b that opt needs to make in region y will hold true for opt as well but all cases must be considered these cases must concern pages a and b for they account for the only difference between the fast memories of opt and opt consider the first of such a case where opt and opt take different actions in region x this could only occur if some page in the sequence within x not a or b caused the page a or b to be replaced otherwise opt and opt would not be taking different actions this action however would make the fast memories of both solutions identical with an equal number of swaps taken and allow opt to remain optimal now consider a second case where both solutions agree up to step i where opt must perform a swap to put b in fast memory if opt replaces a then opt and opt have the same fast memory with opt having made extra swap contradicting the notion that it is indeed optimal for all inputs so let assume that opt replaces some other page in fast memory that we call z which could be anything other than a or b including c opt in this case could take the opportunity to also replace z and return a to fast memory showing that both solutions can have the same fast memory by step i with the same number of swaps following step i then opt and opt are identical and opt is one step closer to g this however conflicts with the premise that opt was the closest solution to g and causes a contradiction it may also be useful to consider the case that there is no step after k where a occurs in the sequence in this case opt would not have to make any swap at step i to exactly conform to the fast memory of opt step j does not exist and a does not need to be in fast memory but opt would necessarily have to make a swap at step i this suggests that opt would be non optimal for any case where b occurs in the sequence following step k and a does not the halting problem is input a string p and a string i we will think of p as a program output if p halts on i and if p goes into an infinite loop on i theorem turing circa there is no program to solve the halting problem proof assume to reach a contradiction that there exists a program halt p i that solves the halting problem halt p i returns true if and only p halts on i the given this program for the halting problem we could construct the following string code z program string x if halt x x then loop forever else halt end consider what happens when the program z is run with input z case program z halts on input z hence by the correctness of the halt program halt returns true on input z z hence program z loops forever on input z contradiction case program z loops forever on input z hence by the correctness of the halt program halt returns false on input z z hence program z halts on input z contradiction end proof one can now show that there is no program for some new problem problem u by showing that halting u i e halting is reducible to u supposed google and or microsoft interview questions a you are given identical looking balls of them weigh exactly the same and is heavier than the rest you are provided with a simple mechanical balance and you are restricted to only uses find the heavier ball b now you are give identical looking balls one of them is heavier or lighter you don t know which than the rest of the you are provided with a simple mechanical balance and you are restricted to only uses find the odd ball and determine whether it is heavier or lighter than the rest four people need to cross a rickety bridge at night unfortunately they have only one torch and the bridge is too dangerous to cross without one the bridge is only strong enough to support two people at a time not all people take the same time to cross the bridge the times for the four people are min mins mins and mins when two people cross together the time taken is equal to the time of the slower person what is the shortest time needed for all four of them to cross the bridge if you had an infinite supply of water a quart and a quart pail how would you measure exactly quarts you are only allowed to fully fill a pail from your supply transfer from one pail to another until the receiving pail is full or empty a pail each step is considered as a single fill empty or transfer what is the least number of steps you need five pirates discover a chest containing gold coins they decide to sit down and devise a distribution strategy the pirates are ranked based on their expe rience pirate to pirate where pirate is the most experienced the most experienced pirate gets to propose a plan and then all the pirates vote on it if at least half of the pirates agree on the plan the gold is split according to the proposal if not the most experienced pirate is thrown off the ship and this pro cess continues with the remaining pirates until a proposal is accepted the first priority of the pirates is to stay alive and the second is to maximize the gold they get pirate devises a plan which he knows will be accepted for sure and will maximize his gold what is his plan prisoners are stuck in the prison in solitary cells the warden of the prison got bored one day and offered them a challenge he will put one prisoner per day selected at random a prisoner can be selected more than once into a special room with a light bulb and a switch which controls the bulb no other prisoners can see or control the light bulb while not in the room the prisoner in the special room can either turn on the bulb turn off the bulb or do nothing on any day any prisoner can stop this process and say every prisoner has been in the special room at least once if that happens to be true all the prisoners will be set free if it is false then all the prisoners will be executed the prisoners are given some time to discuss and figure out a solution once they agree on a solution and the random choosing starts they are not able to communicate until they are either released or executed in which case they won t be able to communicate anyway how do they ensure they all go free you have two identical eggs standing in front of a floor building you wonder what is the maximum number of floors from which the egg can be dropped without breaking it what is the minimum number of tries needed to find out the solution what if the building has n floors a certain town is comprised of married couples everyone in the town lives by the following rule if a husband cheats on his wife the husband is executed as soon as his wife finds out about him all the women in the town only gossip about the husbands of other women no woman ever tells another woman if her husband is cheating on her so every woman in the town knows about all the cheating husbands in the town except her own it can also be assumed that a husband remains silent about his infidelity one day the mayor of the town announces to the whole town that there is at least cheating husband in the town what happens given an array of natural numbers efficiently find the longest contiguous increasing subsequence efficiently find the longest increasing subsequence i e numbers need not be immediately after each other in the array there are baskets one of them has apples one has oranges and the other has mixture of apples and oranges the labels on their baskets always lie i e if the label says oranges you are sure that it doesn t have oranges only it could be apples only or it could be a mixture the task is to pick one basket and pick only one fruit from it and then correctly label all the three baskets you have two jars red marbles and blue marbles a jar will be picked at random and then a marble will be picked from the jar placing all of the marbles in the jars how can you maximize the chances of a red marble being picked what are the exact odds of getting a red marble using your scheme you have jars of pills each pill weighs grams except for contaminated pills contained in one jar where each pill weighs grams given a scale how could you tell which jar had the contaminated pills in just one measurement you are given three sorted arrays a b and c of n numbers each for any triplet a i b j c k the distance of that triplet is defined as the maximum value of the absolute differences between any of the three values in other words the distance of any triplet is the maximum of a i b j a i c k and b j c k find the minimum distance triplet in o n time given an array t n which contains numbers between and n return the duplicated value also given an array of length n containing integers between and n determine if it contains any duplicates is there an o n time solution that uses only o extra space and does not destroy the original array given a singly linked list determine whether it contains a loop or not using only o words of memory cs parallel algorithms homework problems points consider the problem of computing the and of n bits give an algorithm that runs in time o log n using n processors on an erew pram what is the efficiency of this algorithm using the folding principle what upper bound would you get on the running time for this algorithm on processors give an algorithm that runs in time o log n using n log n processors on an erew pram what is the efficiency of this algorithm using the folding principle what upper bound would you get on the running time for this algorithm on processors give an algorithm that runs in time o using n processors on a crcw common pram what is the efficiency of this algorithm using the folding principle what upper bound would you get on the running time for this algorithm on processors points you know that lots of famous computer scientists have tried to find a fast efficient parallel algorithm for the following boolean formula value problem input a boolean formula f and a truth assignment a of the variables in f output if a makes f true and otherwise moreover most computer scientists believe that there is no fast efficient parallel algorithm for the boolean value problem you want to find a fast efficient parallel algorithm for some new problem n after much effort you can not find a fast efficient parallel algorithm for n nor a proof that n does not have a fast efficient parallel algorithm how could you give evidence that finding a fast efficient parallel algorithm for n is at least a hard of a problem as finding a fast efficient parallel algorithm for boolean formula value problem be as specific as possible and explain how convincing the evidence is note that fast and efficient means poly log time with a polynomial number of processors the term poly log means bounded by o logk n for some constant k points consider the problem of taking as input an integer n and an integer x and creating an array a of n integers where each entry of a is equal to x give an algorithm that runs in time o log n using n processors on an erew pram what is the efficiency of this algorithm using the folding principle what upper bound would you get on the running time for this algorithm on processors give an algorithm that runs in time o log n using n log n processors on an erew pram what is the efficiency of this algorithm using the folding principle what upper bound would you get on the running time for this algorithm on processors give an algorithm that runs in time o using n processors on a crcw common pram what is the efficiency of this algorithm using the folding principle what upper bound would you get on the running time for this algorithm on processors points design a parallel algorithm for the parallel prefix problem that runs in time o log n with n log n processors on a erew pram points give an algorithm that given an integer n computes n that is n factorial in time o log n on an erew pram with n processors make the unrealistic assumption that a word of memory can store arbitrarily large integers points we consider the problem of multiplying two n by n matrices assume that the sequential time algorithm that you wish to compare to has time complexity s n design a parallel algorithm that runs in time n on a crew pram with processors what is the efficiency of this algorithm using the folding principle what upper bound would you get on the running time for this algorithm on processors design a parallel algorithm that runs in time o log n time on a crew pram with processors what is the efficiency of this algorithm using the folding principle what upper bound would you get on the running time for this algorithm on processors design a parallel algorithm that runs in time o log n time on a crew pram with log n processors what is the efficiency of this algorithm using the folding principle what upper bound would you get on the running time for this algorithm on processors design a parallel algorithm that runs in time o log n time on a erew pram with log n processors hint recall problem what is the efficiency of this algorithm using the folding principle what upper bound would you get on the running time for this algorithm on processors points design a parallel algorithm that given a polynomial p x of degree n and an integer k computes the value of p k you algorithm should run in time o log n on a erew pram with o n log n processors assume that the polynomial is represented by its coefficients further assume that all numbers that you will compute will fit within a word of memory points we consider the problem of computing fn the nth fibonacci number given an integer n as input show how to solve this problem in time o log n on a erew pram with n processors make the unrealistic assumption that fn will fit within one word of memory for all n that is assume that all arithmetic operations take constant time recall that fn is defined by the following recurrence and fn fn fn for n hint note that for j l fj l fj l points the input to this problem is a character string c of n letters the problem is to find the largest k such that k n and such that c c c k c n k c n c n that is k is the length of the longest prefix that is also a suffix give a erew parallel algorithm that runs in poly logarithmic time with a polynomial number of processors points the input to this problem is a character string c of n letters the problem is to find the largest k such that k n and such that c c c k c n k c n c n that is k is the length of the longest prefix that is also a suffix give a crcw common parallel algorithm that runs in constant time with a polynomial number of processors points we consider the problem of adding two n bit integers give an algorithm that runs in o log n time on a crew pram with n processors note if your algorithm is erew you might want to rethink since i don t know how to do this easily without cr hint use divide and conquer and generalize the induction hypothesis give an algorithm that runs in o n time on an erew pram with n processors points explain how to modify the all pairs shortest path algorithm for a crew pram that was given in class so that it runs in time o n on a erew pram with processors points explain how to modify the all pairs shortest path algorithm for a crew pram that was given in class so that it actually returns the shortest paths not just their lengths in time o n on a erew pram with processors points explain how to solve the longest common subsequence problem in time o n using at most a polynomial number of processors on a crew pram hint one way to do this is to reduce the longest common subsequence problem to a shortest path problem note that the shortest path algorithm works for any graph for which there are not cycles whose aggregate weight is negative points give an algorithm for the minimum edit distance problem that runs in poly log time on a crew pram with with a polynomial number of processors here poly log means o logk n where n is the input size and k is some constant independent of the input size recall that the input to this problem is a pair of strings a am and b bn the goal is to convert a into b as cheaply as possible the rules are as follows for a cost of you can delete any letter for a cost of you can insert a letter in any position for a cost of you can replace any letter by any other letter points design a parallel algorithms that merges two sorted arrays into one sorted array in time o using a polynomial number of processors on a crcw common pram points design a parallel algorithm that finds the maximum number in a sequence xn of not necessarily distinct integers your algorithm should run in time o log log n on a crcw common pram with n processors hint recall that you can find the maximum of k numbers in o time with  processors try divide and conquer into n subproblems points design a parallel algorithm that finds the maximum number in a sequence xn of not necessarily distinct integers in the range to n your algorithm should run in constant time on a crcw priority pram with n processors note that it is important here that the xi have restricted range in a crcw priority pram each processor has a unique positive integer identifier and in the case of write conflicts the value written is the value that the processor with the lowest identifier is trying to write points design a parallel algorithm that finds the maximum number in a sequence xn of not necessarily distinct integers in the range to n your algorithm should run in constant time on a crcw commmon pram with n processors note that it is important here that the xi have restricted range points consider the following problem compose input array a of size n containing positive integers array b of size n containing integers in the range n output an array c of size n where each entry c i a b i for example if a and b then the output would be c give a parallel algorithm that runs in time o n on an erew pram with n processors points give a parallel algorithm for the following problem that runs in time o log n on an erew pram the input is a binary tree with n nodes assume that each processor has a pointer to a unique node in the tree the problem is to number the leafs consecutively from left to right that is in order note that this algorithm is needed for the algorithm in the notes for computing arithmetic expressions points give a parallel algorithm for the following problem that runs in time o log n on an erew pram the input is a binary tree with n nodes assume that each processor has a pointer to a unique node in the tree the problem is determine the balance factor of each node in the tree the balance factor of a node is the height of its left subtree minus the height of its right subtree points design a parallel algorithm that takes a binary expression tree where the leaves are integers and the internal nodes are the four standard arithmetic operators addition subtraction multiplication and division and computes the value of the expression your algorithm should run in o n time on a crew pram with n processors where n is the number of nodes in the tree you may assume that each processor initially has a pointer to a unique node in the tree hint following the technique used for subtraction in the class notes you need only find a collection of functions that contain the identity function and constant functions and is closed under addition subtraction multiplication division with constants and composition points design a parallel algorithm that takes a binary expression tree where the leaves are boolean values or and the internal nodes are the three standard logical operations not or and and the output should be the value of the expression represented by the tree your algorithm should run in o n time on a crew pram with n processors where n is the number of nodes in the tree you may assume that each processor initially has a pointer to a unique node in the tree points give a parallel algorithms for the following problem that runs in time o log n on an erew pram with n processors the input is a fully paranthesized arithmetic expression with n symbols stored in an n element array in the standard in order fasion the output should be an expression tree with each processor having a pointer to a unique node in the tree no hint this is the one problem assigned this term that i don t know how to do this was from an old text that we used to use i seem to recall that the text said the problem was easy but i worked on it for at least minutes and couldn t solve it so decided to assign it as homework several years ago no one has solved it yet a o log n algorithm for finding the logical and of n bits with n pro cessors on an erew pram see solution for the next problem omit the step where each proces sor sequentially computes the and of log n bits the efficiency with n processors is e n n n n log n log n o log n algorithm for finding the logical and of n bits with n log n processors on an erew pram each processor pi sequentially computes the and of log n bits and stores them in n log n variables the and of these n log n results are computed in parallel as follows each processor pi i n log n computes ri and ri and stores the result in r i by repeatedly and ing the result bits we halve the number of bits with each recursive activation after log n log n ac tivations we obtain the result the total time is t n log n log n log n  log n the best sequential algorithm is linear so e n n log n n n log n logn o algorithm for finding the logical and of n bits with n processors on a crcw pram assume we have a shared variable result which is initialized to true algorithm for each processor pi read ni if ni result false this clearly takes constant time since the best known sequential algorithm is linear e n n n n find a parallel algorithm a to solve the boolean formula value problem that a lacks only code for problem n and b runs in poly log time with a polynomial number of processors then if you could find code for problem n that ran in poly log time with a polynomial number of processors this would then immediately give you code for the boolean formula value problem that runs in poly log time with a polynomial number of processors since polynomial and poly log functions are closed under addition multiplication and composition so find good parallel code for n is at least as hard as finding good parallel code for the boolean formula value problem a o log n algorithm for indexing of an n cell array with n processors on an erew pram similar to the following solution except we have a processor for each array entry the efficiency is e n n n n log n log n o log n algorithm for indexing of an n cell array with n log n pro cessors on an erew pram assume that you have a processor pi for array entry of an array b b n log n at step b is initialized to x at step i i log n pi writes b i to locations b and b then pi copies b i to locations a i log n to a i log n the best known algorithm takes linear time so e n n log n n n log n log n here is crcw code for filling an array code for pi i n read x a i x here is how to solve the parallel prefix problem in log n time with n log n processors on an erew machine each processor pi is given a contiguous log n subarray x i log n xi log n each pi computes the sum si of its log n sized subarray this takes log n time then erew parallel prefix algorithm from the notes is then applied on sn log n note that the number of processors is equal to the size of the s list hence in time log n you have the parallel prefixes of the s list then pi can calculate the i log n j th prefix j log n as the i prefix of the s list plus x i log n x i log n j since multiplication is associative you can use the erew algorithm from the notes for associative operators to solve this problem in o log n time with n log n processors assume that we are trying to compute the n by n matrix c that is the product of n by n matrices a and b compute each of the entries in the resultant matrix in linear time using the standard formula ci j n ai kbk j use n processors per entry ci j in constant time compute the n products ai kbk j k n then in log n time using the di vide algorithm to evaluate an associate operator compute the sum n k ai kbk j o log n algorithm for multiplying two n n matrices with log n processors on a crew pram assume that we are trying to compute ab c each of the entries of c are computed in parallel using n log n processors to compute ci j each processor computes b ai kbk j for a sub range a b of n of length log n these sums are then added in time log n using the tree addition algorithm assuming falsely that the best known algorithm runs in time o the efficiency is e n log n log n log n before running the algorithm in the previous subproblem use n log n processors per entry in a and b to make n copies of each of these entries in o log n time then when each reach to a and b in the al gorithm from the previous subproblem can reach a different memory location a log n time algorithm for evaluating a polynomial p x n aixi with n log n processors on a erew machine in time o log n find k kn using parallel prefix algorithm we can make n copies of k using problem in constant time compute k an kn in time o log n compute p k n aiki using tree algorithm note plus is associative by expanding the recurrence relation fn l ln l fn as matrix multiplication of matrices is associative and takes constant time all of the fibonacci numbers can be computed using parallel prefix solution courtesy of daniel cole we give an erew algorithm for finding the longest prefix that is equal to a suffix in poly log time using a polyno mial number of processors we ll assume that we have processors and we ll run in about phases note we don t try k n each letter is assigned n processors and we run our copy alogrithm from previous homeworks double then number of copied items at each of the steps to get n copies of each item time o log n note that there are n possible answers to this problem so we as sign one of these possible k values to each of the n copies further each copy is assigned a set of n processors for each set of proces sors they have some k value and each processor pi in this set checks if c i is equal to c n k i for i k some processors don t do anything this is checking if a unique character of the prefix is equal to the corresponding unique character of the suffix because threre is overlap here we can just have the n processors make a copy of the input and then we re fine because each character can only appear once in the prefix and once in the suffix in any case each pi now has a or bit based on whether or not their chracter matched note that each set of n processors does all this in constant time thus for eack k we need only to know if all the bits are if so then the prefix and suffix match and that k is valid if not the max in this phase we run our algorithm for anding bits together thus each group of n processors each with a unique k and their bits in the naive way which takes o log n time and at the end we have a or a for each k value the last phase is to find the max k value we only need n processors here and assume that for each for a k value in the last phase instead we just have k and a for a thus we are simply trying to find the max value again we just do a simple naive paralllel max finding algorithm where each processor passes on the max of it two assigned numbers and at the next level those numbers are used by half the processors this will run in o log n time with n processors and produce the maximum k value for the problem as we can see each phase ran in o log n time assuming processors and having a constant number of steps our total time is still o log n solution courtesy of daniel cole we give an crcw algorithm for find ing the longest prefix that is equal to a suffix in constant time using a polynomial number of processors in this problem we just modify the pre vious algorithm again assuming processors note we don t try k n as previously firstly we do not need the first phase as we can do con current read thus in the second phase we do the exact same thing just that every group uses the same n characters this takes constant time and again we are left with n bits for each group of n processors at this point we can run our constant time crcw and algorithm that has each processor simply write a to the desired output and then if a processor bit is it writes a to that output otherwise it does nothing thus the answer will be if and only if no processor has a bit each set of n runs this algorithm in constant time and thus we have for each k whether or not it is feasible now we simply need to find the maximum k again here we assume k instead of which is a constant time update now to find the max k we have n processors assigned to each k value and within each group a processor will first write a k to the common output and will then look at its group k and one other k unique for each processor in a particular k group if its k is bigger it will do nothing else it will write a to the common output for that k group thus at this step each k that was will still be but each k that is not will have been compared to every other k and if a single other k was larger then this k will in effect be now thus only one k will have a value and the rest will be this step takes constant time as we assigned each k value n processors in the final phase we will simply find the k value that is not as we have established that all of them are except the largest whose n group of processors did not find any k larger to do this we assign processor to each of these locations of potential maximum k values and each processor first writes a to the common output and then checks their k if it is not they write it to the common out put this will only be true for one processor thus the common output will contain the largest k this step was done in constant time as well each step of our process each n group tests a k value anding results in each n group clear all k except the max and find the non zero k left over run in constant time thus our algorithm is a constant number of steps of constant time each for a total runtime that is constant on processors o log n algorithm for adding two n bit integers on an crew pram generalize the inductive hypothesis to compute the sum both assuming no carry and assuming a carry of assume that we want to add integers a and b and a and b now recursively solve the problems of adding and and of adding and depending on whether or not there is a carry from adding and one can compute the final sum in constant time once the number of bits is less than log n we can add in log time with one processor this running time is time o log n using n log n processors we explain how to convert the crew shortest path algorithm into an erew algorithm initially make n copies of the n n matrix holding the edge distances this takes o log n time with processors since the distance matrix has entries as only n processors read an edge distance in any one step this means that concurrent reading is no longer necessary so we have an erew algorithm note that within each iteration of the log n rounds of distance updates you must again do this o log n time copying procedure so that none of the distance info being read from the copies of the distance matrix becomes stale and note that the entire algorithm still runs in o n time solution courtesy of daniel cole here we explain how to have the short est path algorithm actually compute the shortest paths here the idea will be to save a bit of extra data during the main algorithm and then rebuild all the paths at the end it doesn t seem like we can build the paths as we go because although we know after each iteration that we have completed the shortests paths of length up to in length we don t know which paths these are necessarily thus we have to wait until the end for extra data we will keep the midpoint used to create the shortest path i e the common point of the two shortest paths that we used to create the new shortest path we will also keep number of hops in the path we can do both of these items in our original algorithm without adding any time as they take an addative constant longer than just keep ing track of the length to see this note that when we are doing the min of a set of combined paths for some i j path we use n midpoints each with a unique value k when we do this min operation we can just keep the winner k value as well as his length to keep the hop length we can see that it can easily be created initially in the first round as for any paths that are created and thereafter we can just calculated it as the hop length of i k plus that of k j for the winning sub paths thus from now on we will assume that in addition to having shortest path length in our two dimensional array we also have the k value used to create the path as well as the number of hops on the path after our erew algorithm from the last question modified according to our requirements finishes we can commence constructing the paths them selves we essentially follow the same procedure as above except that we know the optimal paths and we simply need to construct them the idea thus is again to run for log n iterations guarenteeing that after each iter ation we complete all paths of length we have two phases the path output phase and the path copy phase the output outputs the completed paths the copy phase copies these paths so that they may be used in subsequent rounds output phase each entry i j in the table is assigned n processors and in iteration p any entry with hop count to will be calculated this will be done by copying unique copies of the previously finished paths i k and k j to the proper output location for i j we know that these paths were finished in the last round thus they are available the copy of these paths takes at most log n time per path as each path can be at most n in length and we can copy that in log n time with n processors which is exactly what we have per entry in the next phase we will explain the unique copies of i k and k j copy phase at this point we have created all paths of hop count to hop count we just need to make these paths available to the next iteration note that for a path i j that it can only be used in the next iteration as either i k j or k i j because k is fixed in both cases the other half of the path can only be one of n paths meaning our current i j path can be used at most times so we d like copies so that everyone can have their own unique copy in phase of the next iteration now we must consider that we want to make copies of an unknown number of paths that can be up to in length but consider that in each round we double the number of paths that are finished and even if we have the max possible paths all of length then we still only need copies of paths of length which just gives that we need order n copies of data order by assigning processors for each final copy of the data we can make all our copies in log n time by our standard doubling of data each round further we can identify these path copies by having one copy for all possible i and one copy for all possible j so that in phase one the processors will know where to look use the same method as used for the minimum edit distance problem except with different weights on the arcs e g on horizontal vertical arcs and on diagonal arcs solution courtesty of daniel cole here we just create a graph and then run the all pairs shortest path problem from class followed by a step to extract the minimum edit distance first we construct the graph consider an n by n matrix m where each entry is a vertex and i j has b j as the column and a i as the row we ll assign a single processor to each vertex to create the edges going out of that vertex processor pi j will look at a i and b j if they are equal then it will create an arc from m i j to m i j with cost if they are not equal it will create arcs m i j to m i j with cost or more generally cost opdelete m i j to m i j with cost or more generally cost opinsert m i j to m i j with cost or more generally cost opreplace notice that we need an extra row and an extra column which we place in the n position for both i e we have an extra n row and n column these are needed because comparing a n with b i or a i with b n would cause edges to go outside our n by n matrix but consider what this means it means that we have either removed or replaced all letters in a n row and are left with some of b left or that we have covered all of b n column through insertions or replacements and are left with some of a left if the first case we just need to insert the rest of b thus we simply add edges from m n j to m n j with cost cost opinsert in the second case we just need to remove the rest of a thus we add edges from m j n to m j n with cost cost opdelete we can assign a single processor to each of these entries as well conceptually we think of the mimimum edit distance problem as starting at m and making decisions on conversion for the first entry the optimal either removes a in which case a is now being compared to b hence we go down likewise for inserting or replacing once we ve reached m n n all letters in b have been matched either through insert or replace and we ve made decisions about ever letter in a either through delete or replace and that the key replace handles both a letter in b and a while insert and delete only handle a letter in one string thus once we run all pairs shortest path we need to simply find the minimum distance between point m and m n n the construction of this graph takes constant time on a crew machine with processors as each processor is assigned a single vertex and creates arcs out of that vertex if there is a concurrent write issue with where the arcs are stored then we can just do it in two shifts such that there is a gap between the head and tail of any arc this will still take a constant amount of time in either case we have just constructed a graph such that the shortest path distance from to n n is in fact the minimum edit distance thus we can run the all pairs shortest path algorithm using processors and get the result in o time using our in class algorithm for all pairs shortest path then we can look up the shortest path distance from node to n n in constant time thus our run time is bound by the all pairs shortest path computation making our run time o n with o processors solution courtesy of daniel cole here we would like a crcw algorithm for merging that runs in constant time assume we have two sorted arrays of length n namely xn and yn we give an algorithm for placing yi correctly in its final location in the output array in constant time we use n processors and each processor is assigned an xj value we have an array ai n that we initialize to all zeros in constant time each processor compares xj and yi and if yi xj then set ai j else ai j this takes constant time ai now has a for every xj that is larger than yi and a for all smaller thus all values represent x values that are less than or equal to yi each processor pj now looks at j and j and only one processor will see that ai j and ai j as the xj values are in sorted order thus the final postion of yi is in index i j index from of the output array because we know i y values are smaller than yi and j x values are smaller than yi because only a single processor knows this it can safely write yi to the correct output location creating and populating ai with takes constant time and finding the largest xj that is smaller than yi the largest indexed took constant time thus we have placed yi in constant time thus we can simply assign n processors to every x and y value to place each value in the output array in parallel in constant time note that we should write a for x y instead of x y to give the y values precedence over x values and avoid concurrent writes and empty entries in the output we thus need a total of o processors and our run time is o solution courtesy of daniel cole here we give a parallel algorithm that finds the max of n numbers in o log log n time with n processors on a crcw pram here we simply break our problem up into n subprob lems of n size each and recursively call in parallel our max function on each of these chunks with n processors each we receive a single max value from each of these n calls and because we have n proces sors we can solve the max problem for n values in constant time with n processors using the method from class i e we have processors equal to the number of values squared thus our recurrence relation is t n n t n n c where c is the constant time max parallel algorithm on crcw to solve this we just guess t n n c log log n to get t n n c log log n c c log log n c c log log log n c c log c log log n c c c log log n c c log log n which is what we guessed thus we have a running time of o log log n here we give a parallel algorithm that finds the max of n numbers all of which are in the range to n in constant time on a crcw priority assume we have an array a of size n and denote a processor integer identifier as pi i where i n given n processors on a crcw priority machine we give the following code for processor pi to find the maximum of numbers xn in the range to n a a if a n answer a n not yet turned in no solution given parallel and distributed computation parallel computation tightly coupled processors that can communicate al most as quickly as perform a computation distributed computation loosely couple processor for which communication is much slower than computation pram model a pram machine consists of m synchronous processors with shared memory this model ignores synchronization problems and communication issues and concentrates on the task of parallelization of the problem one gets various variations of this model depending on how various processors are permitted to access the same memory location at the same time er exclusive read only one processor can read a location in any step cr concurrent read any number of processors can read a location in a step ew exclusive write only one processor can write a location in any step cw concurrent write any number of processors can write a location in a step what it processors try to write different values common all processors must be trying to write the same value arbitrary an arbitrary processor succeeds in the case of a write conflict priority the lowest number processor succeeds the right model is probably an erew pram but we will study other models as academic exercises we will sometimes refer to algorithms by the type of model that these algorithms are designed for e g an erew pram algorithm we define t n p to be parallel time for the algorithm under consider ation with p processors on input of size n let s n be the sequential time complexity then the efficiency of a parallel algorithm is defined by e n p s n mt n p efficiencies can range from to the best possible efficiency you can have is generally we prefer algorithm whose efficiency is  the folding principle can be stated in two equivalent ways time based definition for k it must be the case that t n p kt n kp that is increasing the number of processors by a factor of k reduces the time by at most a factor of k or equivalently reducing the number of processors by a factor of k increases time by at most a factor of k efficiency based definition for k e n p e n kp that is more processors can not increase efficiency and there is no loss of effi ciency if you decrease the number of processors the or problem input bits bn output the logical or of the bits one can obtain an erew algorithm with t n p n p log p using a divide and conquer algorithm that is perhaps best understood as a binary tree the p leaves of the binary tree are n p bits each internal node is a processor that or the output of its children the efficiency of the erew algorithm is e n p s n pt n p n p n p log p n p p log p which is  if p o n log n one can also obtain a crcw common algorithm with t n n  in this algorithm each processor pi sets a variable answer to then if bi pi sets answer to the efficiency of this algorithm is e n n  min problem see section and section input integers xn output the smallest xi the results are essentially the same as for the or problem there is an erew divide and conquer algorithm with e n n log n  note that this technique works for any associative operator both or and min are associative there is an crcw common algorithm with t n p and e n p n here code for processor pi j i j j for the crcw common algorithm to compute the location of the minimum num ber if x i x j then t i j else t i j and i if t i j then and i if and i then answer i what happens when the above code is run in the various cw models if there are two smallest numbers what happens in the various cw models if there are two smallest num bers and you just want to compute the value of the smallest number that is if the last line is changed to if and i then answer x i parallel prefix problem input integers xn output sn where si j xj we give a divide and conquer algorithm solve the problem for the even xi and odd xi separately then and x2i this gives an algorithm with time t n n log n on erew pram this can be improved to t n n log n log n thus giving  efficiency note that divide and conquer into the first half and last half is more difficult because of the sum for the first half becomes a bottleneck that all of the last half of the processors want to access also note that this technique works for any associative operator all pairs shortest path problem input a directed edge weighted graph g with no negative cycles output the length of the shortest path d i j between any pair of points i and j first consider the following sequential code for i to n do for j to n do d i j weight of edge i j repeat log n times for i to n do for j to n do for m to n do d i j min d i j d i m d m j the correctness of this procedure can be seen using the following loop invariant after t times through the repeat loop for all i and j if the length of the shortest path between vertices i and j that has or less edges is equal to d i j a parallel for loop is a loop where all operations can be executed in parallel for example for i to n do c i a i b i question so which loops can be replaced by parallel for loops answer the second and third this gives t n p n log n on a crew pram the fourth loop could be replaced by a parallel for loop on a machine with concurrent write machine that always writes the smallest value but note that the fourth loop is just computing a minimum which is an associa tive operator thus using the standard algorithm the compute the value of an associative operator in time log n with n log n processors we get time t n log n log n on an crew pram question what is the efficiency answer it depends what you use for s n s n t n this measures speed up of the parallel algorithm but doesn t give speed up over standard sequential algorithm s n best achievable sequential time but for almost all problems the best achievable sequential time is not known s n sequential time of standard or best known sequential algo rithm but this has the odd property that the efficiency of a parallel algorithm can change when a new sequential algorithm is discovered note that there are simple sequential shortest path algorithms that run in time o and complicated ones that run in time something like o odd even merging see section input sorted lists xn and yn output the two lists merged into one sorted list we give the following divide and conquer algorithm merge merge to get merge to get for i to n do min max this can be implemented on an erew pram to run in time t n n log n thus giving efficiency  log n the following argument establishes the correctness of the algorithm each ai is greater than or equal ai each ai i is larger than bi hence ai each bi is greater than or equal bi each bi i is larger than ai hence bi this same argument shows ai and bi so and must be ai and bi odd even merge sorting see section we give the following divide and conquer algorithm sort merge sort sort this can be implemented on an erew pram to run in time t n n n thus giving efficiency n log n  log n there is a erew sorting algorithm with constant efficiency but it is a bit complicated pointer doubling problem in the pointer doubling problem each of the n processors is given a pointer to a unique element of a singly linked list of n items the goal is for each processor to learn its location in the linked list e g the the processor with the element in the list should know that it is in the list for i to n in parallel do d i d n repeat log n times for each item i in parallel if next i nil then d i d i d next i next i next next i the correctness of the code follows from the following loop invariant the position of i equals d i d next i d next next i note that this is essentially solving the parallel prefix problem with the work done before the recursion instead of after to solve the parallel prefix problem we would replace the initialization for i to n in parallel do d i by for i to n in parallel do d i x i eulerian tour technique to find tree depths the input is a binary tree with one processor per node assume that each processor knows the location of one node the problem is to compute the depth of each node in the tree we show by example how to reduce this to pointer doubling from the following tree a b c d e we create the list the first line a b d b e b a c a and call pointer doubling with d i initialized to either or appropriately the depth of a node is then computed by looking at the sum up to the point shown in the second line expression evaluation the input is an algebraic expression in the form of a binary tree with the leaves being the elements and the internal nodes being the algebraic oper ations the goal is to compute the value of the expression some obvious approaches won t work are evaluate nodes when both values of children are known and parallel prefix the first approach won t give you a speed up if the tree is unbalanced the second approach won t work if the operators are not be associative first assume that the only operation is subtraction we label edges by functions we now define the cut operation if we have a subtree that looks like h x f x g x constant c a b and cut on the root of this subtree we get h f x g c a b if we have a subtree that looks like h x f x g x constant c a b and cut on the root of this subtree we get h f c g x a b thus we are left with finding a class of functions with the base elements being constants that are closed under composition subtraction of constants and subtraction from constants this class is the functions of the form ax b a is or and b can be any number note that in one step we can apply cuts to all nodes with an odd numbered left child that is a leaf note that in one step we can apply cuts to all nodes with an odd numbered right child that is a leaf this leads to the following algorithm repeat log n times for each internal node v in parallel if v has odd numbered left child that is a leaf then cut at v if v has odd numbered right child that is a leaf then cut at v renumber the leaves using pointer doubling note that in log n steps we will down to a constant sized tree since each iteration of the outer loop reduces the number of leaves by one half so t n n n since number the left or right leaves can be done in log n time using the eulerian tour technique a problem that is hard to parallelize no one knows a fast parallel algorithm for the following problem known as the circuit value problem input a boolean circuit f consisting of and and or and not gates and assignment of values to the input lines of the circuit output if the circuit evaluates to be true and otherwise more precisely no one knows of a parallel algorithm that runs in time o logkn for some k with a polynomial number of processors here n is the size of the circuit further this problems is complete for polynomial time sequential algorithms in the sense that if this problem is parallelizable time o logkn for some k with a polynomial number of processors then all problems that have polynomial time sequential algorithms are parallelizable definition of reduction problem a is reducible or more technically turing reducible to problem b denoted a b if there a main program m to solve problem a that lacks only a procedure to solve problem b the program m is called the reduction from a to b note that we use the book notation t and interchangeably a is polynomial time reducible to b if m runs in polynomial time a is linear time reducible to b if m runs in linear time and makes at most a constant number of calls to the procedure for b assume that the reduction runs in time a n exclusive of r n recursive calls to b on an input size of s n then if one plugs in an algorithm for b that runs in time b n one gets an algorithm for a that runs in time a n r n b s n a decision problem is a problem where the output is let a and b be decision problems we say a is many to one reducible to b if there is reduction m from a to b of following special form m contains exactly one call to the procedure for b and this call is the last step in m m returns what the procedure for b returns equivalently a is many to one reducible to b if there exists a computable function f from instances of a to instances of b such that for all instances i of a it is the case that i has output in problem a if and only if f i has output in problem b reductions can be used to both find efficient algorithms for problems and to provide evidence that finding particularly efficient algorithms for some problems will likely be difficult we will mainly be concerned with the later use using reductions to develop algorithms assume that a is some new problem that you would like to develop an algo rithm for and that b is some problem that you already know an algorithm for then showing a b will give you an algorithm for problem a consider the following example let a be the problem of determining whether n numbers are distinct let b be the sorting problem one way to solve the problem a is to first sort the numbers using a call to a black box sorting routine and then in linear time to check whether any two consecutive numbers in the sorted order are equal this give an algorithm for problem a with running time o n plus the time for sorting if one uses an o n log n time algorithm for sorting like mergesort then one obtains an o n log n time algorithm for the element uniqueness problem using reductions to show that a problem is hard absolutely positively you must read the first two paragraphs of chapter this explains the general idea matrix squaring you want to find an o time algorithm to square an n by n matrix the obvious algorithm runs in time  you know that lots of smart computer scientists have tried to find an o time algorithm for multiply two matrices but have not been successful but it is at least conceivable that squaring a matrix multiplying identical matrices might be an easier problem than multiply two arbitrary matrices to show that in fact that matrix squaring is not easier than matrix multiplication we linear time reduce matrix multiplication to matrix squaring theorem if there is an o time algorithm to square an n by n matrix then there is an o time algorithm to multiply two arbitrary n by n matrices proof we show that matrix multiplication is linear time reducible to matrix squaring we exhibit the following reduction program for matrix multipli cation read x and y the two matrices we wish to multiply let call procedure to compute i y x xy xy read xy of from the top left quarter of if you plug in an o b n time algorithm for squaring the running time of the resulting algorithm for squaring is o b thus an o time algorithm for squaring yields an o time algorithm for matrix multiplica tion end proof so the final conclusion is since lots of smart computer scientists have tried to find an o time algorithm for multiply two matrices but have not been successful you probably shouldn t waste a lot of time looking for an o time algorithm to square a matrix convex hull the convex hull problem is defined below input n points pn in the euclidean plane output the smallest either area or perimeter doesn t matter convex polygon that contain all n points the polygon is specified by a linear order ing of its vertices you would like to find an efficient algorithm for the convex hull problem you know that lots of smart computer scientists have tried to find an a linear time algorithm for sorting but have not been successful we want use that fact to show that it will be hard to find a linear time algorithm for the convex hull problem theorem if there is an o n time algorithm for convex hull then there is an o n time algorithm for sorting proof we need to show that sorting convex hull via a linear time reduc tion consider the following reduction algorithm for sorting read xn let pi xi and pn call procedure to compute the convex hull c of the points pn in linear time read the sorted order of the first coordinates off of c by traversing c counter clockwise if you plug in an o b n time algorithm for the convex hull problem the running time of the resulting algorithm for sorting is o n b n thus an o n time algorithm for the convex hull problem yields an o n time algorithm for matrix multiplication end proof so the final conclusion is since lots of smart computer scientists have tried to find an o n time algorithm for sorting but have not been successful you probably shouldn t waste a lot of time looking for an o n time algorithm to solve the convex hull problem np complete np equivalent problems there are a class of problem called np complete problems for our purposes we use np complete and np equivalent interchangeably although there is a technical difference that not really relevant to us the following facts about these np complete are relevant if any np complete has a polynomial time algorithm then they all do another way to think of this is that all pairs of np complete problems are reducible to each other via a reduction that runs in polynomial time there are thousands of known natural np complete problems from every possible area of application that people would like to find poly nomial time algorithms for no one knows of a polynomial time algorithm for any np complete problem no one knows of a proof that no np complete problem can have a polynomial time algorithm almost all computer scientists believe that np complete problems do not have polynomial time algorithms a problem l is np hard if a polynomial time algorithm for l yields a polynomial time algorithm for any all np complete problem or equiva lently if there is an np complete problem c that polynomial time reduces to l a problem l is np easy if a polynomial time algorithm for any np complete problem yields a polynomial time algorithm for l or equivalently if there is an np complete problem c such that l is polynomial time re ducible to c a problem is np equivalent or for us np complete if it is both np hard and np easy fact the decision version of the cnf sat problem determining if a sat isfying assignment exists is np complete self reducibility an optimization problem o is polynomial time self reducible to a decision problem d if there is a polynomial time reduction from o to d almost all natural optimization problems are self reducible hence there is no great loss of generality by considering only decision problems as an example we now that the cnf sat problem is self reducible theorem if there is a polynomial time algorithm for the decision version of cnf sat then there is a polynomial time algorithm to find a satisfying assignment proof consider the following polynomial time reduction from the problem of finding a satisfying assignment to the decision version of the cnf sat problem we consider a formula f one call to the procedure for the decision version of cnf sat will tell whether f is satisfiable or not assuming that f is satisfiable the following procedure will produce a satisfying assignment pick an arbitrary variable x that appears in f create a formula f by removing clauses in f that contain the literal x and removing all occurrences of the literal x call a procedure to see if f is satisfiable if f is satisfiable make x equal to true and recurse on f if f is not satisfiable then create a formula f by removing clauses in f that contain the literal x and removing all occurrences of the literal x make x false and recurse on f end proof cnf sat circuit satisfiability and are polynomial time equivalent the circuit satisfiability problem is defined as follows input a boolean circuit with and or and not gates output if there is some input that causes all of the output lines to be and otherwise the problem is defined as follows input a boolean formula f in cnf with exactly literals per clause output if f is satisfiable otherwise the cnf sat problem is defined as follows input a boolean formula f in conjunctive normal form output if f is satisfiable otherwise we want to show that if one of these problems have a polynomial time al gorithm than they all do obviously is polynomial time reducible to cnf sat a bit less obvious but still easy to see is that cnf sat is polynomial time reducible to circuit satisfiability there is an obvious cir cuit immediately derivable from the formula to complete the proof we will show that cnf sat is polynomial time reducible to and that circuit satisfiability is reducible to cnf sat theorem cnf sat is polynomial time reducible to proof we exhibit the following polynomial time many to one reduction from cnf sat to main program for cnf sat read the formula f create a new formula g by replacing each clause c in f by a collection g c of clauses we get several cases depending on the number of literals in c if c contain one literal x then g c contains the clauses x ac bc x ac bc and x ac bc x ac bc if c contains two literals x and y then g c contains the clauses x y ac and x y ac if c contains literals then g c c if c xk contains k literals then g c contains the clauses and ac a c ac a c ac xk a c k ac k xk xk ac k note that in each case ac bc ac ac k are new variables that appear nowhere else outside of g c clearly g can be computed in polynomial time g has exactly literals per clause furthermore f is satisfiable if and only if g is satisfiable end proof this type of reduction in which each part of the instance is replaced or modified independently is called local replacement theorem circuit satisfiability is polynomial time reducible to cnf sat proof the main issue is that the fan out of the circuit may be high so the obvious reduction won t work the main idea is to introduce a new variable for each line in the circuit and then write a cnf formula that insures consistency in the setting of these variables end proof vertex cover the vertex cover problem is defined as follows input graph g and integer k output if g have a vertex cover of size k or less a vertex cover is a collection s of vertices such that every edge in g is incident to at least one vertex in s theorem vertex cover is np hard proof we show is polynomial time many to one reducible to vertex cover given the formula f in form we create a g and a k as follows for each variable x inf we create two vertices x and x and connect them with an edge then for each clause c we create three vertices and connected by three edges we then put an edge between ci i and the vertex corresponding to the ith literal in c let k the number of variables in f plus twice the number of clauses in f we now claim that g has a vertex cover of size k if and only if f is satisfiable to see this note that a triangle corresponding to a clause requires at least vertices to cover it and and edge between a variable and its negation requires at least vertex subset sum theorem subset sum is np hard proof we show that is polynomial time many to one reducible to subset sum this is by example consider the formula x or y or not z and not x or y or z and x or not y or z we create one number of each variable and two numbers for each clause as follows further the target l is set as shown numbers base representation name l why the dynamic programming algorithm for subset sum is not a polynomial time algorithm recall that the input size is the number of bits required to write down the input and a polynomial time algorithm should run in time bounded by a polynomial in the input size the input is n integers xn and an integer l the inputs size is then which is at least n l n l xi i the running time of the algorithm is o nl note that this time estimate assumed that all arithmetic operations could be performed in constant which may be a bit optimistic for some inputs never the less if n is constant then the input size i  log l and the running time is l  is exponential in the input size a most concrete way to see this is to consider an instance with n and l the input size is about bits or bytes but any algorithm that requires steps is not going to finish in your life time coloring the coloring problem is defined as follows input graph g output if the vertices of g can be colored with colors so that no pair of adjacent vertices are coloring the same and otherwise theorem coloring is np hard proof we show that is polynomial time many to one reducible to coloring once again we do this by example consider formula f x or y or not z and not x or y or z and x or not y or z we create the graph g as follows for each variable x in f we create two vertices x and not x and connect them we then create a triangle with three new vertices true false and c and connect c to each literal vertex so for the example above we would get something like true false what ever color this is we call false vertex c vertex c is connected to each of the literal vertices x not x y not y z not z we then create the gadget subgraph shown in figure for each clause literal literal c true literal figure the clause gadget we claim that this clause gadget can be colored if and only if at least one of literal literal literal is colored true hence it follows that g is colorable if and only if f is satisfiable the final graph g for the above formula is shown below true figure the final graph cs reductions and np hardness homework problems points a square matrix m is lower triangular if each entry above the main diagonal is zero that is each entry mi j with i j is equal to zero show that if there is an o time algorithm for multiplying two n by n lower triangular matrices then there is an o time algorithm for multiplying two arbitrary n by n matrices points show that if there is an o nk k time algorithm for inverting a nonsingular n by n matrix c then there is an o nk time algorithm for multiply two arbitrary n by n matrices a and b for a square matrix a a inverse denoted a is the unique matrix such that aa i where i is the identity matrix with on the main diagonal and everyplace else note that not every square matix has an inverse e g the all zero matrix hint if you want to multiply n by n matrices a and b consider the matrix c of the following form where i is the n n identity matrix i a i b i points show that if there is an o nk k time algorithm for squaring a degree n polynomial then there is an o nk time algorithm for multiplying two degree n polynomials assume that the polynomials are given by their coefficients assume that all coefficients you compute can be fit into one word of computer memory points consider the following variant of the minimum steiner tree problem the input is n points in the plane each point is given by its cartesian coordinates the problem is build a collection of roads between these points so that you can reach any city from any other city and the total length of the roads is minimized the collection of roads should be output as an adjacency list structure that is for each point p of intersection of roads there are a list of roads that meet that point roads must terminate when they meet one of the original input points or when they meet another road for example if the points are the output would be from the point there are roads to the points and from the point there are roads to and from the point there is a road to from the point there is a road to from the point there is a road to roads from to and from to would not be allowed because they cross a road from to would not be allowed since it would cross the point show by reduction that if you can solve this problem in linear time then you can sort n numbers in linear time points show that if one of the following three problems has a polynomial time algorithm then they all do a combinatorial circuit is one that does not contain directed cycles the problem is to determine whether a boolean combinatorial circuit with gates not binary and and binary or has some input that causes all of the output lines to be assume that the fan out the number of gates that the output of a single gate can be fed into of the gates in a circuit may be arbitrary the problem is to determine whether a boolean combinatorial circuit with gates not arbitrary fan in and and arbitrary fan in or has some input that causes all of the output lines to be assume that the fan out the number of gates that the output of a single gate can be fed into of the gates in a circuit may be arbitrary determine whether a boolean formula with not binary and and binary or operations is satisfiable make sure to state what reductions you show even if some of the reductions are trivial hint you need to show how to simulate unbounded fan in and and or gates with bounded fan in and and or gates you need to show how to transform a boolean formula into a circuit and you need to show how to transform a circuit into a boolean formula this is probably the trickiest part due to the unbounded fan out points show that if one of the following three problems has a polynomial time algorithm then they all do the input is two undirected graphs g and h the problem is to determine if the graphs are isomorphic the input is two directed graphs g and h the problem is to determine if the graphs are isomorphic the input is two undirected graphs g and h and an integer k the problem is to determine if the graphs are isomorphic and all the vertices in each graph have degree k intuitively two graphs are isomorphic if on can name label the vertices so that the graphs are identical more formally two undirected graphs g and h are isomorphic if there is a bijection f from the vertices of g to the vertices of h such that v w is an edge in g if and only if f v f w is an edge in h more formally two directed graphs g and h are isomorphic if there is a bijection f from the vertices of g to the vertices of h such that v w is a directed edge in g if and only if f v f w is a directed edge in h the degree of a vertex is the number of edges incident to that vertex note there is no known polynomial time algorithm for the graph isomorphism problem for reasons that are too complicated to go into here it is unlikely that the graph isomorphism problem is np hard but there is no known proof of this hint it is straight forward to solve undirected graph isomorphism using a routine for directed graph isomorphism the other direction is not so straight forward your reductions here probably should introduce some additional pieces to the graphs so the only possible isomorphisms are of the type that you seek in the original problem points show that the subset sum problem is self reducible the decision problem is to take a collection of positive integers xn and an integer l and decide if there is a subset of the xi that sum to l the optimization problem asks you to return the actual subset if it exits so you must show that if the decision problem has a polynomial time algorithm then the optimization problem also has a polynomial time algorithm points the input to the hamiltonian cycle problem is an undirected graph g the problem is to find a hamiltonian cycle if one exists a hamiltonian cycle is a simple cycle that spans g show that the hamiltonian cycle problem is self reducible that it show that if there is a polynomial time algorithm that determines whether a graph has a hamiltonian cycle then there is a polynomial time algorithm to find hamiltonian cycles points show that the clique problem is self reducible the decision problem is to take a graph g and an integer k and decide if g has a clique of size k or not the optimization problem takes a graph g and returns a largest clique in g so you must show that if the decision problem has a polynomial time algorithm then the optimization problem also has a polynomial time algorithm recall that a clique is a collection of mutually adjacent vertices points show that the vertex cover problem is self reducible the decision problem is to take a graph g and an integer k and decide if g has a vertex cover of size k or not the optimization problem takes a graph g and returns a smallest vertex cover in g so you must show that if the decision problem has a polynomial time algorithm then the optimization problem also has a polynomial time algorithm recall that a vertex cover is a collection s of vertices with the property that every edge is incident to a vertex in s points consider the following problem input a undirected graph g and an integer k output if g has two vertex disjoint cliques of size k and otherwise show that this problem is n p hard use the fact that the clique problem in n p complete the input to the clique problem is an undirected graph h and an integer j the output should be if h contains a clique of size j and otherwise note that a clique is a mutually adjacent collection of vertices two cliques are disjoint if they do not share any vertices in common points for each of the following problems either prove that it is np hard by reduction from either the standard clique problem or from the standard independent set problem or from one of the previous subproblems or give a polynomial time algorithm some of the reductions will be trivial and its ok to dispose of these problems with a sentence or two explaining why the reduction is easy the input is an undirected graph g let n be the number of vertices in g the problem is to determine if g contains a clique of size recall that a clique is a collection of mutually adjacent vertices the input is an undirected graph g let n be the number of vertices in g the problem is to determine if g contains an independent set of size recall that an independent set is a collection of mutually nonadjacent vertices the input is an undirected graph g and an integer k the problem is to determine if g contains a clique of size k and an independent set of size k the input is an undirected graph g and an integer k the problem is to determine if g contains a clique of size k or an independent set of size k the input is an undirected graph g let n be the number of vertices in g the problem is to determine if g contains a clique of size and an independent set of size the input is an undirected graph g let n be the number of vertices in g the problem is to determine if g contains a clique of size or an independent set of size points consider the problem where the input is a collection of linear inequalities for example the input might look like x x y the problem is to determine if there is an integer solution that simultaneously satisfies all the inequal ities show that this problem is np hard using the fact that it is np hard to determine if a boolean formula in conjunctive normal form is satisfiable points the input to the three coloring problem is a graph g and the problem is to decide whether the vertices of g can be colored with three colors such that no pair of adjacent vertices are colored the same color the input to the four coloring problem is a graph g and the problem is to decide whether the vertices of g can be colored with four colors such that no pair of adjacent vertices are colored the same color show by reduction that if the four coloring problem has a polynomial time algorithm then so does the three coloring problem points show by reduction that if the decision version of the sat cnf problem has a polynomial time algorithm then the decision version of the coloring problem has a polynomial time algorithm hint given a graph you need to write a boolean formula in conjunctive normal form that expresses the fact that the graph is colorable points in the dominating set problem the input is an undirected graph g the problem is to find the smallest dominating set in g a dominating set is a collection s of vertices with the property that every vertex v in g is either in s or there is an edge between a vertex in s and v show that the dominating set problem is n p hard using a reduction from the vertex cover problem hint local replacement will work points the input to the fixed hamiltonian path problem is an undirected graph g and two different vertices x and y in g the problem is to determine if there is a simple path between x and y in g that spans all the vertices in g a path is simple if it doesn t include any vertex more than once show that if the fixed hamiltonian path problem has a polynomial time algorithm then the hamiltonian cycle problem has a polynomial time algorithm recall that the hamiltonian cycle problem is to determine whether a graph has a simple cycle that spans the vertices hint i think it is easier to do this problem if you don t restrict yourself to one procedure call to the path problem that is feel free to call the path procedure multiple times points extra credit if your cycle code only makes one procedure call to the path code points consider the following problem the input is a graph g v e a subset r of vertices of g and a positive integer k the problem is to determine if there is a subset u of v such that all the vertices in r are contained in u and the number of vertices in u is at most k and for every pair of vertices x and y in r one can walk from x to y in g only traversing vertices that are in u show that this problem is np hard using a reduction from vertex cover recall that the input for the vertex cover problem is a graph h and an integer f and the problem is to determine whether h has a vertex cover of size f or not a vertex cover s is a collection of vertices with the property that every edge is incident on at least one vertex in s hint let h f be the graph and integer pair instance for vertex cover have r consist of a single source vertex plus one vertex for each edge in h now have one more vertex in g but not in r for each vertex in h now add edges to g so that you can connect the vertices in r using few additional vertices if and only if h has a vertex cover of size f points in the disjoint paths problem the input is a directed graph g and pairs sk tk of vertices the problem is to determine if there exist a collection of vertex disjoint paths between the pairs of vertices from each si to each ti show that this problem is n p hard by a reduction from the problem note that this problem is not easy hint construct one pair si ti for each variable xi in your formula f intuitively there will be two possible paths between si and ti depending on whether xi is true or false there will be a component subgraph dj of g for each clause cj in f there will be three possible paths between the si ti pairs for each dj you want that it is possible to route any two of these paths but not all three through dj points the input to the triangle problem is a subset w of the cartesian product x y z of sets x y and z each of cardinality n the problem is to determine if there is a subset u of w such that every element of x is in exactly one element of u every element of y is in exactly one element of u and every element of z is in exactly one element of u here a story version of the same problem you have disjoint collections of n pilots n copilots and n flight engineers for each possible triple of pilot copilot and flight engineer you know if these three people are compatible or not you goal is to determine if you can assign these people to n flights so that every flight has one pilot one copilot and one flight engineer that are compatible show that this problem is np hard using a reduction from hint consider a cyclic collection of an even number of triangles where consecutive triangles in this cycle share a single common element these shared common elements are alternately x and y so to cover all these x and y elements you either need to pick all the odd triangles in the cycle or all the even triangles in the cycle now as a warmup assume that you are reducing from the problem of deciding where there is a truth assignment that makes exactly one literal per clause true and that you know that the number of occurrences of each literal x is equal to the number of occurrences of the literal not x once you see this you can now try to figure out how to modify this to fix the issues that you can have more than one literal per clause being true and the number of occurrences of a literal and its negation may not be the same points we consider the following medical testing problem the input consists a set a of possible diseases a collection tk of medical tests with binary outcomes each medical test is a function from a to if test ti is applied to someone with a disease a a then the medical test returns the outcome ti a which is either or think of test as being able to distinguish between a diseases a and b where the tests outcomes are different so for example one test could be to determine if a patient has an elevated temperature such a test could differentiate between a disease where an elevated temperature is a symptom and a disease where an elevated temperature is not a symptom integer j it is only interesting if j k the dr cuddy problem is to determine whether there is a collection s of j tests such that for all diseases a and b there is a test in s that can distinguish between a and b show that the dr cuddy problem is n p hard by reduction from the triangle problem defined in the previous problem hint local replacement will work points we consider a generalization of the fox goose and bag of beans puzzle wikipedia org wiki fox the input is a graph g an integer k the vertices of g are objects that the farmer has to transport over the river there are an edge between two objects if they can not be left alone together on the same size of the river the goal is to determine if a boat of size k is sufficient to safely transport the objects across the river the size of the boat is the number of objects that the farmer can haul in the boat show that this problem is np hard using a reduction from one of the problems that either i showed was np hard in class or that you showed was np hard in the homework so i am letting you pick the problem to reduce from here you should take some time to reflect which problem would be easiest to reduce from points prove that the following problem is np hard by reduction from the input consists of a finite set s and a collection c of subsets of s the problem is to determine if there is a partition of s into two subsets and such that no set d c is entirely contained in either or hint you will surely need to do a component design problem to reduce the problem of multiplying n n matices a and b to the problem of multiplying two lower triangular matrices c and d simply create a matrix c d b a then we have the matrix ab can easily be obtained by isolating the lower left corner of the product matrix problem to reduce the problem of multiplying n n matices to the problem of inverting a single matrix construct a matrix c of the following form i a i b i where i is the n n identity matrix by inverting c we get i a i b i i a ab i b i ab can be easily obtained by extracting the upper right corner of c problem to reduce the problem of sorting n numbers to the minimum steiner tree problem take the numbers xn to sort and create the points xn in the plane the minimum steiner tree of these points is the tree that contains xn in order problem show that polymult polysquare using the identity a b we have ab a b procedure polymult read a and b x polysquare a b y polysquare a z polysquare b ab x y z the time for procedure polymult is the time for procedure polysquare plus o n problem no solution given problem to reduce the clique problem to the problem create a new graph g by adding a k vertex clique to g do not connect any of the vertices in the new clique to any of the vetices of the original graph call the procedure for on g a will be returned if and only if g has a clique of size k problem no solution given problem to reduce the problem of finding a hamiltonian cycle in a graph g to the problem of determining whether one exists create graph g by removing an arbitrary edge x y call the ham decision algorithm on both g and g if there is a hamiltonian cycle in both recurse on g if there is a hamiltonian cycle in g but not g then we know x y is in the hamiltonian cycle recurse on g taking care not to select edge x y again problem this solution was adapted from miguel dickson and eric wiegandt homework writeup we will prove that determining if graph g contains a clique of size where n is the number of vertices in g is np hard we will show that clique clique the input to the clique problem is graph h and integer k we need to transform these inputs into the input for clique a single graph g there are three cases in which p will represent the number of vertices in h k this case is trivial h requires no transformation and the output to the problem is the same as the output to the clique problem k to transform h into a suitable input for the clique problem add vertices to h until k these vertices have no edges connected to them so they cannot be part of a clique thus there will be a clique of size k in this new graph if and only if there was a clique of size k in h k this is the most complicated case we cannot take vertices out of h without potentially changing an existing clique so we must add vertices and edges to h we will add x vertices to h connecting each added vertex to every vertex already in h these vertices will form a clique of size x and can also be part of any ex isting clique as every vertex added is mutually adjacent to all other vertices added and all vertices in the existing clique in our new graph there will be a clique of size x k if and only if there was a clique of size k in h we will use the following equation to define x p x k x which simplifies to x program transform graph h int k int p of vertices in h if k return h else if k int x k p g h with x vertices and no edges added return g else if k int x g h with x vertices added and edges connecting each new vertex to all other vertices return g the code for the clique problem is now simple program clique graph h int k graph g transform h k return clique g the proof that the problem of determining if graph g with n vertices has an independent set of size is very similar to the proof in part a we will show that independent set independent set graph h and integer k are the inputs to the independent set problem let p be the number of vertices in h there are three cases to trans forming h and k into an input for the independent set problem k no transformation is needed the output to the p independent set problem with input h is the same as the output to the independent set problem with input h and k k we want to add vertices to h until k we do not want these vertices to be part of any independent set we need to add k p vertices to h along with edges connecting each of the new vertices to every other vertex this will ensure that no new vertex can be part of an independent set and this new graph will have an independent set of size k if and only if h has an independent set of size k k we need to add vertices to h so that the indepen dent set problem will output true if and only if h has an indpendent set of size k let x add x vertices to h and no edges in this new graph there will be an independent set of size x these new vertices can be part of any independent set as they are not mutually adjacent to any other vertex thus the new graph will have an in dependent set of size x k if and only if h has an independent set of size k the clique problem will output if the new graph has an independent set of size x k as this problem will ouput whether or not the graph has an independent set of size as p x k x program transform graph h int k int p of vertices in h if k return h else if k int x k p g h with x vertices added and edges connecting each new vertex to all other vertices return g else if k int x g h with x vertices and no edges added return g program independentset graph h int k graph g transform h k return independentset g if k num vertices g then g cannot contain a clique of size k and an independent set of size k since by the pigeonhole principle some of the required vertices to be in the clique have to be included in the independent set which proves they cannot be in the clique and vice versa for k num vertices g we show that this problem is np hard to do this we will show that clique clique and independent set the input to the clique problem is a graph h and an integer j to transform h into the input for the and problem simply add j vertices and no edges to the graph which we will now call g g will always have an independent set of size j as each of the j vertices we added have no edges and can be part of the set g will have a clique if and only if h has a clique as we did not add or remove edges by making one of the conjuncts true calling the and code will return the truth value of the other conjunct whether h has a clique of size j program clique graph h int j g transform g return and g j determining whether g contains a clique of size k or an indepen dent set of size k is np hard we demonstrate this by reduction from the independent set problem that is we show that independent set independent set size k or clique size k either take the input g and run either on it if either returns return for the independent set problem if either returns then construct a new graph h by adding totally disconnected vertices to the graph of g equal to the number of vertices in g call that number n clearly h cannot have a clique of size k n since it cannot have a clique size greater than n since even if k n none of the new points added to h can be part of a clique so run either k n on graph h if either returns then the original graph g had to have an inde pendent set of size k since either result cannot be from a n k size clique it has to be from a n k size independent set of which n points are the newly added disconnected points and the other k are an idnependent set in graph g if either k n returns then the original graph did not have an independent set of size k and thus we have shown that independent set either there is a polynomial time algorithm for determining if graph g with n vertices contains a clique of size and an independent set of size if n it is impossible for g to have both a clique of size and an independent set of size assume that there is a graph g with n and a clique of size and an independent set of size in g there must be at least two vertices that are both in the clique and the independent set these vertices must be connected as they are in the clique these vertices cannot be con nected as the are in the independent set this is a contradiction and our assumption that there is such a graph is false there are cases left to consider n n and n n in this case trivially there is a clique of size and an independent set of size always return true n trivially there is a clique of size and an in dependent set of size always return true n there are possible edges in a graph of size if there are edges there is no clique of size with edge there will be a clique of size the two connected vertices and an independent set of size of size one of the edge endpoint and the vertex not connected to the edge with edges there is a clique of size any connected vertices and an independet set of size the two vertices without an edge connecting them with edges there is not independent set of size as all vertices are connected program and graph g n of vertices in g if n return true else if n return true else if n e edges in g if e or e return false else if e or e return true else return false this problem is np hard because we can reduce independent set to this problem e g show that independent set independent set size or clique size again call this either the proof is essentially a combination of problems b and d above first apply the procedure in b then apply the procedure in d with one slight modification we add n more vertices total where n is the number of vertices of the graph but instead of making them all disconnected only make of them of them disconnected and make the other of them connect completely with each other such that they cannot be part of any independent set then apply the either algorithm for analogous reasons to those described in d this procedure returns if and only if the new graph has an independent set of size which implies that the original graph has an independent set of size and thus we have shown that independent set independent set size or clique size problem to reduce the clique optimaization problem to the clique decision problem we need to show that problem of finding a clique of maximum size is reducible to determine the size of the maximum clique algorithm for finding clique pick a vertex v if the maximum clique size in g v is equal to the maximum clique size in g we can recurse on g v otherwise add v to final clique delete v and vertices in g not adjacent to v and recurse problem given a polynomial time algorithm v c g k which returns true iff graph g contains a vertex cover of size k we can create an algo rithm to find the maximum vertex cover by first calling v c g i for i from v to call k the smallest i such that v c g i returned true we now consider each vertex v and call v c g v k if v c g v k returns true then add v to the cover and set g g v and k k otherwise leave g and k alone to see why this is correct observe that we never add more than k vertices to the cover we only remove edges from g if they are covered and because v c g v k is true all remaining edges can be covered with k vertices still in g v it is not too hard to see that we cannot go through the entire graph more than once before v c g v k returns true thus we iterate at most v times making our reduction polynomial if v c g k is polynomial problem to reduce cnf sat to the linear inequality problem we create a set of inequalities s as follows for each variable xi in boolean formula f we create a integer variable xi and constrain it to be or by adding the inequalities xi and xi for each disjunction we create a sum if the literal is xi we simply add xi and if the literal is x i we add xi we then require the sum to be at least call the set of inequalities s if we map true to and f alse to it not too hard to see that a satisfying assignment to f gives a solution s and a solution to s gives a satisfying assignment to f problem to reduce the coloring problem to the coloring problem create a new vertex v that is connected to every other vertex in the original graph since v is connected to every vertex it must have a different color than the others this new geaph can be colored with colors if and only if the original graph can be colored with colors problem to reduce the coloring decision problem to the sat cnf assume the colors are red blue and green for each vertex x in g create the following variables xred xblue and xgreen for each vertex v g exactly one of vred vblue and vgreen must be true and for each vertex u adjacent to v the u variable coresponding to v color must be false that is v color u color must be true the construction of the entire clause is straightforward simply and together all the vertex clauses if there is an assignment to these variables that makes the entire clause true the graph can be colored with colors problem to reduce the vertex cover problem to the dominating set problem create a new graph g by replacing each edge e x y in g by a new vertex ve and edges x ve y ve and y x the vertex ve essentially forces the edge e to be covered call the procedure for dominating set to get a minimum cardinality dominating set d if a ve d then replace ve by x or y doesn t matter call this new set d note that d is still dominating in g then d is a minimum cardinality vertex cover in g note this reduction only handles the case when g is a connected graph and has at least one edge if g is not connected run this reduction on each non trivial connected component problem non many to one reduction to reduce the hamiltonian cycle problem to the fixed hamiltonian path problem check if there is a hamiltonian path between x and y for each edge e x y g note that this is suffient to answer the problem as stated many to one reduction create graph g as follows add a new vertex v to the input graph g and then pick any vertex v in g and connect v to all of the neighbors of v to solve the hamiltonian cycle decision problem hc g return the value of the fixed hamiltonian path problem on g with start vertex v and end vertex v f hp g v v if f hp g v v is true then by definition if we added an edge between v and v we d now have a hamiltonian cycle in g however as v and v have an identical set of neighbors we know that v neighbor in the path call it u is v neighbor in g thus the section of the path that is in g would start at v and end at u because v and u are neighbors we can complete the cycle if hc g is true then to construct the path for f hp g v v for any v in g remove the edge from the cycle between v and one of v neighbors in the cycle there are two call it u then add the edge from u to v to complete the path from v to v the edge from u to v must exist in g by the construction of g problem this solution was adapted from brian wongchaowart homework writeup to solve the vertex cover problem for an undirected graph g and an in teger m construct a new graph h to be used as input to the problem begin by adding a new vertex r to h then for each vertex vk in g add a vertex to h labeled vk and an edge between this vertex and r for each edge vi vj in g add a vertex to h labeled eij and add an edge between this vertex and the vertices in h labeled vi and vj add vertex r and all of the vertices eij corresponding to edges of g to the set r let e be the number of edges in g use h r and m e as the input to the problem a set u all of the vertices in r and at most m e vertices is a subgraph of h if and only if g has a vertex cover of size m if g has a vertex cover c of size m such a u can be found in h by taking r as the root adding the m vertices labeled with the vertices in c and finally adding each of the e vertices corresponding to an edge in g there is a path from the vertex r to every vertex eij by using the edge connecting it to the vertex from c such an edge must exist by the definition of a vertex cover this set has m e vertices if on the other hand h contains a set u that includes paths between all of the vertices in r and has at most m e vertices the vertices in g that correspond to a vertex in u form a vertex cover for g of size at most m to see why note that r is not connected directly to any vertex eij in h but by definition of r these e vertices must all be in u so u must include some number of the vertices in h such that there is an edge from one of these vertices to each vertex eij this corresponds exactly to choosing a set of vertices in g so that each edge is incident to a vertex in the set i e choosing a vertex cover the number of these vertices the size of this vertex cover cannot be greater than m because r as well as all e vertices each corresponding to an edge in g must by in u this reduction can clearly be carried out in polynomial time as one vertex is added to h for each vertex and each edge in g cs syllabus fall course home page course piazza group this group will be used for announcements the course group is the best place to ask general questions e g a question about a particular homework problem this group will be monitored by the instructor and ta but often other students can provide a quicker answer than the instructor course meeting the course will meet mondays wednesdays and fridays from in sennott square through the end of october a normal semester class has scheduled class meetings of which often one or two are canceled we will have a similar number of class meetings text the official text is foundations of algorithms by neapolitan and naimipour any edition of the textbook is fine for this class you are welcome to consult other introductory textbooks if you prefer most students don t find a textbook particularly useful prerequisites cs and cs if you take this class without these prerequisites you forfeit any right to complain that the class is at an inappropriate level course content the main goal of the course is to learn to think algorithmically like a real computer scientist this course is different than cs in that we will be designing our own algorithms as opposed to learning algorithms in the past most students have found the course material to be challenging most class time will be devoted to examples of algorithm design for particularly interesting problems there will be small group homework assignments due almost every class it is expected that most of your learning will come from the process of solving the homework problems within your group problems on the midterm exams will be very similar usually identical to problems covered in class or assigned as homework we will cover the following topics in the following order deciding the correctness of algorithms greedy algorithms chapter of neapolitan and naimipour dynamic programming chapter of neapolitan and naimipour reductions and np completeness chapter of neapolitan and naimipour parallel algorithms chapter of neapolitan and naimipour grading grades will be based on homework classroom participation and two midterm exams homework will constitute of the final grade each midterm exam will constitute of the final grade there will not be a cumulative final exam attendance will be taken and along with class participation will count for the remaining of the grade i will subjectively set the grading scale at the end of the semester you are not in competition with other students i have no set numbers of a b etc i strongly suggest you cooperate with each other to understand the material this is in all students best interests if a student homework scores are conspicuously suspiciously higher than a student exam grades i reserve the right to base the course grade on only the exam scores and classroom participation generally speaking i usually don t give out a lot of a because i think the top grade should really represent mastery of the material but i tend to be sympathetic in giving out c if a student attends class regularly participates full in class and regularly makes a good faith attempt on homework assignments homework policy you should do your homework in groups of or people groups of different cardinal ities including must be approved by me each group need only provide one write up per group write ups must use latex you may discuss problems with any student in the class with the provisos that you shouldn t feed others complete solutions and you must acknowledge your collaborators and the nature of the collaborations at the end of your the write up you may not seek solutions to assigned problems on the www in other books from friends outside the class etc all homework is due at the start of class on the date due no late homework is accepted the homework will be graded by the ta many students will find some problems demanding it is not expected that all students will be able to answer all the homework questions exam scoring appeal policy you may submit an appeal in writing if you believe that your solution for a problem on an exam is essentially fully correct no appeals are allowed for additional partial credit partial credit is too subjective appeals will not be accepted earlier than class after the exams were returned and will not be accepted later than classes after the exams were returned note that the problem will be regraded there is a possibility that the new grade will be lower than the original grade of course any clerical errors can be corrected disability policy if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union tty as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course missing tests if you are going to miss a test for unavoidable reasons then before the exam or as soon as possible you must contact me cheating policy i have no tolerance for cheating if you are caught cheating you will receive an f grade for the course cs final exam fall directions the test is closed book and closed notes answer at most part b questions answer at most part a questions one quarter credit will be given for unanswered questions answers that are way o the mark will get no credit for the part a questions usually it is then a good idea to have a paragraph giving an overview of the proof strategy technique that you will use and what the key ideas are before launching into the details part b questions de ne entropy and state shannon source coding theorem state ladner theorem formally de ne pseudo random generator state landauer principle give one situation where the principle is applicable and one situation where the principle is not applicable explain why the language of isomorphic graphs is in np explain why it is unlikely that this language is np complete which of these complexity class are known to have complete languages under that natural reductions np co np rp pspace p np n co np sp explain what is di erent about the complexity classes that are known to have complete languages and those that are not known to have complete languages give the walsh hadamard code for show your work in the proof of godel incompleteness theorem we showed that nding proofs for particular types number theoretic statements is not possible explain intuitively what these number theoretic statements are on which principle did the security of the quantum cryptographic protocol that we considered in class rest state this principle explain in one sentence how this principle is related to security draw a venn diagram explaining the known inclusion relationships for the complexity classes exponentialspace p exponentialtime logspace pspace state two gen eral theorems from which all the inclusions follows from state which inclusions are known to be proper and state two general theorems from the properness of these inclusions follow from what is the standard circa proof technique to show that one complexity class properly contains another explain why it is believed that this technique won t be able to resolve the p np question de ne perfect secrecy of a private key cryptographic protocol part a questions consider a programming language mini java that only has one type of loop and the number of iterations of the loop must be determined when the loop is rst encountered so a loop statement might look like repeat n times and the variable n is evaluated when the state ment is reached you can assume that the program has a variable size that is instantiated to the input size when the program starts running otherwise you couldn t even read the in put so all mini java programs must halt on all inputs show by diagonalization that there is a language accepted by a java program that is not accepted by any mini java program use diagonalization give a concrete example of a language that is not acceptable by any mini java program but that is accepted by a java program explain the setup of the half silvered mirror experiment explain how this is formally modeled within quantum mechanics that is how is the direction of the photon modeled how are the mirrors modeled and how is measurement modeled then show that calculations within this model give the result of the experiment in the real world consider the following game played by two players a and b on a directed graph g with a designated start vertex on the rst move player a picks an edge v leaving then and v are designated as visited on each even numbered move player b picks an edge v w from the current vertex v to a vertex w hasn t been visited before if no such vertex w exists then player b loses vertex w then is designated as visited and becomes the current vertex on each odd numbered move player a picks an edge v w from the current vertex v to a vertex w hasn t been visited before if no such vertex w exists then player a loses vertex w then is designated as visited and becomes the current vertex so the players a and b taking turns picking edges in a directed path p starting at with a player losing if he she can t extend the path prove that determining for a particular g and whether the rst player has a winning strategy is pspace complete under polynomial time reductions you must prove hardness by reduction from the problem of determining whether a quanti ed boolean formula f is true prove that determining the language of satis able boolean formulas is np complete from rst principles that is prove cook theorem give an interactive protocol for proving the lower bound on the size of a set within a factor of you need only de ne pairwise independent hash function but you need not explain how to nd them e ciently prove that the protocol is correct with high probability state the pcp theorem and then use the pcp theorem to prove that there exists an such that it is np hard to approximate maxsat within a factor of cs midterm spring directions the test is closed book and closed notes there are part b questions answer at most part b questions please try to limit your answers to one sentence part b questions are worth points per question there are part a questions answer at most part a questions part a questions are worth points per question time will likely be an issue for most students so use time wisely initially concentrate on the main ideas and then ll in details with any remaining time in particular for the part a questions usually it is then a good idea for the start of your answer to de ne relevant terms give an overview of the proof strategy technique that you will use and to explain the key ideas are after this you may launch into details part b questions state the church turing thesis and explain why it is generally believed to be true someone gives you a program iamf amous p i that they claim will determine whether the program p halts on the input i give an example of a program p and input i on which the program iamf amous will not be correct state godel rst incompleteness theorem and explain how its proof is related to the halting problem give an example of a language that can be accepted using log space but that can not be accepted using constant space for each of the following two sentences replace with the smallest function that will make the sentence true a time f n is subset of sp ace b sp ace f n is a subset of time de ne the language tqbf referred to in lecture as just qbf and state the complexity class this language is complete for explain how to prove that a language l is complete for a complexity class c under a type r of reduction draw a venn diagram for the complexity classes bp p rp corp and zpp make sure that areas that are known to be empty have zero area in your drawing for example it is known that the intersection of classes x and y is empty then in your drawing x and y should not intersect give an example of a decidable language that is in p poly but not in p if you pick a function f uniformly at random from all functions from n to the smallest circuit that computes f will likely contain how many gates ignoring multiplicative constants according to the karp lipton theorem what is the consequence if the boolean satis ability problem sat can be solved by boolean circuits with a polynomial number of logic gates de ne the complexity class p part a questions prove time is a strict subset of time that is show that there is a language in time that is not in time let p be some property of languages further assume there is a turing machine that accepts a language that has property p and a turing machine that accepts a language that does not have has property p show by reduction from the halting problem that there is no turing machine that takes as input a turing machine m and determines whether the language l m accepted by m satis es property p assume a log space reduction from a language a to a language b so more precisely there is a turing machine t with three tapes a read only input tape a read write work tape and a write only output tape t only uses log of the input size many cells on the read write work tape further t never backs up the tape head on the write only output tape so the tape head on the write only tape either stays in position or moves to the right the machine t has the property that a string x is in a if and only if the contents of the write tape when t ends computation on input x is in b now show that if there is a log space turing machine s that accepts b then there is a log space turing machine u that accepts a a nondeterministic circuit has two inputs x and y we say that c accepts x if and only if there exists a y such that c x y the size of the circuit is measured as a function of the size of x let np poly be the languages that are decided by polynomial size nondeterministic circuits show bp np np poly recall that bp np is the set of languages l that such that there exists a randomized polynomial time turing machine m such that for all x with probability at least it is the case that the boolean formula output by m x is satis able if and only if x l show that bp p is a subset of sp cs midterm spring directions the test is closed book and closed notes there are part b questions answer at most part b questions please try to limit your answers to one sentence when possible part b questions are worth points per question there are part a questions answer at most part a questions part a questions are worth points per question time will likely be an issue for most students so use time wisely initially concentrate on the main ideas and then ll in details with any remaining time in particular for the part a questions usually it is then a good idea for the start of your answer to de ne relevant terms give an overview of the proof strategy technique that you will use and to explain the key ideas are after this you may launch into details part b questions state the protocol for the interactive proof for graph non isomorphism gni given in the text and in class you can assume that the veri er has access to private random bits that can not be seen by the prover state the perfect zero knowledge interactive proof for graph isomorphism give a formal as you can de nition of what it means for a private key protocol presumably with small keys to be semantically secure the christian science monitor article you read as homework reported on the construction of a particular kind of quantum gate a state the name of this type of gate b give the functionality input output relation of this type of gate give the matrix that represents the bit hadamard quantum operation a de ne a f n g n restricted veri er within the context of probabilistically checkable proofs b de ne pcp f n g n c state the pcp theorem let s be the sum of two independent six sided dice so the probability s is equal to x is x for x the probability s is equal to x is x for x write an arithmetic expression for the entropy of s you need not simplify this expression a de ne the komogorov complexity of a string x b let l be the language consisting of pairs x k where x is a string k is an integer and the kolmogorov complexity of x is equal to k is l in pspace justify your answer explain how a pseudo random generator can be used to derandomize a randomized algorithm to get a deterministic algorithm that is more e cient than the naive derandomization a give the value of u u when u where is the outer tensor product b give the walsh hadamard encoding of u u when u give a diagram to show which of the following statements imply other statements so i m looking for an organized list of implications e g a c c b etc a the existence of good pseudo random generators b p np c the existence of computationally secure private key cryptography with small keys d the existence of secure public key cryptography e the existence of one way functions in the homework you were assigned to read a blog article by scott aaronson on on peter shor quantum algorithm for factoring a according to this article the problem of factoring was reduced to nding a particular property of a sequence state this property b to illustrate the intuition behind the algorithm professor aaronson used a particular type of device that students frequently encounter in their day to day life what is this device part a questions give a computationally zero knowledge interactive proof that a graph has a hamiltonian cycle give a short informal explanation what computationally zero knowledge means and intuitively why this proof this property give the interactive proof ip protocol for showing that a particular boolean formula f has a particular number k of satisfying assignments explain why the proof is correct the goal of this problem is to nd a way to transmit information about a qubit by sending two classical bits alice and bob split up entangled bits a and b in state v assume that now alice is given qubit x so x is in some unknown superposition between states and alice now performs the following reversible cnot operation on x if a then negate x alice then runs qubit a through a bit hadamard gate alice now measures the current values of a and x and sends these two classical bits to bob explain what the state of all the particles a b and x is after each of alice operations then explain how bob can use the two classical particles to change the state of b to the original state of x consider the proof that np pcp poly n in the text and from class a state the np complete problem for which a probabilistically checkable proof is given b explain what the format of the probabilistically checkable proof the book c explain how the veri er determines whether the encoded solution is indeed a solution to the instance of the np complete problem assuming that the encoding in the proof book is in the correct format d list the three properties the veri er has to check to make sure the encoding of the proof book is of the correct form e pick one of these three properties and explain how the veri er checks that the proof book has this property assume x is a letter that a sender wants to send to a receiver over a noisy channel and let y be the letter received by the receiver because the channel is noisy y may not equal x for each pair x y there is a probability that px y that the letter y is received when the letter x is sent let x be a probability distribution of the sent letter x and y the corresponding distribution of the received letter y let i x y be the amount of information one gets about the sent letter x when one sees the received letter y let i y x be the amount of information one gets about the received letter y when one sees the sent letter x is i x y i y x or is i y x i x y or is i x y i y x justify prove that your answer is correct start with the standard de nition of i x y cs syllabus spring course home page https people cs pitt edu kirk index html course piazza group http piazza com pitt home this group will be used for announcements the course group is the best place to ask general questions e g a question about a particular homework problem this group will be monitored by the instructor and ta but often other students can provide a quicker answer than the instructor course meeting the course will meet on selected mondays wednesdays and fridays from 45 in sennott square a normal semester class has scheduled class meetings of which often one or two are canceled we will have a similar number of class meetings text computational complexity a modern approach by arora and barak a rough draft can be found at http theory cs princeton edu complexity book pdf prerequisites cs if you take this class without these prerequisites you forfeit any right to complain that the class is at an inappropriate level course content and grading we will loosely follow the rst eleven chapters of the text by arora and barak there will be daily homework assignments and two midterm exams grading grades will be based on homework classroom participation and two midterm exams homework will constitute of the nal grade each midterm exam will constitute of the nal grade there will not be a cumulative nal exam attendance will be taken and along with class participation including participation on piazza will count for the remaining of the grade i will subjectively set the grading scale at the end of the semester you are not in competition with other students i have no set numbers of a b etc i strongly suggest you cooperate with each other to understand the material this is in all students best interests if a student homework scores are conspicuously suspiciously higher than a student exam grades i reserve the right to base the course grade on only the exam scores and classroom participation generally speaking i usually don t give out a lot of a because i think the top grade should really represent mastery of the material but i tend to be sympathetic in giving out c s if a student attends class regularly participates fully in class and regularly makes a good faith attempt on homework assignments homework policy you should do your homework in groups of or 3 people groups of di erent cardinal ities including must be approved by me each group need only provide one write up per group write ups must use latex http en wikipedia org wiki latex figures may be hand drawn you may discuss problems with any student in the class with the provisos that you shouldn t feed others complete solutions and you must acknowledge your collaborators and the nature of the collaborations at the end of your the write up you may not seek solutions to assigned problems on the www in other books from friends outside the class etc all homework is due at the start of class on the date due no late homework is accepted the homework will be graded by the ta many students will nd some problems demanding it is not expected that all students will be able to answer all the homework questions exam scoring appeal policy you may submit an appeal in writing if you believe that your solution for a problem on an exam is essentially fully correct no appeals are allowed for additional partial credit partial credit is too subjective appeals will not be accepted earlier than 1 class after the exams were returned and will not be accepted later than 2 classes after the exams were returned note that the problem will be regraded there is a possibility that the new grade will be lower than the original grade of course any clerical errors can be corrected disability policy if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union 648 7355 tty as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course missing tests if you are going to miss a test for unavoidable reasons then before the exam or as soon as possible you must contact me cheating policy i have no tolerance for cheating if you are caught cheating you will receive an f grade for the course cs networks crowds and markets syllabus fall class times mwf pm pm sensq course home page http people cs pitt edu kirk index html the plan is to cover the whole text prerequisites consent of the instructor course content this is both the rst time that i have taught a course on this topic and the rst time that i have taught a special topics course that includes undergraduates so i view all plans as tentative and subject to change based on experience the current plan for the rst or so classes is as follows for each class there will be an assigned reading and assigned homework problems there will be a short quiz at the start of class to determine to what extent the students learned the material i will then lecture on either some more portion of the text that is conceptually more di cult usually the portions label advanced material or lecture about conceptually more di cult extensions of the material in the book we should nish o covering the material in the text in these lectures the remaining classes will each consist of two minute conference style talks given by students on selected papers in a recent acm conference on economics and computation ec followed by brief discus sions each student will give two such talks grading grades will be based on homework daily quizzes class attendance class participation and the talks the exact weight of each component will be determined later i will subjectively set the grading scale at the end of the semester you are not in competition with other students i have no set numbers of a b etc i strongly suggest you cooperate with each other to understand the material this is in all students best interests disability policy if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union 412 tty as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course cheating policy i have no tolerance for cheating if you are caught cheating you will receive an f grade for the course combinatorial auctions and the vcg mechanism combinatorial auctions recall that the va is concerned with auctioning off a single good combinatorial auctions are motivated by the following natural question question what if there is a set s of m goods to be auctioned off to n players how can we extend the va to this more general setting a natural idea is to run a separate vickrey auction for each of the m goods this works i e properties hold if each player i has a separate value for each item and the value of a subset t s of goods to player i is the sum of its values for the goods of t exercise check this unfortunately this simple approach ignores the possible dependencies between the outcomes of the different auctions for players more specifically it ignores substitutes a player value of getting say two goods is less than the sum of its values for each individually e g they are at least partially redundant complements a player value of getting say two goods is greater than the sum of its values for each individually e g they are at least partially co dependent indeed one of the applications that kicked off the systematic study of combinatorial auctions was the problem faced by the faa of auctioning off take off and landing slots at airports to the major airlines two take off slots from the same airport at almost the same time are substitutes from an airline perspective whereas a take off slot at one airport and a landing slot at a second airport at the appropriate subsequent time act as complements informally a combinatorial auction ca is an auction that allocates a set of many goods to bidders in the presence of substitutes and complements as we will see designing good combinatorial auctions is much more challenging than designing good auctions for selling a single good the vcg mechanism our first combinatorial auction is a classical powerful mechanism called the vcg mecha nism by mechanism we essentially mean some sort of incentive compatible protocol the v stands for vickrey the c for clarke and the g for groves three researchers who gave successively more general versions of vickrey auction the good news about the vcg mechanism is that it satisfies all of properties from subsection incentive constraints economic efficiency and general valuations the bad news is that it is highly computationally intractable to specify the vcg mechanism we first need to say what we mean by a valuation of a player i when there is a set s of m goods for now we will allow a very general definition later we will look at several special cases we call a subset t s of goods a bundle the valuation vi of the player i is a function from the set of all possible bundles to the nonnegative reals in other words the valuation specifies the value vi t of player i of every conceivable bundle t s of goods that it might receive note that with m goods there are such bundles we assume that vi for every i though this is not an essential assumption for this section we do not even need to assume that vi is nonnegative or that it is monotone i e that t t i implies vi t vi t i though we will make these assumptions in future sections note that such valuations are certainly expressive enough to model substitutes and com plements for example if s contains two goods which are perfect substitutes for a player i then i valuation might be v v v if the two goods are complements then i valuation might be given by v v and v recall that for a single item auction the job of the auction is to determine a winner and what price to charge in a combinatorial auction there can be multiple winners the outcome of a ca is to allocate a bundle ti s to each player i such that bundles given to distinct players are disjoint no good can be allocated to more than one winner accordingly a ca can charge a different price pi to each player i as in the va we again assume quasilinear utilities meaning if player i receives the bundle ti and is charged the price pi then its utility is vi ti pi we now state the vcg mechanism deferring the description of the prices until sec tion compare to the three steps of the va each player i submits a bid bi t for every possible non empty bundle t s we always implicitly assume that bi if the player is truthful then bi t vi t for every t s choose an allocation t t that maximizes n over all feasible allocations ti n n bi ti i feasible means that ti tj whenever i j charge each player i an appropriate price pi to be determined both steps and should alarm theoretical computer scientists more on this shortly nevertheless we can verify the properties and from subsection without even stating the prices proposition the vcg mechanism is economically efficient in other words if all players bid truthfully then the vcg mechanism outputs an allocation that maximizes over all feasible allocations n vi ti i proof immediate from step of the mechanism proposition the vcg mechanism works with general valuations proof by definition to discuss property we need to specify our criteria for computational tractability recall we are interested in auctions that run in polynomial time but polynomial in what question recall that merely specifying the valuation of a player requires pa rameters should we be happy if an auction runs in time polynomial in this input size in this course we will be ambitious our criteria for polynomial time will be polynomial in the number n of players and in the number m of goods in other words we are only interested in cas that scale reasonably with number of players and goods the vcg mechanism clearly does not satisfy this stringent definition of computational tractability merely communicating the bid of a single player in step requires exponential resources the vcg mechanism is also computationally inefficient in a second sense as we will see in section even in special cases where bidders can communicate their entire valuation in polynomial time the optimization problem that the vcg mechanism must solve in step can be highly intractable vcg prices and strategyproofness to determine whether or not the vcg mechanism has property i e satisfies incentive constraints we must specify the prices charged in step question suppose we always set pi for all i would this make the vcg mechanism truthful question we will give prices that generalize those in the va can you think of what this would look like for a ca say with two goods we specify the vcg prices in a form due to clarke in english the definition is a set pi equal to the damage caused to the other players by i presence mathematically we have pi max tj j i bj tj bj t j i j i where the maximum ranges over all feasible allocations of the goods to the n players other than i as usual we insist that tj tk for all j k several comments first to interpret these prices it is often helpful to think of each of the bids bi in as the corresponding true valuation vi after all at the end of the day we will prove that the vcg mechanism is truthful and thus expect bidders to bid their true valuations of course the price pi cannot explicitly refer to a true valuation vi since these are unknown to the mechanism it can only use the received bids as proxies for the true valuations the first term on the right hand side of is the maximum possible surplus if we delete player i bid and optimize only for the n other players note this is precisely the result of rerunning step of the vcg mechanism after deleting i bid from the input since player i did submit a bid however the vcg mechanism instead chose the allocation t n maximizing the surplus n bj t of all of the players from the perspective of the n players other than i their collective benefit in this allocation is j i bj t the second term on the right hand side of the right hand side of is therefore the extent to which the collective benefit of the n players other than i would increase if player i was deleted and the vcg mechanism chose an allocation solely for their benefit the damage caused to these players by i presence the idea of these prices is to force a player to care about the welfare of the other players thus aligning the objective of the player with the global objective of maximizing social surplus this idea is common in economics and is often called internalizing an externality example in the special case of a single good auction the price specializes to the prices in the va for losers the second highest bid for the winner to see this note that with a single item every bundle t has the form either for losers or for the winner where denotes the item being sold when a player submits a bid bi in the va it corresponds to a bid bi in the current notation as usual we implicitly assume that bi for every player note also that step of the vcg mechanism simply means giving the item to the highest bidder which is step of the va first consider a player i that loses so t and bi t let k be the winner so t and bk t is its sealed bid bk the second term on the right hand side of is bk since player i lost i e did not have the highest bid deleting the player and rerunning step of the va would still result in player k winning the item the first term on the right hand side of is also bk resulting in a price pi for player i on the other hand suppose player i wins the item so bi t bi and bj t for every j i the second term on the right hand side of is if player i is deleted and step of the va is rerun then the remaining player with the highest bid the player that originally possessed the second highest bid wins so the first term on the right hand side of is the second highest bid as in the va the definition of the vcg prices immediately gives the following proposition vcg prices are nonnegative proof one feasible solution for the maximization problem in the first term in the right side of is t j i with total value j i bj t the maximum can only be larger we now give a second definition and interpretation of the vcg prices to obtain it we simply add and subtract bi t from and rearrange terms pi bi t n j bj t max tj j i j i bj tj the way to think about is that if the player i receives the bundle t then it pays its bid bi t minus a discount the expression in the square brackets in note that the discount term is precisely the extent to which i presence increases the maximum achievable efficiency question what is the discount term in a single item auction recall that in a first price single item auction there can be an incentive for players to underbid see the discussion following proposition the rough intuition for the discount term above is that it simply gives players up front whatever they could gain by underbidding in a first price version of the vcg mechanism from the second definition of the vcg prices we immediately obtain that the vcg mechanism is individually rational recall proposition proposition the utility of a truthtelling bidder in the vcg mechanism is always nonnegative proof the proposition is equivalent to showing that the discount term in is always nonnegative this holds because adding an extra bidder can only increase the maximum achievable surplus it only enlarges the set of feasible allocations all that remains to prove is that the vcg mechanism is truthful as an exercise the reader is invited to prove that it is also strongly truthful in the sense of proposition proposition the vcg mechanism is strategyproof that is for every player i even if the player knows the full bids of all of the other players player i maximizes its utility by bidding truthfully setting bi t vi t for every non empty bundle t s proof we follow the more general approach of groves which will make the proof of truthfulness more transparent we first prove truthfulness for the wrong set of prices and then show how to shift these prices to recover the vcg prices while maintaining truthfulness modify the vcg mechanism so that in step it computes the following price pi for each player i pi bj t j i where as usual t n denotes the allocation computed in step of the vcg mechanism note these are negative prices i e subsidies and are certainly not the vcg prices of and for example for a single item auction these prices say that the winner should be charged nothing while all the losers should be paid the winner bid question does this result in a strategyproof single item auction note that the price is defined so that any benefit to some other player also benefits the player i more precisely since player i utility is its value for its bundle minus the price paid its utility for a given allocation t n with the price in is vi t bj t suggestively the vcg mechanism chooses in step the allocation t n to maximize over all feasible allocations tj n n bj tj j glibly we might finish this part of the proof by saying that if player i bids truthfully then the vcg mechanism objective and its own are exactly aligned which then results in an optimal outcome from i perspective while this argument is not incorrect we proceed a bit more carefully as a sanity check note that the only thing player i has control over is its bid bi t t s while the player cannot directly control the allocation t n chosen in step of the vcg mechanism it can potentially influence the choice of this allocation by varying its bid similarly it cannot influence the functions vi and bj for j i recall no collusion is allowed only the allocation t n chosen by the vcg mechanism now view as an objective function for a discrete optimization problem over allocations from player i perspective there is some allocation say t j n that maximizes this function the best case scenario for player i is that some bid bi t t s coaxes the vcg mechanism into choosing this allocation t j n as the allocation t n in its step if there exists such a bid then no other bid can provide i with strictly more utility but if player i bids truthfully bi t vi t for all t then the criterion maximized by the vcg mechanism over all feasible allocations tj n is n vi ti bj tj j i and thus the vcg mechanism will indeed choose the allocation t j n in step or some other allocation with equal value from player i perspective thus player i maximizes its utility by bidding truthfully we have shown that the vcg mechanism is truthful provided we use the negative prices in here the key idea of groves suppose we shift each price pi by a function hi bj j i that is independent of i bid bi here by independent we mean that once we fix all bids bj for j i hi is a constant function of bi in particular it cannot depend on the allocation t n chosen by the vcg mechanism which in turn is a function of bi for example in the single item case hi bj j i could be the highest bid maxj i bj by some other player below we use the standard shorthand b i to denote the set bj j i of bids by players other than i the claim is that adding such a function hi to the price pi charged to player i does not affect strategyproofness this follows from two simple facts first the new objective for player i given fixed bids b i by the other players is to choose a bid bi t t s to maximize vi t bj t c where c is the constant hi b i note that the sets of allocations maximizing and are exactly the same second the allocation t n chosen by the vcg mechanism is independent of the prices and of hi in particular and depends only on the bids thus bidding truthfully still causes the vcg mechanism to choose an allocation that maximizes over all feasible allocations this completes the proof of the claim finally note that instantiating hi b i max tj j i bj tj j i for each player i gives the vcg prices summary in this section we described the classical vcg mechanism for cas the mechanism can also be defined much more generally see on the plus side it has properties from subsection it satisfies both incentive constraints and economic efficiency even with general valuations unfortunately it is computationally intractable even the bidding step step requires an exponential in m amount of communication and time single minded bidders the vcg mechanism has all of the properties that we d want of a ca except for compu tational tractability in this section we begin exploring the following question which has been systematically studied only relatively recently since the late mostly by com puter scientists how much do we need to relax the properties of subsection to recover computational tractability we have already noted that if we weaken by assuming that bidders valuations have no complements or substitutes then we can easily achieve the other three properties by running a separate vickrey auction for each good see the discussion following question what can we accomplish with at least some degree of complements and or substitutes preliminaries in this section we will focus on a highly restricted class of valuations which essentially model an extreme form of complements definition let s be a set of goods and i a bidder with valuation vi the bidder i is single minded if there is a set ai s of goods and a value i such that vi ti i whenever ti ai and vi ti otherwise thus from i perspective there are only two distinct outcomes either it gets all of the goods it wants the set ai in which case its value for its bundle is i or it fails to get all of these goods in which case its value for its bundle is the motivation for this definition is twofold first it is a conceptually simple type of valuation that nevertheless models one of the quintessential aspects of cas complements second it immediately gets rid of the initial computational stumbling block for the vcg mechanism now players valuations can be implicitly but completely specified in time poly nomial in n and m since each player i can simply report proxies for its set ai and value i we should therefore ask the following question for the special case of single minded bidders can the vcg mechanism be implemented to run in polynomial time if the answer is yes then we can move on to more general classes of valuations if the answer is no then we will need to design a new computationally tractable mechanism even for the case of single minded bidders the answer to question is no assuming p np the reason is that the vcg mechanism is computationally inefficient in two distinct senses first as we have repeatedly noted the bidding step requires exponential communication for general valuations sec ond even when this problem is assumed away as with single minded bidders the allocation step of vcg can require exponential computation precisely consider the optimization problem of maximizing the surplus given the true valuations of the bidders this problem is typically called the winner determination wd problem note that step of the vcg mechanism is precisely the wd problem where bids are used as surrogates for true valuations for single minded bidders the wd problem has the following form given the valuations truthful bids of the players as specified by the pairs an n grant a set of disjoint bids i e a subset of players such that the corresponding ai are pairwise disjoint to maximize the sum i of the values of the granted bids we next show that the wd problem is hard even in the special case of single minded bidders proposition the wd problem for single minded bidders is np hard proof by a reduction from the np hard problem weighted independent set wis given an instance of wis specified by a graph g v e and a weight wv for each vertex v v construct the following instance of the wd problem the set of goods is the set e of edges of g the set of players is the set v of vertices for a vertex player v v set v wv and av equal to the set of edges of g that are incident to v a subset of vertices players is then a wis of g if and only if it is a subset of bids that can be simultaneously granted moreover this bijective correspondence preserves the total weight value of the solution unfortunately wis is not just an np hard problem it is a really hard np hard prob lem to make this precise recall that a  approximation algorithm for a maximization problem is a polynomial time algorithm that always recovers at least a  fraction of the value of an optimal solution by our convention  is always at least fact for every there is no o e approximation algorithm for wis where n denotes the number of vertices unless np zpp fact basically says that the wis problem admits no non trivial approximation algorithm note that simply picking the max weight vertex gives an n approximation for wis more relevant for cas is the following consequence of fact corollary for every there is no o m e approximation algorithm for wis where m denotes the number of edges unless np zpp corollary follows from fact because the number of edges of a simple graph is at most quadratic in the number of vertices because the reduction in the proof of proposition is approximation preserving it gives a bijection that preserves the objective function values of corresponding solutions of wis and wd it implies the following strong negative result about approximating the wd problem with single minded bidders corollary for every there is no o m e approximation algorithm for wd with single minded bidders where m denotes the number of goods unless np zpp the upshot of corollary is rather bleak if we want a polynomial time ca property from subsection then even if we assume single minded bidders sacrificing significant valuation generality and even if we ignore incentive compatibility then we must take a big hit on property and settle for at best an o m approximation of the surplus at least the bad news stops here we next design a ca for single minded bidders that is poly time implementable achieves the best possible approximation of the surplus under this constraint o m and also satisfies the incentive constraints we present this ca in two parts first we present a poly time o m approximation algorithm for wd with single minded bidders subsection then we show how to charge prices to turn this wd algorithm into an incentive compatible mechanism subsection approximate winner determination we now design an approximation algorithm for the following problem given a set s of m goods and truthful bids an n which bids should we grant to maximize the total value of granted bids here by grant bid ai i we mean assign player i the bundle ti ai obviously granted bids should be pairwise disjoint we will design a greedy approximation algorithm for this wd problem to motivate the algorithm we first consider two greedy algorithms that fail to achieve the target performance guarantee of o m example suppose we sort the bids in decreasing order of value and grant them greedily in other words we go through the bids one by one in sorted order and we grant a bid if and only if all of its items are still available the following example is bad for this algorithm there is a set s of m goods and n m players set s and where is arbitrarily small for i m set i and ai equal to the i th good of s our greedy algorithm grants the first bid and achieves a surplus of the optimal solution grants the rest of the bids and achieves a surplus of m thus this algorithm is no better than an m approximation for the wd problem the greedy algorithm in example performs poorly because it fails to account for the fact that a big bid i e a bid for many items can block a large number of small bids that each have almost the same value as the big one a natural way to fix this problem is to somehow normalize the value of a bid according to the number of items that it requires this motivates our second greedy algorithm example suppose we instead sort the bids in decreasing order of i ai value per good and grant bids greedily this algorithm certainly returns the optimal solution for the input in example what is its performance in general consider the following example a set s of m goods one player with s and m and a second player with and the above greedy algorithm grants the second bid the optimal solution grants the first bid thus the greedy algorithm is no better than an m approximation algorithm for maximizing the surplus the greedy algorithm in example performs poorly because it undervalues large bids that primarily comprise items for which there is no contention our final algorithm the los algorithm due to lehmann o callaghan and shoham interpolates between the greedy algorithms of examples and and considers bids in decreasing order of i ai see figure exercise modify examples and to obtain two different examples showing that the los algorithm is no better than a m approximation algorithm for the wd problem perhaps surprisingly this simple modification is enough to obtain an essentially best possible approximation ratio recall corollary theorem the los algorithm is a m approximation algorithm for the wd problem with single minded bidders input a set s of m goods truthful bids an n reindex the bids so that n a a a for i n if no items of ai have already been assigned to a previous player set ti ai otherwise set ti figure the los approximate winner determination algorithm proof fix a set s of m goods and bids an n let x n denote the indices of the bids granted by the los greedy algorithm and x those of an optimal set of bids we need to show that i x i m i i x our proof approach is a natural one for analyzing a greedy algorithm we use the greedy cri terion to establish a local bound between pieces of the greedy and optimal solutions and then combine these local bounds into the global bound we next make a simple but crucial definition we say that a bid i x blocks a bid i x if ai ai we allow i i in this definition note that if i blocks i and i i then the bids ai and ai cannot both be granted the greedy and optimal algorithms made different decisions as to how to resolve this conflict for a bid i x let fi x denote the bids of x first blocked by i i e i x is placed in fi if and only if i is the first bid in the greedy ordering that blocks i two key points first we can already describe our local bound relating pieces of the optimal and greedy solutions suppose i fi the bid i x is first blocked by i x then at the time the greedy algorithm chose to grant the bid i the bid i was not yet blocked and was a viable alternative by we must have i ai i ai whenever i fi the second key point is that each optimal bid i x lies in precisely one set fi each bid i x must be blocked by at least one bid of x possibly by itself since i would only by passed over by the greedy algorithm if it was blocked by some previously granted bid thus the fi are a partition of x in particular i x i i i x i fi this fact allows us to consider each bid i x separately and then combine the results to obtain the global bound now fix a bid i x summing over all i fi in we have  i a compare to the key question is how big can the expression in parentheses on the rhs of be first since all bids of fi were simultaneously granted by the optimal solution they must be disjoint and hence ai m i fi the worst case is that this inequality holds with equality how would we then partition s among the fi bids of fi to maximize i fi ai the answer is that we would spread the goods out equally m fi goods in each set formally this follows from the cauchy schwarz inequality or from the concavity of the square root function it should also be easy to convince yourself of this fact with simple examples e g the fi case these facts and give   m m i f finally since the bid i blocks all of the bids of fi and bids of fi are disjoint in the worst case each item of ai blocks a distinct bid of fi cf example thus fi ai which implies i m i i fi summing over all i x and applying completes the proof of exercise suppose we modify the los algorithm to grant bids greedily in decreasing order of i ai p where p is a parameter what is the approximation ratio of this algorithm as a function of p a truthful payment scheme now that we ve designed a best possible approximate wd algorithm subject to the con straint of poly time computation we next aim to extend it to a truthful mechanism by charging suitable prices in particular recall that the los algorithm assumes that its input is a set of truthful bids to justify this assumption we seek prices that result in a strate gyproof mechanism otherwise the algorithm is optimizing using the wrong input so its approximation guarantee is meaningless a natural idea is to plug the los wd algorithm into step of the vcg mechanism in other words first all players report their set ai and value i then we determine an allocation using the los algorithm and then we charge player i a price equal to the monetary damage it causes the other players note that this is a poly time mechanism but is it truthful example consider the following modification to example the first player has the set s and value m for i m the ith player wants only the i th item and has value i if all players bid truthfully then the los algorithm will grant only the first player bid but if we delete the first player bid then all of the other players bids will be granted by the los algorithm thus the monetary damage caused by the first player to the rest equals m but then the price charged to the first player by the vcg mechanism is m even though its bid was only m and this player winds up with negative utility thus the vcg mechanism together with the los algorithm is not truthful e g the first player could obtain zero utility by bidding a value of and is not even individually rational in the sense of proposition in fact the vcg mechanism is incompatible with approximate wd algorithms in a quite general sense see nisan and ronen for a detailed study of this issue the moral of example is that if we want to extend the los algorithm to a truth ful mechanism then we have to carefully design a pricing scheme that is tailored to the algorithm the solution to this non trivial problem follows the high level idea of the los pricing scheme is to charge prices that are vickrey like in the sense that a winner i should pay according to a suitable function of the highest value bid that i bid blocks this motivates a key definition definition suppose bid i was granted by the los algorithm while bid j was denied the bid i uniquely blocks the bid j if after deleting the bid i from the input the los algorithm grants the bid j we will use the terminology u blocks as shorthand for uniquely blocks definition is somewhat subtle we give a simple example and encourage the reader to explore more complicated ones example figure shows a rough picture of four bids the bids are numbered accord ing to the los greedy ordering overlap between two circles is meant to indicate that the two bids share at least one item given the full input the los algorithm will grant the first two bids and deny the last two if the first bid is deleted the los algorithm will grant the second and fourth bids thus the first bid u blocks the fourth bid but it does not u block the third bid exercise show that the terminology u block is somewhat misleading in the following sense a bid bi bi can u block a bid bj bj even if bi and bj are disjoint on the other hand show that if bj bj is the first bid in the los ordering that is u blocked by bi bi then bi bj figure illustration of definition u blocking the idea of the los pricing scheme is to charge a winning bidder according to the highest value bid that it u blocks here highest value should be suitably normalized by bid size to reflect the way the los algorithm chooses its ordering precisely the los prices are as follows if the bidder i loses or if its bid wins but u blocks no other bid then pi otherwise suppose i bid is bi bi and let bj bj be the first bid in the los greedy ordering that i bid u blocks set p bj bj bi by the los mechanism we mean the ca that uses the wd algorithm of subsection followed by the above charging scheme individual rationality is almost immediate proposition truthtelling bidders always obtain nonnegative utility in the los mech anism proof we need to show that the price pi charged to a winning bidder i is at most its bid bi let bj bj be the first bid that bi bi u blocks if there is no such bid then pi and there nothing to prove since bj bj must follow bi bi in the los ordering bi bj i j rearranging gives bi pi as desired strategyproofness is much less obvious theorem the los mechanism is strategyproof again we leave it to the reader to investigate the extent to which the los mechanism is strongly truthful in the sense of proposition our first step in proving theorem is to show that bidders have no incentive to lie about their desired sets the ai lemma if a player i can benefit in the los mechanism from a false bid bi bi then it can benefit from such a bid in which bi ai proof suppose there is a player i and a set of bids bj bj j i for the other n players such that i obtains strictly greater utility from falsely bidding bi bi than from truthfully bidding ai i by proposition this can only occur if the los mechanism grants the bid bi bi we aim to show that the false bid ai bi also leads to greater utility than the bid ai i first note that in the false bid bi bi we must have bi ai if bi is missing any items from ai then the los mechanism will never produce an outcome in which i has strictly positive utility and by proposition a truthful bid always leads to nonnegative utility so suppose bi contains ai and that the los mechanism grants the bid bi bi we can complete the proof by showing that the los mechanism would have also granted the bid ai bi and would have only charged player i a smaller price the first part of the above statement is easy to see since ai bi the bid ai bi would only be considered earlier in the greedy los ordering and would therefore be granted for the second part recall from that the price charged to player i by the los mechanism is pi bj bi bj where j is the earliest bid u blocked by i if any bidding ai instead of bi affects this price in two ways first the second term on the rhs of clearly only goes down the second trickier consequence is that the identity of the first u blocked bid could change so suppose the first bid u blocked by the bid bi bi is bj bj and that by ai bi is bk bk to rule out the possibility that there is no u blocked bid add an imaginary bid for all of the items that has zero value the final key claim which we leave as an exercise is that bk bk can only follow bj bj in the greedy los ordering this implies that bidding ai instead of bi can only decrease the first term on the rhs of and completes the proof exercise complete the proof of lemma assume that bi ai and show that if bj bj and bk bk are the first bids u blocked by the bids bi bi and ai bi respectively then bk bk can only follow bj bj in the greedy los ordering see also the proof of theorem below for a similar argument we now complete the proof of theorem proof of theorem as in the proof of lemma assume for contradiction that there is a player i and a set of bids bj bj j i for the other n players such that i obtains strictly greater utility from falsely bidding bi bi than from truthfully bidding ai i by lemma we can assume that bi ai let b i denote the set bj bj j i of other players bids bt the set b i ai i and bf the set b i ai bi by proposition we can assume that the los mechanism granted the bid ai bi given the input bf there are two cases we consider only the case where bi i and leave the other case as an exercise we can assume that the bid ai bi was granted since i bi the bid ai i would have only been considered earlier in the los ordering and thus would also have been granted suppose that bj bj is the first bid u blocked by the false bid ai bi we can complete the proof by showing that ai i does not u block any bid earlier than bj bj as then the price charged by the los mechanism on input bt for the bid ai i is at most that for the bid ai bi on the input bf suppose for contradiction that the first bid bk bk that ai i u blocks precedes bj bj in the los ordering by the definition of u blocking removing the bid ai i from bt and rerunning the los algorithm on the input b i causes the bid bk bk to be granted a key observation is this if ai bi follows bk bk in the los ordering bk bk would also be granted by the los algorithm on the input bf this holds because the los algorithm makes identical decisions on the inputs b i and bf until the point that the bid ai bi is considered in the latter execution since exercise b implies that ai and bk must have at least one item in common and since the bid ai bi is granted by the los algorithm given the input bf this observation implies that ai bi precedes bk bk in the los ordering but then ai bi u blocks bk bk contradicting the assumption that bk bk precedes the first bid bj bj u blocked by ai bi exercise complete the proof of theorem show that if bi ai is a winning bid and bi i then player i utility would have been at least as large had it bid i ai exercise suppose we modify the los mechanism so that the price pi charged for a winning bid bi bi is given by but where the bid bj bj is defined as the first bid blocked by bi bi the first denied bid after bi bi with bi bj does this result in a strategyproof mechanism exercise recall from exercise that the los wd algorithm can be extended to a family of greedy algorithms parametrized by p can all of these wd algorithms be extended to truthful mechanisms via appropriate pricing schemes what about for other classes of greedy criteria e g ordering bids according to i f ai where f is a more general nondecreasing function of set size summary this section studied the los ca for single minded bidders on the plus side this is our first poly time ca for valuations that can have some degree of complements or substitutes in this case a restricted form of complements on the minus side the valuations can have only a very restricted form and the ca guarantees only a relatively weak o m approximation of the maximum surplus in terms of our guiding desiderata from subsection the los ca achieves incentive compatibility and computational tractability while making serious concessions to economic efficiency and valuation generality we have already seen corollary that the trade off between economic efficiency and compu tational tractability is fundamental even for single minded bidders and even ignoring incen tive compatibility the next section shows that even a weaker notion of ca tractability poly time communication and unbounded computation leads to a fundamental trade off between economic efficiency and valuation generality communication complexity of cas last section restricted attention to single minded bidders in part to eliminate communication difficulties and focus on the computational complexity of winner determination this section returns to general valuations where all we know about each valuation vi is that vi and that vi vi whenever and shines the spotlight squarely on communication issues intuitively since a general valuation has an exponential number of free parameters we don t expect to achieve a reasonable allocation in all cases while examining only a polynomial number of them to make this precise we consider the following model of computation see for an overview of the various standard models players participate in a protocol decided upon in advance at each step of the protocol one of the players transmits a bit which is seen by all players crucially the bit transmitted by a player can only depend on its own private information and the protocol history so far i e who transmitted what the communication complexity of a protocol is the worst case number of bits that are transmitted over all possible private inputs of the players the key point to take away from this definition is how powerful the model of computation is in addition to dispensing with any incentive constraints which we will do for this entire section unlimited computation by the players is permitted while the point of this model is lower bounds which are only more compelling in such an unrealistically strong model let develop some intuition by examining some positive results first observe that winner determination with single minded bidders is trivially solvable with a polynomial amount of communication the following protocol works each player broadcasts their private set and value in some predetermined order recall we ignore incentive constraints and each player uses these to compute an optimal solution in a consistent way this is an np hard problem but recall we allow unbounded computation second the los algorithm can be used to achieve a non trivial approximation guarantee with polynomial communication even for general valuations in this model of computation the idea is to conceptually treat a single player with a general valuation vi as different single minded players one single minded player for each bundle t s with inherited valuation vi t to prevent different sub players corresponding to a single original player from simultaneously getting their bundles granted we add one dummy good for each original player i we then supplement the desired set of each of i sub players with this dummy good this ensures that every feasible allocation with the sub players and the dummy goods maps naturally to a feasible allocation of the original instance with the same surplus we have shown how to reduce surplus maximization with n players with general valua tions and m goods to surplus maximization with single minded players and m n goods solving the latter single minded instance by brute force as above would require commu nication exponential in one of the original parameters of interest namely m running the los algorithm directly on the single minded instance suffers the same problem we can simulate the decisions that the los algorithm would make on the single minded instance using only polynomial in n and m communication as follows we define a protocol that works directly on the original instance with n players and m goods the protocol proceeds in rounds all players are initially active and all goods are initially unallocated in each round each active player i broadcasts the bundle t of unallocated goods that i by the player but remember this is permitted all players see all proposed bundles and the one that maximizes vi t t over active players i is understood by all of the players to be allocated the winning player i deactivates itself and the goods in its bundle t are understood by all players to now be allocated the protocol terminates once all players are inactive intuitively each round of the protocol is executing a two stage tournament to identify the bundle that would next be selected by the los algorithm on the induced single minded instance in the first stage each original player runs a tournament to elect the most viable candidate from its induced single minded players this can be done privately without any communication and the second round elects a final winner from the polynomially many candidates that survive the first stage exercise prove that the allocation decisions made by the above protocol for the original instance are isomorphic to those that the los winner determination algorithm would make on the induced single minded instance and therefore it achieves an o m approximation of the surplus the main result in this section is a matching lower bound theorem for every there is no polynomial communication o m e approximation for the general winner determination problem this lower bound is unconditional in that it doesn t depend on any complexity theoretic assumptions like p np it can be extended to cover randomized and nondeterminis tic protocols and similar proof techniques also yield sometimes weaker lower bounds for various restricted classes of valuations see for further details and references at the highest level the proof of theorem is not unlike the familiar argument that comparison based sorting requires  n log n comparisons an algorithm that employs only k comparisons generates at most distinct executions and n different executions are needed to correctly distinguish the n ordinally distinct possible inputs recall n  n log n the proof of theorem needs two additional ideas first the structure of the private information implies that sets of inputs that generate identical protocol transcripts satisfy a natural closure property second to prove the strong approximation lower bound of  m e we require some neat combinatorics to generate winner determination instances that admit either a high surplus feasible solution or only very low surplus solutions the first point is simple consider a protocol and let xi denote the set of possible pri vate inputs of player i e g possible valuations suppose there are two inputs xn and yn for which the communication transcripts of the protocol i e who sent what bits when are identical now consider the mixed input xn by in duction on the rounds of the protocol player cannot distinguish between the inputs xn and yn and the other players cannot distinguish be tween the inputs xn and xn as part of this induction we see that the communication transcript of the protocol on the input xn matches that of xn and yn similarly all mixed versions of xn and yn generate identical communication transcripts this implies that a set of inputs with a common communication transcript form a box meaning a subset a of xn that arises as a product a an for some ai xi for each i lemma every protocol partitions the set x xn of possible inputs into boxes over which its communication transcript is invariant for the winner determination problem a protocol with communication complexity k parti tions the set of valuations into at most boxes and in each box executes identically in particular a common allocation is produced for all inputs in the same box the heart of the proof of theorem is to show that if k is too small i e polynomial then very different looking inputs wind up in a common box and no common allocation can be simultaneously near optimal for both of them to construct a useful family of different looking valuations fix a set s of m goods and a set of n  m e players we first consider the following thought experiment make t different copies of the goods s called st where t is a parameter we choose below randomly partition each sj into n classes one per player i e assign each good of sj independently and uniformly at random to one of the classes sj sj obviously n two differ classes in the same copy sj contain disjoint subsets of the original set s of goods what about two classes sj s belonging to different copies j  for each original good of s there is a probability that it is assigned to both sj and s thus for fixed h i i h j m m and j  the probability that si and sh wind up disjoint is n e note that under our assumption that n  m e this probability is exponentially small indeed by a union bound the probability that there is any pair of sets sj s with j  i h and no good of s in common is less than m thus even when t em n there is a positive probability that every pair sj s of classes with j  overlaps ergo such a collection of t partitions of the goods s exists we fix one sj arbitrarily for the rest of the proof why is this construction useful to gain intuition suppose each bidder i was single minded and wanted the bundle with value then we can allocate all desired bundles to all bidders without conflict and enjoy surplus n  m e if on the other hand each bidder wants a bundle that corresponds to a different copy of the goods we can only obtain surplus recall every pair of classes from different partitions has at least one good of s in common thus this collection of t highly overlapping partitions of s generates winner determination instances with both very high optimal surplus and very low optimal surplus we now give the general argument and prove theorem we first describe the set of valuations that we use let bi t be a bit string of length t associate these t bits with the t partitions of s above interpret the ones of bi as the partitions in which player i is interested and the zeros as the partitions in which it is uninterested the string bi induces a valuation as follows for every copy sj in which i is interested player i has value for the bundle sj the player also has value for supersets of such bundles and value for everything else let xi denote the set of valuations of this form the set x xn of inputs induces a family of winner determination problems consider an input of x which we can uniquely associate with bit strings bn call an instance good if there is an index h such that for every player i the hth bit of bi is i e all players are interested in the hth partition as above a good instance admits a feasible solution with surplus n  m e in which each player gets its bundle corresponding to the hth partition at the other extreme call an instance bad if there is at most one player interested in each partition i e the sets of indices for the ones in bn are mutually disjoint since all pairs of bundles drawn from different partitions intersect the maximum possible surplus in a bad instance is of course there are plenty of instances that are neither good nor bad finally consider a k bit protocol that achieves a better than n approximation for every winner determination problem in x by lemma this protocol partitions x into at most boxes over which the protocol has constant behavior and in particular a constant output by the definition of good and bad instances and the assumption that the protocol is better than an n approximation algorithm good and bad instances cannot intermingle in a common box crucially this restricts the number of bad instances that a single box can contain to see why consider a box a an of x recall lemma that contains no good instances we claim that for each partition s of the goods there is a totally uninterested player i a player i who across all of its valuations in ai never wants its bundle sj from the jth partition for otherwise there is a partition sj and for each player i a valuation vi ai such that when i has this valuation it would happily accept its bundle from the jth partition but then the input vn belongs to this box by the closure property of boxes and by definition is a good instance so the claim is true but why does it imply an upper bound on the bad instance population of a box with no good instances the total number of bad instances is precisely n t with each arising uniquely by choosing for each of the t partitions which if any one of the n players is interested in it within a box with no good instance each bad instance arises a choice one per partition of which if any of the n players other than the necessarily present totally uninterested one is interested in it this gives an upper bound of nt on the number of bad instances per box wrapping up the n t bad instances must be distributed across at least n t nt t different boxes this implies that the communication complexity k of the protocol satisfies t taking logs and using that log x x for small x we find that k t n since t is exponential in m recall so is k summarizing then every protocol with approximation factor o min n m e uses exponential communication references e h clarke multipart pricing of public goods public choice 33 t groves incentives in teams econometrica j hastad clique is hard to approximate within e acta mathematica e kushilevitz and n nisan communication complexity cambridge university press d lehmann l i o callaghan and y shoham truth revelation in approximately efficient combinatorial auctions journal of the acm preliminary version in ec n nisan the communication complexity of approximate set packing in proceedings of the annual international colloquium on automata languages and programming icalp volume of lecture notes in computer science pages n nisan introduction to mechanism design for computer scientists in n nisan t roughgarden e tardos and v vazirani editors algorithmic game theory chap ter pages cambridge university press n nisan and a ronen computationally feasible vcg mechanisms journal of arti ficial intelligence research t sandholm algorithm for optimal winner determination in combinatorial auctions artificial intelligence i segal the communication requirements of combinatorial allocation problems in p cramton y shoham and r steinberg editors combinatorial auctions chapter mit press w vickrey counterspeculation auctions and competitive sealed tenders journal of finance 37 schedule monday august class read chapter quiz at the start of class lecture standard game theory solution concepts wednesday august class read chapter and chapter quiz at the start of class which will concentrate on the advanced material section homework chapter problems and lecture proof of existence of mixed nash equilibrium via brouwer fixed point and sperner lemmas source endofline and ppadcompleteness approximate nash in ppad friday august class read chapter quiz covering this at the start of class skim overview of ppadcompleteness of nash join the google group https groups google com forum forum homework chapter problem chapter problem start perusing papers from the acm conference on economics and computation ec in and your goal is to find a few theoretically oriented papers with nontrivial proofs that are related to the course and that you find interesting post your first choice of paper to give a talk on to https groups google com forum forum which you will have to join include the following information title authors and abstract which all should be cut and paste if you don t think the abstract is sufficiently descriptive another paragraph elaborating on the contributions of the paper a paragraph explaining what you think the paper is interesting and a one sentence statement of what you think the take away message is for the paper before you post check that no one else has picked that paper so first come first served lecture nphardness of source hints of the proof of ppadhardness of nash wednesday september class read chapter quiz covering this at the start of class homework chapter problem lecture evolutionary stable strategies using the hawkdove game as a running example friday september class cs fall http people cs pitt edu kirk index html read chapter and chapter except for the advanced material section quiz covering this at the start of class skim the advanced material section homework chapter problems and lecture advanced material section of chapter bound on the price of anarchy primary source secondary source and example of what slides for a talk in this area might look like monday september class read chapter except for the advanced material section quiz covering this at the start of class skim the advanced material section homework chapter problem chapter problem lecture advanced material section of chapter wednesday september class read chapter except for the advanced material section quiz covering this at the start of class skim the advanced material section homework chapter problem deadline for picking first ec paper to speak about lecture advanced material from chapter friday september class read chapter up through section quiz covering this at the start of class skim chapter homework chapter problem give a formal proof for part b if you can reasonably do so lecture section an algorithm for constructing market clearing pricing monday september class read chapter quiz covering this at the start of class homework chapter problem deadline for picking second ec paper to speaker about post your choice of paper to give a talk on to https groups google com forum forum which you will have to join include the following information title authors and abstract which all should be cut and paste if you don t think the abstract is sufficiently descriptive another paragraph elaborating on the contributions of the paper a paragraph explaining what you think the paper is interesting and a one sentence statement of what you think the take away message is for the paper before you post check that no one else has picked that paper so first come first served lecture finding market clearing prices via convex programming source top trading cycle one sided preference algorithm and galeshapley two sided preference matching algorithm notes wednesday september class read chapter except for advanced material section quiz covering this at the start of class skim the advanced material section homework chapter problem cs fall http people cs pitt edu kirk index html lecture advanced material section a gametheoretic derivation of the nashbargaining solution friday september class read chapter and chapter except for the advanced material section of chapter quiz covering this at the start of class skim the advanced material section homework chapter problem lecture advanced material section of chapter spectral analysis of hubs and authorities and pagerank and relation of pagerank to random walks monday september class read chapter sections and quiz covering this at the start of class skim sections and homework chapter problem chapter problem deadline for preparation of presentation powerpoint or whatever of your first minute talk lecture sections and vcg and proof that it is a truthful for combinatorial auctions los mechanism greedy approximation algorithm for assignment and truthful analysis for singleminded bidders combinatorial auctions notes i believe originally from tim roughgarden wednesday september class read chapter sections to quiz covering this at the start of class skim the advanced material section homework chapter problem lecture advanced material section of chapter proof vcg gives market clearing prices friday september class read chapter through section quiz covering this at the start of class skim chapter homework chapter problem lecture chapter on information cascades and bayes rule monday september class read chapter except for advanced material section quiz covering this at the start of class skim the advanced material section homework chapter problem lecture advanced material section analysis of richgetricher process wednesday october class read chapter except for advanced material section quiz covering this at the start of class skim the advanced material section homework chapter problem chapter problem lecture advanced material section analysis of cascade capacity friday october class read chapter except for advanced material section quiz covering this at the start of class skim the advanced material section homework chapter problem cs fall http people cs pitt edu kirk index html lecture advanced material section analysis of decentralized search monday october class read chapter except for advanced material section quiz covering this at the start of class skim the advanced material section deadline for preparation of presentation powerpoint or whatever of your second minute talk homework chapter problem lecture advanced material section analysis of branching and coalescent processes wednesday october class read chapter except for advanced material section quiz covering this at the start of class skim the advanced material section homework chapter problem lecture advanced material section wealth dynamics in markets friday october class read chapter up through section inclusive quiz covering this at the start of class skim the advanced material section homework chapter problem lecture advanced material section proof of arrow impossibility theorem tuesday october class read chapter from through inclusive chapter quiz covering this at the start of class homework chapter problem chapter problem lecture statement and proof of gibbardsatterthwaite theorem source wednesday october no class friday october no class monday october class neal talk mike talk wednesday october class salim talk vineet talk friday october class ben talk zach talk monday october class neal talk mike talk wednesday october class salim talk vineet talk friday october no class monday november class disclaimer these notes have not been subjected to the usual scrutiny reserved for formal publications they may be distributed outside this class only with the permission of the instructor overview we begin by looking at a set of theorems from various disciplines and how they relate to one another from combinatorics we take sperner lemma which we can use to prove brouwer fixed point theorem from topology brouwer fixed point theorem can be used to prove the arrow debreu theorem from economics which states that general equilibria exist and can also be used to prove kakutani fixed point theorem kakutani fixed point theorem can be used to prove that nash equilibria exist for all games a graph illustrating how these theorems can be used to prove each other is given in figure sperner lemma here we consider an example application of sperner lemma to a simplex in two dimensions though the lemma can be generalized to higher dimensions take the following steps to set up the problem see figure triangulate the simplex so that it divided into smaller triangles number the corners of the simplex and label each of the vertices in the triangulation with either or subject to the following rules a vertex on a side of the simplex cannot be assigned the same number as the corner of the simplex opposite that side and vertices inside the simplex can be labelled with any of the numbers lemma sperner lemma if you label the vertices as described above you will always have a small triangle somewhere in the simplex with its vertices labelled moreover you will have an odd number of such triangles proof to prove sperner lemma we trace a path through the simplex that is guaranteed to end in a triangle in order to make the proof cleaner we first add some triangles to the side of the simplex see figure we begin outside the simplex and enter the simplex using the boundary edge thus we enter a triangle that has at least one edge at any point in the trace if we are not in a triangle the triangle has exactly two edges we continue our trace by crossing the other edge we cannot ever re enter any triangle since a triangle has at most two edges also we cannot exit the figure graph showing which theorems can be used to prove others figure example triangulated and labelled simplex figure example solution a c b figure labelling scheme simplex because there is only one boundary edge since the triangulation has finitely many triangles our trace must end at a traingle with exactly one edge which has to be a using sperner lemma we can now prove brouwer fixed point theorem brouwer fixed point theorem take an equilateral triangle a two dimensional simplex and perform some transformation such that all points on the modified triangle lie somewhere within the boundary of the original triangle some possible transformations are rotating the triangle by some multiple of  flipping the triangle along a bisector or shrinking the triangle this does not include translating the triangle because some of the translated points would not lie within the boundary of the original after any such transformation  brouwer theorem says that there is a point x that is in the same position as it was in the original simplex theorem brouwer fixed point theorem let s be any n dimensional simplex and let  s s be any continuous function then  has a fixed point i e x s such that  x x proof we prove the theorem here for the special case where the simplex s is an equilateral triangle in an essentially similar argument applies to the general case the proof is a direct application of sperner lemma let tn be a sequence of succesively finer triangulations for example diameter tn n we define labellings ln for the triangulations as follows for a vertex x in the triangulation consider the vector from x to  x extend this vector until it meets a boundary of the simplex if it crosses consider each triangle in the triangulation as a room in a castle and a edge as a door note that the castle has only one entry exit and each room has at most two doors what the argument above says is that if we enter the castle and move through the rooms our path should end in a room with exactly one door figure graph of two people utilities and one possible pareto point edge ab resp bc ca we label x as resp we break ties arbitrarily it is easy to see that under this labelling for any  s s the vertices of the triangle get labels respectively no vertex on the edge ab resp bc and ca gets a label resp and thus li is a legal sperner labelling for trianglulation ti thus by sperner lemma there is a triangle say ti in ti let mi be the centroid of triangle ti consider the sequence of points mi this is a infinite sequence in s and thus has a subsequence xi that converges to a point x s thus x is arbitrary close to the centroid of some triangle we claim that x is a fixed point of  this is so because if  x is different from x we can find a small triangle containing x but not  x since  is continuous this triangle cannot be labelled for a more rigorous proof the reader is referred to appendix a arrow debreu theorem general equilibria exist to frame the problem consider a marketplace with n agents a k different commodities each agent i comes to the market place with an endowment which we represent as a vector of goods or services ei rk every agent i has a utility function ui rk r which describes how much utility agent i gets from various amounts of the k goods the utility function may be arbitrary and it is not necessarily linear also we assume that the goods are infinitely divisible when agents come to the market they want to exchange goods with other agents such that they improve their overall utility there is a point in this exchange known as the pareto point where neither agent can improve their utility through further exchanges see figure the question is how should exchanges be carried out to maximize the agents utilities and at what prices it possible that the necessary sequence of exchanges is complicated or creates a cycle and searching for an optimal exchange sequence is computationally intractible instead we could have everyone announce their this is a consequence of the bolzano wierstrass theorem e g see royden page utility functions and try to solve for an optimum but then we have no way of knowing whether people are being honest about their utilities a solution is to use set prices for each commodity pj r which for now we will assume are god given with prices in hand there is no longer a dilemma each agent buys a vector of goods xi that maximizes ui xi subject to p xi p ei where p xi is the amount the agent pays for the goods she wants and p ei is the amount the agent received for her original endowment this can be solved for xi p the optimal purchase of goods for agent i when prices are p we assume that xi p is continuous and has a unique optimum at some point the market may clear meaning that all the goods for sale have been purchased the total amounts of each commodity demanded by all agents is described by x p n xi p and the market clears if where e n x p e ei is the total amount of goods brought to the market theorem arrow debreu theorem there is always a price p such that x p e i e one can always find a price that clears the market such a price is a general equilibrium this price is also a pareto point note while the price equilibrium is unique there may be several pareto points depending upon the agents initial endowments surprisingly finding p can be done with remarkably little communication consider the simplex of all prices pj j if the price is such that there is excess demand then the price should be increased by the amount that demand exceeds the endowment assuming no inflation we can normalize all prices such that they add up to one  p pj max xj ej normalization constant either the endowment is greater than or equal to what the agents want in which case the price stays the same or the price is increased by the difference between what the agents want and the total endowment this transformation maps a vector of prices to another point in the simplex and brouwer fixed point theorem says that there must always be a fixed point to such a tranformation call the fixed point p claim at fixed point p for every product j xj ej proof let d x e let dij max dj we want to show that dji for all commodities j we have from  p p di k for some normalization constant k moreover adding equations for all agents i we get p x p e i e p d since p is a fixed point  p p taking dot product of equation with d we get p d k p di d rearranging and noting that di d di d we get di di k p d thus di must be zero nash equilibria exist the proof of nash theorem requires the use of another fixed point theorem more general than the brouwer fixed point theorem theorem kakutani fixed point theorem let  s be any convex valued function such that for any sequence xi yi converging to x y if i n yi  xi then y  x this property is called graph continuity then  has a fixed point i e x s such that x  x proof we give here a proof sketch for the d simplex case as in the proof of brouwer theorem we consider a sequence of succesively finer triangulations tn for a triangulation ti we define a function i as follows for a vertex x of the triangulation i x is set to some y  x chosen arbitrarily for any other point x s i x is defined by linear interpolation in the triangle containing x thus i is a continuous map by brouwer fixed point theorem i has a fixed point x i now consider the sequence x i it has a convergent subsequence that converges to some point x s using the graph continuity of  it can be shown that x  x for a more rigorous proof of kakutani fixed point theorem the reader is referred to for any game a nash equilibrium suggests that none of the players has an advantage in changing his strategy without the other players changing their strategy as well more formally definition let and be the set of possible strategies for player and respectievely let r resp be the payoff function for player resp player then a tuple is said to be in nash equilibrium if while the above definition captures all pure strategies it does not consider the possibility of mixed strategies i e strategies where a player chooses a move s randomly according to some distribution  on s when we allow mixed strategies we assume that each player tries to maximize his expected payoff thus if player i chooses from a distribution i on si each player tries to maximize pi pi s2 a tuple of mixed strategies is said to be in mixed nash equilibrium if prob distributions on prob distributions on while for many games it is the case that no pure nash equilibria exist mixed nash equilibria always exist theorem nash theorem every game has a mixed strategy nash equilibrium to prove nash theorem let be the set of all mixed strategies d that maximize i1 for a mixed strategy d of player similarly define the set now let  2 it is easy to see that  d as defined above is convex valued and graph continuous hence by kakutani fixed point theorem  has a fixed point   this by definition is a mixed nash equilibrium overview of course topics a brief overview of the topics for the semester background on equilibrium theorems notions of equilibria nash equilibria always exist but often there are too many of them much of game theory has been a critique of nash equilibria games played by automata to explain people strategies in prisoner dilemma we need a more refined theory perhaps repetition and reputation are the the key if we consider resource bounds then automata become relevant to game theory people don t have infinite reasoning capacity and we can model them using automata with limited states evolutionary game theory discuss john maynard smith talk about game theory in the context of evolution finding an evolu tionarily stable strategy e g tit for tat mechanism design inverse game theory given desired outcomes design a game so that agents acting rationally will behave in a desired way an example is the vickero auction where the highest bidder is the winner but the winner pays the amount of the second highest bid fairness auctions including combinatorial auctions price of anarchy take a network where it is possible to control what route each packet takes to its destination if a central authority were to determine an optimal routing policy given all the information in the network the improvement in performance would be at most twice over a system where each agent optimizes its own packets references h l royden real analysis edition prentice hall s kakutani a generalization of brouwer fixed point theorem duke mathematical journal pp appendix a details in proof of brouwer theorem theorem consider the labelling defined in theroem let x be the limit of a sequence xi of centroid of a sequence of triangles with diameter converging to then x is a fixed point of  proof assume the contrary i e  x x then t l x xl let e t for some k since  is continuous and e there exists  such that y n x   y n  x e let ei min e  since xi converges to x n such that n xn n x ei further since the diameter of the triangulations converges to zero n such that n diameter tn ei thus for n max a triangle tn lies wholly inside n x n x  thus by the continuity condition above each of the three vertices a b c of the triangle maps to some point in n  x e outside the triangle since the triangle is a triangle the vectors  a a  b b  c c span an angle of at least  by our labelling scheme however for k large enough the angle spanned can be made arbitrarily small see figure b which is a contradiction hence the claim b figure a proof details b angle spanned by vectors should be small economics letters the gibbard satterthwaite theorem a simple proof jean pierre beno t department of economics and school of law new york university washington square south new york ny usa received january accepted april abstract a simple proof of the gibbard satterthwaite theorem is provided elsevier science s a all rights reserved keywords gibbard satterthwaite manipulability jel classification the classic gibbard satterthwaite theorem gibbard satterthwaite states essentially that a dictatorship is the only non manipulable voting mechanism this theorem is intimately connected to arrow impossibility theorem in this note i provide a simple proof of the theorem adapting an idea used by geanakoplos in one of his simple proofs of arrow theorem consider a set of individuals n with unu n and a set of alternatives m with umu m we assume that each person has a strict linear preference ordering over the alternatives a social choice function selects a single alternative on the basis of the reported preference profile of the individuals the rule is unanimous if it always selects an alternative that is top ranked by everyone it is strategyproof if it is a dominant strategy for an individual with any permissible ranking to truthfully report her preferences theorem gibbard satterthwaite suppose there are at least three alternatives and that for each tel fax e mail address j p beno t reny further emphasizes the connection between the gibbard satterthwaite theorem and arrow theorem by ingeniously providing a shared simple proof for both results his paper was written independently of this one but was also inspired by geanakoplos sen provides a direct proof that does not require the universal domain of preferences see also barbera for an earlier relatively simple proof see front matter elsevier science s a all rights reserved pii j p benot economics letters individual any strict ranking of these alternatives is permissible then the only unanimous strategy proof social choice function is a dictatorship proof suppose we are given a unanimous strategy proof social choice function we show that one of the voters must be a dictator we first observe the following fact fact suppose that alternative a is selected given some preference profile modify the profile by raising some alternative x in individual i ranking holding everything else fixed then either a or x is now selected suppose instead that when x rises some alternative c different from a and x is chosen then if i prefers a to c he would not report the change whereas if he prefers c to a he would have falsely reported this change earlier step we begin with an arbitrary strict profile in which everyone ranks b last alternative b is not selected or else by strategyproofness it would still have to be selected if the voters one at a time raised a to the top of their profiles contradicting unanimity now starting with individual and continuing in order with the other voters one at a time have b jump from the bottom of each ranking to the top leaving the other relative rankings in place let r be the pivotal individual for whom the jump causes b to be selected that is with the profile profile b is not selected whereas with the profile profile b is selected consider profile where b is selected alternative b must still be selected if any player i r submits a different ranking or else i would misrepresent also b must still be selected if any player i r submits a different ranking with b still ranked first or else i would not honestly report this ranking considering the players one at a time we have b is chosen whenever the first r players rank b first bd consider profile where b is not chosen alternative b must still not be chosen if any player j p benot economics letters i r submits a different ranking or else i would do just that also b must still not be chosen if any player i r submits a different ranking with b ranked last or else i would not honestly report this ranking considering the players one at a time we have b is not selected whenever voters r through n rank alternative b last not bd we will show that the pivotal individual r is in fact a dictator step profile first raise k to the top position for all voters by unanimity k is chosen now raise b to the top for voters through r one at a time leaving b on the bottom for the remaining voters profile by not bd alternative b is not chosen so by fact alternative k is still chosen finally raise b to the second position for voter r alternative k is still chosen or else r would not report this change that is k is chosen with the profile profile now reconsider profile and suppose g k is chosen raise b to the top position for the first r voters one at a time by not bd b is not chosen so by fact g is still chosen now raise b in voter r profile to the second position profile suppose b is not selected by fact alternative g is still chosen from bd alternative b will be selected when it is raised to the top of r profile hence voter r should now falsely report this j p benot economics letters preference since she prefers b to g k contradicting strategy proofness therefore with profile alternative b is selected now raise k to the second position for voters through r and the first position for voters r through n alternative b is still chosen or else the first group of voters would not truthfully report this change whereas the second group would have misreported it but this modified profile is the same as profile where k is chosen a contradiction so that g k cannot be chosen with profile step now consider an arbitrary profile in which r ranks some alternative k b on top first modify the profile by dropping b to the bottom for all voters from step alternative k is chosen now restore b to its initial position for all voters one at a time by fact either k or b is chosen now consider r r r n b b b b b a a a profile where c b and c k similarly to step have c jump in the rankings of the voters one at a time until we discover the pivotal voter m for alternative c symmetrically to step this pivotal voter has his top choice selected with profile on the other hand from bd we also know that with profile alternative b is chosen hence the pivot m must be such that m r but a symmetric argument beginning with m and then finding r shows that r m so m r and voter r is pivotal with respect to c as well as b thus in addition to knowing that k or b is chosen for our initial arbitrary profile we have that either k or c is chosen since c b alternative k is chosen finally if k b a similar argument shows that voter r is pivotal for a as well as c and that b is selected hence r is a dictator gibbard satterthwaite theorem from wikipedia the free encyclopedia the gibbard satterthwaite theorem named after allan gibbard and mark satterthwaite is a result about the deterministic voting systems that choose a single winner using only ballots from voters with a finite number of possible ballot types the gibbard satterthwaite theorem states that for three or more candidates one of the following three things must hold for every voting rule the rule is dictatorial i e there is a single individual who can choose the winner or there is some candidate who can never win under the rule or the rule is susceptible to tactical voting in the sense that there are conditions under which a voter with full knowledge of how the other voters are to vote and of the rule being used would have an incentive to vote in a manner that does not reflect his or her preferences rules that forbid particular eligible candidates from winning or are dictatorial are defective hence every deterministic voting system that selects a single winner either is manipulable or does not meet the preconditions of the theorem the theorem does not apply to randomized voting systems such as the system that chooses a voter randomly and selects the first choice of that voter contents definitions formal statement proof related results history references external links definitions a socialchoicefunction is a function that maps a set of individual preferences to a social outcome an example function is the plurality function which says choose the outcome that is the preferred outcome of the largest number of voters we denote a social choice function by and its recommended outcome given a set of preferences by a socialchoice function is called manipulable by player if there is a scenario in which player can gain by reporting untrue preferences i e if the player reports the true preferences then if the player reports untrue preferences then and player prefers to a socialchoice function is called incentivecompatible if it is not manipulable by any player a socialchoice function is called monotone if whenever the following is true when has some preferences when has other preferences gibbard satterthwaite theorem wikipedia https en wikipedia org wiki gibbard then under the preferences player prefers outcome and under the preferences player prefers outcome it can be demonstrated that incentivecompatibility and monotonicity are equivalent for example when there are only two possible outcomes the majority rule is incentivecompatible and monotone when a player switches his preference from one option to the other this can only be better for the other option a player is called a dictator in a socialchoice function if always selects the outcome that player prefers over all other outcomes is called a dictatorship if there is a player who is a dictator in it formal statement if is incentivecompatible and returns at least three different outcomes then is a dictatorship proof the gs theorem can be proved based on arrow impossibility theorem arrow impossibility theorem is a similar theorem that deals with social ranking functions voting systems designed to yield a complete preference order of the candidates rather than simply choosing a winner given a social choice function it is possible to build a social ranking function as follows in order to decide whether the function creates new preferences in which and are moved to the top of all voters preferences then examines whether chooses or it is possible to prove that if is incentivecompatible and not a dictatorship then satisfies the properties unanimity and independenceofirrelevantalternatives and it is not a dictatorship arrow impossibility theorem says that when there are three or more alternatives such a function cannot exist hence such a function also cannot exist in this lecture we are going to talk about the complexity of finding a nash equilibrium in particular the class of problems np is introduced and several important subclasses are identified most importantly we are going to prove that finding a nash equilibrium is ppad complete defined in section for an expository article on the topic see and for a more detailed account see computational complexity for us the best way to think about computational complexity is that it is about search problems the two most important classes of search problems are p and np the complexity class np np stands for non deterministic polynomial it is a class of problems that are at the core of complexity theory the classical definition is in terms of yes no problems here we are concerned with the search problem form of the definition definition complexity class np the class of all search problems a search problem a is a binary predicate a x y that is eciently in polynomial time computable and balanced the length of x and y do not dier exponentially intuitively x is an instance of the problem and y is a solution the search problem for a is this given x find y such that a x y or if no such y exists say no the class of all search problems is called np examples of np problems include the following two sat sat  x given a boolean formula  in conjunctive normal form cnf find a truth assignment x which satisfies  or say no if none exists nash nash g x y given a game g find mixed strategies x y such that x y is a nash equilibrium of g or say no if none exists nash is in np since for a given set of mixed strategies one can always eciently check if the conditions of a nash equilibrium hold or not note however that a big dierence between sat and nash is that we know that the latter always has a solution by nash theorem completeness and reduction sat is actually np complete which informally means that it is a the hardest search problem the concept of reduction makes this notion precise and helps us to see the relations of dierent problems in terms of complexity definition reduction we say problem a reduces to problem b if there exist two functions f and g mapping strings to strings such that f and g are eciently computable functions i e in polynomial time in the length of the input string if x is an instance of a then f x is an instance of b such that x is a no instance for problem a if and only if f x is a no instance for problem b b f x y a x g y the point is that we indirectly solve problem a by reducing it to problem b we take an instance of a compute f x which we feed into our algorithm for problem b and finally we transform the solution for problem b back to a solution for problem a using function g thus if a reduces to b then b is at least as hard as a definition np completeness a problem in np is np complete if all problems in np reduce to it it can be shown that sat is np complete however nobody believes that nash is np complete the reason for this is that nash always has a solution which we know from nash theorem and np complete problems draw their hardness from the possibility that a solution might not exist for problems that are np complete the yes instances have certificates solutions verifiable in polynomial time but the no instances most likely do not it is very unlikely that there is a reduction from sat to nash since if we have an instance of sat which is unsatisfiable the reduction gives us an instance of nash such that some solution to this nash instance implies that the sat instance has no solution despite the fact that nash is very unlikely to be np complete there are many variants of nash in which the existence obstacle is not there that are indeed np complete for example if one seeks a nash equilibrium that maximizes the sum of player utilities or one that uses a given strategy with positive probability then the problem becomes np complete another variant is given a game and a nash equilibrium find another one or output no if none exist here there is no existence theorem and indeed we have the following result lemma is np complete proof to prove np completeness of we provide a reduction from to every instance of is in cnf with three literals per clause let  be an instance of consisting of n literals n m xi i and m clauses cj j we then construct a two player symmetric game where each player has m possible strategies two for each literal denoted by xi n one for each clause denoted by cj j and one extra strategy called d for default to specify the game it remains to define the payos we start by specifying the payos when at least one of the players plays d if they both play d then both of them get a payo of if one of them plays d and the other plays a clause cj j m then the player playing d gets a payo of while the other player gets if one of them plays d and the other plays a literal  then both players get a payo of consequently it is clear that d d is a pure nash equilibrium we now construct the rest of the payos such that there exists another nash equilibrium if and only if there exists a truth assignment x that satisfies the formula  we first define two generalizations of the well known rock paper scissors game grpsn generalized rock paper scissors on n objects given n objects n odd we arrange them in a cycle which defines the beating order each object loses to its left neighbor and beats its right neighbor with payos and for the winner and loser respectively playing any other pair of objects results in a tie giving a payo of to both players there is only one mixed nash equilibrium randomizing uniformly among all n objects giving each of them a weight of n bwgrpsn black and white generalized rock paper scissors on n objects we now have two colored versions of each object each one is available in black and white if the players play two dierent objects then the payo is defined exactly according to grpsn disregarding the colors however if both players play the same object but with dierent colors then both of them get executed less drastically both receive a huge negative payo finally if both play the same object and the same color then there is a tie with payo so in any nash equilibrium the two players first agree at the beginning on a coloring for each object and then randomize uniformly among all n objects with this specific coloring now in the submatrix of the payo matrix where both players play a literal let the payo matrix be given by bwgrpsn a color assignment of the objects corresponds to a truth assignment of the literals but at the end let us give each player a payo of this can be done by shifting the payos of the game from to if the payo matrix were just this submatrix then a nash equilibrium would correspond to a truth assignment and both players randomizing uniformly with weight n among these literals the average payo for both players would be if both players play a clause then let them both receive a payo of finally let us define the payos when one player plays a literal and the other plays a clause we define the payos symmetrically so let us assume that player plays literal  and player plays clause cj the payos are n if  cj i e if  falsifies cj and otherwise since each literal is played with weight n by player if played at all player receives an extra average payo of n for each literal that falsifies the clause so if all literals are falsified in a given clause then player gets an average payo of playing this clause which is more than what he she gets by playing a literal consequently if the truth assignment chosen by the players before playing the bwgrpsn game does not satisfy the original boolean formula  then randomizing uniformly on literals of this truth assignment does not give a nash equilibrium since player has an incentive to play a clause however if the chosen truth assignment does satisfy  then neither player has an incentive to deviate and we have a mixed nash equilibrium which immediately gives us a truth assignment satisfying our formula  note that every nash equilibrium other than the pure equilibrium d d must have its support in the submatrix of the payo matrix where both players play a literal since if e g player plays a clause or d then player has an incentive to play d in which case player has an incentive to play d thus leading us to the pure equilibria d d so we have shown that this game has a second nash equilibrium other than d d if and only if  is satisfiable moreover if there exists a second nash equilibrium and we can find it then we immediately have a truth assignment satisfying our formula  we note that this proof can be modified to prove the np completeness of other variants of nash does the game have at least nash equilibria does it have one in which the payos are the same does it have one in which the sum of the payos is more than how many nash equilibria are there this last one is p complete and so on tfnp a subclass of np due to the fact that nash always has a solution we are interested more generally in the class of search problems for which every instance has a solution we call this class tfnp which stands for total function non deterministic polynomial clearly nash tfnp np is nash tfnp complete probably not because tfnp probably has no complete problems intuitively because the class needs to be difined on a more solid basis than an uncheckable universal statement such as every instance has a solution however it is possible to define subclasses of tfnp which do have complete problems notice that for every problem in tfnp there is a corresponding theorem that proves that every instance of the problem has a solution the idea subdivide tfnp according to the method of proof we can distinguish several classes based on the proof first of all if the proof is constructive and can be implemented in polynomial time then the problem lies in p the empirical observation is that for every problem that lies in tfnp but has not been proven to lie in p the proof of existence of a solution uses some simple combinatorial lemma that is exponentially nonconstructive there are essentially four such known arguments if a graph has an odd degree node then it has another one this is the parity argument which gives rise to the class ppa polynomial parity argument if a directed graph has an unbalanced node a vertex with dierent in degree and out degree then it has another one this is the parity argument for directed graphs which gives rise to the class ppad which is our main focus as we will see shortly every directed acyclic graph has a sink this gives rise to the class pls polynomial local search any function mapping n elements to n elements has at least one collision this is the pigeonhole principle which gives rise to the class ppp a remark on how these classes relate to each other all of these four classes contain p are contained in tfnp and furthermore ppad is contained in both ppa and ppp the question remains how can we actually define these classes in the next section we focus specifically on the ppad class nash is ppad complete we concentrate on the class ppad because of the following main theorem theorem nash is ppad complete the rest of this section is organized as follows first we define the ppad class and also what it means to be ppad complete and then we show that nash is indeed ppad complete finally we conclude the section with some philosophical remarks the class ppad informally ppad is the class of all search problems which always have a solution and whose proof is based on the parity argument for directed graphs to formulate this in a precise way we first define the search problem end of the line the setup is the following we are given a graph g where the in degree and the out degree of each node is at most i e there are four kinds of nodes sources sinks midnodes and isolated vertices it turns out such graphs capture the full power of the argument our graph g is exponential in size since otherwise we would be able to explore the structure of the graph in particular we can identify sources and sinks eciently to be specific suppose g has vertices one for every bit string of length n how do we specify this graph since the graph is exponential in size we cannot list all edges instead the edges of g will be represented by two boolean circuits of size polynomial in n each with n input bits and n output bits the circuits are denoted p and s for potential predecessor and potential successor there is a directed edge from vertex u to vertex v if and only if v s u and u p v i e given input u s outputs v and vice versa given input v p outputs u also we assume that the specific vector has no predecessor the circuit p is so wired that p the search problem end of the line is the following given s p find a sink or another source clearly end of the line np since it is easy to check if a given node is a source or a sink moreover end of the line is a total problem i e it always has a solution so end of the line tfnp we are now ready to define the class ppad definition the class ppad the class ppad contains all search problems in tfnp that reduce to end of the line note that the phrase in tfnp is unnecessary in the definition since every problem in np that reduces to end of the line is in tfnp because reducing means that this argument can prove existence for you nash ppad our first task is to show that nash is in ppad i e that nash reduces to end of the line but we had done just this in the last lecture when we proved nash theorem we showed that nash theorem is implied by brouwer theorem which in turn is implied by sperner lemma which follows from the parity argument for directed graphs consequently if we define the search problems brouwer and sperner naturally then nash reduces to brouwer which reduces to sperner which reduces to end of the line nash is ppad complete we now show that nash is in fact ppad complete i e that end of the line reduces to nash we do this via a series of reductions we first reduce end of the line to a dimensional discrete variant of brouwer then reduce this to nash for separable network games nash for sng and finally reduce this to nash for two player games pictorially end of the line discrete brouwer nash for sng nash for two player games we now sketch these reductions similarly to for complete proofs we refer to brouwer is ppad complete we now give the main idea of reducing end of the line to a dimensional discrete version of brouwer we are given an instance of end of the line i e a pair of circuits s p that define the graph g on vertices our goal is to encode this graph in terms of a continuous easy to compute brouwer function f the domain of f will be the dimensional unit cube and the behavior of f will be defined in terms of its behavior on a very fine rectilinear mesh of grid points in the cube each grid point lies at the center of a tiny cubelet and away from these grid points the behavior of f is determined by interpolation each grid point x receives one of colors that represent the value of the dimensional displacement vectors f x x or our task is to find a panchromatic cubelet and that would be close to a fixpoint we are now ready to encode our graph g into such a brouwer function f we represent every vertex u as two small segments on two edges of the cube whose endpoints are denoted by and respectively see figure if g has an edge from vertex u to vertex v then we can create a long path in the dimensional unit cube going from to then to and and finally to and as indicated in figure we can choose the rectilinear mesh fine enough so that these lines do not cross or come close to each other now the coloring is done in such a way that most grid points get color but for every edge from vertex u to vertex v in g the corresponding long path in the unit cube connects grid points that get colors and moreover there is a circuit f that computes the displacement color of each center of a cubelet based only of the location of the cubelet f looks at the location and determines making calls to circuits s and p whether the cublet lies on the path and if so in which direction does the path traverse it and if this is the case it outputs the correct color among otherwise and if the cubelet does not lie on one of the three faces adjacent to the origin then its color displacement is importantly all colors are adjacent to each other giving an approximate fixed point only at sites that correspond to an end of the line of g from brouwer to nash we now sketch how to reduce brouwer to nash we have to simulate the brouwer function with a game the ppad complete class of brouwer functions that appear above have the property that their function f can be eciently computed using arithmetic circuits that are built up from standard operators such as addition multiplication and comparison the circuits can be written down as a data flow graph with one of these operators at every node the key is to simulate each standard operator with a game and then compose these games according to the data flow graph so that the aggregate game simulates the brouwer function and a nash equilibrium of the game corresponds to a n approximate fixed point of the brouwer function we now demonstrate how to simulate addition with a game consider the graph in figure which shows which players aect other players payos each of the four players will have two possible strategies and the payos are defined as follows if w plays strategy i e plays to the left denoted by l then the payo matrix for w based on what players x and y play is given by if w plays strategy i e plays to the right denoted by r then w gets a payo of if z plays l and a payo of if z plays r player z gets a payo of if he plays l and also w plays l otherwise he gets a payo of z wants w to look the other way in this game the mixed strategy of every player is just a number as well x is the probability that player x plays strategy y is the probability that player y plays strategy etc it turns out that in nash equilibrium we have z min x y which means that we can do arithmetic we can simulate other standard operators such as subtraction multiplication and comparison via similar games then we can put these basic games together into a big game where any nash equilibrium corresponds figure embedding the end of the line graph in a cube figure from figure the four players of the addition game and the graph showing which players aect other players payos figure from to a fixed point of the brouwer function however there is one catch our comparison simulator is brittle in the sense that it can only deal with strict inequality if the inputs are equal then it outputs anything this is a problem because our computation of f is now faulty on inputs that cause the circuit to make a comparison of equal values we can overcome this problem by computing the brouwer function at a grid of many points near the point of interest and averaging the results this makes the computation of f robust but introduces a small error in the computation of f so in fact any nash equilibrium we find corresponds to an approximate fixed point of the brouwer function so far we have shown that the nash equilibrium problem for network games is ppad complete we now show that it is possible to simulate this game with a two player game as follows we color the players the vertices of the graph by two colors red and blue so that no two players who play together have the same color this can be done because the little arithmetic games that compose the graph have this structure input output nodes who play against internal nodes and not between them assume that there are n red and n blue players we may add a few players who do nothing and n is odd how can we simulate this game with two players the idea is to have two lawyers red and blue who correspond to the red and blue players their clients the lawyers task is to then deal with the disputes described by games between people from the two sides since only players with dierent colors have disputes the lawyers can deal with the disputes amongst themselves since all clients have possible actions the lawyers have possible actions we just need to define the utilities we define them in the natural way e g ur if players and play a game and otherwise the only problem is that we have to convince the lawyers to be fair to devote equal amount of time probability mass that is to each client i e to play every game with probability n to do this we have the lawyers play on the side a high stakes grpsn game on the side means that the huge payos of the new game are added to the payos of the original lawyers game depending on which client each lawyer chooses that is the objects are the identities of the clients this allows us to make the lawyers arbitrarily close to fair this is made precise in the following lemma lemma if x y is an equilibrium in the lawyers game from above then xri xri n m i n where m is the payo of the high stakes grpsn and umax is the largest in absolute value payo in the original game so by making m large enough we can force the lawyers to be as fair as we want therefore if we can solve the lawyers game exactly then we can get an approximate equilibrium in the original game note that we need some kind of relaxation like this in the reduction since for two player games we know e g from the lemke howson algorithm that any nash equilibrium is rational i e has rational probabilities in the mixed strategies but for more players we can have irrational numbers in a mixed strategy of a nash equilibrium philosophical remarks to cut a long story short in this section we have shown that nash is intractable in the refined sense defined above what does this say about nash equilibrium as a concept of predictive behavior nash theorem is important because it is a universality result all games have a nash equilibrium in this sense it is a credible form of prediction however if finding this nash equilibrium is intractable i e there exist some specialized instances when one cannot find it in any reasonable amount of time then it loses universality and therefore loses credibility as a predictor of behavior approximate nash equilibria if finding a nash equilibrium is intractable it makes sense to ask if one can find an approximate nash equilibrium and what are the limits to this an approximate nash equilibrium informally means that every player has a low incentive to deviate more formally an  nash equilibrium is a profile of mixed strategies where any player can improve his expected payo by at most  by switching to another strategy the work of several researchers has now ruled out a fully polynomial time approximation scheme for computing approximate nash equilibria the main open question is whether there exists a polynomial time approximation scheme ptas for computing approximate nash equilibria we now present a sub exponential though super polynomial algorithm to compute an approximate nash equilibrium for two players which is due to lipton markakis and mehta the algorithm also works for games with a constant number of players their idea is the following suppose you actually knew a nash equilibrium and were able to sample from it playing t games the sample strategies for the two players are xt and yt respectively these samples are unit vectors representing the strategies from these t samples we can then form mixed strategies for the two players by averaging the samples x t xt y t yt the following lemma then tells us that the set of mixed strategies x y is close to a nash equilibrium lemma let t log n and define x and y as above then x y is an  approximate nash equilibrium with high probability the form of t in the lemma follows from the cherno bound however there is a catch to this procedure this does not tell us how to find this approximate nash equilibrium since we are not able to sample from the nash equilibrium in that case we would already be done the solution to this problem is based on the observation that any such pair of mixed strategies consists of vectors where the entries have denominators equal to t so we can simply go through all possible vectors with denominators equal to t and we will find an  nash equilibrium this gives us an o nlog n algorithm for finding an  nash equilibrium for two players market equilibrium via a primal dual algorithm for a convex program nikhil r devanur georgia institute of technology christos h papadimitriou university of california at berkeley berkeley california amin saberi stanford university stanford california and vijay v vazirani georgia institute of technology abstract we give the first polynomial time algorithm for exactly computing an equilibrium for the linear utilities case of the market model defined by fisher our algorithm uses the primal dual paradigm in the enhanced setting of kkt conditions and convex programs we pinpoint the added difficulty raised by this setting and the manner in which our algorithm circumvents it categories and subject descriptors f analysis of algorithms and problem complexity nonnumeric algorithms and problems general terms algorithms economics additional key words and phrases market equilibria primal dual algorithms a saberi was supported by national science foundation nsf grant ccf and a gift from google vazirani was supported by nsf grant ccf authors addresses n r devanur and v v vazirani computer science systems georgia in stitute of technology atlanta ga e mail vijay vazirani nikhil cc gatech edu c h pa padimitriou computer science division university of california at berkeley soda hall eecs department berkeley ca e mail chrisots cs berkeley edu a saberi terman engineering building room stanford ca e mail permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation copyrights for components of this work owned by others than acm must be honored abstracting with credit is permitted to copy otherwise to republish to post on servers to redistribute to lists or to use any component of this work in other works requires prior specific permission and or a fee permissions may be requested from publications dept acm inc penn plaza suite new york ny usa fax or qc acm doi acm reference format devanur n r papadimitriou c h saberi a and vazirani v v market equilibrium via a primal dual algorithm for a convex program j acm article october pages doi 1411512 introduction we present the first polynomial time algorithm for the linear version of an old problem defined in by irving fisher brainard and scarf consider a market consisting of buyers and divisible goods the money possessed by buyers and the amount of each good are specified also specified are utility functions of buyers which are assumed to be linear fisher original definition assumed concave utility functions the problem is to compute prices for the goods such that even if each buyer is made optimally happy relative to these prices there is no deficiency or surplus of any of the goods that is the market clears fisher work was done contemporarily and independently of walras pioneering work walras on modeling market equilibria through the ensuing years the study of market equilibria occupied center stage within mathematical economics its crowning achievement came with the work of arrow and debreu which established the existence of equilibrium prices in a very general setting through the use of kakutani fixed point theorem fisher and arrow and debreu market equilibrium models are considered the two most fundamental models within mathematical economics the latter can be seen as a generalization of the former it consists of agents who come to the market with initial endowments of goods and at any set prices want to sell all their goods and buy optimal bundles at these prices the problem again is to find market clearing prices prior algorithmic results general equilibrium theory has long en joyed the status of the crown jewel within mathematical economics however other than a few isolated results it is essentially a non algorithmic theory among its al gorithmic results are scarf work on approximately computing fixed points scarf and some very impressive nonlinear convex programs that capture as their optimal solutions equilibrium allocations for the case of linear utility functions the eisenberg gale program for fisher model eisenberg and gale and the nenakov primak program nenakov and primak for the arrow debreu model see codenotti et al for a survey of these works the ellipsoid algo rithm can be used to find approximate solutions to these programs subsequent to our work exact algorithms for solving these programs have also been found see section within theoretical computer science the question of polynomial time solvabil ity of equilibria market equilibria as well as nash equilibria was first consid ered by megiddo and papadimitriou subsequently a complexity theoretic framework for establishing evidence of intractability for such issues was given by papadimitriou our work follows deng et al who gave polynomial time algorithms for the arrow debreu model for the cases that the utility functions are linear and either the number of goods or the number of agents is bounded in retrospect all necessary ingredients for obtaining an exact though not combi natorial polynomial time algorithm for fisher linear case were present even before our work the fact that equilibrium prices and allocations for this case are rational numbers that can be expressed using only polynomially many bits which is shown in this paper as a consequence of our combinatorial algorithm see lemma can also be shown directly using the eisenberg gale program for a proof see for example theorem in vazirani this fact together with the work of newman and primak yields the desired algorithm algorithmic contributions of our work for the linear case of fisher model it is natural to seek an algorithmic answer in the theory of linear programming however there does not seem to be any natural linear programming formulation for this problem instead a remarkable nonlinear convex program given by eisenberg and gale captures as its optimal solutions equilibrium allocations for this case our algorithm uses the primal dual paradigm not in its usual setting of lp duality theory but in the enhanced setting of convex programming and the karush kuhn tucker kkt conditions after introducing some definitions and notation in section we pinpoint in section the added difficulty of working in this enhanced setting and the manner in which our algorithm circumvents this difficulty our algorithm is not strongly polynomial indeed obtaining such an algorithm is an important open question remaining it will require a qualitatively different approach perhaps one which satisfies kkt conditions in discrete steps as is the rule with all other primal dual algorithms known today as pointed out in section we start by suitably relaxing the kkt conditions and our algorithm satisfies these conditions continuously rather than in discrete steps the usual advantages of combinatorial algorithms apply to our work as well namely such algorithms are easier to adapt certainly heuristically and sometimes even formally to related problems and fine tuned for use in special circumstances section offers specific examples our first exposition devanur et al of this algorithm suffered from a major shortcoming although the high level algorithmic idea given in devanur et al was the same as the one given in the current version see sections and the exact implementation using the notion of pre emptive freezing contained a subtle though fatal flaw fixing this flaw involved introducing the notion of balanced flows a nontrivial idea that is likely to find future applications see section we explain briefly the role played by this new notion the primal variables in the eisenberg gale program are allocations to buyers and the dual variables are lagrangian variables corresponding to the packing constraints occurring in the program these are interpreted as prices of goods as is usual in primal dual algo rithms our algorithm alternates between primal and dual update steps throughout the algorithm the prices are such that buyers have surplus money left over each update attempts to decrease this surplus and when it vanishes the prices are right for the market to clear exactly clearly the number of update steps executed needs to be bounded by a polyno mial devanur et al attempted to do this by adjusting the high level algorithm to ensure that in each iteration the decrease in the total surplus money is at least an inverse polynomial fraction of the total however despite numerous attempts no implementation of this idea has yet been found the main new idea is to measure progress with respect to the norm of the vector of surplus money of buyers rather than the norm the latter of course is the total surplus money unlike the norm the norm of the surplus vector depends on the particular allocation chosen the special allocation we choose is the one that minimizes the norm of the surplus vector in turn this allocation corresponds to a balanced flow in the network n p defined in section the following observation may shed additional light the special allocation men tioned above and the notion of balanced flow in network n p has an alternative definition let us compare the vector of surplus money with respect to two allo cations lexicographically after sorting the vectors in decreasing order the special allocation that minimizes the norm of the surplus vector is also the one that yields the lexicographically smallest surplus vector this alternative definition can be used for stating an algorithm that is identical to ours can a polynomial running time be established for the algorithm using the alternative definion thereby dispensing with norm altogether at present we see no way of doing this our proof of the fact that guarantees progress namely corollary crucially uses norm another ingredient for ensuring polynomial running time is new combinatorial facts in parametric bipartite networks see section subsequent algorithmic developments the conference version of this article devanur et al spawned off new algorithmic work along several different directions jain et al devanur and vazirani used this algo rithm to give an approximate market clearing algorithm for the linear case of the arrow debreu model vazirani gave the notion of spending constraint util ity functions for fisher model a polynomial time algorithm for the case of step functions and showed that these utilities are particularly expressive in google ad words market devanur and vazirani extended spending constraint utilities to the arrow debreu model and established many nice properties of these utilities garg and kapoor gave some very interesting approximate equilibrium al gorithms for the linear case of both models using an auction based approach these algorithms have much better running times than ours another exciting development came from a simple observation in kelly and vazirani that fisher linear case can be viewed as a special case of the re source allocation framework given by kelly for modeling and understanding tcp congestion control kelly and vazirani observed that although contin uous time algorithms not having polynomial running times had been developed for kelly problem finding discrete time algorithms would be interesting jain and vazirani explored this issue by defining the class of eisenberg gale markets markets whose equilibrium allocations can be captured via convex programs having the same form as the eisenberg gale program and studying algorithmic solvability and game theoretic properties of these markets this line of work was extended further in chakrabarty et al they study algorithmic solvability of eisenberg gale markets with two agents thereby positively settling some of the open problems of jain and vazirani the above stated works provide combinatorial algorithms for computing equi libria an advantage of this approach is illustrated in vazirani as stated above some of the basic properties of equilibria for linear fisher markets can be easily established using the eisenberg gale convex program e g see theorem in vazirani interestingly enough all these properties also hold for the generalization to spending constraint utility functions they are established in vazirani via a generalization of our combinatorial algorithm to this case at present we do not know of a convex program that captures equilibrium allocations for this case yet another application of the combinatorial structure of markets was to deter mining continuity properties of equilibrium prices and allocations for linear fisher markets and some of its generalizations megiddo and vazirani vazirani and wang the proofs in megiddo and vazirani for linear fisher mar kets are based on the eisneberg gale program however it was the combinatorial structure that made these properties apparent progress has also been made on obtaining convex programs that capture equilibria for various utility functions for the two fundamental market models see codenotti et al as well as on the question of finding exact equilibria by solving convex programs using either the ellipsoid method jain or interior point algorithms ye fisher linear case and the eisenberg gale convex program fisher linear case is the following consider a market consisting of a set b of buyers and a set a of divisible goods assume a n and b ni we are given for each buyer i the amount ei of money she possesses and for each good j the amount b j of this good in addition we are given the utility functions of the buyers our critical assumption is that these functions are linear let uij denote the utility derived by i on obtaining a unit amount of good j given prices pn of the goods it is easy to compute baskets of goods there could be many that make buyer i happiest we will say that pn are market clearing prices if after each buyer is assigned such a basket there is no surplus or deficiency of any of the goods our problem is to compute such prices in polynomial time first observe that without loss of generality we may assume that each b j is unit by scaling the uij appropriately the uij and ei are in general rational by scaling appropriately they may be assumed to be integral now it turns out that there is a market clearing price iff each good has a potential buyer one who derives nonzero utility from this good moreover if there is a solution it is unique gale eisenberg and gale we assume that we are in the latter case the eisenberg gale convex program is the following ni maximize subject to ei log ui i n u x i b ni xij j a where xij is the amount of good j allocated to buyer i the price of good j in the equilibrium is equal to the optimum value of the lagrangean variable corresponding to the second constraint in the above program by the kkt conditions optimal solutions to xij and p j must satisfy the following conditions j a p j a p j i a xij i b j a uij j a uijxij p j i b j a xij ei uij p j j a uijxij via these conditions it is easy to see that an optimal solution to the eisenberg and gale program gives equilibrium allocations for fisher linear case and the corresponding dual variables give equilibrium prices of goods the eisenberg and gale program also helps prove in a very simple manner basic properties of the set of equilibria equilibrium exists under certain conditions the mild conditions stated above the set of equilibria is convex equilibrium utilities and prices are unique and if the program has all rational entries then equilibrium allocations and prices are also rational high level idea of the algorithm let p pn denote a vector of prices if at these prices buyer i is given good j she derives uij pi amount of utility per unit amount of money spent clearly she will be happiest with goods that maximize this ratio define her bang per buck to be i max j uij p j clearly for each i b j a i uij p j if there are several goods maximizing this ratio she is equally happy with any combination of these goods this motivates defining the following bipartite graph g its bipartition is a b and for i b j a i j is an edge in g iff i uij p j we will call this graph the equality subgraph and its edges the equality edges any goods sold along the edges of the equality subgraph will make buyers happiest relative to the current prices computing the largest amount of goods that can be sold in this manner without exceeding the budgets of buyers or the amount of goods available assumed unit for each good can be accomplished by computing max flow in the following network direct edges of g from a to b and assign a capacity of infinity to all these edges introduce source vertex and a directed edge from to each vertex j a with a capacity of p j introduce sink vertex t and a directed edge from each vertex i b to t with a capacity of ei the network is clearly a function of the current prices p and will be denoted n p the algorithm maintains the following throughout invariant the prices p are such that a b t is a min cut in n p the invariant ensures that at current prices all goods can be sold the only even tuality is that buyers may be left with surplus money the algorithm raises prices systematically always maintaining the invariant so that surplus money with buyers keeps decreasing when the surplus vanishes market clearing prices have been at tained this is equivalent to the condition that a b t is also a min cut in n p that is max flow in n p equals the total amount of money possessed by the buyers remark with this setup we can define our market equilibrium problem as an optimization problem find prices p under which network n p supports maximum flow the enhanced setting and how to deal with it we will use the notation set up in the previous section to pinpoint the difficulties involved in solving the eisenberg gale program combinatorially and the manner in which these difficulties are circumvented as is well known the primal dual schema has yielded combinatorial algorithms for obtaining either optimal or near optimal integral solutions to numerous linear programming relaxations other than one exception namely edmonds algorithm for maximum weight matching in general graphs edmonds all other algo rithms raise dual variables via a greedy process the disadvantage of a greedy dual growth process is obvious the fact that a raised dual is bad in the sense that it obstructs other duals that could have led to a larger overall dual solution may become clear only later in the run of the algorithm in view of this the issue of using more sophisticated dual growth processes has received a lot of attention especially in the context of approximation algorithms indeed edmonds algorithm is able to find an optimal dual for matching by a process that increases and decreases duals the problem with such a process is that it will make primal objects go tight and loose and the number of such reversals will have to be upper bounded in the running time analysis the impeccable combinatorial structure of matching supports such an accounting and in fact this leads to a strongly polynomial algorithm however thus far all attempts at making such a scheme work out for other problems have failed the fundamental difference between complimentary slackness conditions for linear programs and kkt conditions for nonlinear convex programs is that whereas the former do not involve both primal and dual variables simultaneously in an equality constraint obtained by assuming that one of the variables takes a non zero value the latter do now our dual growth process is greedy prices of goods are never decreased yet because of the more complex nature of kkt conditions edges in the equality subgraph appear and disappear as the algorithm proceeds hence we are forced to carry out the difficult accounting process alluded to above for bounding the running time we next point out which kkt conditions our algorithm enforces and which ones it relaxes as well as the exact mechanism by which it satisfies the latter throughout our algorithm we enforce the first two conditions listed in section as mentioned in section at any point in the algorithm via a max flow in the network n p all goods can be sold however buyers may have surplus money left over with respect to a balanced flow in network n p see section for a definition of such a flow let mi be the money spent by buyer i thus buyer i surplus money is i ei mi we will relax the third and fourth kkt conditions to the following i b j a uij j a uijxij p j i b j a xij mi uij p j j a uijxij we consider the following potential function ct and we give a process by which this potential function decreases by an inverse polynomial fraction in polynomial time in each phase as detailed in lemma when ct drops all the way to zero all kkt conditions are exactly satisfied there is a marked difference between the way we satisfy kkt conditions and the way primal dual algorithms for lp do the latter satisfy complimentary conditions in discrete steps that is in each iteration the algorithm satisfies at least one new condition so if each iteration can be implemented in strongly polynomial time the entire algorithm has a similar running time on the other hand we satisfy kkt conditions continuously as the algorithm proceeds the kkt conditions corresponding to each buyer get satisfied to a greater extent next let us consider the special case of fisher market in which all uij are there is no known lp that captures equilibrium allocations in this case as well and the only recourse seems to be the special case of the eisenberg gale program in which all uij are restricted to although this is a nonlinear convex program it is easy to derive a strongly polynomial combinatorial algorithm for solving it of course in this case as well the kkt conditions involve both primal and dual variables simultaneously however the setting is so easy that this difficulty never manifests itself the algorithm satisfies kkt conditions in discrete steps much the same way that a primal dual algorithm for solving an lp does in retrospect megiddo and perhaps other papers in the past have implic itly given strongly polynomial primal dual algorithms for solving nonlinear convex programs some very recent papers have also also done so explicitly for example jain and vazirani however the problems considered in these papers are so simple e g multicommodity flow in which there is only one source that the enhanced difficulty of satisfying kkt conditions is mitigated and the primal dual algorithms are not much different than those for solving lp a simple algorithm in this section we give a simple algorithm without the use of balanced flows although we do not know how to establish polynomial running time for it it still provides valuable insights into the problem and shows clearly exactly where the idea of balanced flows fits in we pick up the exposition from the end of section how do we pick prices so the invariant holds at the start of the algorithm the following two conditions guarantee this the initial prices are low enough prices that each buyer can afford all the goods fixing prices at n suffices since the goods together cost one unit and all ei are integral each good j has an interested buyer has an edge incident at it in the equality subgraph compute i for each buyer i at the prices fixed in the previous step and compute the equality subgraph if good j has no edge incident reduce its price to p max uij l the iterative improvement steps follow the spirit of the primal dual schema the primal variables are the flows in the edges of n p and the dual variables are the current prices the current flow suggests how to improve the prices and vice versa s for s b define its money m s p i b ei with respect to prices p for set p for s a define its neighborhood in n p f s j b i s with i j g by the assumption that each good has a potential buyer f a b the invariant can now be more clearly stated lemma for given prices p network n p satisfies the invariant iff s a m s m f s proof the forward direction is trivial since under max flow of value m a every set s a must be sending m s amount of flow to its neighborhood let prove the reverse direction assume t is a min cut in p with a and b the capacity of this cut is m m now f since otherwise the cut will have infinite capacity moving and f to the t side also results in a cut by the condition stated in the lemma the capacity of this cut is no larger than the previous one therefore this is also a min cut in n p hence the invariant holds if the invariant holds it is easy to see that there is a unique maximal set s a such that m s m f s say that this is the tight set with respect to prices p clearly the prices of goods in the tight set cannot be increased without violating the invariant hence our algorithm only raises prices of goods in the active subgraph consisting of the bipartition a s b f s we will say that the algorithm freezes the subgraph s f s observe that in general the bipartite graph s f s may consist of several connected components with respect to equality edges let these be sk tk clearly as soon as prices of goods in a s are raised edges i j with i f s and j a s will not remain in the equality subgraph anymore we will assume that these edges are dropped before proceeding further we must be sure that these changes do not violate the invariant this follows from lemma if the invariant holds and s a is the tight set then each good j a s has an edge in the equality subgraph to some buyer i b f s proof since the invariant holds j a s must have an equality graph edge incident at it if all such edges are incidents at buyers in f s then f s j f s and therefore m s j m s m f s m f s j this contradicts the fact that the invariant holds we would like to raise prices of goods in the active subgraph in such a way that the equality edges in it are retained this is ensured by multiplying prices of all these goods by x and gradually increasing x starting with x to see that this has the desired effect observe that i j and i l are both equality edges iff p j pl uij uil the algorithm raises x starting with x until one of the following happens event a set r goes tight in the active subgraph event an edge i j with i b f s and j s becomes an equality edge observe that as prices of goods in a s are increasing goods in s are becoming more and more desirable to buyers in b f s which is the reason for this event if event happens we redefine the active subgraph to be a s r b f s r and proceed with the next iteration suppose event happens and that j sl because of the new equality edge i j f sl tl i therefore sl is not tight anymore hence we move sl tl into the active subgraph to complete the algorithm we simply need to compute the smallest values of x at which event and event happen and consider only the smaller of these for event this is straightforward below we give an algorithm for event finding tight sets let p denote the current price vector i e at x we first present a lemma that describes how the min cut changes in n x p as x increases throughout this section we will use the function m to denote money with respect to prices p without loss of generality assume that with respect to prices p the tight set in g is empty since we can always restrict attention to the active subgraph for the purposes of finding the next tight set define x min s a m f s m s the value of x at which a nonempty set goes tight let s denote the tight set at prices x p if t is a cut in the network we will assume that a and b lemma with respect to prices x p if x x then a b t is a min cut if x x then a b t is not a min cut moreover if t is a min cut in n x p then s proof suppose x x by definition of x s a x m s m f s therefore by lemma with respect to prices x p the invariant holds hence a b t is a min cut next suppose that x x since x m s x m s m f s with respect to prices x p the cut s f s t has strictly smaller capacity than the cut a b t therefore the latter cannot be a min cut let s and s suppose clearly f otherwise the cut will have infinite capacity if m f x m then by moving and f to the side we can get a smaller cut contradicting the minimality of the cut picked in particular if s then this inequality must hold leading to a contradiction hence furthermore m f x m x m on the other hand m f m f x m m the two imply that m f x m contradicting the definition of x hence and s remark a more complete statement for the first part of lemma which is not essential for our purposes is if x x then a b t is the unique min cut in n x p if x x then the min cuts are obtained by moving a bunch of connected components of s f s to the side of the cut a b t lemma let x m b m a and suppose that x x if b2 t be a min cut in n x p then must be a proper subset of a proof if a then b otherwise this cut has capacity and a b t is a min cut but for the chosen value of x this cut has the same capacity as a b t since x x the latter is not a min cut by lemma hence is a proper subset of a lemma x and s can be found using n max flow computations proof let x m b m a clearly x x if a b t is a min cut in n x p then by lemma x x if so s a otherwise let b2 t be a min cut in n x p by lemmas and s a therefore it is sufficient to recurse on the smaller graph f termination with market clearing prices let m be the total money possessed by the buyers and let f be the max flow computed in network n p at current prices p thus m f is the surplus money with the buyers let us partition the running of the algorithm into phases each phase terminates with the occurrence of event each phase is partitioned into iterations which conclude with a new edge entering the equality subgraph we will show that f must be proportional to the number of phases executed so far hence showing that the surplus must vanish in bounded time let u maxi b j a uij and let nun lemma at the termination of a phase the prices of goods in the newly tight set must be rational numbers with denominator proof let s be the newly tight set and consider the equality subgraph induced on the bipartition s f s assume without loss of generality that this graph is connected otherwise we prove the lemma for each connected component of this graph let j s pick a subgraph in which j can reach all other vertices j i s clearly at most s edges suffice if j reaches j i with a path of length then p ji apj b where a and b are products of l utility parameters uik each since alternate edges of this path contribute to a and b we can partition the uik in this subgraph into two sets such that a and b use uik from distinct sets these considerations lead easily to showing that m s p jc d where c now p j m f s d c hence proving the lemma lemma each phase consists of at most n iterations proof each iteration brings goods from the tight set to the active subgraph clearly this cannot happen more than n times without a set going tight lemma consider two phases p and pi not necessarily consecutive such that good j lies in the newly tight sets at the end of p as well as pi then the increase in the price of j going from p to pi is proof let the prices of j at the end of p and pi be p q and r respectively clearly r p q by lemma q and r therefore the increase in price of j r p q lemma after k phases f k proof consider phase p and let j be a good that lies in the newly tight set at the end of this phase let pi be the last phase earlier than p such that j lies in the newly tight set at the end of pi as well if there is no such phase because p is the first phase in which j appears in a tight set then let pi be the start of the algorithm let us charge to p the entire increase in the price of j going from pi to p even though this increase takes place gradually over all the intermediate phases by lemma this is in this manner each phase can be charged the lemma follows corollary algorithm terminates with market clearing prices in at most m phases and executes o max flow computations remark the upper bound given above is quite loose for example it is easy to shave off a factor of n by giving a tighter version of lemma algorithm the basic algorithm initialization j a p j n i b i min j uij p j compute equality subgraph g j a if degreeg j then p j maxi uij i recompute g f f i the frozen subgraph h h i a b the active subgraph while h do x define j h price of j to be p j x raise x continuously until one of two events happens if s h becomes tight then move s f s from h h i to f f i remove all edges from f i to h if an edge i j i h i j f attains equality i uij p j then add i j to g move connected component of j from f f i to h h i establishing polynomial running time for a given flow f in the network n p define the surplus of buyer i i p f to be the residual capacity of the edge i t with respect to f which is equal to ei minus the flow sent through the edge i t in this section we are trying to speed up algorithm by increasing the prices of goods adjacent only to high surplus buyers however the surplus of a buyer might be different for two different maximum flows in the same graph therefore we will restrict ourselves to a specific flow so that the surplus of a buyer is well defined the following definition serves this purpose define the surplus vector  p f p f p f n p f let ivi denote the norm of vector v definition balanced flow for any given p a maximum flow that mini mizes i p f i over all choices of f is called a balanced flow if i p f i i p f i i then we say f is more balanced than f i for a given p and a flow f in n p let r p f be the residual network of n p with respect to the flow f we will give a characterization of balanced flow via r p f lemma let f and f i be any two maximum flows in n p if i p f i i p f for some i b then there exist a j b such that  j p f j p f i and there is a path from j to i in r p f t there is a path from i to j in r p f i t proof consider the flow f i f it defines a feasible circulation in the network r p f since i p f i i p f there is a positive flow along the edge i t in f i f by following this flow all the way back to t in the circulation one can find a node j such that there is a positive flow from t to j and then to i in f i f since both flows are maximum is an isolated vertex in f i f and this flow does not go through now f i f is a valid flow in r p f and therefore there exists a path from j to i in r p f t moreover having a positive flow from t to j implies that  j p f j p f i a similar argument shows that there is also a path from i to j in r p f i t lemma if a bi i n and  n  j where   j j n then i a bn i j i a  bn n i proof n n n a  bi i bi  i the following property characterizes all balanced flows it defines the flows for which there is no path from a low surplus node to a high surplus node in the residual network property there is no path from node i b to node j b in r p f if surplus i is more than surplus of j in n p f algorithm initialization j a p j n b i min j uij p j define g a b e with i j e iff i uij p j a if degreeg j then p j maxi uij i recompute g  m repeat compute a balanced flow f in g define  to be the maximum surplus in b define h to be the set of buyers with surplus  repeat let h i be the set of neighbors of h in a remove all edges from b h to h i x define j h i price of j to be p j x raise x continuously until one of the two events happens event an edge i j i h j a h i attains equality i uij p j add i j to g recompute f in the residual network corresponding to f in g define i to be the set of buyers that can reach h h h i event s h becomes tight until some subset s h is tight until a is tight theorem a maximum flow f is balanced iff it has property proof suppose f is a balanced flow let i p f j p f for some i and j and suppose for the sake of contradiction that there is a path from j to i in r p f t then one can send a circulation of positive value along t j i t in r p f decreasing i and increasing  j from lemma the resulting flow is more balanced than f contradicting the fact that f is a balanced flow to prove the other direction suppose that f is not a balanced maximum flow let f i be a balanced flow since  p f i  p f there exists i b such that i p f i i p f by lemma there exists j b such that  j p f j p f i and there is a path from j to i in r p f t since f has property i p f  j p f the above three inequalities imply i p f i j p f but again by lemma there is a path from i to j in r p f i t so f i doesn t have property this contradicts the assumption that f i is a balanced flow by what we proved in the first half the theorem the following lemma provides our main tool for proving polynomial running time of algorithm we will use it to prove an upper bound on the norm of the surplus vector of buyers at the end of every phase lemma if f and f are respectively a feasible and a balanced flow in n p and for some i b and  i f i f  then there is a flow f i and for some k there is a set of vertices ik and values k such that k l l  i f i i f  il f i il f l i f i il f i proof consider f f in r p f and in a similar fashion as in lemma follow the incoming flow of node i until you reach or the node i itself let f i be the flow augmented from f by sending back the flow through all these circulations and paths we will have i f i i f  and for a set of vertices ik and values k such that k l  we have i f i i f l corollary i p f i p f proof by lemma i f p i f i p and since f is a balanced flow in n p i f i p i f p corollary for any given p all balanced flows in n p have the same surplus vector as a result one can define the surplus vector for a given price as  p  p f where f is the balanced flow in n p this vector can be found by computing a balanced flow in the equality subgraph in the following way corollary for a given price vector p the balanced flow can be computed by at most n max flow computation proof we will use the divide and conquer method let mavg ni ei mavg from the capacity of each edge adjacent t let s t be the maximal min cut in that network s t t if a s then the current maximum flow is balanced otherwise let and be the networks induced by t and s t respectively claim that the union of balanced flows in and is a balanced flow in n in order to prove the claim it is enough from theorem to show that the surplus of all buyers in in a balanced flow is at least mavg and that of all buyers in is at most mavg we will prove the former the proof of the latter is similar let l be the set of all buyers in with the lowest surplus say suppose mavg let k be the set of goods reachable by l in the residual network of w r t a balanced flow by theorem no other buyers are reachable from l in this network hence f k l since the surplus of all buyers in l is m k m l l m l mavg l this is a contradiction to the fact that s t was a min cut in a set of feasible vectors a vector v is called min max fair iff for every feasible vector u and an index i such that ui vi there is a j for which u j v j and v j vi similarly v is max min fair iff ui vi implies that there is a j for which u j v j and v j vi remark the surplus vector of a balanced flow is both min max and max min fair the polynomial time algorithm the main idea of algorithm is that it tries to reduce  p f in every phase intuitively this goal is achieved by finding a set of high surplus buyers in the balanced flow and increasing the prices of goods in which they are interested if a subset becomes tight as a result of this increase we have reduced  p f because the surplus of a formerly high surplus buyer is dropped to zero the other event that can happen is that a new edge is added to the equality subgraph in that case this edge will help us to make the surplus vector more balanced we can reduce the surplus of high surplus buyers and increase the surplus of low surplus ones this operation will result in the reduction of i p f i the algorithm starts with finding a price vector that does not violate the invariant the rest of the algorithm is partitioned into phases in each phase we have an active graph h h i with h b and h i a and we increase the prices of goods in h i like algorithm let  be the maximum surplus in b the subset h is initially the set of buyers whose surplus is equal to  h i is the set of goods adjacent to buyers in h each phase is divided into iterations in each iteration we increase the prices of goods in h i until either a new edge joins the equality subgraph or a subset becomes tight if a new edge is added to the equality subgraph we recompute the balanced flow f then we add to h all vertices that can reach a member of h in r p f t if a subset becomes tight as a result of increase of the prices then the phase terminates consider a phase in the execution of algorithm define pi and hi to be the price vector and the set of nodes in h after executing the i th iteration in that phase let denote the set of nodes in h before the first iteration lemma the number of iterations executed in a phase is at most n more over in every phase there is an iteration in which surplus of at least one of the vertices is reduced by at least  proof let k denote the number of iterations in the phase every time an edge is added to the equality subgraph h i is increased by at least one therefore k is at most n define i min j hi  j pi for i k  and the phase ends when the surplus of one buyer in h becomes zero so k so there is an iteration t in  n consider the residual network corresponding to the balanced flow computed at iteration t in that network every vertex in ht ht can reach a vertex in ht and therefore by theorem its surplus is greater than or equal to the surplus of that vertex this means that minimum surplus t is achieved by a vertex i in ht hence the surplus of vertex i is decreased by at least t t during iteration t lemma if and p are price vectors before and after a phase i p i proof in every iteration we increase prices of goods in h or add new edges to the equality subgraph moreover all the edges of the network that are deleted in the beginning of a phase have zero flow therefore the balanced flow computed at iteration i is a feasible flow for n pi therefore by lemma i i i i i i i pk i furthermore by the previous lemma there is an iteration t and node i such that i pt i pt  so we have i pt i pt  which means that i p i pt i pt now i so  i i  i p i p remark the upper bound given above is quite loose for example one can reduce the upper bound to by considering all iterations t in which t t by the bound given in the above it is easy to see that after o phases  p is reduced to at most half of its previous value in the beginning  p m once the value of  p the algorithm takes at most one more step this is because lemma and consequently lemma holds for algorithm as well hence the number of phases is at most o log o log n n log u log m as noted before the number of iterations in each phase is at most n each iteration requires at most o n max flow computations hence we get theorem algorithm executes at most o log n n log u log m max flow computations and finds market clearing prices discussion an important question remaining is whether there is a strongly polynomial algo rithm for computing equilibrium for fisher linear case and solving the eisenberg gale program another issue is whether the machinery developed in section is necessary for obtaining a polynomial time algorithm that is does the algorithm given in sections and have a polynomial running time if not it would be nice to find a family of instances on which it takes super polynomial time acknowledgments we wish to thank lisa fleischer and mohammad mahdian for pointing out a subtle though fatal bug in the pre emptive freezing part in devanur et al which has been corrected in this article the nobel prize in economics awarded to alvin e roth and lloyd s shapley for the theory of stable allocations and the practice of market design the related branch of game theory is often referred to as matching theory which studies the design and performance of platforms for transactions between agents roughly speaking it studies who in teracts with whom and how which applicant gets which job which students go to which universities which donors give organs to which patients and so on mark voorneveld game theory extensive form games many methods for finding desirable allocations in matching problems are variants of two algorithms the top trading cycle algorithm the deferred acceptance algorithm for each of the two algorithms i will do the following state the algorithm state nice properties of outcomes generated by the algorithm solve an example using the algorithm describe application give you a homework exercise l s shapley and h scarf on cores and indivisibility journal of mathematical economics the algorithm is described in section p and attributed to david gale input each of n n agents owns an indivisible good a house and has strict preferences over all houses convention agent i initially owns house hi question can the agents benefit from swapping houses ttc algorithm each agent i points to her most preferred house possibly i own each house points back to its owner this creates a directed graph in this graph identify cycles finite cycle exists strict preferences each agent is in at most one cycle give each agent in a cycle the house she points at and remove her from the market with her assigned house if unmatched agents houses remain iterate the ttc assignment is such that no subset of owners can make all of its members better o by exchanging the houses they initially own in a di erent way in technical lingo the ttc outcome is a core allocation the ttc assignment is the only such assignment unique core allocation it is never advantageous to an agent to lie about preferences if the ttc algorithm is used the ttc algorithm is strategy proof mark voorneveld game theory extensive form games mark voorneveld game theory extensive form games agents ranking from best left to worst right only agents and left with updated preferences h h cycle so get and gets remove them and iterate cycle so gets and gets done final match a abdulkadiro glu and t s onmez school choice a mechanism design approach american economic review how to assign children to schools subject to priorities for siblings and distance input students submit strict preferences over schools schools submit strict preferences over students based on priority criteria and if necessary a random number generator modified ttc algorithm each remaining student points at her most preferred unfilled school each unfilled school points at its most preferred remaining student cycles are identified and students in cycles are matched to the school they point at remove assigned students and full schools if unmatched students remain iterate mark voorneveld game theory extensive form games a e roth t s onmez m u u nver kidney exchange quarterly journal of economics a case with patient donor pairs a patient in need of a kidney and a donor family friend who is willing to donate one complications arise due to incompatibility blood tissue groups etc so look at trading cycles patient might get the kidney of donor if patient gets the kidney of donor etc mark voorneveld game theory extensive form games apply the ttc algorithm to the following case h2 h2 h4 h2 h4 h5 d gale and l s shapley college admissions and the stability of marriage american mathematical monthly only seven pages and yes stability of marriage men and women have strict preferences over partners of the opposite sex you may prefer staying single to marrying a certain partner a match is a set of pairs of the form m w m m or w w such that each person has exactly one partner person i is unmatched if the match includes i i i is acceptable to j if j prefers i to being unmatched given a proposed match a pair m w is blocking if both prefer each other to the person they re matched with m prefers w to his match partner w prefers m to her match partner a match is unstable if someone has an unacceptable partner or if there is a blocking pair otherwise it is stable a match is man optimal if it is stable and there is no other stable match that some man prefers woman optimal analogously input a nonempty finite set m of men and w of women each man woman ranks acceptable women men from best to worst da algorithm men proposing each man proposes to the highest ranked woman on his list women hold at most one o er her most preferred acceptable proposer rejecting all others each rejected man removes the rejecting woman from his list if there are no new rejections stop otherwise iterate after stopping implement proposals that have not been rejected remarks da algorithm women proposing switch roles deferred acceptance receiving side defers final acceptance of proposals until the very end mark voorneveld game theory extensive form games mark voorneveld game theory extensive form games the algorithm ends with a stable match by construction no person is matched to an unacceptable candidate no m w can be a blocking pair if m strictly prefers w to his current match he must have proposed to her and been rejected in favor of a candidate that w liked better that is w finds her match better than m this match is man optimal woman pessimal men have no incentives to lie about their preferences women might strategy proof for men see homework exercise there is no mechanism that always ends in a stable match and that is strategy proof for all participants for convenience m w all partners of opposite sex are acceptable ranking matrix interpretation entry in the first row and first column indicates that ranks first among the women and that ranks third among the men is the only person to receive multiple proposals she compares rank with rank and rejects strike this entry from the matrix and iterate is the only person to receive multiple proposals she compares rank with rank and rejects strike this entry from the matrix and iterate mark voorneveld game theory extensive form games mark voorneveld game theory extensive form games is the only person to receive multiple proposals she compares rank with rank and rejects strike this entry from the matrix and iterate is the only person to receive multiple proposals she compares rank with rank and rejects strike this entry from the matrix and iterate m is the only person to receive multiple proposals she compares rank with rank and rejects strike this entry from the matrix and iterate no rejections the algorithm stops with stable match w4 m4 w2 mark voorneveld game theory extensive form games mark voorneveld game theory extensive form games a variant of the marriage problem is the college admission problem each student can be matched to at most one college but a college can accept many students this can be mapped into the marriage problem students one side of the marriage problem e g m colleges other side of the marriage problem e g w split college c with quota n into n di erent women cn create artificial preferences by replacing college c in students rankings by cn in that order consider the ranking matrix find a stable matching using the men proposing da algorithm find a stable matching using the women proposing da algorithm suppose that lies about her preferences and says that she only finds m2 acceptable what is the outcome of the men proposing da algorithm now verify that both women are better o than under a it may pay for the women to lie the complexity of computing a nash equilibrium abstract how long does it take until economic agents converge to an equilibrium by studying the complexity of the problem of computing a mixed nash equilibrium in a game we provide evidence that there are games in which convergence to such an equilibrium takes prohibitively long traditionally com putational problems fall into two classes those that have a polynomial time algorithm and those that are np hard however the concept of np hardness cannot be applied to the rare problems where every instance has a solution for example in the case of games nash theorem asserts that every game has a mixed equilibrium now known as the nash equilibrium in honor of that result we show that finding a nash equilibrium is complete for a class of problems called ppad containing several other known hard problems all problems in ppad share the same style of proof that every instance has a solution introduction in a recent cacm article shoham reminds us of the long relationship between game theory and computer sci ence going back to john von neumann at princeton in the and how this connection became stronger and more crucial in the past decade due to the advent of the inter net strategic behavior became relevant to the design of computer systems while much economic activity now takes place on computational platforms game theory is about the strategic behavior of rational agents it studies games thought experiments modeling var ious situations of conflict one commonly studied model aims to capture two players interacting in a single round for example the well known school yard game of rock paper scissors can be described by the mathematical game shown in figure there are two players one choosing a row and one choosing a column the choices of a player are his her actions once the two players choose simultaneously an action they receive the corresponding payoffs shown in the table the first number denotes the payoff of row the sec ond that of column notice that each of these pairs of numbers sum to zero in the case of figure such games a full version of this paper is to appear in sicomp permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee copyright acm 00 are called zero sum games three other well known games called chicken prisoner dilemma and penalty shot game respectively are shown in figure the penalty shot game is zero sum but the other two are not all these games have two players game theory studies games with many players but these are harder to display figure rock paper scissors the purpose of games is to help us understand economic behavior by predicting how players will act in each particu lar game the predictions game theorists make about player behavior are called equilibria one such prediction is the pure nash equilibrium each player chooses an action that is a best response to the other player choice i e it is the highest payoff for the player in the line row or column cho sen by the other player in the game of chicken in figure a pure nash equilibrium is when one player chooses dare and the other chooses chicken in the prisoner dilemma the only pure nash equilibrium is when both players choose defect unfortunately not all games have a pure nash equilib rium for example it is easy to see that the rock paper scissors game in figure has none this lack of universality is an important defect of the concept of pure nash equilib rium as a predictor of behavior but the rock paper scissors game does have a more sophisticated kind of equilibrium called a mixed nash equilibrium and in fact one that is fa miliar to all who have played this game both players pick an action uniformly at random that is a mixed nash equi librium is a probabilistic distribution on the set of actions of each player each of the distributions should have the prop erty that it is a best response to the other distributions this means that each action assigned positive probability is among the actions that are best responses in expectation to the distribution chosen by the opponent about games such as chess we can capture this and other similar games in the present framework by considering two players black and white each with a huge action set containing all possible maps from positions to moves but of course such formalism is not very helpful for analyzing chess and similar games figure three other player games in john nash proved that all games have a mixed nash equilibrium that is in any game distributions over the players actions exist such that each is a best re sponse to what everybody else is doing this important and far from obvious universality theorem established the mixed nash equilibrium as game theory central equilib rium concept the baseline and gold standard against which all subsequent refinements and competing equilibrium con cepts were judged universality is a desirable attribute for an equilibrium con cept of course such a concept must also be natural and credible as a prediction of behavior by a group of agents for example pure nash seems preferable to mixed nash in games that do have a pure nash equilibrium but there is a third important desideratum on equilibrium concepts of a computational nature an equilibrium concept should be efficiently computable if it is to be taken seriously as a prediction of what a group of agents will do because if computing a particular kind of equilibrium is an intractable problem of the kind that take lifetimes of the universe to solve on the world fastest computers it is ludicrous to ex searchers have sought algorithms for finding mixed nash equilibria that is for solving the computational problem which we will call nash in this paper if a game is zero sum like the rock paper scissors game then it follows from the work of john von neumann in the that nash can be formulated in terms of linear programming a subject identified by george dantzig in the linear programs can be solved efficiently even though we only realized this in the but what about games that are not zero sum several algorithms have been proposed over the past half century but all of them are either of unknown complexity or known to require in the worst case exponential time during the same decades that these concepts were being explored by game theorists computer science theorists were busy developing independently a theory of algorithms and complexity addressing precisely the kind of problem raised in the last two paragraphs given a computational prob lem can it be solved by an efficient algorithm for many common computational tasks such as finding a solution of a set of linear equations there is a polynomial time algo rithm that solves them this class of problems is called p for other such problems such as finding a truth assignment that satisfies a set of boolean clauses a problem known as sat or the traveling salesman problem no such algorithm could be found after many attempts many of these prob lems can be proved np complete meaning they cannot be solved efficiently unless p np an event considered very unlikely from the previous discussion of failed attempts to develop an efficient algorithm for nash one might be tempted to suppose that this problem too is np complete but the sit uation is not that simple nash is unlike any np complete problem because by nash theorem it is guaranteed to al ways have a solution in contrast np complete problems like sat draw their intractability from the possibility that a solution might not exist this possibility is used heavily in the np completeness proof see figure for an argu ment due to nimrod megiddo why it is very unlikely that np completeness can characterize the complexity of nash note however that if one seeks a nash equilibrium with additional properties such as the one that maximizes the sum of player utilities or one that uses a given strategy with positive probability then the problem does become np complete since np completeness is not an option to understand the complexity of nash one must essentially start all over in the path that led us to np completeness we must define a class of problems which contains along with nash some other well known hard problems and then prove that nash is complete for that class indeed in this paper we describe a proof that nash is ppad complete where ppad is a subclass of np that contains several important problems that are suspected to be hard including nash pect that it can be arrived at in real life this consideration suggests the following important question is there an ef ficient algorithm for computing a mixed nash equilibrium in this article we report on results that indicate that the answer is negative our own work obtained this for games with or more players and shortly afterwards the papers extended this unexpectedly to the im portant case of player games ever since nash paper was published in many re but what about the traveling salesman problem one might ask doesn t it always have a solution to com pare fairly the traveling salesman problem with sat and nash one has to first transform it into a search problem of the form given a distance matrix and a budget b find a tour that is cheaper than b or report that none exists no tice that an instance of this problem may or may not have a solution but an efficient algorithm for this problem could be used to find an optimal tour know of efficient algorithms that solve the problem whereas for nash equilibria we do not we are now ready to define the computational problem nash given the description of a game by explicitly giv ing the utility of each player corresponding to each strat egy profile and a rational number e compute an e approximate nash equilibrium this should be at least as tractable as finding an exact equilibrium hence any hard ness result for approximate equilibria carries over to exact equilibria note that an approximate equilibrium as de fined above need not be at all close to an exact equilibrium see for a complexity theory of exact nash equilibria figure megiddo proof that nash is unlikely to be np complete problem statement nash and approxi mate nash equilibria a game in normal form has some number k of players and for each player p p k a finite set sp of pure actions or strategies the set s of pure strategy profiles is the cartesian product of the sp thus a pure strategy profile represents a choice for each player of one of his actions finally for each player p and s the game will specify a payoff or utility up which is the value of the outcome to player p when all the players including p choose the strate gies in in a nash equilibrium players choose probability distributions over their sp called mixed strategies so that no player can deviate from his mixed strategy and improve on his expected payoff see figure for details for two player games the numerical quantities that define a nash equilibrium i e the probabilities used by the play ers are rational numbers assuming the utilities are also rational so it is clear how to write down the solution of a player game however as pointed out in nash original paper when there are more than two players there may be only irrational solutions in this general situation the prob lem of computing a nash equilibrium has to deal with issues of numerical accuracy thus we introduce next the concept of approximate nash equilibrium if nash equilibrium means no incentive to deviate then approximate nash equilibrium stands for low incentive to deviate specifically if e is a small positive quantity we can define an e nash equilibrium as a profile of mixed strategies where any player can improve his expected payoff by at most e by switching to another strategy figure gives a precise definition and shows how the problem reduces to solving a set of algebraic inequalities our focus on approximate so lutions is analogous to the simpler problem of polynomial root finding suppose that we are given a polynomial f with a single variable and we have to find a real root a real number r satisfying f r in general a solution to this problem the number r cannot be written down as a fraction so we should really be asking for some sort of numerical approximation to r for example computing a ra tional number r such that f r e for some small e if f happens to have odd degree we can even say in advance that a solution must exist in a further analogy with nash of course the analogy breaks down in that for root finding we figure writing down the problem algebraically total search problems we think of np as the class of search problems of the form given an input find a solution which then can be easily checked or report that none exists there is an asymmetry between these outcomes in that none exists is not required to be easy to verify we call such a search problem total if the solution al ways exists there are many apparently hard total search problems in np even though as we argued in the intro duction they are unlikely to be np complete perhaps the best known is factoring the problem of taking an integer as an input and outputting its prime factors nash and several other problems introduced below are also total a useful classification of total search problems was pro posed in the idea is this if a problem is total the fact that every instance has a solution must have a mathematical proof unless the problem can be easily solved efficiently in that proof there must be a non constructive step it turns out that for all known total search problems in the fringes of p these non constructive steps are one of very few simple arguments if a graph has a node of odd degree then it must have another this is the parity argument giving rise to the class ppa if a directed graph has an unbalanced node a ver tex with different in degree and out degree then it must have another this is the parity argument for directed graphs giving rise to the class ppad consid ered in this article figure describes the correspond ing search problems every directed acyclic graph must have a sink the corresponding class is called pls for polynomial local search if a function maps n elements to n elements then there is a collision this is the pigeonhole principle and the corresponding class is ppp we proceed with defining more precisely the second class in the list above the class ppad there are two equivalent ways to define np first it is the class of all search problems whose answers are verifiable in polynomial time for example the search problem sat given a boolean formula in cnf find a satisfying truth assignment or report that none exists is in np because it is easy to check whether a truth assignment satisfies a cnf since we know that sat is np complete we can also define np as the class of all problems that can be reduced into instances of sat by reduce we refer to the usual form of polynomial time reduction from search problem a to search problem b an efficient algorithm for transforming any instance of a to an equivalent instance of b together figure end of the line an apparently hard total search problem of the line and therefore all problems in ppad can be reduced to it why should we believe that ppad contains hard problems in the absence of a proof that p np we cannot hope to be sure that ppad contains hard problems the reason is that ppad lies between p and np in the sense that if p np then ppad itself as a subset of np will be with an efficient algorithm for translating any solution of the equal to p but even if p np it may still be the case instance of b back to a solution of the original instance of a we define the class ppad using the second strategy in particular ppad is the class of all search problems that can be reduced to the problem end of the line defined in figure note that since end of the line is a total problem so are all problems in ppad proceeding now in analogy with np we call a problem ppad complete if end that ppad complete problems are easy to solve we be lieve that ppad complete problems are hard for the same reasons of computational and mathematical experience that convince us that np complete problems are hard but as we mentioned our confidence is necessarily a little weaker ppad contains many problems for which researchers have tried for decades to develop efficient algorithms in the next section we introduce one such problem called brouwer however end of the line itself is a pretty convincingly hard problem how can one hope to devise an algorithm that telescopes exponentially long paths in every implicitly given graph from nash to ppad our main result is the following theorem nash is ppad complete in the remainder of this article we outline the main ideas of the proof for full details see we need to prove two things first that nash is in ppad that is it can be re duced to end of the line second see section that it is complete the reverse reduction as it turns out both direc tions are established through a computational problem in spired by a fundamental result in topology called brouwer fixed point theorem described next brouwer fixed points imagine a continuous function mapping a circle together with its interior to itself for example a rotation around the center notice that the center is fixed it hasn t moved under this function you could flip the circle but then all points on a diagonal would stay put or you could do some thing more elaborate shrink the circle translate it so it still lies within the original larger circle and then rotate it a little thought reveals that there is still at least one fixed point or stretch and compress the circle like a sheet of rub ber any way you want and stick it on the original circle still points will be fixed unless of course you tear the circle the function must be continuous there is a topological reason why you can t map continuously the circle on itself without leaving a point unmoved and that brouwer theorem it states that any continuous map from a compact that is closed and bounded and convex that is without holes subset of the euclidean space into itself always has a fixed point brouwer theorem immediately suggests an interesting computational total search problem called brouwer given a continuous function from some compact and convex set to itself find a fixed point but of course for a meaningful def inition of brouwer we need to first address two questions how do we specify a continuous map from some compact and convex set to itself and how do we deal with irra tional fixed points first we fix the compact and convex set to be the unit cube m in the case of more general domains e g the circular domain discussed above we can translate it to this setting by shrinking the circle embedding it into the unit square and extending the function to the whole square so that no new fixed points are introduced we then assume that the function f is given by an efficient algorithm f which for each point x of the cube written in binary com putes f x we assume that f obeys a lipschitz condition for all m d f f k d where d is the euclidean distance and k is the lips chitz constant of f this benign well behavedness condition ensures that approximate fixed points can be localized by examining the value f x when x ranges over a discretized grid over the domain hence we can deal with irrational so lutions in a similar manoeuvre as with nash by only seek ing approximate fixed points in fact we have the following strong guarantee for any e there is an e approximate fixed point that is a point x such that d f x x e whose coordinates are integer multiples of d where d depends on k e and the dimension m in the absence of a lipschitz con stant k there would be no such guarantee and the problem of computing fixed points would become intractable for mally the problem brouwer is defined as follows brouwer an efficient algorithm f for the evaluation of a function f m m a constant input k such that f satisfies and the desired accuracy e output a point x such that d f x x e it turns out that brouwer is in ppad gives a sim ilar result for a more restrictive class of brouwer functions to prove this we will need to construct an end of the line graph associated with a brouwer instance we do this by constructing a mesh of tiny triangles over the domain where each triangle will be a vertex of the graph edges between pairs of adjacent triangles will be defined with respect to a coloring of the vertices of the mesh vertices get colored according to the direction in which f displaces them we argue that if a triangle vertices get all possible colors then f is trying to shift these points in conflicting directions and we must be close to an approximate fixed point we elab orate on this in the next few paragraphs focusing on the dimensional case triangulation first we subdivide the unit square into small squares of size determined by e and k and then divide each lit tle square into two right triangles see figure ignoring for now the colors shading and arrows in the m dimensional case we subdivide the m dimensional cube into m dimensional cubelets and we subdivide each cubelet into the m dimensional analog of a triangle called an m simplex coloring we color each vertex x of the triangles by one of three colors depending on the direction in which f maps x in two dimensions this can be taken to be the angle between vector f x x and the horizontal specifically we color it red if the direction lies between and degrees blue if it ranges between and degrees and yellow otherwise as shown in figure if the direction is degrees we allow either yellow or red similarly for the other two borderline cases using the above coloring convention the vertices get colored in such a way that the following property is satisfied none of the vertices on the lower side of the square uses red no vertex on the left side uses blue and no vertex on the other two sides uses yellow figure shows a coloring of the vertices that could result from the function f ignore the arrows and the shading of triangles sperner lemma it now follows from an elegant result in combinatorics called sperner lemma that in any coloring satisfying property there will be at least one small triangle whose vertices have all three colors verify this in figure the figure the colors assigned to the different direc tions of f x x there is a transition from red to yellow at degrees from yellow to blue at de grees and from blue to red at degrees trichromatic triangles are shaded because we have chosen the triangles to be small any vertex of a trichromatic trian gle will be an approximate fixed point intuitively since f satisfies the lipschitz condition given in it cannot fluc tuate too fast hence the only way that there can be three points close to each other in distance which are mapped in three different directions is if they are all approximately fixed the connection with ppad is the proof of sperner lemma think of all the triangles containing at least one red and yellow vertex as the nodes of a directed graph g there is a directed edge from a triangle t to another triangle t i if t and t i share a red yellow edge which goes from red to yellow clockwise in t see figure the graph g thus created consists of paths and cycles since for every t there is at most one t i and vice versa verify this in figure now we may also assume on the left side of the square there is only one change from yellow to red under this assumption let t be the unique triangle con taining the edge where this change occurs in figure t is marked by a diamond observe that if t is not trichro matic as is the case in figure then the path starting at t is guaranteed to have a sink since it cannot inter sect itself and it cannot escape outside the square notice that there is no red yellow edge on the boundary that can be crossed outward but the only way a triangle can be a sink of this path is if the triangle is trichromatic this establishes that there is at least one trichromatic triangle there may of course be other trichromatic triangles which would correspond to additional sources and sinks in g as in figure g is a graph of the kind in figure to finish the reduction from brouwer to end of the line notice that given a triangle it is easy to compute its colors by invoking f and find its neighbors in g or its single neighbor if it is trichromatic finally from nash to brouwer to finish our proof that nash is in ppad we need a reduction from nash to brouwer such a reduction was essentially given by nash himself in his proof sup pose that the players in a game have chosen some mixed strategies unless these already constitute a nash equilib f gives rise to multiple yellow red adjacencies on the left hand side we deal with this situation by adding an extra array of vertices to the left of the left side of the square and color all these vertices red except for the bottom one which we color yellow this addition does not violate and does not create any additional trichromatic triangles since the left side of the square before the addition did not contain any blue figure the subdivision of the square into smaller squares and the coloring of the vertices of the subdi vision according to the direction of f x x the ar rows correspond to the end of the line graph on the triangles of the subdivision the source t is marked by a diamond figure an illustration of nash function fn for the penalty shot game the horizontal axis cor responds to the probability by which the penalty kicker kicks right and the vertical axis to the proba bility by which the goalkeeper dives left the arrows show the direction and magnitude of fn x x the unique fixed point of fn is corresponding to the unique mixed nash equilibrium of the penalty shot game the colors respect figure but our palette here is continuous rium some of the players will be unsatisfied and will wish to change to some other strategies this suggests that one can construct a preference function from the set of players strategies to itself that indicates the movement that will be made by any unsatisfied players an example of how such a function might look is shown in figure a fixed point of such a function is a point that is mapped to itself a nash equilibrium and brouwer fixed point theorem explained above guarantees that such a fixed point exists in fact it can be shown that an approximate fixed point corresponds to an approximate nash equilibrium therefore nash re duces to brouwer from ppad back to nash to show that nash is complete for ppad we show how to convert an end of the line graph into a corresponding game so that from an approximate nash equilibrium of the game we can efficiently construct a corresponding end of the line we do this in two stages the graph is converted into a brouwer function whose domain is the unit dimensional cube the brouwer function is then represented as a game the resulting game has too many players their number de pends on the size of the circuits that compute the edges of the end of the line graph and so the final step of the proof is to encode this game in terms of another game with three players from paths to fixed points the ppad completeness of brouwer we have to show how to encode a graph g as described in figure in terms of a continuous easy to compute brouwer function f a very different looking mathematical object the encoding is unfortunately rather complicated but is key to the ppad completeness result we proceed by first using the dimensional unit cube as the domain of the function f next the behavior of f shall be defined in terms of its behavior on a very fine rectilinear mesh of grid points in the cube thus each grid point lies at the center of a tiny cubelet and the behavior of f away from the centers of the cubelets shall be gotten by interpolation with the closest grid points each grid point x shall receive one of colors that represent the value of the dimensional displacement vector f x x the possible vectors can be chosen to point away from each other such that f x x can only be approximately zero in the vicinity of all colors we are now ready to fit g itself into the above framework each of the vertices of g shall correspond with special sites in the cube one of which lies along the bottom left hand edge in figure and the other one along the top left edge we use locations that are easy to compute from the identity of a vertex of g while most other grid points in the cube get color from f at all the special sites a particular configuration of the other colors appears if g has an edge from node u to node v then f shall also color a long sequence of points between the corresponding sites in the cube as shown in figure so as to connect them with sequences of grid points that get colors the precise arrangement of these colors can be chosen to be easy to compute using the circuits p and s that define g and such that all colors are adjacent to each other an approximate fixed point only at sites that correspond to an end of the line of g having shown earlier that brouwer is in ppad we es tablish the following theorem brouwer is ppad complete from brouwer to nash the ppad complete class of brouwer functions that we identified above have the property that their function f can be efficiently computed using arithmetic circuits that are built up using a small repertoire of standard operators such as addition multiplication and comparison these circuits can be written down as a data flow graph with one of these operators at each node in order to transform this into a game whose nash equilibria correspond to approximate fixed points of the brouwer function we introduce players for every node on this data flow graph games that do arithmetic figure embedding the end of the line graph in a cube the embedding is used to define a continuous function f whose approximate fixed points corre spond to the unbalanced nodes of the end of the line graph the idea is to simulate each arithmetic gate in the circuit by a game and then compose the games to get the effect of composing the gates the whole circuit is represented by a game with many players each of whom holds a value that is computed by the circuit we give each player two actions stop and go to simulate say multiplication of two values we can choose payoffs for players x y and z such that in any nash equilibrium the probability that z representing the output of the multiplication will go is equal to the product of the probabilities that x and y will go the resulting multiplication gadget see figure has a player w who mediates between x y and z the directed edges show the direct dependencies among the play ers payoffs elsewhere in the game z may input his value into other related gadgets here is how we define payoffs to induce the players to implement multiplication let x y z and w denote the mixed strategies go probabilities of x y z and w we pay w the amount x y for choosing strategy stop and z for choosing go we also pay z to play the opposite from player w it is not hard to check that in any nash equilibrium of the game thus defined it must be the case that z x y for example if z x y then w would prefer strategy go and therefore z would prefer stop which would make z and would violate the assumption z x y hence the rules of the game induce the players to implement multiplication in the choice of their mixed strategies by choosing different sets of payoffs we could ensure that z x y or z x it is a little more challenging to simulate the comparison of two real values which also is needed to simulate the brouwer function below we discuss that issue in more detail computing a brouwer function with games suppose we have a brouwer function f defined on the unit cube include players whose go probabilities represent a point x in the cube use additional players to compute f x via gadgets as described above eventually we can end up with players whose go proba bilities represent f x finally we can give payoffs to and that ensure that in any nash equilibrium their probabilities agree with and then in any nash equilibrium these probabilities must be a fixed point of f figure the players of the multiplication game the graph shows which players affect other players payoffs the brittle comparator problem there just one catch our comparator gadget whose pur pose is to compare its inputs and output a binary signal according to the outcome of the comparison is brittle in that if the inputs are equal then it outputs anything this is inherent because one can show that if a non brittle com parator gadget existed then we could construct a game that has no nash equilibria contradicting nash theorem with brittle comparators our computation of f is faulty on in puts that cause the circuit to make a comparison of equal values we solve this problem by computing the brouwer function at a grid of many points near the point of interest and averaging the results which makes the computation ro bust but introduces a small error in the computation of f therefore the construction described above approximately works and the three special players of the game have to play an approximate fixed point at equilibrium the final step three players the game thus constructed has many players the num ber depends mainly on how complicated the program for computing the function f was and two strategies for each player this presents a problem to represent such a game with n players we need numbers the utility of each player for each of the strategy choices of the n play ers but our game has a special structure called a graphical game see the players are vertices of a graph essen tially the data flow graph of f and the utility of each player depends only on the actions of its neighbors the final step in the reduction is to simulate this game by a three player normal form game this establishes that nash is ppad complete even in the case of three players this is accomplished as follows we color the players nodes of the graph by three colors say red blue and yellow so that no two players who play together or two players who are involved in a game with the same third player have the same color it takes some tweaking and argument to make sure the nodes can be so colored the idea is now to have three lawyers the red lawyer the blue lawyer and the yellow lawyer each represent all nodes with their color in a game involving only the lawyers a lawyer representing m nodes has actions and his mixed strategy a probability distribution over the actions can be used to encode the simpler stop go strategies of the m nodes since no two adjacent nodes are colored the same color the lawyers can represent their nodes without a conflict of interest and so a mixed nash equilibrium of the lawyers game will correspond to a mixed nash equilibrium of the original graphical game but there is a problem we need each of the lawyers to allocate equal amounts of probability to their customers however with the construction so far it may be best for a lawyer to allocate more probability mass to his more lucra tive customers we take care of this last difficulty by having the lawyers play on the side and for high stakes a gener alization of the rock paper scissors game of figure one that forces them to balance the probability mass allocated to the nodes of the graph this completes the reduction from graphical games to three player games and the proof related technical contributions our paper was preceded by a number of important papers that developed the ideas applied in scarf algo rithm was proposed as a general method for finding ap proximate fixed points which should be more efficient than brute force it essentially works by following the line in the associated end of the line graph described in section 1 in the context of games the lemke howson algorithm computes a nash equilibrium for player games by follow ing a similar end of the line path the similarity of these algorithms and the type of parity argument used in showing that they work inspired the definition of ppad in three decades ago bubelis 1 considered reductions among games and showed how to transform any k player game to a player game for k in such a way that given any solu tion of the player game a solution of the k player game can be reconstructed with simple algebraic operations while his main interest was in the algebraic properties of solu tions his reduction is computationally efficient our work implies this result but our reduction is done via the use of graphical games which are critical in establishing our ppad completeness result only a few months after we announced our result chen and deng made the following clever and surprising ob servation the graphical games resulting from our construc tion are not using the multiplication operation except for multiplication by a constant and therefore can even be sim ulated by a two player game leading to an improvement of our hardness result from three to two player games this result was unexpected one reason being that the proba bilities that arise in a 2 player nash equilibrium are always rational numbers which is not the case for games with three or more players our results imply that finding an e nash equilibrium is ppad complete if e is inversely proportional to an ex ponential function of the game size chen et al ex tended this result to the case where e is inversely propor tional to a polynomial in the game size this rules out a fully polynomial time approximation scheme for computing approximate equilibria finally in this paper we have focused on the complexity of computing an approximate nash equilibrium etessami and yannakakis develop a very interesting complexity theory of the problem of computing the exact equilibrium or other fixed points a problem that is important in applications outside game theory such as program verification conclusions and future work our hardness result for computing a nash equilibrium raises concerns about the credibility of the mixed nash equi librium as a general purpose framework for behavior pre diction in view of these concerns the main question that emerges is whether there exists a polynomial time approx imation scheme ptas for computing approximate nash equilibria that is is there an algorithm for e nash equi libria which runs in time polynomial in the game size if we allow arbitrary dependence of its running time on 1 e such an algorithm would go a long way towards alleviating the negative implications of our complexity result while this question remains open one may find hope at least for games with a few players in the existence of a sub exponential al gorithm running in time o nlog n where n is the size of the game how about classes of concisely represented games with many players for a class of tree like graphical games a ptas has been given in but the complexity of the prob lem is unknown for more general low degree graphs finally another positive recent development has been a ptas for a broad and important class of games called anonymous these are games in which the players are oblivious to each other identities that is each player is affected not by who plays each strategy but by how many play each strategy anonymous games arise in many settings including network congestion markets and social interactions and so it is re assuring that in these games approximate nash equilibria can be computed efficiently an alternative form of computational hardness exempli fied in arises where instead of identifying problems that are resistant to any efficient algorithm one identifies problems that are resistant to specific natural algorithms in lower bounds are shown for decoupled dynamics a model of strategic interaction in which there is no central controller to find an equilibrium instead the players need to obtain one in a decentralized manner the study and comparison of these models will continue to be an interest ing research theme finally an overarching research question for the com puter science research community investigating game theoretic issues already raised in but made a little more urgent by the present work is to identify novel concepts of ratio nality and equilibrium especially applicable in the context of the internet and its computational platforms cs final exam fall this test is closed book you will get reasonable partial credit for attempting the correct technique but recognizing that you get stuck at some point you will get signi cantly less partial credit for incorrect solutions that you claim are correct than for partial solutions where you state that you don t see how to nish the proof feel free to ask questions concentrate on communicating the main ideas in your answers don t dwell unnecessarily on details manage your time keep your answers to the rst six questions to at most a few sentences you are welcome to quote well known theorems points you have a database consisting of a single binary relation r a a the query language for the database is existential second order logic using the relation r are there any queries that can not be expressed in this query language explain points a language a is in the class parity p if there is a nondeterministic polynomial time turing machine m with the property that if x l then m accepts x on an odd number of computation paths and if x l then m accepts x on an even possibly number of computation paths consider the following problem parity sat the input for this problem is a boolean formula f in conjunctive normal form the output is if an odd number of the possible assignments to the variables satisfy f and is otherwise do you think you could prove that parity sat is log space complete for parity p explain points are there any languages that are not accepted by a family of boolean circuits explain points explain why if rp n corp p then p np points assume that someone was able to prove that one way functions did not exist would that necessarily resolve the p np problem explain points martha brought in a new york times article at the end of class yesterday trum peting some amazing new mathematical theorem proved by an automated theorem proving program the article implied that mathematicians may someday be unnecessary since all theorems may someday be provable by a computer it this possible that is is it possible to write a program that has the ability to either prove or disprove every mathematical statement explain points show that the following problem is np complete the input is a graph g with n v vertices and the question is whether g has a simple cycle spanning b nc vertices use the fact that the hamiltonian cycle problem is np complete recall that the the input for the hamiltonian cycle problem is a graph g with n vertices and the question is whether g has a simple cycle that spans all of the vertices in g points show that the following problem is undecidable the input is a two turing machine m and n the problem is to determine if l m l n use a reduction from the standard halting problem determining whether a turing machine m halts on an input x points de ne a robust oracle machine m deciding a language b to be one such that l ma b for all oracles a that is the answers are always correct independently of the oracle although the number of steps may vary from oracle to oracle if furthermore ma works in polynomial time we say that oracle a helps the robust machine m let ph be the class of languages decidable in polynomial time by deterministic robust oracle machines that can be helped show that ph np n conp points consider a programming language h that has no recursion or goto statements it only has one type of loop a for loop a for loop has the property that the number of iterations of the loop is xed and known before the loop is executed hence all programs in h much halt in nite time on all inputs show that there is a recursive language that can not be accepted by an h program points show that if time sp ace n then time sp ace points show that re is closed under kleene star recall that if l is a language then the kleene star of l denoted l is the collection of strings x such that x is equal to the concatenation of k strings xk with each xi l for i k cs final exam and theory of computation preliminary exam fall directions the test is closed book and closed notes answer at most part b questions part b questions are worth points each answer at most part a questions part a questions are worth points each one quarter credit will be given for unanswered questions answers that are way o the mark will get no credit for the part a questions usually it is then a good idea to have a paragraph giving an overview of the proof strategy technique that you will use and what the key ideas are before launching into the details part b questions the following sentence is essentially from a paper on the p vs np problem from the arxiv archive it has been previously demonstrated by the present author that the dna string concatenation problem can be reduced to the problem of nding a large clique in a graph and therefore the dna concatenation problem is np complete explain the error with this reasoning explain the famous result of baker gill and soloway from the explain the relevance of this result for resolving the p vs np question explain nash famous theorem about games start with a de nition of a game that is appropriate for this setting consider the following situation alice needs to ip a fair coin until she gets a heads let k be the number of ips until alice sees a heads alice needs to send some bits to bob over a network connection in order to communicate k to bob alice and bob can communicate a priori to determine their protocol give an expression for the expected number of bits that alice must send to bob explain why you know that approximately this many bits is necessary and su cient note that i am not asking for a protocol or a proof that a certain number is necessary i am looking for you to cite and explain well known theorem that we covered in class for which of the following two complexity classes is it likely easier to nd a complete prob lem explain your answer pp is the class of languages l for which which there exists a probabilistic polynomial time turing machine m such that if x l then m accepts x with probability and if x l then m accepts x with probability qq is the class of languages l for which which there exists a probabilistic polynomial time turing machine m such that if x l then m accepts x with probability and if x l then m accepts x with probability consider the following language g k the largest simple cycle in graph g contains exactly k edges show that this language is in start with a de nition of is p is a subset of p poly is p poly is a subset of p start with de nitions of p and p poly then give one sentence justi cations for your answer to each question results by soloway and strassen in the mid and by shor in the mid had analo gous e ects on computer scientists understanding of e cient computation in one sentence state each result then explain what the analogous e ect of each result was de ne what a perfect zero knowledge protocol is explain why bp p is contained in p poly start with a de nition of bp p and p poly state a problem that is complete for the class p with respect to nc reductions de ne nc explain why the existence of a really good pseudo random generator would allow one to conclude that bp p p start with de nitions of bp p and pseudo random generator give the interactive protocol for the graph nonisomorphism problem from the book and class you need not argue about the correctness of the protocol just try to state it clearly explain the setup of experiment with the silvered mirrors that we discussed in class when we rst discussed quantum computation what is unexpected outcome when one carries out this experiment brie y explain why this outcome occurs you have two entangled qubits in state a b c d answer the following questions you need not justify your answers what must be true about the relationship of the values a b c and d if you measure the rst qubit what is the probability that you observe a if you measure the rst qubit and observe a what is the resulting state of the two bits if b c and you rotated the rst bit by p radians what be the resulting state of the bits you can write an expression for the state you need not simplify the pcp theorem gives an alternative characterization of the complexity class np state the pcp theorem and give a de nition of the type of machine protocol system in the alternative characterization part a questions prove from rst principles that there are languages that can be accepted in exponential time that can not be accepted in polynomial time state g odel incompleteness theorem and show that it follows as a logical consequence of the fact that the halting problem is not computable decidable give the perfect zero knowledge protocol for graph isomorphism from the book and prove that the protocol is perfect zero knowledge prove from rst principles that the circuit satis ability problem is np hard the circuit satis able problem takes as input a circuit and asks whether there is any inputs to the circuit that cause a particular output to be so basically i am asking you to give a proof of cook theorem here prove that if np has polynomial time circuits then the polynomial time hierarchy collapses prove that if graph isomorphism is np hard then the polynomial time hierarchy collapses state what problem simon algorithm solves give simon algorithm prove that it solves this problem prove that simon algorithm runs in polynomial time prove that there exists some constant such that it is np hard to approximate maxsat to within a factor of you may assume the pcp theorem cs final exam fall directions the test is closed book and closed notes the test consists of questions each with a part a and a part b answer at most part a questions these are worth points each answer at most part b questions these are worth points each if you are not reaonably certain of your answers please don t take random guesses it is better just to leave it blank you don t need to give every single detail in your answers just hit the main points part a questions should be answered in say to sentences please concentrate on providing clear answers to the questions that you have time to answer rather than on answering the maximum number of questions it could well be that it is not reasonable to provide reasonable answers to the maximum number of questions within the allotted time a state godel incompleteness theorem b sketch the proof of godel incompleteness theorem that uses the fact that there is no algorithm for the halting problem this proof involves the construction of a formula f you do not need to explain how to construct f you need only explain what properties f has a de ne kolmogorov complexity b prove that there is no algorithm that can determine the kolmogorov complexity of a string a draw a venn diagram showing the inclusion relationship between the complexity classes log space polynomial time s p p s p p and polynomial space state which inclusions are known to be proper strict b prove the strictness of these inclusions from rst principles a give an example of a logic problem that is known to be polynomial time complete for the complexity class of polynomial space b prove from rst principles that this problem is complete for polynomial space a de ne p poly uniform nc and non uniform nc b show that if np p poly then p s p a de ne the randomized complexity classes rp co rp zpp and bp p b prove zpp rp n co rp a de ne ip k that is de ne what a k round interactive protocol is how do you decide who sends the rst message in such a protocol b give an ip protocol that takes as input a propositional boolean formula f and an integer k and accepts if the number of satisfying assignments to f is at least k and rejects if the number of satisfying assignments to f is less than k the protocol may accept or reject if the number of satisfying assignments is in between k and k give some intuition not necessarily a formal proof why this protocol is correct a de ne what is meant by semantic security within the context of a cryptographic protocol b construct a direct acyclic graph where the nodes represent the following statements i one time pad private key cryptography with short keys is possible ii bpp algorithms can be derandomized to give subexponential time deterministic algorithms iii public key cryptography is possible iv one way functions exist v pseudo random generators exist where a directed edge x y from a vertex x to a vertex y means that statement x logically implies statement y if your graph contains edges x y and y z then it shouldn t contain the edge x z as this is a logical consequence so in some sense your graph should have the minimal number of edges prove one of the implications hint pick an easy implication some of these implications are very easy to prove and some are very hard to prove a explain the epr experiment you can just explain what alice can bob are supposed to accomplish not how they accomplish it why is this protocol famous or two ask the question a di erent way what did the textbook want us to learn from this example b give alice and bob protocol from the text calculate the probability of bob and alice winning given that they see di erent inputs show your work a when proving np pcp poly explain how error correcting codes come into the picture start with a de nition of pcp poly b to prove that np pcp poly the book showed that an np complete problem was in pcp poly this np comlete problem involved showing that there were solutions to a sequence of equations part of the protocol involved checking the assignment to the variables explain how the assignment was encoded using as an example assignment explain what property of this encoding is checked how it is checked and why this check works computational complexity of games and puzzles many of the games and puzzles people play are interesting because of their difficulty it requires cleverness to solve them often this difficulty can be shown mathematically in the form of computational intractibility results every npcomplete problem is in some sense a puzzle and conversely many puzzles are npcomplete twoplayer games often have higher complexities such as being pspacecomplete ww pp see references below is disparaging of this sort of result writing that this asymptotic result says little about the difficulties of calculating good strategies describing nphard game positions as degenerate and relatively dull and advocating as a response to hardness proofs looking for additional rules and conditions that would make the game easier but to me npcompleteness is not the end but the beginning of the study of a game it shows that the game is complex enough that we can encode interesting computational problems within it if a game is in p it becomes no fun once you learn the trick to perfect play but hardness results imply that there is no such trick to learn the game is inexhaustible and of course npcompleteness or pspacecompleteness doesn t even rule out the possibility of computing game values exactly true it seems to imply that worstcaseexponential algorithms are required but there is still plenty of interesting work in designing analyzing and implementing such algorithms there is a curious relationship between computational difficulty and puzzle quality to me the best puzzles are npcomplete although some good puzzles are in p relying on gaps in human intuition rather than on computational complexity for their difficulty some puzzles are even harder than np for instance sliding block puzzles and sokoban are pspacecomplete but to me this means only that the problem can have an annoyingly long sequence of manipulations in its solution for twoplayer games one encounters a similar phenomenon at a higher level of complexity the tree of potential interactions in a game typically gives rise to pspacecompleteness results of a sort i find more interesting than pspacecompleteness in puzzles however some games are harder exptimecomplete which to me means that it may sometimes be necessary for a wellplayed game to go on for a tediously long sequence of moves perhaps some games or puzzles in which the players start with incomplete knowledge of the game or puzzle configuration might lead to other types of completeness e g macompleteness or pcompleteness for finding the starting move most likely to succeed but i know of no such results i primarily list here real games and puzzles games that were invented to be played rather than to be analyzed so i m not including some of the more artificial entries in e g ww or gj such as sequential truth assignment if someone can point me to a tournament for these games a copy of the game sold in stores or a program for people to play them against their computers i ll consider adding them one caveat npcompleteness is not a concept that applies to a single puzzle or game position or even a finite collection of positions it only makes sense to talk about an infinite family of problems as being npcomplete for this reason games like chess cannot themselves be npcomplete as they only have a finite albeit unthinkably large number of possible positions in many cases however there is a natural generalization from some finite game or puzzle to an infinite family of game positions on arbitrarily large game boards in which it makes sense to talk about npcompleteness the fact that these infinite generalizations are computationally hard gives us some justification for believing that the original finite games are also hard in some less welldefined sense amazons description players move queens on an n n board as part of each move shooting an arrow from the moved piece arrows move like a chess queen when shot but are immovable afterwards the arrows eventually block the movement of the queens the last player to complete a move wins gameplay combines golike goals of surrounding territory with chesslike tactics of blocking opposing pieces lines of play the most commonly used starting configuration involves four queens of each color placed around the edges of a board status pspacecomplete endgames in which pieces of opposing colors are separated from each other are nphard checkers and draughts description players move pieces diagonally forward one square at a time on alternating squares of an n n board removing the other players pieces by jumping diagonally over them object is to leave opponent with no move by blocking or jumping all pieces the version is called checkers but on larger boards it is called draughts status exptimecomplete chess description this game is both too complicated and too wellknown to describe here in detail but the basic idea is to move pieces around an board capturing one opponents pieces until the game is ended either by checkmate one player winning by forcing the capture of the opposing player king or by various kinds of draws status this is a finite game but generalizations to n n boards are exptimecomplete dots and boxes description this childhood game is played with pencil and paper starting from a drawing of a rectangular lattice of dots the players alternately connect adjacent dots with line segments if a player forms a foursegment square he or she scores a point and gets another move status ww describes a generalized version of the game that is nphard by a reduction from finding many vertexdisjoint cycles in graphs the same result would seem to apply as well to positions from the actual game by specializing their reduction to trivalent planar graphs this is very closely related to but not quite the same as maximum independent sets in maximal planar graphs description of the positions in a matrix are filled by tiles leaving one unfilled hole tiles adjoining the hole can be shifted into the hole the object being to form some particular permutation of the tiles typically forming a picture out of fragments printed on the tiles status this is a finite problem but can easily be generalized to n n matrices testing whether a solution exists is in p but finding the solution with the fewest moves is npcomplete cubic description something of a cross between sokoban and same game this puzzle involves pushing blocks left or right in a polygonal region where they are subject to a gravity that pulls them downward whenever possible blocks have colors and when multiple blocks of the same color become adjacent they vanish the goal is to eliminate all blocks status friedman claims to prove that cubic is npcomplete his reduction clearly shows that it is nphard but it is less obvious to me that it is in np go description this ancient game is played by placing stones on a board when a group of stones of one color is completely surrounded by stones of the other color the surrounded group is removed from the board the object is to control empty squares by surrounding them after both players are unwilling to continue play these squares are counted and the scores adjusted by the numbers of stones that had been removed status this is a finite game but can be generalized to n n boards even without ko special rules related to repetition of positions the game is pspacehard with ko japanese rules it is exptimecomplete it is apparently still open whether chinese or us rules go is exptimecomplete even certain simple endgames in which the go board has been decomposed into many small independent regions of play are pspacehard hex description players take turns placing pieces on a diamondshaped board composed of hexagonal tiles each player owns two opposite sides of the board and aims to connect those sides by a contiguous path of pieces status first player wins by a strategystealing argument but pspacecomplete in general the shannon switching game in which pieces are placed on edges known on the square grid as gale a related game has been sold as bridgeit is in p hex description players take turns placing pieces on a diamondshaped board composed of hexagonal tiles each player owns two opposite sides of the board and aims to connect those sides by a contiguous path of pieces status first player wins by a strategystealing argument but pspacecomplete in general the shannon switching game in which pieces are placed on edges known on the square grid as gale a related game has been sold as bridgeit is in p kplumber description rotate tiles containing drawings of pipes to make the pipes form a connected network status npcomplete mastermind description one player sets up a secret configuration of colored pins and the other player makes a sequence of guesses about the configuration after each guess the player with the secret tells the other player how many many pins are correct and how many are the correct color but in the incorrect position the object is to make as few guesses as possible status since this game relies on secret information it can be treated using classical game theory but the relevant payoff matrices are so large as to make computation with them intractible finding a solution compatible with the guesses made so far is npcomplete the complexity of determining whether such a solution is unique or of playing either side of the game optimally remain open othello description this game also known as reversi is played with reversible pieces on a square board players alternate placing pieces on the board placed with the player color up when a piece is placed the player also reverses lines of pieces of the opposite color sandwiched between the new piece and old pieces of the same color so that those lines also become pieces of the player color the object is to have the most pieces when the board becomes filled phutball description played on a go board with one black stone and many white stones players on each turn either place a single white stone on any vacant position or make a sequence of jumps of the black stone over contiguous groups of white stones removing the jumped stones the object is to move the black stone to the edge of the board nearest the opponent status testing for the existance of a winning move is npcomplete the complexity of determining the correct outcome of a game position is pspacehard pearl puzzles description a japanese pencil and paper puzzle in which black and white pearls are placed in a square grid the object is to draw a polygon with edges parallel to the grid lines through all the pearls with vertices at each black pearl and adjacent to each white pearl but no vertices adjacent to any black pearl or at any white pearl status npcomplete rush hour description this is a commercial sliding block puzzle in which pieces representing cars are placed on a grid with walls surrounding its perimeter except for one exit edge the cars take up two or three adjacent grid cells and can only move forwards or backwards the goal is to move a specially marked target car out through the exit similar java applets are available at puzzleworld and rhymezone status this is a finite puzzle but generalizations to arbitrarily large rectangles are pspacecomplete rush hour description this is a commercial sliding block puzzle in which pieces representing cars are placed on a grid with walls surrounding its perimeter except for one exit edge the cars take up two or three adjacent grid cells and can only move forwards or backwards the goal is to move a specially marked target car out through the exit similar java applets are available at puzzleworld and rhymezone status this is a finite puzzle but generalizations to arbitrarily large rectangles are pspacecomplete shanghai description this solitaire game is played with mahjongg tiles which like playing cards can be arranged in groups of four similar or identical tiles the tiles are placed randomly faceup into stacks forming some particular shape most computer solitaire implementations include several shapes of varying difficulty only the top tile of a stack is visible although some shapes involve tiles partially covered by other tiles which can be seen but not removed the object is to remove matching pairs of uncovered tiles until all tiles have been removed status npcomplete even if all tile positions are known pspacehard even to approximate the strategy with maximum probability of success when some tiles are covered and unknown pcomplete to count solutions personal communication from michiel de bondt proof of npcompleteness reduction from joint with michiel de bondt an earlier reduction listed here until early was not valid we have a different type of tile for each variable clause and term appearance of a variable in a clause of the instance with four copies of each type of tile we arrange these into stacks of the following types a big stack with three layers the top and bottom layers each include one copy of each variable tile the middle layer includes one copy of each clause tile for each term a stack with two copies of that term on top of a tile for the clause in which the term appears for each variable two stacks each with a variable tile on top of a stack of term tiles one of these stacks has two copies of each term tile in which the variable tile appears positively and the other stack has two copies of each term tile in which the variable appears negatively the variable tiles can only be matched by pairing one tile from the big stack with one tile from one of the two term stacks if this pairing is done according to a satisfying assignment it will then be possible to match the term tiles from all of the chosen variable stacks freeing at least one copy of each clause tile the freed clause tiles can then be used to complete the pairing of the big stack which in turn allows the remaining variable term and clause tiles to be paired on the other hand if there is no satisfying assignment then there is no way of freeing one copy of each clause gadget and the problem cannot be solved sokoban description a warehouseman moves around a rectilinear maze pushing pallets one at a time from initial locations scattered throughout the maze until they are all placed in a designated loading dock status pspacecomplete twixt description players alternate placing vertices on a square grid each player vertices may be connected with edges connecting pairs of vertices a knight move apart after placing a vertex the player may remove some of his own edges or add more of them no two edges may cross the object is like hex to connect opposite sides of the board by a path status npcomplete to determine whether a single set of vertices can support a connecting path pspacecomplete to determine the game value by a reduction from hex course format i will give all of the lectures there will be homework assignments and a final exam that will count equally in the final grade you may work in groups of size or on the homeworks you may talk to others in the class about the homework but may not consult any outside source e g other students the www other texts to help solve the homework problems solutions are due at the start of class when they are due and must be written using latex figures may be hand drawn tentative schedule historical pre results chapter formalizing computation church turing notes finite state machines cannot count church turing thesis evidence simulation undecidable problems church turing notes halting proof diagonization term rewriting proof reduction logic proofs and computation notes goedel incompleteness theorems formalizing information notes entropy source coding theorem shannon circa kolmogorov complexity circa noncomputability of kolmogorov complexity p ph and pspace chapters and cs fall http people cs pitt edu kirk time and space definition of logspace p pspace exptime constant time and space speed up theorems logspace in p and pspace is in exptime time and space hiearchy theorems machine based complete problems for p pspace exptime circuit value problem is log space complete for p tqbf is polynomial time complete for pspace pspace hardness of some game see for example this list ph and alternation definition of ph ph in pspace machine based complete problems for ph why no obvious complete problems for np intersect conp cook levin theorem np completeness of theorems pspace aptime baker gill soloway ladner theorem circuits chapter definition of uniform nc parallel computation thesis wikepedia nlogspace and logspace in nc definition of p poly karp lipton theorem randomization chapter random polynomial time algorithm for primality soloway and strassen definition of bpp rp co rp zpp why these classes seem to not have complete problems zpp in bpp bpp in p poly cs fall http people cs pitt edu kirk bpp in polynomial time hierarchy interactive proofs chapter examples uno card color graph non isomorphism definitions of am and ip am protocol for approximate set size gni in am and ip k in am k p in ip ip pspace if gi is np complete then the polynomial time hierachy collapses history of ip pspace result great reading about how research happens in the real world cryptography chapter mostly one time pad private key public key cryptography definition of one way function definition of pseudo random generators definition of semantic security one way functions imply pseudo random generators which imply private key cryptography with smallish keys pseudo random generators imply derandomization of bpp and bpp subset subexponential time definition of perfect zero knowledge zero knowledge proof of graph isomorphism energy billiard ball circuits reversable computation minimum energy computation cs fall http people cs pitt edu kirk quantum computation chapter mostly two silvered mirror experiment epr and the parity game provably secure quantum cyrptography heisenberg uncertainty principle simon algorithm grover algorithm shor algorithm popular description approximation algorithms chapter mostly statement of pcp theorem hardness of approximation of maxsat how to use pcp to prove hardness of approximation by reduction np subset pcp poly information theoretic lower bounds chapers and sorting lower bound element uniqueness lower bound communication complexity and the tiling lower bound for equality yao minimax technique for sorting due wednesday august assume alice and bob follow some communication protocol that is overheard by that pesky eavesdropper eve alice and bob would like that their communication is cryptographically secure in that eve doesn t learn anything from what she hears do you best to come up with a formal definition of cryptographically secure you may not look in the text or use any outside sources the point of this exercise is not for you to find the right definition but to understand the difficulties of coming up with such a definition by going through the thought process yourself you proposed definition will not be graded on correctness if you can t think of a definition write a paragraph or two on things that you thought about everyone that turns in something will get full credit here due friday august show from first principles that there is no finite state machine that accepts the language of formulas of the form x y z where natural numbers x y and z encoded in binary in the standard way and where x plus y is indeed equal to z so for example the input is in the language so intuitively this shows that finite state machines can not add in the most obvious interpretation of add show that there is a finite state machine that can accept the language over a string of symbols that intuitively mean where the first column plus the second column is equal to the third column so to reuse our example from problem the symbol input 001 011 is in the language because 111 for simplicity you may assume that you can read the input tape in either direction though this assumption isn t strictly necessary so intuitively this says that finite state machines can add for some reasonable intepretation of add due monday september problem from the text basically you want to show how to simulate a turing machine with a dimensional tape by a turing machine with a one dimensional tape the purpose of assigning this problem is to gain some some familiarity with simulation arguments suggestion use the proof of claim in the back of the book both to get an idea of how to solve this problem and as a model as to the level of detail that you should have in your write up note this is problem in the online preliminary version of the book at http www cs princeton edu theory complexity modelchap pdf in general problem numberings will be from the final version of the text not the online preliminary version due wednesday september consider a programming language mini java that only has one type of loop and the number of iterations of the loop must be determined when the loop is first encountered so a loop statement might look like repeat n times and the variable n is evaluated when the statement is reached you can assume that the program has a variable size that is instantiated to the input size when the program starts running otherwise you couldn t even read the input so all mini java programs must halt on all inputs show by diagonalization that there is a language accepted by a java program that is not accepted by any mini java program use diagonalization give a concrete example of a language that is not acceptable by any mini java program but that is accepted by a java program the purpose of assigning this problem is to gain some familiarity with diagonalization arguments due friday september consider the following decision problem the input consists of a collection of types of unit squares four special colors that we will call l r t and b and a collection of color rules a unit square type is an ordered tuple of colors there is no a priori bound on the number of possible colors and you can use as many unit squares as you like that have these four colors in clockwise order around the outside for example a unit square type might be red red blue green a color rule is a pair of colors meaning that square sides of these colors are allowed to touch for example a color rule might be red blue each of the four finite automata take as input a string of colors and either reject or accept the string the decision is to determine whether there is a rectangle filled with available types of unit squares placed to obey the color rules where the colors on the top are all t the colors on the left are all l the colors on the right are all r and the colors on the bottom are all b show that there is no algorithm to solve this problem hint recall the proof that the term rewriting problem is not computable think of row i of the rectangle as being the configuration of a turing machine after step i think of a sequence of rows as being a computation history think of many of the colors encoding either some small string of tape symbols or a position of the tape head the state in the finite state control and some tape symbols the only issue that is not straightforward is how to guarantee consistency when the tape head moves to handle this think about there being some overlap in information between consecutive colors on the bottom top of a row further hint note that there is no algorithm to determine whether a turing machine halts when given the empty tape as input you might prove this as a warmup or just look in the class notes the purpose of the problem is to get used to the concepts of configuration and computation history and proving hardness via a reduction due monday september the goal of this problem is to finish up the proof in class of a version of goedel incompleteness theorem that there is no complete sound axiomatization of standard arithmetic consider an string s that represents the computation history of a particular turing machine t e g s sk for a computation that takes k steps here si is the state on the ith step the number of symbols j in each si is the number of states in t plus the size of the tape alphabet for t plus one symbol to represent the position of the tape head consider s as a base j integer explain how to express the informal sentence s represents a valid computation for t as a first order logical sentence using standard arithmetic operators such as plus minus times division mod floor ceiling equals less than etc and standard logical operators and or not etc note that this is required for the proof of that there is no complete axiomatization of standard arithmetic the purpose of this problem is to reinforce the proof of godel incompleteness theorem and get comfortable with the arithmetization of computation due wednesday september email to pitt edu by noon consider first order logical sentences of arithmetic where your are allowed to use and without loss of generality we may assume that the only logical operators are and and not and that all quantifiers appear first so you might have a formula like thereexists x forall y forall z thereexists w such that x y z and not y z w x our goal here is to show that there is an algorithm to accept exactly the language of true formula using the following strategy recall from problem that we know how to build a finite state machine that accept tuples x y z w properly encoded that satisfy x y z and a finite state machine that accepts tuples x y z w properly encode that satisfy y z w x explain why if you have finite state machines for x y z and y z w x you can get a finite state machine to tuples x y z w properly encode that satisfy x y z and not y z w x now consider the quantifiers from the inside out explain how to construct a finite state machine that accepts tuples x y z properly encoded with the property that thereexists w such that x y z and not y z w x explain how to construct a finite state machine that accepts tuples x y properly encoded with the property that forall z thereexists w such that x y z and not y z w x extend this to the whole formula then explain how to use the final finite state machine to get your final answer it is ok for your write up to use this example but it should also explain how the algorithm would work on a generic instance concentrate on high level ideas not low level details the purpose of the problem is to increase your comfort with first order logic due friday september email to pitt edu by noon recall the definition of entropy of a probability distribution x over a set u h x x in u prob x lg prob x now define the conditional entropy of not necessarily independent probability distributions x and y to be h x y y prob y x prob x y lg prob x y subproblem a prove that h x y x y prob x y lg prob x y subproblem b explain why intuitively h x y tells you how much information one gains from seeing the outcome of x given that one has already seen the outcome of y subproblem c prove that h x h x y h y h y x subproblem d consider the process of drawing cards without replacement from a deck and then looking at the cards in some order what does the above statement say about how the order that you look at the cards affects the amount of information that you receive after each time you look at the cards the purpose of this problem is to get more confortable with both the intuition and formal definitions of entropy due monday september assume a log space reduction from a language a to a language b so more precisely there is a turing machine t with three tapes a read only input tape a read write work tape and a write only output tape t only uses log of the input size many cells on the read write work tape further t never backs up the tape head on the write only output tape so the tape head on the write only tape either stays in position or moves to the right the machine t has the property that a string x in in a iff the contents of the write tape when t ends computation on input x is in b now show that if there is a log space turing machine s that accepts b then there is a log space turing machine u that accepts a note that this is not trivial because u will have enough work tape to write down the output of t this is a good problem to show how to make use of the fact that space is reusable due wednesday october consider the following game played by two players a and b on a directed graph g with a designated start vertex on the first move player a picks an edge v leaving then and v are designated as visited on each even numbered move player b picks an edge v w from the current vertex v to a vertex w hasn t been visited before if no such vertex w exists then player b loses vertex w then is designated as visited and becomes the current vertex on each odd numbered move player a picks an edge v w from the current vertex v to a vertex w hasn t been visited before if no such vertex w exists then player a loses vertex w then is designated as visited and becomes the current vertex so the players a and b taking turns picking edges in a directed path p starting at s with a player losing if he she can t extend the path prove that determining for a particular g and s whether the first player has a winning strategy is pspace complete under polynomial time reductions you can prove hardness by reduction from the problem of determining whether a quantified boolean formula f is true hint conceptually the graph g will have two parts and that p will traverse in that order in some sense represents the quantifiers in f so there will one vertex in for each quantifier in f and for each such vertex there will be two exiting edges corresponding to true and false then needs to verify that there are choices for the first player corresponding to the existential quantifications in f in such that for all choices of the second player corresponding to the universal quantifications in the first player will win iff f is true due friday october show that if sat is polynomial time many to one reducible to the complement of sat that the polynomial time hierarchy is equal to np this is problem from the text the purpose of the problem is to get comfortable with the definition of the polynomial time hierarchy the problem is almost trivial if one was comfortable with this definition due monday october email to michael pitt edu by noon the time hierarchy theorem leaves open the possibility that time n time n log log n show that it is unlikely that one can show that time n time n log log n by simulation by showing that there is an language b such that time n b is not equal time n log log n b hint try to mimic the proof that there is a language b such that p b is not equal np b first ask yourself what question about b can a time n log log n machine can easily answer that a time n machine can not answer obviously the purpose of this problem is to get more comfortable with the baker gill and soloway result due friday october email to michael pitt edu by noon show that vc dimension is p complete this is exercise in the hard copy fo the book and exercise in the online preliminary copy hint the online copy of the text has a suggestion i m not sure how hard this problem is so it may be hard the purpose of the problem is to gain comfort with the concept of completeness and the definition of the polynomial time hierarchy due monday october show that nc is a subset of logspace hint this of the nc computation as a circuit show how to compute the value of the circuit using log space dfs due wednesday october problems and from the text they are related and quite easy show that for every k that p has languages whose circuit complexity is omega n k the circuit complexity is the size of the minimum sized circuit that decides the language show that if p np then there is a language in exponential time that requires circuits of size omega n n hint the answer for 6 should be useful duw friday october prove that zpp rp intersect co rp problem from the text this is problem in chapter in the onnline preliminary version of the text due wednesday october this homework is to be done idividually not in your group read the history of the ip pspace result given in http www cs pitt edu kirk babaiipstory pdf write a paragraph explaining what you think was the right way to handle the publication issue that is what papers should have come out of this and who should the authors have been due monday october problem from the text this is the same as problem minus part c in chapter in the online preliminary version of the text due wednesday october show that mam is a subset of am note that this is a special case of 7 in the text and that there is a hint in the book due monday november email to the ta by noon problem from the text i strongly encourage you to first do the proof for when n due friday nobember email to the ta by noon from the text due wednesday november problem from the text due friday november 15 read and understand the protocol in problem exercise 17 part b from the text write a one paragraph summary of the protocol in your own words then write a paragraph explaining intuitively what it means for an interactive proof to be computationally zero knowledge then write a paragraph to explain why it is intuitively believable why this protocol is computationally zero knowledge recall the silvered mirror experiment which was essentially a hadamard gate followed by a not followed by a hadamard gate in this experiment the state of the bit coming out of the hadamard gate was the same as the state of the bit entering if the entering bit was not in superposition show by basic calculations that this remains true if the entering bit is in superposition due monday october due monday november 26 part a show that if you have a bit system in state a b c d consider two experiments in one you measure the first qubit and then measure the second qubit and in the other you measure the second qubit and then measure the first qubit show that the probability distributions that the classical bits that you observe in the end is independent the order that you look at the bits this is very easy but and is just a warm up problem to get your brain flowing part b work out in detail the probability that alice and bob observe a b in the case that x y in the epr experiment in the book show every step of your calculations and give lots of explanation due wednesday november consider the same set up as the parity game alice and bob split two entangled bits alice and bob are then split up alice is then given classical bits x and y depending on the value of x and y alice sends bob qubit from this single qubit bob determines with certainty the two classical bits x and y from his entangled bit and the sent qubit explain how to accomplish this you need to explain alice s encoding strategy bob s decoding strategy and why these strategies work note that you can extend this to allow for the transmission of classical bits using only n quantum bits hint the possible states of the sent qubit will be sqrt sqrt sqrt sqrt sqrt sqrt sqrt sqrt due friday november the goal of this problem is to find a way to transmit information about a qubit by sending two classical bits alice and bob split up entangled bits ab in state assume that now alice is given qubit x so x is in some unknown superposition between states and alice now performs the following reversible operation on x if a then negate x alice then runs qubit a through a hadamard gate alice now measures the current values of a and x and sends these two classical bits to bob explain what the state of all the particles a b and x is after each of alice s operations then explain how bob can use the two classical particles to change the state of b to the original state of x note that when alice measures a and x then she can no longer recover the original state of qubit x due monday november problem from the text this is very easy problem 7 from the text this should be pretty straightforward just convert the interactive proof for the permanent from chapter as the interactive proof for graph non isomorphism was converted in example 7 due wednesday november there is no class on this day email to the ta by problem 16 from the text note that the problem allows for rational solutions problem 19 from the text hint there is a simple reduction from maxsat due monday monday december there is no class on this day email to people cs pitt edu kirk notes informationnotes txt strings amount of information appropriate concept terabit of s low kolmogorov complexity terabit of random bits high entropy terbit of bits of pi low kolmogorov complexity terbit of www pages medium kolmogorov complexity entropy examples coin flip prob a prob c prob g prob t lemma if x and y are independent random events then entropy h should satisfy h x y h x h y another example prob a prob c prob g prob t definition entropy of a probability distribution x over a set u h x x in u prob x lg prob x here h x lg prob x is the entropy in bits of the string x note that prob x h x so h x is about how many bits you expect to see source coding theorem shannon assuming that you have an input that is a string s where each element of s is drawn independently according to a distribution x the is a scheme that can transmit s using only about s h x bit every possible scheme uses at least about s h x bits proof divide s into blocks of size n for some reasonably large n the probability that a block will be the string value b is x in b prob x the expected probability that you will see a string b is x in b prob x key insight it is very likely that you will see a block b whose probablity is about n h x so the distribution you see for a particular block is very close to a uniform distribution over n h x achievability so mostly you see blocks of size n of equal probability so you use nh x bits for each such equally probable block use any reasonable encoding on the unlikely blocks optimality forget about encoding unlikely strings you can t do better than using an equal number of bits for equally probable outcomes kolmogorov complexity intuition measures information of a fixed string rather than a distribution over strings objects as does entropy definition the kolmogorov complexity k x of a string x is defined to be http people cs pitt edu kirk notes informationnotes txt 24 people cs pitt edu kirk notes informationnotes txt k x strings y and decoders d such that d y x length of y plus length of d programs p that write x on an empty input length of p theorem the programming language only affects k x by an additive constant theorem there are incompressable strings strings where k x length of x of every length n proof pigeon hole principle theorem there is no algorithm m to compute k x proof to reach a contradiction assume m exists consider the program for i to n do if m ith string of length n n then output this string and halt note that by the previous thoerem always halts and outputs a string with kolmogorov complexity at least n but as has only o log n so for some sufficiently large n we get a contradiction to the output of having kolmogorov complexity n since is a program of length o log n that outputs this string http people cs pitt edu kirk notes informationnotes txt 24 people cs pitt edu kirk notes logicnotes txt history and background hilbert s problems for the next century http en wikipedia org wiki hilbert problem prove that the axioms of arithmetic are consistent godel s incompleteness theorem s kind of kill s hilbert s second problem there is no sound and complete axiomization of arithmetic definition an axiomization is sound if axioms phi then axioms phi that is one can only derive true statements from phi definition an axiomization is complete if axioms phi then axioms phi that is one can derive every true statement using phi example axioms of group theory closure for all a b in g the result of the operation a b is also in g associativity for all a b and c in g the equation a b c a b c holds identity element there exists an element e in g such that for all elements a in g the equation e a a e a holds inverse element for each a in g there exists an element b in g such that a b b a e where e is the identity element venn diagram nt some theory or axiomization of numbers and n is the standard model of arithmetic phi nt phi n phi neither n phi not n not phi n not phi nt not phi not phi detour to review some basic first order logic http people cs pitt edu kirk notes logicnotes txt 24 people cs pitt edu kirk notes logicnotes txt generic model m generic theory t generic statement s t is a theory of m iff t is the set of logical propositions that are true for m definition if any model satisfying also satisfies definition t s if any model satisfying t also satisfies s definition t s if there is a proof of s using t as axioms definition proof sequence of sentences such that each formula is an axiom or each formula follows from sentences by a proof rule usually the proof rules are standard e g modus ponens independent of the system in question two warm up lemmas definition a set is recursively enumerable iff there is a program that outputs only members of that set and every member of the that set is eventually output warm up lemma theoremhood s axioms s is recursively enumerable if the axiomization is finite if there only only finitely many consequence of any proof rule then just do a breadth first search of the proof tree of possible proves what if you have a proof rule like forall x phi x yields phi phi answer on i th iteration consider all nodes in derivation tree of depth i where each node on path from the root is one of the first i children warm up lemma if there is a sound and complete axiomization of arithmetic then phi number theory phi is recursive decidable proof search for all proofs for phi and not phi dovetailing the searches and now the proof proof of godel s incompleteness theorem by reduction from the halting problem let p and i be the input to the halting problem we want to create a first order sentence of the form thereexists w p w where p w verifies that w is a valid computation history of p on i now since phi number theory phi is recursive we can to determine whether thereexists w p w or not thereexists w p w is true this allows us to decide whether p halts on i how find p w configurations are encoded as strings as proof of term rewriting if there are b symbols then these strings are then read as base b integers yields c d where c are d are integers that encode configurations can be expressed in first order logic the div and mod operators allow you to cut the string there exist two places to cut c and d such that c and d are equal to the left of the left cut and to the right of the right cut and are related by the table relation in the fixed number of places between the http people cs pitt edu kirk notes logicnotes txt 24 people cs pitt edu kirk notes logicnotes txt cuts the fact that a sequence of configurations put a new symbol between each configuration encodes a computation can be expressed in first order logic in the following way each configuration yields the next the first configuration is the starting configuration and the last configuration is the accepting configuration proof by example consider the turing machine states p start q h halt input symbols space p q right p p right q p left q p right p space h space stay q space h space stay input parallel computation thesis from wikipedia the free encyclopedia in computational complexity theory the parallel computation thesis is a hypothesis which states that the time used by a reasonable parallel machine is polynomially related to the space used by a sequential machine the parallel computation thesis was set forth by chandra and stockmeyer in in other words for a computational model which allows computations to branch and run in parallel without bound a formal language which is decidable under the model using no more than steps for inputs of length n is decidable by a nonbranching machine using no more than units of storage for some constant k similarly if a machine in the unbranching model decides a language using no more than storage a machine in the parallel model can decide the language in no more than steps for some constant k the parallel computation thesis is not a rigorous formal statement as it does not clearly define what constitutes an acceptable parallel model a parallel machine must be sufficiently powerful to emulate the sequential machine in time polynomially related to the sequential space compare turing machine nondeterministic turing machine and alternating turing machine n blum introduced a model for which the thesis does not hold however the model allows parallel threads of computation after steps see big o notation parberry suggested a more reasonable bound would be or in defense of the thesis goldschlager proposed a model which is sufficiently universal to emulate all reasonable parallel models which adheres to the thesis chandra and stockmeyer originally formalized and proved results related to the thesis for deterministic and alternating turing machines which is where the thesis originated toffoli gate from wikipedia the free encyclopedia in logic circuits the toffoli gate also ccnot gate invented by tommaso toffoli is a universal reversible logic gate which means that any reversible circuit can be constructed from toffoli gates it is also known as the controlledcontrollednot gate which describes its action it has inputs and outputs if the first two bits are set it inverts the third bit otherwise all bits stay the same contents background universality and toffoli gate related logic gates relation to quantum computing see also 6 references 7 external links background an inputconsuming logic gate l is reversible if for any output y there is a unique input x such that applying l x y if a gate l is reversible there is an inverse gate l which maps y to x for which l y x from common logic gates not is reversible as can be seen from its truth table below input output the common and gate is not reversible however the inputs and 10 are all mapped to the output reversible gates have been studied since the the original motivation was that reversible gates dissipate less heat or in principle no heat if we think of a logic gate as consuming its input information is lost since less information is present in the output than was present at the input this loss of information loses energy to the surrounding area as heat because of thermodynamic entropy another way to understand this is that charges on a circuit are grounded and thus flow away taking a small quantity of energy with them when they change state a reversible gate only moves the states around and since no information is lost energy is conserved more recent motivation comes from quantum computing quantum mechanics requires the transformations to be reversible but allows more general states of the computation superpositions thus the reversible gates form a subset of gates allowed by quantum mechanics and if we can compute something reversibly we can also compute it on a quantum computer universality and toffoli gate 24 toffoli gate wikipedia https en wikipedia org wiki any reversible gate that consumes its inputs and allows all input computations must have no more input bits than output bits by the pigeonhole principle for one input bit there are two possible reversible gates one of them is not the other is the identity gate which maps its input to the output unchanged for two input bits the only nontrivial gate is the controlled not gate which xors the first bit to the second bit and leaves the first bit unchanged truth table permutation matrix form input output unfortunately there are reversible functions that cannot be computed using just those gates in other words the set consisting of not and xor gates is not universal if we want to compute an arbitrary function using reversible gates we need another gate one possibility is the toffoli gate proposed in by toffoli this gate has inputs and outputs if the first two bits are set it flips the third bit the following is a table of the input and output bits truth table permutation matrix form input output 0 0 0 0 0 0 1 1 1 0 it can be also described as mapping bits a b c to a b c xor a and b the toffoli gate is universal this means that for any boolean function f xm there is a circuit consisting of toffoli gates that takes xm and some extra bits set to 0 or 1 to outputs xm f xm and some extra bits called garbage essentially this means that one can use toffoli gates to build systems that will perform any desired boolean function computation in a reversible manner related logic gates the fredkin gate is a universal reversible gate that swaps the last two bits if the first bit is 1 a controlledswap operation 24 toffoli gate wikipedia https en wikipedia org wiki 4 fredkin toffoli billiard ball model for gates the nbit toffoli gate is a generalization of toffoli gate it takes n bits xn as inputs and outputs n bits the first n 1 output bits are just xn 1 the last output bit is and and xn 1 xor xn the toffoli gate can be realized by five twoqubit quantum gates this gate is one of the reversiblegate cases that can be modeled with billiard balls see billiardball computer the billiard ball modeling was introduced by fredkin and toffoli an example of how the collisions are used to model an electronic gate is shown in the figure relation to quantum computing any reversible gate can be implemented on a quantum computer and hence the toffoli gate is also a quantum operator however the toffoli gate can not be used for universal quantum computation though it does mean that a quantum computer can implement all possible classical computations the toffoli gate has to be implemented along with some inherently quantum gate s in order to be universal for quantum computation in fact any singlequbit gate with real coefficients that can create a nontrivial quantum state suffices 4 a quantum mechanicsbased toffoli gate has been successfully realized in january at the university of innsbruck austria see also fredkin gate reversible computing bijection quantum computing quantum gate quantum programming final computer science introduction to algorithms spring instructions the test is closed book closed notes ifyouaretaking the fnalforpreliminary examcredit thenwriteyourpreliminary number given to you by keena on the exam answer sheet but do not write your name if you are nottaking the fnalforpreliminary examcredit writeyournameontheexamanswersheet ifyouaretaking thecoursefor somegrading option otherthanthestandardlettergrading option please specify that on your exam answer sheet formost of theproblems i aminterestedintesting whetheryouunderstand thetechniques and concepts more than i am interested in the solution to the particular problem for example ifi askyoutoprovethat aproblemisnp hard i am moreinterestedinlearningif youknowhowtoprovethataproblemisnp hard thani aminthe specifcs oftheproblem ifi askyouproblemthat agreedy algorithmis correct using an exchange argument i am more interested in learning if you know how an exchange argument works than i am in the specifcs of the problem i ask these questions in the context of specifc problems to allow you to demonstrate your understanding in a concrete setting of course you have to take into account the specifcs of the problem but make sure to explain the general method technique concept that you are using as well youmustfullyjustifyall assertionsunlessspecifcallyinstructed nottodoso allarguments shouldbefrom frstprinciples e g youshould notuseresultsfromclass thebook etc unless it is explicitly stated that you may do so for that particular problem partial credit is given for the answer i don t know a blank answer will be inter pretedas idon tknow falseorcompletelyunsubstantiated assertionswill receivelesser credit solve up to of the questions clearly write the number of the problem that you are solving if you answer more than an arbitrary answers will be graded if you are uncertain about anything ask a question assume the following three statements the worst case time complexity of problem p is the worst case time complexity of problem p is o a is an algorithm for problem p which statements of the form the worst case time complexity of a is o f n are logically implied by the above three assumed statements that is for which f n is the resulting statementlogically impliedby the abovethree assumed statements which statements of the form the worst case time complexity of a is o f n are logically consistent with the above three assumed statements that is for which f n isthe resultingstatementlogically consistentby theabovethreeassumed statements which statements of the form the worst case time complexity of a is f n are logically implied by the above three assumed statements that is for which f n is the resulting statementlogically impliedby the abovethree assumed statements which statements of the form the worst case time complexity of a is f n are logically consistent with the above three assumed statements that is for which f n is the resulting statement logically consistent with the above three assumed statements giveaproof of correctnessofprim sminimumspanning tree algorithm using an exchange argument start your answer with a one or two sentence explanation of prim algorithm if it is not obvious from your write up additionally explain why an exchange argument is sucienttoprove correctness prove that integer linear programming is np hard using the fact that is np hard start your answer with a defnition of the integer linear programming problem and the considerthefollowingauctionproblem youhavenbidders bidder i submitted apositive integer bid bi in dollars and a positive integer seat count ci bidder i is willing to pay up to bi dollars to buy ci tickets to some event there is a limit c on the number of seats that you can sell given this information you want to assign a price pi to each person a person will buy if and only if pi bi you need that the total number of bought seats can be at most c you also need that your pricing is monotone in the seat count so if ci cj then you must guarantee that pi pj otherwise customers will complain about unfairness giventhese constraints you wantto maximizetheproft whichisthe sum of the prices that you set give a dynamic programming algorithm whose running time is polynomial in n and c assume that you wish to apply yao technique to show that any o time randomized algorithmfor aparticularproblem musthavehighprobability of error a give a one or two sentence explanation of statement of the appropriate form of yao technique lemma for this setting that is what does yao technique tell you to prove if you want to prove this lower bound b giveaproof thatyao stechnique lemma inthiscontext iscorrect afteryoudefne your notation theproofisjust afewinequalities just state eachinequality andgive a good sentence or two of explanation why that inequality is true givean algorithmfortheparallelprefxproblemthat will runon amachinewhereconcur rent reading and or writing of memory is not allowed your algorithm should run in time o logn on an input of size n with n processors consider a standard bloom flter of size m with hash functions hk a givepseudo code to insert an item into a bloom flter b givepseudo code to search if an item was previously inserted into a bloom flter c assumethatyouhavepreviouslyinserted n items give a formulafor theprobability of afalsepositivefora search the formula shouldbe a function of n m and k the formula can be a bit complicated but it shouldn t be ridiculously complicated you do not need simplify the formula considertheeuclideantravelingsalesmanproblem theinputis acollectionofpointsin the euclidean plane the output is the shortest way to visit all the points starting from the origin without loss of generality you can assume that the origin is one of the input points giveapolynomial approximation algorithmforthisproblem makesureto clearly state what lower bound you are using for optimal in your proof of correctness assume that you hash n items into a closed addressed hash table of size n assume that each item is equally like to hash to each table entry independent of the other items let xi be the number of items that hash to table entry i let x maxi xi show how to compute e using indicator random variables and linearity of expec tation kk n ne show that e x o logn loglogn you mayassume withoutproof that k kk you may also assume without proof that if a b and c are constants the solution to ak bk c nis k logn loglogn assume that you have a collection of n di erent kinds of pills there are also k n di erent kinds of vitamins for each pill and each vitamin you know whether that pill contains that vitamin or not if a pill contains a vitamin then you know that it contains exactly milligram of that vitamin pill i costs ci dollars you can buy part of a pill so of pill i will cost dollars the goal is spend the least amount of money so as to get at least milligram of each type of vitamin explainhowto expressthisproblem as alinearprogram give the dual of this linear program give an english interpretation to the dual problem explain how to show using the dual a lower bound on the amount of money required for aparticularinstance consider the standard network fow problem let v be the number of vertices in the instance e the number of edges in the instance and f the maximum capacity of an edge intheinstance we ll assume all edgeshaveintegercapacity algorithm a has running time v ef algorithm b has running time ve logf algorithm c has running time ve identifywhich algorithms run inpolynomial time identifywhich algorithms run in pseudo polynomial time identify which algorithms run in strong polynomial time startwithdefnitions ofpolynomialtime pseudo polynomial time and stronglypolynomial time consider the online list update problem the online algorithm maintains a list the input in each round is an item i in the list if the item i is in position k in the list then the algorithm pays a cost of k further at no cost the item i may be to any position in the list that is not further from the front of the list than i current position prove using an adversarial argumentthatthe competitive ratio or equivalently the approximation ratio or equivalentlythe worst case ratio of everydeterministic online algorithmforthisproblem must be at least final computer science introduction to algorithms spring instructions the test is closed book closed notes if you are taking the fnal for prelimary exam credit then write your prelimary number given to you by keena on the exam answer sheet but do not write your name if you are nottaking the fnalforpreliminary examcredit writeyournameontheexamanswersheet ifyouaretaking thecoursefor somegrading option otherthanthestandardlettergrading option please specify that on your exam answer sheet formost of theproblems i aminterestedintesting whetheryouunderstand thetechniques and concepts more than i am interested in the solution to the particular problem for example ifi askyoutoprovethat aproblemisnp hard i am moreinterestedinlearningif youknowhowtoprovethataproblemisnp hard thani aminthe specifcs oftheproblem ifi askyouproblemthat agreedy algorithmis correct using an exchange argument i am more interested in learning if you know how an exchange argument works than i am in the specifcs of the problem i ask these questions in the context of specifc problems to allow you to demonstrate your understanding in a concrete setting of course you have to take into account the specifcs of the problem but make sure to explain the general method technique concept that you are using as well youmustfullyjustifyall assertionsunlessspecifcallyinstructed nottodoso allarguments should be from frst principles you should not use results from class the book etc as a black box partial credit is given for the answer i don t know a blank answer will be inter pretedas idon tknow falseorcompletelyunsubstantiated assertionswill receivelesser credit solve up to of the questions clearly write the number of the problem that you are solving if you answer more than an arbitrary answers will be graded if you are uncertain about anything ask a question give a general solution to the recurrence relation t n at n a n lgk n for constants a and k consider the problem where the input is a collection of intervals and the output is a maximum cardinality collection of disjoint intervals give a greedy algorithm for this problem and prove that it is correct using an exchange argument two of the three following defnitions of big oh area logically equivalent identify the two that are logically equivalent and prove that they are equivalent take care in your exposition prove that the thirddefnitionis not logically equivalentto the other two you may assume that we are only consideringfunctions that aredefned on thepositiveintegers and are positive and non decreasing a f n o g n i n f n cg n  b f n o g n i n cg n c f n o g n i n n cg n assume that problem p has worst case time complexity o assume that problem p has worst case time complexity assume that a is an algorithm for problem p for each of the following statements state whether the statement is logically implied by the above information and state whether the statement is logically consistent with the above information so i am looking for yes no answers you need not justify your answers but this question will be graded on an all or nothing basis so take that into account a algorithm a has worst cast time complexity o b algorithm a has worst cast time complexity c algorithm a has worst cast time complexity n d algorithm a has worst cast time complexity o n considerthefollowingproblem theinputisan n by n array e entry e i j specifes the amount of currency j willbepaidin exchangefor one unit of currency i theproblemis to determine whether you have any arbitrage opportunities that is you want to determine whetherthereisany sequenceof exchangesthat would allowtheamount of moneyinsome particular currency to grow in an unbounded way you can assume that you start with one unit of each currency give a polynomial time algorithm for this problem you must justifythe correctness of your algorithm assume that you have a server that fails on a particular day of n days with probability percentindependentof whetheritfails on otherdays letxi be a random variabledenoting the maximum number of days that the server fails starting from day i so if xi k then the serverfails ondays i i i k but either the server doesn t fail on day i k or i k n compute the expected value of max xi xn we consider the quicksort algorithm for sorting n numbers let xi j be an indicator random variabledenoting whether the ith smallest numberis compared to the jth smallest number so xi j is if the ith smallest number is compared to the jth smallest number and otherwise what is e xi j as a function of i j and n use this to compute the expected number of comparisons usedbyquicksort consider the problem where the input is n keys kn with access probabilities pn the output should be a binary search tree t on these keys that minimizes the p expected access time in pidepth t ki give an polynomial time dynamic programming algorithmforthisproblem make sureto explainhowinformationis storedinyour array recall that the k cnf sat problem takes as input a boolean formula f in conjunctive normalform with exactlykliteralsper clause anddetermines whether or not f is satisf able assume that you know that cnf sat is np complete use this fact to establish that cnf sat is np hard we considerthe setting of atwoplayerzero sumgamedescribedby atwodimensiontable specifying how much the column player pays the row player for each possible situation assumetherowplayergoes frst and specifesaprobability distributionoverhispossible moves the columnplayerthen replieswith the movethat minimizesher expectedpayout tothe rowplayer we considertheproblem ofdetermining theprobabilitydistributionfor the row player that will maximize his expected payout explain how to construct a linear programming formulation of this problem it is sucient to construct the linear program for the following instance x y z a b c thengivetheduallinearprogramforyourlinearprogramforthe aboveinstance consider the red andbluejugproblem inthisproblemyouhave n redjugs and n blue jugs youknow thatfor eachjug that thereis a uniquejug of the other color with the same volume the only way thatyou canlearnaboutthe volumesof thejugsisto comparetwo jugs ofdi erent colors this operation tellsyou whetherthe redjughasgreater volume or whetherthebluejug hasgreater volume or whetherthey havethe same volume show using an adversarial argument that any deterministic algorithm that solves this problem requires n logn operations consider again the same red andbluejugproblem inthisproblemyouhave n redjugs and n bluejugs youknowthatfor eachjug thatthereis a uniquejug ofthe other color withthe same volume the only waythatyoucanlearn aboutthevolumesofthejugs isto comparetwojugs ofdi erentcolors this operationtellsyou whetherthe redjug hasgreater volume or whetherthebluejug hasgreater volume or whetherthey havethe same volume assume that you know that every linear time deterministic algorithm a is wrong on atleasthalf ofthe n possible inputs in the collection i of possible inputs explain why for every randomized algorithm a it must be the case that there exists an input i on which a is wrong with probability at least half your argument must be from frst principles weconsiderthestandard networkfowproblem letv be the number ofverticesinthe net work e be the number of edgesinthe network and f be the maximum edge capacityinthe network assume that you have network fow algorithms a b and c with worst case run ning times a v e f v f b v e f v f andc v e f v which of these algorithms are polynomial time algorithms start with a defnition of polynomial time algorithm andjustifyyour answer assumethatyouhaven items withweights wn thatyou wishtopackintok trucks so as to minimize the weight of the most heavilyloaded truck consider thegreedy algorithm that considers that items in an arbitrary order and packs that item onto the truck that currentlyhastheleastload provethatthegreedy algorithmhas worst case ratio at most twoforthisproblem state which twolowerboundsto optimal thatyou use consider the online list update problem the online algorithm maintains a list the input in each round is an item i in the list if the item i is in position k in the list then the algorithm pays a cost of k further at no cost the item i may be to any position in the list that is not further from the front of the list than i current position consider the algorithm mtf that always moves the accessed item closer to the front show that mtfhas cost at mosttwicethe optimal costfor everyinput sequence using thepotential function the number of transpositions between mtf list and the optimal list a pair i j of items is a transposition if i and j occur in di erent order in mtf and the optimal list cs syllabus spring times mwf pm sensq course home page http www cs pitt edu kirk index html course group home page http piazza com pitt home i will use this group for general announcements about the course this group should be used for questions of general interest the course group is the best place to ask general questions e g a question about a particular homework problem this group will be monitored by the instructor and the ta but often other students can provide a quicker answer than the ta or instructor instructor kirk pruhs o ce sennott square phone email kirk cs pitt edu please use the course group for general questions about assignments etc i do not monitor any other email address o ce hours mwf and mw ta nathan ong o ce sennott square email pitt edu o ce hours am noon tuesday and thursdays text the o cial text is introduction to algorithms edition by cormen leiserson rivest and stein i will only loosely follow the text having a hard copy of the text may be helpful but it probably isn t critical although i will assign some problems from the text so you will need to arrange to have some access to the text prerequisites cs and cs or more generally a solid knowledge of mathematical proofs and an undergraduate algorithms class that involves proofs most students that take the course without such prerequisite knowledge struggle course content in approximate order of importance learn how to think algorithmically learn how to be your own adversary learn a little about the research process learn how to think on many levels at the same time especially on the program and algorithm levels learn widely applicable algorithms design and analysis techniques and learn some of the more commonly used algorithms we will concentrate more on understanding the basics at a reasonably mature level rather than on more advanced material this is not a survey class of useful algorithms nor will we discuss many lower level implementation issues the class is targeted toward phd professional computer scientists the goal is to prepare all students to be able to read and understand reasonably sophisticated algorithms papers and to prepare some students to be able to design and analyze new algorithms on their own my goal is to get as many people to the a level as possible there will be homework assignments due almost every class it is expected that most of your learning will come from the process of solving the homework problems the main purpose of the lectures is to prepare you for the homework the nal exam will in large part be based on the homework grading grades will be based on homework classroom attendance participation and a nal exam the nal exam is of the grade homework will constitute of the nal grade attendance will be taken and attendance and participation in discussions and the class group will count for of the grade but students who have high attendance high class participation and who have made a good faith e ort to solve essentially all of the homework assignments will have the option to skip taking the nal exam and instead accept a grade of b students must receive prior permission of the instructor if they wish to choose this option i will subjectively set the grading scale at the end of the semester you are not in competition with other students i have no set numbers of a b etc i strongly suggest you cooperate with each other to understand the material this is in all students best interests if a student homework scores are conspicuously suspiciously higher than a student exam grades i reserve the right to base the course grade on only the exam scores and classroom participation students that attend the regularly and are doing the homework regularly should expect to get a grade of at least b to earn a higher grade students need to demonstrate mastery of the material on the nal exam homework policy you should do you homework in groups of or people other groups sizes including are possible with instructor approval each group need only provide one write up write ups must use latex http en wikipedia org wiki latex you may discuss problems with any student in the class with the provisos that you shouldn t feed others complete solutions and you must acknowledge collaborations in the write up you may not seek solutions on the www in other books former students from friends outside the class etc all homework is due at the start of class on the date due in hardcopy figures may be hand drawn no late homework is accepted the homework will be graded by the ta many students will nd some problems demanding it is not expected that all students will be able to answer all the homework questions disability policy if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union 412 tty as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course missing tests if you are going to miss a test for unavoidable reasons then before the exam or as soon as possible you must contact me if this is not possible contact the computer science departmental secretary at cheating policy i have no tolerance for cheating if you are caught cheating you will receive an f grade for the course problem part a from the clrs text you need not provide justi cations for your order if you aren t able to nd a group you can do this individually while i encourage you to use latex it is not strictly required for this assignment you may hand write these solutions due friday january problem for conjectures that are untrue explain which reasonable class of func tions the statement is true for for example you might say the statement is untrue in general and here is an example but the statement is true if the functions are strictly increasing and here is why due monday january consider the following de nitions for f n m o g n m state which de nitions are logically equivalent fully justify your answers argue about which de nition you think is the best one in the context where f and g are run times of algorithms and the input size is monotonically growing in n and m for example n might be the number of vertices in a graph and m might be the number of edges in a graph a there exists positive constants c such that for all n m where n and m it is the case that f n m cg n m b there exists positive constants c such that for all n m where n or m it is the case that f n m cg n m c there exists a constant c such that lim sup lim sup f n m g n m c n m note that this means that you rst take the limit superior with respect to m the result will be a function of just n you then take the limit superior of this function with respect to n if you don t know what limit superior means you can just assume that the limit exists in which case the limit and limit superior are the same d there exists a constant c such that lim sup lim sup f n m g n m c m n note that this means that you rst take the limit superior with respect to n the result will be a function of just m you then take the limit superior of this function with respect to m if you don t know what limit superior means you can just assume that the limit exists in which case the limit and limit superior are the same note that on the surface that this de nition is di erent than the last on in that the order that you take the limits is switched e there exists a constant c such that for all but nitely many pairs m n it is the case that f n m cg n m due monday january problem from the clrs text except parts d and f apply the master theorem theorem whenever applicable otherwise draw the recursive call tree and sum up the costs level by level due wednesday january problem parts d and f use induction you will need inductive proofs one for the upper bound and one for the lower bound due friday january problem parts d and e from the clrs text due friday january problem from the clrs text give an adversarial strategy and prove that it is correct due friday january problem from the clrs text give an adversarial strategy and prove that it is correct explain the last sentence of the problem statement from the book that is explain why its not rigorous correct to simply combine the lower bounds for the individual subsequences due friday january problem from the clrs text a for part a and b it is essentially asking you to consider the adversarial strategy that answers so as to maximize the number of the original ways of merging two sorted lists that are consistent with the answer you will likely nd stirling approximation for n useful b for parts c and d come up with a di erent adversarial strategy c explain why the bound that you get using the method proposed in parts a and b isn t as good as the bound you get using an adversarial strategy that is in what way are you being too generous to the algorithm in parts a and b in this generosity in the adversarial strategy or in its analysis due monday january consider the problem of determining whether a collection of real numbers xn is nice a collection of numbers is nice i the di erence between consecutive numbers in the sorted order is at most so is nice but is not nice since the di erence between and is more than we want to show that every comparison based algorithm to determine if a collection of n numbers is nice requires o n log n comparisons of the form xi xj c where c is some constant that the algorithm can specify so if c this is a standard comparison hint this is similar to the lower bound for element uniqueness another hint consider the n permutations p of n where p and the corresponding points in n dimensional space note that all n of these points are nice show the midpoint of any pair of these nice points is not nice then explain how to use this fact to give an adversarial strategy showing the o n log n lower bound due monday january consider a setting where you have two computer networking routers a and b each router has collected a list la and lb of ip source addresses for the packets that have passed through the router that day an ip address is n bits and thus there are possible ip addresses now the two routers want to communicate via a two way channel to whether there was some source that sent a packet through one of the routers but not the other so more precisely at the end of the protocol each router should commit to a bit specifying the answer to this question and the bits for both routers should be correct you can assume that each router can send a bit per unit time and that a bit sent on the channel is guaranteed to arrive on the other end in one time unit we want to consider protocols for accomplishing this goal a warm up consider the following protocol a sends to b the list of all of the ip source addresses that it has seen b compares a list to its list and then b sends a a bit if the lists are identical and a bit otherwise show that uses protocol about uses bits in the worst case b warm up give a protocol that uses o bits in the worst case another trivial warmup problem c warm up show that there is no protocol that can solve this problem without exchanging any bits hint its obvious that this is true that isn t the point the point is to understand what arguments you have to make to make this formally correct d show that there is no protocol that can solve this problem that involves a sending one bit to b and no more bits are exchanged hint again its obvious that this is true again that isn t the point again the point is to understand what arguments you have to make to make this formally correct ask yourself how should the adversarial strategy should decide whether this rst bit is a or a e show that there is no protocol that can solve this problem that involves a sending one bit to b and b replying with one bit to a and no more bits are exchanged hint again its obvious that this is true again that isn t the point again the point is to understand what arguments you have to make to make this formally correct f prove that every protocol for this problem must send o bits for its worst case instance of course your argument should involve an adversarial argument g assume that you have a computer networking router that sees a stream of k ip packets each with a source ip address consisting of n bits the router sees a packet optionally records some information in memory and then passes the packet on then sees the next packet optionaly records some information in mem ory and then passes that packet on etc the routers goal is to always know a ip source address that it has seen most frequently to date the most obvious way to accomplish this is to keep a count for each ip source address seen to date show that if k then every algorithm must use o k bits of memory hint this is an easy consequence of the previous subproblem provided that you think about it the right way assume that you had a method that solved this problem using o k bits of memory explain how to use this method to get an algorithm for the previous subproblem that uses less than bits of communica tion due wednesday january show that any comparison based algorithm for computing the median of n numbers you can assume for simplicity that n is odd requires o comparisons hint some variation of the lower bound to compute the largest and smallest number will work but there are a couple of additional complications due friday january problem from the clrs text for part b give an adversarial strategy for part c use linearity of expectations in your algorithm analysis due friday january consider the problem of nding the largest k numbers in sorted order from a list of n numbers see problem in the text consider the following algorithm you consider the numbers one by one maintaining an auxiliary data structure of the largest k numbers seen to date we get various algorithms depending on what the auxiliary data structure is and how one searches and updates it for each of the following variations give the worst case time complexity as a function of n and k for each of the following variations give the average case time complexity as a function of n and k under the assumption that each input permutation is equally likely hint use linearity of expectations these are all similar and easy if you look at them the right way a the auxiliary data structure is an ordered list and you use linear search starting from the end that contains the largest number b the auxiliary data structure is an ordered list and you use linear search starting from the end that contains the smallest number c the auxiliary data structure is a balanced binary search tree and you use standard log time search insert and delete operations d the auxiliary data structure is a balanced binary search tree and you use standard log time insert and delete operations but you start your search from the smallest item in the tree due friday january consider the following problem the input is n disjoint line segments contained in an l by l square s in the euclidean plane the goal is to partition s into convex polygons so that every polygon intersects at most one line segment so it is ok for a line segment to be in multiple polygons but each polygon can intersect at most one line segment consider the following algorithm that starts with the polygon s let p be a random permutation of the line segments while there is a polygon p that contains more than one line segment let be the rst line segment in the p order that intersects p then cut p into two polygons using the linear extension of so you extend the line segment into a line and then use that to cut p into two polygons show that the expected number of resulting polygons is o n log n hint use linearity of expectations first ask yourself how the number of polygons is related to the number of times that line segments get cut in the process consider to line segments u and v let cu v be a random variable that is if the linear extension of u cuts v let n u v denote the number of line segments w where the linear extension of u hits w before hitting v in other words if you starting walking from u on u linear extension towards v n u v is how many line segments you cross before hitting v if you don t hit v then n u v what is the relationship between the probability that cu v and n u v due monday january assume you have a source of random bits so in one time unit this source will produce one random bit that is with probability independent of other bits consider the problem of outputting a random permutation of the integers from to n so each of the n permutations should be produced with probability exactly n a give an algorithm to solve this problem and show that the expected time of the algorithm is o n log n this includes both the time that your algorithm takes plus unit of time for each random bit used b now assume that there is a limited source of at most random bits show that there is no algorithm that can solve the problem using expected time o hint show the result for n why can t you produce a random permutation of using bits then generalize to an arbitrary n further hint often students will say that there two subproblems are contradictory maybe start by understanding why they are not contradictory due monday january from the clrs text due wednesday february a let p be a problem the worst case time complexity of p is o the worst case time complexity of p is n log n let a be an algorithm that solves p which of the following statements are consistent with this information about the complexity of p i a has worst case time complexity o ii a has worst case time complexity o iii a has worst case time complexity o n log n iv a has worst case time complexity t v a has worst case time complexity o n log n vi a has worst case time complexity o n b let p be a problem the worst case time complexity of p is o the worst case time complexity of p is n let a be an algorithm that solves p which of the following statements are logically consistent with this information about the complexity of p i a has worst case time complexity o ii a has worst case time complexity t iii a has worst case time complexity o n log n iv a has worst case time complexity o n v a has worst case time complexity n log n vi a has worst case time complexity o n log n c assume that problem p has worst case time complexity o assume that problem p has worst case time complexity o assume that a is an algorithm for problem p for each of the following statements state whether the statement is logically implied by the above information and state whether the statement is logically consistent with the above information so i am looking for yes no answers i algorithm a has worst cast time complexity o ii algorithm a has worst cast time complexity o iii algorithm a has worst cast time complexity n iv algorithm a has worst cast time complexity o n due friday february from the clrs text due friday february a consider the problem where the input is a sorted array a containing n real num bers and a real number x and the output is an integer k such that if x is in a v then it must lie between positions k and k n assume that each element of a and x are independently and uniformly distributed in the interval show that the following algorithm solves this problem in o average case time last x n if a last x then next last sqrt n while a next x do last next next next sqrt n return k last else if a last x then next last sqrt n while a next x do last next next next sqrt n return k next hint find the bernoulli trials figure out how to think about the outcome of this algorithm in terms of the number of successes failures in some bernoulli trials use a cherno tail bound see appendix c you can use the result of exercise c without proof b explain how to use the above algorithm to obtain an algorithm with o log log n average case running time for the searching problem nding the location of the element in a whose value is closest to x again assume that each element of a and x are independently and uniformly distributed in the interval due monday february the purpose of this problem is to develop a version of yao technique for monte carlo randomized algorithms within the context of the red and blue jug problem from problem in the clrs text assume that if you sorted the jugs by volume that each permutation is equally likely a show that if a deterministic algorithm a always stops in o n log n steps then the probability that a is correct for large n is less than percent b show if there is a distribution of the input on which no deterministic algorithm with running time a n is correct with probability percent then there is no monte carlo algorithm with running time a n that can be correct with proba bility percent hint mimic the proof of yao technique lemma for the case of las vegas algo rithms consider a two dimensional table matrix t where entry t a i is if algorithm a is correct on input i and otherwise c conclude that any monte carlo algorithm for this jug problem must have time complexity o n log n due wednesday february consider the following online problem you given a sequence of bits bn over time each bit is in an envelope you rst see the envelope for then the envelope for when you get the ith envelope you can either look inside to see the bit or destroy the envelope in which case you will never know what the bit is you know a priori that at least n of the bits are you goal is to nd an envelope containing a bit you want to open as few envelopes as possible a give a deterministic algorithm that will open at most n o envelopes hint this is completely straight forward b give an adversarial strategy to show that every deterministic algorithm must open at least n o envelopes hint this is completely straight forward c assume that each of the n permutations of the inputs is equally likely show that there is a deterministic algorithm where the expected number of envelopes that is opens is o hint this is a straight forward consequence of some facts that we learned about bernoulli trials d give a monte carlo algorithm that opens o log n envelopes and has probability of error n show that the probability of error is this small hint this is a straight forward consequence of some facts that we learned about bernoulli trials e show using the version of yao technique for monte carlo algorithms that you developed in the last homework assignment to show that every monte carlo al gorithm must open o log n envelopes if it is to be incorrect with probability n hint this is a straight forward application of the yao technique for monte carlo algorithms that you developed in the previous homework problem f give a las vegas algorithm where the expected number of opened envelopes is o hint take some random guesses for the rst half of the envelopes and then if you don t nd a bit give up and do the most obvious thing see the discussion of the birthday paradox in section you may use facts from the analysis of the birthday paradox in the clrs text or from the wikipedia http en wikipedia org wiki without proof due friday february show that every las vegas algorithm for the previous envelope problem must open o envelopes in expectation hint use yao technique and the following probability distribution with probability v half n uniformly distributed random bits in envelopes n are set to the other v bits in the rst half of the envelopes are set to bits in the envelopes n n n v are all set to and the remaining bits are for k n with probability v n the bits in envelopes are distributed according to distribution dk in dk envelopes n contain a uniformly distributed random set of k and n k v then envelopes n n n contain a uniformly distributed set of v n k and k the remaining bits are this is one of the hardest homework problems of the semester feel free to consider this to be extra credit if you like due monday february problem part c use an exchange argument the obvious exchange works but the proof of optimality is a bit subtle you can nd some notes about what an exchange ar gument is here http people cs pitt edu kirk notes greedynotes pdf due wednesday february problem hint first gure out how to exhaustively generate all feasible solutions as the leaves of some tree then construct a pruning rule that you apply to each level of the tree to prune out some subtrees rooted at this level the natural pruning rule is similar to the one used by bellman ford due wednesday february the input to this problem is two sequences t tn and p pk such that k n and a positive integer cost ci associated with each ti the problem is to nd a subsequence of of t that matches p with maximum aggregate cost that is nd the pk sequence ik such that for all j j k we have tij pj and j cij is maximized so for example if n t xy xxy k p xy c3 and then the optimal solution is to pick the second x in t and the second y in t for a cost of a give a dynamic programming algorithm based on enumerating subsequences of t and using the pruning method b give a dynamic programming algorithm based on enumerating subsequences of p and using the pruning method due friday february there are three shortest path algorithms covered in chapter bellman ford dijkstra and the topological sort algorithm for directed acyclic graphs for each of the following problems pick the most appropriate of these three shortest path algorithms to apply to obtain an algorithm for the problem this may or may not involve modifying the algorithm slightly if you need to modify the algorithm explain how you may need to rst brie y explain why the problem is indeed just a shortest path problem in disguise that is state how one obtains the graph and why the shortest path in this graph corresponds to a solution to the problem give the running time of the resulting algorithm a the problem described in b the problem described in c the problem described in d the problem of nding the path where the minimum edge weight is maximized you need such an algorithm to implement one of the karp edmonds variations on ford fulkerson due monday february for each of the next algorithms state whether the algorithm is a polynomial time algorithm whether the algorithm is a pseudo polynomial time algorithm and whether the algorithm is a strongly polynomial time algorithm justify your answers a read xn y for i to n do for i to n do y y xj xi b read xn y for i to n do for i to xi do y y xj xi c read xn y for i to n do for i to log xi do y y xj xi d read xn y for i to n do for i to n do y y y hint this one is a bit tricky if the answer is not clear explain why due monday february show how each of problems described problems and from the clrs text can be e ciently reduced to network ow give the running time of the result ing algorithms for each problem assuming that you can solve network ow in time t v e f where t is some function of the number of vertices v in the network the number of edges e in the network and the max ow f in the network due wednesday february problem from the clrs text due wednesday february consider the problem of constructing a maximum cardinality bipartite matching the input is a bipartite graph where one bipartition are the girls and one bipartition is the boys there is an edge between a boy and a girl if they are willing to dance together the problem is to match the boys and girls for one dance so that as many couples are dancing as possible see section in the book if you want more details a construct an integer linear program for this problem so you want to explain how to compute maximum cardinality matchings by nding an optimal integer solution to a particular linear program b consider the relaxed linear program where the integrality requirements are dropped explain how to nd an integer optimal solution from any rational optimal solution hint find cycles of edges with associated variables that are not integer due friday february consider the minimum spanning tree problem de ned in chapter of the text a give an integer linear programming formulation using the following intuition and prove that your formulation is correct there is an indicator random variable for each edge you must choose at least n edges n is the number of vertices in the graph for each subset s of k vertices you can choose at most k edges connecting vertices in s explain why the size of this linear program can be exponential in the size of the graph b give an integer linear programming formulation using the following intuition and prove that your formulation is correct there is an indicator random variable for each edge you must choose at exactly n edges for each subset s of vertices s not the empty set and not all the vertices you must choose at least one edge with one endpoint in s and one endpoint not in s explain why the size of this linear program can be exponential in the size of the graph hint theorem in the text may be useful c give a polynomial sized integer linear programming formulation using the follow ing intuition and prove that your formulation is correct call an arbitrary vertex the root r think of a spanning tree as routing ow away from r to the rest of the tree but now you do not have ow conservation at the vertices explain why the size of this linear program is polynomially bounded in the size of the graph d consider a relaxation of the integer linear program in the last subproblem in that now the ows on the edges may be rational and not necessarily integer show how to express an optimal rational solution to the linear program as an a ne combination of optimal rooted spanning trees conclude that one can compute in polynomial time an optimal spanning given the optimal rational solution to this linear program hint the coe cient for the rst tree will be the least ow on any edge and then repeat this idea due friday february consider the following problem the input consists of a directed graph g with a speci ed source vertex and a positive integer pro t for each vertex of g the objective is to nd a subset h of the vertices of maximum total pro t subject to the constraint that there is a collection of vertex disjoint paths from to the vertices in h that is if h vk there there must be paths pk such that each path pi starts at and ends at vi and no pair of paths share a vertex other than a write an integer linear programming formulation for this problem hint as is usually the case the key is to gure out what the variables will be probably the most natural formulation has two types of variables b consider the relaxed linear program where the integrality requirements are dropped explain how to nd an integer optimal solution from any rational optimal solu tion hint morally this rounding is the same as for matching but the implementation is a bit more complicated c give a strongly polynomial time algorithm for this problem note that no one knows of a strongly polynomial time algorithm for linear programming you must prove your algorithm is both correct and runs in strongly polynomial time hint what is gordon gekko most famous quote you can google it if you don t know due monday february consider a two person game speci ed by an m by n payo matrix p the two players can can be thought of as a row player and a column player the number of possible moves for the row player is m and the number of possible moves for the column player is n each player picks one of its moves and then money is exchanged if the row player makes move r and the column player makes move c then the row player pays the column player pr c dollars note that pr c could be negative in which case really the column player is paying money to the row player we assume that the game is played sequentially so that one player speci es his move the other players sees that move and then speci es a response move we cam assume that this player makes the best possible response obviously each player wants to be payed as much money as possible and if this is not possible to pay as little as possible a give a simple e cient algorithm that will compute the best response for the column player give a speci c move by the row player b give a simple e cient algorithm that will compute the best rst move by the row player given that the column player will give its best response c either give an example of a payo matrix where it is strictly better for each player to go second or argue that there is no such payo matrix hint roshambo now we change the problem so that each player speci es a probability distribution over his moves and then the row player pays the column player e pr c where the expectation is taken over the two probability distributions in the natural way d give a simple e cient algorithm that will e ciently compute the best response which is probability distribution over column moves for the column player given a probability distribution speci ed by the row player e give a linear program that will compute the best rst move probability distribu tion over row moves for the row player given that the column player makes the best response f show the linear program you would get for the following payo matrix g give a linear program to compute the best rst move probability distribution over column moves for the column player given that the row player makes the best response h show the linear program you would get for the following payo matrix i either give an example of a payo matrix where it is strictly better for each player to go second or argue that there is no such payo matrix hint strong linear programming duality due wednesday march consider the problem of constructing maximum cardinality bipartite matching see section in the clr text a construct an integer linear program for this problem b construct the dual linear program c give a natural english interpretation of the dual problem e g similar to how we interpreted the dual of diet problem as the pill problem d explain how to give a simple proof that a graph doesn t have a matching of a particular size you should be able to come up with a method that would convince someone who knows nothing about linear programming due wednesday march consider the problem of scheduling a collection of processes on one processor each process ji has a work xi a release time ri and a deadline di all these values are positive integers a job can not be run before its release time or after its deadline the goal is to nd the slowest possible speed that will allow you to nish do all the work from each job if a job is processed at speed for t units of time this will complete t units of work from that job the processor can process at most one job at each moment of time but a processor can switch between processes arbitrarily for example the processor can run for a while then switch to then back to then to etc the times of these switches need not be integer a express this problem as a linear program b construct the dual program c give a natural english interpretation of the dual problem e g similar to how we interpreted the dual of the max ow problem as the min cut problem d explain how to give a simple proof that the input is infeasible for a particular speed you should be able to come up with a method that would convince someone who knows nothing about linear programming hint find the relationship between this problem and the previous problem that is gure out what this problem has to do with bipartite matching you are welcome to use facts you developed in solving the previous problem due friday march assume that you have a park mathematically a dimensional plane containing k lights and n statues in particular you know for each light land for each statue s whether light l will illuminate statue s if light l is lit this is a binary value there is no possibility of partial illumination further you are told for each light l the cost cl for turning l on the goal is to light all the statues while spending as little money as possible a construct an integer linear program for this problem where there are binary in dicator variables for each light signifying whether the light is lit or not b consider the relaxed linear program where the variables are allowed to be any rational between and give an english explanation of the problem that this models hint imagine the lights have a dimmer control c show that the relaxed linear program where the variables are allowed to be any rational between and can have a strictly smaller objective than the optimal objective for the integer linear program for some instances d construct the dual program for the relaxed linear program e give a natural english interpretation of the dual problem the problem modeled by the dual linear program f explain how to give a simple proof that a certain cost is required for the problem modeled by the relaxed linear program the one with dimmer controls using this natural interpretation of the dual due monday march prove that each of the problems de ned in and are np hard using a reduction using a reduction from an np complete problem of your choice that is de ned earlier in chapter so for each problem you need to give one polynomial time reduction the di culty of nding the reductions ranges from trivial to reasonably straight forward due wednesday march 39 show that the color problem is np hard by reduction from the cnf sat prob lem color is de ned in problem in the text which also contains copious hints due friday march we consider a generalization of the fox goose and bag of beans puzzle http en wikipedia org wiki fox the input is a graph g an integer k the vertices of g are objects that the farmer has to transport over the river there are an edge between two objects if they can not be left alone without the farmer supervision on the same size of the river the goal is to determine if a boat of size k is su cient to safely transport the objects across the river the size of the boat is the number of objects that the farmer can haul in the boat show that this problem is np hard using a reduction from one of the problems depicted in gure in the clrs text so i am letting you pick the problem to reduce from here you should take some time to re ect which problem would be easiest to reduce from due monday march problem parts a b and d from the clrs text due monday march from the clrs text due wednesday march problem from the clrs text due friday march problem from the clrs text due friday march prove that if there is a polynomial time approximation algorithm for the maximum clique problem that has approximation ratio then there is a polynomial time approximation algorithm with approximation ratio this is actually a slightly easier problem than problem part b in the clrs text which i suggest that you look at for inspiration note that in some sense this can be viewed as a gap reduction due monday march consider the following problem the input is a graph g v e feasible solutions are subsets s of the vertices v the objective is to maximize the number of edges with one endpoint in s and one endpoint in v s a give a simple polynomial time randomized algorithm for this problem and show that it is approximate hint flip a coin for vertex and consider analysis for from class b develop a deterministic polynomial time approximation algorithm for this prob lem using the method of conditional expectations which considers the vertices one by one but instead of ipping a coin for each vertex v puts v in the bipartition that would maximize the expected number of edges in the cut if coin ips were used for the remaining vertices c give a simple greedy algorithm that ends up implementing this policy d prove that this greedy algorithm has approximation ratio at most due monday march 47 consider the max sat problem see https en wikipedia org wiki a given an integer linear programming formulation of max sat hint there should be two types of variables in the linear program one linear programming variable xv for each variable v in the max sat instance and one linear programming variable yc for each clause c in the max sat instance b let xv be an optimal solution to the relaxed rational linear program show that setting the variable v to with probability xv and to otherwise independent of the setting of the other variables yields a e approximation hint use the fact that the geometric mean is at most the arithmetic mean https en wikipedia org wiki also use the fact that if f x is a concave function on an interval a b you can lower bound f x in this interval the line that passes through a f a and b f b due wednesday march problem from the clrs text you must use a potential function analysis to prove o amortized time hint the potential function for dynamic tables will be useful due friday march problem from the clrs text due friday march assume that you have a collection of n boxes arriving online over time that must be loaded onto m trucks when a box arrives the online algorithm learns the weight of the box and a list of trucks that that box can be loaded on so not every box is allowed to be loaded on every truck at the time that a box arrives the online algorithm must pick a truck to load the box on the objective is to minimize the weight of the most heavily loaded truck give an adversarial argument to show no deterministic online algorithm can achieve approximation ratio o hint in your adversarial strategy later arriving boxes should be made only assignable to trucks that the online algorithm assigned boxes to earlier due monday april consider the paging problem consider the following randomized online algorithm algorithm description each page p has an associated bit denoting whether the page is fresh or stale if requested page p in fast memory then p associated bit is set to fresh if the requested page p is not in fast memory then a stale page is selected uniformly at random from the stale pages in fast memory and ejected and p associated bit is set to fresh if the request page p is not in fast memory and all pages in fast memory are fresh then make all pages in fast memory stale select a stale page uniformly at random from the stale pages in fast memory to evict and p associated bit is set to fresh show that this algorithm is o log k competitive approximate using the following strat egy recall k is the size of the fast memory partition the input sequence into consec utive subsequences phases where there are exactly k distinct pages requested in each subsequence phase the phase breaks are when all pages in fast memory are made stale let mi be the number of pages requested in phase i that were not requested in phase i p a show that the optimal number of page faults is o i mi b show that the expected number of page faults for the randomized algorithm on the page requests in phase i is o mi log k due wednesday april consider an online or approximation problem where there are only nitely many pos sible algorithms and nitely many possible inputs we generalize yao technique to approximation ratios the correct answer is yes to three of the following four ques tions and the correct answer is no for the remaining question identify the three questions where the answer is yes and give a proof that the answer is yes hint first assume that there are two possible deterministic algorithms and two pos sible inputs the general proof is more or less the same but it may be easier to think about this special case a assume that the problem is a minimization problem i assume that you have an input distribution i such that for all deterministic algorithms a it is the case that e a i e opt i c can you logically conclude that the expected competitive ratio for every randomized algorithm is at least c ii assume that you have an input distribution i such that for all deterministic algorithms a it is the case that e a i opt i c can you logically conclude that the expected competitive ratio for every randomized algorithm is at least c b assume that the problem is a maximization problem i assume that you have an input distribution i such that for all deterministic algorithms a it is the case that e opt i e a i c can you logically conclude that the expected competitive ratio for every randomized algorithm is at least c ii assume that you have an input distribution i such that for all deterministic algorithms a it is the case that e opt i a i c can you logically conclude that the expected competitive ratio for every randomized algorithm is at least c c for extra credit prove that the correct answer is no for the remaining question there is a simple example but that does not mean it is easy to nd due friday april show that the expected competitive ratio for every randomized paging algorithms is o log k hint use your results from the previous problem assume that the number of pages is one more than the size of fast memory due monday april consider the following online problem there are two taxis on a line that initially start at the origin at positive integer time t a request point ht on the line arrives in response each taxi can move to a di erent location on the line or stay put at its current point the path traveled by at least one of the two taxis much cross ht the objective is to minimize the total movement of the taxis a as a warmup show that if there is a c competitive algorithm a for this problem then there is a c competitive algorithm b that only moves one taxi in response to each request and that one taxi moves directly from its position to the request b give an adversarial strategy to show that the competitive ratio of every deter ministic algorithm is at least hint come up with a request sequence that makes it hard to decide if one of the taxis should move c consider the following algorithm a if both taxis are to the left of ht then a rightmost taxi moves to ht if both taxis are to the right of ht then a leftmost taxi moves to ht if ht is between the two taxis then both taxis move toward ht at the same rate until one of the taxis reaches ht at which point both taxis stop moving show that this algorithm is competitive using the following potential function f twice the distance between the leftmost taxi for a and the leftmost taxi for optimal plus twice the distance between the rightmost taxi for a and the rightmost taxi for optimal plus the distance between the leftmost and the rightmost taxis for a so you need to show that for each request the cost to a the change in the potential f is at most times the cost to optimal due wednesday april defnitions an instance consists of n jobs where job i hasa release time ri implicitly eachjobhasawork a weight of we assume without loss of generality that rn an online scheduler is not aware of job i until time ri and at time ri learns yi for each time a schedule specifes a job to be run and aspeedat whichthejobis run ajob i completes once unit of work has been performed on i the speed y is the rate at which work is completed a job with work y run at a constant speed completes in seconds every nonnegative real number is an allowable speed the power consumed when running at speed is the energy usedispower integratedover time we assume that preemptionis allowed thatis ajobmaybe suspended and later restarted from the point of suspension ajob is active at time t if it has been released but not completed at timet let x be an arbitrary algorithm let wx t denote the aggregate fractional weight of the active jobs at time t for algorithm x so forexample ifajob i is completed at time t then its fractional weight at time t is let sx t be the speed at time rt for algorithm x and let px t sx t be the power consumed t at time t by algorithm x let ex t k dk be the energy spent up until time t by algorithm a r k px t let wx t k dk be the fractional weighted fow up until time t for algorithm x our k wxobjective function combines fow and energy and we let gx t wx t ex t be the fractional weighted fow and energy up until time t for algorithm x let ex ex wx wx gx gx be the energy fractional weighted fow and fractional weighted fow plus energy respectively for algorithm x weuseoptto denotetheoffineadversary and subscriptavariableby o to denote the value of a variable for the adversary so wo is the fractional weighted fow for the adversary amortized local competitiveness a common notion to measure an on line scheduling algorithm is local competitiveness meaning roughly that the algorithm is competitive at all times during the execution local competitiveness is generally not achievable in speed scaling problems because the adversary may spend essentially all of its energy in some small period of time making it impossible for anyonline algorithm to be locally competitive at that time thus we will analyze our algorithms using amortized local competitiveness which we now defne let x be an arbitrary online scheduling algorithm and h an arbitrary objective function let dh t be the rate dt of increase of the objective h at time t the online algorithm x is amortized locally competitive with potential function f t for objective function h if the following two conditions hold boundary condition f is initially and and fnally nonnegative that is f and there exists some time such that for all t it is the case that f t general condition for all timest dhx t dho t df t dt dtdt we break the general condition into three cases running condition for all timest when no job arrives holds job arrival condition f does not increase when a new job arrives completion condition f does not increase when either the online algorithm or the adversary complete a job observe that when f t is identically zero we have ordinary local competitiveness it is well known that amortized local competitiveness implies that when the algorithm completes the total cost of the online algorithm is at most times the total cost of the optimal offine algorithm if online algorithm x is amortized locally competitive with potential function f t for objec tive function h then hx ho proof let be the events that either a job is released the online algorithm x completes a job or the adversary completes a job let f ti denotethe changein potentialin responsetoevent ti let and integrating overtime wegetthat x hx f ti ho i bythe job arrival condition and the completion condition we can conclude thathx f f ho and fnally by the boundary condition we can conclude that hx ho now consider the case that the objective function is g the fractional weighted fow plus energy then dg t w t p t w t t and equivalent to dt df t wx t sx t wo t so t dt for our purposes we will always consider the algorithm a where sa t wa t thus is equivalent to df t t wo t so t dt if wo t so t then df t dt thus we are essentially required to pick a potential function satisfying equation note that if wo t so t then it must be the case that the adversary has no active jobs at time t and wo t so t if wo t t then canbe rewritten as df t t dt wo t so t since wewant to choose to be as small as possible while still satisfying inequality the right side of this inequality will denote our competitive ratio the result we frst show that the speed scaling algorithma where sa t wa t is competitive for the objective function of fractional fow time plus energy we frst recall the following classic inequality and its corollary young inequality let f be a real valued continuous and strictly increasing function on c with c if f and a b suchthat a c and b f c then zz b a f x dx f x dx ab where f is the inverse function of f for positive realsa b  p and q suchthat p q the following holds p q p bq  ab p q note that for  thisis the classich older inequality we now prove the main result of this section the speed scaling algorithm a where sa t wa t a is competitive with respect to the objective g of fractional fow plus energy proof we prove that algorithma is amortized locally competitive using the potential function f t max wa t wo t we frst need to verify the boundary condition clearly f as wa wo and f t is always non negative f satisfesthejob completion conditionsincethe fractionalweightofajob approaches zero continuously as the job nears completion and there is no discontinuity in wa t or wo t when a job completes f satisfes the job arrival condition since both wa t and wo t increase when a new job arrives we are left to establish the running condition we now break the argument into two cases in the frst case assume that that wa t wo t this case is simpler since the offine adversary has large fractional weight here f t and df t by the defnition of f since we know that wa t wo t it must be dt the case that wo t t andthenthattherightsideof we now turn to the interesting case that wa t wo t for notational ease we will drop the time t from the notation since all variables are understood to be functions of t we consider df dt d wa wo d wa wo wa wo dt dt dt since jobs have unit density the rate at which the fractional weight decreases is exactly the rate at which dw unfnished work decreases which is just the speed of the algorithm thus moreover since dt sa wa by the defnition of a written as df wa wo sa so wa wo w so a dt since wa wa wo it follows that wa wa wo and that df wa wo wa woso dt applyingyoung inequality cf with  a p b wa wo and q we obtain that wa woso wa wo thus written as o df wa wo wa wo wa wo oo dt if wo then implies that and holds if wo then odt o plugging the right sideof we obtaina bound on the competitive ratioof lecture fixed parameter algorithms spring lecture fixed parameter algorithms vertex cover fixed parameter tractability kernelization connection to approximation fixed parameter algorithms fixed parameter algorithms are an alternative way to deal with np hard problems instead of approximation algorithms there are three general desired features of an algorithm solve np hard problems run in polynomial time fast get exact solutions in general unless p np an algorithm can have two of these three features but not all three an algorithm that has features and is an algorithm in p poly time exact an approximation algorithm has features and it solves hard problems and it runs fast but it does not give exact solutions fixed parameter algorithms will have features and they will solve hard problems and give exact solutions but they will not run very fast idea the idea is to aim for an exact algorithm but isolate exponential terms to a speci c parameter when the value of this parameter is small the algorithm gets fast instances hopefully this parameter will be small in practice parameter a parameter is a nonnegative integer k x where x is the problem input typically the parameter is a natural property of the problem some k in input it may not necessarily be e ciently computable e g opt parameterized problem a parameterized problem is simply the problem plus the parameter or the problem as seen with respect to the parameter there are potentially many interesting parameterizations for any given problem lecture fixed parameter algorithms spring goal the goal of xed parameter algorithms is to have an algorithm that is poly nomial in the problem size n but possibly exponential in the parameter k and still get an exact solution k vertex cover given a graph g v e and a nonnegative integer k is there a set s v of vertices of size at most k s k that covers all edges this is a decision problem for vertex cover and is also np hard we will use k as the parameter to develop a xed parameter algorithm for k vertexcover notethatwecan have k v as the gure below shows brute force solution bad vvv v tryall sets of k vertices can skip all terms smaller than kk because bigger sets have more coverage testing coverage takes o m time where m is the number of edges therefore the total runtime is o v k e it is polynomial for xed k but not the same polynomial for all k it is ine cient in most cases hence we de ne nf k to be bad where n v e is the input size bounded search tree algorithm good this is a general technique used to improve brute force searches it works as follows pick arbitrary edge e u v we know that either u s or v s or both but don t know which guess which one try both possibilities add u to s delete u and incident edges from g and recurse with k k do the same but with v instead of u return the or of the two outcomes lecture fixed parameter algorithms spring this is like guessing in dynamic programming but memoization doesn t help here the recursion tree looks like the following at a leaf k return yes if e all edges covered it takes o v time to delete u or v therefore this has a total runtime of v o v for xed k degree of polynomial is independent of k also polynomial for k o lg v practical for e g k hence we de ne f k no to be good fixed parameter tractability a parameterized problem is xed parameter tractable fpt if there is an algorithm with running time f k no such that f n n non negative and k is the parameter and the o degree of the polynomial is independent of k and n o question why f k no and not f k n t cc theorem f k nalgorithm f k n proof t c trivial assuming f k and nare c f k c if n f k then f k n lecture fixed parameter algorithms spring cc if f k n then f k n n cc f k c nc c therefore f k n max f k c n f k nt d alternatively since xy can just make f k f k and c example o n o kernelization kernelization is a simplifying self reduction it is a polynomial time algorithm that converts an input x k intoa small and equivalent input x k here small means x f k and equivalent means the answer to x is thesameasthe answer to x theorem a problem is fpt a kernelization proof kernelize n f k run any nite g n algorithm totals to no g f k time let a be an f k nc algorithm then assuming k is known if n f k it already kernelized if f k n then cc run a f k n ntime output o sized yes no instance as appropriate to kernelize if k is unknown run a for nc time and if it is still not done we know it is already kernelized so we know exponential kernel exists recent work aims to nd polynomial even linear kernels when possible polynomial kernel for k vertex cover to create a kernel for k vertex cover the algorithm follows the following steps make graph simple by removing all self loops and multi edges any vertex of degree k must be in the cover else would need to add k vertices to cover incident edges lecture fixed parameter algorithms spring remove such vertices and incident edges one at a time decreasing k accord ingly remaining graph has maximum degree k each remaining vertex covers k edges if the number of remaining edges is k answer no and output canonical no instance else e remove all isolated vertices degree vertices now v the input has been reduced to instance v e ofsize o the runtime of the kernelization algorithm is naively o ve o v e withmore work after this we can apply either a brute force algorithm on the kernel which yields an overall runtime o v e o v e orwecan apply a bounded search tree solution which yields a runtime of o v e the best algorithm to date o kv by chen kanj xia tcs connection to approximation algorithms take an optimization problem integral opt and consider its associated decision problem opt k and parameterize by k theorem optimization problem has eptas eptas e cient ptas f no e g approxp artition decision problem is fpt proof like fptas pseudopolynomial algorithm say maximization problem and k decision run eptas with t k in f no time relative error k k absolute error k lecture fixed parameter algorithms spring so if we nd a solution with value k thenopt k k k integral opt k yes else opt k d also decision problems are equivalent with respect to fpt can use this relation to prove that eptass don t exists in some cases tentative course outline topic introduction to algorithm analysis azdnd asymptotic notation reading chapters and of clrs read the appendix covering mathematical background if you need to principles of algorithm analysis asymptotic notation comparing growth rate of functions growth rate as a measure of efficiency of use of a faster processor topic basic analysis of algorithms reading chapters and example polynomial multiplication naive divide and conquer strassen algorithm fft algorithm loops and sums sum of polynomially upper bounded increasing function sum of exponentially lower bounded function relation of sum and integral recursion and recurrences drawing the recursion tree and summing level by level master theorem induction including strengthening the induction hypothesis topic problem complexity information theoretic and adversarial lower bounds reading chapter time complexity of a problem example searching lower bound example sorting lower bound example element uniqueness lower bound example lower bound for find the maximum example lower bound for computing smallest and largest number not in the book topic averagecase analysis randomized algorithms and bounding tails of distributions reading chapters and and handout on randomized algorithms linearity of expectations example hiring problem example quicksort expected linear time selection randomly built search trees example expected time for insert in closed addressed hash table example expected time for insert in open addressed hash table using uniform hashing example universal hash functions example randomized montecarlo mincut algorithm las vegas vs monte carlo algorithms course outline http people cs pitt edu kirk sched html bounding tails and union bounds example analysis of streaks in bernoulli trials example high confidence analysis of quicksort expected linear time selection randomly built search trees binomially distributed random variable and chernoff bounds yao technique example average case information theoretic lower bound for sorting example randomized las vegas lower bound for sorting topic single source shortest path algorithms reading chapter example dijkstra greedy and exchange argument proof of correctness example bellmanford dynamic programming example linear time algorithm for directed acyclic graphs topic network flow reading chapter example fordfulkerson algorithm example karpedmonds algorithms polytime pseudo polytime and strong polytime example pushrelabel algorithm skipping on first pass topic linear programming reading chapter example writing network flow as a linear program example writing shortest path as a linear program simplex greedy algorithm vs divide and conquer interior point methods weak duality complementary slackness and statement of strong duality example writing the diet problem as a linear program and computing its dual example dual of network flow example dual of the shortest path problem topic npcompleteness read chapter example nphardness of vertex cover example dynamic program for subset sum and nphardness of subset sum topic approximation algorithms reading chapter example and approximations for traveling salesman example vertex cover using maximal matching lower bound example maximum weight vertex cover using linear programming lower bound example greedy weighted set cover using dual feasible solution as a lower bound course outline http people cs pitt edu kirk sched html example randomized approximation for derandomization using method of conditional expectations example fptas for knapsack skipping on first pass example gap reduction for tsp without triangle inequality to prove nonapproximatability topic fixed parameter tractability skipping on first pass reading mit opencourseware notes local copy example kernalization for vertex cover topic amortization and potential functions reading chapter example multipop example counting in binary example dynamic table topic online algorithms reading paper example paging general lower bound and resource augmentation analysis of lru example move to front for list update and general lower bound example speed scaling for the objective of fractional flow plus energy notes survey paper chernoff bound from wikipedia the free encyclopedia in probability theory the chernoff bound named after herman chernoff but due to herman rubin gives exponentially decreasing bounds on tail distributions of sums of independent random variables it is a sharper bound than the known first or second moment based tail bounds such as markov inequality or chebyshev inequality which only yield powerlaw bounds on tail decay however the chernoff bound requires that the variates be independent a condition that neither the markov nor the chebyshev inequalities require it is related to the historically prior bernstein inequalities and to hoeffding inequality contents the generic bound example additive form absolute error multiplicative form relative error special cases applications matrix bound theorem without the dependency on the dimensions sampling variant proofs chernoffhoeffding theorem additive form multiplicative form see also references additional reading the generic bound the generic chernoff bound for a random variable x is attained by applying markov inequality to etx for every when x is the sum of n random variables xn we get for any t in particular optimizing over t and using the assumption that xi are independent we obtain chernoff bound wikipedia https en wikipedia org wiki similarly and so specific chernoff bounds are attained by calculating for specific instances of the basic variables example let xn be independent bernoulli random variables whose sum is x each having probability p of being equal to for a bernoulli variable so for any taking and gives and and the generic chernoff bound gives the probability of simultaneous occurrence of more than n of the events xk has an exact value a lower bound on this probability can be calculated based on chernoff inequality indeed noticing that  np we get by the multiplicative form of chernoff bound see below or corollary in sinclair class notes chernoff bound wikipedia https en wikipedia org wiki this result admits various generalizations as outlined below one can encounter many flavours of chernoff bounds the original additive form which gives a bound on the absolute error or the more practical multiplicative form which bounds the error relative to the mean additive form absolute error the following theorem is due to wassily hoeffding and hence is called the chernoffhoeffding theorem chernoffhoeffding theorem suppose xn are i i d random variables taking values in let p e xi and  then where is the kullback leibler divergence between bernoulli distributed random variables with parameters x and y respectively if p then a simpler bound follows by relaxing the theorem using d p  p which follows from the convexity of d p  p and the fact that this result is a special case of hoeffding inequality sometimes the bound which is stronger for p is also used multiplicative form relative error multiplicative chernoff bound suppose x x are independent random variables taking values in chernoff bound wikipedia https en wikipedia org wiki multiplicative chernoff bound suppose xn are independent random variables taking values in let x denote their sum and let  e x denote the sum expected value then for any  a similar proof strategy can be used to show that the above formula is often unwieldy in practice so the following looser but more convenient bounds are often used special cases we can obtain stronger bounds using simpler proof techniques for some special cases of symmetric random variables suppose xn are independent random variables and let x denote their sum if then and therefore also if then applications chernoff bounds have very useful applications in set balancing and packet routing in sparse networks 24 chernoff bound wikipedia https en wikipedia org wiki the set balancing problem arises while designing statistical experiments typically while designing a statistical experiment given the features of each participant in the experiment we need to know how to divide the participants into disjoint groups such that each feature is roughly as balanced as possible between the two groups refer to this book section https books google com books id printsec frontcover source gbs cad for more info on the problem chernoff bounds are also used to obtain tight bounds for permutation routing problems which reduce network congestion while routing packets in sparse networks refer to this book section https books google com books i d printsec frontcover source cad for a thorough treatment of the problem chernoff bounds can be effectively used to evaluate the robustness level of an application algorithm by exploring its perturbation space with randomization the use of the chernoff bound permits to abandon the strong and mostly unrealisticsmall perturbation hypothesis the perturbation magnitude is small the robustness level can be in turn used either to validate or reject a specific algorithmic choice a hardware implementation or the appropriateness of a solution whose structural parameters are affected by uncertainties matrix bound rudolf ahlswede and andreas winter introduced a chernoff bound for matrixvalued random variables if m is distributed according to some distribution over d d matrices with zero mean and if mt are independent copies of m then for any  where holds almost surely and c is an absolute constant notice that the number of samples in the inequality depends logarithmically on d in general unfortunately such a dependency is inevitable take for example a diagonal random sign matrix of dimension d the operator norm of the sum of t independent samples is precisely the maximum deviation among d independent random walks of length t in order to achieve a fixed bound on the maximum deviation with constant probability it is easy to see that t should grow logarithmically with d in this scenario the following theorem can be obtained by assuming m has low rank in order to avoid the dependency on the dimensions theorem without the dependency on the dimensions let  and m be a random symmetric real matrix with and almost surely assume that each element on the support of m has at most rank r set if holds almost surely then 24 chernoff bound wikipedia https en wikipedia org wiki where mt are i i d copies of m sampling variant the following variant of chernoff bound can be used to bound the probability that a majority in a population will become a minority in a sample or vice versa suppose there is a general population a and a subpopulation b a mark the relative size of the subpopulation b a by r suppose we pick an integer k and a random sample s a of size k mark the relative size of the subpopulation in the sample b s s by rs then for every fraction d in particular if b is a majority in a i e r we can bound the probability that b will remain majority in s rs by taking d r this bound is of course not tight at all for example when r we get a trivial bound prob proofs chernoffhoeffding theorem additive form let q p  taking a nq in we obtain now knowing that pr xi p pr xi p we have therefore we can easily compute the infimum using calculus 24 chernoff bound wikipedia https en wikipedia org wiki setting the equation to zero and solving we have so that thus as q p  p we see that t so our bound is satisfied on t having solved for t we can plug back into the equations above to find that we now have our desired result that to complete the proof for the symmetric case we simply define the random variable yi xi apply the same proof and plug it into our bound multiplicative form set pr xi pi according to 24 chernoff bound wikipedia https en wikipedia org wiki the third line above follows because takes the value et with probability pi and the value with probability pi this is identical to the calculation above in the proof of the theorem for additive form absolute error rewriting as and recalling that with strict inequality if x we set the same result can be obtained by directly replacing a in the equation for the chernoff bound with   thus if we simply set t log  so that t for  we can substitute and find this proves the result desired final exam computer science introduction to algorithms spring instructions the test is closed book closed notes for most of the problems i am interested in testing whether you understand the techniques and concepts more than i am interested in the solution to the particular problem for example if i ask you to prove that a problem is np hard i am more interested in learning if you know how to prove that a problem is np hard than i am in the speci cs of the problem if i ask you to prove that a greedy algorithm is correct using an exchange argument i am more interested in learning if you know how an exchange argument works than i am in the speci cs of the problem i ask these questions in the context of speci c problems to allow you to demonstrate your understanding in a concrete setting of course you have to take into account the speci cs of the problem but make sure to explain the general method technique concept that you are using as well partial credit is given for the answer i don t know a blank answer will be inter preted as i don t know an answer that displays a major conceptual error will likely receive a grade of zero it is perfectly ne to give an incomplete answer e g here how one proves a problem np hard but i don t know how to prove this problem np hard i will make a judgement call on how much credit such answers should receive but generally it will be around i will assume that if you write something that you are asserting that you have good con dence in the correctness of what you write it is a bad strategy to give an answer that you do not have good con dence in answer up to of the problems clearly write the number of the problem that you are answering if you answer more than an arbitrary answers will be graded if you are uncertain about anything ask a question consider a setting where you have two computer networking routers a and b each router has collected a list la and lb of ip source addresses for the packets that have passed through the router that day an ip address is n bits and thus there are possible ip addresses now the two routers want to communicate via a two way channel to whether there was some source that sent a packet through one of the routers but not the other so more precisely at the end of the protocol each router should commit to a bit specifying the answer to this question and the bits for both routers should be correct you can assume that a bit sent on the channel is guaranteed to arrive on the other end in one time unit we want to consider protocols for accomplishing this goal prove that every protocol for this problem must sent bits for its worst case instance suppose that we use an open addressed hash table of size m to store n m items assuming uniform hashing show that the expected length of the longest probe sequence among the n inserts is o log n a palindrome is a nonempty string over some alphabet that reads the same forward and backward examples of palindromes are all strings of length civic racecar and aibohpho bia fear of palindromes give a polynomial time algorithm to nd the longest palindrome that is a subsequence of a given input string consider a two person game speci ed by an m by n payo matrix p the two players can can be thought of as a row player and a column player the number of possible moves for the row player is m and the number of possible moves for the column player is n each player picks a probability distribution over its moves and then money is exchanged the row player pays the column player e pr c dollars where the expectation is over both the probability distribution for the row player and the probability distribution for the column player we assume that the game is played sequentially so that one player speci es his probability distribution the other players sees it and then speci es a response obviously each player wants to be payed as much money as possible and if this is not possible to pay as little as possible a give a polynomial time algorithm to determine the optimal probability distribution for the row player assuming that the row player goes rst and that the column player responds with his optimal probability distribution b explain how one could simply prove to the row player that there is no probability distribution that he could pick that would result in him not having to pay money you are given n boxes with weights wn that one wants to partition among m trucks the objective is to minimize the weight on the most heavily loaded truck give a polynomial time approximation algorithm for this problem you must prove that the algorithm is a approximation algorithm show that the expected competitive approximation ratio for every randomized paging al gorithms is o logk where k is the size of the fast memory using a variation of yao technique show that the subset sum problem is np hard via a reduction from assume that you want to maintain a dynamic table under the operations insert and delete assume both there operations have cost if the table becomes full then the size of the table is doubled assume the cost of this operation is the size of the new larger table if the table becomes full then the size of the table is halved assume that the cost of this operation is the size of the old larger table show that the total cost of n operations which may be an arbitrary combination of insert and delete is o n using a potential function consider the standard cachine paging problem for each request the cache management algorithm checks whether that element is already in the cache if it is then we have a cache hit otherwise we have a cache miss upon a cache miss the system retrieves the element from the main memory and the cache management algorithm must decide whether to keep the element in the cache if it decides to keep element in cache and the cache already holds k elements then it must evict one element to make room the cache management algorithm evicts data with the goal of minimizing the number of cache misses over the entire sequence of requests give a polynomial time o ine greedy algorithm for this problem we consider the standard network ow problem a state the ford fulkerson algorithm b prove the ford fulkerson algorithm is correct c is ford fulkerson a polynomial time algorithm fully justify your answer d is ford fulkerson a pseudo polynomial time algorithm fully justify your answer final computer science introduction to algorithms spring instructions the test is closed book closed notes for most of the problems i am interested in testing whether you understand the techniques and concepts more than i am interested in the solution to the particular problem for example if i ask you to prove that a problem is np hard i am more interested in learning if you know how to prove that a problem is np hard than i am in the speci cs of the problem if i ask you to prove that a greedy algorithm is correct using an exchange argument i am more interested in learning if you know how an exchange argument works than i am in the speci cs of the problem i ask these questions in the context of speci c problems to allow you to demonstrate your understanding in a concrete setting of course you have to take into account the speci cs of the problem but make sure to explain the general method technique concept that you are using as well partial credit is given for the answer i don t know a blank answer will be inter preted as i don t know an answer that displays a major conceptual error will likely receive a grade of zero it is perfectly ne to give an incomplete answer e g here how one proves a problem np hard but i don t know how to prove this problem np hard i will make a judgement call on how much credit such answers should receive but generally it will be around i will assume that if you write something that you are asserting that you have good con dence in the correctness of what you write it is a bad strategy to give an answer that you do not have good con dence in answer up to of the problems clearly write the number of the problem that you are answering if you answer more than an arbitrary answers will be graded if you are uncertain about anything ask a question consider the problem of nding a minimum weight spanning tree in a graph a give a polynomial sized integer linear programming formulation f b consider the relaxation r of f where the variables are allowed to be arbitrary rational numbers not necessarily integers explain how to round a rational optimal solution to r to obtain an integer optimal solution to f state yao theorem within the context of monte carlo algorithms use yao theorem to prove that no comparison based monte carlo algorithm can sort n numbers using comparisons prove that the vertex cover problem deciding whether a given graph has a vertex cover of a given size is np hard using the fact that determining whether a boolean formula in conjunctive normal form with exactly literals per clause is satis able is known to be np hard assume that you have a collection of n boxes arriving online over time that must be loaded onto m trucks when a box arrives the online algorithm learns the weight of the box and a list of trucks that that box can be loaded on so not every box is allowed to be loaded on every truck at the time that a box arrives the online algorithm must pick a truck to load the box on the objective is to minimize the weight of the most heavily loaded truck give an adversarial argument to show no deterministic online algorithm can achieve approximation ratio o consider the following online problem there are two taxis on a line that initially start at the origin at positive integer time t a request point ht on the line arrives in response each taxi can move to a di erent location on the line or stay put at the current point the path traveled by at least one of the two taxis must cross ht the objective is to minimize the total movement of the taxis consider the following algorithm a if both taxis are to the left of ht then the rightmost taxi moves to ht if both taxis are to the right of ht then the leftmost taxi moves to ht if ht is between the two taxis then both taxis move toward ht at the same rate until one of the taxis reaches ht at which point both taxis stop moving show that this algorithm is competitive using the following potential function f the distance between the leftmost taxi for a and the leftmost taxi for optimal the distance between the rightmost taxi for a and the rightmost taxi for optimal the distance between the leftmost and the rightmost taxis for a consider the following problem the input is a collection c xn yn of points in two dimensions the output is the minimum perimeter vertically simple polygon with for which all the points in c are on the perimeter a polygon is vertically simple if no vertical line intersects the polygon more than twice give a polynomial time algorithm for this problem consider the following problem the input is n disjoint line segments contained in an l by l square s in the euclidean plane the goal is to partition s into convex polygons so that the interior of every polygon intersects at most one line segment so it is ok for a line segment to be in multiple polygons but each polygon can intersect at most one line segment consider the following algorithm that maintains a partition c of s into polygons initially this collection is just the single polygon s that is c s let p be a random permutation of the the line segments while there is a polygon p c that contains more than one line segment let l be the rst line segment in the p order that intersects p and cut p into two polygons using the linear extension of l so you extend the line segment l into a line and then use that to cut p show that the expected number of resulting polygons is o n logn state the bellman ford algorithm and explain explain how it can be used to determine whether one can make a pro t in currency arbitrage in the currency arbitrage problem you given n currencies cn and for each ordered pair ci cj you are given an exchange rate ei j which is the amount of currency cj you can obtain from one unit of currency ci the problem is to determine whether there is a currency ci and a sequence of exchanges where you can end up with more of currency ci than you started with consider the parallel pre x problem the input is n integers xn the output is n numbers representing the sums of all n nonempty pre xes so if the input was the output would be 19 give a parallel algorithm for this problem that would run on a pram where neither concurrent reading or concurrent writing is allowed that would run in time o log n if the number p of processors was n consider the following variant of the traveling salesman problem tsp the input is n points in the plane or some other nite metric space the output should be the shortest route to visit all points and return to the starting point give a polynomial time algorithm that has an approximation ratio of at most prove that the approximation ratio of your algorithm is at most final computer science introduction to algorithms spring instructions the test is closed book closed notes for most of the problems i am interested in testing whether you understand the techniques and concepts more than i am interested in the solution to the particular problem for example if i ask you to prove that a problem is np hard i am more interested in learning if you know how to prove that a problem is np hard than i am in the speci cs of the problem if i ask you problem that a greedy algorithm is correct using an exchange argument i am more interested in learning if you know how an exchange argument works than i am in the speci cs of the problem i ask these questions in the context of speci c problems to allow you to demonstrate your understanding in a concrete setting of course you have to take into account the speci cs of the problem but make sure to explain the general method technique concept that you are using as well 25 partial credit is given for the answer i don t know a blank answer will be inter preted as i don t know an answer that displays a major conceptual error will likely receive a grade of zero it is perfectly ne to give an incomplete answer e g here how one proves a problem np hard but i don t know how to prove this problem np hard i will make a judgement call on how much credit such answers should receive but generally it will be around i will assume that if you write something that you are asserting that you have good con dence in the correctness of what you write it is a bad strategy to give an answer that you do not have reasonable con dence in answer up to of the problems clearly write the number of the problem that you are answering if you answer more than an arbitrary answers will be graded if you are uncertain about anything ask a question consider the following possible de nitions for f n m o g n m state which de ni tions are logically equivalent fully justify your answers a there exists positive constants c such that f n m c g n m for all n m such that n and m b there exists positive constants c such that f n m c g n m for all n m such that n or m m0 c there exists a constant c such that lim supn lim supm f n m g n m c d there exists a constant c such that lim supm lim supn f n m g n m c e there exists a constant c such that for all but nitely many pairs m n it is the case that f n m c g n m consider the problem whose input is n jobs where job i has an integer release time ri and an integer volume xi and whose output is a feasible schedule that minimizes the total ow time of the jobs a schedule a function from some subset of the unit intervals which start and end at an integer to a job that is run during that interval a schedule is feasible if no job is run before its release time and each job i is run for xi units the competition time ci of a job i is the last time that it was run the ow time of job i is ci ri the total p ow time is i ci ri prove using an exchange argument that always running the job with the least remaining unprocessed unrun volume is a correct optimal algorithm give an o time dynamic programming algorithm to nd the longest increasing subse quence in a sequence xn of integers let xn be a sequence of bernoulli trials let yi be the maximum k such that xi xi xi k are all failures show e maxi yi o log n consider the problem of nding the maximum cardinality matching in a bipartite graph give a polynomial sized integer linear program for this problem explain how to obtain in polynomial time a minimum matching from the potentially fractional solution produced by a black box linear programming solver give the dual of the linear program explain what natural problem the dual is a linear program for explain how one can simply prove that there is no matching of size k someone who doesn t know linear programming duality should be able to understand your argument prove using yao technique that every randomized online algorithm for the paging problem has approximation ratio o log k here k is the number of pages in fast memory recall the objective was to minimize the number of accesses to slow memory consider the problem where the input consists of n boxes with weights wn and an integer k the problem is to partition the boxes into k partitions such that the maximum aggregate weight of any partition is minimized give a polynomial time algorithm for this problem and prove that it is a approximation algorithm prove that the problem of solving integer linear programs is np hard using the fact that de ciding whether a boolean formula in conjunctive normal form is satis able is np complete prove using an adversarial argument that any comparison based algorithm must use o comparisons to nd the largest and the smallest number in a list of n numbers state the ford fulkerson algorithm for the network ow problem and prove that it is correct if capacities are integers prove using a potential function the amortized cost of inserting into a dynamic table which doubles in size when it is two thirds full is o assume that you had to solve the hiring problem at a large academic institution where e ectively you couldn t re anyone note that this is a realistic assumption thus once you hire someone the game is over find a strategy that will hire the best person with probability approximately e assuming that each permutation is equally likely the person hiring knows n the number of applicants a priori final computer science introduction to algorithms fall instructions the test is closed book closed notes for most of the problems i am interested in testing whether you understand the techniques and concepts more than i am interested in the solution to the particular problem for example if i ask you to prove that a problem is np hard i am more interested in learning if you know how to prove that a problem is np hard than i am in the speci cs of the problem if i ask you problem that a greedy algorithm is correct using an exchange argument i am more interested in learning if you know how an exchange argument works than i am in the speci cs of the problem i ask these questions in the context of speci c problems to allow you to demonstrate your understanding in a concrete setting of course you have to take into account the speci cs of the problem but make sure to explain the general method technique concept that you are using as well 25 partial credit is given for the answer i don t know a blank answer will be inter preted as i don t know an answer that displays a major conceptual error will likely receive a grade of zero it is perfectly ne to give an incomplete answer e g here how one proves a problem np hard but i don t know how to prove this problem np hard i will make a judgement call on how much credit such answers should receive but generally it will be around i will assume that if you write something that you are asserting that you have good con dence in the correctness of what you write it is a bad strategy to give an answer that you do not have reasonable con dence in answer up to of the problems clearly write the number of the problem that you are answering if you answer more than an arbitrary answers will be graded if you are uncertain about anything ask a question prove the correctness of dijkstra single source shortest path algorithm using an exchange argument start with a brief description of dijkstra algorithm then explain how an exchange argument works a palindrome is a nonempty string that reads the same forward and backwards give an o time dynamic programming algorithm to nd the longest palindrome that is a subsequence of a given input stream give a polynomial sized linear program for the problem of nding the minimum spanning tree of an undirected edge weighted graph explain how to obtain in polynomial time a minimum spanning tree from the potentially fractional solution produced by a black box linear programming solver consider comparison based algorithms for some o ine problem state yao theorem in this context for lower bounding the time required by las vegas algorithms include a de nition of las vegas algorithms explain why yao theorem in this context is a consequence of weak duality of linear programs include a de nition of weak duality consider the following online problem you given a sequence of bits bn that arrive over time each bit is in an envelope you rst see the envelope for then the envelope for etc when you get the envelope i you can either look inside to see the bit or destroy the envelope in which case you will never know what the bit is you know a priori that at least n of the bits are the goal is to nd an envelope containing a bit you want to open as few envelopes as possible show using yao technique for monte carlo algorithms that every monte carlo algorithm must open o log n envelopes if it is to be incorrect with probability n start by stating precisely the version of yao technique theorem that you will use you do not need to prove yao theorem just state it correctly and apply it correctly assume that you have a collection of n boxes bn arriving online over time that must be loaded onto m trucks when a box arrives the online algorithm learns the weight of the box and a list of trucks that that box can be loaded on so not every box is allowed to be loaded on every truck at the time that a box arrives the online algorithm must pick a truck to load the box on the objective is to minimize the weight of the most heavily loaded truck give an adversarial argument to show no deterministic online algorithm can achieve approximation ratio o the input for the bottleneck traveling salesman problem is a complete undirected graph with positive edge weights satisfying the triangle inequality feasible solutions are tours that visit each vertex exactly once the objective is to minimize the length of the longest edge in the tour give a polynomial time approximation algorithm for this problem you must prove your algorithm produces a approximation start with a de nition of what it means for an algorithm to be a approximation algorithm in this setting consider the online paging caching problem the input is a sequence of pages if a page is not currently in a cache a cache miss occurs the objective is to minimize the number of cache misses show that ejecting the least recently used page from cache of size k results in at most twice as many cache misses as the minimum possible number of misses for a size k cache prove that if there is a polynomial time approximation algorithm for the maximum clique problem that has approximation ratio then there is a polynomial time approximation algorithm with approximation ratio 000000001 state the push relabel algorithm for network ow explain why the algorithm terminates in polynomial time with the correct answer when the network is a single path from the source to the sink assume that you have to solve the hiring problem at a large institution where e ectively you can t re anyone note that this is a realistic assumption thus once you hire someone the game is over find a strategy that will hire the best person with probability o assuming that each permutation is equally likely note that for the purposes of this problem hiring the second best person hiring no one and hiring the worst person are all equivalent the only thing that matters is hiring the best person the person hiring knows n the number of applicants a priori justify that the algorithm hires the best person with probability o you have a sorted array a of containing n real numbers each selected independently and uniformly at random from the interval you have an real x in the problem is v to nd a subarray of size n that contains x give an algorithm with expected running time o you must prove this claimed running time you may use without proof the fact that the probability that a binomially distributed random variable x is more than d times its mean  is at most e and the fact that the probability that a binomially distributed random variable x is less than d times its mean  is at most e a binary search tree is a balanced if for every node x the size of the left subtree of x is at most a times the size of the subtree rooted at x and the size of the right subtree of x is at most a times the size of the subtree rooted at x consider a binary search tree with standard insert and delete operations with the following exception let x be the highest node that is no longer balanced then every node in the subtree rooted at x is made balanced you may assume without justi cation that the time required is linear in the size of the tree rooted at x show using the following potential function that the amortized time for n insert and delete operations is o log n de ne x to be the absolute value of the di erence in the size of the left subtree of node x and the right subtree of node x then the potential function x f c x x x start by explaining how a potential function argument works in this setting and what you need to prove in order to establish o log n amortized time state and prove the master theorem for recurrences of the form t n at n b nk for integer constants a b and k you are given the exchanges rates between n di erent currencies give an o time algorithm to determine whether you can make money money via arbitrage assume no costs for conversion between currencies prove the correctness of this algorithm final computer science introduction to algorithms spring instructions the test is closed book closed notes for most of the problems i am interested in testing whether you understand the techniques and concepts more than i am interested in the solution to the particular problem for example if i ask you to prove that a problem is np hard i am more interested in learning if you know how to prove that a problem is np hard than i am in the speci cs of the problem if i ask you problem that a greedy algorithm is correct using an exchange argument i am more interested in learning if you know how an exchange argument works than i am in the speci cs of the problem i ask these questions in the context of speci c problems to allow you to demonstrate your understanding in a concrete setting of course you have to take into account the speci cs of the problem but make sure to explain the general method technique concept that you are using as well 25 partial credit is given for the answer i don t know a blank answer will be inter preted as i don t know false or completely unsubstantiated assertions will receive lesser credit solve up to of the problems clearly write the number of the problem that you are solving if you answer more than an arbitrary answers will be graded if you are uncertain about anything ask a question in the knapsack problem the input consists of the weights wn and values vn of n coins and a weight limit w a feasible solution is a subset of the coins of aggregate weight at most w the objective is to maximize the aggregate value give an o nw time and o n w space algorithm to compute the optimal solution to this optimization problem note that the algorithm needs to output the actual subject of coins not just the subset aggregate weight and value in the knapsack problem the input consists of the weights wn and values vn of n coins and a weight limit w a feasible solution is a subset of the coins of aggregate weight at most w the objective is to maximize the aggregate value assume you have an o nw time dynamic programming algorithm that solves this problem show how to derive a polynomial time approximation scheme for the knapsack problem start by de ning polynomial time approximation scheme note that you need not have answered the previous problem in order to answer this problem you must have some reasonable justi cation that you indeed have given a polynomial time approximation scheme consider a setting where you have two computer networking routers a and b each router has collected a list la and lb of ip source addresses for the packets that have passed through the router that day an ip address is n bits and thus there are possible ip addresses now the two routers want to communicate via a two way channel assume there is no delay of messages to determine whether there was some source that sent a packet through one of the routers but not the other prove using an adversarial argument that every deterministic protocol for this problem must sent o bits for some possible instance assume that you hash n items into a closed addressed hash table of size n with the probability that each item hashes to a particular table energy is n independent of where the other items hash let xi be the number of items that hash to table entry i show that k n ne e maxi xi o log n log log n you may use without proof the fact that k k consider the following online problem you given a sequence of bits bn over time each bit is in an envelope you rst see the envelope for then the envelope for etc when you get the envelope i you can either look inside to see the bit or destroy the envelope in which case you will never know what the bit is you know a priori that at least n of the bits are you goal is to nd an envelope containing a bit you want to open as few envelopes as possible show using yao technique that every randomized monte carlo algorithm for this problem that fails with probability at most n must open o log n envelopes in expectation start by stating clearly the version of yao technique that is applicable here you do not need to prove the correctness of this version of yao technique consider a two person game speci ed by an m by n payo matrix p note that some entries could be negative the two players can be thought of as a row player and a column player the number of possible moves for the row player is m and the number of possible moves for the column player is n each player picks a probability distribution over its moves and then money is exchanged in particular the row player pays the column player er c pr c dollars where the expectation is over the joint probability distribution of the row and column players move we assume that the game is played sequentially so that one player speci es his probability distribution the other players sees that probability distribution and then speci es a response probability distribution we will assume that this player makes the best possible response each player wants to be payed as much money as possible and if this is not possible to pay as little as possible in expectation a show that problem of nding the best probability distribution for the player that goes rst say the row player for concreteness can be expressed as a linear program b explain what linear programming duality says about how the amount of money that the row player makes if she goes rst compares to the amount of money the row player makes if she goes second you must have some justi cation here the input for the maximum cut problem is an undirected graph g v e a feasible solution is a subset s of the vertices the objective is the number of edges between s and v s give a deterministic algorithm for this optimization problem and show that it is approximate include a de nition of approximate that is appropriate for this setting consider the paging problem consider the following deterministic online algorithm algorithm description each page p has an associated bit fresh or stale if requested page p in fast memory then p associated bit is set to fresh if the requested page p is not in fast memory then an arbitrary stale page is selected from the stale pages in fast memory and ejected and p associated bit is set to fresh if the request page p is not in fast memory and all pages in fast memory are fresh then make all pages in fast memory stale select an arbitrary stale page from the stale pages in fast memory to evict and p associated bit is set to fresh we want to consider the homework problem that showed that this algorithm with k pages of fast memory is competitive approximate against the optimal algorithm for k pages of fast memory using the following potential function xx f credit p credit p p on p op t where on is the algorithm cache and op t is the adversary optimal cached and credit p is equal to if page p is in the algorithm fast memory and is fresh and credit p otherwise a write the key equation that you need to show holds for every access b identify the case where you need to use the assumption that the algorithm cache is twice as large as the adversarial optimal cache and show that the key equation holds in this case show that the coloring problem is np hard using a reduction from cnfsat recall the the input for the coloring problem is an undirected graph and the question is whether each vertex can be colored with of possible colors so that no pair of adjacent vertices are colored the same color recall that input to the cnfsat problem is a boolean formula in conjunctive normal form and of or of variables or negation of variables with exactly literals per clause you may assume the existence of a graph s with the following special property s has designated vertices x y z and t and s is colorable if and only if at last one of x y or z is colored the same color as t is colored show using linearity of expectations that the expected running time of randomized quick sort where the splitter pivot is picked uniformly at random from the subarray being sorted is o n log n make sure to clearly de ne your variables consider the ford fulkerson algorithm for network ow on a graph network with v vertices e edges and maximum ow f we gave an algorithm a with worst case running time t fe we gave an algorithm b with worst case running time t we gave an algorithm c with worst case running time t ve log f state with justi cation which of these algorithms are polynomial time algorithms which are strongly polynomial time algorithms and which are pseudo polynomial time algorithms you should start with a de nition of polynomial time algorithm strongly polynomial time algorithm and pseudo polynomial time algorithm the input to the vertex cover problem is an undirected graph g the feasible solutions are subsets s of vertices such that every edge is incident on at least one vertex in s the objective is to minimize the number number of vertices in s a explain how to express this problem as a integer linear program b explain how to use the relaxed linear program where variables are allowed to be non integer rational numbers to develop a polynomial time approximation algorithm for this problem that is approximate you must justify that the algorithm is approximate give a de nition of approximate that is appropriate for this setting final computer science introduction to algorithms spring instructions the test is closed book closed notes formost of theproblems i aminterestedintesting whetheryouunderstand thetechniques and concepts more than i am interested in the solution to the particular problem for example ifi askyoutoprovethat aproblemisnp hard i am moreinterestedinlearningif youknowhowtoprovethataproblemisnp hard thani aminthe specifcs oftheproblem ifi askyouproblemthat agreedy algorithmis correct using an exchange argument i am more interested in learning if you know how an exchange argument works than i am in the specifcs of the problem i ask these questions in the context of specifc problems to allow you to demonstrate your understanding in a concrete setting of course you have to take into account the specifcs of the problem but make sure to explain the general method technique concept that you are using as well 25 partial credit is given for the answer i don t know a blank answer will be inter pretedas idon tknow falseorcompletelyunsubstantiated assertionswill receivelesser credit solve up to of the problems clearly write the number of the problem that you are solving if you answer more than an arbitrary answers will be graded if you are uncertain about anything ask a question give a polynomial time dynamic programming algorithm to compute the shortest bitonic tour of n points in the euclidean plane recall that a bitonic tour starts at the leftmost point moves right until is hits the rightmost point and then moves left until it returns to the leftmost point giveagreedy algorithmforthefollowingproblem andprove using an exchangeargument thatthisalgorithmiscorrect theinputtothisproblem consists of n jobs for eachjob j you aregiven a releasetime rj and a processing time pj the output is a feasible schedule that minimizesthe average completiontime of ajob afeasiblepreemptive scheduleis a mapping from each time t to ajob releasedbeforetime t that is being processed at time t ajob j is completed once it has been processed for pj units of time prove that the subset sum problem is np hard by reduction from vertex cover or by reduction from sat recall that the subset sum problem takes as input positive integers xn l and asks whether there is a subset of the xi that sums to l recall that the input to thevertexcoverproblemis agraph g and an integer k and thequestion is to determine whether g has a vertex cover of size k a vertex cover is a collection of vertices such that every edgeisincident on atleast one vertexinthe collection recall that the input to the satproblemis aboolean formulain conjunctive normal form with exactly clause and thequestionistodetermineif theformulais satisfable giveanadversarial argumenttoshowthat every comparisonbased algorithmtodetermine whether n input numbers are distinct requires n logn comparisons calculate the expected time to insert n items into a hash table of size n using uniform open hashing which means that each probe into the hash table is equally likely to go to every table energy independent of other probes showthatif n items areinsertedinto a closed addressedhash table of size n then expected number of items in the table entry with most items is o logn state explain how the push relabel network fow algorithm works you do not need to provethatitis correct or analyzethe running time just statethe algorithm consider the problem of constructing a maximum cardinality bipartite matching the input is a bipartite graph where one bipartition are the girls and one bipartition is the boys there is an edge between a boy and a girl if they are willing to dance together the problem is to matching the boys and girls for one dance so that as many couples are dancing as possible a construct an integer linear program for this problem b considertheassociatedlinearprogramwheretheintegralityrequirementsaredropped explain how to fnd an integer optimal solution from any rational optimal solution c construct the dual program d give a natural english interpretation of the dual problem e explainhowonecanalwaysgiveasimpleproof that agraphdoesn thaveamatching of a particular size using the dual using linear programming duality you should be able to come up with a method that would convince someone who knows nothing about linearprogramming consider an online or approximation problem where there are only fnitely many possible algorithmsand fnitely manypossibleinputs weconsidergeneralizingyao stechniqueto approximation ratios assumethattheproblemis a minimizationproblem assumethat you have an input distribution i such that for all deterministic algorithms a it is the case that e a i e opt i c showthatyou canlogically concludethatthe expected approximation ratio for every randomized algorithm is at least c assume that you want to load n crates with weights wn onto k trucks so as to minimize the weight of the heaviest load on any one truck consider the algorithm that considers the crates in an arbitrary order and puts the next crate on the truck with the lightest load to date show that the approximation ratio for this algorithm is at most explicitly state what lower bounds you are using for optimal assume that we have a dynamic table to which we can insert and delete items let us say that each of insertion and deletion costs when the table is full the table size is doubled if the table is doubled from size n to size then let us say that this costs when the table is full the table size is halved if the table is halved from size to size n then let us say that this costs prove using a potential function argument that the amortized cost per insertion and deletion operation is o 12 recall that the input to the knapsack problem was n coins with positive integer weights wn positive integer values vn and a weight limit w the problem was to fnd a subset of the coins with maximum aggregate value subject to the constraint that the aggregate weight is at most w give an algorithm to solve this problem in o w space and o nw time you mustjustify the correctnessof the claimed timeand spacebounds notethatthe algorithm mustproducethe actual subset of coins notjustthe weight and the value an algorithm that solves this problem using more time or more space is not worth any partial credit consider the paging problem where there are k pages of fast memory and n k pages of slowmemory soanonlinealgorithmseesovertimeasequenceof requeststopagesinslow memory if thepage requestedisnotinfast memory thenapagefault occurs inresponse to a page fault the online algorithm has to evict a page of its choice from fast memory and copy the requested page to fast memory the objective is to minimize the number of accesses to slow memory show that the paging algorithm fifo with a fast memory of size k has at most twice as many page faults as the optimal paging algorithm with a fast memory of size k recall that fifo evicts the page that entered fast memory at the earliestpoint in time stateandprovethe mastertheorem fordivideand conquerrecurrencerelations recall that themastertheoremgivesthe solution to recurrences of theform t n at n b nk for constants a b and k final computer science introduction to algorithms spring instructions the test is closed book closed notes ifyouaretaking the fnalforpreliminary examcredit thenwriteyourpreliminary number given to you by keena on the exam answer sheet but do not write your name if you are nottaking the fnalforpreliminary examcredit writeyournameontheexamanswersheet ifyouaretaking thecoursefor somegrading option otherthanthestandardlettergrading option please specify that on your exam answer sheet formost of theproblems i aminterestedintesting whetheryouunderstand thetechniques and concepts more than i am interested in the solution to the particular problem for example ifi askyoutoprovethat aproblemisnp hard i am moreinterestedinlearningif youknowhowtoprovethataproblemisnp hard thani aminthe specifcs oftheproblem ifi askyouproblemthat agreedy algorithmis correct using an exchange argument i am more interested in learning if you know how an exchange argument works than i am in the specifcs of the problem i ask these questions in the context of specifc problems to allow you to demonstrate your understanding in a concrete setting of course you have to take into account the specifcs of the problem but make sure to explain the general method technique concept that you are using as well 25 partial credit is given for the answer i don t know a blank answer will be inter pretedas idon tknow falseorcompletelyunsubstantiated assertionswill receivelesser credit solve up to of the problems clearly write the number of the problem that you are solving if you answer more than an arbitrary answers will be graded if you are uncertain about anything ask a question two of the three following defnitions of big omega area logically equivalent identify the two that are logically equivalent and prove that they are equivalent take care in your exposition prove that the thirddefnitionis not logically equivalentto the other two you may assume that we are only consideringfunctions that aredefned on thepositiveintegers and are positive and non decreasing a f n g n i n f n cg n b f n  g n i n cg n c f n o g n i 8n n f n cg n give as simple estimates as you can of the following sums that are accurate to within a multiplicative constant you do not need to fully prove the correctness of each of your answers butgive some convincingjustifcationforyour answers a p x i loglogi i b x log n i logi loglogi c log log n x i d p n x i show that the expected case running time of randomized quicksort is o n logn using linearity of expectation assume that the splitter is picked uniformly at random from the subarray in question 4 consider the o ine paging problem where there are k pages of fast memory and k 1 pages of slow memory the input is an sequence of requests to pages in slow memory if the page requested is to the unique page that is not in fast memory then a page fault occurs in response to a page fault the o ine algorithm has to evict a page of its choice fromfast memory and copy therequestedpagetofast memory givethe o inealgorithm that minimizes the number of page faults prove that this algorithm is correct optimal using an exchange argument 5 considerthefollowingenergy managementproblemfacedby acity thecity ownsahydro electricdamthat cangenerate up to m megawatt hours of electricyperday at a cost of l dollars even if less than m megawatt hours is generated the cost is still l additioally on each day the city can buy additional electricity for c dollars per megawatt hour the city can store unused energy in batteries that have a capacity of b megawatt hours but this costs h dollars per megawatt hour per day the city knows its energy needs for n consecutive days are dn megawatt hours give an algorithm that computes the optimal least cost energy generation schedule that meets the city energy needs on each day this schedule needs to specify on each day how much energy to generate from the dam how much energy to buy how much energy to take out of the batteries and how much energy to store in the battery the algorithm running time should be polynomial in n and b assume allparameters arepositiveintegers note thatfor simplicitythat eachdayisbeing represented as a point in time obviously in real life batteries could be flled emptied continuously instead of discretely consider the situation where you have a web server running over n days on some days the server is up and running and on other days the server is down there events are not bernoulli trials there may be dependence between the server states on various days let xi be the random variable denoting the number of consecutive days that the server is up starting from day i 1 i n all you know is that for all i the probability that xi is more than k is at calculate as tight of an upper bound as you can on the expected value of max xn 7 explain how to modify dijkstra algorithm to compute longest path in graphs with positive edges weights where now the length of a path is the minimum edge weight on thepath soyouwantto fndpath wheretheminimumedgeweightismaximized showthattheproblem of solving anintegerlinearprogramisnp hard using thefactthat 3 cnf sat isnp complete recall that the inputfor 3 cnf sat is a booleanformulain conjunctivenormalform with exactly threedistinctliteralsper clause and theproblemis to determine if the formula is satisfable showthatifthereisapolynomial approximationalgorithmforthecliqueproblem then there is a polynomial time approximation algorithm for the clique problem the inputforthe cliqueproblemisan undirectedgraph g and the output is the largest clique a collection ofmutually adjacent vertices considertheonlinepagingproblemwherethereare k pages offastmemory and k of slow memory so an online algorithm sees over time a sequence of requests to pages in slow memory if thepage requestedistothe uniquepagethatis notinfast memory then a page fault occurs in response to a page fault the online algorithm has to evict a page ofits choicefromfast memory and copy the requestedpagetofast memory assumethat each of the pages of slow memory is equally likely to be requested independent of other requests a show that for every deterministic online algorithm the expected number of requests between page faults is k what sort of distribution does the number of requests between page faults have b showthattheexpectednumberof requestsbetweenpagefaultsfortheoptimal o ine algorithms is o k logk c concludethattheexpected numberofpagefaultsfor everydeterministiconlinealgo rithm is logk times the optimal number of page faults consider the paging problem where there are k pages of fast memory and k 1 pages of slow memory so an online algorithm sees over time a sequence of requests to pages in slow memory if the page requested is to the unique page that is not in fast memory then a page fault occurs in response to a page fault the online algorithm has to evict a page ofitschoicefromfast memory and copy the requestedpagetofast memory assume that you are given that there is an input distribution such that the expected number of pagefaultsforeverydeterministiconlinealgorithmis logk timesthe optimal number of page faults show that for every randomized online algorithm there in an input sequence such thattheexpected numberofpagefaultsfortheonlinealgorithmis logk times the optimal numberofpagefaults sobasically youare asked toreprovethatyao stechnique worksinthe case of approximation competitive ratiofor a minimizationproblem 12 consider the problem where the input is n numbers and the output is the largest and smallest number prove using an adversarial argumentthat everydeterministiccomparison basedalgorithm o 1 comparisons sothe onlywaythe algorithm can access the input is by comparing two numbers in the input consider the problem computing the shortest tour connected n points in the euclidean plane give a polynomial time approximation algorithm for this problem you must prove that your algorithm is a 2 approximation algorithm provethefactthat max fowequalsmin cutisaconsequenceof stronglinearprogramming duality thatthe objectivesforprimal anddual optimal solutions arethe same 15 considerthefollowingproblem theinputisa directed graph withpositiveweightsonthe edges and a designated vertex the goal is to fnd the least cost spanning tree t rooted at with all edges in t directed away from so in t there is a unique path in t from s to each vertex in the graph the cost of a tree t isjust the sum of the weights ofthe edges explainhowthisproblem canbe solvedinpolynomial time assumethat we wanttoprovethat an online scheduling algorithm a guarantees that the average waiting timeforfor ajobis at timesthe average waiting timeforshortest remaining processing time scheduling algoritm which is optimal for the objective of average waiting time using apotentialfunction inthe mannerthat we usedto analyze round robin state what statements need to be proved in order to complete this proof thepurpose of thisquestionistodetermineifyouknowhow apotentialfunction argument works afew reminders about scheduling eachjob i has a release time ri which is the earliest time that it can be run and a size pi ifjob i runs on a speed s processor it will complete after running pi s timeunits thewaiting timefor ajob i is the di erenecein time ci ri between ri and when it is completed at time ci the average waiting time for n jobs is p then in 1 ci ri n course description this will be an intermediate level graduate course on approximation algorithms in general the area of approximation algorithms deals with finding efficient algorithms for np hard problems that have known worst case performance guarantees for example the traveling salesman problem finding the shortest tour visiting all points in some space is known to be np hard and thus can not be solved efficiently for all inputs unless p np but if the points are in the euclidean plane then there is an efficient algorithm to compute a tour that is at most more than optimal for all inputs we will concentrate on understanding the main ideas techniques and concepts at the expense of covering all details we will concentrate on lp based algorithms and in particular on primal dual algorithms course text cs fall http people cs pitt edu kirk we will rather closely follow selected portions of the excellent text authored by vijay vazirani the table of contents can be found on the amazon page for the text the preface table of contents and first chapter of the text can be found here for background on linear programming i recommend linear programming by vasek chvatal if you can afford combinatorial optimation algorithms and complexity by christos papadimitriou and kenneth steiglitz only on amazon including shipping for background on basic algorithmics i suggest introduction to algorithms by thomas cormen ronald rivest charles leiserson and clifford stein course format i will present the overwhelming majority of the lectures each student will give one lecture at the end of the term on some paper from the area of approximation algorithms most lectures will devoted to understanding a particular problem and a particular algorithm i use the socratic method that is i try to develop the ideas by asking the students leading questions the grading will be based on daily homework and the final presentation prerequisites the ideal background would include an introductory algorithms course such as cs and an introductory course on mathematical linear programming many of algorithms covered in the text use techniques normally covered in an introductory linear programming course however the official prerequisite is either an introductory algorithms course or an introductory course on linear programming class time and location cs fall http people cs pitt edu kirk tuesdays and thursdays from to we will meet room of the sennott square building not room as listed in the schedule of classes tentative schedule introduction classes approximation algorithms with explicit lower bound examples approximate mst algorithm for tsp approximate matching algorithm for vertex cover and approximate lp algorithm for vertex cover reading chapters and homework problem and from vazirani assume you have a polynomial time algorithm for finding minimum weight matchings and minimum weight matchings due tuesday sept approximation algorithms without explicit lower bounds examples o log n approximate greedy set cover algorithm and approximate multiway cut algorithm reading chapters and homework problems and due thursday september k center class reading chapter homework problem due tuesday september shortest superstring class reading chapter homework problems and due thursday september review of algebraic geometric and economic views of duality classes reading chapter homework problems due tuesday september homework problem due thursday september set cover via dual fitting day reading chapter homework problem due tuesday september set cover via rounding day reading chapter homework problems and due thursday september max sat day read chapter homework problems and in and can you develop a derandomized algorithm that only needs to solve lp instance due thursday october scheduling on unrelated machines day reading chapter cs fall http people cs pitt edu kirk homework problem due tuesday october set cover via primal dual day reading chapter homework none multicut in trees day reading chapter homework due tuesday october edge multiway cut day reading chapter homework problems and due thursday october node multiway cut day reading chapter homework problem due tuesday october multicut in general graphs day reading chapter homework problems and sparsest cut day reading chapter no homework steiner forest day reading chapter homework problem due thursday october steiner network day reading chapter homework problem due tuesday november facilty location day reading chapter k median day reading chapter hardness of approximation day reading chapter martin p al and eva tardos key points to discuss the cost sharing problem metric facility location single source rent or buy defnition of key properties developing group strategyproof mechanisms for both problems using similar primal dual algorithms approximation factors for both algorithms the cost sharing problem inputs a set u of potential users each user has a private utility ui of being serviced by a facility or connected to a network idea we want to install or maintain a facility or network such that the users of the facil ity network share the cost of its installation maintenance outputs a set j u of users that will be users of the facility or part of the network we also want the cost of installation or mainte nance to be shared among the users so that ev eryone is content with the decision known as cost shares meaning of content properties of group strategyproofness a user should not have incentives to change his mind about contributing or not contributing to a facility or about being in or not being in the network later a user should not have incentives to lie about his utility a group of users should not be tempted to buy a separate facility or to build a separate network of cost smaller than the sum of their current contributions cross monotonicity the cost share of each user never goes up as the set of participating users increases moulin and shenker show that cross monotonic cost sharing leads to group strate gyproof mechanisms for determining the set of users j competitiveness the sum of the cost shares cannot be more than the true cost cost recovery the sum of the cost shares must pay for the true cost a approximate cost recovery users are only required to recover pay for a fraction of the true cost developing a group strategyproof mechanism for the cost sharing problem we need to determine the set j of participating users determine cost shares with the properties cross monotonicity competitiveness and a approximate cost recovery focus on two problems metric facility location easy and single source rent or buy more dicult metric facility location review from class inputs a set of potential facilities f and a set j u of users idea open a subset of the facilities paying amount fp for each open facility p and build a link from each user j j to some open facility given the cost of connecting user j to facility p defned as cjp outputs which facilities to open which users are connected to which facilities primal min x fpyp xx cjpxjp pp j t x xjp j p yp xjp j p xjp yj dual max x aj j t aj jp cjp j p x jp fp p j aj jp key concepts grow a ball ghost around each user until either the ghost touches a full facility or a facility that the ghost is already touching becomes full when facility p becomes full we open it i there is no already open facility q such that c p q p where t p is the time when p becomes full this is slightly di erent than the method we discussed in class but achieves the same approximation the cost of the solution constructed is at most times the sum of the cost shares thus the sum of the costs that a group of users pays to open a facility cost shares is at least of the cost of opening the facilities plus connec tion costs key concepts con t cross monotonicity the cost share of each user never goes up as the set of participating users increases by adding more users each facility can only get flled more quickly and hence each individual user can only become satisfed earlier competitiveness the method cannot charge users more than the true cost the algorithm provides a lower bound to optimum a approximate cost recovery as stated pre viously this algorithm recovers of the cost of the solution constructed main result this primal dual algorithm is a approximate cross monotonic cost sharing i e group strategyproof method for metric facility location single source rent or buy inputs a set of j u of users and a source residing in a graph g v e a parameter m idea build a tree such that there exists a path between each user and the source using edges in e each edge can either be bought at a cost mce or rented at a cost of ce bought edges can support any number of paths the rental cost ce must be paid for every path that uses edge e outputs a list of bought edges and rented edges structure of the optimal solution it is not hard to see that the bought edges in the optimum rent or buy network must form a steiner tree with at its root the rental edges form a shortest path con nection from each user j to the closest point in the tree primal and dual formulations min xx cijxij m x ceze ji e e t x xij j i v x ze x xis x xitforalls v eoutofs i si s x ze x xis x xitforalls v eoutofs i si s grow a ghost around every user until the user ghost touches a full center or a cen ter the ghost is touching becomes full as in fl a location is full if it is touched by m or more users ghosts since there are no opening costs we open a center at any full location p use the primal dual steiner tree algorithm to connect the opened centers cost shares each user pays for its rental edges to connect to a center share the costs of buying edges to a center among the users of that center start with each center in a separate com ponent grow a ghost around each center when two ghosts touch check if the two centers are in the same com ponent if not buy a shortest path between the ghost centers merging the two components into one the center p of the frst ghost to reach the source is allowed to buy a shortest path be tween p and the algorithm incurs a cost m per time unit for growing each component that doesn t contain the root vertex problems if we open all full centers many more cen ters are opened than necessary the cost of building a steiner tree on all centers may be very large each user may be connected to multiple centers making decisions on how to allocate users to centers is necessarily infuenced by the addition of new users it is hard to guar antee cross monotonicity a better idea simultaneously grow ghosts around users to determine rental edges and build a steiner tree to determine which edges to buy simplifying assumption it is convenient to think of an edge e as a line segment of length ce containing a continuum of points the term location refers to both original vertices and in termediate points it has been shown that a solution that is al lowed to rent and buy edge segments arbitrarily can be transformed into a solution that does not use intermediate points of no greater cost b j t the ghost around j with radius t c the time varying set of all locations that have been reached by m or more user ghosts at any time c can be represented as a collec tion of vertices edges and segments of edges let c c connected component of c any inclusion maximal c c such that any two location c are joined by a path lying completely within c we say that a user j at time t is connected to a component c of c if b j t c  a user j is satisfed if it is connected to a component containing the source vertex t j the time when user j frst becomes con nected to a component j the time when user j becomes satisfed the algorithm maintains a list of components c initialize c  begin growing a ghost around each user as time progresses a user ghost may reach an other user location let s be this set of users that have reached a location p when s m we add the set nj sb j t of locations as a new component of c at this time location p is consider to be full we declare a full p open if at time t p there is no open center within a radius p of p as time progresses two components may touch when this occurs we join the components by constructing a shortest path between their cen ters we continue until all users are satisfed and c contains a single component we should make every user pay for the rental costs associated with him if a user is con nected to multiple components we only let him contribute to the component where his share is the smallest let j c denote the set of users connected to c for a connected user j let aj t be the maxi mum size j c over all components c that j is connected to at time t defne fj as follows fj t for t t j fj t m aj t for t j t j and fj t for t j defne a and for each user as follows z j z t j aj fj t dt and aj fj t dt t j maintaining key properties theorem cross monotonicity the cost shares a and are cross monotonic functions of the set j proof sketch by adding more users we can only make the set c larger and hence make each user connect earlier moreover each com ponent of c can only increase by adding more users hence as the number of users con nected to that component increases the cost shares can only grow slower theorem competitiveness every feasible solution to the single source rent or buy in stance has a cost of at least max pj j aj pj j a approximate cost recovery lemma let p be an open center and let j sp then j t p for a j that does not belong to any sp there is an open ghost center p such that c j p j this lemma shows that the cost shares can pay for of the rental cost of the network similar to the facility location case lemma the cost of the tree we buy is at most pj aj bounding the tree cost next slide bounding the tree cost we can think of the algorithm as incurring a cost m per time unit for growing each compo nent that does not contain the root vertex if a t denotes the number of non root compo nents at time t the total cost of growing the components is z ma t dt where the integral is over the entire execution of the algorithm by standard arguments the cost of the tree constructed is at most two times the growing cost for each steiner component we defne a time varying set contrib of users that will be responsible for maintaining a steady fow of funding of at least m per time unit from the time increments fj t of their cost shares thus we have that at any time t the users are able to collect ma t revenue from their contributions at time t hence z z ma t dt x fj t dt j z x fj t dt x jj since m a t dt can pay for half of the tree the buying costs of our solution does not ex ceed pj aj main result lemma the cost of the solution constructed is at most pj j follows from previous slides theorem the cost shares are cross monotonic competitive and recover fraction of the cost of the solution constructed the approximation factor follows from the fact that a feasible solution has a cost of at least max pj j aj pj and the j bound stated above lp based approximation algorithms for broadcast scheduling mahmoud elhaddad class presentation approximation algorithms u pittsburgh fall b kalyanasundaram k pruhs and m velauthapillai scheduling broadcasts in wireless networks proc esa r ghandi s khuller y a kim and y c wan algorithms for min response time in broadcast scheduling proc ipco r ghandi s khuller s parthasarathy and a srinivasan dependent rounding in bipartite graphs proc focs broadcast server with slotted broadcast channel n one slot pages pn requests ri t i n are received at the end of slot t objective is to minimize avg total response time ci t pi t t t t t t ri t offline problem shown np hard ultimate goal o approximation algorithm resource augmentation can we get an o approximation alg by throwing o more bandwidth at the problem a k speed server is one that broadcasts up to k pages every time slot speed approx alg speed approx and speed approx speed approx and speed approx reduction to integer min cost flow reduction probabilistic interpretation of the lp solution impose a structure on the lp solution to facilitate the definition of a speed alg and is an integer prove that the algorithm guarantees a factor approximate solution d d min t n t n n wi t tt fi t tt n t tt t i fi n i tt t fi tt t tt t n fi t tt i n t t n fi tt t t t n tt t i fi t tt i n t tt t n fi t n t i n t t t n lemma the optimal value of ip is the minimum total response time for the broadcast problem lopt is n feasible fraction of each page can be transmitted in a slot without violating the fourth constraint lemma all requests are eventually satisfied in the lp solution equiv ri t j t k t n probabilistic interpretation of lp solution tt t fi tt t min tt t pi tt ni t pi t fi j k ni t the ri t requests are satisfied at t n with prob pi they are satisfied at ni t with prob fi t the prob solution n t ni t lemma vl lopt let is an integer they seek a feasible solution bi j min t k pi k j the jth breakpoint for pi i ii j bi j bi j i n j h i interval ii j is satisfied at time t if pi is broadcast at t ii j assume all requests up to time t have been scheduled to schedule the requests at t let u be the set of currently unsatisfied intervals and pick from u up to intervals with smallest right endpoints broadcast them at time t lemma every interval is satisfied in ledf ledf is a approximation alg lemma the total response time of the feasible schedule ledf is at most lopt theorem there is a polynomial time speed approx alg for the broadcast problem where the objective is to min the average response time ilp formulation relaxed lp solution using lopt define a simplified problem instance that allows a speed alg and is an integer the simplified instance can be reduced to min cost flow on bipartite graph exact solution to relaxed simplified instance by the integrality theorem prove the solution for simplified instance is at most solution the optimal yit iff pi is broadcast at tt xi t iff requests i t are satisfied at tt t t min t n n tt t ri t xttt t tt t i yit xi t i n tt t t t n tt t tt xttt i n t yit tt i i t i n t tt yit i n tt let x y be the optimal fractional solution for r t define f t i t min ttt ttt i t transform i to it by consolidating the requests so that the following property holds for it property let nit be the set times of requests for pi in it and tut tvt then t tt nit f t i tut tvt requests i t grouped with i g i t g i t t f t i t g i t recall x y is the optimal n feasible fractional solution for i consider the solution x y tt t if tt f t i t i tt tt ttt t i tt if tt f t i t otherwise the solution x y is feasible in the weak sense that constraint becomes i yit tt however without integer solution x y up to n pages may need to be broadcast each time slot construct the following min cost flow network n ttj ttj rt tx t j tx tk tkt d tml x ty ty tml analysis lemma the cost of speed frac sol in it and that of an optimal frac sol for i are related as follows ri t t n tt g i t x t ri t g i t t lemma for any feasible flow in n there is a speed feasible fractional sol for it of the same cost and vice versa lemma there exists a speed integral solution for it of the same cost as the speed fractional sol x y theorem there is a speed broadcast problem approx solution for the strategy overview same ilp formulation as same relaxed lp solution using lopt define a simplified problem instance that allows a speed alg difference don t combine requests just drop those in the middle the simplified instance can be reduced to min cost flow on bipartite graph different from apply dependent rounding on the bipartite graph to get a speed approx integral sol a speed approx algorithm dependent rounding dr cs topics in algorithms spring topic is algorithmic game theory instructors class is canceled for wednesday january course description we will cover most of the very recent text fall on algorithmic game theory by nisan roughgarden tardos and vazirani after using the text here is a review that i wrote at the request of frits spieksma cs spring http people cs pitt edu kirk the amazon page contains a table of contents and an except a rather nice forward was written by christos papadimitriou this course is intended to be a broad introduction to algorithmic game theory that should be accessible to a wide range of students course format the instructors will present the initial lectures each student will be expected to lead some lectures discussions later in the semester covering some portion of the text one or two exercises may be assigned from each chapter to provide some practice using the concepts from the chapter class time and location mondays and wednesdays and maybe a rare friday to make up for classes missed due to instructors travels to academic conferences from to in room of the sennott square building tentative schedule date topic reference homework due at the start of next class occasionally i will ask someone to present the homework monday jan definition strategic game dominance pareto efficiency optimality pure nash equilibrium mixed nash equilibrium chapter wednesday jan lemke howson algorithm for general person games chapter is tough sledding homework consider the game in lewiki notes find a mixed nash equilibrium for the game cs spring http people cs pitt edu kirk insteead try good slides from class taught by michael lewiki apply the lemke howson algorithm to this nash equilibrium until you get to a new nash equilibrium your first step will be arbitrary show the path on the d and d spaces shown in the notes monday jan complexity of nash equilbrim reduction to linear programming for zero sum person games sperner lemma brower fixed point existence of nash chapter good notes from a class taught by christos papadimitriou wednesday jan np hardness of deciding if mixed nash equilibrium in a person game paper by conitzer and sandholm slides for the paper slides revised by the students group homework elaborate improve on the first page of these slides to make the proof more complete convincing illuminating in particular explain why a player won t play both a literal an its negation why a player won t play any literal with probability greater than n where n is the number of variables why a player won t play a variable move strategy why a player won t play a clause move strategy if the formula is satisfiable why you need the default move you may also change the instance constructed game say by adding infinity payoffs if you think that that helps email me the slides and i will post your revision don t get carried away i am thinking that this should take or at most hours monday jan no class mlk day wednesday jan class cancelled l monday jan learning regret minimization and equilibria chapter cs spring http people cs pitt edu kirk guest lecture katrina ligett wednesday jan kkt conditions resource allocation markets t paths ti paths section slides and slides by vijay vazirani monday feb fischer linear market section slides by amin saberi group homework consider a market of goods and buyers where the utility only increases as the square root of the quantity of good received that is a buyer i gets utility i j i j from i j units of good j it is probably more realistic to assume that utility is a concave function of quantity write eisenberg gale program for the market clearing problem for this market apply the kkt conditions determine from the kkt conditions which conditions in theorem hold for this market what if anything goes wrong if one tries to apply the algorithm in section 8 for fischer linear case to this market don t worry about running time concentrate on correctness of the algorithm if what goes wrong is minor can it be easily fixed wednesday feb social choice arrow impossibility lemma and gibbard satterthwaite section notes from class by christos papadimitriou group homework in the proof of arrow impossibility lemma that i did in class basically the same proof as in the book we consider the sequence f f recall that f satisfied the conditions of unanimity and independence of irrelevant alternatives it is easy to see that there must exist a k such that b a if f for j k and a b in f that is the preference flips at k in my argument i cs spring http people cs pitt edu kirk assumed that it was then also the case that a b in f for j k that is once the preference flips it has to stay flipped shenoda pointed out that it at least wasted obvious that this is true that is it is possible that the preference between a and b could flip several times your goal is to determine which of the following is true the preference can only flip once the preference can flip more than once but the proof can still go through with at most minor modification monday feb christine will be speaking intro to inefficiency of equilibria sections wednesday feb christine will be speaking routing games sections monday feb christine will be speaking network formation games a local connection game sections wednesday feb christine will be speaking network formation games a global connection game and the potential function method section group homework problem monday feb mechanisms with money vcg clark pivot rule sections group homework consider the auction problem of selling k identical items to k different bidders is have the i th highest bidder pay the bid of the i st highest bid truthful wednesday feb house allocation and stable marriage sections monday march combinatorial auctions the greedy algorithm for single minded bidders section wednesday march combinatorial auctions walrasian equilibrium and the lp relaxation communication complexity section section group homework prove lemma 13 using the kkt conditions or lp duality group homework problem from the text spring break monday march bgp routing section 3 group homework can you come up with a precise formulation of theorem that is both 3 3 cs spring http people cs pitt edu kirk interesting and at least plausibly true the main issue in the proof that follows is what is the domain of quantification when it is claimed that each as gets their most valued route wednesday march cost sharing chapter monday march cost sharing chapter group homework problem wednesday march cascading behaviour in networks chapter monday march lory presentation on sponsored search chapter talk slides wednesday april shenoda presentation on selfish load balancing chapter talk slides monday april no class wednesday april 9 no class monday april josh presentation on reputation systems chapter slides wednesday april rich presentation on biological applications of games slides monday april tomas presentation on bayesian approaches paper by jim ratliff talk slides wednesday april panickos presentation on peer to peer applications of games chapter bayesian games games and equilibrium concepts under uncertainty tomas singliar april examples blackjack action space draw card pass add up card values maximum total wins what the next card jpmorgan buys bear stearns action space share price offered payoff true value of business payment what is the extent of the mess ebay how much did that other guy bid outline goal to show how to incorporate incomplete information into games define equilibrium concepts and show how tricky they become setup sequential games of incomplete information sender receiver games decision making under uncertainty bayesian nash concepts and desirable properties sequential games games where players take turns optimal algorithm decision tree zero sum minimax tree deterministic game chooses max utility action chooses max utility action actions conditioned on outcomes utilities of outcome bayesian game setup dynamic game played sequentially each player i has a set of types i nature selects player type i i according to p prior joint distribution over types publicly known player is privately informed of his type uncertainty players don t know other players types players now choose their actions a in some order players receive payoffs according to outcome payoff function on the outcome ui a i r sender receiver games simplest case two players sender receiver running example sender applies for job at receiver her message is what she did last year i e went to beach or college receiver decides whether to hire or reject sender type is fixed i e going to college does not make you smart sender types private   smart dull action send a message m m beach college receiver only one type deterministic action a a hire or reject holds a prior belief p  over the sender type p  smart  p  dull  type space message space action space this belief is assumed to be common knowledge s r payoff graphical representation players nature sender receiver sender types smart dull receiver type r does not know which part of the tree he is in his information set all nodes he could be in graphical representation everything else being equal sender prefers to be hired by go to the beach by it doesn t matter if she smart or dull receiver wants an educated applicant so hires iff sender went to college more notation payoffs u for sender v for receiver pure strategies u v m a r sender has only her type  receiver has only the message mixed strategies sender prob distribution over m receiver distribution over a m m a m a m m a sender best response fix a message m we know payoff for m is expected value over r reaction ea u a m u m a a a sender best responses then are the max utility messages m def arg ma x a m u m a is a best response to m m a a if it is nonzero only in m i e supp m receiver best response receiver has obtained m m a knows but does not know let a be the prob that rcvr plays a after m wishes to maximize his expected utility r updates its belief about s type given m message carries a signal about type of s r decides based on the posterior belief receiver belief update bayes rule p m p m p m p m p m if denominator is non the message is on path some sender type has a non zero probability of sending m receiver decision receiver maximizes his utility for each message separately a p m arg max a a p m v m a strategy is a best response to iff supp m remember p depends on a p m bayesian equilibrium definition a bayesian equilibrium of the s r game is a triple such that p m a m m for all supp for all on the path messages m m supp m the conditional posterior belief system p is consistent with the bayes rule in our example x y l r t sender action if smart sender action if dull receiver action if beach if college receiver belief of smart if beach if college in our example eq c c r h  sender action if smart sender action if dull receiver action if beach if college receiver belief of smart if beach if college this is a bayesian equilibrium pooling strategy profile in our example eq b b r r  sender action if smart sender action if dull receiver action if beach if college receiver belief of smart if beach if college this is a bayesian equilibrium but an unstable one perfect bayesian equilibrium definition a bayesian equilibrium of the s r game is a triple such that p m a m m for all supp for all on the path messages m m supp m the conditional posterior belief system consistent with the bayes rule p is existence of bayesian equilibria nash thm a mixed nash always exists if there are finitely many players with finite action sets create a regular game with  n players replicate player for each type p p with utilities v j eui i i i agent plays what the corresponding typed agent would in the corresponding normal game nash thm gives an equilibrium mixture over strategies  then i j w p i j is a bayesian equilibrium more trouble with bayesian equilibria additional conditions of equilibrium reasonableness college is tough for dulls education is unproductive no gain for rcvr from coll but signals the type of applicant c b r h is a separating pbe smart to college reject beachgoers beachgoers are smart wp college is tough for dulls education is unproductive no gain for rcvr from coll b b r r t t is a pbe everybody to beach reject everybody beachgoers are smart wp problem rcvr interprets deviation c as coming from a type  d who has no incentive to deviate dominated messages college is dominated for dulls whatever the outcome r or h dull type is better off with beach definition a message m is dominated for if there exists m such that min a a m u m a max a a m u m a where a m are all the actions that can be a best response for some type a m a p m p m test of dominated messages a pbe fails the test of dominated messages if for any arbitrary message m the receiver puts a positive weight on the sender being of the type for which the message is dominated the receiver assumes the sender is irrational because she should have sent the dominating message technical note there must be some type for which m is not dominated otherwise a technical problem b we are computing a response to something a fully rational agent could not do equilibrium domination b b h r t t is a be that passes test of dominated messages 3 everybody gets hired for pts but smarts want to deviate equilibrium domination m is equilibrium dominated wrt eq if expected payoff u from the equilibrium exceeds what the player can get by playing m u max a a m u m a the dull sender should not deviate p dull college thanks further readings book section bayesian mechanism design junk slides complexity results about nash equilibria vincent conitzer and tuomas sandholm carnegie mellon university computer science department forbes avenue pittsburgh pa usa conitzer sandholm cs cmu edu abstract noncooperative game theory provides a norma tive framework for analyzing strategic interactions however for the toolbox to be operational the so lutions it defines will have to be computed in this paper we provide a single reduction that demon strates hardness of determining whether nash equilibria with certain natural properties exist and demonstrates the hardness of counting nash equilibria or connected sets of nash equilibria we also show that determining whether a pure strategy bayes nash equilibrium exists is hard and that determining whether a pure strategy nash equilibrium exists in a stochastic markov game is hard even if the game is invisible this remains hard if the game is fi nite all of our hardness results hold even if there are only two players and the game is symmetric introduction noncooperative game theory provides a normative frame work for analyzing strategic interactions of agents however for the toolbox to be operational the solutions it defines will have to be computed there has been growing interest in the computational complexity of natural questions in game theory starting at least as early as the complexity the orists have focused on the complexity of playing particular highly structured games usually board games such as chess or go but also games such as geography or qsat these games tend to be alternating move zero sum games with enormous state spaces which can nevertheless be con cisely represented due to the simple rules governing the tran sition between states as a result effort on finding results for general classes of games has often focused on complex languages in which such structured games can be concisely represented real world strategic settings are generally not nearly as structured nor do they generally possess the other proper ties most notably zero sumness of board games and the the material in this paper is based upon work supported by the national science foundation under career award iri grant iis itr iis and itr iis like algorithms for analyzing this more general class of games strategically are a necessary component of sophisti cated agents that are to play such games additionally they are needed by mechanism designers who have some con trol over the rules of the game and would like the outcome of the game to have certain properties such as maximum social welfare noncooperative game theory provides languages for rep resenting large classes of strategic settings as well as so phisticated notions of what it means to solve such games the best known solution concept is that of nash equilib rium where the players strategies are such that no in dividual player can derive any benefit from deviating from its strategy the question of how complex it is to construct such an equilibrium has been dubbed a most fundamental com putational problem whose complexity is wide open and to gether with factoring the most important concrete open question on the boundary of today while this question remains open important concrete ad vances have been made in determining the complexity of re lated questions for example person zero sum games can be solved using linear programming in polynomial time as another example determining the existence of a joint strategy where each player gets expected payoff at least k is p complete in a concisely representable extensive form game where both players receive the same utility as yet another example in player general sum normal form games determining the existence of nash equilibria with cer tain properties is p hard finally the complexity of best responding of guaranteeing payoffs and of finding an equilibrium in repeated and sequential games has been stud ied in in this paper we provide new complexity results on ques tions related to nash equilibria in section we provide a single reduction which significantly improves on many of gilboa and zemel results on determining the existence of nash equilibria with certain properties in section we use the same reduction to show that counting the number of nash equilibria or connected sets of nash equilibria is p hard in section we show that determining whether a game can be converted to a normal form game as well but it will grow exponentially in size and the hardness result does not go through pure strategy bayes nash equilibrium exists is hard fi nally in section we show that determining whether a pure strategy nash equilibrium exists in a stochastic markov game is hard even if the game is invisible this remains hard if the game is finite all of our hardness results hold even if there are only two players and the game is symmetric equilibria with certain properties in normal form games when one analyzes the strategic structure of a game espe cially from the viewpoint of a mechanism designer who tries to construct good rules for a game finding a single equilib rium is far from satisfactory more desirable equilibria may exist in this case the game becomes more attractive espe cially if one can coax the players into playing a desirable equilibrium also less desirable equilibria may exist in this case the game becomes less attractive before we can make a definite judgment about the quality of the game we would like to know the answers to questions such as what is the game most desirable equilibrium is there a unique equilib rium if not how many equilibria are there algorithms that tackle these questions would be useful both to players and to the mechanism designer furthermore algorithms that answer certain existence questions may pave the way to designing algorithms that con struct a nash equilibrium for example if we had an algo rithm that told us whether there exists any equilibrium where a certain player plays a certain strategy this could be useful in eliminating possibilities in the search for a nash equilibrium however all the existence questions that we have investi gated turn out to be hard these are not the first results of this nature most notably gilboa and zemel provide some hardness results in the same spirit we provide a sin gle reduction which in demonstrates sometimes stronger ver sions of most of their hardness results and interesting new results additionally as we show in section the reduction shows p hardness of counting the number of equilibria definition let  be a boolean formula in conjunctive nor mal form let v be its set of variables with v n l the set of corresponding literals a positive and a negative one for each variable and c its set of clauses the func tion v l v gives the variable corresponding to a lit eral e g v v we define g  to be the following symmetric player game in normal form let  l v c f let the utility functions for all l with l l l l for all l l l x x l for all l l x  l v l l v for all v v l l with v l v v l l v n for all v v l l with v l v v x x v for all v v x  l c l l c for all c c l l with l c c l l c n for all c c l l with l c c x x c for all c c x  l f f f f f x x f for all x  f theorem if ln where v li xi satisfies  then there is a nash equilibrium of g  where both players play li with probability with expected utility for each player the only other nash equilibrium is the one where both players play f and receive expected utility each proof we first demonstrate that these combinations of mixed strategies indeed do constitute nash equilibria if ln where v li xi satisfies  and the other player plays li with probability playing one of these li as well gives utility on the other hand playing the negation of one of these li gives utility n play ing some variable v gives utility n n since one of the l we first need some standard definitions from game theory definition in a normal form game we are given a set of agents a and for each agent i a strategy set i and a utility function ui  a r definition a mixed strategy i for player i is a probability distribution over i a special case of a mixed strategy is a pure strategy where all of the probability mass is on one element of i definition nash given a normal form game a nash equilibrium ne is vector of mixed strategies one for each agent i such that no agent has an incentive to deviate from its mixed strategy given that the others do not deviate that is for any i and any alternative mixed strategy il we have e ui si a e ui sli a where each si is drawn from i that the other player sometimes plays has v li v playing some clause c gives utility at most n n since one of the li that the other player sometimes plays occurs in clause c since the li satisfy  finally playing f gives utility it follows that playing any one of the li that the other player sometimes plays is an optimal response and hence that both players playing each of these li with probability is a nash equilibrium clearly both players playing f is also a nash equilibrium since play ing anything else when the other plays f gives utility now we demonstrate that there are no other nash equilib ria if the other player always plays f the unique best re sponse is to also play f since playing anything else will give utility otherwise given a mixed strategy for the other player consider a player expected utility given that the other player does not play f that is the probability distribution over the other player strategies is proportional to the proba bility distribution constituted by that player mixed strategy i and sli from il if is a variable and are literals we make a now we are ready to present our reduction distinction between the variable and the literal except f occurs with probability if this expected utility is less than the player is strictly better off playing f which gives utility when the other player does not play f and also performs better than the original strategy when the other player does play f so this cannot occur in equilibrium there are no nash equilibria where one player always plays f but the other does not so suppose both players play f with probability less than one consider the expected social welfare e given that neither player plays f it is easily verified that there is no outcome with social welfare greater than also any outcome in which one player plays an element of v or c has social welfare strictly below it follows that if either player ever plays an element of v or c the expected social welfare given that neither player plays f is strictly below by linearity of expectation it follows that the expected utility of at least one player is strictly below given that neither player plays f and by the above reasoning this player would be strictly better off playing f instead of its randomization over strategies other than f it follows that no element of v or c is ever played in a nash equilibrium so we can assume both players only put positive probabil ity on strategies in l f then if the other player puts pos itive probability on f playing f is a strictly better response than any element of l since both give utility if the other player plays an element of l but f does better if the other player plays f it follows that the only equilibrium where f is ever played is the one where both players always play f now we can assume that both players only put positive probability on elements of l suppose that for some l l the probability that a given player plays either l or l is less than then the expected utility for the other player of play hard for any such definition additionally the first kind of equilibrium is in various senses an optimal outcome for the game even if the players were to cooperate so even finding out whether such an optimal equilibrium exists is hard the following corollaries illustrate these points each corollary is immediate from theorem corollary even in symmetric player games it is hard to determine whether there exists a ne with expected standard social welfare e ui at least k even i a when k is the maximum social welfare that could be obtained in the game corollary even in symmetric player games it is hard to determine whether there exists a ne where all players have expected utility at least k even when k is the largest number such that there exists a distribution over outcomes of the game such that all players have expected utility at least k corollary even in symmetric player games it is hard to determine whether there exists a pareto optimal ne a distribution over outcomes is pareto optimal if there is no other distribution over outcomes such that every player has at least equal expected utility and at least one player has strictly greater expected utility corollary even in symmetric player games it is hard to determine whether there exists a ne where player has expected utility at least k some additional interesting corollaries are corollary even in symmetric player games it is np ing n is strictly greater than n hard to determine whether there is more than one nash equi hence this cannot be a nash equilibrium so we can assume that for any l l the probability that a given player plays n if there is an element of l such that player puts posi tive probability on it and player on its negation both play ers have expected utility less than and would be better off switching to f so in a nash equilibrium if player plays l with some probability player must play l with probability and thus player must play l with probability thus corollary even in symmetric player games it is hard to determine whether there is an equilibrium where player sometimes plays x corollary even in symmetric player games it is hard to determine whether there is an equilibrium where player never plays x all of these results indicate that it is hard to obtain sum n n mary information about a game nash equilibria corol we can assume that for each variable exactly one of its cor responding literals is played with probability by both play ers it follows that in any nash equilibrium besides the one where both players play f literals that are sometimes played indeed correspond to an assignment to the variables all that is left to show is that if this assignment does not satisfy  it does not correspond to a nash equilibrium let c c be a clause that is not satisfied by the assignment that is none of its literals are ever played then playing c would give utility and both players would be better off playing this hence there exists a nash equilibrium in g  where each player gets utility if and only if  is satisfiable otherwise lary and versions of corollaries and were first proven by gilboa and zemel counting the number of equilibria in normal form games existence questions do not tell the whole story in general we are interested in characterizing all the equilibria of a game one rather weak such characterization is the number of equi we can use theorem to show that even determining this number in a given normal form game is hard corollary even in symmetric player games counting the number of nash equilibria is p hard each of them gets since any sensible definition of welfare optimization would prefer the first kind of equilibrium it fol lows that determining whether a good equilibrium exists is results prove hardness in a slightly more restricted setting number of equilibria in normal form games has been stud ied both in the worst case and in the average case proof the number of nash equilibria in our game g  is the number of satisfying assignments to the variables of  e i i e ui i sli i a  a where each si  is drawn from i  and sl from l plus one counting the number of satisfying assignments to a i i i i i i cnf formula is p hard it is easy to construct games where there is a continuum of nash equilibria in such games it is more meaningful to ask how many distinct continuums of equilibria there are more formally one can ask how many maximal connected sets of equilibria a game has a maximal connected set is a connected set which is not a proper subset of a connected set corollary even in symmetric player games counting the we can now define the computational problem definition pure strategy bne we are given a bayesian game we are asked whether there exists a bne where all the strategies i i are pure to show our hardness result we will reduce from the set cover problem definition set cover we are given a set sn subsets sm of s with proof every nash equilibrium in g  constitutes a maxi mal connected set by itself so the number of maximal con nected sets is the number of satisfying assignments to the variables of  plus one the most interesting hardness results are the ones where the corresponding existence and search questions are easy such as counting the number of perfect bipartite matchings in the case of nash equilibria the existence question is trivial it has been analytically shown by kakutani fixed point the orem that a nash equilibrium always exists the com plexity of the search question remains open pure strategy bayes nash equilibria equilibria in pure strategies are particularly desirable because they avoid the uncomfortable requirement that players ran domize over strategies among which they are indifferent in normal form games with small numbers of players it is easy to determine the existence of pure strategy equilibria one can simply check for each combination of pure strate gies whether it constitutes a nash equilibrium however this is not feasible in bayesian games where the players have private information about their own preferences represented by types here players may condition their actions on their types so the strategy space of each player is exponential in the number of types in this section we show that the question of whether a pure strategy bayes nash equilibrium exists is in fact hard even in symmetric two player games first we need the standard definition of a bayesian game and bayes nash equi librium from game theory definition in a bayesian game we are given a set of agents a for each agent i a set of types i a commonly known prior distribution  over  a for each agent i a set of strategies i and for each agent i a utility function ui i  a r theorem pure strategy bne is hard even in symmetric player games where  is uniform proof we reduce an arbitrary set cover instance to the following pure strategy bne instance let there be two players with  k  is uniform furthermore  sm sn the utility functions we choose in fact do not depend on the types so we omit the type argument in their definitions they are as follows si sj sj si for all si and sj si sj sj si for all si and sj si si sj sj si for all si and sj si si sj sj si for all si and sj sj si si sj for all si and sj si sj si si sj for all si and sj si we now show the two instances are equivalent first sup pose there exist sck such that i k sci s suppose both players play as follows when their type is i they play sci we claim that this is a bne for suppose the other player employs this strategy then because for any sj there is at least one sci such that sj sci we have that the expected utility of playing sj is at most k it follows that playing any of the sj which gives utility is optimal so there is a pure strategy bne on the other hand suppose that there is a pure strategy bne we first observe that in no pure strategy bne both players play some element of s for some type for if the other player sometimes plays some sj the utility of playing some si is at most k whereas playing some si instead guarantees a utility of at least so there is at least one player who never plays any element of s now suppose the other player sometimes plays some sj we know there is some si such that sj si if the former player plays this si this will give it a utility of at least k definition harsanyi given a bayesian game a bayes nash equilibrium bne is a vector of mixed strate gies one for each pair i i i such that no agent has an incentive to deviate for any of its types given that the others do not deviate that is for any i i i and any alternative l since it must do at least this well in the equilibrium and it never plays elements of s it must sometimes receive utility it follows that there exist sa and sb sa such that the former player sometimes plays sa and the latter sometimes plays sb but then playing sb gives the latter player a utility of at most k and it would be better off i e i i e ui i si i a  a playing some si instead contradiction it follows that in no pure strategy bne any element of s is ever played now in our given pure strategy equilibrium consider the set of all the si that are played by player for some type clearly there can be at most k such sets we claim they cover for if they do not cover some element the expected off to player i in state where the players play actions a a a discount factor  such that the total utility of agent s j i is ku sk ak ak where sk is the state never plays any element of s either is not playing optimally contradiction hence there exists a set cover ak ak a in stage k if one allows for general mixed strategies a bayes nash equilibrium always exists however the question of how efficiently one can be constructed remains open pure strategy nash equilibria in stochastic markov games we now shift our attention from single shot games to games with multiple stages in each stage the players get to act and obtain payoffs there has already been some research into the complexity of playing repeated and sequential games for ex ample determining whether a particular automaton is a best response is complete it is complete to compute a best response automaton when the automata under consid eration are bounded the question of whether a given player with imperfect recall can guarantee itself a given pay off using pure strategies is complete and in general best responding to an arbitrary strategy can even be noncom putable in this section we present to our knowledge the first pspace hardness result on the existence of a pure in general a player need not always be aware of the cur rent state of the game the actions the others played in pre vious stages or the payoffs that the player has accumulated in the extreme case players never find out any of these and are hence playing blindly we call such a markov game in visible it is relatively easy to specify a pure strategy in an invisible markov game because there is nothing to condition on hence such a strategy is simply an infinite sequence of actions for player i a sequence ak where it plays action ak in stage k regardless in spite of this apparent simplicity of the game we show that determining whether pure strategy equilibria exist is extremely hard definition pure strategy invisible markov ne we are given an invisible markov game we are asked whether there exists a nash equilibrium where all the strategies are pure we show that this problem is pspace hard by reducing from periodic sat which is pspace complete definition periodic sat we are given a cnf for mula  over the variables let a multi stage game is typically represented as a stochastic markov game where there is an underlying set of states and  k be the same formula except that all the superscripts are incremented by k we are asked whether there exists a boolean assignment to the variables xk xk the players actions but also on the state furthermore the probability of transitioning to a given state is determined by the current state and the players current actions hardness results for such games cannot be obtained simply by formu lating a known hard game such as generalized go or qsat as a markov game because such a formulation would have to specify an exponential number of states even if the number of states is polynomial one might suspect hard ness because the strategy spaces are extremely rich however in this section we show hardness even in a variant where the strategy spaces are simple in the sense that the players cannot condition their actions on events in the game definition a stochastic markov game consists of theorem pure strategy invisible markov ne is hard even when the game is symmetric player and the transition process is deterministic proof we reduce an arbitrary periodic sat instance to the following symmetric player pure strategy invisible markov ne instance the state space is s si i n i c c i c c r where c is the set of clauses in  furthermore  t f c the transition probabilities are p si si modn for i n and all  p for all t f a set of players a a set of states s among which the game transits p c b p b c for all b t f and c c for all b t f and c c for each player i a set of actions i that can be played p r for all c in any state a transition probability function p s p ti c t for all i j c c and   a s where p a a gives the probability of the game being in state in the next p c r for all j c c and stage given that the current state of the game is and the players play actions a a for each player i a payoff function ui s  a r where ui a a gives the pay  do not need to worry about issues of credible threats and subgame perfection in this setting so we can simply use nash equi librium as our solution concept p r r for all  l k n i c and setting xl k to b satisfies c the discount some of the utilities obtained in a given stage are as follows we do not specify utilities irrelevant to our analysis si si for i n and all  u u for all ing is insignificant enough that this more than cancels out the earned in stage kn player will get at most in the other stages up to the first stage in state r and given that we made the payoffs in the game in state r sufficiently small relative to  player will not earn enough in the remaining t f c b b c for all b t f and c c when setting variable to b does not satisfy c c b b c for all b t f and c c when setting variable to b does satisfy c for all c2 c to deviate thus a pure strategy ne exists on the other hand suppose that no assignment satisfying the periodic sat formula exists let us investigate whether a nash equilibrium could exist we know that in such a nash equilibrium we never leave the si so both players receive utility and no c is ever played in a stage with state x b b x for k since playing a c in one of the other stages can have no kn i c i n all kn i c c c and b t f such that setting deterrent value we may suppose that only elements of t f variable xk to b does not satisfy c and all x  are played now consider the following assignment to the i xk if player plays b in stage kn i xk is set to b since tkn i c x b tkn i c b x for k i i i n all c c and b t f such that setting variable xk to b does satisfy c and all x  no assignment satisfying the periodic sat formula exists we know there is some clause c and some k such that no variable xl among xk xk xk xk is set to some u x cl u cl x for k i n n i n all c cl c and all x  i deviates to play this c in stage kn it will receive payoff additionally the game played in state r is some symmet ric zero sum game without a pure strategy equilibrium for example a generalization of rock paper scissors with very in this stage and payoff in all the remaining stages up to the first stage in state r furthermore player can guarantee itself at least payoff in each stage in state r as this state small payoffs finally the discount factor is  so corresponds to a zero sum symmetric game it follows that that this deviation gives player positive utility and is hence beneficial thus no pure strategy ne exists we start our analysis with a few observations first there can be no pure strategy equilibrium in which state r is reached at some point because since r is an absorbing state this would require that some pure strategy equilibrium of the game in state r were played whenever state r occurred oth erwise a player who is not best responding in one of these stages could simply switch to a best response in this stage and because the game is invisible the rest of the game would remain unaffected so this would give higher utility but such an equilibrium does not exist second if we ever reach one of the tj states we will inevitably reach state r at some point after this it follows that all pure strategy nash equilibria never leave the si states now suppose an assignment satisfying the periodic sat formula exists let both players play as follows in stage kn i with i n b t f is played where b is the value that the variable xk is set to clearly both players receive utility with these strategies does either player have an incentive to deviate the only deviation of any signifi cance is to play some c c when the current state is so without loss of generality because of the symmetry of the game say player deviates to playing c c in stage kn when the state is we know that in the satisfying assign ment some variable xl among xk xk xk xk is set to some b such that setting xl k to b satisfies c if it is xk a simpler version of the same argument shows a weaker form of hardness for the case where the game is restricted to have only finitely many stages we omit the proof due to limited space theorem pure strategy invisible markov ne is hard even when the game is symmetric player the transition process is deterministic and the number of stages in the game is finite conclusions and future research noncooperative game theory provides a normative frame work for analyzing strategic interactions however for the toolbox to be operational the solutions it defines will have to be computed in this paper we provided a single reduction that demonstrates hardness of determining whether nash equilibria with certain natural properties exist and demonstrates the hardness of counting nash equilibria or connected sets of nash equilibria we also showed that determining whether a pure strategy bayes nash equilibrium exists is hard and that determining whether a pure strategy nash equilibrium exists in a stochastic markov game is hard even in invisible games and hard if the game is finite all of our hardness results hold i which is set to b then in stage kn player plays b and player gets payoff in this stage since we are in state and setting to b satisfies c otherwise if it is xl with even if there are only two players and the game is symmetric there are numerous open research questions in computing solutions to noncooperative games some recent work has fo i l k or i which is set to b then player will get payoff in stage kn but in stage ln i player plays b and player gets payoff in this stage since we are in state cused on novel knowledge representations which in certain settings can drastically speed up equilibrium finding e g 6 8 one avenue of future work includes identifying re perfect bayesian equilibrium in sender receiver games introduction sender receiver games strategies in sender receiver games sender best response strategies receiver best response strategies updating the receiver beliefs message wise optimization bayesian equilibrium perfect bayesian equilibrium the test of dominated messages introduction we previously studied static games of imperfect information each player i i has private information which was summarized by her type i i each player knows her own type but does not in general know the types of her opponents each player i beliefs about the types  i  i of her opponents are derived from her knowledge of her own type i and a common prior belief p  over the space of type profiles nature moves first picking a type profile   according to the probability distribution p  nature then privately informed each player i i of her type player is type player is type player n is type n then each player i i of the n players simultaneously chooses an action ai ai from her action space a payoff uia  is then awarded to each player which depends on the action profile a a the players chose and the type profile  nature chose it was because the players simultaneously chose their actions that we called these games static now we want to generalize our analysis by considering dynamic games of incomplete information i e we consider games in which some players take actions before others and these actions are observed to some extent by some other players sender receiver games we consider here the simplest dynamic games of incomplete information sender receiver games there by jim ratliff see the static games of incomplete information handout are only two players a sender s and a receiver r the sender action will be to send a message m m chosen from a message space m to the receiver the receiver will observe this message m and respond to it by choosing an action a a from his action space a to make this game a simple but nontrivial game of incomplete information we endow the sender with some private information which we describe by her type   the receiver has no private information so he has but a single type which we then have no need to mention further the receiver does have prior beliefs i e prior to observing the sender message about the sender type which are described by the probability distribution p  over the sender type space  in other words before observing the sender message the receiver believes that the probability that the sender is some particular type   is p we will typically assume that the type space  the message space m and the receiver action space a are finite sets after the receiver takes an action a a each player is awarded a payoff which can in general depend on the message m the sender sent the action a the receiver took in response and the type  which nature chose for the sender the payoffs to the sender and receiver to a message action type triple m a  m a  are um a  and vm a  respectively i e u v m a   we can express this game of incomplete information as an extensive form game of imperfect information by explicitly representing nature who chooses a type   for the sender because the sender observes this choice of nature every sender information set is a singleton and the number of sender information sets is equal to the number of possible sender types viz  the receiver observes only the message sent by the sender therefore the number of receiver information sets is equal to the number of possible messages the sender can transmit viz m within each of his information sets the receiver cannot distinguish between the sender possible types so each receiver information set has a number of nodes equal to the number of possible sender types viz  therefore the total number of receiver nodes is the product of the cardinalities of the message and type spaces viz m  of course in this case the message seems more than a mere message the terms message for the sender and action for the receiver both refer to actions taken by a player the distinction between the two is only interpretational we use message for the sender action to acknowledge that the sender realizes that the receiver will respond to the sender action and therefore the sender can attempt to influence the receiver response through her choice of message dull therefore her type space would be  bright dull the receiver prior beliefs concerning the probability that the sender is bright or dull can be described by a single number with probability the sender is bright with probability she is dull i e the receiver prior beliefs p  are defined by pbright and pdull consider the simple sender receiver game shown in figure note that the sender has two information sets corresponding to her two types viz bright and dull the receiver also has two information sets but these correspond to the sender two possible messages viz beach and college rather than to the sender possible types the receiver left hand information set is his beach information set and his right hand information set is his college information set bright b s n c r h r h b s c r r dull figure a simple sender receiver game let interpret the payoffs shown in figure the first and second payoffs of each ordered pair are the sender and receiver payoffs respectively for a particular type message action triple for a fixed type and receiver action the sender payoff to going to the beach is always two greater than her payoff to going to college for fixed educational and employment decisions the sender payoff is independent of her type for a fixed type and educational decision the sender receives a payoff from being hired which is greater than her payoff if she is rejected to summarize the sender payoffs with the appropriate ceteris paribus qualifications the sender prefers the beach over going to college prefers being hired over being rejected and is not discriminated against due to aptitude whenever the receiver rejects an applicant the receiver gets a payoff of zero although the sender aptitude did not directly influence the sender payoffs aptitude is payoff relevant to the receiver when he hires for a fixed educational decision the receiver payoff to hiring is greater for example if the bright applicant is hired she receives a payoff of from the beach but only from college if the dull applicant is rejected she receives a payoff of from the beach but only from college for example if the applicant goes to the beach and is hired she receives a payoff of regardless of whether she is bright or dull for example if the bright applicant goes to college she receives if she is hired and only if she is rejected if the dull applicant goes to the beach she receives if she is hired and only if she is rejected strategies in sender receiver games a pure strategy for a player in any extensive form game is a mapping from her information sets to her available actions at the relevant information set there is a one to one correspondence between the sender information sets and her type space  therefore a pure strategy for the sender is a map m  m from her type space  to her message space m there is a one to one correspondence between the receiver information sets and the sender message space therefore a pure strategy for the receiver is a mapping a m a from the sender message space m to the receiver action space a we can also define behavior strategies for the players the sender can send mixed messages let mfim be the set of probability distributions over the sender message space m a behavior strategy for the sender is a map   m from her type space  to mixtures over her message space therefore for all types    m is a mixture over messages in particular for any message m m we denote by m  the probability according to the sender behavior strategy  that a type  sender will send the message m for a given sender strategy  a message m is on the path if according to  there exists a type  who sends m with positive probability the set of on the path messages for sender strategy  is the receiver can also randomize his actions in response to his message observation let afia be the set of probability distributions over the receiver action space then a behavior strategy for the receiver is a map m a from the sender message space m to the mixed action space a therefore for all messages m m m a is a mixture over receiver actions in particular for any action a a we denote by a m the probability according to the receiver behavior strategy that the receiver will choose the action a conditional on having observed the message m m if the sender goes to college the receiver payoff to hire is when the sender is bright but only when the sender is dull if the sender goes to the beach the receiver payoff to hire is when the sender is bright and when she is dull for example if the receiver hires the dull sender the receiver gains a payoff of if the sender went to college compared to a payoff of if the sender had gone to the beach instead note that the support of  is the set of messages which a type  sender sends with positive probability when she is playing according to the behavior strategy  the symbols  and were selected for these behavior strategies to be mnemonically friendly i e in the hope that sigma and rho would suggest sender and receiver respectively sender best response strategies we first ask when is a sender strategy m m a best response to some receiver behavior strategy am consider the case where a type  sender chooses to send a message m m knowing that the receiver will respond according to his behavior strategy am this sender expected utility will be a convex combination of her payoffs to particular pure actions by the receiver viz a a a mum a  a pure strategy m m will be a best response for the sender to a receiver behavior strategy am if for every type   of sender the message specified by m maximizes the expected utility of a type  sender given that the receiver will respond to the message m according to the strategy for a given receiver mixed strategy am the set of optimal messages for a type  sender is m fiarg max a a a m um a  therefore a sender strategy m m is a best response to the receiver strategy am if and only if for all   m m  a sender behavior strategy  m is a best response to the receiver behavior strategy am if and only if for all   supp m  receiver best response strategies now we ask when is a receiver strategy a am a best response to a sender behavior strategy  m updating the receiver beliefs the receiver chooses an action after he observes the sender message he wants to choose the action which is optimal given the best beliefs he can have concerning the sender type the receiver enters the game with prior beliefs p  concerning the sender type  because the receiver knows the sender type contingent message sending strategy  m the receiver might be able to infer something more about the sender type and thereby update his beliefs as long as the observed message is not totally unexpected given that the sender is playing the behavior strategy  i e there is some type which according to  sends that message with positive probability we can use bayes rule to update the receiver prior beliefs p  specifically for any observed on the path message m m we denote the receiver posterior belief that the sender is type  by pb m which is given from bayes rule by pb mfi p m    p  m   we see the justification for the restriction to on the path not totally unexpected messages if some for any sets a and b ab is the set of all functions from b a the numerator of the right hand side is the probability of the event the sender is type  and sends message m the denominator is the probability that message m is sent message m is never sent regardless of which type the sender is then the denominator of the right hand side will vanish in general we can define the receiver posterior beliefs even after observing off the path and therefore totally unexpected messages for every message m m we let pm  be the receiver posterior beliefs after observing the message m about the sender type in other words the receiver attaches the probability p m to the event that the sender has type   conditional upon the receiver observing the message m m so p m  is a conditional posterior belief system where does the receiver conditional posterior belief system p m  come from it is derived from the receiver prior beliefs p  and updated in response to his observation of the sender message m we require that this updating be done according to bayes rule whenever possible this means that for all on the path messages m m and for all types   pm  pbm  alternatively but equivalently we can say that the conditional posterior belief system p m  is consistent with bayes rule if the restriction of p to the on the path messages m is pb message wise optimization consider a receiver pure strategy a am if a type  sender sends the message m m and the receiver responds according to his pure strategy a the receiver payoff will be vm am  the probability with which he receives this particular payoff is the probability of the event the sender is type  and sends message m this probability is the probability that the sender is type  viz p multiplied by the probability that the sender sends the message m conditional on the sender being type  viz m  therefore the expected utility va  to the receiver who plays the strategy a am against the sender behavior strategy  m is the sum of the probability weighted payoffs pm vm am  over all possible combinations of messages and types va fim m   pm vm am  a receiver strategy a am will be a best response to the sender behavior strategy  m if and only if it maximizes the receiver expected utility over all possible receiver pure strategies i e a arg max va  a am at first glance the optimization problem in might appear problematic because it requires maximization over a function space fortunately the maximand from is additively separable in the various messages m m so we ll be able to construct a best response receiver strategy a am via a message by message optimization to find individual best response actions am for each message m m this simplification is justified by the following lemma which you are invited to prove for yourself let a be a set and m be a finite set let f be a function f m a  then a arg max fm am a am if and only if for all m m am arg max fm a a a to apply this lemma to the optimization problem we define fm afi  pm vm a  now we have from va fim m fm am therefore from 9 and the lemma we see that the receiver strategy a am is a best response to the sender behavior strategy  m if and only if for all m m am arg max fm a a a if a message m is off the path i e m m m then it is sent by no type for every type   m therefore m m m a a fm therefore all actions a a are maximizers of fm a when m is an off the path message i e m m m a arg max fm a a a when m m is an on the path message it is useful to divide the maximand of by the guaranteed nonzero probability that m is sent viz  p m   this does not change the set of maximizers of this division allows us using and to express the condition in terms of the receiver bayes updated beliefs m m am arg max pb mvm a  a a   but this maximand is simply the receiver expected utility given her bayes updated beliefs about the sender type when she chooses the action a a after observing the on the path message m m therefore condition states that it is necessary and sufficient in order that the receiver strategy a am to be a best response to the sender behavior strategy  m that it specify for each on the path message an action which is a best response to that message given the receiver bayes updated beliefs for a given conditional posterior belief system p m  the receiver expected utility to the action a a conditional upon having observed the message m m is  p mvm a  therefore for a given conditional posterior belief system p the set of receiver best response actions to some message m is given by ap m arg max   p m vm a  a receiver pure strategy a am is a best response to the sender behavior strategy  m if and only if for all m m am apb m a receiver behavior strategy am is a best response to the sender behavior strategy  m if and only if for all m m supp mapb m bayesian equilibrium a bayesian equilibrium of the sender receiver game is a triple  p m am  m satisfying the following three conditions for all types   supp m  for all on the path messages m m supp map m the conditional posterior belief system p is consistent with bayes rule whenever possible in the sense that the restriction of p to the on the path messages m is pb note that optimality from the receiver is required only at on the path information sets therefore the only receiver information sets at which the specification of the conditional posterior belief system p enters into the definition of bayesian equilibrium is at on the path message information sets where these beliefs are just the ones derived from bayes rule from h bright b s n t h c r r h r b s c r r r dull figure life is a beach with the receiver conditional posterior beliefs indicated we can represent a strategy profile by the ordered sextuple x y l r t where x sender action if bright y sender action if dull l receiver action if beach is observed r receiver action if college is observed receiver belief probability given that beach is observed that the sender is bright t receiver belief probability given that college is observed that the sender is bright consider the following strategy profile c c r h this strategy profile is depicted in figure by the thick line segments we note that according to this strategy profile the beach message is never sent by any type of sender and is therefore off the path therefore in order to evaluate whether this strategy profile is a bayesian equilibrium we need not specify conditional posterior beliefs for the receiver at this information set hence the in the above specification let verify that this strategy profile is a bayesian equilibrium of this game first we check whether any type of sender wishes to deviate away from going to college in favor of going to the beach instead given the hiring policies of the receiver each type of sender receives a payoff of from conforming to college each would receive a lower payoff of instead if she went to the beach therefore neither type of sender would deviate to check whether the receiver would prefer to change his hiring policy given the sender type contingent strategy we need only check the only on the path information set viz the college information set the easy way to see that hiring is optimal at the college information set is to notice that i e both types of sender go to college the receiver rejects any beachgoers and hires any college graduates if the receiver observes college he believes that the probability is that the sender is bright hiring is better for the receiver than rejecting for each type of sender separately therefore regardless of the receiver belief t the corresponding convex combination of hiring payoffs will exceed the zero he would get if he rejects more formally for any receiver beliefs that the sender is bright conditional on observing college the receiver expected payoff to hiring at the college information set is therefore for any the specified strategy profile is a bayesian equilibrium the specification of posterior beliefs at the college information set viz t implies that even after observing the sender message the receiver beliefs about the sender type are unchanged from her prior beliefs this no updating result occurs because this is a pooling strategy profile i e all types of the sender send the same message we can also use to see formally that this specification t is consistent with bayes rule this is the last step in verifying that the strategy profile is a bayesian equilibrium letting m college and  bright pbbright college pbright college bright pbright college bright pdull college dull i e pbright college t pbbright college exactly as required by condition for bayesian equilibrium now let look at another strategy profile b b r r this strategy profile is indicated below in figure each type of sender is sending the optimal message given the receiver hiring policy by choosing beach because each type of sender will be rejected whatever message she sends she ll choose the most pleasant message viz go to the beach to check the optimality of the receiver hiring plans we need to check only the single on the path information set viz beach uneducated senders aren t worth hiring so rejection at this information set is optimal for the receiver you can also verify similarly to the demonstration for the strategy profile of figure that the specification is consistent with bayes rule h bright b s n t h c r r h r b s c r r r 1_ dull figure a less credible bayesian equilibrium however note why the above strategy profile specification for the sender is a best response to the perfect bayesian equilibrium we saw in the above example that the strategy profile depicted in figure was a bayesian equilibrium of the game but was suspect because it relied on a nonoptimal action by the receiver at an off the path receiver information set we can eliminate this strategy profile by a simple strengthening of our solution concept a perfect bayesian equilibrium of the sender receiver game is a triple  p m am  m satisfying the following three conditions for all types   supp m  for all messages m m supp map m the conditional posterior belief system p is consistent with bayes rule whenever possible in the sense that the restriction of p to the on the path messages m is pb note that the only difference between this definition of perfect bayesian and the earlier definition of bayesian equilibrium is in the strengthening of the original receiver optimality condition which imposed optimality only at on the path message information sets resulting in which requires optimality of the receiver strategy at all message information sets note from that this also implies that now the receiver posterior beliefs are important even at off the path message information sets however we aren t constrained by bayes rule in the specification of these off the path beliefs the strategy profile from figure would fail to be a perfect bayesian equilibrium regardless of how we specified t 1 because as we saw in the analysis of the example of figure for any beliefs hiring is better for the receiver at the college information set is better than rejecting there also note that if all messages are on the path then if the strategy profile is a bayesian equilibrium it is also a perfect bayesian equilibrium example perfect bayesian equilibria can still be undesirable consider the same basic game we ve been considering but with the different payoffs shown in figure note that for a fixed hiring decision each type of sender prefers going to beach over going to college but the bright sender finds college less onerous than the dull sender does in fact this difference is extreme in the following sense a bright sender is willing to incur the cost of college if it means that it makes the difference between being hired and being rejected however the dull sender finds college such a drag that she unwilling to skip the beach regardless of the effect her action has on the hiring decision of the receiver for a fixed education decision the receiver prefers to hire the bright sender but prefers to reject the dull sender for a fixed type of sender the receiver is indifferent between hiring a college educated vs a beach tanned sender note that with this payoff structure education is unproductive but because going to college has a higher cost for the lower ability type of sender education might provide a costly signal of the sender type to the receiver figure education is unproductive but an effective signal of ability consider the strategy profile c b r h 1 this is not only a bayesian equilibrium but also a perfect bayesian equilibrium because every message is on the path this is a separating equilibrium because each type of sender chooses a different action when each type of sender sends a distinct message the receiver can deduce with certainty the identity of the sender from her observed message you can use 3 to verify that the posterior belief assignments s and t 1 are those determined by bayes rule let 2 consider the strategy profile b b r r t where t 2 this is a pooling strategy profile this is a perfect bayesian equilibrium the off the path posterior beliefs imply that if a defection to college is observed the defector is more likely to be dull than bright however such off the path beliefs are objectionable for the following reason no matter what influence a deviation to if going to the beach implies that the bright sender will be rejected then going to the beach implies a payoff of zero if going to college is necessary to be hired then college implies a payoff of 1 therefore the bright sender will go to college if that is necessary for being hired the test of dominated messages we saw in the above example that the pooling perfect bayesian equilibrium profile was undesirable because it relied on the receiver interpreting a deviation as coming from a type who would never find it optimal to deviate the college message was dominated for the dull type in the following sense no matter how badly for the sender the receiver might respond to the prescribed message beach and no matter how favorably for the sender the receiver might respond to the deviation message college the dull sender would still prefer to send the prescribed message denote the set of receiver actions which are best responses conditional on the message m for some conditional posterior beliefs by am p  ap m m a sender who sends the message m m would never have to worry about a receiver response which fell outside of the set am because such an action would not be a best response by the receiver to any posterior belief she could possibly hold message m m is dominated for type   if there exists a message m m such that min a am  um a  max um a  a am let fi  p be a perfect bayesian equilibrium the equilibrium fails the test of dominated messages if there exist types    and an off the equilibrium path message m m m such the receiver puts positive weight conditional on m being observed that the message was sent by type  i e p m m is dominated for type  and m is not dominated for type  before we can reject an equilibrium because it puts positive weight on a deviant message originating it is more common to allow m to be any message inm the test stated here is equivalent because no on the equilibrium path message m could possibly be dominated for a type  for whom p m 0 because along the equilibrium path p is derived by bayes rule i e this would imply that m   0 and thus that in equilibrium type  were sending a dominated message the statement given here simplifies the proof of the theorem to come from a type for whom the message is dominated we must be able to identify a type of sender for whom this message is not dominated otherwise this logic would force us to put zero weight on all types at this information set and this would not be a legitimate conditional probability distribution we see that the pooling perfect bayesian equilibrium fail the test of dominated messages example the separating equilibrium disappears and the pooling becomes reasonable consider the example in figure now college though more costly for the dull than for the bright sender is not as costly for the dull sender as it was in the example of figure going to college is no longer dominated for the dull sender she would be willing to go to college if that made the difference between being hired and being rejected figure pooling is now reasonable and separation is not the separating strategy profile c b r h 0 1 which was a perfect bayesian equilibrium in the game of figure is not an equilibrium of the present game because the dull senders would now deviate to going to college the pooling equilibrium b b r r t where t 0 2 of figure 4 is not only still a perfect bayesian equilibrium in this game it is no longer rejected by the test of dominated messages figure the test of dominated messages is not strong enough consider the following equilibrium b b h r t for t 0 2 i e if a deviation to college is observed it is more likely that the deviator is a dull sender this equilibrium passes the test of dominated messages because the dull sender could do worse by going to beach getting a zero than by the most optimistic hopes for going to college where she could get a 1 however the bright sender could hope to gain by deviation relative to his equilibrium potential but the dull type cannot hope this therefore we shouldn t attribute positive probability to the dull sender deviating let fi  p be a perfect bayesian equilibrium let u be the type  sender s expected payoff in this equilibrium message m m is equilibrium dominated with respect to for type   if u max a am um a  i quickly verify that domination implies equilibrium domination if m m is dominated for type   then for every perfect bayesian equilibrium m is equilibrium dominated with respect to for type  let m m be a message which dominates m for type  for any equilibrium receiver strategy the sender s expected payoff to the message m is a m  um a  min um a  a am  a am  which is derived from and ap m am  for any m supp  u a a a m  um a  assume that m is not equilibrium dominated with respect to the equilibrium then from the converse of and a a m  a m  um a  a a a m  um a  therefore m m  note in 2 that a m  0 for m a am  which contradicts  let fi  p be a perfect bayesian equilibrium the equilibrium fails the refinement i the intuitive criterion if there exist types    and an off the equilibrium path message m m m such the receiver puts positive weight conditional on m being observed that the message was sent by type  i e p m 0 m is equilibrium dominated with respect to for type  and m is not equilibrium dominated with respect to for type  it is often asserted or at least strongly suggested that i is an equilibrium refinement of d however a perfect bayesian equilibrium strategy profile can pass the intuitive criterion yet fail the test of dominated messages yet if a perfect bayesian equilibrium survives the intuitive criterion then there exists a perfect bayesian equilibrium which yields the same outcome i e probability distribution over terminal nodes and which survives both the test of dominated messages and the intuitive criterion see ratliff reference cho in koo and david m kreps signaling games and stable equilibria quarterly journal of economics 2 may fudenberg drew and jean tirole noncooperative game theory for industrial organization an introduction and overview in handbook of industrial organization eds richard schmalensee and robert d willig vol 1 north holland pp fudenberg drew and jean tirole game theory mit press gibbons robert game theory for applied economists princeton university press kreps david m a course in microeconomic theory princeton university press ratliff james d a note on the test of dominated messages and the intuitive criterion in as in the definition of refinement d the restriction of m to off the equilibrium path messages is without loss of generality for more on these refinements see cho and kreps and kreps for example fudenberg and tirole say the idea is roughly to extend the elimination of weakly dominated strategies to strategies which are dominated relative to equilibrium payoffs so doing eliminates more strategies and thus refines the equilibrium concept further fudenberg and tirole suggest that replacing the equilibrium path by its payoff results in an equilibrium refinement whose rejection requirements are weaker and easier to apply kreps says that the intuitive criterion is a stronger test than the test of dominated messages gibbons says that because equilibrium dominance is easier to satisfy than dominance the intuitive criterion makes the test of dominated messages redundant sender receiver games mimeo chapter networks in their surrounding contexts in chapter we considered some of the typical structures that characterize social net works and some of the typical processes that affect the formation of links in the network our discussion there focused primarily on the network as an object of study in itself relatively independent of the broader world in which it exists however the contexts in which a social network is embedded will generally have signif icant effects on its structure each individual in a social network has a distinctive set of personal characteristics and similarities and compatibilities among two people characteris tics can strongly influence whether a link forms between them each individual also engages in a set of behaviors and activities that can shape the formation of links within the network these considerations suggest what we mean by a network surrounding contexts factors that exist outside the nodes and edges of a network but which nonetheless affect how the network structure evolves in this chapter we consider how such effects operate and what they imply about the structure of social networks among other observations we will find that the surrounding contexts affecting a network formation can to some extent be viewed in network terms as well and by expanding the network to represent the contexts together with the individuals we will see in fact that several different processes of network formation can be described in a common framework d easley and j kleinberg networks crowds and markets reasoning about a highly connected world cambridge university press draft version june homophily one of the most basic notions governing the structure of social networks is homophily the principle that we tend to be similar to our friends typically your friends don t look like a random sample of the underlying population viewed collectively your friends are generally similar to you along racial and ethnic dimensions they are similar in age and they are also similar in characteristics that are more or less mutable including the places they live their occupations their levels of affluence and their interests beliefs and opinions clearly most of us have specific friendships that cross all these boundaries but in aggregate the pervasive fact is that links in a social network tend to connect people who are similar to one another this observation has a long history as mcpherson smith lovin and cook note in their extensive review of research on homophily the underlying idea can be found in writings of plato similarity begets friendship and aristotle people love those who are like themselves as well as in proverbs such as birds of a feather flock together its role in modern sociological research was catalyzed in large part by influential work of lazarsfeld and merton in the homophily provides us with a first fundamental illustration of how a network sur rounding contexts can drive the formation of its links consider the basic contrast between a friendship that forms because two people are introduced through a common friend and a friendship that forms because two people attend the same school or work for the same company in the first case a new link is added for reasons that are intrinsic to the network itself we need not look beyond the network to understand where the link came from in the second case the new link arises for an equally natural reason but one that makes sense only when we look at the contextual factors beyond the network at some of the social environments in this case schools and companies to which the nodes belong often when we look at a network such contexts capture some of the dominant fea tures of its overall structure figure for example depicts the social network within a particular town middle school and high school encompassing grades in this image produced by the study author james moody students of different races are drawn as differently colored circles two dominant divisions within the network are apparent one division is based on race from left to right in the figure the other based on age and school attendance separates students in the middle school from those in the high school from top to bottom in the figure there are many other structural details in this network but the effects of these two contexts stand out when the network is viewed at a global level of course there are strong interactions between intrinsic and contextual effects on the formation of any single link they are both operating concurrently in the same network for example the principle of triadic closure that triangles in the network tend to close as links form between friends of friends is supported by a range of mechanisms that range from the intrinsic to the contextual in chapter we motivated triadic closure by figure homophily can produce a division of a social network into densely connected homogeneous parts that are weakly connected to each other in this social network from a town middle school and high school two such divisions in the network are apparent one based on race with students of different races drawn as differently colored circles and the other based on friendships in the middle and high schools respectively hypothesizing intrinsic mechanisms when individuals b and c have a common friend a then there are increased opportunities and sources of trust on which to base their interactions and a will also have incentives to facilitate their friendship however social contexts also provide natural bases for triadic closure since we know that a b and a c friendships already exist the principle of homophily suggests that b and c are each likely to be similar to a in a number of dimensions and hence quite possibly similar to each other as well as a result based purely on this similarity there is an elevated chance that a b c friendship will form and this is true even if neither of them is aware that the other one knows a the point isn t that any one basis for triadic closure is the correct one rather as we take into account more and more of the factors that drive the formation of links in a social figure using a numerical measure one can determine whether small networks such as this one with nodes divided into two types exhibit homophily network it inevitably becomes difficult to attribute any individual link to a single factor and ultimately one expects most links to in fact arise from a combination of several factors partly due to the effect of other nodes in the network and partly due to the surrounding contexts measuring homophily when we see striking divisions within a network like the one in figure it is important to ask whether they are genuinely present in the network itself and not simply an artifact of how it is drawn to make this question concrete we need to formulate it more precisely given a particular characteristic of interest like race or age is there a simple test we can apply to a network in order to estimate whether it exhibits homophily according to this characteristic since the example in figure is too large to inspect by hand let consider this question on a smaller example where we can develop some intuition let suppose in particular that we have the friendship network of an elementary school classroom and we suspect that it exhibits homophily by gender boys tend to be friends with boys and girls tend to be friends with girls for example the graph in figure shows the friendship network of a small hypothetical classroom in which the three shaded nodes are girls and the six unshaded nodes are boys if there were no cross gender edges at all then the question of homophily would be easy to resolve it would be present in an extreme sense but we expect that homophily should be a more subtle effect that is visible mainly in aggregate as it is for example in the real data from figure is the picture in figure consistent with homophily there is a natural numerical measure of homophily that we can use to address questions like this to motivate the measure using the example of gender as in figure we first ask the following question what would it mean for a network not to exhibit ho mophily by gender it would mean that the proportion of male and female friends a person has looks like the background male female distribution in the full population here a closely related formulation of this no homophily definition that is a bit easier to analyze if we were to randomly assign each node a gender according to the gender balance in the real network then the number of cross gender edges should not change significantly relative to what we see in the real network that is in a network with no homophily friendships are being formed as though there were random mixing across the given characteristic thus suppose we have a network in which a p fraction of all individuals are male and a q fraction of all individuals are female consider a given edge in this network if we independently assign each node the gender male with probability p and the gender female with probability q then both ends of the edge will be male with probability and both ends will be female with probability on the other hand if the first end of the edge is male and the second end is female or vice versa then we have a cross gender edge so this happens with probability so we can summarize the test for homophily according to gender as follows homophily test if the fraction of cross gender edges is significantly less than then there is evidence for homophily in figure for example of the edges in the graph are cross gender since p and q in this example we should be comparing the fraction of cross gender edges to the quantity in other words with no homophily one should expect to see cross gender edges rather than than and so this example shows some evidence of homophily there are a few points to note here first the number of cross gender edges in a random assignment of genders will deviate some amount from its expected value of and so to perform the test in practice one needs a working definition of significantly less than standard measures of statistical significance quantifying the significance of a deviation below a mean can be used for this purpose second it also easily possible for a network to have a fraction of cross gender edges that is significantly more than in such a case we say that the network exhibits inverse homophily the network of romantic relationships in figure from chapter is a clear example of this almost all the relationships reported by the high school students in the study involved opposite sex partners rather than same sex partners so almost all the edges are cross gender finally it easy to extend our homophily test to any underlying characteristic race ethnicity age native language political orientation and so forth when the characteristic can only take two possible values say one voting preference in a two candidate election then we can draw a direct analogy to the case of two genders and use the same formula when the characteristic can take on more than two possible values we still perform a general version of the same calculation for this we say that an edge is heterogeneous if it connects two nodes that are different according to the characteristic in question we then ask how the number of heterogeneous edges compares to what we d see if we were to randomly assign values for the characteristic to all nodes in the network using the proportions from the real data as probabilities in this way even a network in which the nodes are classified into many groups can be tested for homophily using the same underlying comparison to a baseline of random mixing mechanisms underlying homophily selection and social influence the fact that people tend to have links to others who are similar to them is a statement about the structure of social networks on its own it does not propose an underlying mechanism by which ties among similar people are preferentially formed in the case of immutable characteristics such as race or ethnicity the tendency of people to form friendships with others who are like them is often termed selection in that people are selecting friends with similar characteristics selection may operate at several different scales and with different levels of intentionality in a small group when people choose friends who are most similar from among a clearly delineated pool of potential contacts there is clearly active choice going on in other cases and at more global levels selection can be more implicit for example when people live in neighborhoods attend schools or work for companies that are relatively homogeneous compared to the population at large the social environment is already favoring opportunities to form friendships with others like oneself for this discussion we will refer to all these effects cumulatively as selection when we consider how immutable characteristics interact with network formation the order of events is clear a person attributes are determined at birth and they play a role in how this person connections are formed over the course of his or her life with characteristics that are more mutable on the other hand behaviors activities interests beliefs and opinions the feedback effects between people individual characteristics and their links in the social network become significantly more complex the process of selection still operates with individual characteristics affecting the connections that are formed but now another process comes into play as well people may modify their behaviors to bring them more closely into alignment with the behaviors of their friends this process has been variously described as socialization and social influence since the existing social connections in a network are influencing the individual characteristics of the nodes social influence can be viewed as the reverse of selection with selection the individual characteristics drive the formation of links while with social influence the existing links in mechanisms underlying homophily selection and social the network serve to shape people mutable characteristics the interplay of selection and social influence when we look at a single snapshot of a network and see that people tend to share mutable characteristics with their friends it can be very hard to sort out the distinct effects and relative contributions of selection and social influence have the people in the network adapted their behaviors to become more like their friends or have they sought out people who were already like them such questions can be addressed using longitudinal studies of a social network in which the social connections and the behaviors within a group are both tracked over a period of time fundamentally this makes it possible to see the behavioral changes that occur after changes in an individual network connections as opposed to the changes to the network that occur after an individual changes his or her behavior this type of methodology has been used for example to study the processes that lead pairs of adolescent friends to have similar outcomes in terms of scholastic achievement and delinquent behavior such as drug use empirical evidence confirms the intuitive fact that teenage friends are similar to each other in their behaviors and both selection and social influence have a natural resonance in this setting teenagers seek out social circles composed of people like them and peer pressure causes them to conform to behavioral patterns within their social circles what is much harder to resolve is how these two effects interact and whether one is more strongly at work than the other as longitudinal behavior relevant to this question became available researchers began quantifying the relative impact of these different factors a line of work beginning with cohen and kandel has suggested that while both effects are present in the data the outsized role that earlier informal arguments had accorded to peer pressure i e social influence is actually more moderate the effect of selection here is in fact comparable to and sometimes greater than the effect of social influence understanding the tension between these different forces can be important not just for identifying underlying causes but also for reasoning about the effect of possible interventions one might attempt in the system for example once we find that illicit drug use displays homophily across a social network with students showing a greater likelihood to use drugs when their friends do we can ask about the effects of a program that targets certain high school students and influences them to stop using drugs to the extent that the observed homophily is based on some amount of social influence such a program could have a broad impact across the social network by causing the friends of these targeted students to stop using drugs as well but one must be careful if the observed homophily is arising instead almost entirely from selection effects then the program may not reduce drug use are other cognitive effects at work as well for example people may systematically misperceive the characteristics of their friends as being more in alignment with their own than they really are for our discussion here we will not focus explicitly on such effects beyond the students it directly targets as these students stop using drugs they change their social circles and form new friendships with students who don t use drugs but the drug using behavior of other students is not strongly affected another example of research addressing this subtle interplay of factors is the work of christakis and fowler on the effect of social networks on health related outcomes in one recent study using longitudinal data covering roughly people they tracked obesity status and social network structure over a year period they found that obese and non obese people clustered in the network in a fashion consistent with homophily according to the numerical measure described in section people tend to be more similar in obesity status to their network neighbors than in a version of the same network where obesity status is assigned randomly the problem is then to distinguish among several hypotheses for why this clustering is present is it i because of selection effects in which people are choosing to form friendships with others of similar obesity status ii because of the confounding effects of homophily according to other characteristics in which the network structure indicates existing patterns of similarity in other dimensions that correlate with obesity status or iii because changes in the obesity status of a person friends was exerting a presumably behavioral influence that affected his or her future obesity status statistical analysis in christakis and fowler paper argues that even accounting for effects of types i and ii there is significant evidence for an effect of type iii as well that obesity is a health condition displaying a form of social influence with changes in your friends obesity status in turn having a subsequent effect on you this suggests the intriguing prospect that obesity and perhaps other health conditions with a strong behavioral aspect may exhibit some amount of contagion in a social sense you don t necessarily catch it from your friends the way you catch the flu but it nonetheless can spread through the underlying social network via the mechanism of social influence these examples and this general style of investigation show how careful analysis is needed to distinguish among different factors contributing to an aggregate conclusion even when people tend to be similar to their neighbors in a social network it may not be clear why the point is that an observation of homophily is often not an endpoint in itself but rather the starting point for deeper questions questions that address why the homophily is present how its underlying mechanisms will affect the further evolution of the network and how these mechanisms interact with possible outside attempts to influence the behavior of people in the network figure an affiliation network is a bipartite graph that shows which individuals are affiliated with which groups or activities here anna participates in both of the social foci on the right while daniel participates in only one affiliation thus far we have been discussing contextual factors that affect the formation of links in a network based on similarities in characteristics of the nodes and based on behaviors and activities that the nodes engage in these surrounding contexts have been viewed appropriately as existing outside the network but in fact it possible to put these contexts into the network itself by working with a larger network that contains both people and contexts as nodes through such a network formulation we will get additional insight into some broad aspects of homophily and see how the simultaneous evolution of contexts and friendships can be put on a common network footing with the notion of triadic closure from chapter in principle we could represent any context this way but for the sake of concreteness we ll focus on how to represent the set of activities a person takes part in and how these affect the formation of links we will take a very general view of the notion of an activity here being part of a particular company organization or neigborhood frequenting a particular place pursuing a particular hobby or interest these are all activities that when shared between two people tend to increase the likelihood that they will interact and hence form a link in the social network adopting terminology due to scott feld we ll refer to such activities as foci that is focal points of social interaction constituting social psychological legal or physical entit ies around which joint activities are organized e g workplaces voluntary organizations hangouts etc affiliation networks as a first step we can represent the participation of a set of people in a set of foci using a graph as follows we will have a node for each person and a node for each focus and we will connect person a to focus x by an edge if a participates in x figure one type of affiliation network that has been widely studied is the memberships of people on corporate boards of directors a very small portion of this network as of mid is shown here the structural pattern of memberships can reveal subtleties in the interactions among both the board members and the companies a very simple example of such a graph is depicted in figure showing two people anna and daniel and two foci working for a literacy tutoring organization and belonging to a karate club the graph indicates that anna participates in both of the foci while daniel participates in only one we will refer to such a graph as an affiliation network since it represents the affiliation of people drawn on the left with foci drawn on the right more generally affiliation networks are examples of a class of graphs called bipartite graphs we say that a graph is bipartite if its nodes can be divided into two sets in such a way that every edge connects a node in one set to a node in the other set in other words there are no edges joining a pair of nodes that belong to the same set all edges go between the two sets bipartite graphs are very useful for representing data in which the items under study come in two categories and we want to understand how the items in one category are associated with the items in the other in the case of affiliation networks the two categories are the people and the foci with each edge connecting a person to a focus that he or she participates in bipartite graphs are often drawn as in figure with the two different sets of nodes drawn as two parallel vertical columns and the edges crossing between the two columns affiliation networks are studied in a range of settings where researchers want to un derstand the patterns of participation in structured activities as one example they have received considerable attention in studying the composition of boards of directors of major corporations boards of directors are relatively small advisory groups populated by high status individuals and since many people serve on multiple boards the overlaps in their participation have a complex structure these overlaps can be naturally represented by an affiliation network as the example in figure shows there is a node for each person and a node for each board and each edge connects a person to a board that they belong to affiliation networks defined by boards of directors have the potential to reveal interesting relationships on both sides of the graph two companies are implicitly linked by having the same person sit on both their boards we can thus learn about possible conduits for information and influence to flow between different companies two people on the other hand are implicitly linked by serving together on a board and so we learn about particular patterns of social interaction among some of the most powerful members of society of course even the complete affiliation network of people and boards of which figure is only a small piece still misses other important contexts that these people inhabit for example the seven people in figure include the presidents of two major universities and a former vice president of the united states co evolution of social and affiliation networks it clear that both social networks and affiliation networks change over time new friendship links are formed and people become associated with new foci moreover these changes represent a kind of co evolution that reflects the interplay between selection and social influence if two people participate in a shared focus this provides them with an opportunity to become friends and if two people are friends they can influence each other choice of foci there is a natural network perspective on these ideas which begins from a network representation that slightly extends the notion of an affiliation network as before we ll have nodes for people and nodes for foci but we now introduce two distinct kinds of edges as well the first kind of edge functions as an edge in a social network it connects two structure of this network changes over time as well and sometimes in ways that reinforce the points in our present discussion for example the board memberships shown in figure are taken from the middle of by the end of arthur levinson had resigned from the board of directors of google thus removing one edge from the graph as part of the news coverage of this resignation the chair of the u s federal trade commission jon leibowitz explicitly invoked the notion of overlaps in board membership saying google apple and mr levinson should be commended for recognizing that overlapping board members between competing companies raise serious antitrust issues and for their willingness to resolve our concerns without the need for litigation beyond this matter we will continue to monitor companies that share board members and take enforcement actions where appropriate figure a social affiliation network shows both the friendships between people and their affiliation with different social foci people and indicates friendship or alternatively some other social relation like professional collaboration the second kind of edge functions as an edge in an affiliation network it connects a person to a focus and indicates the participation of the person in the focus we will call such a network a social affiliation network reflecting the fact that it simultaneously contains a social network on the people and an affiliation network on the people and foci figure depicts a simple social affiliation network once we have social affiliation networks as our representation we can appreciate that a range of different mechanisms for link formation can all be viewed as types of closure processes in that they involve closing the third edge of a triangle in the network in particular suppose we have two nodes b and c with a common neighbor a in the network and suppose that an edge forms between b and c there are several interpretations for what this corresponds to depending on whether a b and c are people or foci i if a b and c each represent a person then the formation of the link between b and c is triadic closure just as in chapter see figure a ii if b and c represent people but a represents a focus then this is something different it is the tendency of two people to form a link when they have a focus in common see figure b this is an aspect of the more general principle of selection forming links to others who share characteristics with you to emphasize the analogy with triadic closure this process has been called focal closure iii if a and b are people and c is a focus then we have the formation of a new affiliation b takes part in a focus that her friend a is already involved in see figure c this is a kind of social influence in which b behavior comes into closer alignment person a triadic closure focus b focal closure person c membership closure figure each of triadic closure focal closure and membership closure corresponds to the closing of a triangle in a social affiliation network with that of her friend a continuing the analogy with triadic closure we will refer to this kind of link formation as membership closure thus three very different underlying mechanisms reflecting triadic closure and aspects of selection and social influence can be unified in this type of network as kinds of closure the formation of a link in cases where the two endpoints already have a neighbor in common figure shows all three kinds of closure processes at work triadic closure leads to a new link between anna and claire focal closure leads to a new link between anna and daniel and membership closure leads to bob affiliation with the karate club oversimplifying the mechanisms at work they can be summarized in the following succinct way i bob introduces anna to claire ii karate introduces anna to daniel iii anna introduces bob to karate tracking link formation in on line data in this chapter and the previous one we have identified a set of different mechanisms that lead to the formation of links in social networks these mechansisms are good examples figure in a social affiliation network containing both people and foci edges can form under the effect of several different kinds of closure processes two people with a friend in common two people with a focus in common or a person joining a focus that a friend is already involved in of social phenomena which are clearly at work in small group settings but which have traditionally been very hard to measure quantitatively a natural research strategy is to try tracking these mechanisms as they operate in large populations where an accumulation of many small effects can produce something observable in the aggregate however given that most of the forces responsible for link formation go largely unrecorded in everyday life it is a challenge to select a large clearly delineated group of people and social foci and accurately quantify the relative contributions that these different mechanisms make to the formation of real network links the availability of data from large on line settings with clear social structure has made it possible to attempt some preliminary research along these lines as we emphasized in chapter any analysis of social processes based on such on line datasets must come with a number of caveats in particular it is never a priori clear how much one can extrapolate from digital interactions to interactions that are not computer mediated or even from one computer mediated setting to another of course this problem of extrapolation is present whenever one studies phenomena in a model system on line or not and the kinds of mea surements these large datasets enable represent interesting first steps toward a deeper quan titative understanding of how mechanisms of link formation operate in real life exploring these questions in a broader range of large datasets is an important problem and one that will become easier as large scale data becomes increasingly abundant triadic closure with this background in mind let start with some questions about triadic closure here a first basic numerical question how much more likely is a link to figure a larger network that contains the example from figure pairs of people can have more than one friend or more than one focus in common how does this increase the likelihood that an edge will form between them form between two people in a social network if they already have a friend in common in other words how much more likely is a link to form if it has the effect of closing a triangle here a second question along the same lines as the first how much more likely is an edge to form between two people if they have multiple friends in common for example in figure anna and esther have two friends in common while claire and daniel only have one friend in common how much more likely is the formation of a link in the first of these two cases if we go back to the arguments for why triadic closure operates in social networks we see that they all are qualitatively strengthened as two people have more friends in common there are more sources of opportunity and trust for the interaction there are more people with an incentive to bring them together and the evidence for homophily is arguably stronger we can address these questions empirically using network data as follows i we take two snapshots of the network at different times ii for each k we identify all pairs of nodes who have exactly k friends in common in the first snapshot but who are not directly connected by an edge iii we define t k to be the fraction of these pairs that have formed an edge by the time 005 003 number of common friends figure quantifying the effects of triadic closure in an e mail dataset the curve determined from the data is shown in the solid black line the dotted curves show a compar ison to probabilities computed according to two simple baseline models in which common friends provide independent probabilities of link formation of the second snapshot this is our empirical estimate for the probability that a link will form between two people with k friends in common iv we plot t k as a function of k to illustrate the effect of common friends on the formation of links note that t is the rate at which link formation happens when it does not close a triangle while the values of t k for larger k determine the rate at which link formation happens when it does close a triangle thus the comparison between t and these other values addresses the most basic question about the power of triadic closure kossinets and watts computed this function t k using a dataset encoding the full history of e mail communication among roughly undergraduate and graduate students over a one year period at a large u s university this is a who talks to whom type of dataset as we discussed in chapter from the communication traces kossinets and watts constructed a network that evolved over time joining two people by a link at a given instant if they had exchanged e mail in each direction at some point in the past days they then determined an average version of t k by taking multiple pairs of snapshots they built a curve for t k on each pair of snapshots using the procedure described above and then averaged all the curves they obtained in particular the observations in each snapshot were one day apart so their computation gives the average probability that two people form a link per day as a function of the number of common friends they have figure shows a plot of this curve in the solid black line the first thing one notices is the clear evidence for triadic closure t is very close to after which the probability of link formation increases steadily as the number of common friends increases moreover for much of the plot this probability increases in a roughly linear fashion as a function of the number of common friends with an upward bend away from a straight line shape the curve turns upward in a particularly pronounced way from to to friends having two common friends produces significantly more than twice the effect on link formation compared to having a single common friend the upward effect from to to friends is also significant but it occurs on a much smaller sub population since many fewer people in the data have this many friends in common without having already formed a link to interpret this plot more deeply it helps to compare it to an intentionally simplified baseline model describing what one might have expected the data to look like in the presence of triadic closure suppose that for some small probability p each common friend that two people have gives them an independent probability p of forming a link each day so if two people have k friends in common the probability they fail to form a link on any given day is p k this is because each common friend fails to cause the link to form with probability p and these k trials are independent since p k is the probability the link fails to form on a given day the probability that it does form according to our simple baseline model is tbaseline k p k we plot this curve in figure as the upper dotted line given the small absolute effect of the first common friend in the data we also show a comparison to the curve p k which just shifts the simple baseline curve one unit to the right again the point is not to propose this baseline as an explanatory mechanism for triadic closure but rather to look at how the real data compares to it both the real curve and the baseline curve are close to linear and hence qualitatively similar but the fact that the real data turns upward while the baseline curve turns slightly downward indicates that the assumption of independent effects from common friends is too simple to be fully supported by the data a still larger and more detailed study of these effects was conducted by leskovec et al who analyzed properties of triadic closure in the on line social networks of linkedin flickr del icio us and yahoo answers it remains an interesting question to try under standing the similarities and variations in triadic closure effects across social interaction in a range of different settings 0004 0002 number of common foci figure quantifying the effects of focal closure in an e mail dataset again the curve determined from the data is shown in the solid black line while the dotted curve provides a comparison to a simple baseline focal and membership closure using the same approach we can compute probabil ities for the other kinds of closure discussed earlier specifically focal closure what is the probability that two people form a link as a function of the number of foci they are jointly affiliated with membership closure what is the probability that a person becomes involved with a particular focus as a function of the number of friends who are already involved in it as an example of the first of these kinds of closure using figure anna and grace have one activity in common while anna and frank have two in common as an example of the second esther has one friend who belongs to the karate club while claire has two how do these distinctions affect the formation of new links for focal closure kossinets and watts supplemented their university e mail dataset with information about the class schedules for each student in this way each class became a focus and two students shared a focus if they had taken a class together they could then compute the probability of focal closure by direct analogy with their computation for triadic closure determining the probability of link formation per day as a function of the number of shared foci figure shows a plot of this function a single shared class turns out to have roughly the same absolute effect on link formation as a single shared friend but after this the probability of joining a community when k friends are already members 02 005 k figure quantifying the effects of membership closure in a large online dataset the plot shows the probability of joining a livejournal community as a function of the number of friends who are already members curve for focal closure behaves quite differently from the curve for triadic closure it turns downward and appears to approximately level off rather than turning slightly upward thus subsequent shared classes after the first produce a diminishing returns effect comparing to the same kind of baseline in which the probability of link formation with k shared classes is p k shown as the dotted curve in figure we see that the real data turns downward more significantly than this independent model again it is an interesting open question to understand how this effect generalizes to other types of shared foci and to other domains for membership closure the analogous quantities have been measured in other on line domains that possess both person to person interactions and person to focus affiliations figure is based on the blogging site livejournal where friendships are designated by users in their profiles and where foci correspond to membership in user defined communities thus the plot shows the probability of joining a community as a function of the number of friends who have already done so figure shows a similar analysis for wikipedia here the social affiliation network contains a node for each wikipedia editor who maintains a user account and user talk page on the system and there is an edge joining two such editors if they have communicated with one editor writing on the user talk page of the other each figure quantifying the effects of membership closure in a large online dataset the plot shows the probability of editing a wikipedia articles as a function of the number of friends who have already done so wikipedia article defines a focus an editor is associated with a focus corresponding to a particular article if he or she has edited the article thus the plot in figure shows the probability a person edits a wikipedia article as a function of the number of prior editors with whom he or she has communicated as with triadic and focal closure the probabilities in both figure and increase with the number k of common neighbors representing friends associated with the foci the marginal effect diminishes as the number of friends increases but the effect of subsequent friends remains significant moreover in both sources of data there is an initial increasing effect similar to what we saw with triadic closure in this case the probability of joining a livejournal community or editing a wikipedia article is more than twice as great when you have two connections into the focus rather than one in other words the connection to a second person in the focus has a particularly pronounced effect and after this the diminishing marginal effect of connections to further people takes over of course multiple effects can operate simultaneously on the formation of a single link for example if we consider the example in figure triadic closure makes a link between bob and daniel more likely due to their shared friendship with anna and focal closure also makes this link more likely due to the shared membership of bob and daniel in the karate club if a link does form between them it will not necessarily be a priori clear how to attribute it to these two distinct effects this is also a reflection of an issue we discussed in section when describing some of the mechanisms behind triadic closure since the principle of homophily suggests that friends tend to have many characteristics in common the existence of a shared friend between two people is often indicative of other possibly unobserved sources of similarity such as shared foci in this case that by themselves may also make link formation more likely quantifying the interplay between selection and social influence as a final illustration of how we can use large scale on line data to track processes of link formation let return to the question of how selection and social influence work together to produce homophily considered in section we ll make use of the wikipedia data discussed earlier in this section asking how do similarities in behavior between two wikipedia editors relate to their pattern of social interaction over time to make this question precise we need to define both the social network and an underlying measure of behavioral similarity as before the social network will consist of all wikipedia editors who maintain talk pages and there is an edge connecting two editors if they have communicated with one writing on the talk page of the other an editor behavior will correspond to the set of articles she has edited there are a number of natural ways to define numerical measures of similarity between two editors based on their actions a simple one is to declare their similarity to be the value of the ratio number of articles edited by both a and b number of articles edited by at least one of a or b for example if editor a has edited the wikipedia articles on ithaca ny and cornell uni versity and editor b has edited the articles on cornell university and stanford university then their similarity under this measure is since they have jointly edited one article cornell out of three that they have edited in total cornell ithaca and stanford note the close similarity to the definition of neighborhood overlap used in section indeed the measure in equation is precisely the neighborhood overlap of two editors in the bipartite affiliation network of editors and articles consisting only of edges from editors to the articles they ve edited pairs of wikipedia editors who have communicated are significantly more similar in their behavior than pairs of wikipedia editors who have not communicated so we have a case where homophily is clearly present therefore we are set up to address the question of selec tion and social influence is the homophily arising because editors are forming connections with those who have edited the same articles they have selection or is it because editors are led to the articles of those they talk to social influence technical reasons a minor variation on this simple similarity measure is used for the results that follow however since this variation is more complicated to describe and the differences are not significant for our purposes we can think of similarity as consisting of the numerical measure just defined figure the average similarity of two editors on wikipedia relative to the time at which they first communicated time on the x axis is measured in discrete units where each unit corresponds to a single wikipedia action taken by either of the two editors the curve increases both before and after the first contact at time indicating that both selection and social influence play a role the increase in similarity is steepest just before time because every action on wikipedia is recorded and time stamped it is not hard to get an initial picture of this interplay using the following method for each pair of editors a and b who have ever communicated record their similarity over time where time here moves in discrete units advancing by one tick whenever either a or b performs an action on wikipedia editing an article or communicating with another editor next declare time for the pair a b to be the point at which they first communicated this results in many curves showing similarity as a function of time one for each pair of editors who ever communicated and each curve shifted so that time is measured for each one relative to the moment of first communication averaging all these curves yields the single plot in figure it shows the average level of similarity relative to the time of first interaction over all pairs of editors who have ever interacted on wikipedia there are a number of things to notice about this plot first similarity is clearly increas ing both before and after the moment of first interaction indicating that both selection and social influence are at work however the the curve is not symmetric around time the period of fastest increase in similarity is clearly occurring before indicating a particular role for selection there is an especially rapid rise in similarity on average just before two editors meet also note that the levels of similarity depicted in the plot are much higher than for pairs of editors who have not interacted the dashed blue line at the bottom of the plot shows similarity over time for a random sample of non interacting pairs it is both far lower and also essentially constant as time moves forward at a higher level the plot in figure once again illustrates the trade offs involved in working with large scale on line data on the one hand the curve is remarkably smooth because so many pairs are being averaged and so differences between selection and social influence show up that are genuine but too subtle to be noticeable at smaller scales on the other hand the effect being observed is an aggregate one it is the average of the interaction histories of many different pairs of individuals and it does not provide more detailed insight into the experience of any one particular pair a goal for further research is clearly to find ways of formulating more complex nuanced questions that can still be meaningfully addressed on large datasets overall then these analyses represent early attempts to quantify some of the basic mechanisms of link formation at a very large scale using on line data while they are promising in revealing that the basic patterns indeed show up strongly in the data they raise many further questions in particular it natural to ask whether the general shapes of the curves in figures are similar across different domains including domains that are less technologically mediated and whether these curve shapes can be explained at a simpler level by more basic underlying social mechanisms a spatial model of segregation one of the most readily perceived effects of homophily is in the formation of ethnically and racially homogeneous neighborhoods in cities traveling through a metropolitan area one finds that homophily produces a natural spatial signature people live near others like them and as a consequence they open shops restaurants and other businesses oriented toward the populations of their respective neighborhoods the effect is also striking when superimposed on a map as figure by m obius and rosenblat illustrates their images depict the make sure that these are editors with significant histories on wikipedia this plot is constructed using only pairs of editors who each had at least actions both before and after their first interaction with each other the individual histories being averaged took place at many distinct points in wikipedia history it is also natural to ask whether the aggregate effects operated differently in different phases of this history this is a natural question for further investigation but initial tests based on studying these types of properties on wikipedia datasets built from different periods show that the main effects have remained relatively stable over time a chicago b chicago figure the tendency of people to live in racially homogeneous neighborhoods produces spatial patterns of segregation that are apparent both in everyday life and when superim posed on a map as here in these maps of chicago from and in blocks colored yellow and orange the percentage of african americans is below while in blocks colored brown and black the percentage is above percentage of african americans per city block in chicago for the years and in blocks colored yellow and orange the percentage is below while in blocks colored brown and black the percentage is above this pair of figures also shows how concentrations of different groups can intensify over time emphasizing that this is a process with a dynamic aspect using the principles we ve been considering we now discuss how simple mechansisms based on similarity and selection can provide insight into the observed patterns and their dynamics the schelling model a famous model due to thomas schelling shows how global patterns of spatial segregation can arise from the effect of homophily operating at a local level there are many factors that contribute to segregation in real life but schelling model focuses on an intentionally simplified mechanism to illustrate how the forces leading to segregation are remarkably robust they can operate even when no one individual explicitly wants a segregated outcome a agents occupying cells on a grid b neighbor relations as a graph figure in schelling segregation model agents of two different types x and o occupy cells on a grid the neighbor relationships among the cells can be represented very simply as a graph agents care about whether they have at least some neighbors of the same type the general formulation of the model is as follows we assume that there is a population of individuals whom we ll call agents each agent is of type x or type o we think of the two types as representing some immutable characteristic that can serve as the basis for homophily for example race ethnicity country of origin or native language the agents reside in the cells of a grid intended as a stylized model of the two dimensional geography of a city as illustrated in figure a we will assume that some cells of the grid contain agents while others are unpopulated a cell neighbors are the cells that touch it including diagonal contact thus a cell that is not on the boundary of the grid has eight neighbors we can equivalently think of the neighbor relationships as defining a graph the cells are the nodes and we put an edge between two cells that are neighbors on the grid in this view the agents thus occupy the nodes of a graph that are arranged in this grid like pattern as shown in figure b for ease of visualization however we will continue to draw things using a geometric grid rather than a graph the fundamental constraint driving the model is that each agent wants to have at least some other agents of its own type as neighbors we will assume that there is a threshold t common to all agents if an agent discovers that fewer than t of its neighbors are of the same type as itself then it has an interest in moving to a new cell we will call such an agent unsatisfied with its current location for example in figure a we indicate with an asterisk all the agents that are unsatisfied in the arrangement from figure a when the threshold t is equal to in figure a we have also added a number after each agent this is simply to provide each with a unique name the key distinction is still whether each agent is of type x or type o x10 a an initial configuration o1 x4 o3 o6 x7 o11 x10 o5 o9 b after one round of movement figure after arranging agents in cells of the grid we first determine which agents are unsatisfied with fewer than t other agents of the same type as neighbors in one round each of these agents moves to a cell where they will be satisfied this may cause other agents to become unsatisfied in which case a new round of movement begins the dynamics of movement thus far we have simply specified a set of agents that want to move given an underlying threshold we now discuss how this gives the model its dynamic aspect agents move in a sequence of rounds in each round we consider the unsatisfied agents in some order and for each one in turn we have it move to an unoccupied cell where it will be satisfied after this the round of movement has come to an end representing a fixed time period during which unsatisfied agents have changed where they live these new locations may cause different agents to be unsatisfied and this leads to a new round of movement in the literature on this model there are numerous variations in the specific details of how the movement of agents within a round is handled for example the agents can be scheduled to move in a random order or in an order that sweeps downward along rows of the grid they can move to the nearest location that will make them satisfied or to a random one there also needs to be a way of handling situations in which an agent is scheduled to move and there is no cell that will make it satisified in such a case the agent can be left where it is or moved to a completely random cell research has found that the qualitative results of the model tend to be quite similar however these issues are resolved and different investigations of the model have tended to resolve them differently for example figure b shows the results of one round of movement starting from the arrangement in figure a when the threshold t is unsatisfied agents are scheduled to move by considering them one row at a time working downward through the grid and each agent moves to the nearest cell that will make it satisfied the unique name of each agent in the figure allows us to see where it has moved in figure b relative to the initial state in figure a notice that in some concrete respects the pattern of agents has become more segregated after this round of movement for example in figure a there is only a single agent with no neighbors of the opposite type after this first round of movement however there are six agents in figure b with no neighbors of the opposite type as we will see this increasing level of segregation is the key behavior to emerge from the model larger examples small examples of the type in figures and are helpful in working through the details of the model by hand but at such small scales it is difficult to see the kinds of typical patterns that arise for this computer simulation is very useful there are many on line computer programs that make it possible to simulate the schelling model as with the published literature on the model they all tend to differ slightly from each other in their specifics here we discuss some examples from a simulation written by sean luke which is like the version of the model we have discussed thus far except that unsatisfied agents move to a random location in figure we show the results of simulating the model on a grid with rows and a a simulation with threshold b another simulation with threshold figure two runs of a simulation of the schelling model with a threshold t of on a by grid with agents of each type each cell of the grid is colored red if it is occupied by an agent of the first type blue if it is occupied by an agent of the second type and black if it is empty not occupied by any agent columns agents of each type and empty cells the threshold t is equal to as in our earlier examples the two images depict the results of two different runs of the simulation with different random starting patterns of agents in each case the simulation reached a point shown in the figures at which all agents were satisfied after roughly rounds of movement because of the different random starts the final arrangement of agents is different in the two cases but the qualitative similarities reflect the fundamental consequences of the model by seeking out locations near other agents of the same type the model produces large homogeneous regions interlocking with each other as they stretch across the grid in the midst of these regions are large numbers of agents who are surrounded on all sides by other agents of the same type and in fact at some distance from the nearest agent of the opposite type the geometric pattern has become segregated much as in the maps of chicago from figure with which we began the section interpretations of the model we ve now seen how the model works what it looks like at relatively large scales and how it produces spatially segregated outcomes but what broader insights into homophily and segregation does it suggest the first and most basic one is that spatial segregation is taking place even though no x x o o x x x x o o x x o o x x o o o o x x o o x x o o x x x x o o x x figure with a threshold of it is possible to arrange agents in an integrated pattern all agents are satisfied and everyone who is not on the boundary on the grid has an equal number of neighbors of each type individual agent is actively seeking it sticking to our focus on a threshold t of we see that although agents want to be near others like them their requirements are not particularly draconian for example an agent would be perfectly happy to be in the minority among its neighbors with five neighbors of the opposite type and three of its own type nor are the requirements globally incompatible with complete integration of the population by arranging agents in a checkerboard pattern as shown in figure we can make each agent satisfied and all agents not on the boundary of the grid have exactly four neighbors of each type this is a pattern that we can continue on as large a grid as we want thus segregation is not happening because we have subtly built it into the model agents are willing to be in the minority and they could all be satisfied if we were only able to carefully arrange them in an integrated pattern the problem is that from a random start it is very hard for the collection of agents to find such integrated patterns much more typically agents will attach themselves to clusters of others like themselves and these clusters will grow as other agents follow suit moreover there is a compounding effect as the rounds of movement unfold in which agents who fall below their threshold depart for more homogeneous parts of the grid causing previously satisfied agents to fall below their thresholds and move as well an effect that schelling describes as the progressive unraveling of more integrated regions in the long run this process will tend to cause segregated regions to grow at the expense of more integrated ones the overall effect is one in which the local preferences of individual agents have produced a global pattern that none of them necessarily intended a after steps b after steps c after steps d after steps figure four intermediate points in a simulation of the schelling model with a threshold t of on a by grid with agents of each type as the rounds of movement progress large homogeneous regions on the grid grow at the expense of smaller narrower regions this point is ultimately at the heart of the model although segregation in real life is amplified by a genuine desire within some fraction of the population to belong to large clusters of similar people either to avoid people who belong to other groups or to acquire a critical mass of members from one own group we see here that such factors are not necessary for segregation to occur the underpinnings of segregation are already present in a system where individuals simply want to avoid being in too extreme a minority in their own local area the process operates even more powerfully when we raise the threshold t in our examples from to even with a threshold of nodes are willing to have an equal number of neighbors of each type and a slightly more elaborate checkerboard example in the spirit of figure shows that with careful placement the agents can be arranged so that all are satisfied and most still have a significant number of neighbors of the opposite type but now not only is an integrated pattern very hard to reach from a random starting arrangement any vestiges of integration among the two types tends to collapse completely over time as one example of this figure shows four intermediate points in one run of a simulation with threshold and other properties the same as before a by grid with agents of each type and random movement by unsatisfied agents figure a shows that after rounds of movement we have an arrangement of agents that roughly resembles what we saw with a lower threshold of however this does not last long crucially the long tendrils where one type interlocks with the other quickly wither and retract leaving the more homogeneous regions shown after rounds in figure b this pulling back continues passing through a phase with a large and small region of each type after rounds figure c eventually to a point where there is only a single significant region of each type after roughly rounds figure d note that this is not the end of the process since there remain agents around the edges still looking for places to move but by this point the overall two region layout has become very stable finally we stress that this figure corresponds to just a single run of the simulation but computational experiments show that the sequence of events it depicts leading to almost complete separation of the two types is very robust when the threshold is this high viewed at a still more general level the schelling model is an example of how character istics that are fixed and unchanging such as race or ethnicity can become highly correlated with other characteristics that are mutable in this case the mutable characteristic is the decision about where to live which over time conforms to similarities in the agents im mutable types producing segregation but there are other non spatial manifestation of the same effect in which beliefs and opinions become correlated across racial or ethnic lines and for similar underlying reasons as homophily draws people together along immutable characteristics there is a natural tendency for mutable characteristics to change in accor dance with the network structure as a final point we note that while the model is mathematically precise and self contained the discussion has been carried out in terms of simulations and qualitative obser vations this is because rigorous mathematical analysis of the schelling model appears to be quite difficult and is largely an open research question for partial progress on analyzing properties of the schelling model see the work of young who compares properties of different arrangements in which all agents are satisfied m obius and rosenblat who perform a probabilistic analysis and vinkovi c and kirman who develop analogies to models for the mixing of two liquids and other physical phenomena exercises figure a social network where triadic closure may occur consider the social network represented in figure suppose that this social net work was obtained by observing a group of people at a particular point in time and recording all their friendship relations now suppose that we come back at some point in the future and observe it again according to the theories based on empirical studies of triadic closure in networks which new edge is most likely to be present i e which pair of nodes who do not currently have an edge connecting them are most likely to be linked by an edge when we return to take the second observation also give a brief explanation for your answer given a bipartite affiliation graph showing the membership of people in different social foci researchers sometimes create a projected graph on just the people in which we join two people when they have a focus in common a draw what such a projected graph would look like for the example of memberships on corporate boards of directors from figure here the nodes would be the exercises seven people in the figure and there would be an edge joining any two who serve on a board of directors together b give an example of two different affiliation networks on the same set of people but with different foci so that the projected graphs from these two different affiliation networks are the same this shows how information can be lost when moving from the full affiliation network to just the projected graph on the set of people figure an affiliation network on six people labeled a f and three foci labeled x y and z consider the affiliation network in figure with six people labeled a f and three foci labeled x y and z a draw the derived network on just the six people as in exercise joining two people when they share a focus b in the resulting network on people can you identify a sense in which the triangle on the nodes a c and e has a qualitatively different meaning than the other triangles that appear in the network explain figure a graph on people arising from an unobserved affiliation network given a network showing pairs of people who share activities we can try to reconstruct an affiliation network consistent with this data for example suppose that you are trying to infer the structure of a bipartite affiliation network and by indirect observation you ve obtained the projected network on just the set of people constructed as in exercise there is an edge joining each pair of people who share a focus this projected network is shown in figure a draw an affiliation network involving these six people together with four foci that you should define whose projected network is the graph shown in figure b explain why any affiliation network capable of producing the projected network in figure must have at least four foci chapter positive and negative relationships in our discussion of networks thus far we have generally viewed the relationships con tained in these networks as having positive connotations links have typically indicated such things as friendship collaboration sharing of information or membership in a group the terminology of on line social networks reflects a largely similar view through its em phasis on the connections one forms with friends fans followers and so forth but in most network settings there are also negative effects at work some relations are friendly but others are antagonistic or hostile interactions between people or groups are regularly beset by controversy disagreement and sometimes outright conflict how should we reason about the mix of positive and negative relationships that take place within a network here we describe a rich part of social network theory that involves taking a network and annotating its links i e its edges with positive and negative signs positive links represent friendship while negative links represent antagonism and an important problem in the study of social networks is to understand the tension between these two forces the notion of structural balance that we discuss in this chapter is one of the basic frameworks for doing this in addition to introducing some of the basics of structural balance our discussion here serves a second methodological purpose it illustrates a nice connection between local and global network properties a recurring issue in the analysis of networked systems is the way in which local effects phenomena involving only a few nodes at a time can have global consequences that are observable at the level of the network as a whole structural balance offers a way to capture one such relationship in a very clean way and by purely mathematical analysis we will consider a simple definition abstractly and find that it inevitably leads to certain macroscopic properties of the network d easley and j kleinberg networks crowds and markets reasoning about a highly connected world cambridge university press draft version june structural balance we focus here on perhaps the most basic model of positive and negative relationships since it captures the essential idea suppose we have a social network on a set of people in which everyone knows everyone else so we have an edge joining each pair of nodes such a network is called a clique or a complete graph we then label each edge with either or a label indicates that its two endpoints are friends while a label indicates that its two endpoints are enemies note that since there an edge connecting each pair we are assuming that each pair of people are either friends or enemies no two people are indifferent to one another or unaware of each other thus the model we re considering makes the most sense for a group of people small enough to have this level of mutual awareness e g a classroom a small company a sports team a fraternity or sorority or for a setting such as international relations in which the nodes are countries and every country has an official diplomatic position toward every other the principles underlying structural balance are based on theories in social psychology dating back to the work of heider in the and generalized and extended to the language of graphs beginning with the work of cartwright and harary in the the crucial idea is the following if we look at any two people in the group in isolation the edge between them can be labeled or that is they are either friends or enemies but when we look at sets of three people at a time certain configurations of and are socially and psychologically more plausible than others in particular there are four distinct ways up to symmetry to label the three edges among three people with and see figure we can distinguish among these four possibilities as follows given a set of people a b and c having three pluses among them as in figure a is a very natural situation it corresponds to three people who are mutual friends having a single plus and two minuses in the relations among the there people is also very natural it means that two of the three are friends and they have a mutual enemy in the third see figure c the other two possible labelings of the triangle on a b and c introduce some amount of psychological stress or instability into the relationships a triangle with two pluses and one minus corresponds as in figure b to a person a who is friends with each of b and c but b and c don t get along with each other in this type of situation there would be implicit forces pushing a to try to get b and c to become in section we will consider the more general setting in which not every pair of nodes is necessarily connected by an edge a a b and c are mutual friends balanced b a is friends with b and c but they don t get along with each other not balanced c a and b are friends with c as a mutual en emy balanced d a b and c are mutual enemies not bal anced figure structural balance each labeled triangle must have or positive edges friends thus turning the b c edge label to or else for a to side with one of b or c against the other turning one of the edge labels out of a to a similarly there are sources of instability in a configuration where each of a b and c are mutual enemies as in figure d in this case there would be forces motivating two of the three people to team up against the third turning one of the three edge labels to a based on this reasoning we will refer to triangles with one or three as balanced since they are free of these sources of instability and we will refer to triangles with zero or two as unbalanced the argument of structural balance theorists is that because unbalanced triangles are sources of stress or psychological dissonance people strive to minimize them in their personal relationships and hence they will be less abundant in real social settings than balanced not balanced figure the labeled four node complete graph on the left is balanced the one on the right is not balanced triangles defining structural balance for networks so far we have been talking about struc tural balance for groups of three nodes but it is easy to create a definition that naturally generalizes this to complete graphs on an arbitrary number of nodes with edges labeled by and specifically we say that a labeled complete graph is balanced if every one of its triangles is balanced that is if it obeys the following structural balance property for every set of three nodes if we consider the three edges connecting them either all three of these edges are labeled or else exactly one of them is labeled for example consider the two labeled four node networks in figure the one on the left is balanced since we can check that each set of three nodes satisfies the structural balance property above on the other hand the one on the right is not balanced since among the three nodes a b c there are exactly two edges labeled in violation of structural balance the triangle on b c d also violates the condition our definition of balanced networks here represents the limit of a social system that has eliminated all unbalanced triangles as such it is a fairly extreme definition for example one could instead propose a definition which only required that at least some large percentage of all triangles were balanced allowing a few triangles to be unbalanced but the version with all triangles balanced is a fundamental first step in thinking about this concept and mutual antagonism between sets set x set y figure if a complete graph can be divided into two sets of mutual friends with complete mutual antagonism between the two sets then it is balanced furthermore this is the only way for a complete graph to be balanced as we will see next it turns out to have very interesting mathematical structure that in fact helps to inform the conclusions of more complicated models as well characterizing the structure of balanced networks at a general level what does a balanced network i e a balanced labeled complete graph look like given any specific example we can check all triangles to make sure that they each obey the balance conditions but it would be much better to have a simple conceptual description of what a balanced network looks like in general one way for a network to be balanced is if everyone likes each other in this case all triangles have three labels on the other hand the left hand side of figure suggests a slightly more complicated way for a network to be balanced it consists of two groups of friends a b and c d with negative relations between people in different groups this is actually true in general suppose we have a labeled complete graph in which the nodes can be divided into two groups x and y such that every pair of nodes in x like each other every pair of nodes in y like each other and everyone in x is the enemy of everyone in y see the schematic illustration in figure you can check that such a network is balanced a triangle contained entirely in one group or the other has three labels and a triangle with two people in one group and one in the other has exactly one label so this describes two basic ways to achieve structural balance either everyone likes each other or the world consists of two groups of mutual friends with complete antagonism between the groups the surprising fact is the following these are the only ways to have a balanced network we formulate this fact precisely as the following balance theorem proved by frank harary in balance theorem if a labeled complete graph is balanced then either all pairs of nodes are friends or else the nodes can be divided into two groups x and y such that every pair of nodes in x like each other every pair of nodes in y like each other and everyone in x is the enemy of everyone in y the balance theorem is not at all an obvious fact nor should it be initially clear why it is true essentially we re taking a purely local property namely the structural balance property which applies to only three nodes at a time and showing that it implies a strong global property either everyone gets along or the world is divided into two battling factions we re now going to show why this claim in fact is true proving the balance theorem establishing the claim requires a proof we re going to suppose we have an arbitrary labeled complete graph assume only that it is balanced and conclude that either everyone is friends or that there are sets x and y as described in the claim recall that we worked through a proof in chapter as well when we used simple assumptions about triadic closure in a social network to conclude all local bridges in the network must be weak ties our proof here will be somewhat longer but still very natural and straightforward we use the definition of balance to directly derive the conclusion of the claim to start suppose we have a labeled complete graph and all we know is that it balanced we have to show that it has the structure in the claim if it has no negative edges at all then everyone is friends and we re all set otherwise there is at least one negative edge and we need to somehow come up with a division of the nodes into sets of mutual friends x and y with complete antagonism between them the difficulty is that knowing so little about the graph itself other than that it is balanced it not clear how we re supposed to identify x and y let pick any node in the network we ll call it a and consider things from a perspective every other node is either a friend of a or an enemy of a thus natural candidates to try for the sets x and y would be to define x to be a and all its friends and define y to be all the enemies of a this is indeed a division of all the nodes since every node is either a friend or an enemy of a recall what we need to show in order for these two sets x and y to satisfy the conditions of the claim i every two nodes in x are friends ii every two nodes in y are friends friends of a enemies of a figure a schematic illustration of our analysis of balanced networks there may be other nodes not illustrated here iii every node in x is an enemy of every node in y let argue that each of these conditions is in fact true for our choice of x and y this will mean that x and y do satisfy the conditions of the claim and will complete the proof the rest of the argument establishing i ii and iii is illustrated schematically in figure for i we know that a is friends with every other node in x how about two other nodes in x let call them b and c must they be friends we know that a is friends with both b and c so if b and c were enemies of each other then a b and c would form a triangle with two labels a violation of the balance condition since we know the network is balanced this can t happen so it must be that b and c in fact are friends since b and c were the names of any two nodes in x we have concluded that every two nodes in x are friends let try the same kind of argument for ii consider any two nodes in y let call them d and e must they be friends we know that a is enemies with both d and e so if d and e were enemies of each other then a d and e would form a triangle with no labels a violation of the balance condition since we know the network is balanced this can t happen so it must be that d and e in fact are friends since d and e were the names of any two nodes in y we have concluded that every two nodes in y are friends finally let try condition iii following the style of our arguments for i and ii consider a node in x call if b and a node in y call it d must they be enemies we know a is friends with b and enemies with d so if b and d were friends then a b and d would form a triangle with two labels a violation of the balance condition since we know the network is balanced this can t happen so it must be that b and d in fact are enemies since b and d were the names of any node in x and any node in y we have concluded that every such pair constitutes a pair of enemies so in conclusion assuming only that the network is balanced we have described a division of the nodes into two sets x and y and we have checked conditions i ii and iii required by the claim this completes the proof of the balance theorem applications of structural balance structural balance has grown into a large area of study and we ve only described a simple but central example of the theory in section we discuss two extensions to the basic theory one to handle graphs that are not necessarily complete and one to describe the structure of complete graphs that are approximately balanced in the sense that most but not all their triangles are balanced there has also been recent research looking at dynamic aspects of structural balance theory modeling how the set of friendships and antagonisms in a complete graph in other words the labeling of the edges might evolve over time as the social network implicitly seeks out structural balance antal krapivsky and redner study a model in which we start with a random labeling choosing or randomly for each edge we then repeatedly look for a triangle that is not balanced and flip one of its labels to make it balanced this captures a situation in which people continually reassess their likes and dislikes of others as they strive for structural balance the mathematics here becomes quite complicated and turns out to resemble the mathematical models one uses for certain physical systems as they reconfigure to minimize their energy in the remainder of this section we consider two further areas in which the ideas of struc tural balance are relevant international relations where the nodes are different countries and on line social media sites where users can express positive or negative opinions about each other international relations international politics represents a setting in which it is natural to assume that a collection of nodes all have opinions positive or negative about one another here the nodes are nations and and labels indicate alliances or animosity research in political science has shown that structural balance can sometimes provide an effective explanation for the behavior of nations during various international crises for example moore describing the conflict over bangladesh separation from pakistan in explicitly invokes structural balance theory when he writes t he united states somewhat surprising support of pakistan becomes less surprising when one considers that the ussr a three emperors league b triple alliance c german russian lapse d french russian alliance e entente cordiale f british russian alliance figure the evolution of alliances in europe the nations gb fr ru it ge and ah are great britain france russia italy germany and austria hungary respec tively solid dark edges indicate friendship while dotted red edges indicate enmity note how the network slides into a balanced labeling and into world war i this figure and example are from antal krapivsky and redner was china enemy china was india foe and india had traditionally bad relations with pakistan since the u s was at that time improving its relations with china it supported the enemies of china enemies further reverberations of this strange political constellation became inevitable north vietnam made friendly gestures toward india pakistan severed diplomatic relations with those countries of the eastern bloc which recognized bangladesh and china vetoed the acceptance of bangladesh into the u n antal krapivsky and redner use the shifting alliances preceding world war i as another example of structural balance in international relations see figure this also reinforces the fact that structural balance is not necessarily a good thing since its global outcome is often two implacably opposed alliances the search for balance in a system can sometimes be seen as a slide into a hard to resolve opposition between two sides trust distrust and on line ratings a growing source for network data with both positive and negative edges comes from user communities on the web where people can express positive or negative sentiments about each other examples include the technology news site slashdot where users can designate each other as a friend or a foe and on line product rating sites such as epinions where a user can express evaluations of different products and also express trust or distrust of other users guha kumar raghavan and tomkins performed an analysis of the network of user evaluations on epinions their work identified an interesting set of issues that show how the trust distrust dichotomy in on line ratings has both similarities and differences with the friend enemy dichotomy in structural balance theory one difference is based on a simple structural distinction we have been considering structural balance in the context of undirected graphs whereas user evaluations on a site like epinions form a directed graph that is when a user a expresses trust or distrust of a user b we don t necessarily know what b thinks of a or whether b is even aware of a a more subtle difference between trust distrust and friend enemy relations becomes ap parent when thinking about how we should expect triangles on three epinions users to behave certain patterns are easy to reason about for example if user a trusts user b and user b trusts user c then it is natural to expect that a will trust c such triangles with three forward pointing positive edges make sense here by analogy with the all positive undirected triangles of structural balance theory but what if a distrusts b and b dis trusts c should we expect a to trust or to distrust c there are appealing arguments in both directions if we think of distrust as fundamentally a kind of enemy relationship then the arguments from structural balance theory would suggest that a should trust c otherwise we d have a triangle with three negative edges on the other hand if a distrust of b expresses a belief that she is more knowledgeable and competent than b and if b distrust of c reflects a corresponding belief by b then we might well expect that a will distrust c and perhaps even more strongly than she distrusts b it is reasonable to expect that these two different interpretations of distrust may each apply simply in different settings and both might apply in the context of a single product rating site like epinions for example among users who are primarily rating best selling books by political commentators trust distrust evaluations between users may become strongly aligned with agreement or disagreement in these users own political orientations in such a case if a distrusts b and b distrusts c this may suggest that a and c are close to each other on the underlying political spectrum and so the prediction of structural balance theory that a should trust c may apply on the other hand among users who are primarily rating consumer electronics products trust distrust evaluations may largely reflect the relative expertise of users about the products their respective features reliability and so forth in such a case if a distrusts b and b distrusts c we might conclude that a is set w mutual friends inside v set v set x mutual friends inside x mutual antagonism between all sets set y mutual friends inside y set z figure a complete graph is weakly balanced precisely when it can be divided into multiple sets of mutual friends with complete mutual antagonism between each pair of sets far more expert than c and so should distrust c as well ultimately understanding how these positive and negative relationships work is impor tant for understanding the role they play on social web sites where users register subjective evaluations of each other research is only beginning to explore these fundamental questions including the ways in which theories of balance as well as related theories can be used to shed light on these issues in large scale datasets a weaker form of structural balance in studying models of positive and negative relationships on networks researchers have also formulated alternate notions of structural balance by revisiting the original assumptions we used to motivate the framework in particular our analysis began from the claim that there are two kinds of structures on a group of three people that are inherently unbalanced a triangle with two positive edges and one negative edge as in figure b and a triangle with three negative edges as in figure d in each of these cases we argued that the relationships within the triangle contained a latent source of stress that the network might try to resolve the underlying arguments in the two cases however were fundamentally different in a triangle with two positive edges we have the problem of a person whose two friends don t get along in a triangle with three negative edges there is the possibility that two of the nodes will ally themselves against the third james davis and others have argued that in many settings the first of these factors may be significantly stronger than the second we may see friends of friends trying to recon cile their differences resolving the lack of balance in figure b while at the same time there could be less of a force leading any two of three mutual enemies as in figure d to become friendly it therefore becomes natural to ask what structural properties arise when we rule out only triangles with exactly two positive edges while allowing triangles with three negative edges to be present in the network characterizing weakly balanced networks more precisely we will say that a com plete graph with each edge labeled by or is weakly balanced if the following property holds weak structural balance property there is no set of three nodes such that the edges among them consist of exactly two positive edges and one negative edge since weak balance imposes less of a restriction on what the network can look like we should expect to see a broader range of possible structures for weakly balanced networks beyond what the balance theorem required for networks that were balanced under our original definition and indeed figure indicates a new kind of structure that can arise suppose that the nodes can be divided into an arbitrary number of groups possibly more than two so that two nodes are friends when they belong to the same group and enemies when they belong to different groups then we can check that such a network is weakly balanced in any triangle that contains at least two positive edges all three nodes must belong to the same group therefore the third edge of this triangle must be positive as well in other words the network contains no triangles with exactly two edges just as the balance theorem established that all balanced networks must have a simple structure an analogous result holds for weakly balanced networks they must have the structure depicted in figure with any number of groups characterization of weakly balanced networks if a labeled complete graph is weakly balanced then its nodes can be divided into groups in such a way that friends of a enemies of a figure a schematic illustration of our analysis of weakly balanced networks there may be other nodes not illustrated here every two nodes belonging to the same group are friends and every two nodes belonging to different groups are enemies the fact that this characterization is true in fact provided another early motivation for studying weak structural balance the cartwright harary notion of balance predicted only dichotomies or mutual consensus as its basic social structure and thus did not provide a model for reasoning about situations in which a network is divided into more than two factions weak structural balance makes this possible since weakly balanced complete graphs can contain any number of opposed groups of mutual friends proving the characterization it is not hard to give a proof for this characteriza tion following the structure of our proof for the balance theorem and making appropriate changes where necessary starting with a weakly balanced complete graph the characteriza tion requires that we produce a division of its nodes into groups of mutual friends such that all relations between nodes in different groups are negative here is how we will construct this division first we pick any node a and we consider the set consisting of a and all its friends let call this set of nodes x we d like to make x our first group and for this to work we need to establish two things i all of a friends are friends with each other this way we have indeed produced a group of mutual friends ii a and all his friends are enemies with everyone else in the graph this way the people in this group will be enemies with everyone in other groups however we divide up the rest of the graph fortunately ideas that we already used inside the proof of the balance theorem can be adapted to our new setting here to establish i and ii the idea is shown in figure first for i let consider two nodes b and c who are both friends with a if b and c were enemies of each other then the triangle on nodes a b and c would have exactly two labels which would violate weak structural balance so b and c must indeed be friends with each other for ii we know that a is enemies with all nodes in the graph outside x since the group x is defined to include all of a friends how about an edge between a node b in x and a node d outside x if b and d were friends then the triangle on nodes a b and d would have exactly two labels again a violation of weak structural balance so b and d must be enemies since properties i and ii hold we can remove the set x consisting of a and all his friends from the graph and declare it to be the first group we now have a smaller complete graph that is still weakly balanced we find a second group in this graph and proceed to remove groups in this way until all the nodes have been assigned to a group since each group consists of mutual friends by property i and each group has only negative relations with everyone outside the group by property ii this proves the characterization it is interesting to reflect on this proof in relation to the proof of the balance theorem in particular the contrast reflected by the small differences between figures and in proving the balance theorem we had to reason about the sign of the edge between d and e to show that the enemies of the set x themselves formed a set y of mutual friends in characterizing weakly balanced complete graphs on the other hand we made no attempt to reason about the d e edge because weak balance imposes no condition on it two enemies of a can be either friends or enemies as a result the set of enemies in figure might not be a set of mutual friends when only weak balance holds it might consist of multiple groups of mutual friends and as we extract these groups one by one over the course of the proof we recover the multi faction structure illustrated schematically in figure advanced material generalizing the definition of structural balance in this section we consider more general ways of formulating the idea of structural balance in a network in particular our definition of structural balance thus far is fairly demanding in two respects figure in graphs that are not complete we can still define notions of structural balance when the edges that are present have positive or negative signs indicating friend or enemy relations it applies only to complete graphs we require that each person know and have an opinion positive or negative on everyone else what if only some pairs of people know each other the balance theorem showing that structural balance implies a global division of the world into two factions only applies to the case in which every triangle is balanced can we relax this to say that if most triangles are balanced then the world can be approximately divided into two factions in the two parts of this section we discuss a pair of results that address these questions the first is based on a graph theoretic analysis involving the notion of breadth first search from chapter while the second is typical of a style of proof known as a counting argument throughout this section we will focus on the original definition of structural balance from sections and rather than the weaker version from section a a graph with signed edges b filling in the missing edges to achieve balance y c dividing the graph into two sets figure there are two equivalent ways to define structural balance for general non complete graphs one definition asks whether it is possible to fill in the remaining edges so as to produce a signed complete graph that is balanced the other definition asks whether it is possible to divide the nodes into two sets x and y so that all edges inside x and inside y are positive and all edges between x and y are negative a structural balance in arbitrary non complete networks first let consider the case of a social network that is not necessarily complete that is there are only edges between certain pairs of nodes but each of these edges is still labeled with or so now there are three possible relations between each pair of nodes a positive edge indicating friendship a negative edge indicating enmity or the absence of an edge indicating that the two endpoints do not know each other figure depicts an example of such a signed network defining balance for general networks drawing on what we ve learned from the special case of complete graphs what would be a good definition of balance for this more general kind of structure the balance theorem suggests that we can view structural balance in either of two equivalent ways a local view as a condition on each triangle of the network or a global view as a requirement that the world be divided into two mutually opposed sets of friends each of these suggests a way of defining structure balance for general signed graphs one option would be to treat balance for non complete networks as a problem of filling in missing values suppose we imagine as a thought experiment that all people in the group in fact do know and have an opinion on each other the graph under consideration is not complete only because we have failed to observe the relations between some of the pairs we could then say that the graph is balanced if it possible to fill in all the missing labeled edges in such a way that the resulting signed complete graph is balanced in other words a non complete graph is balanced if it can be completed by adding edges to form a signed complete graph that is balanced for example figure a shows a graph with signed edges and figure b shows how the remaining edges can be filled in to produce a balanced complete graph we declare the missing edge between nodes and to be positive and the remaining missing edges to be negative and one can check that this causes all triangles to be balanced alternately we could take a more global view viewing structural balance as implying a division of the network into two mutually opposed sets of friends with this in mind we could define a signed graph to be balanced if it is possible to divide the nodes into two sets x and y such that any edge with both ends inside x or both ends inside y is positive and any edge with one end in x and the other in y is negative that is people in x are all mutual friends to the extent that they know each other the same is true for people in y and people in x are all enemies of people in y to the extent that they know each other continuing the example from figure a in figure c we show how to divide this graph into two sets with the desired properties this example hints at a principle that is true in general these two ways of defining balance are equivalent an arbitrary signed graph is balanced under the first definition if and only if it is balanced under the second definition this is actually not hard to see if a signed graph is balanced under the first definition then after filling in all the missing edges appropriately we have a signed complete graph to which we can apply the balance theorem this gives us a division of the network into two sets x and y that satisfies the properties of the second definition on the other hand if a signed graph is balanced under the second definition then after finding a division of the nodes into sets x and y we can fill in positive edges inside x and inside y and fill in figure if a signed graph contains a cycle with an odd number of negative edges then it is not balanced indeed if we pick one of the nodes and try to place it in x then following the set of friend enemy relations around the cycle will produce a conflict by the time we get to the starting node negative edges between x and y and then we can check that all triangles will be balanced so this gives a filling in that satisfies the first definition the fact that the two definitions are equivalent suggests a certain naturalness to the definition since there are fundamentally different ways to arrive at it it also lets us use either definition depending on which is more convenient in a given situation as the example in figure suggests the second definition is generally more useful to work with it tends to be much easier to think about dividing the nodes into two sets than to reason about filling in edges and checking triangles characterizing balance for general networks conceptually however there is some thing not fully satisfying about either definition the definitions themselves do not provide much insight into how to easily check that a graph is balanced there are after all lots of ways to choose signs for the missing edges or to choose ways of splitting the nodes into sets x and y and if a graph is not balanced so that there is no way to do these things suc cessfully what could you show someone to convince them of this fact to take just a small example to suggest some of the difficulties it may not be obvious from a quick inspection of figure that this is not a balanced graph or that if we change the edge connecting nodes and to be positive instead of negative it becomes a balanced graph in fact however all these problems can be remedied if we explore the consequences of the definitions a little further what we will show is a simple characterization of balance in general signed graphs also due to harary 204 and the proof of this characterization also provides an easy method for checking whether a graph is balanced the characterization is based on considering the following question what prevents a graph from being balanced figure shows a graph that is not balanced obtained from figure a and changing the sign of the edge from node to node it also illustrates a reason why it not balanced as follows if we start at node and try to divide the nodes into sets x and y then our choices are forced at every step suppose we initially decide that node should belong to x for the first node it doesn t matter by symmetry then since node is friends with node it too must belong to x node an enemy of must therefore belong to y hence node a friend of must belong to y as well and node an enemy of must belong to x the problem is that if we continue this reasoning one step further then node an enemy of should belong to y but we had already decided at the outset to put it into x we had no freedom of choice during this process so this shows that there is no way to divide the nodes in sets x and y so as to satisfy the mutual friend mutual enemy conditions of structural balance and hence the signed graph in figure is not balanced the reasoning in the previous paragraph sounds elaborate but in fact it followed a simple principle we were walking around a cycle and every time we crossed a negative edge we had to change the set into which we were putting nodes the difficulty was that getting back around to node required crossing an odd number of negative edges and so our original decision to put node into x clashed with the eventual conclusion that node ought to be in y this principle applies in general if the graph contains a cycle with an odd number of negative edges then this implies the graph is not balanced indeed if we start at any node a in the cycle and place it in one of the two sets and then we walk around the cycle placing the other nodes where they must go the identity of the set where we re placing nodes switches an odd number of times as we go around the cycle thus we end up with the wrong set by the time we make it back to a a cycle with an odd number of negative edges is thus a very simple to understand reason why a graph is not balanced you can show someone such a cycle and immediately convince them that the graph is not balanced for example the cycle back in figure consisting of nodes contains five negative edges thus supplying a succinct reason why this graph is not balanced but are there other more complex reasons why a graph is not balanced in fact though it may seem initially surprising cycles with an odd number of negative edges are the only obstacles to balance this is the crux of the following claim 204 claim a signed graph is balanced if and only if it contains no cycle with an odd figure to determine if a signed graph is balanced the first step is to consider only the positive edges find the connected components using just these edges and declare each of these components to be a supernode in any balanced division of the graph into x and y all nodes in the same supernode will have to go into the same set number of negative edges we now show how to prove this claim this is done by designing a method that analyzes the graph and either finds a division into the desired sets x and y or else finds a cycle with an odd number of negative edges proving the characterization identifying supernodes let recall what we re try ing to do find a division of the nodes into sets x and y so that all edges inside x and y are positive and all edges crossing between x and y are negative we will call a partitioning into sets x and y with these properties a balanced division we now describe a procedure that searches for a balanced division of the nodes into sets x and y either it succeeds or it stops with a cycle containing an odd number of negative edges since these are the only two possible outcomes for the procedure this will give a proof of the claim the procedure works in two main steps the first step is to convert the graph to a reduced one in which there are only negative edges and the second step is to solve the problem on this reduced graph the first step works as follows notice that whenever two nodes are figure suppose a negative edge connects two nodes a and b that belong to the same supernode since there is also a path consisting entirely of positive edges that connects a and b through the inside of the supernode putting this negative edge together with the all positive path produces a cycle with an odd number of negative edges connected by a positive edge they must belong to the same one of the sets x or y in a balanced division so we begin by considering what the connected components of the graph would be if we were to only consider positive edges these components can be viewed as a set of contiguous blobs in the overall graph as shown in figure we will refer to each of these blobs as a supernode each supernode is connected internally via positive edges and the only edges going between two different supernodes are negative if there were a positive edge linking two different supernodes we should have combined them together into a single supernode now if any supernode contains a negative edge between some pair of nodes a and b then we already have a cycle with an odd number of negative edges as illustrated in the example of figure consider the path of positive edges that connects a and b inside the supernode and then close off a cycle by including the negative edge joining a and b this cycle has only a single negative edge linking a and b and so it shows that the graph is not balanced if there are no negative edges inside any of the supernodes then there is no internal problem with declaring each supernode to belong entirely to one of x or y so the problem is now how to assign a single label x or y to each supernode in such a way that these choices are all consistent with each other since the decision making is now at the level of supernodes we create a new version of the problem in which there is a node for each figure the second step in determining whether a signed graph is balanced is to look for a labeling of the supernodes so that adjacent supernodes which necessarily contain mutual enemies get opposite labels for this purpose we can ignore the original nodes of the graph and consider a reduced graph whose nodes are the supernodes of the original graph supernode and an edge joining two supernodes if there is an edge in the original that connects the two supernodes figure shows how this works for the example of figure we essentially forget about the individual nodes inside the supernodes and build a new graph at the level of the large blobs of course having done so we can draw the graph in a less blob like way as in figure we now enter the second step of the procedure using this reduced graph whose nodes are the supernodes of the original graph proving the characterization breadth first search of the reduced graph re call that only negative edges go between supernodes since a positive edge between two su pernodes would have merged them together into a single one as a result our reduced graph has only negative edges the remainder of the procedure will produce one of two possible outcomes the first possible outcome is to label each node in the reduced graph as either x or y in such a way that every edge has endpoints with opposite labels from this we figure a more standard drawing of the reduced graph from the previous figure a negative cycle is visually apparent in this drawing can create a balanced division of the original graph by labeling each node the way its supernode is labeled in the reduced graph the second possible outcome will be to find a cycle in the reduced graph that has an odd number of edges we can then convert this to a potentially longer cycle in the original graph with an odd number of negative edges the cycle in the reduced graph connects supernodes and corresponds to a set of negative edges in the original graph we can simply stitch together these negative edges using paths consisting entirely of positive edges that go through the insides of the supernodes this will be a path containing an odd number of negative edges in the original graph for example the odd length cycle in figure through nodes a through e can be realized in the original graph as the darkened negative edges shown in figure this can then be turned into a cycle in the original graph by including paths through the supernodes in this example using the additional nods and in fact this version of the problem when there are only negative edges is known in graph theory as the problem of determining whether a graph is bipartite whether its nodes can be divided into two groups in this case x and y so that each edge goes from one group to the other we saw bipartite graphs when we considered affiliation networks in chapter but there the fact that the graphs were bipartite was apparent from the ready made division of the nodes into people and social foci here on the other hand we are handed a graph in figure having found a negative cycle through the supernodes we can then turn this into a cycle in the original graph by filling in paths of positive edges through the inside of the supernodes the resulting cycle has an odd number of negative edges the wild with no pre specified division into two sets and we want to know if it is possible to identify such a division we now show a way to do this using the idea of breadth first search from chapter resulting either in the division we seek or in a cycle of odd length we simply perform breadth first search starting from any root node in the graph producing a set of layers at increasing distances from this root figure shows how this is done for the reduced graph in figure with node g as the starting root node now because edges cannot jump over successive layers in breadth first search each edge either connects two nodes in adjacent layers or it connects two nodes in the same layer if all edges are of the first type then we can find the desired division of nodes into sets x and y we simply declare all nodes in even numbered layers to belong to x and all nodes in odd numbered layers to belong to y since edges only go between adjacent layers all edges have one end in x and the other end in y as desired otherwise there is an edge connecting two nodes that belong to the same layer let call them a and b as they are in figure for each of these two nodes there is a path that descends layer by layer from the root to it consider the last node that is common to these two paths let call this node d as it is in figure the d a path and the figure when we perform a breadth first search of the reduced graph there is either an edge connecting two nodes in the same layer or there isn t if there isn t then we can produce the desired division into x and y by putting alternate layers in different sets if there is such an edge such as the edge joining a and b in the figure then we can take two paths of the same length leading to the two ends of the edge which together with the edge itself forms an odd cycle d b path have the same length k so a cycle created from the two of these plus the a b edge must have length an odd number this is the odd cycle we seek and this completes the proof to recap if all edges in the reduced graph connect nodes in adjacent layers of the breadth first search then we have a way to label the nodes in the reduced graph as into x and y which in turn provides a balanced division of the nodes in the original graph into x and y in this case we ve established that the graph is balanced otherwise there is an edge connecting two nodes in the same layer of the breadth first search in which case we produce an odd cycle in the reduced graph as in figure in this case we can convert into this to a cycle in the original graph containing an odd number of negative edges as in figure since these are the only two possibilities this proves the claim b approximately balanced networks we now return to the case in which the graph is complete so that every node has a positive or negative relation with every other node and we think about a different way of generalizing the characterization of structural balance first let write down the original balance theorem again with some additional format ting to make its logical structure clear claim if all triangles in a labeled complete graph are balanced then either a all pairs of nodes are friends or else b the nodes can be divided into two groups x and y such that i every pair of nodes in x like each other ii every pair of nodes in y like each other and iii everyone in x is the enemy of everyone in y the conditions of this theorem are fairly extreme in that we require every single triangle to be balanced what if we only know that most triangles are balanced it turns out that the conditions of the theorem can be relaxed in a very natural way allowing us to prove statements like the following one we phrase it so that the wording remains completely parallel to that of the balance theorem claim if at least of all triangles in a labeled complete graph are balanced then either a there is a set consisting of at least of the nodes in which at least of all pairs are friends or else b the nodes can be divided into two groups x and y such that i at least of the pairs in x like each other ii at least of the pairs in y like each other and iii at least of the pairs with one end in x and the other end in y are enemies this is a true statement though the choice of numbers is very specific here is a more general statement that includes both the balance theorem and the preceding claim as special cases claim let  be any number such that  and define   if at least  of all triangles in a labeled complete graph are balanced then either a there is a set consisting of at least  of the nodes in which at least  of all pairs are friends or else b the nodes can be divided into two groups x and y such that i at least  of the pairs in x like each other ii at least  of the pairs in y like each other and iii at least  of the pairs with one end in x and the other end in y are enemies notice that the balance theorem is the case in which  and the other claim above is the case in which  since in this latter case   we now prove this last claim the proof is self contained but it is most easily read with some prior experience in what is sometimes called the analysis of permutations and combinations counting the number of ways to choose particular subsets of larger sets the proof loosely follows the style of the proof we used for the balance theorem we will define the two sets x and y to be the friends and enemies respectively of a designated node a things are trickier here however because not all choices of a will give us the structure we need in particular if a node is personally involved in too many unbalanced triangles then splitting the graph into its friends and enemies may give a very disordered structure consequently the proof consists of two steps we first find a good node that is not involved in too many unbalanced triangles we then show that if we divide the graph into the friends and enemies of this good node we have the desired properties warm up counting edges and triangles before launching into the proof itself let consider some basic counting questions that will show up as ingredients in the proof recall that we have a complete graph with an undirected edge joining each pair of nodes if n is the number of nodes in the graph how many edges are there we can count this quantity as follows there are n possible ways to choose one of the two endpoints and then n possible ways to choose a different node as the other endpoint for a total of n n possible ways to choose the two endpoints in succession if we write down a list of all these possible pairs of endpoints then an edge with endpoints a and b will appear twice on the list once as ab and once as ba in general each edge will appear twice on the list and so the total number of edges is n n a very similar argument lets us count the total number of triangles in the graph specif ically there are n ways to pick the first corner then n ways to pick a different node as the second corner and then n ways to pick a third corner different from the first two this yields a total of n n n sequences of three corners if we write down this list of n n n sequences then a triangle with corners a b and c will appear six times as abc acb bac bca cab and cba in general each triangle will appear six times in this list and so the total number of triangles is n n n the first step finding a good node now let move on to the first step of the proof which is to find a node that isn t involved in too many unbalanced triangles since we are assuming that at most an  fraction of triangles are unbalanced and the total number of triangles in the graph is n n n it follows that the total number of unbalanced triangles is at most n n n suppose we define the weight of a node to be the number of unbalanced triangles that it is a part of thus a node of low weight will be precisely what we re seeking a node that is in relatively few unbalanced triangles one way to count the total weight of all nodes would be to list for each node the unbalanced triangles that it belongs to and then look at the length of all these lists combined in these combined lists each triangle will appear three times once in the list for each of its corners and so the total weight of all nodes is exactly three times the number of unbalanced triangles as a result the total weight of all nodes is at most n n n n n there are n nodes so the average weight of a node is at most  n n it not possible for all nodes to have weights that are strictly above the average so there is at least one node whose weight is equal to the average or below it let pick one such node and call it a this will be our good node a node whose weight is at most  n n since n n n this good node is in at most n triangles and because the algebra is a bit simpler with this slightly larger quantity we will use it in the rest of the analysis the second step splitting the graph according to the good node by analogy with the proof of the balance theorem we divide the graph into two sets a set x consisting of a and all its friends and a set y consisting of all the enemies of a as illustrated in figure now using the definition of unbalanced triangles and the fact that node a is not involved in too many of them we can argue that there are relatively few negative edges inside each of x and y and relatively few positive edges between them specifically this works as follows each negative edge connecting two nodes in x creates a distinct unbalanced triangle involving node a since there are at most n unbalanced triangles involving a there are at most n negative edges inside x a closely analogous argument applies to y each negative edge connecting two nodes in y creates a distinct unbalanced triangle involving node a and so there are at most n negative edges inside y is a very common trick in counting arguments referred to as the pigeonhole principle to compute the average value of a set of objects and then argue that there must be at least one node that is equal to the average or below also of course there must be at least at least one object that is equal to the average or above although this observation isn t useful for our purposes here friends of a a good node a enemies of a figure the characterization of approximately balanced complete graphs follows from an analysis similar to the proof of the original balance theorem however we have to be more careful in dividing the graph by first finding a good node that isn t involved in too many unbalanced triangles and finally an analogous argument applies to edges with one end in x and the other end in y each such edge that is positive creates a distinct unbalanced triangle in volving a and so there are at most n positive edges with one end in x and the other end in y we now consider several possible cases depending on the sizes of the sets x and y essen tially if either of x or y consists of almost the entire graph then we show that alternative a in the claim holds otherwise if each of x and y contain a non negligible number of nodes then we show that alternative b in the claim holds we re also going to assume to make the calculations simpler that n is even and that the quantity n is a whole number although this is not in fact necessary for the proof to start let x be the number of nodes in x and y be the number of nodes in y suppose first that x  n since  and   it follows that  and so x n now recall our earlier counting argument that gave a formula for the number of edges in a complete graph in terms of its number of nodes in this case x has x nodes so it has x x edges since x n this number of edges is at least n n n n there are at most n negative edges inside x and so the fraction of negative edges inside x is at most n n  where we use the facts that  and  we thus conclude that if x contains at least  n nodes then it is a set containing at least a  fraction of the nodes in which at least  of all pairs are friends satisfying part a in the conclusion of the claim the same argument can be applied if y contains at least  n nodes thus we are left with the case in which both x and y contain strictly fewer than  n and in this case we will show that part b in the conclusion of the claim holds first of all the edges with one end in x and the other in y what fraction are positive the total number of edges with one end in x and the other end in y can be counted as follows there are x ways to choose the end in x and then y ways to choose the end in y for a total of xy such edges now since each of x and y are less than  n and they add up to n this product xy is at least n  n   n n where the last inequality follows from the fact that  there are at most n positive edges with one end in x and the other in y so as a fraction of the total this is at most n  n    finally what fraction of edges inside each of x and y are negative let calculate this for x the argument for y is exactly the same there are x x edges inside x in total and since we re in the case where x n this total number of edges is at least n n n there are at most n negative edges inside x so as a fraction of the total this is at most n   thus the division of nodes into sets x and y satisfies all the requirements in conclusion b of the claim and so the proof is complete as a final comment on the claim and its proof one might feel that the difference between  in the assumption of the claim and  is a bit excessive as we saw above when  this means we need to assume that of all triangles are balanced in order to get sets with a density of edges having the correct sign but in fact it is possible to construct examples showing that this relationship between  and  is in fact essentially the best one can do in short the claim provides the kind of approximate version of the balance theorem that we wanted at a qualitative level but we need to assume a fairly small fraction of unbalanced triangles in order to be able to start drawing strong conclusions exercises suppose that a team of anthropologists is studying a set of three small villages that neighbor one another each village has people consisting of extended families everyone in each village knows all the people in their own village as well as the people in the other villages when the anthropologists build the social network on the people in all three villages taken together they find that each person is friends with all the other people in their own village and enemies with everyone in the two other villages this gives them a network on people i e in each village with positive and negative signs on its edges according to the definitions in this chapter is this network on people balanced give a brief explanation for your answer consider the network shown in figure there is an edge between each pair of nodes with five of the edges corresponding to positive relationships and the other five of the edges corresponding to negative relationships figure a network with five positive edges and five negative edges each edge in this network participates in three triangles one formed by each of the additional nodes who is not already an endpoint of the edge for example the a b edge participates in a triangle on a b and c a triangle on a b and d and a triangle on a b and e we can list triangles for the other edges in a similar way for each edge how many of the triangles it participates in are balanced and how many are unbalanced notice that because of the symmetry of the network the answer will be the same for each positive edge and also for each negative edge so it is enough to consider this for one of the positive edges and one of the negative edges when we think about structural balance we can ask what happens when a new node tries to join a network in which there is existing friendship and hostility in fig ures each pair of nodes is either friendly or hostile as indicated by the or label on each edge figure a node social network in which all pairs of nodes know each other and all pairs of nodes are friendly toward each other a d joins the network by becom ing friends with all nodes b d joins the network by becom ing enemies with all nodes figure there are two distinct ways in which node d can join the social network from figure without becoming involved in any unbalanced triangles first consider the node social network in figure in which all pairs of nodes know each other and all pairs of nodes are friendly toward each other now a fourth node d wants to join this network and establish either positive or negative relations with each existing node a b and c it wants to do this in such a way that it doesn t become involved in any unbalanced triangles i e so that after adding d and the labeled edges from d there are no unbalanced triangles that contain d is this possible in fact in this example there are two ways for d to accomplish this as indicated in figure first d can become friends with all existing nodes in this way all the triangles containing it have three positive edges and so are balanced alternately it can become enemies with all existing nodes in this way each triangle containing it has exactly one positive edge and again these triangles would be balanced so for this network it was possible for d to join without becoming involved in any unbalanced triangles however the same is not necessarily possible for other networks we now consider this kind of question for some other networks figure all three nodes are mutual enemies a consider the node social network in figure in which all pairs of nodes know each other and each pair is either friendly or hostile as indicated by the or label on each edge a fourth node d wants to join this network and establish either positive or negative relations with each existing node a b and c can node d do this in such a way that it doesn t become involved in any unbalanced triangles if there is a way for d to do this say how many different such ways there are and give an explanation that is how many different possible labelings of the edges out of d have the property that all triangles containing d are balanced if there is no such way for d to do this give an explanation why not in this and the subsequent questions it possible to work out an answer by rea soning about the new node options without having to check all possibilities b same question but for a different network consider the node social network in figure in which all pairs of nodes know each other and each pair is either friendly or hostile as indicated by the or label on each edge a fourth node d wants to join this network and establish either positive or negative relations with each existing node a b and c can node d do this in such a way that it doesn t become involved in any unbalanced triangles if there is a way for d to do this say how many different such ways there are and give an explanation that is how many different possible labelings figure node a is friends with nodes b and c who are enemies with each other of the edges out of d have the property that all triangles containing d are balanced if there is no such way for d to do this give an explanation why not c using what you ve worked out in questions and consider the following ques tion take any labeled complete graph on any number of nodes that is not balanced i e it contains at least one unbalanced triangle recall that a labeled complete graph is a graph in which there is an edge between each pair of nodes and each edge is labeled with either or a new node x wants to join this network by attaching to each node using a positive or negative edge when if ever is it possible for x to do this in such a way that it does not become involved in any unbalanced triangles give an explanation for your answer hint think about any unbalanced triangle in the network and how x must attach to the nodes in it together with some anthropologists you re studying a sparsely populated region of a rain forest where farmers live along a mile long stretch of river each farmer lives on a tract of land that occupies a mile stretch of the river bank so their tracts exactly divide up the miles of river bank that they collectively cover the numbers are chosen to be simple and to make the story easy to describe the farmers all know each other and after interviewing them you ve discovered that each farmer is friends with all the other farmers that live at most miles from him or her and is enemies with all the farmers that live more than miles from him or her you build the signed complete graph corresponding to this social network and you wonder whether it satisfies the structural balance property this is the question is the network structurally balanced or not provide an explanation for your answer part ii game theory chapter games in the opening chapter of the book we emphasized that the connectedness of a complex social natural or technological system really means two things first an underlying structure of interconnecting links and second an interdependence in the behaviors of the individuals who inhabit the system so that the outcome for any one depends at least implicitly on the combined behaviors of all the first issue network structure was addressed in the first part of the book using graph theory in this second part of the book we study interconnectedness at the level of behavior developing basic models for this in the language of game theory game theory is designed to address situations in which the outcome of a person decision depends not just on how they choose among several options but also on the choices made by the people they are interacting with game theoretic ideas arise in many contexts some contexts are literally games for example choosing how to target a soccer penalty kick and choosing how to defend against it can be modeled using game theory other settings are not usually called games but can be analyzed with the same tools examples include the pricing of a new product when other firms have similar new products deciding how to bid in an auction choosing a route on the internet or through a transportation network deciding whether to adopt an aggressive or a passive stance in international relations or choosing whether to use performance enhancing drugs in a professional sport in these examples each decision maker outcome depends on the decisions made by others this introduces a strategic element that game theory is designed to analyze as we will see later in chapter game theoretic ideas are also relevant to settings where no one is overtly making decisions evolutionary biology provides perhaps the most striking example a basic principle is that mutations are more likely to succeed in a population when they improve the fitness of the organisms that carry the mutation but often this fitness cannot be assessed in isolation rather it depends on what all the other non mutant d easley and j kleinberg networks crowds and markets reasoning about a highly connected world cambridge university press draft version june organisms are doing and how the mutant behavior interacts with the non mutants be haviors in such situations reasoning about the success or failure of the mutation involves game theoretic definitions and in fact very closely resembles the process of reasoning about decisions that intelligent actors make similar kinds of reasoning have been applied to the success or failure of new cultural practices and conventions it depends on the existing patterns of behavior into which they are introduced this indicates that the ideas of game theory are broader than just a model of how people reason about their interactions with oth ers game theory more generally addresses the question of which behaviors tend to sustain themselves when carried out in a larger population game theoretic ideas will appear in many places throughout the book chapters and describe two initial and fundamental applications to network traffic where travel time depends on the routing decisions of others and to auctions where the success of a bidder depends on how the other bidders behave there will be many further examples later in the book including the ways in which prices are set in markets and the ways in which people choose to adopt new ideas in situations where adoption decisions are affected by what others are doing as a first step then we begin with a discussion of the basic ideas behind game theory for now this will involve descriptions of situations in which people interact with one an other initially without an accompanying graph structure once these ideas are in place we will bring graphs back into the picture in subsequent chapters and begin to consider how structure and behavior can be studied simultaneously what is a game game theory is concerned with situations in which decision makers interact with one another and in which the happiness of each participant with the outcome depends not just on his or her own decisions but on the decisions made by everyone to help make the definitions concrete it useful to start with an example a first example suppose that you re a college student and you have two large pieces of work due the next day an exam and a presentation you need to decide whether to study for the exam or to prepare for the presentation for simplicity and to make the example as clean as possible we ll impose a few assumptions first we ll assume you can either study for the exam or prepare for the presentation but not both second we ll assume you have an accurate estimate of the expected grade you ll get under the outcomes of different decisions the outcome of the exam is easy to predict if you study then your expected grade is a while if you don t study then your expected grade is an the presentation is a bit more complicated to think about for the presentation you re what is a game doing it jointly with a partner if both you and your partner prepare for the presentation then the presentation will go extremely well and your expected joint grade is a if just one of you prepares and the other doesn t you ll get an expected joint grade of and if neither of you prepares your expected joint grade is the challenge in reasoning about this is that your partner also has the same exam the next day and we ll assume that he has the same expected outcome for it if he studies and if he doesn t he also has to choose between studying for the exam and preparing for the presentation we ll assume that neither of you is able to contact the other so you can t jointly discuss what to do each of you needs to make a decision independently knowing that the other will also be making a decision both of you are interested in maximizing the average grade you get and we can use the discussion above to work out how this average grade is determined by the way the two of you invest your efforts if both of you prepare for the presentation you ll both get on the presentation and on the exam for an average of if both of you study for the exam you ll both get on the exam and on the presentation for an average of if one of you studies for the exam while the other prepares for the presentation the result is as follows the one who prepares for the presentation gets a on the presentation but only an on the exam for an average of on the other hand the one who studies for the exam still gets a on the presentation since it a joint grade this person benefits from the fact that one of the two of you prepared for it this person also get a on the exam through studying and so gets an average of there a simple tabular way to summarize all these outcomes as follows we represent your two choices to prepare for the presentation or to study for the exam as the rows of a table we represent your partner two choices as the columns so each box in this table represents a decision by each of you in each box we record the average grade you each receive first yours then your partner writing all this down we have the table shown in figure this describes the set up of the situation now you need to figure out what to do prepare for the presentation or study for the exam clearly your average grade depends not just on which of these two options you choose but also on what your partner decides therefore as part of your decision you have to reason about what your partner is likely to do thinking you presentation exam your partner presentation exam figure exam or presentation about the strategic consequences of your own actions where you need to consider the effect of decisions by others is precisely the kind of reasoning that game theory is designed to facilitate so before moving on to the actual outcome of this exam or presentation scenario it is useful to introduce some of the basic definitions of game theory and then continue the discussion in this language basic ingredients of a game the situation we ve just described is an example of a game for our purposes a game is any situation with the following three aspects i there is a set of participants whom we call the players in our example you and your partner are the two players ii each player has a set of options for how to behave we will refer to these as the player possible strategies in the example you and your partner each have two possible strategies to prepare for the presentation or to study for the exam iii for each choice of strategies each player receives a payoff that can depend on the strategies selected by everyone the payoffs will generally be numbers with each player preferring larger payoffs to smaller payoffs in our current example the payoff to each player is the average grade he or she gets on the exam and the presentation we will generally write the payoffs in a payoff matrix as in figure our interest is in reasoning about how players will behave in a given game for now we focus on games with only two players but the ideas apply equally well to games with any number of players also we will focus on simple one shot games games in which the players simultaneously and independently choose their actions and they do so only once in section at the end of this chapter we discuss how to reinterpret the theory to deal with dynamic games in which actions can be played sequentially over time reasoning about behavior in a game once we write down the description of a game consisting of the players the strategies and the payoffs we can ask how the players are likely to behave that is how they will go about selecting strategies underlying assumptions in order to make this question tractable we will make a few assumptions first we assume everything that a player cares about is summarized in the player payoffs in the exam or presentation game described in section this means that the two players are solely concerned with maximizing their own average grade however nothing in the framework of game theory requires that players care only about personal rewards for example a player who is altruistic may care about both his or her own benefits and the other player benefit if so then the payoffs should reflect this once the payoffs have been defined they should constitute a complete description of each player happiness with each of the possible outcomes of the game we also assume that each player knows everything about the structure of the game to begin with this means that each player knows his or her own list of possible strategies it seems reasonable in many settings to assume that each player also knows who the other player is in a two player game the strategies available to this other player and what his or her payoff will be for any choice of strategies in the exam or presentation game this corresponds to the assumption that you realize you and your partner are each faced with the choice of studying for the exam or preparing for the presentation and you have an accurate estimate of the expected outcome under different courses of action there is considerable research on how to analyze games in which the players have much less knowledge about the underlying structure and in fact john harsanyi shared the nobel prize in economics for his work on games with incomplete information finally we suppose that each individual chooses a strategy to maximize her own payoff given her beliefs about the strategy used by the other player this model of individual behavior which is usually called rationality actually combines two ideas the first idea is that each player wants to maximize her own payoff since the individual payoff is defined to be whatever the individual cares about this hypothesis seems reasonable the second idea is that each player actually succeeds in selecting the optimal strategy in simple settings and for games played by experienced players this too seems reasonable in complex games or for games played by inexperienced players it is surely less reasonable it is interesting to consider players who make mistakes and learn from the play of the game there is an extensive literature which analyzes problems of this sort but we will not consider these issues here reasoning about behavior in the exam or presentation game let go back to the exam or presentation game and ask how we should expect you and your partner the two players in the game to behave we first focus on this from your point of view the reasoning for your partner will be symmetric since the game looks the same from his point of view it would be easier to decide what to do if you could predict what your partner would do but to begin with let consider what you should do for each possible choice of strategy by your partner first if you knew your partner was going to study for the exam then you would get a payoff of by also studying and a payoff of only by preparing for the presentation so in this case you should study for the exam on the other hand if you knew that your partner was going to prepare for the pre sentation then you d get a payoff of by also preparing for the presentation but a payoff of by studying for the exam so in this case too you should study for the exam this approach of considering each of your partner options separately turns out to be a very useful way of analyzing the present situation it reveals that no matter what your partner does you should study for the exam when a player has a strategy that is strictly better than all other options regardless of what the other player does we will refer to it as a strictly dominant strategy when a player has a strictly dominant strategy we should expect that they will definitely play it in the exam or presentation game studying for the exam is also a strictly dominant strategy for your partner by the same reasoning and so we should expect that the outcome will be for both of you to study each getting an average grade of so this game has a very clean analysis and it easy to see how to end up with a prediction for the outcome despite this there something striking about the conclusion if you and your partner could somehow agree that you would both prepare for the presentation you would each get an average grade of in other words you would each be better off but despite the fact that you both understand this this payoff of cannot be achieved by rational play the reasoning above makes it clear why not even if you were to personally commit to preparing for the presentation hoping to achieve the outcome where you both get and even if your partner knew you were doing this your partner would still have an incentive to study for the exam so as to achieve a still higher payoff of for himself this result depends on our assumption that the payoffs truly reflect everything each player values in the outcome in this case that you and your partner only care about maximizing your own average grade if for example you cared about the grade that your partner received as well then the payoffs in this game would look different and the outcome could be different similarly if you cared about the fact that your partner will be angry at you for not preparing for the joint presentation then this too should be incorporated into the payoffs again potentially affecting the results but with the payoffs as they are we are left with the interesting situation where there is an outcome that is better for both of you an average grade of each and yet it cannot be achieved by rational play of the game a related story the prisoner dilemma the outcome of the exam or presentation game is closely related to one of the most famous examples in the development of game the ory the prisoner dilemma here is how this example works suppose that two suspects have been apprehended by the police and are being interro gated in separate rooms the police strongly suspect that these two individuals are respon sible for a robbery but there is not enough evidence to convict either of them of the robbery however they both resisted arrest and can be charged with that lesser crime which would carry a one year sentence each of the suspects is told the following story if you confess and your partner doesn t confess then you will be released and your partner will be charged with the crime your confession will be sufficient to convict him of the robbery and he will be sent to prison for years if you both confess then we don t need either of you to testify against the other and you will both be convicted of the robbery although in this case your sentence will be less years only because of your guilty plea finally if neither of you confesses then we can t convict either of you of the robbery so we will charge each of you with resisting arrest your partner is being offered the same deal do you want to confess to formalize this story as a game we need to identify the players the possible strategies and the payoffs the two suspects are the players and each has to choose between two possi ble strategies confess c or not confess n c finally the payoffs can be summarized from the story above as in figure note that the payoffs are all or less since there are no good outcomes for the suspects only different gradations of bad outcomes suspect n c c suspect n c c figure prisoner dilemma as in the exam or presentation game we can consider how one of the suspects say suspect should reason about his options if suspect were going to confess then suspect would receive a payoff of by confessing and a payoff of by not confessing so in this case suspect should confess if suspect were not going to confess then suspect would receive a payoff of by confessing and a payoff of by not confessing so in this case too suspect should confess so confessing is a strictly dominant strategy it is the best choice regardless of what the other player chooses as a result we should expect both suspects to confess each getting a payoff of we therefore have the same striking phenomenon as in the exam or presentation game there is an outcome that the suspects know to be better for both of them in which they both choose not to confess but under rational play of the game there is no way for them to achieve this outcome instead they end up with an outcome that is worse for both of them and here too it is important that the payoffs reflect everything about the outcome of the game if for example the suspects could credibly threaten each other with retribution for confessing thereby making confessing a less desirable option then this would affect the payoffs and potentially the outcome interpretations of the prisoner dilemma the prisoner dilemma has been the subject of a huge amount of literature since its introduction in the early since it serves as a highly streamlined depiction of the difficulty in establishing cooperation in the face of individual self interest while no model this simple can precisely capture complex scenarios in the real world the prisoner dilemma has been used as an interpretive framework for many different real world situations for example the use of performance enhancing drugs in professional sports has been modeled as a case of the prisoner dilemma game here the athletes are the players and the two possible strategies are to use performance enhancing drugs or not if you use drugs while your opponent doesn t you ll get an advantage in the competition but you ll suffer long term harm and may get caught if we consider a sport where it is difficult to detect the use of such drugs and we assume athletes in such a sport view the downside as a smaller factor than the benefits in competition we can capture the situation with numerical payoffs that might look as follows the numbers are arbitrary here we are only interested in their relative sizes athlete don t use drugs use drugs athlete don t use drugs use drugs figure performance enhancing drugs here the best outcome with a payoff of is to use drugs when your opponent doesn t since then you maximize your chances of winning however the payoff to both using drugs is worse than the payoff to both not using drugs since in both cases you re evenly matched but in the former case you re also causing harm to yourself we can now see that using drugs is a strictly dominant strategy and so we have a situation where the players use drugs even though they understand that there a better outcome for both of them more generally situations of this type are often referred to as arms races in which two competitors use an increasingly dangerous arsenal of weapons simply to remain evenly matched in the example above the performance enhancing drugs play the role of the weapons but the prisoner dilemma has also been used to interpret literal arms races between opposing nations where the weapons correspond to the nations military arsenals to wrap up our discussion of the prisoner dilemma we should note that it only arises when the payoffs are aligned in a certain way as we will see in the remainder of the chapter there are many situations where the structure of the game and the resulting behavior looks very different indeed even simple changes to a game can change it from an instance of the prisoner dilemma to something more benign for example returning to the exam or presentation game suppose that we keep everything the same as before except that we make the exam much easier so that you ll get a on it if you study and a if you don t then we can check that the payoff matrix now becomes you presentation exam your partner presentation exam 96 figure exam or presentation game with an easier exam furthermore we can check that with these new payoffs preparing for the presentation now becomes a strictly dominant strategy so we can expect that both players will play this strategy and both will benefit from this decision the downsides of the previous scenario no longer appear like other dangerous phenomena the prisoner dilemma only manifests itself when the conditions are right best responses and dominant strategies in reasoning about the games in the previous section we used two fundamental concepts that will be central to our discussion of game theory as such it is useful to define them carefully here and then to look further at some of their implications the first concept is the idea of a best response it is the best choice of one player given a belief about what the other player will do for instance in the exam or presentation game we determined your best choice in response to each possible choice of your partner we can make this precise with a bit of notation as follows if s is a strategy chosen by player and t is a strategy chosen by player then there is an entry in the payoff matrix corresponding to the pair of chosen strategies s t we will write s t to denote the payoff to player as a result of this pair of strategies and s t to denote the payoff to player as a result of this pair of strategies now we say that a strategy s for player is a best response to a strategy t for player if s produces at least as good a payoff as any other strategy paired with t s t st t for all other strategies st of player naturally there is a completely symmetric definition for player which we won t write down here in what follows we ll present the definitions from player point of view but there are direct analogues for player in each case notice that this definition allows for multiple different strategies of player to be tied as the best response to strategy t this can make it difficult to predict which of these multiple different strategies player will use we can emphasize that one choice is uniquely the best by saying that a strategy s of player is a strict best response to a strategy t for player if s produces a strictly higher payoff than any other strategy paired with t s t st t for all other strategies st of player when a player has a strict best response to t this is clearly the strategy she should play when faced with t the second concept which was central to our analysis in the previous section is that of a strictly dominant strategy we can formulate its definition in terms of best responses as follows we say that a dominant strategy for player is a strategy that is a best response to every strategy of player we say that a strictly dominant strategy for player is a strategy that is a strict best response to every strategy of player in the previous section we made the observation that if a player has a strictly dominant strategy then we can expect him or her to use it the notion of a dominant strategy is slightly weaker since it can be tied as the best option against some opposing strategies as a result a player could potentially have multiple dominant strategies in which case it may not be obvious which one should be played the analysis of the prisoner dilemma was facilitated by the fact that both players had strictly dominant strategies and so it was easy to reason about what was likely to happen but most settings won t be this clear cut and we now begin to look at games which lack strictly dominant strategies a game in which only one player has a strictly dominant strategy as a first step let consider a setting in which one player has a strictly dominant strategy and the other one doesn t as a concrete example we consider the following story suppose there are two firms that are each planning to produce and market a new product these two products will directly compete with each other let imagine that the population of consumers can be cleanly divided into two market segments people who would only buy a low priced version of the product and people who would only buy an upscale version let also assume that the profit any firm makes on a sale of either a low price or an upscale product is the same so to keep track of profits it good enough to keep track of sales each firm wants to maximize its profit or equivalently its sales and in order to do this it has to decide whether its new product will be low priced or upscale so this game has two players firm and firm and each has two possible strategies to produce a low priced product or an upscale one to determine the payoffs here is how the firms expect the sales to work out first people who would prefer a low priced version account for of the population and people who would prefer an upscale version account for of the population firm is the much more popular brand and so when the two firms directly compete in a market segment firm gets of the sales and firm gets of the sales if a firm is the only one to produce a product for a given market segment it gets all the sales based on this we can determine payoffs for different choices of strategies as follows if the two firms market to different market segments they each get all the sales in that segment so the one that targets the low priced segment gets a payoff and the one that targets the upscale segment gets if both firms target the low priced segment then firm gets of it for a payoff of and firm gets of it for a payoff of analogously if both firms target the upscale segment then firm gets a payoff of and firm gets a payoff of this can be summarized in the following payoff matrix firm low priced upscale firm low priced upscale 08 figure marketing strategy notice that in this game firm has a strictly dominant strategy for firm low priced is a strict best response to each strategy of firm on the other hand firm does not have a dominant strategy low priced is its best response when firm plays upscale and upscale is its best response when firm plays low priced still it is not hard to make a prediction about the outcome of this game since firm has a strictly dominant strategy in low priced we can expect it will play it now what should firm do if firm knows firm payoffs and knows that firm wants to maximize profits then firm can confidently predict that firm will play low priced then since upscale is the strict best response by firm to low priced we can predict that firm will play upscale so our overall prediction of play in this marketing game is low priced by firm and upscale by firm resulting in payoffs of and respectively note that although we re describing the reasoning in two steps first the strictly dom inant strategy of firm and then the best response of firm this is still in the context of a game where the players move simultaneously both firms are developing their marketing strategies concurrently and in secret it is simply that the reasoning about strategies natu rally follows this two step logic resulting in a prediction about how the simultaneous play will occur it also interesting to note the intuitive message of this prediction firm is so strong that it can proceed without regard to firm decision given this firm best strategy is to stay safely out of the way of firm finally we should also note how the marketing strategy game makes use of the knowl edge we assume players have about the game being played and about each other in particu lar we assume that each player knows the entire payoff matrix and in reasoning about this specific game it is important that firm knows that firm wants to maximize profits and that firm knows that firm knows its own profits in general we will assume that the players have common knowledge of the game they know the structure of the game they know that each of them know the structure of the game they know that each of them know that each of them know and so on while we will not need the full technical content of common knowledge in anything we do here it is an underlying assumption and a topic of research in the game theory literature as mentioned earlier it is still possible to analyze games in situations where common knowledge does not hold but the analysis becomes more complex it also worth noting that the assumption of common knowledge is a bit stronger than we need for reasoning about simple games such as the prisoner dilemma where strictly dominant strategies for each player imply a particular course of action regardless of what the other player is doing nash equilibrium when neither player in a two player game has a strictly dominant strategy we need some other way of predicting what is likely to happen in this section we develop methods for doing this the result will be a useful framework for analyzing games in general nash equilibrium an example a three client game to frame the question it helps to think about a simple example of a game that lacks strictly dominant strategies like our previous example it will be a marketing game played between two firms however it has a slightly more intricate set up suppose there are two firms that each hope to do business with one of three large clients a b and c each firm has three possible strategies whether to approach a b or c the results of their two decisions will work out as follows if the two firms approach the same client then the client will give half its business to each firm is too small to attract business on its own so if it approaches one client while firm approaches a different one then firm gets a payoff of if firm approaches client b or c on its own it will get their full business however a is a larger client and will only do business with the firms if both approach a because a is a larger client doing business with it is worth and hence to each firm if it split while doing business with b or c is worth and hence to each firm if it split from this description we can work out the following payoff matrix a firm b c firm a b c figure three client game if we study how the payoffs in this game work we see that neither firm has a dominant strategy indeed each strategy by each firm is a strict best response to some strategy by the other firm for firm a is a strict best response to strategy a by firm b is a strict best response to b and c is a strict best response to c for firm a is a strict best response to strategy a by firm c is a strict best response to b and b is a strict best response to c so how should we reason about the outcome of play in this game defining nash equilibrium in john nash proposed a simple but powerful prin ciple for reasoning about behavior in general games and its underlying premise is the following even when there are no dominant strategies we should expect players to use strategies that are best responses to each other more precisely suppose that player chooses a strategy s and player chooses a strategy t we say that this pair of strategies s t is a nash equilibrium if s is a best response to t and t is a best response to s this is not a concept that can be derived purely from rationality on the part of the players instead it is an equilibrium concept the idea is that if the players choose strategies that are best responses to each other then no player has an incentive to deviate to an alternative strategy so the system is in a kind of equilibrium state with no force pushing it toward a different outcome nash shared the nobel prize in economics for his development and analysis of this idea to understand the idea of nash equilibrium we should first ask why a pair of strategies that are not best responses to each other would not constitute an equilibrium the answer is that the players cannot both believe that these strategies will be actually used in the game as they know that at least one player would have an incentive to deviate to another strategy so nash equilibrium can be thought of as an equilibrium in beliefs if each player believes that the other player will actually play a strategy that is part of a nash equilibrium then she is willing to play her part of the nash equilibrium let consider the three client game from the perspective of nash equilibrium if firm chooses a and firm chooses a then we can check that firm is playing a best response to firm strategy and firm is playing a best response to firm strategy hence the pair of strategies a a forms a nash equilibrium moreover we can check that this is the only nash equilibrium no other pair of strategies are best responses to each other this discussion also suggests two ways to find nash equilibria the first is to simply check all pairs of strategies and ask for each one of them whether the individual strategies are best responses to each other the second is to compute each player best response to each strategy of the other player and then find strategies that are mutual best responses multiple equilibria coordination games for a game with a single nash equilibrium such as the three client game in the previ ous section it seems reasonable to predict that the players will play the strategies in this equilibrium under any other play of the game at least one player will not be using a best response to what the other is doing some natural games however can have more than one nash equilibrium and in this case it becomes difficult to predict how rational players will actually behave in the game we consider some fundamental examples of this problem here a coordination game a simple but central example is the following coordination game which we can motivate through the following story suppose you and a partner are this discussion each player only has three available strategies a b or c later in this we will introduce the possibility of more complex strategies in which players can randomize over their available options with this more complex formulation of possible strategies we will find additional equilibria for the three client game each preparing slides for a joint project presentation you can t reach your partner by phone and need to start working on the slides now you have to decide whether to prepare your half of the slides in powerpoint or in apple keynote software either would be fine but it will be much easier to merge your slides together with your partner if you use the same software so we have a game in which you and your partner are the two players choosing power point or choosing keynote form the two strategies and the payoffs are as shown in figure you powerpoint keynote your partner powerpoint keynote figure coordination game this is called a coordination game because the two players shared goal is really to coordinate on the same strategy there are many settings in which coordination games arise for example two manufacturing companies that work together extensively need to decide whether to configure their machinery in metric units of measurement or english units of measurement two platoons in the same army need to decide whether to attack an enemy left flank or right flank two people trying to find each other in a crowded mall need to decide whether to wait at the north end of the mall or at the south end in each case either choice can be fine provided that both participants make the same choice the underlying difficulty is that the game has two nash equilibria i e power point powerpoint and keynote keynote in our example from figure if the players fail to coordinate on one of the nash equilibria perhaps because one player expects power point to be played and the other expects keynote then they receive low payoffs so what do the players do this remains a subject of considerable discussion and research but some proposals have received attention in the literature thomas schelling introduced the idea of a focal point as a way to resolve this difficulty he noted that in some games there are natural reasons possibly outside the payoff structure of the game that cause the players to focus on one of the nash equilibria for example suppose two drivers are approaching each other at night on an undivided country road each driver has to decide whether to move over to the left or the right if the drivers coordinate making the same choice of side then they pass each other but if they fail to coordinate then they get a severely low payoff due to the resulting collision fortunately social convention can help the drivers decide what to do in this case if this game is being played in the u s convention strongly suggests that they should move to the right while if the game is being played in england convention strongly suggests that they should move to the left in other words social conventions while often arbitrary can sometimes be useful in helping people coordinate among multiple equilibria variants on the basic coordination game one can enrich the structure of our basic coordination game to capture a number of related issues surrounding the problem of mul tiple equilibria to take a simple extension of our previous example suppose that both you and your project partner each prefer keynote to powerpoint you still want to coordinate but you now view the two alternatives as unequal this gives us the payoff matrix for an unbalanced coordination game shown in figure you powerpoint keynote your partner powerpoint keynote figure unbalanced coordination game notice that powerpoint powerpoint and keynote keynote are still both nash equi libria for this game despite the fact that one of them gives higher payoffs to both players the point is that if you believe your partner will choose powerpoint you still should choose powerpoint as well here schelling theory of focal points suggests that we can use a feature intrinsic to the game rather than an arbitrary social convention to make a prediction about which equilibrium will be chosen by the players that is we can predict that when the players have to choose they will select strategies so as to reach the equilib rium that gives higher payoffs to both of them to take another example consider the two people trying to meet at a crowded mall if the north end of the mall has a bookstore they both like while the south end consists of a loading dock the natural focal point would be the equilibrium in which they both choose the north end things get more complicated if you and your partner don t agree on which software you prefer as shown in the payoff matrix of figure you powerpoint keynote your partner powerpoint keynote figure battle of the sexes in this case the two equilibria still correspond to the two different ways of coordinating but your payoff is higher in the keynote keynote equilibrium while your partner payoff is higher in the powerpoint powerpoint equilibrium this game is traditionally called the battle of the sexes because of the following motivating story a husband and wife want to see a movie together and they need to choose between a romantic comedy and an action movie they want to coordinate on their choice but the romance romance equilibrium gives a higher payoff to one of them while the action action equilibrium gives a higher payoff to the other in battle of the sexes it can be hard to predict the equilibrium that will be played using either the payoff structure or some purely external social convention rather it helps to know something about conventions that exist between the two players themselves suggesting how they resolve disagreements when they prefer different ways of coordinating it worth mentioning one final variation on the basic coordination game which has attracted attention in recent years this is the stag hunt game the name is motivated by the following story from writings of rousseau suppose that two people are out hunting if they work together they can catch a stag which would be the highest payoff outcome but on their own each can catch a hare the tricky part is that if one hunter tries to catch a stag on his own he will get nothing while the other one can still catch a hare thus the hunters are the two players their strategies are hunt stag and hunt hare and the payoffs are as shown in figure hunter hunt stag hunt hare hunter hunt stag hunt hare figure stag hunt this is quite similar to the unbalanced coordination game except that if the two players miscoordinate the one who was trying for the higher payoff outcome gets penalized more than the one who was trying for the lower payoff outcome in fact the one trying for the lower payoff outcome doesn t get penalized at all as a result the challenge in reasoning about which equilibrium will be chosen is based on the trade off between the high payoff of one and the low downside of miscoordination from the other it has been argued that the stag hunt game captures some of the intuitive challenges that are also raised by the prisoner dilemma the structures are clearly different since the prisoner dilemma has strictly dominant strategies both however have the property that players can benefit if they cooperate with each other but risk suffering if they try cooperating while their partner doesn t another way to see some of the similarities between the two games is to notice that if we go back to the original exam or presentation game and make one small change then we end up changing it from an instance of prisoner dilemma to something closely resembling stag hunt specifically suppose that we keep the grade outcomes the same as in section except that we require both you and your partner to prepare for the presentation in order to have any chance of a better grade that is if you both prepare you both get a on the presentation but if at most one of you prepares you both get the base grade of with this change the payoffs for the exam or presentation game become what is shown in figure you presentation exam your partner presentation exam figure exam or presentation game stag hunt version we now have a structure that closely resembles the stag hunt game coordinating on presentation presentation or exam exam are both equilibria but if you attempt to go for the higher payoff equilibrium you risk getting a low grade if your partner opts to study for the exam multiple equilibria the hawk dove game multiple nash equilibria also arise in a different but equally fundamental kind of game in which the players engage in a kind of anti coordination activity probably the most basic form of such a game is the hawk dove game which is motivated by the following story suppose two animals are engaged in a contest to decide how a piece of food will be divided between them each animal can choose to behave aggressively the hawk strategy or passively the dove strategy if the two animals both behave passively they divide the food evenly and each get a payoff of if one behaves aggressively while the other behaves passively then the aggressor gets most of the food obtaining a payoff of while the passive one only gets a payoff of but if both animals behave aggressively then they destroy the food and possibly injure each other each getting a payoff of thus we have the payoff matrix in figure animal d h animal d h figure hawk dove game this game has two nash equilibria d h and h d without knowing more about the animals we cannot predict which of these equilibria will be played so as in the coordination games we looked at earlier the concept of nash equilibrium helps to narrow down the set of reasonable predictions but it does not provide a unique prediction the hawk dove game has been studied in many contexts for example suppose we sub stitute two countries for the two animals and suppose that the countries are simultaneously choosing whether to be aggressive or passive in their foreign policy each country hopes to gain through being aggressive but if both act aggressively they risk actually going to war which would be disastrous for both so in equilibrium we can expect that one will be aggressive and one will be passive but we can t predict who will follow which strategy again we would need to know more about the countries to predict which equilibrium will be played hawk dove is another example of a game that can arise from a small change to the payoffs in the exam or presentation game let again recall the set up from the opening section and now vary things so that if neither you nor your partner prepares for the presentation you will get a very low joint grade of if one or both of you prepare the grades for the presentation are the same as before if we compute the average grades you get for different choices of strategies in this version of the game we have the payoffs in figure you presentation exam your partner presentation exam figure exam or presentation hawk dove version in this version of the game there are two equilibria presentation exam and exam presentation essentially one of you must behave passively and prepare for the presentation while the other achieves the higher payoff by studying for the exam if you both try to avoid the role of the passive player you end up with very low payoffs but we cannot predict from the structure of the game alone who will play this passive role the hawk dove game is also known by a number of other names in the game theory literature for example it is frequently referred to as the game of chicken to evoke the image of two teenagers racing their cars toward each other daring each other to be the one to swerve out of the way the two strategies here are swerve and don t swerve the one who swerves first suffers humiliation from his friends but if neither swerves then both suffer an actual collision mixed strategies in the previous two sections we have been discussing games whose conceptual complexity comes from the existence of multiple equilibria however there are also games which have no nash equilibria at all for such games we will make predictions about players behavior by enlarging the set of strategies to include the possibility of randomization once players are allowed to behave randomly one of john nash main results establishes that equilibria always exist probably the simplest class of games to expose this phenomenon are what might be called attack defense games in such games one player behaves as the attacker while the other behaves as the defender the attacker can use one of two strategies let call them a and b while the defender two strategies are defend against a or defend against b if the defender defends against the attack the attacker is using then the defender gets the higher payoff but if the defender defends against the wrong attack then the attacker gets the higher payoff matching pennies a simple attack defense game is called matching pennies and is based on a game in which two people each hold a penny and simultaneously choose whether to show heads h or tails t on their penny player loses his penny to player if they match and wins player penny if they don t match this produces a payoff matrix as shown in figure player h t player h t figure matching pennies matching pennies is a simple example of a large class of interesting games with the property that the payoffs of the players sum to zero in every outcome such games are called zero sum games and many attack defense games and more generally games where the players interests are in direct conflict have this structure games like matching pennies have in fact been used as metaphorical descriptions of decisions made in combat for example the allied landing in europe on june one of the pivotal moments in world war ii involved a decision by the allies whether to cross the english channel at normandy or at calais and a corresponding decision by the german army whether to mass its defensive forces at normandy or calais this has an attack defense structure that closely resembles the matching pennies game the first thing to notice about matching pennies is that there is no pair of strategies that are best responses to each other to see this observe that for any pair of strategies one of the players gets a payoff of and this player would improve his or her payoff to by switching strategies so for any pair of strategies one of the players wants to switch what they re doing although it not crucial for the discussion here it interesting to note that the three client game used as an example in section can be viewed intuitively as a kind of hybrid of the matching pennies game and the stag hunt game if we look just at how the two players evaluate the options of approaching clients b and c we have matching pennies firm wants to match while firm wants to not match however if they coordinate on approaching client a then they both get even higher payoffs this means that if we treat each player as simply having the two strategies h or t then there is no nash equilibrium for this game this is not so surprising if we consider how matching pennies works a pair of strategies one for each player forms a nash equilibrium if even given knowledge of each other strategies neither player would have an incentive to switch to an alternate strategy but in matching pennies if player knows that player is going to play a particular choice of h or t then player can exploit this by choosing the opposite and receiving a payoff of analogous reasoning holds for player when we think intuitively about how games of this type are played in real life we see that players generally try to make it difficult for their opponents to predict what they will play this suggests that in our modeling of a game like matching pennies we shouldn t treat the strategies as simply h or t but as ways of randomizing one behavior between h and t we now see how to build this into a model for the play of this kind of game mixed strategies the simplest way to introduce randomized behavior is to say that each player is not actually choosing h or t directly but rather is choosing a probability with which she will play h so in this model the possible strategies for player are numbers p between and a given number p means that player is committing to play h with probability p and t with probability p similarly the possible strategies for player are numbers q between and representing the probability that player will play h since a game consists of a set of players strategies and payoffs we should notice that by allowing randomization we have actually changed the game it no longer consists of two strategies by each player but instead a set of strategies corresponding to the interval of numbers between and we will refer to these as mixed strategies since they involve mixing between the options h and t notice that the set of mixed strategies still includes the original two options of committing to definitely play h or t these two choices correspond to selecting probabilities of or respectively and we will refer to them as the two pure strategies in the game to make things more informal notationally we will sometimes refer to the choice of p by player equivalently as the pure strategy h and similarly for p and q or payoffs from mixed strategies with this new set of strategies we also need to deter mine the new set of payoffs the subtlety in defining payoffs is that they are now random quantities each player will get with some probability and will get with the remain ing probability when payoffs were numbers it was obvious how to rank them bigger was better now that payoffs are random it is not immediately obvious how to rank them we want a principled way to say that one random outcome is better than another to think about this issue let start by considering matching pennies from player analogously to the two hunters coordinating to hunt stag point of view and focus first on how she evaluates her two pure strategies of definitely playing h or definitely playing t suppose that player chooses the strategy q that is he commits to playing h with probability q and t with probability q then if player chooses pure strategy h she receives a payoff of with probability q since the two pennies match with probability q in which event she loses and she receives a payoff of with probability q since the two pennies don t match with probability q alternatively if player chooses pure strategy t she receives with probability q and with probability q so even if player uses a pure strategy her payoffs can still be random due to the randomization employed by player how should we decide which of h or t is more appealing to player in this case in order to rank random payoffs numerically we will attach a number to each distribution that represents how attractive this distribution is to the player once we have done this we can then rank outcomes according to their associated number the number we will use for this purpose is the expected value of the payoff so for example if player chooses the pure strategy h while player chooses a probability of q as above then the expected payoff to player is q q similarly if player chooses the pure strategy t while player chooses a probability of q then the expected payoff to player is q q we will assume players are seeking to maximize the expected payoff they get from a choice of mixed strategies although the expectation is a natural quantity it is a subtle question whether maximizing expectation is a reasonable modeling assumption about the behavior of players by now however there is a well established foundation for the assumption that players rank distributions over payoffs according to their expected values and so we will follow it here we have now defined the mixed strategy version of the matching pennies game strategies are probabilities of playing h and payoffs are the expectations of the payoffs from the four pure outcomes h h h t t h and t t we can now ask whether there is a nash equilibrium for this richer version of the game equilibrium with mixed strategies we define a nash equilibrium for the mixed strategy version just as we did for the pure strategy version it is a pair of strategies now probabilities so that each is a best response to the other first let observe that no pure strategy can be part of a nash equilibrium this is equivalent to the reasoning we did at the outset of this section suppose for example that the pure strategy h i e probability p by player were part of a nash equilibrium then player unique best response would be the pure strategy h as well since player gets whenever he matches but h by player is not a best response to h by player so in fact this couldn t be a nash equilibrium analogous reasoning applies to the other possible pure strategies by the two players so we reach the natural conclusion that in any nash equilibrium both players must be using probabilities that are strictly between and next let ask what player best response should be to the strategy q used by player above we determined that the expected payoff to player from the pure strategy h in this case is while the expected payoff to player from the pure strategy t is now here the key point if then one of the pure strategies h or t is in fact the unique best response by player to a play of q by player this is simply because one of or is larger in this case and so there is no point for player to put any probability on her weaker pure strategy but we already established that pure strategies cannot be part of any nash equilibrium for matching pennies and because pure strategies are the best responses whenever probabilities that make these two expectations unequal cannot be part of a nash equilibrium either so we ve concluded that in any nash equilibrium for the mixed strategy version of match ing pennies we must have 2q or in other words q the situation is symmetric when we consider things from player point of view and evaluate the payoffs from a play of probability p by player we conclude from this that in any nash equilibrium we must also have p thus the pair of strategies p and q is the only possibility for a nash equilibrium we can check that this pair of strategies in fact do form best responses to each other as a result this is the unique nash equilibrium for the mixed strategy version of matching pennies interpreting the mixed strategy equilibrium for matching pennies having de rived the nash equilibrium for this game it useful to think about what it means and how we can apply this reasoning to games in general first let picture a concrete setting in which two people actually sit down to play matching pennies and each of them actually commits to behaving randomly according to probabilities p and q respectively if player believes that player will play h strictly more than half the time then she should definitely play t in which case player should not be playing h more than half the time the symmetric reasoning applies if player believes that player will play t strictly more than half the time in neither case would we have a nash equilibrium so the point is that the choice of q by player makes player indifferent between playing h or t the strategy q is effectively non exploitable by player this was in fact our original intuition for introducing randomization each player wants their behavior to be unpredictable to the other so that their behavior can t be taken advantage of we should note that the fact that both probabilities turned out to be is a result of the highly symmetric structure of matching pennies as we will see in subsequent examples in the next section when the payoffs are less symmetric the nash equilibrium can consist of unequal probabilities this notion of indifference is a general principle behind the computation of mixed strategy equilibria in two player two strategy games when there are no equilibria involving pure strategies each player should randomize so as to make the other player indifferent between their two alternatives this way neither player behavior can be exploited by a pure strategy and the two choices of probabilities are best responses to each other and although we won t pursue the details of it here a generalization of this principle applies to games with any finite number of players and any finite number of strategies nash main mathematical result accompanying his definition of equilibrium was to prove that every such game has at least one mixed strategy equilibrium it also worth thinking about how to interpret mixed strategy equilibria in real world situations there are in fact several possible interpretations that are appropriate in different situations sometimes particularly when the participants are genuinely playing a sport or game the players may be actively randomizing their actions a tennis player may be randomly deciding whether to serve the ball up the center or out to the side of the court a card player may be randomly deciding whether to bluff or not two children may be randomizing among rock paper and scissors in the perennial elementary school contest of the same name we will look at examples of this in the next section sometimes the mixed strategies are better viewed as proportions within a population suppose for example that two species of animals in the process of foraging for food regularly engage in one on one attack defense games with the structure of matching pennies here a single member of the first species always plays the role of attacker and a single member of the second species always plays the role of defender let suppose that each individual animal is genetically hard wired to always play h or always play t and suppose further that the population of each species consists half of animals hard wired to play h and half of animals hard wired to play t then with this population mixture h animals in each species do exactly as well on average over many random interactions as t animals hence the population as a whole is in a kind of mixed equilibrium even though each individual is playing a pure strategy this story suggests an important link with evolutionary biology which has in fact been developed through a long line of research this topic will be our focus in chapter maybe the most subtle interpretation is based on recalling from section that nash equilibrium is often best thought of as an equilibrium in beliefs if each player believes that her partner will play according to a particular nash equilibrium then she too will want to play according to it in the case of matching pennies with its unique mixed equilibrium this means that it is enough for you to expect that when you meet an arbitrary person they will play their side of matching pennies with a probability of in this case playing a probability of makes sense for you too and hence this choice of probabilities is self reinforcing it is in equilibrium across the entire population mixed strategies examples and empirical anal ysis because mixed strategy equilibrium is a subtle concept it useful to think about it through further examples we will focus on two main examples both drawn from the realm of sports and both with attack defense structures the first is stylized and partly metaphorical while the second represents a striking empirical test of whether people in high stakes situations actually follow the predictions of mixed strategy equilibrium we conclude the section with a general discussion of how to identify all the equilibria of a two player two strategy game the run pass game first let consider a streamlined version of the problem faced by two american football teams as they plan their next play in a football game the offense can choose either to run or to pass and the defense can choose either to defend against the run or to defend against the pass here is how the payoffs work if the defense correctly matches the offense play then the offense gains yards if the offense runs while the defense defends against the pass the offense gains yards if the offense passes while the defense defends against the run the offense gains yards hence we have the payoff matrix shown in figure if you don t know the rules of american football you can follow the discussion simply by taking the payoff matrix as self contained intuitively the point is simply that we have offense pass run defense defend pass defend run figure run pass game an attack defense game with two players named offense and defense respectively and where the attacker has a stronger option pass and a weaker option run just as in matching pennies it easy to check that there is no nash equilibrium where either player uses a pure strategy both have to make their behavior unpredictable by ran domizing so let work out a mixed strategy equilibrium for this game let p be the prob ability that the offense passes and let q be the probability that the defense defends against the pass we know from nash result that at least one mixed strategy equilibrium must exist but not what the actual values of p and q should be we use the principle that a mixed equilibrium arises when the probabilities used by each player makes his opponent indifferent between his two options first suppose the defense chooses a probability of q for defending against the pass then the expected payoff to the offense from passing is q q while the expected payoff to the offense from running is q q to make the offense indifferent between its two strategies we need to set and hence q next suppose the offense chooses a probability of p for passing then the expected payoff to the defense from defending against the pass is p p with the expected payoff to the defense from defending against the run is p p to make the defense indifferent between its two strategies we need to set and hence p thus the only possible probability values that can appear in a mixed strategy equilibrium are p for the offense and q for the defense and this in fact forms an equilibrium notice also that the expected payoff to the offense with these probabilities is and the corresponding expected payoff to the defense is also in contrast to matching pennies notice that because of the asymmetric structure of the payoffs here the probabilities that appear in the mixed strategy equilibrium are unbalanced as well strategic interpretation of the run pass game there are several things to notice about this equilibrium first the strategic implications of the equilibrium probabilities are intriguing and a bit subtle specifically although passing is the offense more powerful weapon it uses it less than half the time it places only probability p on passing this initially seems counter intuitive why not spend more time using your more powerful option but the calculation that gave us the equilibrium probabilities also supplies the answer to this question if the offense placed any higher probability on passing then the defense best response would be to always defend against the pass and the offense would actually do worse in expectation we can see how this works by trying a larger value for p like p in this case the defense will always defend against the pass and so the offense expected payoff will be since it gains half the time and the other half the time above we saw that with the equilibrium probabilities the offense has an expected payoff of moreover because p makes the defense indifferent between its two strategies an offense that uses p is guaranteed to get no matter what the defense does one way to think about the real power of passing as a strategy is to notice that in equilibrium the defense is defending against the pass of the time even though the offense is using it only of the time so somehow the threat of passing is helping the offense even though it uses it relatively rarely this example clearly over simplifies the strategic issues at work in american football there are many more than just two strategies and teams are concerned with more than just their yardage on the very next play nevertheless this type of analysis has been applied quantitatively to statistics from american football verifying some of the main qualitative conclusions at a broad level that teams generally run more than they pass and that the expected yardage gained per play from running is close to the expected yardage gained per play from passing for most teams 84 the penalty kick game the complexity of american football makes it hard to cast it truly accurately as a two person two strategy game we now focus on a different setting also from professional sports where such a formalization can be done much more exactly the modeling of penalty kicks in soccer as a two player game in ignacio palacios huerta undertook a large study of penalty kicks from the per spective of game theory and we focus on his analysis here as he observed penalty kicks capture the ingredients of two player two strategy games remarkably faithfully the kicker can aim the ball to the left or the right of the goal and the goalie can dive to either the left or right as well the ball moves to the goal fast enough that the decisions of the kicker and goalie are effectively being made simultaneously and based on these decisions the kicker is likely to score or not indeed the structure of the game is very much like matching pennies if the goalie dives in the direction where the ball is aimed he has a good chance of blocking it if the goalie dives in the wrong direction it is very likely to go in the goal based on an analysis of roughly penalty kicks in professional soccer palacios huerta determined the empirical probability of scoring for each of the four basic outcomes whether the kicker aims left or right and whether the goalie dives left or right this led to a payoff matrix as shown in figure kicker l r goalie l r figure the penalty kick games from empirical data there are a few contrasts to note in relation to the basic matching pennies game first a kicker has a reasonably good chance of scoring even when the goalie dives in the correct direction although a correct choice by the goalie still greatly reduces this probability second kickers are generally right footed and so their chance of scoring is not completely symmetric between aiming left and aiming right despite these caveats the basic premise of matching pennies is still present here there is no equilibrium in pure strategies and so we need to consider how players should random ize their behavior in playing this game using the principle of indifference as in previous examples we see that if q is the probability that a goalie chooses l we need to set q so as to make the kicker indifferent between his two options q q 93 q q solving for q we get q we can do the analogous calculation to obtain the value of p that makes the goalie indifferent obtaining p the striking punchline to this study is that in the dataset of real penalty kicks the goalies dive left a fraction of the time matching the prediction to two decimal places up the center and decisions by the goalie to remain in the center are very rare and can be ignored in a simple version of the analysis purposes of the analysis we take all the left footed kickers in the data and apply a left right reflection to all their actions so that r always denotes the natural side for each kicker and the kickers aim left a fraction of the time coming within of the prediction it is particularly nice to find the theory predictions borne out in a setting such as professional soccer since the two player game under study is being played by experts and the outcome is important enough to the participants that they are investing significant attention to their choice of strategies finding all nash equilibria to conclude our discussion of mixed strategy equilibria we consider the general question of how to find all nash equilibria of a two player two strategy game first it is important to note that a game may have both pure strategy and mixed strategy equilibria as a result one should first check all four pure outcomes given by pairs of pure strategies to see which if any form equilibria then to check whether there are any mixed strategy equilibria we need to see whether there are mixing probabilities p and q that are best responses to each other if there is a mixed strategy equilibrium then we can determine player strategy q from the requirement that player randomizes player will only randomize if his pure strategies have equal expected payoff this equality of expected payoffs for player gives us one equation which we can solve to determine q the same process gives an equation to solve for determining player strategy p if both of the obtained values p and q are strictly between and and are thus legitimate mixed strategies then we have a mixed strategy equilibrium thus far our examples of mixed strategy equilibria have been restricted to games with an attack defense structure and so we have not seen an example exhibiting both pure and mixed equilibria however it is not hard to find such examples in particular coordination and hawk dove games with two pure equilibria will have a third mixed equilibrium in which each player randomizes as an example let consider the unbalanced coordination game from section you powerpoint keynote your partner powerpoint keynote figure unbalanced coordination game suppose that you place a probability of p strictly between and on powerpoint and your partner places a probability of q strictly between and on powerpoint then you ll be indifferent between powerpoint and keynote if q q q q or in other words if q since the situation is symmetric from your partner point of view we also get p thus in addition to the two pure equilibria we also get an equilibrium in which each of you chooses powerpoint with probability note that unlike the two pure equilibria this mixed equilibrium comes with a positive probability that the two of you will miscoordinate but this is still an equilibrium since if you truly believe that your partner is choosing powerpoint with probability and keynote with probability then you ll be indifferent between the two options and will get the same expected payoff however you choose pareto optimality and social optimality in a nash equilibrium each player strategy is a best response to the other player strategies in other words the players are optimizing individually but this doesn t mean that as a group the players will necessarily reach an outcome that is in any sense good the exam or presentation game from the opening section and related games like the prisoner dilemma serve as examples of this we redraw the payoff matrix for the basic exam or presentation game in figure you presentation exam your partner presentation exam 86 92 92 86 88 88 figure exam or presentation it is interesting to classify outcomes in a game not just by their strategic or equilibrium properties but also by whether they are good for society in order to reason about this latter issue we first need a way of making it precise there are two useful candidates for such a definition as we now discuss pareto optimality the first definition is pareto optimality named after the italian economist vilfredo pareto who worked in the late and early a choice of strategies one by each player is pareto optimal if there is no other choice of strategies in which all players receive payoffs at least as high and at least one player receives a strictly higher payoff to see the intuitive appeal of pareto optimality let consider a choice of strategies that is not pareto optimal in this case there an alternate choice of strategies that makes at least one player better off without harming any player in basically any reasonable sense this alternate choice is superior to what currently being played if the players could jointly agree on what to do and make this agreement binding then surely they would prefer to move to this superior choice of strategies pareto optimality and social optimality the motivation here relies crucially on the idea that the players can construct a binding agreement to actually play the superior choice of strategies if this alternate choice is not a nash equilibrium then absent a binding agreement at least one player would want to switch to a different strategy as an illustration of why this is a crucial point consider the outcomes in the exam or presentation game the outcome in which you and your partner both study for the exam is not pareto optimal since the outcome in which you both prepare for the presentation is strictly better for both of you this is the central difficulty at the heart of this example now phrased in terms of pareto optimality it shows that even though you and your partner realize there is a superior solution there is no way to maintain it without a binding agreement between the two of you in this example the two outcomes in which exactly one of you prepares for the pre sentation are also pareto optimal in this case although one of you is doing badly there is no alternate choice of strategies in which everyone is doing at least as well so in fact the exam or presentation game and the prisoner dilemma are examples of games in which the only outcome that is not pareto optimal is the one corresponding to the unique nash equilibrium social optimality a stronger condition that is even simpler to state is social optimality a choice of strategies one by each player is a social welfare maximizer or socially optimal if it maximizes the sum of the players payoffs in the exam or presentation game the social optimum is achieved by the outcome in which both you and your partner prepare for the presentation which produces a combined payoff of 90 90 of course this definition is only appropriate to the extent that it makes sense to add the payoffs of different players together it not always clear that we can meaningfully combine my satisfaction with an outcome and your satisfaction by simply adding them up outcomes that are socially optimal must also be pareto optimal if such an outcome weren t pareto optimal there would be a different outcome in which all payoffs were at least as large and one was larger and this would be an outcome with a larger sum of payoffs on the other hand a pareto optimal outcome need not be socially optimal for example the exam or presentation game has three outcomes that are pareto optimal but only one of these is the social optimum finally of course it not the case that nash equilibria are at odds with the goal of social optimality in every game for example in the version of the exam or presentation game with an easier exam yielding the payoff matrix that we saw earlier in figure the unique nash equilibrium is also the unique social optimum advanced material dominated strategies and dynamic games in this final section we consider two further issues that arise in the analysis of games first we study the role of dominated strategies in reasoning about behavior in a game and find that the analysis of dominated strategies can provide a way to make predictions about play based on rationality even when no player has a dominant strategy second we discuss how to reinterpret the strategies and payoffs in a game to deal with situations in which play actually occurs sequentially through time before doing this however we begin with a formal definition for games that have more than two players a multi player games a multi player game consists as in the two player case of a set of players a set of strategies for each player and a payoff to each player for each possible outcome specifically suppose that a game has n players named n each player has a set of possible strategies an outcome or joint strategy of the game is a choice of a strategy for each player finally each player i has a payoff function pi that maps outcomes of the game to a numerical payoff for i that is for each outcome consisting of strategies sn there is a payoff pi sn to player i now we can say that a strategy si is a best response by player i to a choice of strategies si si sn by all the other players if pi si si si sn pi si sit si sn for all other possible strategies sit available to player i finally an outcome consisting of strategies sn is a nash equilibrium if each strategy it contains is a best response to all the others b dominated strategies and their role in strategic reasoning in sections and we discussed strictly dominant strategies strategies that are a strict best response to every possible choice of strategies by the other players clearly if a player has a strictly dominant strategy then this is the strategy she should employ but we also saw that even for two player two strategy games it is common to have no dominant strategies this holds even more strongly for larger games although dominant and strictly dominant strategies can exist in games with many players and many strategies they are rare figure in the facility location game each player has strictly dominated strategies but no dominant strategy however even if a player does not have a dominant strategy she may still have strategies that are dominated by other strategies in this section we consider the role that such dominated strategies play in reasoning about behavior in games we begin with a formal definition a strategy is strictly dominated if there is some other strategy available to the same player that produces a strictly higher payoff in response to every choice of strategies by the other players in the notation we ve just developed strategy si for player i is strictly dominated if there is another strategy sit for player i such that pi si sit si sn pi si si si sn for all choices of strategies si si sn by the other players now in the two player two strategy games we ve been considering thus far a strategy is strictly dominated precisely when the other strategy available to the same player is strictly dominant in this context it wouldn t make sense to study strictly dominated strategies as a separate concept on their own however if a player has many strategies then it possible for a strategy to be strictly dominated without any strategy being dominant in such cases we will find that strictly dominated strategies can play a very useful role in reasoning about play in a game in particular we will see that there are cases in which there are no dominant strategies but where the outcome of the game can still be uniquely predicted using the structure of the dominated strategies in this way reasoning based on dominated strategies forms an intriguing intermediate approach between dominant strategies and nash equilibrium on the one hand it can be more powerful than reasoning based solely on dominant strategies but on the other hand it still relies only on the premise that players seek to maximize payoffs and doesn t require the introduction of an equilibrium notion to see how this works it useful to introduce the approach in the context of a basic example example the facility location game our example is a game in which two firms compete through their choice of locations suppose that two firms are each planning to open a store in one of six towns located along six consecutive exits on a highway we can represent the arrangement of these towns using a six node graph as in figure now based on leasing agreements firm has the option of opening its store in any of towns a c or e while firm has the option of opening its store in any of towns b d or f these decisions will be executed simultaneously once the two stores are opened customers from the towns will go to the store that is closer to them so for example if firm open its store in town c and firm opens its store in town b then the store in town b will attract customers from a and b while the store in town c will attract customers from c d e and f if we assume that the towns contain an equal number of customers and that payoffs are directly proportional to the number of customers this would result in a payoff of for firm and for firm since firm claims customers from towns while firm claims customers from the remaining towns reasoning in this way about the number of towns claimed by each store based on proximity to their locations we get the payoff matrix shown in figure a firm c e firm b d f figure facility location game we refer to this as a facility location game the competitive location of facilities is a topic that has been the subject of considerable study in operations research and other areas moreover closely related models have been used when the entities being located are not stores along a one dimensional highway but the positions of political candidates along a one dimensional ideological spectrum here too choosing a certain position relative to one electoral opponent can attract certain voters while alienating others we will return to issues related to political competition though in a slightly different direction in chapter we can verify that neither player has a dominant strategy in this game for example if firm locates at node a then the strict best response of firm is b while if firm locates at node e then the strict best response of firm is d the situation is symmetric if we interchange the roles of the two firms and read the graph from the other direction dominated strategies in the facility location game we can make progress in reasoning about the behavior of the two players in the facility location game by thinking about their dominated strategies first notice that a is a strictly dominated strategy for firm in any situation where firm has the option of choosing a it would receive a strictly higher payoff by choosing c similarly f is a strictly dominated strategy for firm in any situation where firm has the option of choosing f it would receive a strictly higher payoff by choosing d it is never in a player interest to use a strictly dominated strategy since it should always be replaced by a strategy that does better so firm isn t going to use strategy a moreover since firm knows the structure of the game including firm payoffs firm knows that firm won t use strategy a it can be effectively eliminated from the game the same reasoning shows that f can be eliminated from the game we now have a smaller instance of the facility location game involving only the four nodes b c d and e and the payoff matrix shown in figure firm c e firm b d figure smaller facility location game now something interesting happens the strategies b and e weren t previously strictly dominated they were useful in case the other player used a or f respectively but with a and f eliminated the strategies b and e now are strictly dominated so by the same reasoning both players know they won t be used and so we can eliminate them from the game this gives us the even smaller game shown in figure firm c firm d figure 22 even smaller facility location game at this point there is a very clear prediction for the play of the game firm will play c and firm will play d and the reasoning that led to this is clear after repeatedly removing strategies that were or became strictly dominated we were left with only a single plausible option for each player the process that led us to this reduced game is called the iterative deletion of strictly dominated strategies and we will shortly describe it in its full generality before doing this however it worth making some observations about the example of the facility location game first the pair of strategies c d is indeed the unique nash equilibrium in the game and when we discuss the iterated deletion of strictly dominated strategies in general we will see that it is an effective way to search for nash equilibria but beyond this it is also an effective way to justify the nash equilibria that one finds when we first introduced nash equilibrium we observed that it couldn t be derived purely from an assumption of rationality on the part of the players rather we had to assume further that play of the game would be found at an equilibrium from which neither player had an incentive to deviate on the other hand when a unique nash equilibrium emerges from the iterated deletion of strictly dominated strategies it is in fact a prediction made purely based on the assumptions of the players rationality and their knowledge of the game since all the steps that led to it were based simply on removing strategies that were strictly inferior to others from the perspective of payoff maximization a final observation is that iterated deletion can in principle be carried out for a very large number of steps and the facility location game illustrates this suppose that instead of a path of length six we had a path of length with the options for the two firms still strictly alternating along this path constituting possible strategies for each player then it would be still be the case that only the outer two nodes would be strictly dominated after their removal we d have a path of length in which the two new outer nodes had now become strictly dominated we can continue removing nodes in this way and after steps of such reasoning we ll have a game in which only the and nodes have survived as strategies this is the unique nash equilibrium for the game and this unique prediction can be justified by a very long sequence of deletions of dominated strategies it also interesting how this prediction is intuitively natural and one that is often seen in real life two competing stores staking out positions next to each other near the center of the population or two political candidates gravitating toward the ideological middle ground as they compete for voters in a general election in each case this move toward the center is the unique way to maximize the territory that you can claim at the expense of your competitor iterated deletion of dominated strategies the general principle in general for a game with an arbitrary number of players the process of iterated deletion of strictly dominated strategies proceeds as follows we start with any n player game find all the strictly dominated strategies and delete them we then consider the reduced game in which these strategies have been removed in this reduced game there may be strategies that are now strictly dominated despite not having been strictly dominated in the full game we find these strategies and delete them we continue this process repeatedly finding and removing strictly dominated strategies until none can be found an important general fact is that the set of nash equilibria of the original game coincides with the set of nash equilibria for the final reduced game consisting only of strategies that survive iterated deletion to prove this fact it is enough to show that the set of nash equilibria does not change when we perform one round of deleting strictly dominated strategies if this is true then we have established that the nash equilibria continue to remain unchanged through an arbitrary finite sequence of deletions to prove that the set of nash equilibria remains the same through one round of deletion we need to show two things first any nash equilibrium of the original game is a nash equilibrium of the reduced game to see this note that otherwise there would be a nash equilibrium of the original game involving a strategy s that was deleted but in this case s is strictly dominated by some other strategy st hence s cannot be part of a nash equilibrium of the original game it is not a best response to the strategies of the other players since the strategy st that dominates it is a better response this establishes that no nash equilibrium of the original game can be removed by the deletion process second we need to show that any nash equilibrium of the reduced game is also a nash equilibrium of the original game in order for this not to be the case there would have to be a nash equilibrium e sn of the reduced game and a strategy sit that was deleted from the original game such that player i has an incentive to deviate from its strategy si in e to the strategy sit but strategy sit was deleted because it was strictly dominated by at least one other strategy we can therefore find a strategy sitt that strictly dominated it and was not deleted then player i also has an incentive to deviate from si to sitt and sitt is still present in the reduced game contradicting our assumption that e is a nash equilibrium of the reduced game this establishes that the game we end up with after iterated deletion of strictly domi nated strategies still has all the nash equilibria of the original game hence this process can be a powerful way to restrict the search for nash equilibria moreover although we described the process as operating in rounds with all currently strictly dominated strategies being removed in each round this is not essential one can show that eliminating strictly dominated strategies in any order will result in the same set of surviving strategies weakly dominated strategies it is also natural to ask about notions that are slightly weaker than our definition of strictly dominated strategies one fundamental definition in this spirit is that of a weakly dominated strategy we say that a strategy is weakly dominated if there is another strategy that does at least as well no matter what the other players do and does strictly better against some joint strategy of the other players in our notation from earlier we say that a strategy si for player i is weakly dominated if there is another strategy sit for player i such that pi si sit si sn pi si si si sn for all choices of strategies si si sn by the other players and pi si sit si sn pi si si si sn for at least one choice of strategies si si sn by the other players for strictly dominated strategies the argument for deleting them was compelling they are never best responses for weakly dominated strategies the issue is more subtle such strategies could be best responses to some joint strategy by the other players so a rational player could play a weakly dominated strategy and in fact nash equilibria can involve weakly dominated strategies there are simple examples that make this clear even in two player two strategy games consider for example a version of the stag hunt game in which the payoff from successfully catching a stag is the same as the payoff from catching a hare hunter hunt stag hunt hare hunter hunt stag hunt hare figure stag hunt a version with a weakly dominated strategy in this case hunt stag is a weakly dominated strategy since each player always does at least as well and sometimes strictly better by playing hunt hare nevertheless the outcome in which both players choose hunt stag is a nash equilibrium since each is playing a best response to the other strategy thus deleting weakly dominated strategies is not in general a safe thing to do if one wants to preserve the essential structure of the game such deletion operations can destroy nash equilibria of course it might seem reasonable to suppose that a player should not play an equilib rium involving a weakly dominated strategy such as hunt stag hunt stag if he had any uncertainty about what the other players would do after all why not use an alternate strategy that is at least as good in every eventuality but nash equilibrium does not take into account this idea of uncertainty about the behavior of others and hence has no way to rule out such outcomes in the next chapter we will discuss an alternate equilibrium concept known as evolutionary stability that in fact does eliminate weakly dominated strategies in a principled way the relationship between nash equilibrium evolutionary stability and weakly dominated strategies is considered in the exercises at the end of chapter c dynamic games our focus in this chapter has been on games in which all players choose their strategies simultaneously and then receive payoffs based on this joint decision of course actual simultaneity is not crucial for the model but it has been central to our discussions so far that each player is choosing a strategy without knowledge of the actual choices made by the other players many games however are played over time some player or set of players moves first other players observe the choice made and then they respond perhaps according to a player a b player a b a b figure a simple game in extensive form predetermined order of governing who moves when such games are called dynamic games and there are many basic examples board games and card games in which players alternate turns negotiations which usually involve a sequence of offers and counter offers and bidding in an auction or pricing competing goods where participants must make decisions over time here we ll discuss an adaptation of the theory of games that incorporates this dynamic aspect normal and extensive forms of a game to begin with specifying a dynamic game is going to require a new kind of notation thus far we ve worked with something called the normal form representation of a game this specifies the list of players their possible strategies and the payoffs arising from every possible simultaneous choice of strategies by the players for two player games the payoff matrices we ve seen in this chapter encode the normal form representation of a game in a compact way to describe a dynamic game we re going to need a richer representation we need to be able to specify who moves when what each player knows at any opportunity they have to move what they can do when it is their turn to move and what the payoffs are at the end of the game we refer to this specification as the extensive form representation of the game let start with a very simple example of a dynamic game so that we can discuss what its extensive form representation looks like as we ll see this game is simple enough that it avoids some of the subtleties that arise in the analysis of dynamic games but it is useful as a first illustration and we ll proceed to a more complex second example afterward in our first example we imagine that there are two firms firm and firm each of whom is trying to decide whether to focus its advertising and marketing on two possible regions named a or b firm gets to choose first if firm follows firm into the same region then firm first mover advantage gives it of the profit obtainable from the market in that region while firm will only get if firm moves into the other region then each firm gets all the profit obtainable in their respective region finally region a has twice as large a market as region b the total profit obtainable in region a is equal to while in region b it we write the extensive form representation as a game tree depicted in figure this tree is designed to be read downward from the top the top node represents firm initial move and the two edges descending from this node represent its two options a or b based on which branch is taken this leads to a node representing firm subsequent move firm can then also choose option a or b again represented by edges descending from the node this leads to a terminal node representing the end of play in the game each terminal node is labeled with the payoffs to the two players thus a specific play determined by a sequence of choices by firm and firm corresponds to a path from the top node in the tree down to some terminal node first firm chooses a or b then firm chooses a or b and then the two players receive their payoffs in a more general model of dynamic games each node could contain an annotation saying what information about the previous moves is known to the player currently making a move however for our purposes here we will focus on the case in which each player knows the complete history of past moves when they go to make their current move reasoning about behavior in a dynamic game as with simultaneous move games we d like to make predictions for what players will do in dynamic games one way is to reason from the game tree in our current example we can start by considering how firm will behave after each of the two possible opening moves by firm if firm chooses a then firm maximizes its payoff by choosing b while if firm chooses b then firm maximizes its payoff by choosing a now let consider firm opening move given what we ve just concluded about firm subsequent behavior if firm chooses a then it expects firm to choose b yielding a payoff of for firm if firm chooses b then it expects firm to choose a yielding a payoff of for firm since we expect the firms to try maximizing their payoffs we predict that firm should choose a after which firm should choose b this is a useful way to analyze dynamic games we start one step above the terminal nodes where the last player to move has complete control over the outcome of the payoffs this lets us predict what the last player will do in all cases having established this we then move one more level up the game tree using these predictions to reason about what the player one move earlier will do we continue in this way up the tree eventually making predictions for play all the way up to the top node a different style of analysis exploits an interesting connection between normal and exten sive forms allowing us to write a normal form representation for a dynamic game as follows suppose that before the game is played each player makes up a plan for how to play the entire game covering every possible eventuality this plan will serve as the player strategy one way to think about such strategies and a useful way to be sure that they include a complete description of every possibility is to imagine that each player has to provide all of the information needed to write a computer program which will actually play the game in their place for the game in figure firm only has two possible strategies a or b since firm moves after observing what firm did and firm has two possible choices for each of the two options by firm firm has four possible plans for playing the game they can be written as contingencies specifying what firm will do in response to each possible move by firm a if a a if b a if a b if b b if a a if b and b if a b if b or in abbreviated form as aa ab aa bb ba ab and ba bb if each player chooses a complete plan for playing the game as its strategy then we can determine the payoffs directly from this pair of chosen strategies via a payoff matrix firm a b firm aa ab aa bb ba ab ba bb figure conversion to normal form because the plans describe everything about how a player will behave we have managed to describe this dynamic game in normal form each player chooses a strategy consisting of a complete plan in advance and from this joint choice of strategies we can determine payoffs we will see later that there are some important subtleties in using this interpretation of the underlying dynamic game and in particular the translation from extensive to normal form will sometimes not preserve the full structure implicit in the game but the translation is a useful tool for analysis and the subtle lack of fidelity that can arise in the translation is in itself a revealing notion to develop and explore with this in mind we first finish our simple example where the translation will work perfectly and then move on to a second example where the complications begin to arise for the normal form payoff matrix corresponding to our first example the payoff matrix has eight cells while the extensive form representation only has four terminal nodes with payoffs this occurs because each terminal node can be reached with two different pairs of strategies with each pair forming a cell of the payoff matrix both pairs of strategies dictate the same actions in the path of the game tree which actually occurs but describe different hypothetical actions in other unrealized paths for example the payoffs in the entries for a aa ab and for a aa bb are the same because both strategy combinations lead to the same terminal node in both cases firm chooses a in response to what firm actually does firm plan for what to do in the event firm chose b is not realized by the actual play now using the normal form representation we can quickly see that for firm strategy a is strictly dominant firm does not have a strictly dominant strategy but it should play a best response to firm which would be either ba ab or ba bb notice that this prediction of play by firm and firm based on the normal form representation is the same as our prediction based on direct analysis of the game tree where we reasoned upward from the terminal nodes firm will play a and in response firm will play b a more complex example the market entry game in our first dynamic game reasoning based on the extensive and normal form representations led to essentially identical conclusions as games get larger extensive forms are representationally more streamlined than normal forms for dynamic games but if this were the only distinction it would be hard to argue that dynamic games truly add much to the overall theory of games in fact however the dynamic aspect leads to new subtleties and this can be exposed by considering a case in which the translation from extensive form to normal form ends up obscuring some of the structure that is implicit in the dynamic game for this we consider a second example of a dynamic game also played between two competing firms we call this the market entry game and it motivated by the following scenario consider a region where firm is currently the only serious participant in a given line of business and firm is considering whether to enter the market the first move in this game is made by firm who must decide whether to stay out of the market or enter it if firm chooses to stay out then the game ends with firm getting a payoff of and firm keeping the payoff from the entire market player figure extensive form representation of the market entry game if firm chooses to enter then the game continues to a second move by firm who must choose whether to cooperate and divide the market evenly with firm or retaliate and engage in a price war if firm cooperates then each firm gets a payoff corresponding to half the market if firm retaliates then each firm gets a negative payoff choosing numerical payoffs to fill in this story we can write the extensive form representation of the market entry game as in figure subtle distinctions between extensive and normal form representations let take the two ways we developed to analyze our previous dynamic game and apply them here first we can work our way up the game tree starting at the terminal nodes as follows if firm chooses to enter the market then firm achieves a higher payoff by cooperating than by retaliating so we should predict cooperation in the event the game reaches this point given this when firm goes to make its first move it can expect a payoff of by staying out and a payoff of by entering so it should choose to enter the market we can therefore predict that firm will enter the market and then firm will cooperate now let consider the normal form representation firm possible plans for playing the game are just to choose stay out s or enter e firm possible plans are to choose retaliation in the event of entry or cooperation in the event of entry we ll denote these two plans by r and c respectively this gives us the payoff matrix in figure firm s e firm r c figure normal form of the market entry game here the surprise when we look at this game in normal form we discover two distinct pure strategy nash equilibria e c and s r the first of these corresponds to the prediction for play that we obtained by analyzing the extensive form representation what does the second one correspond to to answer this it helps to recall our view of the normal form representation as capturing the idea that each player commits in advance to a computer program that will play the game in its place viewed this way the equilibrium s r corresponds to an outcome in which firm commits in advance to a computer program that will automatically retaliate in the event that firm enters the market firm meanwhile commits to a program that stays out of the market given this pair of choices neither firm has an incentive to change the computer program they re using for example if firm were to switch to a program that entered the market it would trigger retaliation by the program that firm is using this contrast between the prediction from the extensive and normal forms highlights some important points first it shows that the premise behind our translation from extensive to normal form that each player commits ahead of time to a complete plan for playing the game is not really equivalent to our initial premise in defining dynamic games namely that each player makes an optimal decision at each intermediate point in the game based on what has already happened up to that point firm decision to retaliate on entry highlights this clearly if firm can truly pre commit to this plan then the equilibrium s r makes sense since firm will not want to provoke the retaliation that is encoded in firm plan but if we take the dynamic game as originally defined in extensive form then pre commitment to a plan is not part of the model rather firm only gets to evaluate its decision to cooperate or retaliate once firm has already entered the market and at that point its payoff is better if it cooperates given this firm can predict that it is safe to enter in game theory the standard model for dynamic games in extensive form assumes that players will seek to maximize their payoff at any intermediate stage of play that can be reached in the game in this interpretation there is a unique prediction for play in our market entry game corresponding to the equilibrium e c in normal form however the issues surrounding the other equilibrium s r are not simply notational or representational they are deeper than this for any given scenario it is really a question of what we believe is being modeled by the underlying dynamic game in extensive form it is a question of whether we are in a setting where a player can irrevocably pre commit to a certain plan to the extent that other players will believe the commitment as a credible threat or not further the market entry game shows how the ability to commit to a particular course of action when possible can in fact be a valuable thing for an individual player even if that course of action would be bad for everyone if it were actually carried out in particular if firm could make firm believe that it really would retaliate in the event of entry then firm would choose to stay out resulting in a higher payoff for firm in practice this suggests particular courses of action that firm could take before the game even starts for example suppose that before firm had decided whether to enter the market firm were to publically advertise an offer to beat any competitor price by this would be a safe thing to do as long as firm is the only serious participant in the market but it becomes dangerous to both firms if firm actually enters the fact that the plan has been publically announced means that it would be very costly reputationally and possibly legally for firm to back away from it in this way the announcement can serve as a way of switching the underlying model from one in which firm threat to retaliate is not credible to one in which firm can actually pre commit to a plan for retaliation relationship to weakly dominated strategies in discussing these distinctions it is also interesting to note the role that weakly dominated strategies play here notice that in the normal form representation in figure the strategy r for firm is weakly dominated and for a simple reason it yields the same payoff if firm chooses s since then firm doesn t actually get to move and it yields a lower payoff if firm chooses e so our translation from extensive form to normal form for dynamic games provides another reason to be careful about predictions of play in a normal form game that rely on weakly dominated strategies if the structure in fact arises from a dynamic game in extensive form then information about the dynamic game that is lost in the translation to normal form could potentially be sufficient to eliminate such equilibria however we can t simply fix up the translation by eliminating weakly dominated strate gies we saw earlier that iterated deletion of strictly dominated strategies can be done in any order all orders yield the same final result but this is not true for the iterated deletion of weakly dominated strategies to see this suppose we vary the market entry game slightly so that the payoff from the joint strategy e c is in this version both firms know they will fail to gain a positive payoff even if firm cooperates on entry although they still don t do as badly as when firm retaliates r is a weakly dominated strategy as before but now so is e e and s produce the same payoff for firm when firm chooses c and s produces a strictly higher payoff when firm chooses r in this version of the game there are now three pure strategy nash equilibria s c e c and s r if we first eliminate the weakly dominated strategy r then we are left with s c and e c as equilibria alternately if we first eliminate the weakly dominated strategy e then we are left with s c and s r as equilibria in both cases no further elimination of weakly dominated strategies is possible so the order of deletion affects the final set of equilibria we can ask which of these equilibria actually make sense as predictions of play in this game if this normal form actually arose from the dynamic version of the market entry game then c is still the only reasonable strategy for firm while firm could now play either s or e final comments the analysis framework we developed for most of this chapter is based on games in normal form one approach to analyzing dynamic games in extensive form is to first find all nash equilibria of the translation to normal form treating each of these as a candidate prediction of play in the dynamic game and then go back to the extensive form version to see which of these make sense as actual predictions there is an alternate theory that works directly with the extensive form representation the simplest technique used in this theory is the style of analysis we employed to analyze an extensive form representation from the terminal nodes upward but there are more complex components to the theory as well allowing for richer structure such as the possibility that players at any given point have only partial information about the history of play up to that point while we will not go further into this theory here it is developed in a number of books on game theory and microeconomic theory 398 exercises say whether the following claim is true or false and provide a brief sentence explanation for your answer claim if player a in a two person game has a dominant strategy sa then there is a pure strategy nash equilibrium in which player a plays sa and player b plays a best response to sa consider the following statement in a nash equilibrium of a two player game each player is playing an optimal strategy so the two player strategies are social welfare maximizing is this statement correct or incorrect if you think it is correct give a brief sentence explanation for why if you think it is incorrect give an example of a game discussed in chapter that shows it to be incorrect you do not need to spell out all the details of the game provided you make it clear what you are referring to together with a brief sentence explanation find all pure strategy nash equilibria in the game below in the payoff matrix below the rows correspond to player a strategies and the columns correspond to player b strategies the first entry in each box is player a payoff and the second entry is player b payoff player a u d player b l r consider the two player game with players strategies and payoffs described in the following game matrix t player a m b player b l m r figure payoff matrix a does either player have a dominant strategy explain briefly sentences b find all pure strategy nash equilibria for this game consider the following two player game in which each player has three strategies u player a m d player b l m r find all the pure strategy nash equilibria for this game in this question we will consider several two player games in each payoff matrix below the rows correspond to player a strategies and the columns correspond to player b strategies the first entry in each box is player a payoff and the second entry is player b payoff a find all pure non randomized strategy nash equilibria for the game described by the payoff matrix below player a u d player b l r b find all pure non randomized strategy nash equilibria for the game described by the payoff matrix below player a u d player b l r c find all nash equilibria for the game described by the payoff matrix below player a u d player b l r hint this game has a both pure strategy equilibria and a mixed strategy equilibrium to find the mixed strategy equilibrium let the probability that player a uses strategy u be p and the probability that player b uses strategy l be q as we learned in our analysis of matching pennies if a player uses a mixed strategy one that is not really just some pure strategy played with probability one then the player must be indifferent between two pure strategies that is the strategies must have equal expected payoffs so for example if p is not or then it must be the case that q q q as these are the expected payoffs to player a from u and d when player b uses probability q in this question we will consider several two player games in each payoff matrix below the rows correspond to player a strategies and the columns correspond to player b strategies the first entry in each box is player a payoff and the second entry is player b payoff a find all nash equilibria for the game described by the payoff matrix below player a u d player a u d player b l r player b l r b find all nash equilibria for the game described by the payoff matrix below include an explanation for your answer hint this game has a mixed strategy equilibrium to find the equilibrium let the probability that player a uses strategy u be p and the probability that player b uses strategy l be q as we learned in our analysis of matching pennies if a player uses a mixed strategy one that is not really just some pure strategy played with probability one then the player must be indifferent between two pure strategies that is the strategies must have equal expected payoffs so for example if p is not or then it must be the case that q q as these are the expected payoffs to player a from u and d when player b uses probability q consider the two player game described by the payoff matrix below player a u d player b l r a find all pure strategy nash equilibria for this game b this game also has a mixed strategy nash equilibrium find the probabilities the players use in this equilibrium together with an explanation for your answer c keeping in mind schelling focal point idea from chapter what equilibrium do you think is the best prediction of how the game will be played explain for each of the following two player games find all nash equilibria in each payoff matrix below the rows correspond to player a strategies and the columns correspond to player b strategies the first entry in each box is player a payoff and the second entry is player b payoff a b player a u d player b l r player b l r player a u d in the payoff matrix below the rows correspond to player a strategies and the columns correspond to player b strategies the first entry in each box is player a payoff and the second entry is player b payoff player a u d player b l r a find all pure strategy nash equilibria of this game b notice from the payoff matrix above that player a payoff from the pair of strategies u l is can you change player a payoff from this pair of strate gies to some non negative number in such a way that the resulting game has no pure strategy nash equilibrium give a brief sentence explanation for your answer note that in answering this question you should only change player a payoff for this one pair of strategies u l in particular leave the rest of the structure of the game unchanged the players their strategies the payoff from strategies other than u l and b payoff from u l c now let go back to the original payoff matrix from part a and ask an analogous question about player b so we re back to the payoff matrix in which players a and b each get a payoff of from the pair of strategies u l can you change player b payoff from the pair of strategies u l to some non negative number in such a way that the resulting game has no pure strategy nash equilibrium give a brief sentence explanation for your answer again in answering this question you should only change player b payoff for this one pair of strategies u l in particular leave the rest of the structure of the game unchanged the players their strategies the payoff from strategies other than u l and a payoff from u l in the text we ve discussed dominant strategies and noted that if a player has a domi nant strategy we would expect it to be used the opposite of a dominant strategy is a strategy that is dominated the definition of dominated is a strategy i is dominated if player i has another strategy sti with the property that player i payoff is greater from sti than from i no matter what the other players in the game do we do not expect a player to use a strategy that is dominated and this can help in finding nash equilibria here is an example of this idea in this game m is a dominated strategy it is dominated by r and player b will not use it player a u d player b l m r so in analyzing the game we can delete m and look at the remaining game player a u d player b l r now player a has a dominant strategy u and it is easy to see that the nash equilib rium of the by game is u l you can check the original game to see that u l is a nash equilibrium of course using this procedure requires that we know that a dominated strategy cannot be used in nash equilibrium consider any two player game which has at least one pure strategy nash equilibrium explain why the strategies used in an equilibrium of this game will not be dominated strategies in chapter we discussed dominant strategies and noted that if a player has a dominant strategy we would expect it to be used the opposite of a dominant strategy is a strategy that is dominated there are several possible notions of what it means for a strategy to be dominated in this problem we will focus on weak domination a strategy i is weakly dominated if player i has another strategy sti with the property that is actually true for any number of players it would also help to know that if we iteratively remove dominated strategies in any order and analyze the reduced games we still find the nash equilibria of the original game this is also true but it is a bit more complicated a no matter what the other player does player i payoff from sti is at least as large as the payoff from i and b there is some strategy for the other player so that player i payoff from sti is strictly greater than the payoff from i a it seems unlikely that a player would use a weakly dominated strategy but these strategies can occur in a nash equilibrium find all pure non randomized nash equilibria for the game below do any of them use weakly dominated strategies player a u d player b l r b one way to reason about the weakly dominated strategies that you should have found in answering the question above is to consider the following sequential game suppose that the players actually move sequentially but the player to move second does not know what the player moving first chose player a moves first and if he chooses u then player b choice does not matter effectively the game is over if a chooses u as no matter what b does the payoff is if player a chooses d then player b move matters and the payoff is if b chooses l or if b chooses r note that as b does not observe a move the simultaneous move game with payoff matrix above is equivalent to this sequential move game in this game how would you expect the players to behave explain your reasoning the players are not allowed to change the game they play it once just as it is given above you may reason from the payoff matrix or the story behind the game but if you use the story remember that b does not observe a move until after the game is over 13 here we consider a game with three players named and to define the game we need to specify the sets of strategies available to each player also when each of the three players chooses a strategy this gives a triple of strategies and we need to specify the payoff each player receives from any possible triple of strategies played let suppose that player strategy set is u d players strategy set is l r and player strategy set is l r one way to specify the payoffs would be to write down every possible triple of strategies and the payoffs for each a different but equivalent way to interpret triples of strategies which makes it easier to specify the payoffs is to imagine that player chooses which of two distinct two player games players and will play if chooses l then the payoff matrix is payoff matrix l player a u d player b l r where the first entry in each cell is the payoff to player the second entry is the payoff to player and the third entry is the payoff to player if chooses r then the payoff matrix is payoff matrix r player a u d player b l r so for example if player chooses u player chooses r and player chooses r the payoffs are for each player a first suppose the players all move simultaneously that is players and do not observe which game player has selected until after they each chose a strategy find all of the pure strategy nash equilibria for this game b now suppose that player gets to move first and that players and observe player move before they decide how to play that is if player chooses the strategy r then players and play the game defined by payoff matrix r and they both know that they are playing this game similarly if player chooses the strategy l then players and play the game defined by payoff matrix l and they both know that they are playing this game let also suppose that if players and play the game defined by payoff matrix r they play a pure strategy nash equilibrium for that game and similarly if players and play the game defined by payoff matrix l they play a pure strategy nash equilibrium for that game finally let suppose that player understands that this is how players and will behave what do you expect player to do and why what triple of strategies would you expect to see played is this list of strategies a nash equilibrium of the simultaneous move game between the three players consider the two player game with players strategies and payoffs described in the following game matrix player u d player l r a find all of the nash equilibria of this game b in the mixed strategy equilibrium you found in part a you should notice that player plays strategy u more often than strategy d one of your friends remarks that your answer to part a must be wrong because clearly for player strategy d is a more attractive strategy than strategy u both u and d give player a payoff of 4 on the off diagonal elements of the payoff matrix but d gives player a payoff of on the diagonal while u only gives player a payoff of on the diagonal explain what is wrong with this reasoning two identical firms let call them firm and firm must decide simultaneously and independently whether to enter a new market and what product to produce if they do enter the market each firm if it enters can develop and produce either product a or product b if both firms enter and produce product a they each lose ten million dollars if both firms enter and both produce product b they each make a profit of five million dollars if both enter and one produces a while the other produces b then they each make a profit of ten million dollars any firm that does not enter makes a profit of zero finally if one firm does not enter and the other firm produces a it makes a profit of fifteen million dollars while if the single entering firm produces b it makes a profit of thirty million dollars you are the manager of firm and you have to choose a strategy for your firm a set this situation up as a game with two players firms and and three strategies for each firm produce a produce b or do not enter b one of your employees argues that you should enter the market although he is not sure what product you should produce because no matter what firm does entering and producing product b is better than not entering evaluate this argument c another employee agrees with the person in part b and argues that as strategy a could result in a loss if the other firm also produces a you should enter and produce b if both firms reason this way and thus enter and produce product b will their play of the game form a nash equilibrium explain d find all the pure strategy nash equilibria of this game e another employee of your firm suggests merging the two firms and deciding co operatively on strategies so as to maximize the sum of profits ignoring whether this merger would be allowed by the regulators do you think its a good idea explain chapter evolutionary game theory in chapter we developed the basic ideas of game theory in which individual players make decisions and the payoff to each player depends on the decisions made by all as we saw there a key question in game theory is to reason about the behavior we should expect to see when players take part in a given game the discussion in chapter was based on considering how players simultaneously reason about what the other players may do in this chapter on the other hand we explore the notion of evolutionary game theory which shows that the basic ideas of game theory can be applied even to situations in which no individual is overtly reasoning or even making explicit decisions rather game theoretic analysis will be applied to settings in which individuals can exhibit different forms of behavior including those that may not be the result of conscious choices and we will consider which forms of behavior have the ability to persist in the population and which forms of behavior have a tendency to be driven out by others as its name suggests this approach has been applied most widely in the area of evolu tionary biology the domain in which the idea was first articulated by john maynard smith and g r price evolutionary biology is based on the idea that an organism genes largely determine its observable characteristics and hence its fitness in a given envi ronment organisms that are more fit will tend to produce more offspring causing genes that provide greater fitness to increase their representation in the population in this way fitter genes tend to win over time because they provide higher rates of reproduction the key insight of evolutionary game theory is that many behaviors involve the interaction of multiple organisms in a population and the success of any one of these organisms depends on how its behavior interacts with that of others so the fitness of an individual organism can t be measured in isolation rather it has to be evaluated in the context of the full population in which it lives this opens the door to a natural game theoretic analogy d easley and j kleinberg networks crowds and markets reasoning about a highly connected world cambridge university press draft version june an organism genetically determined characteristics and behaviors are like its strategy in a game its fitness is like its payoff and this payoff depends on the strategies characteristics of the organisms with which it interacts written this way it is hard to tell in advance whether this will turn out to be a superficial analogy or a deep one but in fact the connections turn out to run very deeply game theoretic ideas like equilibrium will prove to be a useful way to make predictions about the results of evolution on a population fitness as a result of interaction to make this concrete we now describe a first simple example of how game theoretic ideas can be applied in evolutionary settings this example will be designed for ease of explanation rather than perfect fidelity to the underlying biology but after this we will discuss examples where the phenomenon at the heart of the example has been empirically observed in a variety of natural settings for the example let consider a particular species of beetle and suppose that each beetle fitness in a given environment is determined largely by the extent to which it can find food and use the nutrients from the food effectively now suppose a particular mutation is introduced into the population causing beetles with the mutation to grow a significantly larger body size thus we now have two distinct kinds of beetles in the population small ones and large ones it is actually difficult for the large beetles to maintain the metabolic requirements of their larger body size it requires diverting more nutrients from the food they eat and so this has a negative effect on fitness if this were the full story we d conclude that the large body size mutation is fitness decreasing and so it will likely be driven out of the population over time through multiple generations but in fact there more to the story as we ll now see interaction among organisms the beetles in this population compete with each other for food when they come upon a food source there crowding among the beetles as they each try to get as much of the food as they can and not surprisingly the beetles with large body sizes are more effective at claiming an above average share of the food let assume for simplicity that food competition in this population involves two beetles interacting with each other at any given point in time this will make the ideas easier to describe but the principles we develop can also be applied to interactions among many individuals simultaneously when two beetles compete for some food we have the following possible outcomes when beetles of the same size compete they get equal shares of the food when a large beetle competes with a small beetle the large beetle gets the majority of the food in all cases large beetles experience less of a fitness benefit from a given quantity of food since some of it is diverted into maintaining their expensive metabolism thus the fitness that each beetle gets from a given food related interaction can be thought of as a numerical payoff in a two player game between a first beetle and a second beetle as follows the first beetle plays one of the two strategies small or large depending on its body size and the second beetle plays one of these two strategies as well based on the two strategies used the payoffs to the beetles are described by figure beetle small large beetle small large figure the body size game notice how the numerical payoffs satisfy the principles just outlined when two small beetles meet they share the fitness from the food source equally large beetles do well at the expense of small beetles but large beetles cannot extract the full amount of fitness from the food source in this payoff matrix the reduced fitness when two large beetles meet is particularly pronounced since a large beetle has to expend extra energy in competing with another large beetle this payoff matrix is a nice way to summarize what happens when two beetles meet but compared with the game in chapter there something fundamentally different in what being described here the beetles in this game aren t asking themselves what do i want my body size to be in this interaction rather each is genetically hard wired to play one of these two strategies through its whole lifetime given this important difference the idea of choosing strategies which was central to our formulation of game theory is missing from the biological side of the analogy as a result in place of the idea of nash equilibrium which was based fundamentally on the relative benefit of changing one own personal strategy we will need to think about strategy changes that operate over longer time scales taking place as shifts in a population under evolutionary forces we develop the fundamental definitions for this in the next section evolutionarily stable strategies in chapter the notion of nash equilibrium was central in reasoning about the outcome of a game in a nash equilibrium for a two player game neither player has an incentive to deviate from the strategy they are currently using the equilibrium is a choice of strategies that tends to persist once the players are using it the analogous notion for evolutionary settings will be that of an evolutionarily stable strategy a genetically determined strategy that tends to persist once it is prevalent in a population we formulate this as follows suppose in our example that each beetles is repeatedly paired off with other beetles in food competitions over the course of its lifetime we will assume the population is large enough that no two particular beetles have a significant probability of interacting with each other repeatedly a beetle overall fitness will be equal to the average fitness it experiences from each of its many pairwise interactions with others and this overall fitness determines its reproductive success the number of offspring that carry its genes and hence its strategy into the next generation in this setting we say that a given strategy is evolutionarily stable if when the whole population is using this strategy any small group of invaders using a different strategy will eventually die off over multiple generations we can think of these invaders either as migrants who move to join the population or as mutants who were born with the new behavior directly into the population we capture this idea in terms of numerical payoffs by saying that when the whole population is using a strategy s then a small group of invaders using any alternate strategy t should have strictly lower fitness than the users of the majority strategy s since fitness translates into reproductive success evolutionary principles posit that strictly lower fitness is the condition that causes a sub population like the users of strategy t to shrink over time through multiple generations and eventually die off with high probability more formally we will phrase the basic definitions as follows we say the fitness of an organism in a population is the expected payoff it receives from an interaction with a random member of the population we say that a strategy t invades a strategy s at level x for some small positive number x if an x fraction of the underlying population uses t and a x fraction of the underlying population uses s finally we say that a strategy s is evolutionarily stable if there is a small positive number y such that when any other strategy t invades s at any level x y the fitness of an organism playing s is strictly greater than the fitness of an organism playing t evolutionarily stable strategies in our first example let see what happens when we apply this definition to our example involving beetles competing for food we will first check whether the strategy small is evolutionarily stable and then we will do the same for the strategy large following the definition let suppose that for some small positive number x a x fraction of the population uses small and an x fraction of the population uses large this is what the picture would look like just after a small invader population of large beetles arrives what is the expected payoff to a small beetle in a random interaction in this popula tion with probability x it meets another small beetle receiving a payoff of while with probability x it meets a large beetle receiving a payoff of therefore its expected payoff is x x what is the expected payoff to a large beetle in a random interaction in this population with probability x it meets a small beetle receiving a payoff of while with probability x it meets another large beetle receiving a payoff of therefore its expected payoff is x x it easy to check that for small enough values of x and even for reasonably large ones in this case the expected fitness of large beetles in this population exceeds the expected fitness of small beetles therefore small is not evolutionarily stable now let check whether large is evolutionarily stable for this we suppose that for some very small positive number x a x fraction of the population uses large and an x fraction of the population uses small what is the expected payoff to a large beetle in a random interaction in this population with probability x it meets another large beetle receiving a payoff of while with probability x it meets a small beetle receiving a payoff of therefore its expected payoff is x x 5x what is the expected payoff to a small beetle in a random interaction in this popu lation with probability x it meets a large beetle receiving a payoff of while with probability x it meets another small beetle receiving a payoff of therefore its expected payoff is x x 4x in this case the expected fitness of large beetles in this population exceeds the expected fitness of small beetles and so large is evolutionarily stable interpreting the evolutionarily stable strategy in our example intuitively this analysis can be summarized by saying that if a few large beetles are introduced into a population consisting of small beetles then the large beetles do extremely well since they rarely meet each other they get most of the food in almost every competition they experience as a result the population of small beetles cannot drive out the large ones and so small is not evolutionarily stable on the other hand in a population of large beetles a few small beetles will do very badly losing almost every competition for food as a result the population of large beetles resists the invasion of small beetles and so large is evolutionarily stable therefore if we know that the large body size mutation is possible we should expect to see populations of large beetles in the wild rather than populations of small ones in this way our notion of evolutionary stability has predicted a strategy for the population as we predicted outcomes for games among rational players in chapter but by different means what striking about this particular predicted outcome though is the fact that the fitness of each organism in a population of small beetles is which is larger than the fitness of each organism in a population of large beetles in fact the game between small and large beetles has precisely the structure of a prisoner dilemma game the motivating scenario based on competition for food makes it clear that the beetles are engaged in an arms race like the game from chapter in which two competing athletes need to decide whether to use performance enhancing drugs there it was a dominant strategy to use drugs even though both athletes understand that they are better off in an outcome where neither of them uses drugs it simply that this mutually better joint outcome is not sustainable in the present case the beetles individually don t understand anything nor could they change their body sizes even if they wanted to nevertheless evolutionary forces over multiple generations are achieving a completely analogous effect as the large beetles benefit at the expense of the small ones later in this chapter we will see that this similarity in the conclusions of two different styles of analysis is in fact part of a broader principle here is a different way to summarize the striking feature of our example starting from a population of small beetles evolution by natural selection is causing the fitness of the organisms to decrease over time this might seem troubling initially since we think of natural selection as being fitness increasing but in fact it not hard to reconcile what happening with this general principle of natural selection natural selection increases the fitness of individual organisms in a fixed environment if the environment changes to become more hostile to the organisms then clearly this could cause their fitness to go down this is what is happening to the population of beetles each beetle environment includes all the other beetles since these other beetles determine its success in food competitions therefore the increasing fraction of large beetles can be viewed in a sense as a shift to an environment that is more hostile for everyone empirical evidence for evolutionary arms races biologists have offered recent evi dence for the presence of evolutionary games in nature with the prisoner dilemma structure we ve just seen it is very difficult to truly determine payoffs in any real world setting and so all of these studies are the subject of ongoing investigation and debate for our purposes in this discussion they are perhaps most usefully phrased as deliberately streamlined examples illustrating how game theoretic reasoning can help provide qualitative insight into different forms of biological interaction it has been argued that the heights of trees can obey prisoner dilemma payoffs if two neighboring trees both grow short then they share the sunlight equally they also share the sunlight equally if they both grow tall but in this case their payoffs are each lower because they have to invest a lot of resources in achieving the additional height the trouble is that if one tree is short while its neighbor is tall then the tall tree gets most of the sunlight as a result we can easily end up with payoffs just like the body size game among beetles with the trees evolutionary strategies short and tall serving as analogues to the beetles strategies small and large of course the real situation is more complex than this since genetic variation among trees can lead to a wide range of different heights and hence a range of different strategies rather than just two strategies labeled short and tall within this continuum prisoner dilemma payoffs can only apply to a certain range of tree heights there is some height beyond which further height increasing mutations no longer provide the same payoff structure because the additional sunlight is more than offset by the fitness downside of sustaining an enormous height similar kinds of competition take place in the root systems of plants suppose you grow two soybean plants at opposite ends of a large pot of soil then their root systems will each fill out the available soil and intermingle with each other as they try to claim as many resources as they can in doing so they divide the resources in the soil equally now suppose that instead you partition the same quantity of soil using a wall down the middle so that the two plants are on opposite sides of the wall then each still gets half the resources present in the soil but each invests less of its energy in producing roots and consequently has greater reproductive success through seed production this observation has implications for the following simplified evolutionary game involving root systems imagine that instead of a wall we had two kinds of root development strategies available to soybean plants conserve where a plant roots only grow into its own share of the soil and explore where the roots grow everywhere they can reach then we again have the scenario and payoffs from the body size game with the same conclusion all plants are better off in a population where everyone plays conserve but only explore is evolutionarily stable as a third example there was recent excitement over the discovery that virus populations can also play an evolutionary version of the prisoner dilemma turner and chao studied a virus called phage which infects bacteria and manufactures products needed for its own replication a mutational variant of this virus called phage is also able to replicate in bacterial hosts though less effectively on its own however is able to take advantage of chemical products produced by which gives a fitness advantage when it is in the presence of this turns out to yield the structure of the prisoner dilemma viruses have the two evolutionary strategies and viruses in a pure population all do better than viruses in a pure population and regardless of what the other viruses are doing you as a virus are better off playing thus only is evolutionarily stable the virus system under study was so simple that turner and chao were able to infer an actual payoff matrix based on measuring the relative rates at which the two viral variants were able to replicate under different conditions using an estimation procedure derived from these measurements they obtained the payoffs in figure the payoffs are re scaled so that the upper left box has the value virus virus figure the virus game whereas our earlier examples had an underlying story very much like the use of performance enhancing drugs this game among phages is actually reminiscent of a different story that also motivates the prisoner dilemma payoff structure the scenario behind the exam or presentation game with which we began chapter there two college students would both be better off if they jointly prepared for a presentation but the payoffs led them to each think selfishly and study for an exam instead what the virus game here shows is that shirking a shared responsibility isn t just something that rational decision makers do evolutionary forces can induce viruses to play this strategy as well a general description of evolutionarily stable strate gies the connections between evolutionary games and games played by rational participants are suggestive enough that it makes sense to understand how the relationship works in general we will focus here as we have thus far on two player two strategy games we will also should be noted that even in a system this simple there are many other biological factors at work and hence this payoff matrix is still just an approximation to the performance of and populations under real experimental and natural conditions other factors appear to affect these populations including the density of the population and the potential presence of additional mutant forms of the virus a general description of evolutionarily stable strategies restrict our attention to symmetric games as in the previous sections of this chapter where the roles of the two players are interchangeable the payoff matrix for a completely general two player two strategy game that is sym metric can be written as in figure organism s t organism s t a a b c c b d d figure general symmetric game let check how to write the condition that s is evolutionarily stable in terms of the four variables a b c and d as before we start by supposing that for some very small positive number x a x fraction of the population uses s and an x fraction of the population uses t what is the expected payoff to an organism playing s in a random interaction in this population with probability x it meets another player of s receiving a payoff of a while with probability x it meets a player of t receiving a payoff of b therefore its expected payoff is a x bx what is the expected payoff to an organism playing t in a random interaction in this population with probability x it meets a player of s receiving a payoff of c while with probability x it meets another player of t receiving a payoff of d therefore its expected payoff is c x dx therefore s is evolutionarily stable if for all sufficiently small values of x the inequality a x bx c x dx holds as x goes to the left hand side becomes a and the right hand side becomes c hence if a c then the left hand side is larger once x is sufficiently small while if a c then the left hand side is smaller once x is sufficiently small finally if a c then the left hand side is larger precisely when b d therefore we have a simple way to express the condition that s is evolutionarily stable in a two player two strategy symmetric game s is evolutionarily stable precisely when either i a c or ii a c and b d it is easy to see the intuition behind our calculations that translates into this condition as follows first in order for s to be evolutionarily stable the payoff to using strategy s against s must be at least as large as the payoff to using strategy t against s otherwise an invader who uses t would have a higher fitness than the rest of population and the fraction of the population who are invaders would have a good probability of growing over time second if s and t are equally good responses to s then in order for s to be evolu tionarily stable players of s must do better in their interactions with t than players of t do with each other otherwise players of t would do as well as against the s part of the population as players of s and at least as well against the t part of the population so their overall fitness would be at least as good as the fitness of players of s 4 relationship between evolutionary and nash equi libria using our general way of characterizing evolutionarily stable strategies we can now under stand how they relate to nash equilibria if we go back to the general symmetric game from the previous section we can write down the condition for s s i e the choice of s by both players to be a nash equilibrium s s is a nash equilibrium when s is a best response to the choice of s by the other player this translates into the simple condition a c if we compare this to the condition for s to be evolutionarily stable i a c or ii a c and b d we immediately get the conclusion that if strategy s is evolutionarily stable then s s is a nash equilibrium we can also see that the other direction does not hold it is possible to have a game where s s is a nash equilibrium but s is not evolutionarily stable the difference in the two conditions above tells us how to construct such a game we should have a c and b d to get a sense for where such a game might come from let recall the stag hunt game from chapter here each player can hunt stag or hunt hare hunting hare successfully just requires your own effort while hunting the more valuable stag requires that you both do so this produces payoffs as shown in figure 4 4 relationship between evolutionary and nash equilibria hunter hunt stag hunt hare hunter hunt stag hunt hare 4 4 figure 4 stag hunt in this game as written hunt stag and hunt hare are both evolutionarily stable as we can check from the conditions on a b c and d to check the condition for hunt hare we simply need to interchange the rows and columns of the payoff matrix to put hunt hare in the first row and first column however suppose we make up a modification of the stag hunt game by shifting the payoffs as follows in this new version when the players mis coordinate so that one hunts stag while the other hunts hare then the hare hunter gets an extra benefit due to the lack of competition for hare in this way we get a payoff matrix as in figure hunter hunt stag hunt hare hunter hunt stag hunt hare 4 4 4 4 figure stag hunt a version with added benefit from hunting hare alone in this case the choice of strategies hunt stag hunt stag is still a nash equilibrium if each player expects the other to hunt stag then hunting stag is a best response but hunt stag is not an evolutionarily stable strategy for this version of the game because in the notation from our general symmetric game we have a c and b d informally the problem is that a hare hunter and a stag hunter do equally well when each is paired with a stag hunter but hare hunters do better than stag hunters when each is paired with a hare hunter there is also a relationship between evolutionarily stable strategies and the concept of a strict nash equilibrium we say that a choice of strategies is a strict nash equilibrium if each player is using the unique best response to what the other player is doing we can check that for symmetric two player two strategy games the condition for s s to be a strict nash equilibrium is that a c so we see that in fact these different notions of equilibrium naturally refine each other the concept of an evolutionarily stable strategy can be viewed as a refinement of the concept of a nash equilibrium the set of evolutionarily stable strategies s is a subset of the set of strategies s for which s s is a nash equilibrium similarly the concept of a strict nash equilibrium when the players use the same strategy is a refinement of evolutionary stability if s s is a strict nash equilibrium then s is evolutionarily stable it is intriguing that despite the extremely close similarities between the conclusions of evolutionary stability and nash equilibrium they are built on very different underlying stories in a nash equilibrium we consider players choosing mutual best responses to each other strategy this equilibrium concept places great demands on the ability of the players to choose optimally and to coordinate on strategies that are best responses to each other evolutionary stability on the other hand supposes no intelligence or coordination on the part of the players instead strategies are viewed as being hard wired into the players perhaps because their behavior is encoded in their genes according to this concept strategies which are more successful in producing offspring are selected for although this evolutionary approach to analyzing games originated in biology it can be applied in many other contexts for example suppose a large group of people are being matched repeatedly over time to play the general symmetric game from figure now the payoffs should be interpreted as reflecting the welfare of the players and not their number of offspring if any player can look back at how others have played and can observe their payoffs then imitation of the strategies that have been most successful may induce an evolutionary dynamic alternatively if a player can observe his own past successes and failures then his learning may induce an evolutionary dynamic in either case strategies that have done relatively well in the past will tend to be used by more people in the future this can lead to the same behavior that underlies the concept of evolutionarily stable strategies and hence can promote the play of such strategies evolutionarily stable mixed strategies as a further step in developing an evolutionary theory of games we now consider how to handle cases in which no strategy is evolutionarily stable in fact it is not hard to see how this can happen even in two player games that have pure strategy nash equilibria perhaps the most natural example is the hawk dove game from chapter and we use this to introduce the basic ideas of this section recall that in the hawk dove game two animals compete for a piece of food an animal that plays the strategy hawk h behaves aggressively while an animal that plays the strategy dove d behaves passively if one animal is aggressive while the other is passive then the aggressive animal benefits by getting most of the food but if both animals are aggressive then they risk destroying the food and injuring each other this leads to a payoff matrix as shown in figure in chapter we considered this game in contexts where the two players were making choices about how to behave now let consider the same game in a setting where each that a player is using a pure strategy if she always plays a particular one of the strategies in the game as opposed to a mixed strategy in which she chooses at random from among several possible strategies animal d h animal d h figure hawk dove game animal is genetically hard wired to play a particular strategy how does it look from this perspective when we consider evolutionary stability neither d nor h is a best response to itself and so using the general principles from the last two sections we see that neither is evolutionarily stable intuitively a hawk will do very well in a population consisting of doves but in a population of all hawks a dove will actually do better by staying out of the way while the hawks fight with each other as a two player game in which players are actually choosing strategies the hawk dove game has two pure nash equilibria d h and h d but this doesn t directly help us identify an evolutionarily stable strategy since thus far our definition of evolutionary stability has been restricted to populations in which almost all members play the same pure strategy to reason about what will happen in the hawk dove game under evolutionary forces we need to generalize the notion of evolutionary stability by allowing some notion of mixing between strategies defining mixed strategies in evolutionary game theory there are at least two natural ways to introduce the idea of mixing into the evolutionary framework first it could be that each individual is hard wired to play a pure strategy but some portion of the population plays one strategy while the rest of the population plays another if the fitness of individuals in each part of the population is the same and if invaders eventually die off then this could be considered to exhibit a kind of evolutionary stability second it could be that each individual is hard wired to play a particular mixed strategy that is they are genetically configured to choose randomly from among certain options with certain probabilities if invaders using any other mixed strategy eventually die off then this too could be considered a kind of evolutionary stability we will see that for our purposes here these two concepts are actually equivalent to each other and we will focus initially on the second idea in which individuals use mixed strategies essentially we will find that in situations like the hawk dove game the individuals or the population as a whole must display a mixture of the two behaviors in order to have any chance of being stable against invasion by other forms of behavior the definition of an evolutionarily stable mixed strategy is in fact completely parallel to the definition of evolutionary stability we have seen thus far it is simply that we now greatly enlarge the set of possible strategies so that each strategy corresponds to a particular randomized choice over pure strategies specifically let consider the general symmetric game from figure a mixed strat egy here corresponds to a probability p between and indicating that the organism plays s with probability p and plays t with probability p as in our discussion of mixed strategies from chapter this includes the possibility of playing the pure strategies s or t by simply setting p or p when organism uses the mixed strategy p and organism uses the mixed strategy q the expected payoff to organism can be computed as follows there is a probability pq of an x x pairing yielding a for the first player there is a probability p q of an x y pairing yielding b for the first player there is a probability p q of a y x pairing yielding c for the first player and there is a probability p q of a y y pairing yielding d for the first player so the expected payoff for this first player is v p q pqa p q b p qc p q d as before the fitness of an organism is its expected payoff in an interaction with a random member of the population we can now give the precise definition of an evolutionarily stable mixed strategy in the general symmetric game p is an evolutionarily stable mixed strategy if there is a small positive number y such that when any other mixed strategy q invades p at any level x y the fitness of an organism playing p is strictly greater than the fitness of an organism playing q this is just like our previous definition of evolutionarily stable pure strategies except that we allow the strategy to be mixed and we allow the invaders to use a mixed strategy an evolutionarily stable mixed strategy with p or p is evolutionarily stable under our original definition for pure strategies as well however note the subtle point that even if s were an evolutionarily stable strategy under our previous definition it is not necessarily an evolutionarily stable mixed strategy under this new definition with p the problem is that it is possible to construct games in which no pure strategy can successfully invade a population playing s but a mixed strategy can as a result it will be important to be clear in any discussion of evolutionary stability on what kinds of behavior an invader can employ directly from the definition we can write the condition for p to be an evolutionarily stable mixed strategy as follows for some y and any x y the following inequality holds for all mixed strategies q p x v p p xv p q x v q p xv q q this inequality also makes it clear that there is a relationship between mixed nash equilibria and evolutionarily stable mixed strategies and this relationship parallels the one we saw earlier for pure strategies in particular if p is an evolutionarily stable mixed strategy then we must have v p p v q p and so p is a best response to p as a result the pair of strategies p p is a mixed nash equilibrium however because of the strict inequality in equation it is possible for p p to be a mixed nash equilibrium without p being evolutionarily stable so again evolutionary stability serves as a refinement of the concept of mixed nash equilibrium evolutionarily stable mixed strategies in the hawk dove game now let see how to apply these ideas to the hawk dove game first since any evolutionarily stable mixed strategy must correspond to a mixed nash equilibrium of the game this gives us a way to search for possible evolutionarily stable strategies we first work out the mixed nash equilibria for the hawk dove and then we check if they are evolutionarily stable as we saw in chapter in order for p p to be a mixed nash equilibrium it must make the two players indifferent between their two pure strategies when the other player is using the strategy p the expected payoff from playing d is p 2p while the expected payoff from playing h is setting these two quantities equal to capture the indifference between the two strategies we get p so is a mixed nash equilibrium in this case both pure strategies as well as any mixture between them produce an expected payoff of when played against the strategy p now to see whether p is evolutionarily stable we must check inequality when some other mixed strategy q invades at a small level x here is a first observation that makes evaluating this inequality a bit easier since p p is a mixed nash equilibrium that uses both pure strategies we have just argued that all mixed strategies q have the same payoff when played against p as a result we have v p p v q p for all q subtracting these terms from the left and right of inequality and then dividing by x we get the following inequality to check v p q v q q the point is that since p p is a mixed equilibrium the strategy p can t be a strict best response to itself all other mixed strategies are just as good against it therefore in order for p to be evolutionarily stable it must be a strictly better response to every other mixed strategy q than q is to itself that is what will cause it to have higher fitness when q invades in fact it is true that v p q v q q for all mixed strategies q p and we can check this as follows using the fact that p 3 we have v p q 3 q 3 3 q 3 q 4q 3 while now we have v q q 3 q q q q v p q v q q 2q 3 6q 3 3 this last way of writing v p q v q q shows that it is a perfect square and so it is positive whenever q 3 this is just what we want for showing that v p q v q q whenever q p and so it follows that p is indeed an evolutionarily stable mixed strategy interpretations of evolutionarily stable mixed strategies the kind of mixed equi librium that we see here in the hawk dove game is typical of biological situations in which organisms must break the symmetry between two distinct behaviors when consistently adopting just one of these behaviors is evolutionarily unsustainable we can interpret the result of this example in two possible ways first all participants in the population may actually be mixing over the two possible pure strategies with the given probability in this case all members of the population are genetically the same but whenever two of them are matched up to play any combination of d and h could potentially be played we know the empirical frequency with which any pair of strategies will be played but not what any two animals will actually do a second interpretation is that the mixture is taking place at the population level it could be that 3 of the animals are hard wired to always play d and 3 are hard wired to always play h in this case no individual is actually mixing but as long as it is not possible to tell in advance which animal will play d and which will play h the interaction of two randomly selected animals results in the same frequency of outcomes that we see when each animal is actually mixing notice also that in this case the fitnesses of both kinds of animals are the same since both d and h are best responses to the mixed strategy p 3 thus these two different interpretation of the evolutionarily stable mixed strategy lead to the same calculations and the same observed behavior in the population there are a number of other settings in which this type of mixing between pure strategies has been discussed in biology a common scenario is that there is an undesirable fitness lowering role in a population of organisms but if some organisms don t choose to play this role then everyone suffers considerably for example let think back to the virus game in figure and suppose purely hypothetically for the sake of this example that the payoff when both viruses use the strategy were as shown in figure virus virus 00 00 50 50 figure the virus game hypothetical payoffs with stronger fitness penalties to in this event rather than having a prisoner dilemma type of payoff structure we d have a hawk dove payoff structure having both viruses play is sufficiently bad that one of them needs to play the role of 6 the two pure equilibria of the resulting two player game viewed as a game among rational players rather than a biological interaction would be 6 and 6 in a virus population we d expect to find an evolutionarily stable mixed strategy in which both kinds of virus behavior were observed this example like the examples from our earlier discussion of the hawk dove game in section suggests the delicate boundary that exists between prisoner dilemma and hawk dove in both cases a player can choose to be helpful to the other player or selfish in prisoner dilemma however the payoff penalties from selfishness are mild enough that selfishness by both players is the unique equilibrium while in hawk dove selfishness is sufficiently harmful that at least one player should try to avoid it there has been research into how this boundary between the two games manifests itself in other biological settings as well one example is the implicit game played by female lions in defending their territory when two female lions encounter an attacker on the edge of their territory each can choose to play the strategy confront in which she confronts the attacker or lag in which she lags behind and tries to let the other lion confront the attacker first if you re one of the lions and your fellow defender chooses the strategy confront then you get a higher payoff by choosing lag since you re less likely to get injured what harder to determine in empirical studies is what a lion best response should be to a play of lag by her partner choosing confront risks injury but joining your partner in lag risks a successful assault on the territory by the attacker understanding which is the best response is important for understanding whether this game is more like prisoner dilemma or hawk dove and what the evolutionary consequences might be for the observed behavior within a lion population in this as in many examples from evolutionary game theory it is beyond the power of current empirical studies to work out detailed fitness values for particular strategies how ever even in situations where exact payoffs are not known the evolutionary framework can provide an illuminating perspective on the interactions between different forms of behav ior in an underlying population and how these interactions shape the composition of the population exercises in the payoff matrix below the rows correspond to player a strategies and the columns correspond to player b strategies the first entry in each box is player a payoff and the second entry is player b payoff player a x y player b x y a find all pure strategy nash equilibria b find all evolutionarily stable strategies give a brief explanation for your answer c briefly explain how the sets of predicted outcomes relate to each other in the payoff matrix below the rows correspond to player a strategies and the columns correspond to player b strategies the first entry in each box is player a payoff and the second entry is player b payoff player a x y player b x y 4 4 3 3 5 5 a find all pure strategy nash equilibria b find all evolutionarily stable strategies give a brief explanation for your answer c briefly explain how the answers in parts and relate to each other 3 in this problem we will consider the relationship between nash equilibria and evolu tionarily stable strategies for games with a strictly dominant strategy first let define what we mean by strictly dominant in a two player game strategy x is said to be a strictly dominant strategy for a player i if no matter what strategy the other player j uses player i payoff from using strategy x is strictly greater than his payoff from any other strategy consider the following game in which a b c and d are non negative numbers player a x y player b x y a a b c c b d d suppose that strategy x is a strictly dominant strategy for each player i e a c and b d a find all of the pure strategy nash equilibria of this game b find all of the evolutionarily stable strategies of this game c how would your answers to parts a and b change if we change the assumption on payoffs to a c and b d player a x y player b x y x x 3 3 4 consider following the two player symmetric game where x can be or a for each of the possible values of x find all pure strategy nash equilibria and all evolutionarily stable strategies b your answers to part a should suggest that the difference between the predictions of evolutionary stability and nash equilibrium arises when a nash equilibrium uses a weakly dominated strategy we say that a strategy i is weakly dominated if player i has another strategy sti with the property that a no matter what the other player does player i payoff from sti is at least as large as the payoff from i and b there is some strategy for the other player so that player i payoff from sti is strictly greater than the payoff from i now consider the following claim that makes a connection between evolutionarily stable strategies and weakly dominated strategies claim suppose that in the game below x x is a nash equilibrium and that strategy x is weakly dominated then x is not an evolutionarily stable strategy player a x y player b x y a a b c c b d d explain why this claim is true you do not need to write a formal proof a careful explanation is fine chapter modeling network traffic using game theory among the initial examples in our discussion of game theory in chapter we noted that traveling through a transportation network or sending packets through the internet involves fundamentally game theoretic reasoning rather than simply choosing a route in isolation individuals need to evaluate routes in the presence of the congestion resulting from the decisions made by themselves and everyone else in this chapter we develop models for network traffic using the game theoretic ideas we ve developed thus far in the process of doing this we will discover a rather unexpected result known as braess paradox which shows that adding capacity to a network can sometimes actually slow down the traffic traffic at equilibrium let begin by developing a model of a transportation network and how it responds to traffic congestion with this in place we can then introduce the game theoretic aspects of the problem we represent a transportation network by a directed graph we consider the edges to be highways and the nodes to be exits where you can get on or off a particular highway there are two particular nodes which we ll call a and b and we ll assume everyone wants to drive from a to b for example we can imagine that a is an exit in the suburbs b is an exit downtown and we re looking at a large collection of morning commuters finally each edge has a designated travel time that depends on the amount of traffic it contains to make this concrete consider the graph in figure the label on each edge gives the d easley and j kleinberg networks crowds and markets reasoning about a highly connected world cambridge university press draft version june 2010 figure a highway network with each edge labeled by its travel time in minutes when there are x cars using it when cars need to get from a to b they divide evenly over the two routes at equilibrium and the travel time is minutes travel time in minutes when there are x cars using the edge in this simplified example the a d and c b edges are insensitive to congestion each takes minutes to traverse regardless of the number of cars traveling on them on the other hand the a c and d b edges are highly sensitive to congestion for each one it takes x minutes to traverse when there are x cars using the edge now suppose that cars want to get from a to b as part of the morning commute there are two possible routes that each car can choose the upper route through c or the lower route through d for example if each car takes the upper route through c then the total travel time for everyone is minutes since the same is true if everyone takes the lower route on the other hand if the cars divide up evenly between the two routes so that each carries cars then the total travel time for people on both routes is equilibrium traffic so what do we expect will happen the traffic model we ve described is really a game in which the players correspond to the drivers and each player possible strategies consist of the possible routes from a to b in our example this means that each player only has two strategies but in larger networks there could be many strategies for each player the payoff for a player is the negative of his or her travel time we use the negative since large travel times are bad travel times here are simplified to make the reasoning clearer in any real application each road would have both some minimum travel time and some sensitivity to the number of cars x that are using it however the analysis here adapts directly to more intricate functions specifying the travel times on edges braess s paradox this all fits very naturally into the framework we ve been using one thing to notice of course is that up to now we have focused primarily on games with two players whereas the current game will generally have an enormous number of players in our example but this poses no direct problem for applying any of the ideas we ve developed a game can have any number of players each of whom can have any number of available strategies and the payoff to each player depends on the strategies chosen by all a nash equilibrium is still a list of strategies one for each player so that each player strategy is a best response to all the others the notions of dominant strategies mixed strategies and nash equilibrium with mixed strategies all have direct parallels with their definitions for two player games in this traffic game there is generally not a dominant strategy for example in figure either route has the potential to be the best choice for a player if all the other players are using the other route the game does have nash equilibria however as we will see next any list of strategies in which the drivers balance themselves evenly between the two routes on each is a nash equilibrium and these are the only nash equilibria why does equal balance yield a nash equilibrium and why do all nash equilibria have equal balance to answer the first question we just observe that with an even balance between the two routes no driver has an incentive to switch over to the other route for the second question consider a list of strategies in which x drivers use the upper route and the remaining x drivers use the lower route then if x is not equal to the two routes will have unequal travel times and any driver on the slower route would have an incentive to switch to the faster one hence any list of strategies in which x is not equal to cannot be a nash equilibrium and any list of strategies in which x is a nash equilibrium braess paradox in figure everything works out very cleanly self interested behavior by all drivers causes them at equilibrium to balance perfectly between the available routes but with only a small change to the network we can quickly find ourselves in truly counterintuitive territory the change is as follows suppose that the city government decides to build a new very fast highway from c to d as indicated in figure to keep things simple we ll model its travel time as regardless of the number of cars on it although the resulting effect would happen even with more realistic but small travel times it would stand to reason that people travel time from a to b ought to get better after this edge from c to d is added does it here the surprise there is a unique nash equilibrium in this new highway network but it leads to a worse travel time for everyone at equilibrium every driver uses the route through both c and d and as a result the travel time for every driver is since 4000 to see why this is an equilibrium note that no driver can figure the highway network from the previous figure after a very fast edge has been added from c to d although the highway system has been upgraded the travel time at equilibrium is now minutes since all cars use the route through c and d benefit by changing their route with traffic snaking through c and d the way it is any other route would now take minutes and to see why it the only equilibrium you can check that the creation of the edge from c to d has in fact made the route through c and d a dominant strategy for all drivers regardless of the current traffic pattern you gain by switching your route to go through c and d in other words once the fast highway from c to d is built the route through c and d acts like a vortex that draws all drivers into it to the detriment of all in the new network there is no way given individually self interested behavior by the drivers to get back to the even balance solution that was better for everyone this phenomenon that adding resources to a transportation network can sometimes hurt performance at equilibrium was first articulated by dietrich braess in and it has become known as braess paradox like many counterintuitive anomalies it needs the right combination of conditions to actually pop up in real life but it has been observed empirically in real transportation networks including in seoul korea where the destruction of a six lane highway to build a public park actually improved travel time into and out of the city even though traffic volume stayed roughly the same before and after the change some reflections on braess paradox having now seen how braess paradox works we can also appreciate that there is actually nothing really paradoxical about it there are many settings in which adding a new strategy to a game makes things worse for everyone for example the prisoner dilemma from chapter 6 can be used to illustrate this point if the only strategy for each player were not confess an admittedly very simple game then both players would be better off compared with the game where confess is added as an option indeed that why the police offer confess as an option in the first place still it reasonable to view the analogous phenomenon at the heart of the braess paradox as more paradoxical at an intuitive level we all have an informal sense that upgrading a network has to be a good thing and so it is surprising when it turns out to make things worse the example in this section is actually the starting point for a large body of work on game theoretic analysis of network traffic for example we could ask how bad braess paradox can be for networks in general how much larger can the equilibrium travel time be after the addition of an edge relative to what it was before suppose in particular that we allow the graph to be arbitrary and we assume that the travel time on each edge depends in a linear way on the number of cars traversing it that is all travel times across edges have the form ax b where each of a and b is either or a positive number in this case elegant results of tim roughgarden and e va tardos can be used to show that if we add edges to a network with an equilibrium pattern of traffic there is always an equilibrium in the new network whose travel time is no more than 4 3 times as large moreover 4 3 is the factor increase that we d get in the example from figures and if we replace the two travel times of with in that case the travel time at equilibrium would jump from to when we add the edge from c to d so the roughgarden tardos result shows that this simple example is as bad as the braess paradox can get in a quantitative sense when edges respond linearly to traffic when edges can respond non linearly things can be much worse there are many other types of questions that can be pursued as well for example we could think about ways of designing networks to prevent bad equilibria from arising or to avoid bad equilibria through the judicious use of tolls on certain parts of the network many of these extensions as well as others are discussed by tim roughgarden in his book on game theoretic models of network traffic 3 advanced material the social cost of traffic at equilibrium the braess paradox is one aspect of a larger phenomenon which is that network traffic at equilibrium may not be socially optimal in this section we try to quantify how far from optimal traffic can be at equilibrium we would like our analysis to apply to any network and so we introduce the following general definitions the network can be any directed graph there is a set of drivers and different drivers may have different starting points and destinations now each edge e has a travel times written as explicit functions of x b travel times written as annotations on the edges figure 3 a network annotated with the travel time function on each edge a travel time function te x which gives the time it takes all drivers to cross the edge when there are x drivers using it these travel times are simply the functions that we drew as labels inside the edges in the figures in section we will assume that all travel time functions are linear in the amount of traffic so that te x aex be for some choice of numbers ae and be that are either positive or zero for example in figure 3 we draw another network on which braess paradox arises with the travel time functions scaled down to involve smaller numbers the version of the drawing in figure 3 a has the travel time functions explicitly written out while the version of the drawing in figure 3 b has the travel time functions written as labels inside the edges finally we say that a traffic pattern is simply a choice of a path by each driver and the social cost of a given traffic pattern is the sum of the travel times incurred by all drivers when they use this traffic pattern for example figure 4 shows two different traffic patterns on the network from figure 3 when there are four drivers each with starting node a and destination node b the first of these traffic patterns in figure 4 a achieves the minimum possible social cost each driver requires units of time to get to their destination and so the social cost is we will refer to such a traffic pattern which achieves the minimum possible social cost as socially optimal there are other traffic patterns on this network that also achieve a social cost of that is there are multiple traffic patterns for this network that are socially optimal note that socially optimal traffic patterns are simply the social welfare maximizers of this traffic game since the sum of the drivers payoffs is the negative of the social cost the second traffic pattern figure 4 b is the unique nash equilibrium and it has a larger social cost of the main two questions we consider in the remainder of this chapter are the following first in any network with linear travel time functions is there always an equilibrium traffic a the social optimum b the nash equilibrium figure 4 a version of braess paradox in the socially optimal traffic pattern on the left the social cost is while in the unique nash equilibrium on the right the social cost is pattern we have seen examples in chapter 6 of games where equilibria do not exist using pure strategies and it is not a priori clear that they should always exist for the traffic game we ve defined here however we will find in fact that equilibria always do exist the second main question is whether there always exists an equilibrium traffic pattern whose social cost is not much more than the social optimum we will find that this is in fact the case we will show a result due to roughgarden and tardos that there is always an equilibrium whose social cost is at most twice that of the optimum a how to find a traffic pattern at equilibrium we will prove that an equilibrium exists by analyzing the following procedure that explicitly searches for one the procedure starts from any traffic pattern if it is an equilibrium we are done otherwise there is at least one driver whose best response given what everyone else is doing is some alternate path providing a strictly lower travel time we pick one such driver and have him switch to this alternate path we now have a new traffic pattern and we again check whether it is an equilibrium if it isn t then we have some driver switch to his best response and we continue in this fashion this procedure is called best response dynamics since it dynamically reconfigures the fact stronger results of roughgarden and tardos supplemented by subsequent results of anshelevich et al establish that in fact every equilibrium traffic pattern has social cost at most 4 3 times the optimum one can show that this implies their result on the braess paradox cited in the previous section that with linear travel times adding edges can t make things worse by a factor of more than 4 3 however since it is harder to prove the bound of 4 3 we limit ourselves here to proving the easier but weaker factor of between the social optimum and some equilibrium traffic pattern players strategies by constantly having some player perform his or her best response to the current situation if the procedure ever stops in a state where everyone is in fact playing their best response to the current situation then we have an equilibrium so the key is to show that in any instance of our traffic game best response dynamics must eventually stop at an equilibrium but why should it certainly for games that lack an equilibrium best response dynamics will run forever for example in the matching pennies game from chapter 6 when only pure strategies are allowed best response dynamics will simply consist of the two players endlessly switching their strategies between h and t it seems plausible that for some network this could happen in the traffic game as well one at a time drivers shift their routes to ones that are better for them thus increasing the delay for another driver who then switches and continues the cascade in fact however this cannot happen in the traffic game we now show that best response dynamics must always terminate in an equilibrium thus proving not only that equilibria exist but also that they can be reached by a simple process in which drivers constantly update what they re doing according to best responses analyzing best response dynamics via potential energy how should we go about proving that best response dynamics must come to a halt when you have a process that runs according to some set of instructions like do the following ten things and then stop it generally obvious that it will eventually come to an end the process essentially comes with its own guarantee of termination but we have a process that runs according to a different kind of rule one that says keep doing something until a particular condition happens to hold in this case there is no a priori reason to believe it will ever stop in such cases a useful analysis technique is to define some kind of progress measure that tracks the process as it operates and to show that eventually enough progress will be made that the process must stop for the traffic game it natural to think of the social cost of the current traffic pattern as a possible progress measure but in fact the social cost is not so useful for this purpose some best response updates by drivers can make the social cost better for example if a driver leaves a congested road for a relatively empty one but others can make it worse as in the sequence of best response updates that shifts the traffic pattern from the social optimum to the inferior equilibrium in the braess paradox so in general as best response dynamics runs the social cost of the current traffic pattern can oscillate between going up and going down and it not clear how this is related to our progress toward an equilibrium instead we re going to define an alternate quantity that initially seems a bit mysterious however we will see that it has the property that it strictly decreases with each best response update and so it can be used to track the progress of best response dynamics we will refer to this quantity as the potential energy of a traffic pattern the potential energy of a traffic pattern is defined edge by edge as follows if an edge e currently has x drivers on it then we define the potential energy of this edge to be energy e te te te x if an edge has no drivers on it its potential energy is defined to be the potential energy of a traffic pattern is then simply the sum of the potential energies of all the edges with their current number of drivers in this traffic pattern in figure 5 we show the potential energy of each edge for the five traffic patterns that best response dynamics produces as it moves from the social optimum to the unique equilibrium in the braess paradox network from figure 4 notice that the potential energy of an edge e with x drivers is not the total travel time experienced by the drivers that cross it since there are x drivers each experiencing a travel time of te x their total travel time is xte x which is a different number the potential energy instead is a sort of cumulative quantity in which we imagine drivers crossing the edge one by one and each driver only feels the delay caused by himself and the drivers crossing the edge in front of him of course the potential energy is only useful for our purpose if it lets us analyze the progress of best response dynamics we show how to do this next proving that best response dynamics comes to an end our main claim is the following each step of best response dynamics causes the potential energy of the current traffic pattern to strictly decrease proving this will be enough to show that best response dynamics must come to an end for the following reason the potential energy can only take a finite number of possible values one for each possible traffic pattern if it is strictly decreasing with each step of best response dynamics this means that it is consuming this finite supply of possible values since it can never revisit a value once it drops below it so best response dynamics must come to a stop by the time the potential energy reaches its minimum possible value if not sooner and once best response dynamics comes to a stop we must be at an equilibrium for otherwise the dynamics would have a way to continue thus showing that the potential energy strictly decreases in every step of best response dynamics is enough to show the existence of an equilibrium traffic pattern as an example let return to the sequence of best response steps from figure 5 al though the social cost is rising through the five traffic patterns increasing from to the potential energy decreases strictly in each step in the sequence in fact it is easy to track the change in potential energy through this sequence as follows from one traffic pattern to the next the only change is that one driver abandons his current path and switches to a new one suppose we really view this switch as a two step process first the a the initial traffic pattern potential energy is b after one step of best response dynamics po tential energy is 24 c after two steps potential energy is d after three steps potential energy is e after four steps equilibrium is reached potential energy is 20 figure 5 we can track the progress of best response dynamics in the traffic game by watching how the potential energy changes a the potential energy of a traffic pattern not in equilibrium b potential energy is released when a driver aban dons their current path c potential energy is put back into the system when the driver chooses a new path figure 6 when a driver abandons one path in favor of another the change in potential energy is exactly the improvement in the driver travel time drivers abandons his current path temporarily leaving the system then the driver returns to the system by adopting a new path this first step releases potential energy as the driver leaves the system and the second step adds potential energy as he re joins what the net change for example the transition from figure 5 a to 5 b occurs because one driver aban dons the upper path and adopts the zigzag path as shown in figure 6 abandoning the upper path releases 5 7 units of potential energy while adopting the zigzag path puts 3 units of potential energy back into the system the resulting change is a decrease of notice that the decrease of 7 is simply the travel time the driver was experiencing on the path he abandoned and the subsequent increase of 5 is the travel time the driver now experiences on the path he has adopted this relationship is in fact true for any network and any best response by a driver and it holds for a simple reason specifically the potential energy of edge e with x drivers is te te te x te x and when one of these drivers leaves it drops to te te te x hence the change in potential energy on edge e is te x exactly the travel time that the driver was experiencing on e summing this over all edges used by the driver we see that the potential energy released when a driver abandons his current path is exactly equal to the travel time the driver was experiencing by the same reasoning when a driver adopts a new path the potential energy on each edge e he joins increases from te te te x to te te te x te x and the increase of te x is exactly the new travel time the driver experiences on this edge hence the potential energy added to the system when a driver adopts a new path is exactly equal to the travel time the driver now experiences it follows when a driver switches paths the net change in potential energy is simply his new travel time minus his old travel time but in best response dynamics a driver only changes paths when it causes his travel time to decrease so the change in potential energy is negative for any best response move this establishes what we wanted to show that the potential energy in the system strictly decreases throughout best response dynamics as argued above since the potential energy cannot decrease forever best response dynamics must therefore eventually come to an end at a traffic pattern in equilibrium b comparing equilibrium traffic to the social optimum having shown that an equilibrium traffic pattern always exists we now consider how its travel time compares to that of a socially optimal traffic pattern we will see that the potential energy we ve defined is very useful for making this comparison the basic idea is to establish a relationship between the potential energy of an edge and the total travel time of all drivers crossing the edge once we do this we will sum these two quantities over all the edges to compare travel times at equiibrium and at social optimality figure 7 the potential energy is the area under the shaded rectangles it is always at least half the total travel time which is the area inside the enclosing rectangle relating potential energy to travel time for a single edge we denote the po tential energy of an edge by energy e and we recall that when there are x drivers this potential energy is defined by energy e te te te x on the other hand each of the x drivers experiences a travel time of te x and so the total travel time experienced by all drivers on the edge is total travel time e xte x for purposes of comparison with the potential energy it is useful to write this as follows total travel time e te x te x te x x t e rms since the potential energy and the total travel time each have x terms but the terms in the latter expression are at least as large as the terms in the former we have energy e total travel time e figure 7 shows how the potential energy and the total travel time compare when te is a linear function the total travel time is the shaded area under the horizontal line with y value te x while the potential energy is the total area under all the unit width rectangles of heights te te te x as this figure makes clear geometrically since te is a linear function we have te te te x xte x alternately we can see this by a bit of simple algebra recalling that te x aex be te te te x ae x bex aex x b x e x ae x b e x aex be xte x in terms of energies and total travel times this says energy e total travel time e so the conclusion is that the potential energy of an edge is never far from the total travel time it is sandwiched between the total travel time and half the total travel time relating the travel time at equilibrium and social optimality we now use this relationship between potential energy and total travel to relate the equilibrium and socially optimal traffic patterns let z be a traffic pattern we define energy z to be the total potential energy of all edges when drivers follow the traffic pattern z we write social cost z to denote the social cost of the traffic pattern recall that this is the sum of the travel times experienced by all drivers equivalently summing the social cost edge by edge social cost z is the sum of the total travel times on all the edges so applying our relationships between potential energy and travel time on an edge by edge basis we see that the same relationships govern the potential energy and social cost of a traffic pattern social cost z energy z social cost z now suppose that we start from a socially optimal traffic pattern z and we then allow best response dynamics to run until they stop at an equilibrium traffic pattern zt the social cost may start increasing as we run best response dynamics but the potential energy can only go down and since the social cost can never be more than twice the potential energy this shrinking potential energy keeps the social cost from ever getting more than twice as high as where it started this shows that the social cost of the equilibrium we reach is at most twice the cost of the social optimum we started with hence there is an equilibrium with at most twice the socially optimal cost as we wanted to show let write this argument out in terms of the inequalities on energies and social costs first we saw in the previous section that the potential energy decreases as best response dynamics moves from z to zt and so energy zt energy z second the quantitative relationships between energies and social cost say that social cost zt 2 energy zt and energy z social cost z now we just chain these inequalities together concluding that social cost zt 2 energy zt 2 energy z 2 social cost z note that this really is the same argument that we made in words in the previous paragraph the potential energy decreases during best response dynamics and this decrease prevents the social cost from every increasing by more than a factor of two thus tracking potential energy is not only useful for showing that best response dynamics must reach an equilibrium by relating this potential energy to the social cost we can use it to put a bound on the social cost of the equilibrium that is reached 4 exercises there are cars which must travel from town a to town b there are two possible routes that each car can take the upper route through town c or the lower route through town d let x be the number of cars traveling on the edge ac and let y be the number of cars traveling on the edge db the directed graph in figure indicates that travel time per car on edge ac is x if x cars use edge ac and similarly the travel time per car on edge db is y if y cars use edge db the travel time per car on each of edges cb and ad is 12 regardless of the number of cars on these edges each driver wants to select a route to minimize his travel time the drivers make simultaneous choices a find nash equilibrium values of x and y b now the government builds a new one way road from town c to town d the new road adds the path acdb to the network this new road from c to d has a travel figure traffic network time of per car regardless of the number of cars that use it find a nash equilibrium for the game played on the new network what are the equilibrium values of x and y what happens to total cost of travel the sum of total travel times for the cars as a result of the availability of the new road c suppose now that conditions on edges cb and ad are improved so that the travel times on each edge are reduced to 5 the road from c to d that was constructed in part b is still available find a nash equilibrium for the game played on the network with the smaller travel times for cb and ad what are the equilibrium values of x and y what is the total cost of travel what would happen to the total cost of travel if the government closed the road from c to d 2 there are two cities a and b joined by two routes there are travelers who begin in city a and must travel to city b there are two routes between a and b route i begins with a highway leaving city a this highway takes one hour of travel time regardless of how many travelers use it and ends with a local street leading into city b this local street near city b requires a travel time in minutes equal to plus the number of travelers who use the street route ii begins with a local street leaving city a which requires a travel time in minutes equal to 10 plus the number of travelers who use this street and ends with a highway into city b which requires one hour of travel time regardless of the number of travelers who use this highway a draw the network described above and label the edges with the travel time needed to move along the edge let x be the number of travelers who use route i the network should be a directed graph as all roads are one way b travelers simultaneously chose which route to use find the nash equilibrium value of x c now the government builds a new two way road connecting the nodes where local streets and highways meet this adds two new routes one new route consists of the figure traffic network local street leaving city a on route ii the new road and the local street into city b on route i the second new route consists of the highway leaving city a on route i the new road and the highway leading into city b on route ii the new road is very short and takes no travel time find the new nash equilibrium hint there is an equilibrium in which no one chooses to use the second new route described above d what happens to total travel time as a result of the availability of the new road e if you can assign travelers to routes then in fact it possible to reduce total travel time relative to what it was before the new road was built that is the total travel time of the population can be reduced below that in the original nash equilibrium from part b by assigning travelers to routes there are many assignments of routes that will accomplish this find one explain why your reassignment reduces total travel time hint remember that travel on the new road can go in either direction you do not need find the total travel time minimizing assignment of travelers one approach to this question is to start with the nash equilibrium from part b and look for a way to assign some travelers to different routes so as to reduce total travel time 3 there are cars which must travel from city a to city b there are two possible routes that each car can take the upper route through city c or the lower route through city d let x be the number of cars traveling on the edge ac and let y be the number of cars traveling on the edge db the directed graph in figure 8 indicates that total travel time per car along the upper route is x 3 if x cars use the upper route and similarly the total travel time per car along the lower route is 3 y if y cars take the lower route each driver wants to select a route to minimize his total travel time the drivers make simultaneous choices a find nash equilibrium values of x and y b now the government builds a new one way road from city a to city b the new route has a travel time of 5 per car regardless of the number of cars that use it draw the new network and label the edges with the cost of travel needed to move along the edge the network should be a directed graph as all roads are one way find a nash equilibrium for the game played on the new network what happens to total cost of travel the sum of total travel times for the cars as a result of the availability of the new road c now the government closes the direct route between city a and city b and builds a new one way road which links city c to city d this new road between c and d is very short and has a travel time of regardless of the number of cars that use it draw the new network and label the edges with the cost of travel needed to move along the edge the network should be a directed graph as all roads are one way find a nash equilibrium for the game played on the new network what happens to total cost of travel as a result of the availability of the new road d the government is unhappy with the outcome in part c and decides to reopen the road directly linking city a and city b the road that was built in part b and closed in part c the route between c and d that was constructed in part c remains open this road still has a travel time of 5 per car regardless of the number of cars that use it draw the new network and label the edges with the cost of travel needed to move along the edge the network should be a directed graph as all roads are one way find a nash equilibrium for the game played on the new network what happens to total cost of travel as a result of re opening the direct route between a and b 4 there are two cities a and b joined by two routes i and ii all roads are one way roads there are travelers who begin in city a and must travel to city b route i links city a to city b through city c this route begins with a road linking city a to city c which has a cost of travel for each traveler equal to 5 x where x is the number of travelers on this road route i ends with a highway from city c to city b which has a cost of travel for each traveler of regardless of the number of travelers who use it route ii links city a to city b through city d this route begins with a highway linking city a to city d which has a cost of travel for each traveler of regardless of the number of travelers who use it route ii ends with a road linking city d to city b which has a cost of travel for each traveler equal to 5 y where y is the number of travelers on this road these costs of travel are the value that travelers put on the time lost due to travel plus the cost of gasoline for the trip currently there are no tolls on these roads so the government collects no revenue from travel on them a draw the network described above and label the edges with the cost of travel needed to move along the edge the network should be a directed graph as all roads are one way b travelers simultaneously chose which route to use find nash equilibrium values of x and y c now the government builds a new one way road from city c to city d the new road is very short and has cost of travel find a nash equilibrium for the game played on the new network d what happens to total cost of travel as a result of the availability of the new road e the government is unhappy with the outcome in part c and decides to impose a toll on users of the road from city a to city c and to simultaneously subsidize users of the highway from city a to city d they charge a toll of 125 to each user and thus increase the cost of travel by this amount for users of the road from city a to city c they also subsidize travel and thus reduce the cost of travel by this amount for each user of the highway from city a to city d by 125 find a new nash equilibrium if you are curious about how a subsidy could work you can think of it as a negative toll in this economy all tolls are collected electronically much as new york state attempts to do with its e zpass system so a subsidy just reduces the total amount that highway users owe f as you will observe in solving part e the toll and subsidy in part e were designed so that there is a nash equilibrium in which the amount the government collects from the toll just equals the amount it loses on the subsidy so the government is breaking even on this policy what happens to total cost of travel between parts c and e can you explain why this occurs can you think of any break even tolls and subsidies that could be placed on the roads from city c to city b and from city d to city b that would lower the total cost of travel even more chapter auctions in chapter 8 we considered a first extended application of game theoretic ideas in our analysis of traffic flow through a network here we consider a second major application the behavior of buyers and sellers in an auction an auction is a kind of economic activity that has been brought into many people everyday lives by the internet through sites such as ebay but auctions also have a long history that spans many different domains for example the u s government uses auctions to sell treasury bills and timber and oil leases christie and sotheby use them to sell art and morrell co and the chicago wine company use them to sell wine auctions will also play an important and recurring role in the book since the simplified form of buyer seller interaction they embody is closely related to more complex forms of economic interaction as well in particular when we think in the next part of the book about markets in which multiple buyers and sellers are connected by an underlying network structure we ll use ideas initially developed in this chapter for understanding simpler auction formats similarly in chapter we ll study a more complex kind of auction in the context of a web search application analyzing the ways in which search companies like google yahoo and microsoft use an auction format to sell advertising rights for keywords types of auctions in this chapter we focus on different simple types of auctions and how they promote different kinds of behavior among bidders we ll consider the case of a seller auctioning one item to a set of buyers we could symmetrically think of a situation in which a buyer is trying to purchase a single item and runs an auction among a set of multiple sellers each of whom is able to provide the item such procurement auctions are frequently run by governments to d easley and j kleinberg networks crowds and markets reasoning about a highly connected world cambridge university press 2010 draft version june 10 2010 purchase goods but here we ll focus on the case in which the seller runs the auction there are many different ways of defining auctions that are much more complex than what we consider here the subsequent chapters will generalize our analysis to the case in which there are multiple goods being sold and the buyers assign different values to these goods other variations which fall outside the scope of the book include auctions in which goods are sold sequentially over time these more complex variations can also be analyzed using extensions of the ideas we ll talk about here and there is a large literature in economics that considers auctions at this broad level of generality the underlying assumption we make when modeling auctions is that each bidder has an intrinsic value for the item being auctioned she is willing to purchase the item for a price up to this value but not for any higher price we will also refer to this intrinsic value as the bidder true value for the item there are four main types of auctions when a single item is being sold and many variants of these types ascending bid auctions also called english auctions these auctions are carried out interactively in real time with bidders present either physically or electronically the seller gradually raises the price bidders drop out until finally only one bidder remains and that bidder wins the object at this final price oral auctions in which bidders shout out prices or submit them electronically are forms of ascending bid auctions 2 descending bid auctions also called dutch auctions this is also an interactive auction format in which the seller gradually lowers the price from some high initial value until the first moment when some bidder accepts and pays the current price these auctions are called dutch auctions because flowers have long been sold in the netherlands using this procedure 3 first price sealed bid auctions in this kind of auction bidders submit simultaneous sealed bids to the seller the terminology comes from the original format for such auctions in which bids were written down and provided in sealed envelopes to the seller who would then open them all together the highest bidder wins the object and pays the value of her bid 4 second price sealed bid auctions also called vickrey auctions bidders submit simul taneous sealed bids to the sellers the highest bidder wins the object and pays the value of the second highest bid these auctions are called vickrey auctions in honor of william vickrey who wrote the first game theoretic analysis of auctions including the second price auction vickery won the nobel memorial prize in economics in for this body of work 2 when are auctions appropriate 2 when are auctions appropriate auctions are generally used by sellers in situations where they do not have a good estimate of the buyers true values for an item and where buyers do not know each other values in this case as we will see some of the main auction formats can be used to elicit bids from buyers that reveal these values known values to motivate the setting in which buyers true values are unknown let start by considering the case in which the seller and buyers know each other values for an item and argue that an auction is unnecessary in this scenario in particular suppose that a seller is trying to sell an item that he values at x and suppose that the maximum value held by a potential buyer of the item is some larger number y in this case we say there is a surplus of y x that can be generated by the sale of the item it can go from someone who values it less x to someone who values it more y if the seller knows the true values that the potential buyers assign to the item then he can simply announce that the item is for sale at a fixed price just below y and that he will not accept any lower price in this case the buyer with value y will buy the item and the full value of the surplus will go to the seller in other words the seller has no need for an auction in this case he gets as much as he could reasonably expect just by announcing the right price notice that there is an asymmetry in the formulation of this example we gave the seller the ability to commit to the mechanism that was used for selling the object this ability of the seller to tie his hands by committing to a fixed price is in fact very valuable to him assuming the buyers believe this commitment the item is sold for a price just below y and the seller makes all the surplus in contrast consider what would happen if we gave the buyer with maximum value y the ability to commit to the mechanism in this case this buyer could announce that she is willing to purchase the item for a price just above the larger of x and the values held by all other buyers with this announcement the seller would still be willing to sell since the price would be above x but now at least some of the surplus would go to the buyer as with the seller commitment this commitment by the buyer also requires knowledge of everyone else values these examples show how commitment to a mechanism can shift the power in the trans action in favor of the seller or the buyer one can also imagine more complex scenarios in which the seller and buyers know each other values but neither has the power to unilater ally commit to a mechanism in this case one may see some kind of bargaining take place over the price we discuss the topic of bargaining further in chapter 12 as we will discover in the current chapter the issue of commitment is also crucial in the context of auctions specifically it is important that a seller be able to reliably commit in advance to a given auction format unknown values thus far we ve been discussing how sellers and buyers might interact when everyone knows each other true values for the item beginning in the next section we ll see how auctions come into play when the participants do not know each other values for most of this chapter we will restrict our attention to the case in which the buyers have independent private values for the item that is each buyer knows how much she values the item she does not know how much others value it and her value for it does not depend on others values for example the buyers could be interested in consuming the item with their values reflecting how much they each would enjoy it later we will also consider the polar opposite of this setting the case of common values suppose that an item is being auctioned and instead of consuming the item each buyer plans to resell the item if she gets it in this case assuming the buyers will do a comparably good job of reselling it the item has an unknown but common value regardless of who acquires it it is equal to how much revenue this future reselling of the item will generate buyers estimates of this revenue may differ if they have some private information about the common value and so their valuations of the item may differ in this setting the value each buyer assigns to the object would be affected by knowledge of the other buyers valuations since the buyers could use this knowledge to further refine their estimates of the common value 3 relationships between different auction formats our main goal will be to consider how bidders behave in different types of auctions we begin in this section with some simple informal observations that relate behavior in interactive auctions ascending bid and descending bid auctions which play out in real time with behavior in sealed bid auctions these observations can be made mathematically rigorous but for the discussion here we will stick to an informal description descending bid and first price auctions first consider a descending bid auction here as the seller is lowering the price from its high initial starting point no bidder says anything until finally someone actually accepts the bid and pays the current price bidders therefore learn nothing while the auction is running other than the fact that no one has yet accepted the current price for each bidder i there a first price bi at which she ll be willing to break the silence and accept the item at price bi so with this view the process is equivalent to a sealed bid first price auction this price bi plays the role of bidder i bid the item goes to the bidder with the highest bid value and this bidder pays the value of her bid in exchange for the item 3 relationships between different auction formats ascending bid and second price auctions now let think about an ascending bid auction in which bidders gradually drop out as the seller steadily raises the price the winner of the auction is the last bidder remaining and she pays the price at which the second to last bidder drops out suppose that you re a bidder in such an auction let consider how long you should stay in the auction before dropping out first does it ever make sense to stay in the auction after the price reaches your true value no by staying in you either lose and get nothing or else you win and have to pay more than your value for the item second does it ever make sense to drop out before the price reaches your true value for the item again no if you drop out early before your true value is reached then you get nothing when by staying in you might win the item at a price below your true value so this informal argument indicates that you should stay in an ascending bid auction up to the exact moment at which the price reaches your true value if we think of each bidder i drop out price as her bid bi this says that people should use their true values as their bids moreover with this definition of bids the rule for determining the outcome of an ascending bid auction can be reformulated as follows the person with the highest bid is the one who stays in the longest thus winning the item and she pays the price at which the second to last person dropped out in other words she pays the bid of this second to last person thus the item goes to the highest bidder at a price equal to the second highest bid this is precisely the rule used in the sealed bid second price auction with the difference being that the ascending bid auction involves real time interaction between the buyers and seller while the sealed bid version takes place purely through sealed bids that the seller opens and evaluates but the close similarity in rules helps to motivate the initially counter intuitive pricing rule for the second price auction it can be viewed as a simulation using sealed bids of an ascending bid auction moreover the fact that bidders want to remain in an ascending bid auction up to exactly the point at which their true value is reached provides the intuition for what will be our main result in the next section after formulating the sealed bid second price auction in terms of game theory we will find that bidding one true value is a dominant strategy conceptually simplest to think of three things happening simultaneously at the end of an ascending bid auction i the second to last bidder drops out ii the last remaining bidder sees that she is alone and stops agreeing to any higher prices and iii the seller awards the item to this last remaining bidder at the current price of course in practice we might well expect that there is some very small increment by which the bid is raised in each step and that the last remaining bidder actually wins only after one more raising of the bid by this tiny increment but keeping track of this small increment makes for a more cumbersome analysis without changing the underlying ideas and so we will assume that the auction ends at precisely the moment when the second highest bidder drops out comparing auction formats in the next two sections we will consider the two main formats for sealed bid auctions in more detail before doing this it worth making two points first the discussion in this section shows that when we analyze bidder behav ior in sealed bid auctions we re also learning about their interactive analogues with the descending bid auction as the analogue of the sealed bid first price auction and the ascending bid auction as the analogue of the sealed bid second price auction second a purely superficial comparison of the first price and second price sealed bid auctions might suggest that the seller would get more money for the item if he ran a first price auction after all he ll get paid the highest bid rather than the second highest bid it may seem strange that in a second price auction the seller is intentionally undercharging the bidders but such reasoning ignores one of the main messages from our study of game theory that when you make up rules to govern people behavior you have to assume that they ll adapt their behavior in light of the rules here the point is that bidders in a first price auction will tend to bid lower than they do in a second price auction and in fact this lowering of bids will tend to offset what would otherwise look like a difference in the size of the winning bid this consideration will come up as a central issue at various points later in the chapter 4 second price auctions the sealed bid second price auction is particularly interesting and there are a number of examples of it in widespread use the auction form used on ebay is essentially a second price auction the pricing mechanism that search engines use to sell keyword based advertising is a generalization of the second price auction as we will see in chapter one of the most important results in auction theory is the fact we mentioned toward the end of the previous section with independent private values bidding your true value is a dominant strategy in a second price sealed bid auction that is the best choice of bid is exactly what the object is worth to you formulating the second price auction as a game to see why this is true we set things up using the language of game theory defining the auction in terms of players strategies and payoffs the bidders will correspond to the players let vi be bidder i true value for the object bidder i strategy is an amount bi to bid as a function of her true value vi in a second price sealed bid auction the payoff to bidder i with value vi and bid bi is defined as follows if bi is not the winning bid then the payoff to i is if bi is the winning bid and some other bj is the second place bid then the payoff to i is vi bj 4 second price auctions figure if bidder i deviates from a truthful bid in a second price auction the payoff is only affected if the change in bid changes the win loss outcome to make this completely well defined we need to handle the possibility of ties what do we do if two people submit the same bid and it tied for the largest one way to handle this is to assume that there is a fixed ordering on the bidders that is agreed on in advance and if a set of bidders ties for the numerically largest bid then the winning bid is the one submitted by the bidder in this set that comes first in this order our formulation of the payoffs works with this more refined definition of winning bid and second place bid and note that in the case of a tie the winning bidder receives the item but pays the full value of her own bid for a payoff of zero since in the event of a tie the first place and second place bids are equal there is one further point worth noting about our formulation of auctions in the language of game theory when we defined games in chapter 6 we assumed that each player knew the payoffs of all players in the game here this isn t the case since the bidders don t know each other values and so strictly speaking we need to use a slight generalization of the notions from chapter 6 to handle this lack of knowledge for our analysis here however since we are focusing on dominant strategies in which a player has an optimal strategy regardless of the other players behavior we will be able to disregard this subtlety truthful bidding in second price auctions the precise statement of our claim about second price auctions is as follows claim in a sealed bid second price auction it is a dominant strategy for each bidder i to choose a bid bi vi to prove this claim we need to show that if bidder i bids bi vi then no deviation from this bid would improve her payoff regardless of what strategy everyone else is using there are two cases to consider deviations in which i raises her bid and deviations in which i lowers her bid the key point in both cases is that the value of i bid only affects whether i wins or loses but never affects how much i pays in the event that she wins the amount paid is determined entirely by the other bids and in particular by the largest among the other bids since all other bids remain the same when i changes her bid a change to i bid only affects her payoff if it changes her win loss outcome this argument is summarized in figure with this in mind let consider the two cases first suppose that instead of bidding vi bidder i chooses a bid bti vi this only affects bidder i payoff if i would lose with bid vi but would win with bid bti in order for this to happen the highest other bid bj must be between bi and bti in this case the payoff to i from deviating would be at most vi bj and so this deviation to bid bti does not improve i payoff next suppose that instead of bidding vi bidder i chooses a bid btit vi this only affects bidder i payoff if i would win with bid vi but would lose with bid btit so before deviating vi was the winning bid and the second place bid bk was between vi and btit in this case i payoff before deviating was vi bk and after deviating it is since i loses so again this deviation does not improve i payoff this completes the argument that truthful bidding is a dominant strategy in a sealed bid second price auction the heart of the argument is the fact noted at the outset in a second price auction your bid determines whether you win or lose but not how much you pay in the event that you win therefore you need to evaluate changes to your bid in light of this this also further highlights the parallels to the ascending bid auction there too the analogue of your bid i e the point up to which you re willing to stay in the auction determines whether you ll stay in long enough to win but the amount you pay in the event that you win is determined by the point at which the second place bidder drops out the fact that truthfulness is a dominant strategy also makes second price auctions con ceptually very clean because truthful bidding is a dominant strategy it is the best thing to do regardless of what the other bidders are doing so in a second price auction it makes 5 first price auctions and other formats sense to bid your true value even if other bidders are overbidding underbidding colluding or behaving in other unpredictable ways in other words truthful bidding is a good idea even if the competing bidders in the auction don t know that they ought to be bidding truthfully as well we now turn to first price auctions where we ll find that the situation is much more complex in particular each bidder now has to reason about the behavior of her competitors in order to arrive at an optimal choice for her own bid 5 first price auctions and other formats in a sealed bid first price auction the value of your bid not only affects whether you win but also how much you pay as a result most of the reasoning from the previous section has to be redone and the conclusions are now different to begin with we can set up the first price auction as a game in essentially the same way that we did for second price auctions as before bidders are players and each bidder strategy is an amount to bid as a function of her true value the payoff to bidder i with value vi and bid bi is simply the following if bi is not the winning bid then the payoff to i is if bi is the winning bid then the payoff to i is vi bi the first thing we notice is that bidding your true value is no longer a dominant strategy by bidding your true value you would get a payoff of if you lose as usual and you would also get a payoff of if you win since you d pay exactly what it was worth to you as a result the optimal way to bid in a first price auction is to shade your bid slightly downward so that if you win you will get a positive payoff determining how much to shade your bid involves balancing a trade off between two opposing forces if you bid too close to your true value then your payoff won t be very large in the event that you win but if you bid too far below your true value so as to increase your payoff in the event of winning then you reduce your chance of being the high bid and hence your chance of winning at all finding the optimal trade off between these two factors is a complex problem that de pends on knowledge of the other bidders and their distribution of possible values for example it is intuitively natural that your bid should be higher i e shaded less closer to your true value in a first price auction with many competing bidders than in a first price auction with only a few competing bidders keeping other properties of the bidders the same this is simply because with a large pool of other bidders the highest competing bid is likely to be larger and hence you need to bid higher to get above this and be the highest bid we will discuss how to determine the optimal bid for a first price auction in section 7 all pay auctions there are other sealed bid auction formats that arise in different set tings one that initially seems counter intuitive in its formulation is the all pay auction each bidder submits a bid the highest bidder receives the item and all bidders pay their bids regardless of whether they win or lose that is the payoffs are now as follows if bi is not the winning bid then the payoff to i is bi if bi is the winning bid then the payoff to i is vi bi games with this type of payoff arise in a number of situations usually where the notion of bidding is implicit political lobbying can be modeled in this way each side must spend money on lobbying but only the successful side receives anything of value for this expenditure while it is not true that the side spending more on lobbying always wins there is a clear analogy between the amount spent on lobbying and a bid with all parties paying their bid regardless of whether they win or lose one can picture similar considerations arising in settings such as design competitions where competing architectural firms spend money on preliminary designs to try to win a contract from a client this money must be spent before the client makes a decision the determination of an optimal bid in an all pay auction shares a number of qualitative features with the reasoning in a first price auction in general you want to bid below your true value and you must balance the trade off between bidding high increasing your probability of winning and bidding low decreasing your expenditure if you lose and increasing your payoff if you win in general the fact that everyone must pay in this auction format means that bids will typically be shaded much lower than in a first price auction the framework we develop for determining optimal bids in first price auctions will also apply to all pay auctions as we will see in section 7 6 common values and the winner curse thus far we have assumed that bidders values for the item being auctioned are independent each bidder knows her own value for the item and is not concerned with how much it is worth to anyone else this makes sense in a lot of situations but it clearly doesn t apply to a setting in which the bidders intend to resell the object in this case there is a common eventual value for the object the amount it will generate on resale but it is not necessarily known each bidder i may have some private information about the common value leading to an estimate vi of this value individual bidder estimates will typically be slightly wrong and they will also typically not be independent of each other one possible model for such estimates is to suppose that the true value is v and that each bidder i estimate vi is defined by vi v xi where xi is a random number with a mean of representing the error in i estimate 6 common values and the winner s curse auctions with common values introduce new sources of complexity to see this let start by supposing that an item with a common value is sold using a second price auction is it still a dominant strategy for bidder i to bid vi in fact it not to get a sense for why this is we can use the model with random errors v xi suppose there are many bidders and that each bids her estimate of the true value then from the result of the auction the winning bidder not only receives the object she also learns something about her estimate of the common value that it was the highest of all the estimates so in particular her estimate is more likely to be an over estimate of the common value than an under estimate moreover with many bidders the second place bid which is what she paid is also likely to be an over estimate as a result she will likely lose money on the resale relative to what she paid this is known as the winner curse and it is a phenomenon that has a rich history in the study of auctions richard thaler review of this history notes that the winner curse appears to have been first articulated by researchers in the petroleum industry in this domain firms bid on oil drilling rights for tracts of land that have a common value equal to the value of the oil contained in the tract the winner curse has also been studied in the context of competitive contract offers to baseball free agents with the unknown common value corresponding to the future performance of the baseball player being courted 2 rational bidders should take the winner curse into account in deciding on their bids a bidder should bid her best estimate of the value of the object conditional on both her private estimate vi and on winning the object at her bid that is it must be the case that at an optimal bid it is better to win the object than not to win it this means in a common value auction bidders will shade their bids downward even when the second price format is used with the first price format bids will be reduced even further determining the optimal bid is fairly complex and we will not pursue the details of it here it is also worth noting that in practice the winner curse can lead to outright losses on the part of the winning bidder since in a large pool of bidders anyone who in fact makes an error and overbids is more likely to be the winner of the auction these cases as well as others one could argue that the model of common values is not entirely accurate one oil company could in principle be more successful than another at extracting oil from a tract of land and a baseball free agent may flourish if he joins one team but fail if he joins another but common values are a reasonable approximation to both settings as to any case where the purpose of bidding is to obtain an item that has some intrinsic but unknown future value moreover the reasoning behind the winner curse arises even when the item being auctioned has related but non identical values to the different bidders 7 advanced material bidding strategies in first price and all pay auctions in the previous two sections we offered some intuition about the way to bid in first price auctions and in all pay auctions but we did not derive optimal bids we now develop models of bidder behavior under which we can derive equilibrium bidding strategies in these auctions we then explore how optimal behavior varies depending on the number of bidders and on the distribution of values finally we analyze how much revenue the seller can expect to obtain from various auctions the analysis in this section will use elementary calculus and probability theory a equilibrium bidding in first price auctions as the basis for the model we want to capture a setting in which bidders know how many competitors they have and they have partial information about their competitors values for the item however they do not know their competitors values exactly let start with a simple case first and then move on to a more general formulation in the simple case suppose that there are two bidders each with a private value that is independently and uniformly distributed between and 3 this information is common knowledge among the two bidders a strategy for a bidder is a function v b that maps her true value v to a non negative bid b we will make the following simple assumptions about the strategies the bidders are using i is a strictly increasing differentiable function so in particular if two bidders have different values then they will produce different bids ii v v for all v bidders can shade their bids down but they will never bid above their true values notice that since bids are always non negative this also means that these two assumptions permit a wide range of strategies for example the strategy of bidding your true value is represented by the function v v while the strategy of shading your bid downward to by a factor of c times your true value is represented by v cv more complex strategies such as v are also allowed although we will see that in first price auctions they are not optimal the two assumptions help us narrow the search for equilibrium strategies the second of our assumptions only rules out strategies based on overbidding that are non optimal fact that the and are the lowest and highest possible values is not crucial by shifting and re scaling these quantities we could equally well consider values that are uniformly distributed between any other pair of endpoints the first assumption restricts the scope of possible equilibrium strategies but it makes the analysis easier while still allowing us to study the important issues finally since the two bidders are identical in all ways except the actual value they draw from the distribution we will narrow the search for equilibria in one further way we will consider the case in which the two bidders follow the same strategy equilibrium with two bidders the revelation principle let consider what such an equilibrium strategy should look like first assumption i says that the bidder with the higher value will also produce the higher bid if bidder i has a value of vi the probability that this is higher than the value of i competitor in the interval is exactly vi therefore i will win the auction with probability vi if i does win i receives a payoff of vi vi putting all this together we see that i expected payoff is g vi vi vi vi now what does it mean for to be an equilibrium strategy it means that for each bidder i there is no incentive for i to deviate from strategy if i competitor is also using strategy it not immediately clear how to analyze deviations to an arbitrary strategy satisfying assumptions i and ii above fortunately there is an elegant device that lets us reason about deviations as follows rather then actually switching to a different strategy bidder i can implement her deviation by keeping the strategy but supplying a different true value to it here is how this works first if i competitor is also using strategy then i should never announce a bid above since i can win with bid and get a higher payoff with bid than with any bid b so in any possible deviation by i the bid she will actually report will lie between and s 1 therefore for the purposes of the auction she can simulate her deviation to an alternate strategy by first pretending that her true value is vit rather than vi and then applying the existing function s to vit instead of vi this is a special case of a much broader idea known as the revelation principle for our purposes we can think of it as saying that deviations in the bidding strategy function can instead be viewed as deviations in the true value that bidder i supplies to her current strategy s with this in mind we can write the condition that i does not want to deviate from strategy s as follows vi vi s vi v vi s v 2 for all possible alternate true values v between and 1 that bidder i might want to supply to the function s is there a function that satisfies this property in fact it is not hard to check that s v v 2 satisfies it to see why notice that with this choice of s the left hand side of inequality 2 becomes vi vi vi 2 2 while the right hand side becomes v vi v 2 vvi 2 collecting all the terms on the left the inequality becomes simply 1 2 i v2 which holds because the left hand side is the square 1 v vi 2 thus the conclusion in this case is quite simple to state if two bidders know they are competing against each other and know that each has a private value drawn uniformly at random from the interval 1 then it is an equilibrium for each to shade their bid down by a factor of 2 bidding half your true value is optimal behavior if the other bidder is doing this as well note that unlike the case of the second price auction we have not identified a dominant strategy only an equilibrium in solving for a bidder s optimal strategy we used each bidder s expectation about her competitor s bidding strategy in an equilibrium these expectations are correct but if other bidders for some reason use non equilibrium strategies then any bidder should optimally respond and potentially also play some other bidding strategy deriving the two bidder equilibrium in our discussion of the equilibrium s v v 2 we initially conjectured the form of the function s and then checked that it satisfied inequality 2 but this approach does not suggest how to discover a function s to use as a conjecture an alternate approach is to derive s directly by reasoning about the condition in inequality 2 here is how we can do this in order for s to satisfy inequality 2 it must have the property that for any true value vi the expected payoff function g v v vi s v is maximized by setting v vi therefore vi should satisfy gt vi 0 where gt is the first derivative of g with respect to v since gt v vi s v vst v by the product rule for derivatives we see that s must solve the differential equation st v 1 s vi vi for all vi in the interval 0 1 this differential equation is solved by the function s vi vi 2 equilibrium with many bidders now let s suppose that there are n bidders where n can be larger than two to start with we ll continue to assume that each bidder i draws her true value vi independently and uniformly at random from the interval between 0 and 1 much of the reasoning for the case of two bidders still works here although the basic formula for the expected payoff changes specifically assumption i still implies that the bidder with the highest true value will produce the highest bid and hence win the auction for a given bidder i with true value vi what is the probability that her bid is the highest this requires each other bidder to have a value below vi since the values are chosen independently this event has a probability of vn 1 therefore bidder i s expected payoff is g vi vn 1 vi s vi 3 the condition for s to be an equilibrium strategy remains the same as it was in the case of two bidders using the revelation principle we view a deviation from the bidding strategy as supplying a fake value v to the function s given this we require that the true value vi produces an expected payoff at least as high as the payoff from any deviation vn 1 vi s vi vn 1 vi s v 4 for all v between 0 and 1 from this we can derive the form of the bidding function s using the differential equation approach that worked for two bidders the expected payoff function g v vn 1 vi s v must be maximized by setting v vi setting the derivative gt vi 0 and applying the product rule to differentiate g we get n 1 vn n 1 vn vi vn vi 0 for all vi between 0 and 1 dividing through by n 1 vn 2 and solving for st vi we get the equivalent but typographically simpler equation st v n 1 1 s vi 5 for all vi between 0 and 1 this differential equation is solved by the function s v n 1 v so if each bidder shades her bid down by a factor of n 1 n then this is optimal behavior given what everyone else is doing notice that when n 2 this is our two bidder strategy the form of this strategy highlights an important principle that we discussed in section 5 about strategic bidding in first price auctions as the number of bidders increases you generally have to bid more aggressively shading your bid down less in order to win for the simple case of values drawn independently from the uniform distribution our analysis here quantifies exactly how this increased aggressiveness should depend on the number of bidders n general distributions in addition to considering larger numbers of bidders we can also relax the assumption that bidders values are drawn from the uniform distribution on an interval suppose that each bidder has her value drawn from a probability distribution over the non negative real numbers we can represent the probability distribution by its cumulative distribution function f for any x the value f x is the probability that a number drawn from the distribution is at most x we will assume that f is a differentiable function most of the earlier analysis continues to hold at a general level the probability that a bidder i with true value vi wins the auction is the probability that no other bidder has a larger value so it is equal to f vi n 1 therefore the expected payoff to vi is catalogue description introduces more of the basic concepts of computer science and object oriented software development with an emphasis on fundamental data structures lists stacks queues trees and associated algorithms this course includes recursion abstract data types and selected topics exploring some of the breadth of computer science prerequisite cmpt or cmpt or cmpt or cmpt with grade at least course website all course relevant information announcements course materials assignments exam schedules etc will be on the moodle website http moodle cs usask ca each student is responsible for checking this website regularly it is also your responsibility to check your paws email account regularly course overview this course introduces the basic concepts of computer science and object oriented software development you will learn about fundamental data structures for organizing data including lists stacks queues trees and hash tables and associated algorithms as well as their time and space efficiency the course will emphasize abstract data types for the design of data storage mechanisms that can be reused and revised you will learn the basics of object oriented programming as a natural technological extension of abstract data types as the practical part of the course you will develop a familiarity with memory management including static and dynamic memory allocation and pointers through hands on implementations an underlying theme of the course is for students to gain programming and debugging skills the con ceptual material covered in the course is actually fairly straightforward and can be mastered with a moderate amount of study the real challenge in this course is to develop the programming skills needed to complete the homework students should practice time management problem solving strategies and critical analytical and scientific thinking learning outcomes by the end of this course your are expected to be able to be proficient in fundamentals of procedural programming specifically programming in the procedural subset of c design algorithms using pseudocode and analyze algorithms written in pseudocode analyze time and space complexity of algorithms and to compare and evaluate algorithms and data structures understand and use dynamic memory and static memory in procedural programming describe and apply the techniques associated with references pointers and addresses explain the concepts behind the use of data structures and determine the appropriateness of different data structures for various purposes design implement and apply specific data types linked lists arrays trees binary search trees and hash table data structures explain the concept of abstract data types in terms of interface and encapsulation design implement and apply abstract data types for linked lists arrays trees binary search trees and hash table data structures apply recursion to computational tasks involving data structures describe and apply the fundamentals of object oriented programming in c specifically as an extension of the adt concept resources textbook information there are no required textbooks however we provide the following recommended references richard f gilberg and behrouz a forouzan data structures a pseudocode approach in c course tech nology thompson k n king c programming a modern approach w w norton company substantial lecture notes will be provided on the course webpage however students should not rely solely on the lecture notes for the course grading scheme the grading scheme for this course appears in the following table assignments eight one tutorial exercises ten midterm exam february final exam april total assignments see the schedule on page for the due date of each assignment there will be nine assignments in this course one approximately every week even numbered assignments will involve no programming but will have written analytical or design problems odd numbered assignments will be exclusively programming programming will often be based on a design or analysis question from a previous assignment submission instructions will be included with each assignment description generally you will upload your solutions as files to moodle unless you are instructed otherwise generally text files are preferred to documents that include formatting e g msword documents a document that cannot be opened will receive a grade of zero do not assume the markers will take the time to open your file if it is in a file format that is not standard note all computer programs must be written in c and must compile using the gnu c compiler g under linux the standard will be the tuxworld usask ca cluster of machines which is the same version of gnu c as found on the linux desktops in the lab introduction there are several concepts that are essential to an understanding of this text we discuss these concepts in the first two chapters chapter basic concepts covers general materials that we use throughout the book chapter recursion discusses the concept of recursion figure i shows the organization of part i part i introduction pseudocode abstract data type implementations generic code algorithm efficiency figure i part i contents recursion versus repetition recursive algorithms chapters covered this part includes two chapters chapter basic concepts the first chapter covers concepts that we use throughout the text you may find that some of them are a review of material from your programming course the major concepts are outlined below pseudocode in all of our texts we use the basic tenet design comes before code throughout the text therefore we separate algorithm design from the code that implements it in a specific language although the underlying language in this book is c pseudocode allows us to separate the algorithm from the implementation abstract data type an abstract data type adt implements a set of algorithms generically so that they can be applied to any data type or construct the beauty of an adt implementation is that the algorithms can handle any data type whether it is a simple integer or a complex record adt implementations in general there are two basic data structures that can be used to implement an abstract data type arrays and linked lists we discuss basic linked list con cepts in chapter and expand on them as necessary in subsequent chapters part i introduction generic code for adts to implement the adt concept we need to use generic code each language provides a different set of tools to implement generic code the c language uses two tools pointer to void and pointer to function algorithm efficiency while many authors argue that today computers and compilers make algo rithm efficiency an academic discussion we believe that an understanding of algorithm efficiency provides the framework for writing better algorithms although we discuss the efficiency of specific algorithms when we develop them in this chapter we discuss the basic concepts and tools for discussing algorithm efficiency chapter recursion in chapter we discuss recursion a concept that is often skipped in an introductory programming course we need to understand recursion to dis cuss data structures because many of the abstract data types are recursive by nature and algorithms that handle them can be better understood using recursion we use recursive algorithms extensively especially in part iii non linear lists recursion versus repetition the first part of the chapter compares and contrasts recursion and repetition and when each should be used recursive algorithms although recursive algorithms are generally elegant they can be difficult to understand in the second part of the chapter we introduce several algo rithms to make the recursive concept clear and to provide a sense of design for creating recursive algorithms chapter basic concepts this text assumes that the student has a solid foundation in structured pro gramming principles and has written programs of moderate complexity although the text uses c for all of its implementation examples the design and logic of the data structure algorithms are based on pseudocode this approach creates a language independent environment for the algorithms in this chapter we establish a background for the tools used in the rest of the text most specifically pseudocode the abstract data type algorithm effi ciency analysis and the concepts necessary to create generic code pseudocode although several tools are used to define algorithms one of the most common is pseudocode pseudocode is an english like representation of the algorithm logic it is part english part structured code the english part provides a relaxed syntax that describes what must be done without showing unnecessary details such as error messages the code part consists of an extended version of the basic algorithmic constructs sequence selection and iteration in this text we use pseudocode to represent both data structures and code data items do not need to be declared the first time we use a data name in an algorithm it is automatically declared its type is determined by context the following statement declares a numeric data item named count and sets its value to zero set count to section pseudocode the structure of the data on the other hand must be declared we use a simple syntactical statement that begins with a structure name and concludes with the keyword end and the name of the structure within the structure we list the structural elements by indenting the data items as shown below this data definition describes a node in a self referential list that consists of a nested structure data and a pointer to the next node link an ele ment type is implied by its name and usage in the algorithm as mentioned pseudocode is used to describe an algorithm to facilitate a discussion of the algorithm statements we number them using the hierar chical system shown in algorithm the following sections describe the components of an algorithm colored comments provide documentation or clarification when required algorithm example of pseudocode algorithm header each algorithm begins with a header that names it lists its parameters and describes any preconditions and postconditions this information is impor tant because it serves to document the algorithm therefore the header information must be complete enough to communicate to the programmer everything he or she must know to write the algorithm in algorithm there is only one parameter the page number chapter basic concepts purpose conditions and return the purpose is a short statement about what the algorithm does it needs to describe only the general algorithm processing it should not attempt to describe all of the processing for example in algorithm the purpose does not need to state that the file will be opened or how the report will be printed similarly in the search example the purpose does not need to state which of the possible array searches will be used the precondition lists any precursor requirements for the parameters for example in algorithm the algorithm that calls sample must pass the page number by reference sometimes there are no preconditions in which case we still list the precondition with a statement that nothing is required as shown below if there are several input parameters the precondition should be shown for each for example a simple array search algorithm has the following header in search the precondition specifies that the two input parameters list and argument must be initialized if a binary search were being used the precondition would also state that the array data must be sorted the postcondition identifies any action taken and the status of any out put parameters in algorithm the postcondition contains two parts first it states that the report has been printed second the reference parameter pagenumber contains the updated number of pages in the report in the search algorithm shown above there is only one postcondition which may be one of two different values if a value is returned it is identified by a return condition often there is none and no return condition is needed in algorithm we return the number of lines printed the search algorithm returns true if the argument was found false if it was not found statement numbers statements are numbered using an abbreviated decimal notation in which only the last of the number sequence is shown on each statement the expanded number of the statement in algorithm that reads the file is section pseudocode the statement that writes the page heading is this technique allows us to identify an individual statement while providing statements that are easily read variables to ensure that the meaning is understood we use intelligent data names that is names that describe the meaning of the data however it is not necessary to define the variables used in an algorithm especially when the name indicates the context of the data the selection of the name for an algorithm or variable goes a long way toward making the algorithm and its coded implementation more readable in general you should follow these rules do not use single character names do not use generic names in application programs examples of generic names are count sum total row column and file in a program of any size there are several counts sums and totals rather add an intelligent qualifier to the generic name so that the reader knows exactly to which piece of data the name refers for example studentcount and numberofstudents are both better than count abbreviations are not excluded as intelligent data names for example stucnt is a good abbreviation for student count and numofstu is a good abbreviation for number of students note however that nostu would not be a good abbreviation for number of students because it is too easily read as no students statement constructs when he first proposed the structured programming model edsger dijkstra stated that any algorithm could be written using only three programming constructs sequence selection and loop our pseudocode contains only these three basic constructs the implementation of these constructs relies on the richness of the implementation language for example the loop can be implemented as a while do while or for statement in the c language sequence a sequence is one or more statements that do not alter the execution path within an algorithm although it is obvious that statements such as assign and add are sequence statements it is not so obvious that a call to other algorithms is also considered a sequence statement the reason calls are considered sequential statements lies in the structured programming concept that each algorithm has only one entry and one exit furthermore when an algorithm completes it returns to the statement immediately after the call that invoked it therefore we can consider an algorithm call a sequence statement in algorithm statements and are sequence statements chapter basic concepts selection a selection statement evaluates a condition and executes zero or more alternatives the results of the evaluation determine which alternates are taken the typical selection statement is the two way selection as implemented in an if statement whereas most languages provide for multiway selections such as the switch in c we provide none in the pseudocode the parts of the selection are identified by indentation as shown in the short pseudocode statement below statement in algorithm is an example of a selection statement the end of the selection is indicated by the end if in statement loop a loop statement iterates a block of code the loop that we use in our pseudocode closely resembles the while loop it is a pretest loop that is the condition is evaluated before the body of the loop is executed if the condition is true the body is executed if the condition is false the loop terminates in algorithm statement is an example of a loop the end of the loop is indicated by end loop in statement algorithm analysis for selected algorithms we follow the algorithm with an analysis section that explains some of its salient points not every line of code is explained rather the analysis examines only those points that either need to be emphasized or that may require some clarification the algorithm analysis also often intro duces style or efficiency considerations pseudocode example as another example of pseudocode consider the logic required to calculate the deviation from a mean in this problem we must first read a series of numbers and calculate their average then we subtract the mean from each number and print the number and its deviation at the end of the calculation we also print the totals and the average the obvious solution is to place the data in an array as they are read algorithm contains the code for this simple problem as it would be imple mented in a callable algorithm section the abstract data type algorithm print deviation from mean for series algorithm analysis there are two points worth mentioning in algorithm first there are no parameters second as previously explained we do not declare variables a variable type and purpose should be easily determined by its name and usage the abstract data type in the history of programming concepts we started with nonstructured linear programs known as spaghetti code in which the logic flow wound through the program like spaghetti on a plate next came the concept of modular programming in which programs were organized in functions each of which still used a lin ear coding technique in the the basic principles of structured programming were formulated by computer scientists such as edsger dijkstra and niklaus wirth they are still valid today atomic and composite data atomic data are data that consist of a single piece of information that is they cannot be divided into other meaningful pieces of data for example the inte ger may be considered a single integer value of course we can decom pose it into digits but the decomposed digits do not have the same characteristics of the original integer they are four single digit integers rang ing from to in some languages atomic data are known as scalar data because of their numeric properties the opposite of atomic data is composite data composite data can be bro ken out into subfields that have meaning as an example of a composite data item consider your telephone number a telephone number actually has three different parts first there is the area code then what you con sider to be your phone number is actually two different data items a prefix consisting of a three digit exchange and the number within the exchange chapter basic concepts consisting of four digits in the past these prefixes were names such as davenport and cypress data type a data type consists of two parts a set of data and the operations that can be performed on the data thus we see that the integer type consists of values whole numbers in some defined range and operations add subtract multi ply divide and any other operations appropriate for the data table shows three data types found in all systems type values operations integer floating point character a b a b table three data types data structure a data structure is an aggregation of atomic and composite data into a set with defined relationships in this definition structure means a set of rules that holds the data together in other words if we take a combination of data and fit them into a structure such that we can define its relating rules we have made a data structure data structures can be nested we can have a data structure that consists of other data structures for example we can define the two structures array and record as shown in table array record homogeneous sequence of data or data types known as elements heterogeneous combination of data into a single structure with an identi fied key position association among the elements no association table data structure examples section the abstract data type most of the programming languages support several data structures in addi tion modern programming languages allow programmers to create new data structures for an application abstract data type generally speaking programmers capabilities are determined by the tools in their tool kits these tools are acquired by education and experience a knowledge of data structures is one of those tools when we first started programming there were no abstract data types if we wanted to read a file we wrote the code to read the physical file device it did not take long to realize that we were writing the same code over and over again so we created what is known today as an abstract data type adt we wrote the code to read a file and placed it in a library for all pro grammers to use this concept is found in modern languages today the code to read the keyboard is an adt it has a data structure a character and a set of opera tions that can be used to read that data structure using the adt we can not only read characters but we can also convert them into different data struc tures such as integers and strings with an adt users are not concerned with how the task is done but rather with what it can do in other words the adt consists of a set of defini tions that allow programmers to use the functions while hiding the implemen tation this generalization of operations with unspecified implementations is known as abstraction we abstract the essence of the process and leave the implementation details hidden consider the concept of a list at least four data structures can support a list we can use a matrix a linear list a tree or a graph if we place our list in an adt users should not be aware of the structure we use as long as they can insert and retrieve data it should make no difference how we store the data figure shows four logical structures that might be used to hold a list chapter basic concepts b linear list a matrix c tree figure some data structures d graph as another example consider the system analyst who needs to simulate the waiting line of a bank to determine how many tellers are needed to serve customers efficiently this analysis requires the simulation of a queue how ever queues are not generally available in programming languages even if a queue type were available our analyst would still need some basic queue operations such as enqueuing insertion and dequeuing deleting for the simulation there are two potential solutions to this problem we can write a pro gram that simulates the queue our analyst needs in this case our solution is good only for the one application at hand or we can write a queue adt that can be used to solve any queue problem if we choose the latter course our analyst still needs to write a program to simulate the banking application but doing so is much easier and faster because he or she can concentrate on the application rather than the queue we are now ready to define adt an abstract data type is a data declara tion packaged together with the operations that are meaningful for the data type in other words we encapsulate the data and the operations on the data and then we hide them from the user we cannot overemphasize the importance of hiding the implementation the user should not have to know the data structure to use the adt referring to our queue example the application program should have no knowledge of the data structure all references to and manipulation of the data in the queue must be handled through defined interfaces to the section model for an abstract data type structure allowing the application program to directly reference the data structure is a common fault in many implementations this keeps the adt from being fully portable to other applications model for an abstract data type the adt model is shown in figure the colored area with an irregular out line represents the adt inside the adt are two different aspects of the model data structures and functions public and private both are entirely contained in the model and are not within the application program scope however the data structures are available to all of the adt functions as needed and a function may call on other functions to accomplish its task in other words the data structures and the functions are within scope of each other figure abstract data type model adt operations data are entered accessed modified and deleted through the external inter face drawn as a passageway partially in and partially out of the adt only the public functions are accessible through this interface for each adt opera tion there is an algorithm that performs its specific task only the operation name and its parameters are available to the application and they provide the only interface to the adt chapter basic concepts adt data structure when a list is controlled entirely by the program it is often implemented using simple structures similar to those used in your programming class because the abstract data type must hide the implementation from the user however all data about the structure must be maintained inside the adt just encapsulating the structure in an adt is not sufficient it is also neces sary for multiple versions of the structure to be able to coexist consequently we must hide the implementation from the user while being able to store dif ferent data in this text we develop adts for stacks queues lists binary search trees avl trees b trees heaps and graphs if you would like a preview look at the stack adt in chapter adt implementations there are two basic structures we can use to implement an adt list arrays and linked lists array implementations in an array the sequentiality of a list is maintained by the order structure of elements in the array indexes although searching an array for an individual element can be very efficient addition and deletion of elements are complex and inefficient processes for this reason arrays are seldom used especially when the list changes frequently in addition array implementations of non linear lists can become excessively large especially when there are several successors for each element appendix f provides array implementations for two adts linked list implementations a linked list is an ordered collection of data in which each element contains the location of the next element or elements in a linked list each element con tains two parts data and one or more links the data part holds the application data the data to be processed links are used to chain the data together they contain pointers that identify the next element or elements in the list we can use a linked list to create linear and non linear structures in lin ear linked lists each element has only zero or one successor in non linear linked lists each element can have zero one or more successors the major advantage of the linked list over the array is that data are eas ily inserted and deleted it is not necessary to shift elements of a linked list to make room for a new element or to delete an element on the other hand because the elements are no longer physically sequenced we are limited to sequential searches we cannot use a binary search sequential and binary searches are discussed in chapter when we examine trees you will see several data structures that allow for easy updates and efficient searches section adt implementations figure a shows a linked list implementation of a linear list the link in each element except the last points to its unique successor the link in the last element contains a null pointer indicating the end of the list figure b shows a linked list implementation of a non linear list an element in a non linear list can have two or more links here each element contains two links each to one successor figure c contains an exam ple of an empty list linear or non linear we define an empty list as a null list pointer list data link data link data link a linear list data link link data link b non linear list link data link in this section we discuss only the basic concepts for linked lists we expand on these concepts in future chapters nodes in linked list implementation the elements in a list are called nodes a node is a structure that has two parts the data and one or more links figure shows two different nodes one for a linear list and the other for a non linear list the nodes in a linked list are called self referential structures in a self referential structure each instance of the structure contains one or more pointers to other instances of the same structural type in figure the colored boxes with arrows are the pointers that make the linked list a self referential structure the data part in a node can be a single field multiple fields or a struc ture that contains several fields but it always acts as a single field figure shows three designs for a node of a linear list the upper left node contains a chapter basic concepts a node in a linear list b node in a non linear list figure nodes single field a number and a link the upper right node is more typical it contains three data fields a name an id and grade points grdpts and a link the third example is the one we recommend the fields are defined in their own structure which is then put into the definition of a node structure figure linked list node structures pointers to linked lists a linked list must always have a head pointer depending on how we use the list we may have several other pointers as well for example if we are going to search a linked list we will need an additional pointer to the location where we found the data we were looking for furthermore in many struc tures programming is more efficient if there is a pointer to the last node in the list as well as a head pointer generic code for adts in data structures we need to create generic code for abstract data types generic code allows us to write one set of code and apply it to any data type for section generic code for adts example we can write generic functions to implement a stack structure we can then use the generic functions to implement an integer stack a float stack a double stack and so on although some high level languages such as c and java provide special tools to handle generic code c has limited capability through two features pointer to void and pointer to function pointer to void the first feature is pointer to void because c is strongly typed operations such as assign and compare must use compatible types or be cast to compatible types the one exception is the pointer to void which can be assigned without a cast in other words a pointer to void is a generic pointer that can be used to represent any data type during compilation or run time figure shows the idea of a pointer to void note that a pointer to void is not a null pointer it is pointing to a generic data type void void void figure pointer to void p f figure pointers for program program uses a pointer to void that we can use to print either an integer or a floating point number chapter basic concepts program demonstrate pointer to void demonstrate pointer to void written by date include stdio h int main local definitions void p int i float f statements p i printf i contains d n int p p f printf f contains f n float p return main results i contains f contains program analysis the program is trivial but it demonstrates the point the pointer p is declared as a void pointer but it can accept the address of an integer or floating point number however we must remember a very import point about pointers to void a pointer to void cannot be dereferenced unless it is cast in other words we cannot use p without casting that is why we need to cast the pointer in the print function before we use it for printing example as another example let us look at a system function malloc this function returns a pointer to void the designers of the malloc function needed to dynamically allocate any type of data however instead of using several mal loc functions each returning a pointer to a specific data type int float double and so on they designed a generic function that returns a pointer to void void while it is not required we recommend that the returned pointer be cast to the appropriate type the following shows the use of malloc to create a pointer to an integer intptr int malloc sizeof int section generic code for adts example now let look at an example that is similar to what we use to implement our adts we need to have a generic function to create a node structure the structure has two fields data and link the link field is a pointer to the node structure the data field however can be any type integer floating point string or even another structure to make the function generic so that we can store any type of data in the node we use a void pointer to data stored in dynamic memory we declare the node structure as shown in figure to next node figure pointer to node node now let write the program that calls a function that accepts a pointer to data of any type and creates a node that stores the data pointer and a link pointer because we don t know where the link pointer will be pointing we make it null the pointer design is shown in figure figure pointers for programs and typically adts are stored in their own header files we begin therefore by writing the code for creating the node and placing the code in a header file this code is shown in program program create node header file header file for create node structure continued chapter basic concepts program create node header file continued written by date typedef struct node void dataptr struct node link node createnode creates a node in dynamic memory and stores data pointer in it pre itemptr is pointer to data to be stored post node created and its address returned node createnode void itemptr node nodeptr nodeptr node malloc sizeof node nodeptr dataptr itemptr nodeptr link null return nodeptr createnode now that we ve created the data structure and the create node function we can write program to demonstrate the use of void pointers in a node program demonstrate node creation function demonstrate simple generic node creation function written by date include stdio h include stdlib h include h header file int main void local definitions int newdata int nodedata node node statements newdata int malloc sizeof int newdata continued section generic code for adts program demonstrate node creation function continued node createnode newdata nodedata int node dataptr printf data from node d n nodedata return main results data from node program analysis there are several important concepts in this program first the data to be stored in the node is represented by a void pointer because there are usually many instances of these nodes in a program the data are stored in dynamic memory the allocation and storage of the data are the responsibility of the programmer we show these two steps in state ments and the createnode function allocates a node structure in dynamic memory stores the data void pointer in the node and then returns the node address in statement we store the void pointer from the node into an integer pointer because c is strongly typed this assignment must be cast to an integer pointer so while we can store an address in a void pointer without knowing its type the reverse is not true to use a void pointer even in an assignment it must be cast example adt structures generally contain several instances of a node to better dem onstrate the adt concept therefore let modify program to contain two different nodes in this simple example we point the first node to the second node the pointer structure for the program is shown in figure figure structure for two linked nodes the pointer values in figure represent the settings at the end of program chapter basic concepts program create list with two linked nodes create a list with two linked nodes written by date include stdio h include stdlib h include h header file int main void local definitions int newdata int nodedata node node statements newdata int malloc sizeof int newdata node createnode newdata newdata int malloc sizeof int newdata node link createnode newdata nodedata int node dataptr printf data from node d n nodedata nodedata int node link dataptr printf data from node d n nodedata return main results data from node data from node program analysis this program demonstrates an important point in a generic structure such as shown in the program the nodes and the data must both be in dynamic memory when study ing the program follow the code through figure pointer to function the second tool that is required to create c generic code is pointer to func tion in this section we discuss how to use it functions in your program occupy memory the name of the function is a pointer constant to its first byte of memory for example imagine that you have four functions stored in memory main fun pun and sun this section generic code for adts relationship is shown graphically in figure the name of each function is a pointer to its code in memory main int main void void fun void int pun int int pointers to function figure functions in memory memory double sun float defining pointers to functions just as with all other pointer types we can define pointers to function vari ables and store the address of fun pun and sun in them to declare a pointer to function we code it as if it were a prototype definition with the function pointer in parentheses this format is shown in figure the parentheses are important without them c interprets the function return type as a pointer using pointers to functions now that you ve seen how to create and use pointers to functions let write a generic function that returns the larger of any two pieces of data the func tion uses two pointers to void as described in the previous section while our function needs to determine which of the two values represented by the void pointers is larger it cannot directly compare them because it doesn t know what type casts to use with the void pointers only the application program knows the data types the solution is to write simple compare functions for each program that uses our generic function then when we call the generic compare function we use a pointer to function to pass it the specific compare function that it must use example as we saw in our discussion of pointers to void we place our generic func tion which we call larger in a header file so that it can be easily used the program interfaces and pointers are shown in figure chapter basic concepts figure pointers to functions figure design of larger function the code is shown in program program larger compare function generic function to determine the larger of two values referenced as void pointers pre and are pointers to values of an unknown type ptrtocmpfun is address of a function that knows the data types post data compared and larger value returned continued section generic code for adts program larger compare function continued void larger void void int ptrtocmpfun void void if ptrtocmpfun return else return larger program contains an example of how to use our generic compare program and pass it a specific compare function program compare two integers demonstrate generic compare functions and pointer to function written by date include stdio h include stdlib h include h header file int compare void void int main void local definitions int i int j int lrg statements lrg int larger i j compare printf larger value is d n lrg return main compare integer specific compare function pre and are pointers to integer values post returns if returns if int compare void void continued chapter basic concepts program compare two integers continued if int int return else return compare results larger value is example now let write a program that compares two floating point numbers we can use our larger function but we need to write a new compare function we repeat program changing only the compare function and the data specific statements in main the result is shown in program program compare two floating point values demonstrate generic compare functions and pointer to function written by date include stdio h include stdlib h include h header file int compare void void int main void local definitions float float float lrg statements lrg float larger compare printf larger value is 1f n lrg return main compare float specific compare function pre and are pointers to integer values post returns if continued section algorithm efficiency program compare two floating point values continued returns if int compare void void if float float return else return compare results larger value is algorithm efficiency there is seldom a single algorithm for any problem when comparing two different algorithms that solve the same problem you often find that one algorithm is an order of magnitude more efficient than the other in this case it only makes sense that you be able to recognize and choose the more effi cient algorithm although computer scientists have studied algorithms and algorithm effi ciency extensively the field has not been given an official name brassard and bratley coined the term algorithmics which they define as the systematic study of the fundamental techniques used to design and analyze efficient algo rithms we use the term in this book if a function is linear that is if it contains no loops or recursions its efficiency is a function of the number of instructions it contains in this case its efficiency depends on the speed of the computer and is generally not a fac tor in the overall efficiency of a program on the other hand functions that use loops or recursion vary widely in efficiency the study of algorithm effi ciency therefore focuses on loops our analysis concentrates on loops because recursion can always be converted to a loop as we study specific examples we generally discuss the algorithm efficiency as a function of the number of elements to be processed the general format is f n efficiency the basic concepts are discussed in this section gilles brassard and paul bratley algorithmics theory and practice englewood cliffs n j prentice hall xiii chapter basic concepts linear loops let us start with a simple loop we want to know how many times the body of the loop is repeated in the following code assuming i is an integer the answer is times the number of itera tions is directly proportional to the loop factor the higher the factor the higher the number of loops because the efficiency is directly propor tional to the number of iterations it is f n n however the answer is not always as straightforward as it is in the above example for instance consider the following loop how many times is the body repeated in this loop here the answer is times why in this example the number of iterations is half the loop factor once again however the higher the factor the higher the number of loops the efficiency of this loop is proportional to half the factor which makes it f n n if you were to plot either of these loop examples you would get a straight line for that reason they are known as linear loops logarithmic loops in a linear loop the loop update either adds or subtracts in a logarithmic loop the controlling variable is multiplied or divided in each iteration how many times is the body of the loops repeated in the following program segments to help you understand this problem table analyzes the values of i for each iteration as you can see the number of iterations is in both cases the reason is that in each iteration the value of i doubles for the mul tiply loop and is cut in half for the divide loop thus the number of iterations for algorithm efficiency analysis we use c code so that we can clearly see the looping constructs section algorithm efficiency multiply divide iteration value of i iteration value of i 500 125 64 exit exit table analysis of multiply and divide loops generalizing the analysis we can say that the iterations in loops that multiply or divide are determined by the following formula f n logn nested loops loops that contain loops are known as nested loops when we analyze nested loops we must determine how many iterations each loop completes the total is then the product of the number of iterations in the inner loop and the number of iterations in the outer loop iterations outer loop iterations x inner loop iterations we now look at three nested loops linear logarithmic quadratic and dependent quadratic chapter basic concepts linear logarithmic the inner loop in the following code is a loop that multiplies to see the mul tiply loop look at the update expression in the inner loop the number of iterations in the inner loop is therefore however because the inner loop is controlled by an outer loop the above formula must be multiplied by the number of times the outer loop executes which is this gives us which is generalized as f n n logn quadratic in a quadratic loop the number of times the inner loop executes is the same as the outer loop consider the following example the outer loop for i is executed times for each of its iterations the inner loop for j is also executed times the answer therefore is which is the square of the loops this formula generalizes to dependent quadratic in a dependent quadratic loop the number of iterations of the inner loop depends on the outer loop consider the nested loop shown in the following example section algorithm efficiency the outer loop is the same as the previous loop however the inner loop depends on the outer loop for one of its factors it is executed only once the first iteration twice the second iteration three times the third iteration and so forth the number of iterations in the body of the inner loop is calculated as shown below if we compute the average of this loop it is which is the same as the number of iterations plus divided by mathematically this calculation is generalized to multiplying the inner loop by the number of times the outer loop is exe cuted gives us the following formula for a dependent quadratic loop big o notation with the speed of computers today we are not concerned with an exact mea surement of an algorithm efficiency as much as we are with its general order of magnitude if the analysis of two algorithms shows that one executes iterations while the other executes iterations they are both so fast that we can t see the difference on the other hand if one iterates times and the other times we should be concerned we have shown that the number of statements executed in the function for n elements of data is a function of the number of elements expressed as f n although the equation derived for a function may be complex a dominant fac tor in the equation usually determines the order of magnitude of the result therefore we don t need to determine the complete measure of efficiency only the factor that determines the magnitude this factor is the big o as in on the order of and is expressed as o n that is on the order of n this simplification of efficiency is known as big o analysis for example if an algorithm is quadratic we would say its efficiency is or on the order of n squared the big o notation can be derived from f n using the following steps in each term set the coefficient of the term to chapter basic concepts keep the largest term in the function and discard the others terms are ranked from lowest to highest as shown below for example to calculate the big o notation for we first remove all coefficients this gives us which after removing the smaller factors gives us which in big o notation is stated as we first eliminate all of the coefficients as shown below the largest term in this expression is the first one so we can say that the order of a polynomial expression is standard measures of efficiency computer scientists have defined seven categories of algorithm efficiency we list them in table in order of decreasing efficiency and show the first five of them graphically in figure section algorithm efficiency efficiency big o iterations estimated time logarithmic o logn microseconds linear o n seconds linear logarithmic o n logn seconds quadratic o minutes polynomial o nk hours exponential o cn intractable factorial o n intractable table measures of efficiency for n any measure of efficiency presumes that a sufficiently large sample is being considered if you are dealing with only elements and the time required is a fraction of a second there is no meaningful difference between two algorithms on the other hand as the number of elements being pro cessed grows the difference between algorithms can be staggering returning for a moment to the question of why we should be concerned about efficiency consider the situation in which you can solve a problem in three ways one is linear another is linear logarithmic and a third is quadratic the magnitude of their efficiency for a problem containing o n nlogn n logn n figure plot of effeciency measures chapter basic concepts elements shows that the linear solution requires a fraction of a second whereas the quadratic solution requires minutes see table looking at the problem from the other end if we are using a computer that executes a million instructions per second and the loop contains instructions we spend second for each iteration of the loop table also contains an estimate of the time needed to solve the problem given differ ent efficiencies big o analysis examples to demonstrate the concepts we have been discussing we examine two more algorithms add and multiply two matrices add square matrices to add two square matrices we add their corresponding elements that is we add the first element of the first matrix to the first element of the second matrix the second element of the first matrix to the second element of the second matrix and so forth of course the two matrices must be the same size this concept is shown in figure the pseudocode to add two matrices is shown in algorithm algorithm add two matrices section algorithm efficiency algorithm analysis in this algorithm we see that for each element in a row we add all of the elements in a column this is the classic quadratic loop the efficiency of the algorithm is therefore o or o multiply square matrices when two square matrices are multiplied we must multiply each element in a row of the first matrix by its corresponding element in a column of the second matrix the value in the resulting matrix is then the sum of the products for example given the matrix in our addition example above the first element in the resulting matrix that is the element at is the sum of the products obtained by multiplying each element in the first row row by its corresponding element in the first column column the value of the element at index location is the sum of the products of each element in the first row again row multiplied by its corresponding element in the second column column the value of the element at index location is the sum of the products of each element in the second row multiplied by the corresponding elements in the third column once again the square matrices must be the same size figure graphi cally shows how two matrices are multiplied generalizing this concept we see that the pseudocode used for multiplying matrices is provided in algorithm algorithm multiply two matrices continued chapter basic concepts algorithm multiply two matrices continued algorithm analysis in this algorithm we see three nested loops because each loop starts at the first ele ment we have a cubic loop loops with three nested loops have a big o efficiency of o or o it is also possible to multiply two matrices if the number of rows in the first matrix is the same as the number of columns in the second we leave the solution to this prob lem as an exercise exercise figure multiply matrices section summary key terms abstract data type adt algorithmics atomic data big o notation composite data construct data data structure data type dependent quadratic loop empty list encapsulation generic code intelligent data names linear loop link linked list logarithmic loop loop modular programming nested loop node pointer to void pointer to function pseudocode quadratic loop return condition self referential selection statement sequence spaghetti code structured programming summary one of the most common tools used to define algorithms is pseudocode pseudocode is an english like representation of the code required for an algorithm it is part english part structured code atomic data are data that are single nondecomposable entities atomic data types are defined by a set of values and a set of operations that act on the values a data structure is an aggregation of atomic and composite data with a defined relationship an abstract data type adt is a data declaration packaged together with the operations that are meaningful for the data type there are two basic structures used to implement an adt list arrays and linked lists in an array the sequentiality of a list is preserved by the ordered structure of elements although searching an array is very efficient adding and deleting is not although adding and deleting in a linked list is efficient searching is not because we must use a sequential search in a linked list each element contains the location of the next element or elements chapter basic concepts abstract data types require generic algorithms which allow an algorithm to be used with multiple data types the c language has two features that allow the creation of generic code pointer to void and pointer to function a void pointer is a generic pointer that can be used to represent any data type a pointer to void cannot be dereferenced which means that nonassign ment references to a void pointer must be cast to the correct type the name of a function is a pointer constant to the first byte of a function we can use pointer to function as a place holder for the name of a func tion in a parameter list of a generic function algorithm efficiency is generally defined as a function of the number of elements being processed and the type of loop being used the efficiency of a logarithmic loop is f n logn the efficiency of a linear loop is f n n the efficiency of a linear logarithmic loop is f n n logn the efficiency of a quadratic loop is f n the efficiency of a dependent quadratic loop is f n n n the efficiency of a cubic loop is f n the simplification of efficiency is known as big o notation the seven standard measures of efficiencies are o logn o n o n logn o o nk o cn and o n practice sets exercises structure charts and pseudocode are two different design tools how do they differ and how are they similar using different syntactical constructs write at least two pseudocode state ments to add to a number for example any of the following statements could be used to get data from a file explain how an algorithm in an application program differs from an algorithm in an abstract data type identify the atomic data types for your primary programming language section practice sets identify the composite data types for your primary programming language reorder the following efficiencies from smallest to largest a b n c d e nlog n reorder the following efficiencies from smallest to largest a nlog n b n c d determine the big o notation for the following a b n c nlog n d calculate the run time efficiency of the following program segment for i i n i printf d i calculate the run time efficiency of the following program segment for i i n i for j j n j for k k n k print d d d n i j k if the algorithm doit has an efficiency factor of calculate the run time efficiency of the following program segment for i i n i doit if the efficiency of the algorithm doit can be expressed as o n cal culate the efficiency of the following program segment for i i n i for j j n j doit if the efficiency of the algorithm doit can be expressed as o n cal culate the efficiency of the following program segment for i i n i doit given that the efficiency of an algorithm is if a step in this algorithm takes nanosecond seconds how long does it take the algorithm to process an input of size chapter basic concepts given that the efficiency of an algorithm is if a step in this algorithm takes nanosecond seconds how long does it take the algorithm to process an input of size given that the efficiency of an algorithm is n if a step in this algo rithm takes nanosecond seconds how long does it take the algo rithm to process an input of size 17 an algorithm processes a given input of size n if n is the run time is milliseconds if n is 384 the run time is milliseconds what is the efficiency what is the big o notation 18 an algorithm processes a given input of size n if n is the run time is milliseconds if n is 384 the run time is milliseconds what is the efficiency what is the big o notation an algorithm processes a given input of size n if n is the run time is milliseconds if n is 384 the run time is milliseconds what is the efficiency what is the big o notation three students wrote algorithms for the same problem they tested the three algorithms with two sets of data as shown below a case n run time for student run time for student run time for student what is the efficiency for each algorithm which is the best which is the worst what is the minimum number of test cases n in which the best algorithm has the best run time we can multiply two matrices if the number of columns in the first matrix is the same as the number of rows in the second write an algorithm that multiplies an m n matrix by a n k matrix write a compare function see program to compare two strings problems write a pseudocode algorithm for dialing a phone number write a pseudocode algorithm for giving all employees in a company a cost of living wage increase of assume that the payroll file includes all current employees section practice sets write a language specific implementation for the pseudocode algorithm in problem write a pseudocode definition for a textbook data structure write a pseudocode definition for a student data structure projects your college bookstore has hired you as a summer intern to design a new textbook inventory system it is to include the following major processes a ordering textbooks b receiving textbooks c determining retail price d pricing used textbooks e determining quantity on hand f recording textbook sales g recording textbook returns write the abstract data type algorithm headers for the inventory system each header should include name parameters purpose preconditions postconditions and return value types you may add additional algorithms as required by your analysis write the pseudocode for an algorithm that converts a numeric score to a letter grade the grading scale is the typical absolute scale in which or more is an a to is a b to is a c and to is a d anything below is an f write the pseudocode for an algorithm that receives an integer and then prints the number of digits in the integer and the sum of the digits for example given it would print that there are digits with a sum of 31 write the pseudocode for a program that builds a frequency array for data values in the range to and then prints their histogram the data are to be read from a file the design for the program is shown in figure 17 figure 17 design for frequency histogram program chapter basic concepts each of the subalgorithms is described below a the getdata algorithm reads the file and stores the data in an array b the printdata algorithm prints the data in the array c the makefrequency algorithm examines the data in the array one ele ment at a time and adds to the corresponding element in a frequency array based on the data value d the makehistogram algorithm prints out a vertical histogram using asterisks for each occurrence of an element for example if there were five value and eight value in the data it would print rewrite program to create a list of nodes each node consists of two fields the first field is a pointer to a structure that contains a student id integer and a grade point average float the second field is a link the data are to be read from a text file then write a program to read a file of at least students and test the function you wrote you will also need to use the generic compare code in program in your program catalogue description continues the development of programming skills started in cmpt with an emphasis on object oriented programming data structures for the storage and efficient retrieval of information will be studied and analyzed in particular stacks queues linked lists and simple binary trees examples and exercises will be drawn from engineering applications and numerical methods prerequisite cmpt or cmpt with grade at least course website all course relevant information announcements course materials assignments exam schedules etc will be on the moodle website http moodle cs usask ca each student is responsible for checking this website regularly it is also your responsibility to check your paws email account regularly course overview this course introduces the basic concepts of computer science and object oriented software development you will learn about fundamental data structures for organizing data including lists stacks queues trees and hash tables and associated algorithms as well as their time and space efficiency the course will emphasize abstract data types for the design of data storage mechanisms that can be reused and revised you will learn the basics of object oriented programming as a natural technological extension of abstract data types as the practical part of the course you will develop a familiarity with memory management including static and dynamic memory allocation and pointers through hands on implementations an underlying theme of the course is for students to gain programming and debugging skills the con ceptual material covered in the course is actually fairly straightforward and can be mastered with a moderate amount of study the real challenge in this course is to develop the programming skills needed to complete the homework students should practice time management problem solving strategies and critical analytical and scientific thinking learning outcomes by the end of this course your are expected to be able to be proficient in fundamentals of procedural programming specifically programming in the procedural subset of c design algorithms using pseudocode and analyze algorithms written in pseudocode analyze time and space complexity of algorithms and to compare and evaluate algorithms and data structures understand and use dynamic memory and static memory in procedural programming describe and apply the techniques associated with references pointers and addresses explain the concepts behind the use of data structures and determine the appropriateness of different data structures for various purposes design implement and apply specific data types linked lists arrays trees binary search trees and hash table data structures explain the concept of abstract data types in terms of interface and encapsulation design implement and apply abstract data types for linked lists arrays trees binary search trees and hash table data structures apply recursion to computational tasks involving data structures describe and apply the fundamentals of object oriented programming in c specifically as an extension of the adt concept resources textbook information there are no required textbooks however we provide the following recommended references richard f gilberg and behrouz a forouzan data structures a pseudocode approach in c course tech nology thompson k n king c programming a modern approach w w norton company substantial lecture notes will be provided on the course webpage however students should not rely solely on the lecture notes for the course grading scheme the grading scheme for this course appears in the following table assignments ten tutorial exercises ten midterm exam february evening final exam april total assignments see the schedule on page for the due date of each assignment there will be ten assignments in this course one approximately every week assignments will consist of two portions written questions that require written answers programming questions which require you to write computer programs submission instructions will be included with each assignment description generally you will upload your solutions as files to moodle unless you are instructed otherwise generally text files are preferred to documents that include formatting e g msword documents a document that cannot be opened will receive a grade of zero do not assume the markers will take the time to open your file if it is in a file format that is not standard note all computer programs must be written in c and must compile using the gnu c compiler g under linux the standard will be the tuxworld usask ca cluster of machines which is the same version of gnu c as found on the linux desktops in the lab tutorials tutorials have associated exercises which we expect you to do for the skills you learn by doing them you are expected to submit your tutorial exercises weekly along with your assignment solutions these exercises will be graded and will make up of your total course grade you should expect to complete these exercises in the time allotted for tutorials and should not require any time outside the tutorials mid term examination the midterm exam is scheduled for the evening of february location tba it will be held in common with cmpt and both lecture sections of cmpt there are two seatings that students can choose from to meet their schedule time note 30pm 00pm students cannot leave early before 30pm 30pm 00pm students will not be allowed to enter late after we ll use moodle to sign up for the midterms starting about weeks before the rules about leaving early and arriving late are to ensure fairness in the examination the mid term examination is written closed book only bring water your student card and writing instruments the mid term examination is intended to provide practice for the final exam and to provide feedback to students regarding their current performance please see the section on policies for important information concerning missed midterms and final exams in the case of a missed midterm the instructor in consultation with the student will determine how the missed work will be compensated for one potential alternative is transferring the weight of the midterm onto the final examination final examination the final examination common to cmpt and both lecture sections of cmpt will be scheduled by central timetabling to occur during the usual final examination interval it will be three hours long written closed book bring only water your student card and writing instruments please see the section on policies for important information concerning missed midterms and final exams attendance expectations we expect attendance in lecture and in tutorial with reasonable allowances for illness and unforeseen life events in other words treat cmpt as if you were an intern at a real company your bosses the instructors and teaching assistants expect you to show up on time to all lectures and tutorials master your skills do your share of the work and behave professionally there are almost no consequences for missing class or tutorial apart from the opportunity cost of paying tuition and not being present to discuss course material with instructors tutorials and help sessions tutorials tutorials in a laboratory setting are mandatory and include new material not presented in class lectures emphasize the data organization concepts using pseudocode tutorials focus on how to implement in c the concepts studied in lecture material presented in tutorial is examinable if you miss a tutorial section you may try to attend another section during that week but there is considerable risk you will not be able to find a seat section day time location leader monday 30pm 50pm spinks matthew miller tuesday 30pm 50pm spinks matthew miller help sessions obstacles to progress and completion of assignments can sometimes be part of the homework i e something we want you to think about carefully and sometimes beyond the scope of the course i e a problem that you can t really be expected to manage in first year and it can be nearly impossible to tell the difference without some advice from a ta or instructor there are several help sessions in the spinks computer lab that are specifically for cmpt students the tas are all prepared for the assignments and lab questions we highly recommend you to work in the computer lab because it is very helpful if you can get help when you have difficulties the schedule will be announced in the first two weeks of the term on moodle lecture tutorial schedule and topics the following schedule is subject to minor adjustments the topics may shift a bit but due dates will not date lecture topic tutorial topic week jan first day of classes topic introduction topic software design topic algorithm analysis no tutorials this week week cmpt tutorials start this week topic algorithm analysis topic pointers and references tut the unix command line and the com piler assignment due on jan at pseudocode software design algorithm complexity last day for changing registration week topic pointers and references topic memory tut dynamic memory pointers refs ar rays and records assignment due on jan at pointers references c strings arrays week topic abstract data types topic 1 lists tut multiple file compilation assignment due on jan at adts memory references week feb topic lists topic list traversal tut c strings and arrays as pointers assignment due on feb at lists dynamic memory week topic stacks topic recursion 1 5hr topic queues 5hr tut singly linked list implementation assignment due on feb at stacks recursion 26 mid term break week feb topic 2 queues 5hr topic 1 trees 2 5hr no tutorial work this week tutorials open for consulting for midterm feb mid term exam 30pm see section on midterm examination no assignment due this week week mar 2 5 topic 2 trees 5hr topic 1 binary search trees topic 1 objects 5hr tut 6 array based stack adt assignment 6 due on mar at recursion trees week topic 2 objects 2 5hr topic recap lists stacks queues as objects 5hr tut binary tree adt implementation assignment due on mar at bsts queues last day for withdrawing without penalty week 16 topic recap lists stacks queues as objects 2 5hr topic 1 hashing 5hr tut object oriented programming assignment due on mar at object oriented programming week 23 topic 2 hashing topic 1 topics tut 9 tba assignment 9 due on mar at object oriented programming week mar 30 apr topic 2 topics tut tba assignment due on apr at tba apr 3 good friday university closed week apr 6 topic course review no tutorials this week last day of classes final exam 30 centrally scheduled course syllabus cmpt 3 intermediate data structures and algorithms catalogue description object oriented design of formal abstract data types this course focuses on data structure design and use in java basic data structures are reviewed in an object oriented context and new data structures and related algorithms are introduced ordered trees balanced trees simple spatial trees graph representations and searching path algorithms depth breadth first searches direct and b tree files and advanced sorting algorithms there is emphasis on algorithm analysis in the context of measuring the efficiency of various data structure operations and suitability of data structures to various tasks prerequisite cmpt course objectives upon completion of this course successful students should be able to use and implement data structures for efficient storage and retrieval of data in collections lists queues balanced trees use and implement graph data structures adjacency list adjacency matrix use and implement abstract iteration mechanisms for containers and graphs create new data structures with minimal effort and maximum re use through restriction and extension of existing data structures implement customized data structures from scratch within an object oriented design framework choose appropriate data structures for maximizing efficiency of data storage and retrieval for a given task solve problems that can be represented by a graph using basic graph algorithms e g spanning tree shortest path algorithms max flow and select form among advanced sorting algorithms the one that is most suitable to a particular data sorting problem based on the nature of the data to be sorted student evaluation grading scheme assignments midterm exam final exam total criteria that must be met to pass students must write the final exam if a student does not write the final exam the student will receive a grade of at most attendance expectation students are expected to attend every class and participate actively there will be short reading assign ments for all classes and students are expected to come to class having completed the readings there will be group problem solving activities in class come prepared and do not disappoint your group there is no penalty for missed lectures however attendance in class is crucial for success in the course students are expected to attend all tutorial sessions these are opportunities to practice the course mate rial with the guidance of a teaching assistant there will also on occasion be new materials presented in tutorials that are not presented in class which will be covered on examinations this is because tutorials contain some flexible time whose content is driven by the needs of the students sometimes this means that content will be presented without being prepared in advance and such content will not appear in lectures or be posted online but will still be fair game for testing on examinations there is no penalty for missed tutorial sessions but it is expected that students will attend and take advantage of the opportunity for asking questions seeing more examples and getting additional practice note all students must be properly registered in order to attend lectures and receive credit for this course midterm examination scheduling the mid term exam date will be held in class on wednesday february final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students must not make travel plans or schedule other activities during the period scheduled for ex aminations deferred examinations are not granted for these reasons http www usask ca programs colleges schools arts science deferredandsupplementalexaminations course overview lectures will be opportunities to apply the concepts covered in the course discuss them look at examples and solve problems as well as to ask questions and receive guidance we will not use much class time reading slides to you short readings will be assigned before each class and you will be expected to be prepared to discuss ask questions and participate tutorial times are listed below these are your opportunities to review and practice the current week material under the guidance of a teaching assistant we will have one midterm examination see above for the schedule the final examination is scheduled by the university assignments are mostly weekly to ensure that all the relevant material is put into regular consistent practice some early assignments may seem easy and later assignments will definitely challenge you even a simple assignment can turn into a time consuming affair if you get stuck on something that blocks your progress working at the last minute is a guaranteed source of stress and burn out to manage your workload you must learn effective time management start every assignment early to allow yourself time to get help if you run into a problem make use of the teaching resources instructors office hours tas labs lectures discussion forums etc available to you textbook information required texts and resources required textbook m eramian intermediate data structures and algorithms course readings for cmpt second edition you cannot buy this in the bookstore this book is available for free on the course website recommended texts java reference big java early objects edition by cay horstmann wiley sons you don t have to buy this but if you want a book on java this one is pretty good does not include java but we should not be using any language features specific to java lecture schedule the following schedule is approximate other topics may be added if time allows topic details review of lists array based lists singly linked list insertion algorithm doubly circularly linked variants regression testing black box white box managing expected and unexpected exceptions generalized collection iteration cursors iterators algorithm timing analysis formal defintion statement counting approach active operation approach big o big theta notation abstract data types and specification adt defintion review formal specification of adts standard trees tree traversals ordered trees general trees specialized trees not all covered in detail avl trees 2 3 trees b trees splay trees tries range searching k d trees graphs directed and undirected graphs breadth first and depth first search shortest path algorithms e g dijkstra algoirthm efficient sorting algorithms review of o searches merge sort quick sort heap sort linear sorts bucket radix extra topics time permitting greedy algorithms backtracking algorithms design of more data structures assignment schedule assignments will be mostly weekly later assignments will be bi weekly and will be lengthier the assign ment deadline schedule follows in the highly unlikely event that this schedule is disrupted so much that an assignment needs to be cancelled the weighting of the remaining assignments will be pro rated so that they comprise the percentage of the final grade listed under the grading scheme section above assignment due date 1 january 19 2016 00pm 2 january 26 2016 00pm 3 february 2 2016 00pm february 9 2016 00pm 5 march 1 2016 00pm 6 march 2016 00pm 7 march 2016 00pm 8 april 5 2016 00pm tutorial sections tutorial sessions begin the week of january section day time room wednesday 4 00pm 5 30pm thorv thursday 4 00pm 5 30pm thorv the tutorial sessions will be guided by teaching assistants the contact information for the teaching assistants will be made available on course moodle webpage http moodle cs usask ca the open undergraduate computing labs on the floor of the spinks addition to the thorvaldson build ing is available for student use outside of lab time many tas and instructors for several cmpt courses will hold office hours in the open lab don t be shy if you see an instructor or ta who is not your ta or instructor in the lab don t hesitate to call them over to help you description computers are ubiquitous tools that have and will continue to shape our world in common usage for only half of a century in their short existence they have already revolutionized science art business and entertainment however at their core computers are simple machines that can only do basic mathematical computations addition subtraction multiplication division etc and make decisions based upon fundamental rules of logic how then can such basic operations be utilized to make something so powerful the act of programming a computer requires us to translate a problem we wish to solve be it a math problem a business problem or even a video game into a representation that the computer and its simple operations can manipulate this requires at least two steps first we must model the problem as information we call data data can be anything from the values of a science experiment to a collection of the very letters that comprise the sentence you are reading for real life sized problems we will build as complex of a model as is necessary up from smaller components starting with those fundamental to a computer and turning them into larger units that we will call objects second we must encode the interaction and manipulation of those objects as code the collection of computer operations that transforms data using well dened rules in this class we will choose to describe our computer programs in a programming language called java java is a programming language built around the creation and management of objects in terms of the data they contain and the code that acts upon them there are many object oriented programming languages but java is a good choice for both learning and real life use the skills that we learn in java will be easily transferrable to other programming languages and even other programming paradigms prerequisites this class is meant to be a second or later class in programming the prerequisite is prior programming experience that can be in any language since most share enough in common if not syntactically then at least in terms of the act of programming prior experience in java is a plus but any reasonable language will do and the beginning of the term will be a quick introduction to the basics of the java language this means that the course will not teach you about the basic ideas of variables or control structures and how to use them to express basic ideas as code rather we will focus on the specic task of taking computational thinking and expressing it in the java programming language if you have any questions about the prerequisite material for the course please ask at the beginning of the term course purposes and goals at the end of this course students will be able to write programs in the java programming language that utilize objects from the java class library as well as custom objects that model the problem being solved understand and utilize the tools necessary to create and run a java program understand the output when errors occur and to solve errors at compile time and run time using the skill of debugging use and apply simple data structures such as arrays and similar collections to hold aggregate data develop and appreciate basic algorithms such as searching and sorting have the foundation necessary to proceed to further courses in the computer science engineering and information science elds textbooks required text gaddis tony starting out with java from control structures through objects sixth edition addison wesley isbn class policies exams there will be two midterms and a nal the exams will be closed book notes the nal exam will be wednesday april from 50am in the normal classroom cheating on exams will not be tolerated anyone caught cheating will be given a zero for the test or for the course and reported to the department following university procedures at the risk of seeming like a luddite smart watches will be forbidden on test days please keep them someplace out of sight for the duration of the exam projects there will be out of class assignments given these are to be completed in the given time no extensions will be given without a valid excuse late work is not accepted contact me before the deadline for clarications these are meant to be your own work anyone found to be collaborating will be disciplined in accordance to university policy cheating means but is not limited to using code from previous terms other universities your friends nding it on the internet geting help from unapproved forums or outsourcing it we will be using moss a tool from stanford for determining inappropriate collaboration labs and quizzes attending lab is an important part of this course in lab you will be able to work in a structured seting while completing small tasks concepts from class will be expanded upon and tested with unannounced quizzes participation attendance will not be taken but in a small class any absence will be noticed several unexcused missed classes will adversely aect your grade due to this class being taken by students with a wide variety of backgrounds i will be doing my best to ensure a comfortable classroom environment for those with less programming experience if your participation in terms of answers or questions are well beyond the level we are currently at please keep them to after class oce hours or emails expect gentle and if necessary not so gentle reminders of this throughout the term disability resources and services if you have a disability for which you are requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course academic integrity students are expected to comply with the university of pittsburgh policy on academic integrity any student suspected of violating this obligation for any reason during the semester will be required to participate in the procedural process as outlined in the university guidelines on academic integrity for further information see in this rst lab we will introduce the compung environment in the lab classroom give you some hints about how to set up java at home and walk you through the submission process that we will use for labs and projects throughout the term getting started with programming in java the computers in our lab are running windows and are already congured with the tools we will need to create and run programs written in java after you log in you ll be presented with the large windows start menu from the ulies column you may need to scroll to the right to see it launch the notepad applicaon text editing notepad is one of many programmers text editors the source code for java is just plain human readable text there no formang or styles like bold or italic or underline just letters punctuaon and whitespace we could then write programs enrely in a basic no frills text editor like windows own notepad however the specic task of programming has some common challenges that these more specialized programs like notepad hope to make easier on us you may get a dialog box asking you to update some plugins it doubul this would install correctly due to us not having permissions to install software on the lab computers so let just skip that at home if you decide to use notepad you can certainly update when it prompts you to you are now able to type something in so let type in the hello world program we saw in class here the code if you need it i suggest that each indented line be indented with a tab so before public hit tab once before system hit tab again you ll note that you didn t have to hit tab twice since when you hit enter it remembered your previous level of indentaon this is something a basic text editor wouldn t do that a programmer text editor like notepad will do for you you might also noce it highlighng the left parenthesis when you type a right parenthesis or the same when you type the curly braces and these help you to know which open grouping character you are closing so that you can have the right number when there are many but there more things that a programmer text editor can do if it knows what programming language you are wring in right now notepad doesn t know that you want to program in java you can tell it one of two ways you can either save the le with a java extension or you can go to the language menu and tell it which language you re programming in so let try both first go to the language menu select the j submenu and then choose java all of a sudden your plain text no longer looks that plain notepad does what is called syntax highligh ng this means that when you type something special a reserved word a string a number etc something that has meaning in java it will turn a dierent color than the ideners and expressions we write that gives us strong hints about the structure of our code and reminds us to not name variables with words that java forbids notepad also does something else after the system out println hello world line hit enter and add a new line let have you print the string goodbye cruel world as you start to type system you ll note that it pops up a box that suggests what you might be typing similar to how your smartphone might at any point you can hit the tab key to have it nish typing what you started if the highlighted selecon is what you want if it isn t you can keep typing to get better suggesons or use the mouse to select a dierent opon note though that just because it doesn t suggest something doesn t mean you re typing something wrong it really only remembers things you ve already typed that you might reuse or some common snippets it can guess at syntax highlighng and auto compleon can make us very producve programmers but make sure that they don t become a crutch you sll must know the language well enough to write code on paper one of our goals is uency so that even if we don t have the tools we are used to we can sll get work done compiling and running java now save the le you are eding with the name hello java the default locaon to save is probably your documents folder and we are okay with that if you re somewhere else navigate to your documents and save there notepad is not a full integrated development environment ide an ide would give you one click access to the compiler and to run the resulng program it would have syntax highlighng and even better auto compleon however they are complex and can hide the details of what you are doing to prevent you from learning the basics we won t advocate using an ide in the class so we are going to need direct access to the compiler to build our program bring up the windows start menu either by dragging your mouse all the way into the lower left hand corner or by hing the windows key on the keyboard from there nd the system accessories category of applicaons and choose the command prompt to run you ll now have a small black window pop up with a cursor that is prompng you to type a command this is a command line interface and it is an older and in some ways simpler way of interacng with the computer than the graphical user interface gui than what you re maybe used to the line showing lowest on the screen where the cursor is blinking tells you what directory we are currently in we want to go into our documents directory where we saved our hello java le so we must issue a command rather than clicking on an icon that command is cd which stands for change directory we tell it what the name of the directory is and hit enter so type and now the text at the current prompt should say that you are in the documents directory the lab computers have been congured with the software and sengs necessary to compile and run java we can test that by asking what version of java we have installed execute the command it will likely report back a version that starts with and some other numbers that indicate that bugs have been xed several mes the latest version of java is actually and so if you do this at home you might nd a bigger number that okay in this class we ll mostly be okay with version or later that was just a test to see if everything was okay we don t need to run that command that way to build our program let do that now type the command and hit enter the compiler javac follows the unix model of program output if you see no output on the screen nothing went wrong if you do see a message it is telling you that you have typed something against the rules of the java language it will tell you what line it guesses you messed up on it not always right but it is usually close double check what you typed with what the program was supposed to be save again and try to recompile when you succeed you ll have produced a new le hello class you can see the list of les in the current directory by typing if you mistakenly put the class extension or you try to run the java le without compiling you ll get an error such as error could not nd or load main class hello class turning it in open up windows explorer by clicking the folder icon in the taskbar navigate to documents if you aren t already there so that you can see the hello java and hello class les you created to submit we are going to need to create a zip le and upload it to the class submission site to create a zip le select the les you want to compress which in this case is just hello java we can create hello class from that so no need to submit it right click and say send to compressed zipped folder this will create hello zip now go to the submission site in your preferred web browser log in with your pitt edu email address include the pitt edu part and peoplesoft number click the browse button and navigate to your hello zip le in your documents folder select it for opening and then on the webpage click the send button if you try this from o campus you will be met with an unauthorized error page go instead to the direcons on that due wednesday march your third project is to implement the game of mastermind in this game the computer chooses pegs each with one of colors the player job is then to guess the colors that the computer has chosen in the proper order aer each guess by the player if the player guess is not correct the computer will give two numbers as feedback the first number is how many pegs are the proper color and in the proper position the second number is how many pegs are the proper color but not in the correct position the game ends when the color string is correct and the player wins or they give incorrect guesses and they lose what you need to do generate a random computer guess of four colors out of red orange yellow green blue purple read a guess from the user as a string of colors score the guess and report the two values back to the user allow the user to continue to guess until they get it correct or reach turns and they lose allow the user to play the game multiple times example welcome to mastermind would you like to play yes enter guess number rrrr colors in the correct place colors correct but in wrong position enter guess number model view controller a common patiern for writing programs is known as mvc model view controller this patiern applies whenever we have some problem we can model like a game board and a user interface that displays and interacts with that model the view the controller is the code that manipulates the model in response to actions from the view the idea of mvc is that each part of the program is sufficiently abstracted from each other that they can change without needing to modify the other parts for instance our view is currently a textual interface but later in the course we could alter this to be a graphical user interface if we did that ideally we would not need to change the model or the controller only the view code for us the implementation of the model is very simple either an array or string of colors ints chars etc that represents the randomly chosen colors we are trying to guess the view is a simple text based program as we ve writien many times so far it will prompt the user for their guesses and display if the guess is correct or show the two statistics that we must calculate the controller links these two things together we will then make three classes a main class named mastermind that serves as our view creates the model and controller and deals with user mastermind http people cs pitt edu jmisurda teaching htm a main class named mastermind that serves as our view creates the model and controller and deals with user input and output a model class named mastermindmodel that stores the representation of the computer guess and uses a constructor and accessors to create and query the solution the player is trying to guess defined as follows class mastermindmodel private variable to store the answer public mastermindmodel make the answer public char getcolorat int index return color at position index as a char first converted if stored as a number a controller class named mastermindcontroller that is defined as follows class mastermindcontroller public mastermindcontroller mastermindmodel model public boolean iscorrect string guess public int getrightcolorrightplace string guess public int getrightcolorwrongplace string guess you are to provide the implementation of all three classes but you must define your controller and model using at least the methods above any additional methods or fields you want to add must be private to your classes hints and notes for right color wrong place you will need to not count colors from the guess that are the right color in the right place you also need to avoid double counting a color as being in the wrong position to help you solve these issues you may find it necessary to make some auxiliary arrays that keep track of what you have used already you may want to play the game on paper where you consider various guesses and solutions and score them to see the issue the perfect player can always win this game in guesses or less submission create a mastermind zip file that contains your three java files upload it to the submission website as we have been doing with labs make sure to do this prior to the deadline no late work is accepted if prior to the deadline you realize that you need to submit a different version of your code name it mastermind zip you can add new numbers as necessary but try not to submit many times we will grade the last submission due all source files plus a completed assignment information sheet zipped into a single zip file and submitted properly to the submission site by on friday january note see the submission information page for submission details late due date on monday january purpose to refresh your java programming skills and to emphasize the objectoriented programming approach used in java specifically you will work with control structures classbuilding interfaces and generics to create and utilize a simple arraybased data structure goal to design and implement a simple class multids t that will act as a simple data structure for accessing java objects your multids t class will primarily implement interfaces primq t and reorder the details of these interfaces are explained in the files primq java and reorder java read these files over very carefully before implementing your multids t class goal to utilize your multids t class by implementing a simple version of the card game war in this case your program will be a client using multids t and the details of the multids t implementation will be abstracted out details for the details on the functionality of your multids t class carefully read over the files primq java reorder java and java provided on the web site you must use these files as specified and cannot remove alter any of the code that is already written in them there are different ways of implementing the primq t and reorder interface methods some of which are more efficient than others try to think of the best way of implementing these methods in this assignment but the most important thing at this point is getting them to work i recommend a lot of pencil and paper work before actually starting to write your code later we will discuss the relative merits of different implementations your multids t class header should be public class multids t implements primq t reorder important note the primary data within your multids t class must be an array you may not use any predefined java collection class for your multids t data after you have finished your coding of multids t the java file provided for you should compile and run correctly and should give output identical to the output shown in the sample executions except for the segments where the data is shuffled since it will be pseudorandom in that case details war is a card game played often by children that has many variations you will implement the simple version as described below initially shuffle a standard deck of cards deal the cards out completely to two players alternating cards to each player do the following until one player is out of cards or until a set number of rounds have completed each player plays the top card in his her hand if one player card beats the other has a higher rank suits don t matter that player puts both cards into his discard pile if the players tie it is a war so do the following until the war is resolved each player plays a card without comparing each player plays one more card and compares in the same way as above the winner of the war takes all played cards initially compared cards uncompared cards second compared cards for a single war this will be total cards if the war cards also yield a tie repeat the process one uncompared card one compared card until there is a winner thus a single round of the game could in fact have an arbitrary number of card comparisons and put an arbitrary number of cards at risk the following rules also apply to the game people cs pitt edu ramirez assigs htm http people cs pitt edu ramirez assigs assig1 htm if at any point in the game a player hand is empty the player should move the cards in his discard pile into his hand and shuffle them if at any point in the game even in the middle of a round a player has no cards remaining in both his her hand and his her discard pile he she has lost the game if the both players have cards remaining after a set number of rounds can vary the player with the most cards hand discard pile is the winner in this case it is possible for a tie to occur with each player having cards implementation requirements your initial card deck the player hands the player discard piles and the cards in play must all be stored in multids card objects the card class must be used as provided and cannot be changed the maximum number of rounds for the game must be read in from the command line to allow better testing of your program you must output when wars occur and when discard piles are reshuffled see the various sample output files for required output information you must submit in a single zip file minimally the following complete working source files for full credit primq java reorder java java card java the above four files are given to you and must not be altered in any way multids java war java the above two files must be created so that they work as described if you create any additional files be sure to include those as well the idea from your submission is that your ta can unzip your zip file then compile and run both of the main programs java and war java from the command line without any additional files or changes so be sure to test it thoroughly before submitting it if you cannot get the programs working as given clearly indicate any changes you made and clearly indicate why ex i could not get the reverse method to work so i eliminated code that used it on your assignment information sheet you will lose some credit for not getting it to work properly but getting the main programs to work with modifications is better than not getting them to work at all see the cs web site for an assignment information sheet template you do not have to use this template but your sheet should contain the same information note if you use an ide such as netbeans to develop your programs make sure they will compile and run on the command line before submitting this may require some modifications to your program such as removing some package information hints notes see program java for some help with the card class and using the multids t class with card objects see file txt to see how your output for should look as noted your output when running java should be identical to this with the exception of the order of the values after being shuffled see files txt txt txt for example runs of my war java program your war program output does not have to look exactly like this but the functionality should be the same your multids t class will need to allow for a variable number of objects up to some maximum number set by the initial size you can implement this by having an integer instance variable ex count that indicates how many slots in the multids t are filled for example you could have a multids card with a capacity of for your hands for a hand with anywhere from up to cards in it in this case the array length i e physical size would be but the count i e logical size would be some value between and when implementing multids t be careful to use the count value rather than the array length when doing operations such as the tostring method online wednesday february due all source java and data files plus a completed assignment information sheet zipped into a single zip file and submitted to the proper directory in the submission site by on thursday march note see the submission information page for submission details late due date on saturday march purpose we have discussed recursion and in particular backtracking algorithms such as eight queens in this assignment you will get some practice at recursive programming by writing a backtracking algorithm to find phrases in a puzzle idea word search puzzles are common in newspapers on restaurant placemats and even in their own collections within books these vary in format from puzzle to puzzle but one common format is as follows a user is presented with a grid of letters and a list of words the user must find all of the words from the list within the grid indicating where they are by drawing ovals around them words may be formed in any direction up down left or right we will not allow diagonals even though most puzzles do allow them but all of the letters in a word must be in a straight line this idea can be extended to allow phrases to be embedded in the grids however requiring an entire phrase to be in a straight line on the board is not practical as the dimensions of the board would need to be overly large therefore we will still require the letters in each word to be in a straight line but we are allowed to change direction between words for example we might be given the following grid of letters and the following phrases abstract abstract data types abstract data types are really cool abstract data types are really awesome upon searching assuming the grid starts in the upper left corner with position and that the row is the first coordinate abstract would be found in two places to and to abstract data types would also be found in two places to to to and to to to abstract data types are really cool would be found at to to to to to to abstract data types are really awesome would not be found details your task is to write a java program to read in a grid of letters from a file and then interactively allow a user to enter phrases until he or she wants to quit for each phrase your program must output whether or not the phrase is found and if found specifically where it is located input details the grid of letters for your program will be stored in a text file formatted as follows line two integers r and c separated by a single blank space these will represent the number of rows r and columns c in the grid remaining lines r lines containing c lower case characters each the user input will be phrases of words with a single space between each word and no punctuation each phrase will be entered on a single line the user may enter either upper or lower case letters but the string should be converted to lower case before searching the grid the program will end when the user enters no data for the current phrase i e hits enter without typing any characters beforehand output details if a phrase is not found in the grid the output should simply state that fact if a phrase is found in the grid your program must find one occurrence of the phrase and the output must indicate this fact in two ways show the coordinates of each word in the phrase as a pair of row column pairs show the grid with the letters of the phrase indicated in upper case for example for the grid above and the phrase abstract data types your output would be abstract to data to types to algorithm details your search algorithm must be a recursive backtracking algorithm note that you do not need recursion to match the letters within individual words although you may do this recursively if you prefer where the recursion is necessary is when moving from one word to the next since it is here where you may change direction to make the program more consistent and easier to grade we will have the following requirements for the recursive process no letter location on the grid may appear more than one time in any part of a solution when given a choice of directions the options must be tried in the following order right down left up given this ordering and the grid above the solution for abstract data would be to to rather than to to since the right direction is tried before the left direction if the last letter in a word is at location i j the first letter of the next word must be at one of locations i j i j i or j the direction chosen to find the first letter of a word is the same direction that must be used for all of the letters of the word for example in the grid shown above for the phrase abstract data the following would not be a valid solution to to this solution is not legal because we proceeded right from the t of abstract to find the d in data but then proceeded down to find the remaining letters in data similarly also in the grid shown above for the phrase abstract data types are the following would not be a valid solution to to to to this solution is not legal because we proceeded down from the s of types to find the a in are but then proceeded right to find the remaining letters in are the idea is that you are building a solution word by word each time you complete a word you can look for the next word in any of the four directions this is where the recursion occurs if the next word cannot be found in any of the directions you must delete the most recently completed word and backtrack to the previous word for example consider the board above with the search phrase data types are really the algorithm first tries to find the word data starting at position it tries in all four directions and does not succeed it proceeds through the other starting positions until it finds data in locations to it now recursively tries to find types in the following ways going right to position this will not work since is already being used in data going down to position this will succeed and types is found in locations to the algorithm now recurses again this time looking for are in the following ways going right to position this will succeed and are is found in locations to the algorithm now recurses again this time looking for really in the following ways going right to position this will not work since character is an going down to position this will not work since character is a d going left to position this will not work since is already being used in are going up to position this will not work since character is a d since all directions have been tried the search for really has failed and the algorithm must backtrack the word are is removed and the previous search for are resumes going down to position this will succeed and are is found in locations to the algorithm now recurses again this time looking for really in the following ways going right to position this will succeed and really is found in locations to the last word in the phrase has been found and the algorithm succeeds clearly more backtracking may be necessary in other situations i recommend tracing through the process with some example phrases before you start coding your solution a nontrivial part of this assignment is keeping track of the path of the solution and printing the path out once the solution has been found you will likely need to use some data structure to store update this path think carefully how you can do this part of the assignment test files i have placed several test files and some sample output online see the cs assignments page for these files and make sure that your program works correctly for all of the sample files help cs coe spring assignment http people cs pitt edu ramirez assigs htm help if you are having trouble with recursion and backtracking the program findword java may be of help to you this program finds individual words in a grid of characters using recursion and backtracking it recurses on individual characters rather than on words but the recursive process is similar in both programs see also test file txt important note the code from findword java to read in and set up the grid can be taken and used directly in your program if you wish however you should not use the recursive part of findword directly in your program since it is not solving the problem you are trying to solve further you should not use this code to find individual words in your grid since findword allows directions to change within a single word whereas in your assignment you are only allowed to change directions between words see additional comments in the findword java code submission make sure you submit all of the following in a single zip file all source files that you either wrote or utilized ex from the author files call your main program file java all data files your completed assignment information sheet as with assignments and the idea from your submission is that your ta can compile and run your programs without any additional files so be sure to test them thoroughly before submitting them if you use an ide for development make sure your program runs from the command line without the ide before submitting it if you cannot get the program working as specified clearly indicate any changes you made and clearly indicate why so that the ta can best give you partial credit extra credit idea allow your program to recurse diagonally as well as horizontally and vertically if you do this you will need to make up your own test files to demonstrate that your program works correctly purpose to give you experience implementing and using linked lists goal to implement a subclass of llist actually that adds some functionality to it and to implement a subclass of that class that can store and manipulate arbitrary length integers details we can think of a decimal integer as a sequence of digits for example the number could be stored as the digit followed by the digit followed by the digit followed by the digit we can store these digits in an array or as is required in this assignment in a linked list clearly to perform operations on a number that is stored in this fashion we must access the digits one at a time in some systematic way more specific details follow below part the first part of this assignment is to write a new subclass of t called linkedlistplus t class t is mostly the author llist t class with a few changes i have removed the listinterface and a few methods and i have changed the data from private to protected this will allow you to access the instance variables directly from the subclass see the code in java to see how the data is stored and how the methods are implemented linkedlistplus will not have any new instance variables but will extend by adding the following methods public void leftshift int num shift the contents of the list num places to the left assume the beginning is the leftmost node removing the leftmost num nodes for example if a list has nodes in it numbered from to a leftshift of would shift out nodes and and the old node would now be node if num leftshift should do nothing and if num the length of the list the result should be an empty list public void rightshift int num same idea as leftshift above but in the opposite direction for example if a list has nodes in it numbered from to a rightshift of would shift out nodes and and the old node would now be the last node in the list if num rightshift should do nothing and if num the length of the list the result should be an empty list public void leftrotate int num in this method you will still shift the contents of the list num places to the left but rather than removing nodes from the list you will simply change their ordering in a cyclic way for example if a list has nodes in it numbered from to a leftrotate of would shift nodes and to the end of the array so that the old node would now be node and the old nodes and would now be nodes and in that order the rotation should work modulo the length of the list so for example if the list is length then a leftrotate of should be equivalent to a leftrotate of if num the rotation should still be done but it will in fact be a right rotation rather than a left rotation public void rightrotate int num same idea as leftrotate above but in the opposite direction for example if a list has nodes in it numbered from to a rightrotate of would shift nodes 7 and to the beginning of the array so that the old node would now be node the old node 7 would now be node and the old node would now be node the behavior for num the length of the list and for num should be analogous to that described above for leftrotate public void reverse this method should reverse the nodes in the list note that in the methods above you may not create any new node objects the purpose of them is to rearrange the nodes that already exist to see how these should work i strongly recommend drawing one or more pictures you will also need to write the following constructors public linkedlistplus public linkedlistplus linkedlistplus t oldlist people cs pitt edu ramirez assigs htm http people cs pitt edu ramirez assigs htm the first contructor simply initializes the list to an empty state and the second generates a new list that is a copy of the argument list copying all of the nodes inside the old list thus this copy is deepish finally you will need to override the following method public string tostring this method will return a string that is the result of all of the data in the list being appended together separated by spaces important notes all of your linkedlistplus methods must be implemented in an efficient way utilizing the underlying linked list for example a poor implementation of a left rotation could be done via repeated calls to remove and add this getlength x however doing this would require multiple traversals of the list and has a very bad runtime think arithmetic series this and similar implementations are not allowed and if implemented in this way you will not receive credit the leftrotate rightrotate amd reverse methods should not create any new node objects rather they should move the node objects currently in the list into other locations you may use temporary node variables for these methods but you may not create any new nodes to verify that your linkedlistplus class works properly you will use it with the program llptest java which will be provided for you on the assignments web page your output must match that shown in llptest txt to help you out with the assignment i have implemented some of the methods above for you with comments see the code in linkedlistplus java part the second part of this assignment is to write the reallylongint class with the specifications as given below you may assume all numbers will be nonnegative inheritance reallylongint must be a subclass of linkedlistplus however since linkedlistplus is generic while reallylongint is not generic you should use the following header public class reallylongint extends linkedlistplus integer implements comparable reallylongint note that rather than t the underlying element data is now integer this means that the individual digits of your reallylongint will be integer objects data the data for this class is inherited and you may not add any additional instance variables you will certainly need method variables for the various operations but the only instance variables that you need are those inherited from via linkedlistplus operations your reallylongint class must implement the methods shown below note that the compareto method is necessary for the comparable interface private reallylongint the default constructor will create an empty reallylongint note that this leaves the number in an inconsistent state having no actual value so it should only be used within the class itself as a utility method for example you will probably need it in your add and subtract methods for this reason it is a private method public reallylongint string the string consists of a valid sequence of digits with no leading zeros except for the number itself special case insert the digits as integer objects into your list such that the least significant digit is at the beginning of the list for example the string would be stored in a reallylongint as firstnode 6 note actual nodes are not shown but implicit public reallylongint reallylongint rightop this just requires a call to super however it is dependent upon a correct implementation of the copy constructor for the linkedlistplus class public string tostring people cs pitt edu ramirez assigs htm http people cs pitt edu ramirez assigs htm return a string that accurately shows the integer as we would expect to see it based on the way we have stored the integer this should be accomplished by going backward through the list since we cannot actually do that we will reverse the list then traverse it then reverse it again to help you out with the assignment i have implemented the methods above for you with comments see the code in reallylongint java public reallylongint add reallylongint rightop return a new reallylongint that is the sum of the current reallylongint and the parameter reallylongint without altering the original values for example reallylongint x new reallylongint reallylongint y new reallylongint reallylongint z z x add y system out println x y z should produce the output be careful to handle carries correctly and to process the nodes in the correct order since the numbers are stored with the least significant digit at the beginning the add method can be implemented by traversing both numbers in a systematic way this must be done efficiently using references to traverse the lists in other words you should start at the beginning of each reallylongint and traverse one time while doing the addition think how you can do this with reference variables also be careful to handle numbers with differing numbers of digits public reallylongint subtract reallylongint rightop return a new reallylongint that is the difference of the current reallylongint and the parameter reallylongint since reallylongint is specified to be nonnegative if rightop is greater than the current reallylongint you should throw an arithmeticexception otherwise subtract digit by digit borrowing if necessary as expected as with the add method you must implement this efficiently via a single traversal of both lists this method is tricky because it can result in leading zeros which we don t want be careful to handle this case and consider the tools provided by linkedlistplus that will allow you to handle it for example reallylongint x new reallylongint reallylongint y new reallylongint reallylongint z z x subtract y system out println x y z should produce the output as with the add method be careful to handle numbers with differing numbers of digits also note that borrowing may extend over several digits see rlitest java for some example cases public int compareto reallylongint rightop defined the way we expect compareto to be defined for numbers if one number has more digits than the other then clearly it is bigger since there are no leading otherwise the numbers must be compared digit by digit since this requires the most significant digit to be processed first which is at the end of the list we cannot just iterate through the digits as given with a singlylinked list we cannot go backward either so we will use what is given in the linkedlistplus class first reverse the nodes in both numbers then do the comparison in a sequential way then reverse again to restore the original numbers public boolean equals object rightop defined the way we expect equals to be defined for objects comparing the data and not the reference don t forget to cast rightop to reallylongint so that its nodes can be accessed note the argument here is object rather than reallylongint because we are overriding equals from the version defined in class object note this method can easily be implemented once compareto has been completed public void multtentothe int num multiply the current reallylongint by note that this can be done very simply through adding of nodes containing 0 people cs pitt edu ramirez assigs assig2 htm http people cs pitt edu ramirez assigs assig2 assig2 htm 4 public void divtentothe int num divide the current reallylongint by note that this can be done very simply through shifting to verify that your reallylongint class works correctly you will use it with the program rlitest java which will be provided for you on the assignments page your output should match that shown in rlitest txt more important notes when implementing these operations you may discover that it would be much easier to do them if the underlying linked list were bidirectional i e a doublylinked list this is absolutely true but unfortunately you are required to use the singlylinked list that is provided both the add and subtract methods are tricky and have different cases to consider for these methods especially i recommend working out some examples on paper to see what needs to be considered before actually coding them extra credit here are a couple nontrivial extra credit ideas either one done well could get you the full extra credit points however don t attempt either until you are confident that your required classes are working correctly allow the numbers to be signed so that we can have both positive and negative numbers this may require an extra instance variable for the sign and will clearly affect many of your methods if you choose this extra credit you must submit it as a separate class in addition to your original reallylongint class you must also submit a separate driver program to test demonstrate your signed class add a multiply method to your reallylongint class if you implement this you should also submit a separate driver program to test demonstrate the multiply method submission you must submit the following complete working source files for full credit java llptest java rlitest java the above three files are given to you and must not be altered in any way linkedlistplus java reallylongint java the above two files must written by you so that they work as described note that i have provided partial implementations of both download those as a starting point the idea from your submission is that your ta can compile and run your program without any additional files or processing this means compilation and execution on the command line using the javac and java commands test your programs from the command line before submitting especially if you use an ide such as netbeans or eclipse to develop your project if you cannot get the programs working as given clearly indicate any changes you made and clearly indicate why on your assignment information sheet true false explain false statements for full credit points each ex given nonnull string variables and the statement will test if their string values match each other in the cs course we use java classes to represent adts abstract data types in the author alist implementation of the list adt to delete an item from the middle of the list we simply move the last item to that location thereby saving the time required for shifting short answers define compare explain justify or otherwise elaborate on the statements shown ex in the arraybag implementation of the baginterface explain how the remove t anentry method actually removes that item from the array state why this is acceptable and when it may not be acceptable trace give all output produced by the execution of a given java program in the correct order ex see class handouts try tracing these by hand and then running them to see the actual output coding write java code as instructed in the problem ex consider the contains t anentry method in the arraybag implementation of the baginterface complete the method recall the instance data for the arraybag class private t bag private int numberofentries solutions fill in the blanks provide the most appropriate answers points each people cs pitt edu ramirez quizhelp html http people cs pitt edu ramirez quizhelp html ex because the node class used in a linkedbag has an instance variable of class node it is called a data type in the list bag adt the order that the data is stored does not matter true false explain false statements for full credit points each ex given nonnull string variables and the statement will test if their string values match each other it will test if the references are the in the cs course we use java classes to represent adts abstract data types we use interfaces to represent in the author alist implementation of the list adt to delete an item from the middle of the list we simply move the last item to that location thereby saving the time required for shifting the list adt must maintain positional short answers define compare explain justify or otherwise elaborate on the statements shown ex in the arraybag implementation of the baginterface explain how the remove t anentry method actually removes that item from the array state why this is acceptable and when it may not be acceptable consider an item somewhere in the middle of the array that will be removed to remove it creates a hole in the array that must be filled rather than filling the hole by shifting all of the remaining items over in the arraybag the last item is simply copied to the deleted index and the last reference is then set to null this in effect moves the last item up to somewhere in the middle of the array which is ok since the data in a bag is unordered if order mattered ex a general list then we would have to shift rather than doing this technique trace give all output produced by the execution of a given java program in the correct order ex see class handouts coding write java code as instructed in the problem ex consider the contains t anentry method in the arraybag implementation of the baginterface complete the method recall the instance data for the arraybag class private t bag private int numberofentries one possible solution from text public boolean contains t anentry 3 3 people cs pitt edu ramirez quizhelp html http people cs pitt edu ramirez quizhelp html 3 3 return getindexof anentry or 0 end contains private int getindexof t anentry int where boolean found false for int index 0 found index numberofentries index if anentry equals bag index found true where index end if end for return where end getindexof 3 3 people cs pitt edu ramirez handouts carrano listinterface java http people cs pitt edu ramirez handouts carrano listinterface java an interface for the adt list entries in a list have positions that begin with author frank m carrano author timothy m henry version 4 0 public interface listinterface t adds a new entry to the end of this list entries currently in the list are unaffected the list size is increased by param newentry the object to be added as a new entry public void add t newentry adds a new entry at a specified position within this list entries originally at and above the specified position are at the next higher position within the list the list size is increased by param newposition an integer that specifies the desired position of the new entry param newentry the object to be added as a new entry throws indexoutofboundsexception if either newposition or newposition getlength public void add int newposition t newentry removes the entry at a given position from this list entries originally at positions higher than the given position are at the next lower position within the list and the list size is decreased by param givenposition an integer that indicates the position of the entry to be removed return a reference to the removed entry throws indexoutofboundsexception if either givenposition or givenposition getlength public t remove int givenposition removes all entries from this list public void clear replaces the entry at a given position in this list param givenposition an integer that indicates the position of the entry to be replaced param newentry the object that will replace the entry at the position givenposition return the original entry that was replaced throws indexoutofboundsexception if either givenposition or givenposition getlength public t replace int givenposition t newentry retrieves the entry at a given position in this list param givenposition an integer that indicates the position of the desired entry return a reference to the indicated entry throws indexoutofboundsexception if either givenposition or givenposition getlength public t getentry int givenposition retrieves all entries that are in this list in the order in which they occur in the list return a newly allocated array of all the entries in the list if the list is empty the returned array is empty public t toarray 3 3 people cs pitt edu ramirez handouts carrano listinterface java http people cs pitt edu ramirez handouts carrano listinterface java sees whether this list contains a given entry param anentry the object that is the desired entry return true if the list contains anentry or false if not public boolean contains t anentry gets the length of this list return the integer number of entries currently in the list public int getlength sees whether this list is empty return true if the list is empty or false if not public boolean isempty end listinterface general each student is expected to do his her own work for a first offense a student caught collaborating cheating in any way will receive a zero for the exam homework project in question in the event of a second offense the student will receive an f for the course and may be subject to stronger action note submissions that are alike in a substantive way not due to coincidence will be considered to be cheating by all involved parties please protect yourselves by only storing your files in private directories and by retrieving all printouts promptly for more information on the academic integrity policies of the school of arts and sciences see this link students are encouraged to attend all lectures which frequently include material that is not directly taken from the text if a student misses a lecture he she is still responsible for the material covered and is advised to copy the notes from a classmate an abundance of information including announcements handouts review sheets and solutions will be posted on my web page and possibly ta pages as well throughout the term students are expected to aware of all information that is posted on our web pages and should access them frequently all graded materials that a student receives back should be saved in a safe place until after the term has ended and he she has received and accepts his her final grade in this way any grade discrepancies can be easily resolved 3 3 cs home page http people cs pitt edu ramirez 2 students with disabilities if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course more info at http www studentaffairs pitt edu drs exams the dates for the exams are listed in the important dates table students are expected to be present for both exams makeup exams will only be given in the event of an emergency and only if i am informed in advance if you cannot contact me directly send me email or call the cs department and leave me a message failure to notify me prior to missing an exam will result in a zero for the exam programs there will be approximately programs to be written in java assigned throughout the term students are expected to hand in all of the assignments failure to hand in two assignments will result in a final grade no higher than a c failure to hand in three or more assignments will result in a final grade no higher than a d programs will be submitted electronically to a submission site set up for the course more details on submission guidelines will be provided on the course web site ontime projects must be received at the submission site by on the due date any project received after this time will be considered to be late late projects must be received at the submission site by on the late due date late projects will be penalized points out of projects submitted after the late due date will not be graded and will receive a score of 0 students are expected to have a backup disk or storage somewhere for every assignment they turn in in this way if there is any problem with the primary copy the work will not be lost more details on the programs and submission guidelines will be given with the first assignment quizzes quizzes will be given during recitations and will be announced at least one class or recitation before they are given they will generally include material covered in the previous class es and recitation s two quizzes will be given each counting for of the final grade as with exams quizzes can only be made up if you contact the recitation instructor in advance if you miss a quiz without informing the recitation instructor you will receive a zero for that quiz questions of the day at the end of each lecture there will be a very short miniquiz testing material from the current and previous lectures from hereon referred to as the questions of the day qotd no qotd will be given during lectures in which exams are given the lowest 4 scores will be dropped and the remaining will be averaged and weighted for of the overall course grade qotd cannot be made up a student not in attendance for a qotd will receive a 0 for that qotd turning technologies response cards will be used for the qotd and each student is required to have a response card for the cs course these can be purchased at the university store on fifth motivation in cs we often discuss the importance of data structure design and implementation to the wide variety of computing applications despite decades of study organizations must still regularly develop custom data structures to fulfill their applications specific needs and as such the field remains hugely relevant to both computer scientists and software engineers as an example of the magnitude of impact that data structures can have on a large system read the following news article http www pcworld com article the tao of facebook data management html in this assignment you will implement two data structures to satisfy their specifications which are provided in the form of interfaces provided code first look over the provided code you can find this code on pitt box in a folder named where is your pitt username the setinterface e interface describes a set a data structure similar to a bag except that it does not allow duplicates it is a generic interface that declares abstract methods for adding an item removing an item specified or unspecified checking if the set is empty checking the number of items in the set and fetching the items from the set into an array you should not modify this interface the setfullexception class is included to allow hypothetical implementations of setinterface that have a fixed capacity your implementation should not be fixed capacity and thus should not throw this exception the profileinterface interface describes a social network user profile it declares abstract methods for setting and getting the user name and about me blurb following other profiles returning an array of the profile followed profiles and recommending a new user to follow based on this profile followed by those i follow set you should not modify this interface the socialclient class is a sample client of both set and profile it is a social networking simulator that allows the user to carry out following unfollowing etc on a simple social network this class maintains a set of profiles stored as setinterface profileinterface it also stores its data in a file socialclientdata bin when quitting so that it can restore this data when it is run again as noted above this code is provided only as an example and may not test all functionality of the required classes tasks implement set points develop the generic class set e a dynamic capacity array based implementation of the set adt described in setinterface e read the interface carefully including comments to ensure you implement it properly it will be graded using a client that assumes all of the functionality described in the setinterface not just the behavior demonstrated in socialclient you must include a constructor public set int capacity that initializes the array to the specified initial capacity and a constructor public set that uses a reasonable default initial capacity whenever the capacity is reached the array should resize using the techniques discussed in lecture i e you should never throw setfullexception method points set set int int getcurrentsize boolean isempty boolean add e boolean remove e e remove void clear boolean contains e e toarray implement profile points develop the profile class an implementation of the adt described in profileinterface read the interface carefully including comments to ensure you implement it properly as with set it will be graded using a client that expects the functionality described in its interface the profile class should be a client of the set data structure use composition with your set e class to store the following set as a data member of type set profile or set profileinterface you must include a constructor public profile that initializes the name and about me blurb to be empty strings and a constructor public profile string name string about that initial izes these data members with the specified values in the latter you must check for null values for both and replace any null value with an empty string method points profile profile string string void setname string string getname void setabout string string getabout boolean follow profileinterface boolean unfollow profileinterface profileinterface following int profileinterface recommend testing socialclient is provided as an example client of the profile and set classes it does not ex haustively test the functionality of these classes you are responsible for ensuring your implementations work properly in all cases even those not tested by socialclient and follow the adts described in the provided interfaces thus it is highly recommended that you write additional test client code to test all of the corner cases described in the interfaces for help getting started re read the section of the textbook starting at chapter submission upload your java files in the provided box directory if you overwrite the provided interfaces remember to restore them to their original versions all programs will be tested on the command line so if you use an ide to develop your program you must export the java files from the ide and ensure that they compile and run on the command line do not submit the ide project files your ta should be able to download your directory from box and compile and run your code as discussed in lab for instance javac socialclient java and java socialclient must compile and run socialclient in addition to your code you may wish to include a readme txt file that describes features of your program that are not working as expected to assist the ta in grading the portions that do work as expected your project is due at pm on thursday february you should upload your progress frequently even far in advance of this deadline no late assignments will be accepted motivation in lecture we discussed an stack algorithm for converting an infix expression to a postfix expression and another for evaluating postfix expressions we briefly described how one might pipeline these algorithms to evaluate an infix expression in a single pass using two stacks without generating the full postfix expression in between in this assignment you will be implementing this combined algorithm your completed program will evaluate an infix arithmetic expression using two stacks you are provided with the skeleton of infixexpressionevaluator this class accepts input from an inputstream and parses it into tokens when it detects an invalid token it throws an expressionerror to end execution to facilitate ease of use this class also contains a main method this method instantiates an object of type infixexpressionevaluator to read from system in then evaluates whatever expression is typed it also catches any potential expressionerrors and prints the reason for the error infixexpressionevaluator uses composition to store the operator and operand stacks and calls several private helper methods to manipulate these stacks when processing various tokens you will need to complete these helper methods and add error checking to ensure the expression is valid tasks first carefully read the provided code you can find this code on pitt box in a folder named where is your pitt username implement helper methods points as tokens are parsed helper methods are called to process them in the included code these helper methods do not do anything you must implement the following methods to process the various types of tokens processoperand double each time the evaluator encounters an operand it passes it as a double to this method which should process it by manipulating the operand and or operator stack according to the infix to postfix and postfix evaluation algorithms processoperator char each time the evaluator encounters an operator it passes it as a char to this method which should process it by manipulating the operand and or operator stack according to the infix to postfix and postfix evaluation algorithms each of the following operators must be supported follow standard operator prece dence you can assume that is always the binary subtraction operator addition subtraction multiplication division exponentiation processopenbracket char each time the evaluator encounters an open bracket it passes it as a char to this method which should process it by manipulating the operand and or operator stack according to the infix to postfix and postfix evaluation algo rithms you must support both round brackets and square brackets these brackets can be used interchangeably but must be nested properly a cannot be closed with a and vice versa processclosebracket char each time the evaluator encounters a close bracket it passes it as a char to this method which should process it by manipulating the operand and or operator stack according to the infix to postfix and postfix evaluation algo rithms processremainingoperators when the evaluator encounters the end of the expression it calls this method to handle the remaining operators on the operator stack error checking points this task requires that you modify your program to account for errors in the input expres sion the provided code throws expressionerror when encountering an unknown token for instance you must modify your program to throw this exception with an appropriate message whenever the expression is invalid this requires careful consideration of all the possible syntax errors what if an operand follows another operand an operator following an open bracket what about brackets that do not nest properly all such syntax errors must be handled using expressionerror grading the following points breakdown will be used to assign grades points are allocated for correctly evaluating valid expressions these expressions will range from very simple to very complex but none will contain syntax errors you should test your program extensively to ensure that it works properly even for complex expressions points are allocated for correctly identifying the syntax errors in invalid expres sions these expressions will contain a wide variety of syntax errors you should test your program with many invalid expressions to ensure that it correctly detects all possible syntax errors submission upload your java files in the provided box directory as edits or additions to the provided code all programs will be tested on the command line so if you use an ide to develop your program you must export the java files from the ide and ensure that they compile and run on the command line do not submit the ide project files your ta should be able to download your directory from box and compile and run your code specifically javac infixexpressionevaluator java and java infixexpressionevaluator must compile and run your program when executed from the root of your directory in addition to your code you may wish to include a readme txt file that describes features of your program that are not working as expected to assist the ta in grading the portions that do work as expected your project is due at pm on thursday february you should upload your progress frequently even far in advance of this deadline no late assignments will be accepted motivation in this assignment you will use backtracking to solve the sudoku puzzle if you are not familiar with this puzzle review the rules here http www sudoku name rules en provided files first carefully read the provided files you can find these files on pitt box in a folder named where is your pitt username provided code the queens class includes a backtracking solution to the queens problem if you run this class with the t flag it will test its reject isfullsolution extend next methods the sudoku class includes the basic skeleton of a backtracking solution to the sudoku puzzle this class includes the method readboard which reads in a board and returns it as a int array this array will contain a for all empty cells it also includes the method printboard which prints out a board to system out it is recommended that you use this provided code as a starting point however you may choose not to use this provided code as long as your program reads the same input format and uses the required backtracking techniques example inputs example sudoku boards are available for you to test your code in the boards directory within your personal box folder tasks you must write a backtracking program to find values for all of the cells in a sudoku puzzle without changing any of the cells specified in the original puzzle you must also satisfy the sudoku rules no number may appear twice in any row column or region you must accomplish this using the backtracking techniques discussed and demonstrated in lecture and in queens java that is you need to build up a solution recursively one cell at a time until you determine that the current board assignment is impossible to complete in which case you will backtrack and try another assignment or that the current board assignment is complete and valid your program will read in the initial board from a file this initial board will have several cells already filled the remaining cells will be empty the goal is to find the values of all the cells in the puzzle without changing any of the cells specified in the original puzzle this file will contain lines of text each containing characters each character will either be a digit through or a non numeric character such as a space or a dot designating an empty cell your class must be named sudoku and therefore must be in a file with the name sudoku java it must also be in the package your program must be usable from the command line using the following command java sudoku su if there is a way to solve the puzzle described in su your program should output the completed puzzle you do not need to find every possible solution if there are multiple solutions just one solution will do if there is no solution your program should output a message indicating as such required methods as stated above you must use the techniques we discussed in lecture for recursive backtracking as such you will then need to write the following methods to support your backtracking algorithm isfullsolution a method that accepts a partial solution and returns true if it is a complete valid solution reject a method that accepts a partial solution and returns true if it should be rejected because it can never be extended into a complete solution extend a method that accepts a partial solution and returns another partial solution that includes one additional choice added on this method will return null if there are no more choices to add to the solution next a method that accepts a partial solution and returns another partial solution in which the most recent choice that was added has been changed to its next option this method will return null if there are no more options for the most recent choice that was made test methods when developing a complex program such as this one it is important to test your progress as you go for this reason in addition to the backtracking supporting methods above you will be required to test your methods as you develop them re read starting at chapter to review the concepts behind writing test methods to test each of the backtracking methods you need to write the following test methods for each one you should create a wide variety of partial solutions that fit as many corner cases as you can think of include enough test cases that the correct output convinces you that your method works properly in all situations testisfullsolution a method that generates partial solutions and ensures that the isfullsolution method correctly determines whether each is a complete solution testreject a method that generates partial solutions and ensures that the reject method correctly determines whether each should be rejected testextend a method that generates partial solutions and ensures that the extend method correctly extends each with the correct next choice testnext a method that generates partial solutions and ensures that the in each next method correctly changes the most recent choice that was added to its next option you may either hardcode your test partial solutions or upload su files containing the boards you want to test in either case when your program is run with the t flag your program must run the above four test methods and output the results grading your grade for this assignment will be based on your program success at solving a series of unknown sudoku puzzles and the thoroughness of your test methods submission upload your java files in the provided box directory as edits or additions to the provided code all programs will be tested on the command line so if you use an ide to develop your program you must export the java files from the ide and ensure that they compile and run on the command line do not submit the ide project files your ta should be able to download your directory from box and compile and run your code specifically javac sudoku java and java sudoku su must compile and run your program when executed from the root of your directory in addition to your code you may wish to include a readme txt file that describes features of your program that are not working as expected to assist the ta in grading the portions that do work as expected your project is due at pm on saturday march you should upload your progress frequently even far in advance of this deadline no late submissions will be accepted motivation imagine that you are starting a streaming radio business partnerships have been established to provide you with audio content and you will stream that content to users you plan to use your knowledge of what songs your users like dislike to make effective suggestions for new songs that they will like in this assignment you will consider such a streaming radio service you will be de signing an abstract data type adt that stores the service music library and users like dislike of songs this adt represents a data structure that will be used by both client applications and for administrative uses e g adding new songs to the library once you complete this adt you could provide it to the programmers of the client applications and they will be able to develop applications based on this specification much like you were tasked with doing in on the other hand you can also distribute the adt to the engineers in charge of implementing the functionality of the data structure thus these two parties can work in parallel on their respective components which can work together by virtue of both following the contract that your adt provides provided files first carefully read the provided files you can find these files on pitt box in a folder named where is your pitt username the streamingradio interface is a skeleton of your adt many details will need to be filled in see tasks the user song and station interfaces are provided as types for your methods parameters and or return types the doc directory contains javadoc style documentation of the provided code open index html tasks you must write an abstract data type as a java interface you will need to declare several abstract methods that the data structure would need to support you should consider all possible corner cases in your specification and describe what the class should do in all cases a client that reads your adt should know all the things that could go wrong and how they will be handled for some of these corner cases you will need to make a design decision regarding the best way to handle them helper types when developing your adt you can assume the following types exist user represents a user song represents a song station represents a collection of songs a playlist for a radio station you do not need to make any assumptions nor write any code regarding the details of these data structures they are provided to help you specify parameter and return types correctly interfaces representing these types are provided but do not declare any methods since their details are not needed in this assignment you can assume that these interfaces and their implementations would be developed separately abstract methods to declare you will need to declare each of the following abstract methods in your streamingradio interface addsong adds a new song to the system removesong removes an existing song from the system addtostation adds an existing song to the playlist for an existing radio station removefromstation removes a song from the playlist for a radio station ratesong sets an existing user rating for an existing song as either like or dislike clearrating clears an existing user rating on a song predictrating predicts whether a user will like or dislike a song that they have not yet rated suggestsong suggests a song for a user that they are predicted to like you can assume that the set of users and the set of radio stations are fixed e g at initial ization and thus you do not need to include operations to change these collections you will be completing the interface streamingradio a template of which is included in the provided code we have completed clearrating for you as an example to help you get started you are allowed to modify this method for consistency with the others if you take a different approach than that presented the other abstract methods are initially declared as void methods with no parameters you need to expand each declaration to specify a return type and parameters as necessary you also need to include a detailed comment for each abstract method describing its effect its input parameters its return value any corner cases that the client may need to consider any exceptions the method may throw including a description of the circumstances under which this will happen and so on you should include enough details that a client could use this data structure without ever being surprised or not knowing what will happen even though they haven t seen the implementation finally you should use javadoc style comments where appropriate so that running javadoc on your code will produce a readable documentation page you do not need to use advanced features of javadoc but you should include param return and throws comments as in our example code from class and the textbook a documentation page for the initial streamingradio interface is included in the provided code this was generated using the following command javadoc d doc java note here that java specifies that all java files within should be processed and d doc specifies that the javadoc output should be created in a directory named doc finally to test if your interface is well formed you should compile it using javac from the command line you must include your code in the package you will not be able to run your program because you will not be writing any executable code much less a main method it should however compile successfully grading your grade will be assigned based on whether the code is in the correct form and compiles successfully whether javadoc generates documentation correctly and your specification of each of the required abstract methods addsong removesong addtostation removefromstation ratesong predictrating and suggestsong submission upload your java files in the provided box directory as edits or additions to the provided code all programs will be tested on the command line so if you use an ide to develop your program you must export the java files from the ide and ensure that they compile and generate javadoc documentation from the command line do not submit the ide project files your project is due at pm on thursday march you should upload your progress frequently even far in advance of this deadline no late submissions will be accepted cs data structures https people cs pitt edu bill home cs data structures section b spring home policy lectures labs assignments general information cs data structures https people cs pitt edu bill home 2 course description this course emphasizes the study of the basic data structures of computer science e g bags lists stacks queues trees and their implementations using the java language we will cover objects and reference variables as well as programming techniques that use recursion students will be introduced to various searching and sorting methods and will develop an intuitive understanding of the complexity of algorithms textbook frank m carrano and timothy m henry data structures and abstractions with java edition pearson isbn online textbook resources available here top hat students must subscribe to top hat the platform we will use for lecture participation please see lectures for instructions grading exams higher grade is assignments lowest grade is weighted the others top hat attendance and lecture questions recitation quizzes cs data structures spring midterm examination study guide java prerequisites topics classes objects and references access modifiers arguments and parameters garbage collection self test questions appendix c designing classes topics composition and inheritance access modifiers static keyword overriding methods dynamic binding and method polymorphism reference type vs object type interfaces typecasting generic interfaces classes and methods generic type declarations including bounded types type wildcards and bounded wildcards assertions self test questions appendix d prelude java interludes exercises prelude 2 bag note since this was our first data structure we covered several topics in this section even though they weren t specific only to bags topics adts collections data structures and their relation to interfaces and classes client vs implementer considering corner cases test methods bag adt and interface bag array vs linked implementations resizing arrays inner classes static and non static self test questions chapters exercises chapter chapter 2 chapter algorithm analysis topics asymptotic analysis big o notation growth rates amortized analysis sum of the first n integers analysis of bag implementations self test questions chapter exercises chapter 1 10 17 stack topics stack interface using stacks to match brackets using stacks to evaluate postfix using stacks to convert infix to postfix array vs linked implementations of stack runtime memory usage program stack run time stack self test questions chapters exercises chapter 1 chapter 1 3 recursion topics breaking problems into subproblems requirements for recursion to work activation records divide conquer vs general recursion tail recursion easy vs hard recursive algorithms to make iterative overheads of recursion recursive backtracking general goals the specific structure we used next extend isfullsoln reject analyzing recursive methods with recursion trees processing arrays recursively by specifying bounds of subarray self test questions chapters exercises chapter 1 2 5 16 chapter 4 6 7 sorting topics simple sorts selection sort bubble sort insertion sort shell sort runtime analysis of sort methods self test questions chapters exercises chapter 1 3 13 java by dissection edition ira pohl charlie mcdowell university of california santa cruz ii copyright c by ira pohl and charlie mcdowell isbn table of contents table of contents iii preface xi acknowledgments xiv chapter introduction recipes algorithms being precise implementing our algorithm in java why java network computing and the web human computer interaction and the gui summary review questions exercises chapter program fundamentals hello world in java compiling and running your java program lexical elements white space comments keywords identifiers literals operators and punctuation data types and variable declarations variables variable initialization an example string concatenation strings versus identifiers versus variables user input calling predefined methods print println and printf formatting output with printf number types the integer types the floating point types the char type numbers versus strings arithmetic expressions an integer arithmetic example change type conversion assignment operators the increment and decrement operators precedence and associativity of operators programming style summary review questions exercises applet exercise chapter statements and control flow expression block and empty statements empty statement boolean expressions relational and equality operators logical operators the if statement problem solving with the if statement the if else statement nested if else statements if else if else if the dangling else problem the while statement problem solving with loops the do statement the for statement local variables in the for statement the break and continue statements the switch statement using the laws of boolean algebra programming style summary review questions exercises applet exercise chapter methods functional abstraction method invocation static method definitions the return statement scope of variables top down design problem solving random numbers a simulation probability calculations invocation and call by value problem solving a computer game twenty one pickup requirements analysis twenty one pickup design twenty one pickup implementation twenty one pickup testing recursion problem solving mathematical functions method overloading programming style summary review questions exercises applet exercise chapter arrays and containers one dimensional arrays indexing an array element array initialization array member length the for iterator statement passing arrays to methods array assignment finding the minimum and maximum of an array a simple sorting method searching an ordered array big oh choosing the best algorithm type and array booleans the sieve of eratosthenes char using a line buffer double accumulate two dimensional arrays two dimensional initializer lists an elementary simulation the game of life the game of life requirements analysis the game of life design the game of life implementation arrays of nonprimitive types arrays of strings the container arraylist problem solving palindromes programming style counting from zero summary review questions exercises chapter objects data abstraction the enumeration types enum methods instance methods enumeration operations a simplest class member access operator adding instance methods class counter access public and private data hiding constructor methods and object creation static fields and methods calling methods a recap calling a method in the same class calling instance methods calling class methods problem solving making change accessing another object private fields passing objects reference types scope nested classes keyword final and class constants arrays of objects object oriented design programming style summary review questions exercises applet exercise chapter inheritance a student is a person overriding instance methods calling print and println the access modifiers private and public revisited packages using packages creating packages the access modifier protected type object and inheritance abstract classes an example predator prey simulation interfaces multiple inheritance inheritance and design type conversions and polymorphism the operator instanceof automatic boxing of primitives automatic unboxing of primitives programming style summary review questions exercises chapter graphical user interfaces part i hello world button listening to events text and numerical input using several components drawing with swing the layout manager flowlayout a simple drawing program applets programming style summary review questions exercises chapter graphical user interfaces part ii arranging components in a gui the class borderlayout nesting containers in other containers getting a component to resize problem solving plotting data the class graphics changing the stroke used in drawing adding menus to a gui event listeners and adapters listener adapter classes pop ups and dialogs programming style summary review questions exercises chapter reading and writing files types of files writing text files reading text files parsing text streams formatting text output conversion characters width and precision specifications selecting arguments for conversion java printf versus c var args problem solving encrypting text files reading and writing binary files detecting the end of an input stream serialization and writing objects programming style summary review questions exercises chapter coping with errors exceptions exception handling with try and catch catching an eofexception abrupt return from methods catching several different exceptions the finally clause program correctness throwing an exception runtimeexceptions and the throws clause testing testing with junit assertions adding assert statements to your program enabling and disabling assertions preconditions postconditions and invariants debugging jdb jdb on programs that read from system in other jdb commands a debugger with a gui summary review questions exercises chapter dynamic data structures self referential structures a linked list implementation of a stack a singly linked list more operations on lists tostring for the class intlist doubly linked lists a generic stack an example polish notation and stack evaluation queues iterators using iterators to implement append sorting a linked list iterators and the interface iterator deleting objects packages package access using packages creating packages programming style summary review questions exercises chapter threads concurrent programming implicit threads from awt creating threads the interface runnable communication between two threads synchronizing two threads mutual exclusion using synchronized signal wait synchronization condition variables versus semaphores arrayblockingqueue passing messages to another computer a multithreaded server reusing worker threads methods sleep wait and notify calling sleep from anywhere calling wait and notify the method notifyall futures programming style summary review questions exercises appendix getting down to the bits a integer binary representation a two complement binary arithmetic a floating point representations a manipulating bits a bitwise operators a shift operators reference tables b operator precedence table b the standard java math functions summary of selected swing components c the class jbutton c the class jcombobox c the class jlist c the class jlabel c the class jtextfield c the class jtextarea c the class jpanel c the class jscrollpane index java by dissection is an introduction to programming in java that assumes no prior programming experi ence it thoroughly teaches modern programming techniques using java with generics it shows how all the basic data types and control statements are used traditionally the book then progresses to the object ori ented features of the language and their importance to program design as such the first half of the book can be used in a first programming course for college students by careful choice of exercises and supplemental material it could be used for a gentle introduction to programming for computer science or engineering majors also the second half of the book explains in detail much that is sophisticated about java such as its threading exception handling graphical user interface gui generics and file manipulation capabilities as such the book is suitable as the primary text in an advanced programming course or as a supplementary text in a course on data structures software methodology comparative languages or other course in which the instructor wants java to be the language of choice java invented at sun microsystems in the mid is a powerful modern successor language to c and c java like c adds to c the object oriented programming concepts of class inheritance and run time type binding the class mechanism provides user defined types also called abstract data types while sharing many syntactic features with c and c java adds a number of improvements including automatic memory reclamation called garbage collection bounds checking on arrays and strong typing in addition the stan dard java libraries called packages in java provide platform independent support for distributed program ming multi threading and graphical user interfaces although java shares many syntactic similarities to c unlike c java is not a superset of c this has allowed the creators of java to make a number of syntactic improvements that make java a much safer pro gramming language than c as a result java is much better as a first programming language java by dissection begins with a classical programming style starting with programs as a simple sequence of instructions then adding in control flow and functional abstraction after that comes arrays and data abstraction using classes which can be covered in either order arrays first or data abstraction with classes first then comes the material on inheritance and graphical user interfaces again the chapter on inheritance can be covered before or after the first chapter on graphical user interfaces finally come the advanced chapters the following figure illustrates the flexibility in the order in which the material can be covered introduction fundamentals control flow methods arrays objects data structures guis inheritance guis part ii files exceptions threads the book emphasizes working code one or more programs particularly illustrative of the chapter themes are analyzed by dissection which is similar to a structured walk through of the code dissection explains to the reader newly encountered programming elements and idioms because java includes a relatively easy to use standard package for creating graphical user interfaces it is possible to introduce the use of guis in a beginning programming book creating programs with guis is as fundamental today as being able to create nicely formatted text output to fully understand the gui pack ages in java it is necessary to have some understanding of oop and inheritance the main chapters on gui building immediately follow the chapters on objects and inheritance for those students interested in getting an early exposure to some of the graphical aspects of java we have provided a series of extended exercises at the end of the early chapters that introduce guis and applets these exercises provide templates for some simple applets without providing complete explanations of some of the language features required the following summarizes the primary design features that are incorporated into this book dissections each chapter has several important example programs major elements of these programs are explained by dissection this step by step discussion of new programming ideas helps the reader encounter ing these ideas for the first time to understand them this methodology has been proven effective in numer ous programming books since its first use in a book on c in teaching by example the book is a tutorial that stresses examples of working code right from the start the student is introduced to full working programs exercises are integrated with the examples to encourage experimentation excessive detail is avoided in explaining the larger elements of writing working code each chapter has several important example programs major elements of these programs are explained by dissec tion object oriented programming oop the reader is led gradually to an understanding of the object oriented style objects as data values are introduced in chapter program fundamentals this allows an instructor partial to the objects early approach to emphasize oo from the start chapter objects data abstraction shows how the programmer can benefit in important ways from java and object oriented programming object oriented concepts are defined and the way in which these concepts are supported by java is intro duced chapter inheritance develops inheritance and dynamic method dispatch two key elements in the oop paradigm preface xiii terminal input and output java has added a scanner class that makes terminal input easy this replaces our first edition tio package which is no longer needed also the addition of printf for variable argu ment fromatted output is discussed container classes and generics in java the text covers this added feature to java containers are imple mented as generics standard containers include vector linkedlist arraylist and priorityqueue these also allow iterator access and can be used with the for iterator statement that hides indexing swing and graphical user interfaces an important part of java is its support for platform independent cre ation of graphical user interfaces and the web based programs called applets inchapter graphical user interfaces part i and chapter graphical user interfaces part ii we present a basic introduction to using the standard java package swing for building guis these chapters provide enough information to create useful and interesting applets and guis a few additional gui components are presented briefly in appendix c summary of selected swing components for students anxious to begin writing applets simple applets are introduced in a series of exercises beginning in chapter program fundamentals these exercises are com pletely optional the coverage of applets and guis in chapter graphical user interfaces part i and chapter graphical user interfaces part ii does not depend upon the student having done or read the earlier applet exercises threads multi threaded programming is not usually discussed in a beginning text however some under standing of threads is essential for a true understanding of the workings of event driven gui based pro grams in addition the authors feel that thread based programming will become increasingly important at all levels of the programming curriculum threading is explained in chapter threads concurrent program ming and used to introduce the reader to client server computing this book gives a treatment suitable to the beginning programmer that has mastered the topics in the preceding chapters course tested this book is the basis of courses given by the authors who have used its contents to train stu dents in various forums since the material is course tested and reflects the author considerable teaching and consulting experience code examples all the major pieces of code were tested a consistent and proper coding style is adopted from the beginning and is one chosen by professionals in the java community the code is available at the addison wesley longman web site www awl com common programming errors many typical programming bugs along with techniques for avoiding them are described this books explains how common java errors are made and what must be done to correct them the new edition also describes the use of assertions that came in with exercises the exercises test and often advance the student knowledge of the language many are intended to be done interactively while reading the text encouraging self paced instruction web site the examples both within the book are intended to exhibit good programming style all examples for this book are available on the web at www soe ucsc edu pohl java new in the second edition java came out in september of with important changes over the original language these include container based generics enum types assertions more threading refinements ter minal io using scanner and formatted io using printf we have included all these important topics and where possible revised the text to take advantage of them course use first course in programming java by dissection is designed to be used as a basic first programming course similar in scope to courses that used c pascal or c chapters cover such a curriculum first course in programming swing emphasis by using supplementary material at the end of each of the first chapters the instructor can take a gui approach as opposed to the traditional terminal i o approach course in programming by covering material at a more rapid pace for well prepared computer sci ence majors the book can function as the primary text in a sequence the instructor would select more advanced exercises and cover material through chapter second course in programming the book can be used as a second or advanced course covering object oriented programming chapters can be skimmed by anyone already familiar with a procedural pro gramming language such as c or pascal a programmer already familiar with oop concepts could also skim chapters and chapters provide a mix of interesting advanced topics not generally covered in a beginning programming course acknowledgments our special thanks go to debra dolsberry and linda werner for their encouragement and careful reading and suggestions for improvement debra was especially helpful with typesetting issues our student sarah berner was an important contributor to the effectiveness of the text in its first edition and especially helpful in con verting many examples and exercises over to swing thanks to uwe f mayer for his careful review of the second edition which caught numerous errors charlie mcdowell and ira pohl university of california santa cruz java is the first major programming language to be shaped by the world wide web the web java allows you to do traditional programming java also has many special features and libraries that allow you conveniently to write programs that can use the web resources these include extensive support for graphical user inter faces the ability to embed a java program in a web document easy communication with other computers around the world and the ability to write programs that run in parallel or on several computers at the same time in this chapter we give an overview of how to solve a problem on a computer in this process you must first construct a recipe for solving the problem then you must convert the recipe into a detailed set of steps that the computer can follow finally you must use a programming language such as java to express the steps in a form understandable by the computer the java form of the solution is then translated by a program called a compiler into the low level operations that the computer hardware can follow directly we then discuss why java is creating such a fuss in the computing world in general terms we explain the importance of computing on the web and the character of the graphical user interfaces or guis that are partly behind the switch to java throughout this text we feature carefully described examples many of which are complete programs we often dissect them allowing you to see in detail how each java programming construct works topics intro duced in this chapter are presented again in later chapters with more detailed explanations the code and examples are meant to convey the flavor of how to program you should not be concerned about understand ing the details of the examples they are given in the spirit of providing an overview if you are already famil iar with what a program is and want to start immediately on the nitty gritty of java programming you may skip or scan this chapter computer programs are detailed lists of instructions for performing a specific task or for solving a particular type of problem programlike instruction lists sometimes called algorithms for problem solving and task performance are commonly found in everyday situations examples include detailed instructions for knitting a sweater making a dress cooking a favorite meal registering for classes at a university traveling from one destination to another and using a vending machine examining one of these examples is instructive consider this recipe for preparing a meat roast the recipe is typically imprecise what does sprinkle mean where is the thermometer to be inserted and what is a sufficient amount of pan drippings however the recipe can be formulated more precisely as a list of instructions by taking some liberties and reading between the lines cooking a roast sprinkle roast with teaspoon salt and pepper turn oven on to c insert meat thermometer into center of roast wait a few minutes if oven does not yet register c go back to step place roast in oven wait a few minutes check meat thermometer if temperature is less than c go back to step remove roast from oven if there is at least cup of pan drippings go to step prepare gravy from meat stock and go to step prepare gravy from pan drippings serve roast with gravy these steps comprise three categories of instructions and activities those that involve manipulating or changing the ingredients or equipment those that just examine or test the state of the system and those that transfer to the next step steps and are examples of the first category the temperature test in step and the pan dripping test in step are instances of the second category and the transfers in steps and go to step x are examples of the last category by using suitable graphical symbols for each of these categories a simple two dimensional representation of our cooking algorithm can be obtained as shown in the following illustration such a figure is called a flowchart to perform the program prepare the roast just follow the arrows and the instructions in each box the manipulation activities are contained in rectangles the tests are shown in dia monds and the transfer or flow of control is determined by the arrows because of their visual appeal and clarity flowcharts are often used instead of lists of instructions for informally describing programs some cookbook authors even employ flowcharts extensively in this book we use flowcharts in chapter state ments and control flow when describing the behavior of some java language constructs our recipe for preparing a roast can t be executed by a computer because the individual instructions are too loosely specified let consider another example one that manipulates numbers instead of food you need to pay for some purchase with a dollar bill and get change in dimes and pennies the problem is to determine the correct change with the fewest pennies most people do this simple everyday transaction unthinkingly but how do we precisely describe this algorithm in solving such a problem trying a specific case can be useful let say that you need to pay cents and need change for a dollar you can easily see that one dollar minus the cents leaves you with cents in change the correct change having the fewest coins in dimes and pennies would be two dimes and three pen nies the number of dimes is the integer result of dividing by and discarding any fraction or remain der the number of pennies is the remainder of dividing the cents by an algorithm for performing this change for a dollar is given by the following steps change making algorithm assume that the price is written in a box labeled price subtract the value of price from and place it in a box labeled change divide the value in change by discard the remainder and place the result in a box labeled dimes take the integer remainder of change divided by and place it in a box labeled pennies print out the values in the boxes dimes and pennies with appropriate labels halt this algorithm has four boxes namely price change dimes and pennies let execute this algorithm with the values given suppose that the price is cents always start with the first instruction the contents of the four boxes at various stages of execution are shown in the following table box step step step step step price change dimes pennies to execute step place the first number in the box price at the end of instruction the result of sub tracting from is which is placed in the box change each step of the algorithm performs a small part of the computation by step the correct values are all in their respective boxes and are printed out study the example until you re convinced that this algorithm will work correctly for any price under a good way to do so is to act the part of a computer following the recipe following a set of instructions in this way formulated as a computer program is called hand simulation or bench testing it is a good way to find errors in an algorithm or program in computer parlance these errors are called bugs and finding and remov ing them is called debugging we executed the change making algorithm by acting as an agent mechanically following a list of instructions the execution of a set of instructions by an agent is called a computation usually the agent is a computer in that case the set of instructions is a computer program in the remainder of this book unless explicitly stated otherwise we use program to mean computer program the algorithm for making change has several important features that are characteristic of all algorithms characteristics of algorithms the sequence of instructions will terminate the instructions are precise each instruction is unambiguous and subject to only one interpre tation the instructions are simple to perform each instruction is within the capabilities of the execut ing agent and can be carried out exactly in a finite amount of time such instructions are called effective there are inputs and outputs an algorithm has one or more outputs answers that depend on the particular input data the input to the change making algorithm is the price of the item pur chased the output is the number of dimes and pennies our description of the change making algorithm although relatively precise is not written in any formal pro gramming language such informal notations for algorithms are called pseudocode whereas real code is something suitable for a computer where appropriate we use pseudocode to describe algorithms doing so allows us to explain an algorithm or computation to you without all the necessary detail needed by a com puter the term algorithm has a long involved history originally stemming from the name of a well known arabic mathematician of the ninth century abu jafar muhammed musa al khwarizmi it later became associated with arithmetic processes and then more particularly with euclid algorithm for computing the greatest common divisor of two integers since the development of computers the word has taken on a more precise meaning that defines a real or abstract computer as the ultimate executing agent any terminating computa tion by a computer is an algorithm and any algorithm can be programmed for a computer in this section we implement our change making algorithm in the java programming language you need not worry about following the java details at this point we cover all of them fully in the next two chapters we specifically revisit this example in section an integer arithmetic example change on page for now simply note the similarity between the following java program and the informal algorithm presented earlier you not only have to be able to formulate a recipe and make it algorithmic but you finally have to express it in a computer language makechange java change in dimes and pennies import java util use scanner for input class makechange public static void main string args int price change dimes pennies scanner scan new scanner system in system out println type price to price scan nextint change price how much change dimes change number of dimes pennies change number of pennies system out print the change is system out print dimes system out print dimes system out print pennies system out print pennies n price scan nextint the scan nextint is used to obtain the input from the keyboard the value read is stored in the vari able price the symbol is called the assignment operator read the first line as price is assigned the value obtained from the typing an integer on your keyboard at this point you must type in an integer price for example you would type and then hit enter change price how much change this line computes the amount of change dimes change number of dimes pennies change number of pennies the number of dimes is the integer or whole part of the result of dividing change by the symbol when used with two integers computes the whole nonfraction part of the division the number of pen nies is the integer remainder of change divided by the symbol is the integer remainder or modulo operator in java so if change is the integer divide of is and the integer remainder or modulo of is system out print the change is system out print dimes system out print dimes system out print pennies system out print pennies n in this example the system out print statements cause the values between the parentheses to be printed on the computer console the first one just prints out the characters between the quotation marks the second one converts the value in dimes to the sequence of digits and prints those digits the other print statements are similar for an input value of the output would be the n in the last print statement indicates that a newline should be sent to the console ending the line of output a variety of programming languages are available for expressing programs but the most useful ones are suitable for both machine and human consumption in this book we cover programming by using one such language the programming language java java is a relatively new programming language first introduced in in this book a language is needed that allows algorithms to be easily understood designed analyzed and implemented as computer programs the following is an excerpt from the original paper introducing java to the world network computing and the web in his science fiction novel the rolling stones robert a heinlein comments every technology goes through three stages first a crudely simple and quite unsatisfactory gadget second an enormously complicated group of gadgets designed to overcome the short comings of the original and achieving thereby somewhat satisfactory performance through extremely complex compromise third a final proper design therefrom heinlein comment could well describe the evolution of many programming languages java presents a new viewpoint in the evolution of programming languages creation of a small and simple language that still sufficiently comprehensive to address a wide variety of software application development although java is superficially similar to c and c java gained its simplicity from the systematic removal of features from its predecessors we agree with its creators that java has to a large extent lived up to the promise of a small simple yet com prehensive programming language as such java is both an excellent programming language for real pro gram development and a good first programming language possibly even more important for java initial success are the features that make it appropriate for develop ment of programs distributed over the internet these programs called applets execute inside internet browsers such as firefox mozilla netscape and internet explorer much of modern computing is done in a connected environment that is computers are connected to other computers in a network this connection allows the computers to exchange information or to talk to each other java was developed with network computing as the normal environment it has many features and libraries of parts of programs that promote networking earlier languages and systems viewed a computer as a lone instrument doing significant computations the results of which would largely be output to a screen or printer by promoting networking whereby computers are connected and pass results to each other dynami cally and cooperate on large scale computations java became the principal language for computing on networks the largest network is the global network called the internet using the internet researchers at the european particle physics laboratory cern developed a way to share information via a formatting language called hyper text markup language or html the computers exchanged html documents by means of a protocol called hyper text transfer protocol or http a protocol is like a language that computers use to talk to each other html allows one electronic document to link to another electronic document on a different computer the program used to view html documents and follow the links is called a browser with the click of a but ton a user could move from one document on one computer to another related document on another com puter because of these links the resulting web of documents was dubbed the world wide web or simply the web today many people equate the web with the internet but the web is only one application of the tech nology known as the internet java programs called applets can be written to automatically load and execute by following a link in the web this ability to embed java programs in html documents was a primary factor in the early success of java with java applets web documents are no longer static text images or video segments web documents can provide all the interaction of any program we discuss graphical user interfaces and applets in chapter graphical user interfaces part i if you re interested in an early exposure to applets we also provide templates for simple applets in the exer cises at the end of chapters through from the world wide web www and http you can begin to make some sense of the ubiquitous internet addresses such as http www company com this type of address is called a universal resource locator or url these are addresses embedded in html documents linking one document to another client server computing is an essential new element in computers that manage communications and compu tations the server typically a fast computer with a very large amount of hard disk storage gives out infor mation to clients upon request an example would be a server giving out stock market quotes to clients that request it the network support in java makes implementing client server programming solutions easy in chapter graphical user interfaces part ii we show you how to implement a simple java server program that can connect and respond to requests from multiple clients on other computers human computer interaction and the gui in the early days of computing most interaction between a computer and a human being was in the form of typed input to the computer and printed output to the person most human computer interaction today is done with a graphical user interface gui pronounced gooey the user has a keyboard usually similar to a standard typewriter keyboard a pointing device usually a mouse and a display device capable of displaying both text and graphics the user display is usually divided into viewing areas called windows associated with the windows are menus of commands relating to manipulation of the data displayed in the window the display screen may also contain icons or small graphical images whose visual appearance is designed to signify their functions for example a picture of a trash can represents deleting a file and an arrowhead is used to scroll a viewing screen the following screen shot of a typical desktop shows several windows icons and a row of menus across the top of each window for many years the task of creating a graphical user interface for a new application was extremely time con suming and difficult the programming languages and operating systems in use did not make building such interfaces easy these interfaces were usually built from libraries of program parts that were suitable only for one particular operating system if a company building a new program wanted to have its program run on computers with different operating systems it would have to create multiple versions of the programs java provides a solution software companies now sell programs that make building graphical user interfaces relatively easy java includes a library of program parts called swing for building graphical user interfaces the program parts in swing can be used on any computer that can run java programs thus a programmer can now write one version of a program and have it run on many different kinds of computers although in depth coverage of swing is beyond the scope of this book swing is simple enough to use that even beginning programmers can begin to build programs with graphical user interfaces we discuss swing further in chap ter graphical user interfaces part i and chapter graphical user interfaces part ii by the end of this book you will be writing your own sophisticated applets and regular java programs with guis if you ve used a java enabled browser such as netscape or explorer you ve probably unknowingly made use of an applet if not you can try out a simple applet calculator right now by going to www soe ucsc edu charlie java jbd minicalcapplet html this is the same applet presented in section applets on page summary informally an algorithm is a list of instructions for performing a specific task or for solving a particular type of problem algorithmlike specifications for problem solving and task performance are commonly found in everyday situations a recipe is a kind of algorithm one graphical representation of an algorithm is a flow chart to perform the algorithm the user just fol lows the arrows and the instructions in each box the manipulation activities are contained in rectangles the tests are shown in diamonds and the transfer or flow of control is determined by the arrows because of their visual appeal and clarity flow charts are often used instead of lists of instructions for informally describing algorithms algorithms that can be executed on computers are called programs programs are written in special lan guages called programming languages such as java which we use in this book modern computing is done in a connected environment a computer is connected to other computers in a network this connection allows the computers to exchange information or to talk to each other java was developed with network computing as the normal environment the largest network is the global network called the internet this network is used to exchange documents throughout the world with a common format called html this format allows links to be followed across the internet because of these links the resulting web of documents was named the world wide web or simply the web it is possible to write java programs called applets that are automatically loaded and executed as the result of following a link in the web this ability to embed java programs in html documents is a pri mary factor in the success of java with java applets web documents are no longer static text images or video segments web documents can provide all the interactions of any program review questions what is an algorithm a graphical description of a computation using boxes and arrows is called a informal but relatively precise descriptions of algorithms are called what is bench testing why is it important what did java util refer to in the makechange java program what was system out print used for in the java program what is html what is http what is url what is a browser name a commonly used browser what are the primary components of a typical human computer interface today the information provided by the user to a program when it executes is called the the answers provided by the program are called the what is swing exercises write a flow chart for the following recipe for cooking baked beans taken from the natural foods cook book nitty gritty productions concord california let m and n be positive integers draw a flowchart for the following algorithm for computing the quo tient q and remainder r of dividing m by n set q to if m n then stop q is the quotient and m is the remainder replace m by m n increment q by go to step bench test the algorithm in the previous exercise use m and n as values let a rectangle that is aligned with the coordinate axes be represented by the coordinates of its lower left and upper right corners xmin ymin and xmax ymax respectively as shown in the following illustra tion given two such rectangles and devise an algorithm that finds the rectangle if any that is common to both and the input data are the eight real numbers representing the coordinates of the rectangles corners the illustration shows two rectangles that have a common rectangle shown in grey suppose that a line segment is represented by its two endpoints pe xe ye and pp xp yp for two line segments and devise an algorithm that computes the coordinates of their point of intersec tion if any two line segments intersect if they have only one point in common make a list of all the automatic interactions that you have with computers for example utility bills automatic mailing lists class scheduling and so on give a detailed step by step procedure also called an algorithm for traveling from your place of residence to school or work manually dividing one number by another long division taking an unordered set of numbers and putting them in ascending sequence i e sorting the numbers in ascending sequence or playing and never losing the game of tic tac toe in this chapter we introduce programming fundamentals in java we write a java program by using different elements of the java language the most basic elements are the tokens of the language tokens are the words that make up the sentences of the program programming is comparable to writing to write an essay we write words that make up sentences sentences that make up paragraphs and paragraphs that make up essays in this chapter we concentrate on how to use the words and combine them into the program equiv alent of useful sentences and paragraphs hello world in java a simple first java program is the classic hello world program so named because it prints the message hello world on the computer screen helloworld java purpose the classic hello world program it prints a message to the screen author jane programmer as derived from kernighan and richie class helloworld public static void main string args system out println hello world dissection of the helloworld program helloworld java purpose everything between a and a is a comment comments are ignored by the java compiler and are inserted to make the program more understandable to the human reader every program should begin with a comment such as the one in this example in our examples the name of the file appears in the comment this example indicates that it is from the file helloworld java other things to include in pro gram comments are the function or purpose of the program the author of the program and a revision history with dates indicating major modifications to the program class helloworld the word class is a keyword preceding the name of the class a keyword has a predefined special pur pose a class is a named collection of data and instructions the name of the class being defined in this example is helloworld the left brace begins the definition of a class a matching right brace is needed to end the class definition forgetting to match braces is a common error public static void main string args this line declares that the class helloworld contains a method with the name main a method is a named group of instructions within a class in this example only one method named main is defined for the class helloworld in chapter methods functional abstraction we create classes that contain several methods in a single class when we use the name of a method we add parentheses at the end to remind you that it is a method this convention comes from actual java code wherein the name of a method is always followed by parentheses the method defined by this line is thus referred to as main there are two kinds of java programs stand alone applications and applets the method main appears in every stand alone java program indicating where program execution will begin later we use a different line that serves a similar purpose for applets an explanation of the words public static and void in this line is left until later system out println hello world the entire body of the method main the real instructions to the computer appears between the braces in this example just one instruction prints the desired message you can memorize this instruc tion and use it as an incantation to get something printed out on your computer screen what gets printed is between the quotation marks compiling and running your java program you will be dealing with two different representations of your programs the part you write and a form more suitable for the computer to use when it finally runs your program the text you write to give the computer instructions is called the source code or simply the source this source code will be compiled by the java com piler into a form more suitable as instructions to the computer called object code the source code form of compiling and running your java program the program is represented in the language java that you will be learning informally you can think of the source code as the raw form of the program in contrast to the object code which is the cooked or compiled form in java all source code file names have the suffix java such as helloworld java the result of correctly compiling helloworld java is the object code helloworld class in some situations the name of the file with out the java and the name of the class defined in the file must be the same although this requirement does not apply to the programs in the first part of this book we follow that practice even when it isn t required and suggest that you do the same there are many names and forms for the machine representation of a program after it has been processed somewhat by the computer a common first step in processing source code is to compile it which means to translate it into a form more suitable for the computer for most common programming languages this com piled form is called the machine code object code or binary form when you buy a piece of software you usu ally get binaries or an executable image for java this form is a bit different and is called java bytecode these bytecodes are the same whether you are running on a macintosh on an intel machine with microsoft windows or on a machine from sun running unix this sameness is an important advantage of java pro grams as they can be written to be platform independent which is generally not true for most other pro gramming languages such as c or cobol in java all bytecode files must have a name that ends in class such as helloworld class the word class is used because in java programs are broken into chunks called classes much like a chapter is broken into sections the following diagram illustrates the compilation process and the conventions just described the compilation process helloworld class bytecode there are two principal methods for compiling and running java programs one method uses an integrated development environment ide of which there are many for java the actual details of compiling for each of the ides vary slightly but the basic steps are the same java via the ide use the editor that comes with the ide to create the source file these editors generally help with syntax by using special fonts or colors for keywords and helping you match braces and so on create a project and add your source file to the project select run from a menu the ide will automatically determine that the source file needs to be compiled and compile it before trying to run the program the other principal method is the command line approach in it you run the compiler yourself from the com mand line of either a command prompt in windows or unix terminal or shell program for both unix and windows the steps are the same java via the command line use your favorite text editor to create the source file save the source into a file ending with the extension java compile the program with the command javac followed by the source file name for example javac helloworld java if the program compiles without errors run the program with the command java followed by the name of the class do not append class as in helloworld class use only the class name for example java helloworld the last two steps are shown as follows for the program helloworld os prompt javac helloworld java os prompt java helloworld hello world os prompt java has rules about how its language elements can be put together to form a complete program in this sec tion we begin by looking at the various words and symbols called lexical elements that are used to construct java programs the most fundamental element in the structure of a program is a single character that can be displayed on a computer screen or typed at a computer keyboard prior to java most programming languages such as c and c used the ascii character set it provides for different characters which is enough to represent all the characters on the conventional english language keyboard this set may seem like a lot but when you consider all the human languages in the world and the various symbols they use it is inadequate because java was designed to be used throughout the world not just in english speaking countries java developers adopted the unicode character set it provides for more than different characters when a java compiler first begins to analyze a java program it groups the individual characters into larger lexical elements usually called tokens some tokens such as the plus sign which is the java symbol used to add two numbers are only one character long other tokens such as the keywords class and public are many characters long these basic tokens are then combined into larger language forms such as expres sions an example of an expression comprising three tokens is x y there are five types of tokens keywords identifiers literals operators and punctuation white space and comments are two additional lexical elements that are discarded early in the compilation process white space white space in java refers to the space character which you get when you strike the space bar on the key board the tab character which is actually one character although it may appear as several spaces on your screen and the newline character which you get when you hit the return or enter key on the keyboard white space is used primarily to make the program look nice and also serves to separate adjacent tokens that are not separated by any other punctuation and would otherwise be considered a single longer token for exam ple white space is used to separate the following three tokens public static void in the helloworld program in such situations where one white space character is required any number of white space characters can be used for example we could have put each of the words public static and void on separate lines or put lots of spaces between them as in public static void main except for string literals which we discuss shortly any number of adjacent white space characters even mixing tab space and newline characters is the same as just one white space character as far as the struc ture and meaning of the program are concerned stated another way if you can legally put in one space you can put in as many spaces tabs and newlines as you want you can t put white space in the middle of a key word or identifier such as a variable or class name we discuss keywords identifiers and variables later in this chapter comments comments comments are very important in writing good code and are too often neglected the purpose of a comment is to provide additional information to the person reading a program it serves as a concise form of program documentation as far as the computer is concerned the comment does not result in a token it separates other tokens or is ignored completely when it isn t needed to separate tokens java has three ways to specify comments a single line comment begins with and causes the rest of the line all characters to the next newline to be treated as a comment and ignored by the compiler it is called a single line comment because the comment can t be longer than a single line by definition the comment ends at the end of the line containing the a multiline comment can extend across several lines in a program the beginning of the comment is marked with and the end of the comment is marked with everything between the marks and the marks them selves is a comment and is ignored here is the multiline comment from our first java program helloworld java purpose this is the classic hello world program it simply prints a message to the screen author jane programmer as derived from kernighan and richie the single asterisks on the intermediate lines are not required and are used merely to accent the extent of the comment these comments are also called block comments the third style of comment is a minor variation on the multiline comment the beginning marker has an additional asterisk that is the beginning of the comment is marked with and the end of the comment is marked with these comments are identical to the multiline comment except that they are recognized by a special program called javadoc that automatically extracts such comments and produces documentation for the program organized as an html document see section programming style on page for more about javadoc keywords keywords also known as reserved words have a predefined special purpose and can t be used for any but that purpose each of the keywords in java has a special meaning to the java compiler a keyword must be separated from other keywords or identifiers by white space a comment or some other punctuation symbol the following table shows all the java keywords abstract continue for new synchronized assert default goto package this boolean do if private throw break double implements protected throws byte else import public transient case enum instanceof return try catch extends int short void char final interface static volatile class finally long super while const float native switch the keywords const and goto have no meaning in java they are keywords in c a language that was a precursor to java they are included as keywords to facilitate error reporting when programmers with c experience accidentally use them in addition the words null true and false look like keywords in that they have a predefined meaning but they are in fact literals as discussed later identifiers identifiers are the names used to specify different elements of a java program such as a class method or variable we discuss variables in section variables on page an identifier in our first program was helloworld a name we picked for the class another identifier was the library method name println a name picked by the java developers in both cases the name gives a clue as to the use of that element of the program an identifier is any sequence of java letters and digits the first of which must be a java letter with two exceptions a keyword can t be an identifier and the special literal terms true false and null can t be used as identifiers the java letters and digits include the letter and digit symbols for many modern written languages the java letters include the english language uppercase and lowercase letters the and the underscore the last two are included because of java close kinship to the programming language c which included these symbols as legal characters in identifiers the java digits are through in our exam ples we use only english letters and digits the following are some examples of legal identifiers along with comments providing some explanation data variable name conveying its use helloworld class name youcanalmostmakeasentence unlikely nextint method name from scanner class x simple variable usually double obscure name a poor choice the following are some illegal identifiers along with comments indicating what the sequence of symbols really is a digit or integer literal x y an expression where x and y are identifiers some name illegal internal characters no space intended was nospace cannot start with a digit class keyword cannot be used as an identifier literals java has built in types for numbers characters and booleans java also has a standard class type for strings the type of a data value tells the computer how to interpret the data these built in types are called primitive types in java literals also called constants are the literal program representations of values for the primitive numeric types the primitive type boolean the primitive character type char and the standard class string type string without going into the details of these various types the following are some examples of literal values java type explanation examples int integers numbers without fractional parts double double precision numbers with fractional parts 01 string arbitrary strings of characters oh j boolean logical values true or false true false char single characters a operators and punctuation like keywords the symbols true and false can t be used as identifiers they are reserved to represent the two possible boolean values we explain later in more detail what constitutes an acceptable literal for each data type operators and punctuation in addition to keywords identifiers and literals a java program contains operators and separators or punc tuation the operators are things like and the separators are things like the that terminates some statements and the braces used to group things to fully understand operators you need to understand type precedence and associativity type determines the kind of value computed such as int or double precedence determines among operators such as and used in an expression which is done first associa tivity is the order in which operators of the same precedence are evaluated and is usually left most first for example int n n n an assignment expression the variable n is an integer variable initialized to next n is multiplied by the integer literal this result is added to and finally this value is assigned to n replacing the original value of with the new value of precedence of the multiplication operator is higher than so the multiplication is done before the addi tion precedence of the assignment operator is lowest so assignment occurs as the last action in this expression we discuss precedence and associativity of operators further in section precedence and associativity of operators on page in order to do something useful computer programs must store and manipulate data many programming languages including java require that each data item have a declared type that is you must specify the kind of information represented by the data or the data type the data type determines how data is repre sented in the computer memory and what operations can be performed on the data different programming languages support different data types a data type can be something as fundamen tal as a type for representing integers or as complex as a type for representing a digital movie some exam ples of data types found in java are int for representing integers or whole numbers double for representing numbers having a fraction string for representing text button for representing a push button in a graphical user interface and point for representing points in a plane the types of data that are created stored and manipulated by java programs can be separated into two main groups primitive types and class types or simply classes there are eight primitive types the numeric types byte short int long float and double for storing numeric values the character type char for storing a single alphabetic character digit or symbol and the type boolean for storing true or false primitive data values can be created by using literals such as a and true primitive values can also be operated on by using built in operators such as for addition and for subtraction of two numeric values producing a new primitive value for example uses addition to operate on the two numeric literal values and to produce the new primitive value standard java has more than classes the string button and point types mentioned previously are standard java classes you will learn in chapter objects data abstraction that you can create your own classes also in chapter arrays and containers we discuss arrays which are a special case of class types the data values that are class types are called objects you can create object data values by using the special operator new followed by the class name and possibly some additional values needed to create the new object for example new button quit creates a new object describing a button with the label quit you can create new objects of type string as a special case by using the string literal notation that sur rounds the text of the string with double quotation marks for example hello world creates a new string object in most cases the operations supported for the particular class are given a name and invoked by placing the name after the object value separated by a dot for example hello world length operates on the literal string hello world and evaluates to the number of characters in this string including any blanks operations such as length defined for a particular class are called methods we dis cuss methods in great detail in subsequent chapters variables in all but the most trivial programs such as helloworld you will declare variables that are identifiers used to refer to data values that are stored in the computer memory these are called variables because a vari able actually refers to a particular place in the computer memory and the value stored in the computer memory can vary as the program runs a variable declaration always begins with a type and ends with a semi colon the type is used to identify the kind of data that will be stored in the memory location associated with the variable being declared some examples of variable declarations are int i j string sentence boolean hot cold lukewarm button clicktoexit note that you can declare several variables of the same type by separating the names with a comma good choice of variable names is important to writing clearly understandable code stylistically choose variable names that are meaningful also variable names usually start in lowercase and if they are multiword the internal words start with an uppercase character as in clicktoexit variable initialization variables can be given initial values the preceding set of declarations given initial values for each variable becomes int i j string sentence i am a camera boolean hot true cold false lukewarm false button clicktoexit new button exit initializing variables with literals of their respective type is normal in this example the int variable i is ini tially given the value the boolean variable hot is initially true the string variable sentence is initial ized with a string literal an example string concatenation with the exception of string java has no literals for creating object values you initialize the button vari able by creating a button object using new as discussed briefly earlier we discuss object creation in chapter objects data abstraction the following example is a complete program that declares three variables and sentence values are then assigned to the parts of the computer memory referred to by those variables java simple variable declarations class public static void main string args string declare a string variable string sentence declare two more hello world sentence concat system out println sentence dissection of the program string declare a string variable string sentence declare two more whenever you introduce a new identifier you must declare it you declare that an identifier is a variable by first writing the name of the kind of value the variable refers to called the type the type is string in this example insert the name of the new variable after the type for variables in java the computer must always know the type of value to be stored in the memory location associated with that variable as shown in the second line you can declare more than one variable at a time by giving first the type and then a comma separated list of new identifiers hello world the symbol is called the assignment operator and is used to store values in variables read the first statement as gets the value hello or is assigned the value hello here it is used to assign the string literals hello and world to the newly declared variables and respectively the variable name will always be on the left and the new value to be assigned to the vari able will always be on the right saying assign the value hello to the variable really means to store the string value hello in the computer memory location associated with the variable sentence concat this statement contains an expression used to create a third string value which is then assigned to the variable sentence this expression uses the method concat which is defined for values of type string recall that operations on objects are called methods this particular operation requires a sec ond string value placed between the parentheses the method name concat is short for concatenation the concatenation of two strings is a new string that contains the symbols from the first string followed by the symbols from the second string note that the first string contains a space at the end thus when we concatenate the two strings we get the new string hello world which is assigned to the variable sentence as we showed earlier when a variable is declared it can be given an initial value this approach essentially combines an assignment with the declaration using this notation you can now write the body of main in as string hello string world string sentence concat system out println sentence you could even combine two initializations in a single statement string world sentence concat although as a general rule multiple complex initializations such as this should be placed on separate lines strings versus identifiers versus variables a string is a particular data value that a program can manipulate a variable is a place in the computer memory with an associated identifier the following example uses the identifier hello to refer to a variable of type string the identifier stringvary refers to a second variable of type string we first assign stringvary the value associated with the string variable hello later we reassign it the string value hello stringvsid java contrast strings and identifiers class stringvsid public static void main string args string hello hello world string stringvary stringvary hello system out println stringvary stringvary hello system out println stringvary the output of this program is the program demonstrates two important points first it shows the difference between the identifier hello which in fact refers to the string hello world and the string hello which is referred to at one point by the variable stringvary this example also shows that a variable can vary the variable stringvary first refers to the value hello world but later refers to the value hello in the programs presented so far we have generated output only by using system out println most programs input some data as well as generate output there are lots of ways to input data in java but the simplest is to use a class scanner provided in the package java util as shown in the following example area java reading from the keyboard import java util class area public static void main string args double width height area scanner scan new scanner system in system out println type two doubles for the width and height of a rectangle width scan nextdouble height scan nextdouble assert width height area width height system out print the area is system out println area here is a table of some of the methods used for getting input from the terminal using a scanner as you can see the basic data types have a corresponding nexttype method for reading and returning an input of that type if the input is of the wrong type these methods fail and throw an exception to avoid this we can use methods of the form hasnexttype that return true if the next input can be correctly interpreted an exam ple would be hasnextint that checks that the input can be interpreted as an int scanner method explanation examples nextint integers expected nextdouble doubles expected 01 next arbitrary strings java nextboolean logical values true false scanner attaches source of input new scanner system in a method is a group of instructions having a name in the programs introduced so far we ve defined a single method called main in addition we ve used some methods from other classes that were standard java classes the methods have names which makes it possible for you to request the computer to perform the instructions that comprise the method that what we re doing by using the expression system out println hello world we re calling a method with the name println and asking that the instruc tions be executed just as two people can have the same name two methods can have the same name we must therefore in general tell java where to look for the method using system out tells java that we re interested in the println method associated with the object identified by system out we must still postpone a full explanation of the meaning of system out as we have shown the same method can be called several times in one program in simpleinput we called the method println twice for many methods we need to provide some data values in order for the method to perform its job these values are provided to the method by placing them between parentheses following the name of the method these values are called the parameters of the method and we say that we are passing the parameters to the method the println method requires one parameter which is the value to be printed as we indicated previously this parameter can be either a string or a numeric value in the latter case the numeric value will be converted by the println method to a string and then printed if more than one parameter is required we separate the parameter values by commas for example the predefined method math min is used to determine the minimum of two numbers the following program fragment will print out int numberone numbertwo smallest smallest math min numberone numbertwo system out println smallest the method min is contained in the standard java class math which includes other common mathematical functions such as math sqrt that is used to find the square root of a number here are some of the predefined methods that we ve mentioned so far system out print x print the value of x system out println x print the value of x followed by a newline scan nextint get an int from the keyboard math min x y find the smaller of x and y math sqrt x find the square root of x concat concatenate the strings and word length find the length of the string word java includes a rich set of many more predefined methods they include methods for the most common mathematical functions see appendix b reference tables for creating graphical user interfaces see chap ter graphical user interfaces part i and chapter graphical user interfaces part ii for reading and writ ing information from and to files see chapter reading and writing files for communicating with programs on other computers using a network see chapter threads concurrent programming and many more an important aspect of java is the ability to use parts of programs created by others the print and println methods can print all the primitive types the method system out println is a convenient variant of system out print which appends a newline to the end of the output you can always achieve the same effect by adding n to the end of what is being printed for example the following two lines are equivalent system out print hello world n system out println hello world recall however that you can t put a an actual newline in the middle of a string so the following would not be legal system out println type two integers for the width and height of a box in the same way that you can combine two strings with the string concatenation operator you can also com bine one string and one value of any other type in this case the nonstring operand is first converted into a new string and then the two strings are concatenated this allows rewriting the printing of the result in area from section user input on page in the form system out println the area is area this version emphasizes that one message will appear on a single line of the output the double value area is first converted to a string and then combined with the other string using string concatenation what about the opposite what if you wanted to have an output message that spanned several output lines but with a single println you can do so by putting the symbols n in a string literal to represent new lines within the string such a string will print on more than one output line thus for system out println one nword nper nline the output is the pair of symbols n when used in a string literal mean to put a newline at this point in the string this escape sequence allows you to escape from the normal meaning of the symbols and n when used sepa rately you can find out more about escape sequences in section the char type on page care must be taken when you re using string concatenation to combine several numbers sometimes paren theses are necessary to be sure that the is interpreted as string concatenation not numeric addition for int x y system out println x y x y the output is x y because the string x y is first concatenated with the string and then is concatenated onto the end of the string x y to print x y you should use parentheses first to force the addition of x and y as integers and then concatenate the string representation of the result with the initial string as in system out println x y x y formatting output with printf the method printf was introduced in java it provides formatted output as is done in c the printf method is passed a list of arguments that can be thought of as and where is a string and may contain conversion specifications or formats a conversion specifi cation begins with a character and ends with a conversion character for example in the format the let ter is the conversion character printf abc the format causes the argument abc to be printed in the format of a string yet another way to do this is with the statement printf c c c a b c single quotes are used to designate character constants thus a is the character constant corresponding to the lowercase letter a the format c prints the value of an expression as a character notice that a con stant by itself is considered an expression printf conversion characters conversion character how the corresponding argument is printed c as a character d as a decimal integer e as a floating point number in scientific nota tion f as a floating point number g in the e format or f format whichever is shorter as a string when an argument is printed the place where it is printed is called its field and the number of char acters in its field is called its field width the field width and a precision specifier can be specified in a format as two integers separated by a dot occurring between the and the conversion character thus the statement printf airfare is 2f nparking is 2f n specifies that the two numbers should be printed in a field of characters with two digits the precision to the right of the decimal point it will print we will explain the various detailed conventions for using formats in chapter on page there are two basic representations for numbers in most modern programming languages integer represen tations and floating point representations the integer types are used to represent integers or whole num bers the floating point types are used to represent numbers that contain fractional parts or for numbers whose magnitude exceeds the capacity of the integer types the integer types a bit is the smallest unit of information that can be stored in a computer it can represent only two values such as or on off or true false a sequence of bits can be used to represent a larger range of values for the integer types the bits are interpreted with the binary number system the binary number system has only two digits and these two digits can be combined to represent any number in the same way that the digits of the normal decimal number system can be combined to form any number see appendix a get ting down to the bits some mechanism is needed to represent positive and negative numbers one simple solution would be to set aside one bit to represent the sign of the number however this results in two different representations of zero and which causes problems for computer arithmetic therefore an alternative system called two complement is used an bit value called a byte can represent different values java supports five integral numeric types the type char although normally used to represent symbolic characters in a bit format called unicode can be interpreted as an integer as discussed shortly the result of combining a char value with another numeric value will never be of type char the char value is first converted to one of the other numeric types these types are summarized in the following table type number of bits range of values byte to short to char to int to long to the asymmetry in the most negative value and the most positive value of the integer types results from the two complement system used in java to represent negative values literal integer values are represented by a sequence of digits possibly preceded by a minus sign an integer literal is either an int or long an explicit conversion called a cast of an int literal must be used to assign them to the smaller integer types short and byte see section type conversion on page integer literals can be expressed in base decimal base octal and base hexadecimal number systems decimal literals begin with a digit octal literals begin with the digit and hexadecimal literals begin with the two character sequence to specify a long literal instead of an int literal append the letter el uppercase or lowercase to the sequence of digits here are some examples the decimal value two hundred seventeen an octal number equivalent to in the decimal system would be illegal because is not a valid octal digit only are octal digits a hexadecimal number equivalent to in the decimal system the hexadecimal digits include plus a b c d e and f a long decimal literal without the l this would be an error because is too large to store in the bits of an int type integer the floating point types java supports two floating point numeric types a floating point number consists of three parts a sign a magnitude and an exponent these two types are summarized in the following table type number of bits approximate range of values approximate precision float to decimal digits double to 308 decimal digits to represent floating point literals you can simply insert a decimal point into a sequence of digits as in 14159 if the magnitude of the number is too large or too small to represent in this fashion then a nota tion analogous to scientific notation is used the letter e from exponent is used to indicate the exponent of as shown in the following examples java representation value 17e x x unless specified otherwise a floating point literal is of type double to specify a floating point literal of type float append either f or f to the literal for example 17e the char type java provides char variables to represent and manipulate characters this type is an integer type and can be mixed with the other integer types each char is stored in memory in bytes this size is large enough to store the integer values through as distinct character codes or nonnegative integers and these codes are called unicode unicode uses more storage per character than previous common character encodings because it was designed to represent all the world alphabets not just one particular alphabet for english a subset of these values represents actual printing characters these include the lowercase and uppercase letters digits punctuation and special characters such as and the character set also includes the white space characters blank tab and newline this important subset is represented by the first codes which are also known as the ascii codes earlier languages such as c and c worked only with this more limited set of codes and stored them in byte the following table illustrates the correspondence between some character literals and integer values char acter literals are written by placing a single character between single quotes as in a some character constants and their corresponding integer values character constants a b c z corresponding values 122 character constants a b c z corresponding values 90 character constants 9 corresponding values character constants corresponding values there is no particular relationship between the value of the character constant representing a digit and the digit intrinsic integer value that is the value of is not the property that the values for a b c and so on occur in order is important it makes the sorting of characters words and lines into lexicographi cal order convenient note that character literals are different from string literals which use double quotes as in hello string literals can be only one character long but they are still string values not char values for example a is a string literal some nonprinting and hard to print characters require an escape sequence the horizontal tab character for example is written as t in character constants and in strings even though it is being described by the two characters and t it represents a single character the backslash character is called the escape character and is used to escape the usual meaning of the character that follows it another way to write a character constant is by means of a hexadecimal digit escape sequence as in this is the alert character or the audible bell these hexadecimal digits are prefixed by the letter u to indicate their use as a unicode lit eral the unicode characters can be written in hexadecimal form from to uffff the following table contains some nonprinting and hard to print characters name of character escape int hex backslash backspace b u0008 carriage return r u000d double quote formfeed f u000c horizontal tab t 9 u0009 newline n u000a single quote null character alert the alert character is special it causes the bell to ring to hear the bell try executing a program that contains the line system out print character values are small integers and conversely small integer values can be characters consider the dec laration char c a the variable c can be printed either as a character or as an integer system out printf c c a is printed system out printf d c is printed 9 numbers versus strings the sequence of and inside a computer used to store the string value is different from the sequence of and used to store the int value which is different from the sequence of and used to store the double value the string form is more convenient for certain types of manipula tion such as printing on the screen or for combining with other strings the int form is better for some numeric calculations and the double form is better for others so how does the computer know how to inter pret a particular sequence of and the answer is look at the type of the variable used to store the value this answer is precisely why you must specify a type for each variable without the type information the sequence of and could be misinterpreted we do something similar with words all the time just as computers interpret sequences of and human beings interpret sequences of alphabetic symbols how people interpret those symbols depends on the con text in which the symbols appear for example at times the same word can be a noun and at other times it can be a verb also certain sequences of letters mean different things in different languages take the word pie for example what does it mean if it is english it is something good to eat if it is spanish it means foot from the context of the surrounding words we can usually figure out what the type of the word is verb or noun english or spanish some programming languages do something similar and figure out the type of a variable these programming languages are generally considered to be more error prone than languages such as java which require the programmer to specify the type of each variable languages such as java are called strongly typed languages the basic arithmetic operators in java are addition subtraction multiplication division and modulus you can use all arithmetic operators with all primitive numeric types char byte short int long float and double in addition you can combine any two numeric types by using these operators in what is known as mixed mode arithmetic although you can use the operators with any numeric type java actually does arithmetic only with the types int long float and double therefore the following rules are used first to convert both operands into one of four types if either operand is a double then the other is converted to double otherwise if either operand is a float then the other is converted to float otherwise if either operand is a long then the other is converted to a long otherwise both are converted to int this conversion is called binary numeric promotion and is also used with the binary relational operators dis cussed in section relational and equality operators on page when both operands are integer types the operations of addition subtraction and multiplication are self evident except when the result is too large to be represented by the type of the operands integer values can t represent fractions in java integer division truncates toward for example is and is a common mistake is to forget that integer division of nonzero values can result in to obtain fractional results you must force one of the operands to be a floating point type in expressions involving literals you can do so by adding a decimal point as in which results in the floating point value in addition integer division by zero will result in an error called an arithmeticexception an exception as the name implies is something unexpected java provides a way for you to tell the computer an integer arithmetic example change what to do when exceptions occur if you don t do anything and such an error occurs the program will print an appropriate error message and terminate we discuss exceptions in chapter coping with errors unlike some programming languages java doesn t generate an exception when integer arithmetic results in a value that is too large instead the extra bits of the true result are lost and in some cases pollute the bit used for the sign for example adding two very large positive numbers could generate a negative result likewise subtracting a very large positive number from a negative number could generate a positive result if values are expected to be near the limit for a particular type you should either use a larger type or check the result to determine whether such an overflow has occurred when one of the operands is a floating point type but both operands are not of the same type one of them is converted to the other as described earlier unlike some programming languages floating point arithmetic operations in java will never generate an exception instead three special values can result positive infinity negative infinity and not a number see appendix a getting down to the bits for more details the modulus operator returns the remainder from integer division for example is because divided by is with a remainder of the modulus operator is mostly used with integer operands how ever in java it can be used with floating point operands for floating point values x y is n where n is the largest integer such that y n is less than or equal to x an integer arithmetic example change the computation in section implementing our algorithm in java on page whereby we made change for a dollar is a perfect illustration of the use of the two integer division operators and makechange java change in dimes and pennies import java util class makechange public static void main string args int price change dimes pennies scanner scan new scanner system in system out println type price to price scan nextint change price how much change dimes change number of dimes pennies change number of pennies system out print the change is system out println dimes dimes pennies pennies type conversion you may want or need to convert from one primitive numeric type to another as mentioned in the preceding section java will sometimes automatically convert the operands of a numeric operator these automatic con versions are also called widening primitive conversions these conversions always convert to a type that requires at least as many bits as the type being converted hence the term widening in most but not all cases a widening primitive conversion doesn t result in any loss of information an example of a widening primi tive conversion that can lose some precision is the conversion of the int value to a float value which results in to understand this loss of information see appendix a getting down to the bits the following are the possible widening primitive conversions from to byte short int long float or double short int long float or double char int long float or double int long float or double long float or double float double in addition to performing widening conversions automatically as part of mixed mode arithmetic widening primitive conversions are also used to convert automatically the right hand side of an assignment operator to the type of the variable on the left for example the following assignment will automatically convert the integer result to a floating point value int x y float z z x y automatic widening from int to float a narrowing primitive conversion is a conversion between primitive numeric types that may result in signifi cant information loss the following are narrowing primitive conversions type conversion from to byte char short byte or char char byte or short int byte short or char long byte short char or int float byte short char int or long double byte short char int long or float narrowing primitive conversions generally result only from an explicit type conversion called a cast a cast is written as type expression where the expression to be converted is preceded by the new type in parenthe ses a cast is an operator and as the table in section 13 precedence and associativity of operators on page indicates has higher precedence than the five basic arithmetic operators for example if you are inter ested only in the integer portion of the floating point variable somefloat then you can store it in some integer as in int someinteger float somefloat 14159 someinteger int somefloat system out println someinteger the output is if the cast is between two integral types the most significant bits are simply discarded in order to fit the resulting format this discarding can cause the result to have a different sign from the original value the fol lowing example shows how a narrowing conversion can cause a change of sign int i j byte iasbyte byte i jasbyte byte j system out println iasbyte system out println jasbyte the output is the largest positive value that can be stored in a byte is attempting to force a narrowing conversion on a value greater than will result in the loss of significant information in this case the sign is reversed to understand exactly what happens in this example see appendix a to change the value of a variable we have already made use of assignment statements such as a b c assignment is an operator and its precedence is lower than all the operators we ve discussed so far the associativity for the assignment operator is right to left in this section we explain in detail its significance to understand as an operator let first consider for the sake of comparison the binary operator takes two operands as in the expression a b the value of the expression is the sum of the values of a and b by comparison a simple assignment expression is of the form variable righthandside where righthandside is itself an expression a semicolon placed at the end would make this an assignment statement the assignment operator has the two operands variable and righthandside the value of right handside is assigned to variable and that value becomes the value of the assignment expression as a whole to illustrate let consider the statements b c a b c where the variables are all of type int by making use of assignment expressions we can condense these statements to a b c the assignment expression b assigns the value to the variable b and the assignment expression itself takes on this value similarly the assignment expression c assigns the value to the variable c and the assignment expression itself takes on this value finally the values of the two assignment expressions are added and the resulting value is assigned to a although this example is artificial in many situations assignment occurs naturally as part of an expression a frequently occurring situation is multiple assignment consider the statement assignment operators a b c because the operator associates from right to left an equivalent statement is a b c first c is assigned the value and the expression c has value then b is assigned the value and the expression b c has value finally a is assigned the value and the expression a b c has value in addition to there are other assignment operators such as and an expression such as k k will add to the old value of k and assign the result to k and the expression as a whole will have that value the expression k accomplishes the same task the following list contains all the assignment operators assignment operators all these operators have the same precedence and all have right to left associativity the meaning is speci fied by variable op expression which is equivalent to variable variable op expression with the exception that if variable is itself an expression it is evaluated only once note carefully that an assignment expression such as j k is equivalent to j j k rather than j j k the following table illustrates how assignment expressions are evaluated declarations and initializations int i j k m expression equivalent expression equivalent expression value i j k j k m i j k j k m i i j k j j k m 12 the increment and decrement operators computers are very good at counting as a result many programs involve having an integer variable that takes on the values one way to add to a variable is i i which changes the value stored in the variable i to be more than it was before this statement was executed this procedure is called incrementing a variable because it is so common java like its predecessor c includes a shorthand notation for incrementing a variable the following statement gives the identical result i the operator is known as the increment operator similarly there is a decrement operator so that the following two statements are equivalent i i i here is a simple program that demonstrates the increment operator increment java demonstrate incrementing class increment public static void main string args int i system out println i i i i system out println i i i system out println i i i system out println i i the output of this program is note that both increment and decrement operators are placed after the variable to be incremented when placed after its argument they are called the postfix increment and postfix decrement operators these opera tors also can be used before the variable they are then called the prefix increment and prefix decrement operators each of the expressions i prefix and i postfix has a value moreover each causes the stored value of i in memory to be incremented by the expression i causes the stored value of i to be incre mented first with the expression then taking as its value the new stored value of i in contrast the expres sion i has as its value the current value of i then the expression causes the stored value of i to be incremented the following code illustrates the situation 13 precedence and associativity of operators int a b c a c b c system out println a a a is printed system out println b b b is printed system out println c c c is printed similarly i causes the stored value of i in memory to be decremented by first with the expression then taking this new stored value as its value with i the value of the expression is the current value of i then the expression causes the stored value of i in memory to be decremented by note that and cause the value of a variable in memory to be changed other operators do not do so for example an expression such as a b leaves the values of the variables a and b unchanged these ideas are expressed by saying that the operators and have a side effect not only do these operators yield a value but they also change the stored value of a variable in memory of course this is also true of the assignment operator in some cases we can use in either prefix or postfix position with both uses producing equivalent results for example each of the two statements i and i is equivalent to i i in simple situations you can consider and as operators that provide concise notation for the incre menting and decrementing of a variable in other situations you must pay careful attention as to whether prefix or postfix position is used 13 precedence and associativity of operators when evaluating expressions with several operators you need to understand the order of evaluation of each operator and its arguments operator precedence gives a hierarchy that helps determine the order in which operations are evaluated for example precedence determines which arithmetic operations are evaluated first in x b math sqrt b b a c a which you may recognize as the expression to compute one of the roots of a quadratic equation written like this in your mathematics class x to take a simpler example what is the value of integer variable x after the assignment x the answer is not which is the reason is that multiplication has higher precedence than addi tion therefore is evaluated before is added to the result in addition to some operators having higher precedence than others java specifies the associativity or the order in which operators of equal precedence are to be evaluated for example the value of is not this is because and have equal precedence and arithmetic operators of equal precedence are evaluated from left to right if you wanted to do the multiplication before the division you would write the expression as parentheses can be used to override the normal operator precedence rules this left to right ordering is important in some cases that might not appear obvious consider the expression x y z from simple algebra the associativity of addition tells us that x y z is the same as x y z unfortunately this is true only when you have numbers with infinite precision suppose that y is the largest positive integer that java can represent x is and z is evaluating y z first will result in integer overflow which in java will be equivalent to some very large negative number clearly not the expected result if instead x y is evaluated first then adding z will result in an integer that is still in range and the result will be the correct value the precedence and associativity of all java operators is given in appendix appendix b reference tables the following table gives the rules of precedence and associativity for the operators of java that we have used so far operator precedence and associativity operator associativity postfix postfix left to right unary unary prefix prefix right to left new type expr right to left left to right left to right etc right to left all the operators on the same line such as and have equal precedence with respect to each other but have higher precedence than all the operators that occur on the lines below them the associativity rule for all the operators on each line appears in the right hand column in addition to the binary which represents addition there is a unary and both operators are represented by a plus sign the minus sign also has binary and unary meanings the following table gives some additional examples of precedence and associativity of operators declarations and initializations int a b c d expression equivalent expression value a b c a b c a b c a b c a b c a b c b d b d a clear consistent style is important to writing good code we use a style that is largely adapted from the java professional programming community having a style that is readily understandable by the rest of the programming community is important we ve already mentioned the importance of comments for documenting a program anything that aids in explaining what is otherwise not clear in the program should be placed in a comment comments help the programmer keep track of decisions made while writing the code without good documentation you may return to some code you have written only to discover that you have forgotten why you did some particular thing the documentation should enable someone other than the original programmer to pick up use and modify the code all but the most trivial methods should have comments at the beginning clearly stating the purpose of the method also complicated blocks of statements should be preceded by comments summariz ing the function of the block comments should add to the clarity of the code not simply restate the pro gram statement by statement here is an example of a useless comment area width height compute the area good documentation includes proper choice of identifiers identifiers should have meaningful names cer tain simple one character names are used to indicate auxiliary variables such as i j or k as integer vari ables the code should be easy to read visibility is enhanced by the use of white space in general we present only one statement to a line and in all expressions separate operators from arguments by a space as we progress to more complex programs we shall present by example or explicit mention accepted layout rules for pro gram elements some naming conventions used by many java programmers class names start with uppercase and embedded words as in helloworld are capitalized methods and variables start with lowercase and embedded words as in readint data tostring and loopindex are capitalized although legal the dollar sign should not be used except in machine generated java pro grams summary to create a java program first define a class give the class a method called main put whatever instructions you want the computer to execute inside the body of the method main a program stores data in variables each variable is given a type such as int for storing integers or string for storing strings you can use literals to embed constant values of various types in a program such as in the constant string hello world or the integer constant you can combine literals and variables in expressions by using operators such as for addition or string concatenation and for numeric multiplication you can store the result of evaluating an expression in a variable by using the assignment operator the variable is always on the left and the expression being assigned to the variable is always on the right you can call a method from another class by writing the name of the class followed by a dot and the name of the method for example math sqrt the lexical elements of a java program are keywords identifiers literals operator symbols punctuation comments and white space you can print strings and numbers to the screen by using the method system out print or sys tem out println the latter appends a newline to whatever is printed these methods are part of the standard java classes java introduced printf which allows formatted printing as derived from the c language you can input from the keyboard by using the methods of class scanner these include next for strings nextint for integers and nextdouble for doubles java supports the primitive integer types char byte short int and long it also supports two floating point types float and double integer division truncates toward zero it doesn t round to the nearest whole number you can use math round if rounding is what you want review questions what line appears in every complete java program indicating where to begin executing the program what one line instruction would you use to have a java program print goodbye what affect do strings such as what is this have on the execution of a java program what is a variable what is a method complete the following table text legal id why or why not no digit is first character a b main count class what does the symbol do in java programmers say a method when they mean go and execute the instructions for the method 9 true or false a multiline comment can be placed anywhere white space could be placed true or false keywords can also be used as variables but then the special meaning of the keyword is overridden what convention for identifiers given in this chapter is used in whatami howaboutthis somename 12 what primitive types are used to store whole numbers 13 what is the difference between x a and x a what is the difference between hello and hello which version of the java program helloworld is the one you can view and edit with a text editor hel loworld java or helloworld class what does the other one contain what program created the one you do not edit what is unicode list the primitive types in java before it can be used every variable must be declared and given a what is the value of the java expression don t ignore the quotation marks they are crucial write a java statement that could be used to read an integer value from the keyboard and store it in the variable somenumber what is wrong with the following java statement system out println this statement is supposed to print a message what is wrong every group of input statements should be preceded by what how do you write x times y in java what is the difference between system out print message and system out println mes sage write a single java statement that will produce the following output approximately what is the largest value that can be stored in the primitive type int one thousand one million one billion one trillion even larger what primitive java type can store the largest numbers what is the value of the following java expressions exercises write a java program that prints hello your name you can do this by a simple modification to the hel loworld program compile and run this program on your computer write a java program that prints a favorite poem of at least eight lines be sure to print it out neatly aligned at the end of the poem print two blank lines and then the author name design your own signature logo such as a sailboat icon if you like sailing and print it followed by yours truly your name a sailboat signature logo might look like yours truly bruce mcpohl write a java program to read in two numbers and print the sum be sure to include a message to prompt the user for input and a message identifying the output see what happens if you type in something that is not a number when the program is run see how large a number you can type in and still have the pro gram work correctly the following code contains three syntax errors and produces two syntax error messages from javac fix the problems java fixing syntax errors class public static void main string args system out println hello world the javac compiler message reads the following code produces one syntax error message from javac fix the problem java more syntax errors class public static void main string args int count i system out println count i count i the javac compiler message reads here unlike the previous exercise the compiler doesn t as clearly point to the errors frequently errors in punctuation lead to syntax error messages that are hard to decipher after you fix the first syntax error in the code a second error will be identified continue with the code class if you fixed just the syntax errors you may get a running program that still has a run time bug namely the output may not be the sum of count i fixing run time or semantic bugs is harder than fixing syntax bugs because something is wrong with your understanding of how to program the solution without introducing any other variables fix the run time bug write a program that draws a box like the one shown using a single println statement 9 use scan nextdouble to read in one double precision floating point number and then print the results of calling math sin math cos math asin math exp math log math floor and math round with the input value as a parameter be sure to prompt the user for input and label the output write a program to read two double precision floating point numbers using nextdouble print the sum difference product and quotient of the two numbers try two very small numbers two very large numbers and one very small number with one very large number you can use the same notation used for literals to enter the numbers for example is a very small number write a program to compute the area of a circle given its radius let radius be a variable of type double and use nextdouble to read in its value be sure that the output is understandable the java class math contains definitions for the constants e and pi so you can use math pi in your program 12 extend the previous program to write out the circumference of a circle and the volume of a sphere given the radius as input recall that the volume of a sphere is r3 v 13 write a program that asks for a double and then prints it out then ask for a second double this time printing out the sum and average of the two doubles then ask for a third double and again print out the accumulated sum and the average of the three doubles use variables and sum later when we discuss loops you will see how this is easily done for an arbitrary number of input val ues write a program that reads in an integer and prints it as a character remember that character codes can be nonprinting write a program that asks for the number of quarters dimes nickels and pennies you have then com pute the total value of your change and print the number of dollars and the remaining cents write a program capable of converting one currency to another for example given u s dollars it should print out the equivalent number of euros look up the exchange rate and use it as input change the makechange program to use variables that are doubles run the program and see what goes wrong class makechange public static void main string args double price change dimes pennies the following is a c program for printing hello world hello world in c purpose the classic hello world program it simply prints a message to the screen author jane programmer as derived from kernighan and richie include stdio h needed for io int main void printf hello world n return unneeded in java note how similar this program is to the java version a key difference is the lack of class encapsulation of main as in java main starts the program execution in c methods are known as functions the printf function is found in the standard input output library imported by the c compiler for use in this program the return ends program execution and is not used in java convert the following c program to java include stdio h int main void printf hello world n printf my name is george n printf yada yada yada n return applet exercise the following program is an example of a java applet this program uses several features of java that we explain later note that there is no method main instead there is the method paint for now just con centrate on the body of the method paint treating the surrounding code as a template to be copied verba tim by invoking the appropriate drawing operations on the graphics object g you can draw on the applet to place this applet in a web page add the following two lines to the html document for the page applet code firstapplet class width height applet awt and swing together comprise the collection of classes used for building graphical java programs import java awt required for programs that draw import javax swing required for swing applets public class firstapplet extends japplet public void paint graphics g draw a line from the upper left corner to pixels below the top center of the applet g drawline draw a line from the end of the previous line up to the top center of the applet g drawline draw an oval inscribed in an invisible rectangle with its upper left corner at the intersection of the two lines drawn above g drawoval 100 the class graphics is used for simple drawing and many drawing operations are defined for it in this exam ple we use the method drawline to draw a line the first two numbers in parentheses for the drawline operation are the xy coordinates of one end of the line and the last two numbers are the xy coordinates of the other end as you can see from the output of the program the location is in the upper left corner with increasing x moving to the right and increasing y moving down to draw an oval give the coordinates of the upper left corner and the width and height on an invisible rectangle the oval will be inscribed inside the rectangle to execute an applet first compile it like you do other java programs then you can either run the program appletviewer or use a web browser to view the applet to view the applet firstapplet using the applet viewer on unix and windows machines type the following at a command line prompt appletviewer firstapplet java notice that we are passing firstapplet java not firstapplet or firstapplet class to the applet viewer this procedure is different from running regular java programs in fact appletviewer just looks in the text file passed to it for an applet element an applet element begins with applet and ends with applet any text file containing the applet element shown in the opening comment for firstapplet java would work just as well to view the applet in a web browser create a file for example firstapplet html put the applet tag in the html file put the html file in the same directory as your applet and then open the html file with a web browser the applet looks like the following when run with an appletviewer modify this applet to draw a simple picture look up the documentation for graphics on the web at http java sun com docs api java awt graphics html and use at least one method operation of graphics not used in firstapplet the program examples presented until now have executed from top to bottom without making any decisions in this chapter we have programs select among two or more alternatives we also demonstrate how to write programs that repeatedly execute the same sequence of instructions both instructions to computers and instructions in everyday life are filled with conditional and iterative statements a conditional instruction for your microwave oven might say if you wish to defrost press the defrost button otherwise press the full power button an iterative instruction for baking a loaf of bread might say let the dough rise in a warm place until it has doubled in size conditional and iterative statements are controlled by boolean expres sions a boolean expression is either true or false if it is raining wear your raincoat is an instruction given by many parents and is followed if it is true that it is raining in java expressions that evaluate as true or false are of type boolean to direct the flow of control properly you need to learn how to write boolean expressions expression block and empty statements java has many kinds of statements most of the statements that we have shown have specified the evaluation of an expression we ll soon look at statements that select between two alternatives and statements that repeat many times before doing that we need to look more closely at the statements that we have been using the normal flow of instructions in java is to execute the statements of the program in sequential order from top to bottom all the statements used so far have been either variable declaration statements or expression statements variable declaration statements begin with a type such as int or string and end with a semicolon as in int width height area string hello hello world double size x the first declares three variables of type int the second declares one variable of type string and initializes it the third declares and initializes the variable size but not the variable x declaration statements start with a type and are followed by one or more variables separated by commas the variables may be initialized by using the equals sign followed typically by a literal in java all variables need to be declared expression statements are formed by adding a semicolon to the end of an expression expressions are basic to performing computations not all expressions are valid in expression statements the two types of expres sions used so far that are valid in expression statements are assignment expressions and method call expres sions an assignment expression is any expression involving the assignment operator a method call expression does not involve an assignment operator the following are examples of expression statements area width height simple assignment statement system out println method call expression a statement used for grouping a number of statements is a block a block is a sequence of one or more state ments enclosed by braces a block is itself a statement a simple example is x y x system out println y system out println x statements inside a block can also be blocks the inside block is called an inner block which is nested in the outer block an example is outer block x inner block y 2 system out println y end of inner block system out println x this example merely demonstrates the syntax of a block we wouldn t normally put a block inside another block for no reason most nested blocks involve declaration statements that create local variables a simple example of a block with declarations is int i j i is created in this block j is from elsewhere end of block i disappears in this example the int variable i is created when this block is executed when this block is started i is placed in memory with its initial value calculated as plus the value of j when the block is exited the vari able disappears blocks are not terminated by semicolons rather they are terminated by a closing brace also called the right brace recall that the semicolon when used is part of the statement not something added to the statement for example the semicolon turns an expression into a statement understanding this will make it much eas ier for you to create syntactically correct programs with the new statement types that we introduce in this chapter empty statement the simplest statement is the empty statement or null statement it is just a semicolon all by itself and results in no action a semicolon placed after a block is an empty statement and is irrelevant to the pro gram actions the following code fragment produces exactly the same result as the nested block example in the preceding section the string of semicolons simply create seven empty statements following the inner block 2 boolean expressions x y 2 system out println y system out println x a boolean expression is any expression that evaluates to either true or false java includes a primitive type boolean the two simplest boolean expressions are the boolean literals true and false in addition to these two literals boolean values result from expressions involving either relational operators for comparing num bers or logical operators that act on boolean values 2 relational and equality operators all conditional statements require some boolean expression to decide which execution path to follow java uses four relational operators less than greater than less than or equal and greater than or equal java also contains two equality operators equal and not equal they can be used between any two numeric values the equality operators may also be used when comparing nonnumeric types they are listed in the following table operator name example less than is true greater than is false equal is false less than or equal 10 is true greater than or equal 10 is true not equal 10 is true the relational operators can be used in assignment to boolean variables as in int i j boolean flag flag flag is now true flag i j flag is now false flag j 2 flag is now true 2 2 logical operators once you have a boolean value either stored in a variable representing a primitive boolean value for exam ple boolean done false or as the result of an expression involving a relational operator for example x y you can combine these boolean values by using the logical operators java provides three logical operators and or and not the meaning of these operators is given in the following table operator name description example assume x is 10 and y is and the expression x y is true if both x and y are true and false otherwise x y is true or the expression x y is true if either x or y or both is true and false otherwise x y is true not the expression x is true if x is false and false otherwise x 20 is false for example if you wanted to determine whether a person in a database was an adult but not a senior citi zen you could check if their age was greater than or equal to and their age was less than the follow ing java code fragment will print out full fare adult is true if this condition is met otherwise it prints full fare adult is false boolean b ageofperson ageofperson system out println full fare adult is b for an example of the use of or consider the opposite situation as above where you wanted to find out if a reduced fair was appropriate you might write b ageofperson ageofperson system out println reduced fare is b the logical operators and use short circuit evaluation in the preceding example of a logical and expression if the ageofperson were 10 then the test for ageofperson would be omitted used partly for efficiency reasons this approach is helpful when the second part of such an expression could lead to an undesirable result such as program termination as with other operators the relational equality and logical operators have rules of precedence and associa tivity that determine precisely how expressions involving these operators are evaluated as shown in the fol lowing table t operator precedence and associativity operators associativity postfix postfix left to right unary unary prefix prefix right to left left to right left to right left to right left to right left to right left to right etc right to left note that with the exception of the boolean unary operator negation the relational boolean and equality operators have lower precedence than the arithmetic operators only the assignment operators have lower precedence the if statement computers make decisions by evaluating expressions and executing different statements based on the value of the expression the simplest type of decision is one that can have only two possible outcomes such as go left versus go right or continue versus stop in java we use boolean expressions to control decisions that have two possible outcomes the if statement is a conditional statement an if statement has the general form if booleanexpr statement if the expression booleanexpr is true then the statement statement is executed otherwise statement is skipped statement is called the then statement in some programming languages but not java then is used to signal the then statement after the if statement has been executed control passes to the next statement the flow of execution can skip around statement as shown in the following diagram execution enters if statement continue with rest of program note the absence of semicolons in the general form of the if statement recall that the semicolon when required is part of the statement and is not used to separate statements as in if temperature system out println warning below freezing system out println it temperature degrees the message warning below freezing is printed only when the temperature is less than the sec ond print statement is always executed this example has a semicolon at the end of the if statement because the statement inside the if statement is an expression statement that ends with a semicolon when the statement inside an if statement is a block you get if statements that look like if temperature system out println warning warning warning system out println warning below freezing system out println warning warning warning here you can see the importance of the block as a means of grouping statements in this example what oth erwise would be three separate statements are grouped and all are executed when the boolean expression is true the formatting shown of the if statement with a block as the statement aligns vertically with the braces of the block statement an alternative formatting and the one that we use places the opening brace on the same line as the keyword if and then aligns the closing brace with the keyword as shown here if temperature system out println warning warning warning system out println warning below freezing system out println warning warning warning at the end of this chapter we discuss further which style to choose problem solving with the if statement a different number is initially placed in each of three boxes labeled a b and c respectively the problem is to rearrange or sort the numbers so that the final number in box a is less than that in box b and that the number in box b is less than that in box c initial and final states for a particular set of numbers are as fol lows before after a a b b c c pseudocode for performing this sorting task involves the following steps pseudocode for three number sort place the first number in box a 2 place the second number in box b place the third number in box c if the number in a is not larger than the number in b go to step interchange the number in a with that in b if the number in b is larger than the number in c then go to step otherwise halt 7 interchange the numbers in b and c if the number in a is larger than that in b then go to step 9 otherwise halt 9 interchange the numbers in a and b 10 halt let execute this pseudocode with the three specific numbers previously given and in that order we always start with the first instruction the contents of the three boxes at various stages of execution are shown in the following table box step step 2 step step 5 step 7 a b 6 c problem solving with the if statement to execute step we place the first number in box a similarly at the end of instruction the 6 has been inserted into box b and box c contains the 11 as is larger than 6 the condition tested in step 4 is false and we proceed to instruction 5 this step switches the values into boxes a and b so that box a now con tains the 6 and box b has the step 6 has now been reached and we compare the number in box b to that in box c 11 is greater than 11 so a transfer is made to step 7 the numbers in boxes b and c are then interchanged so that box b has the 11 and box c has the the test in step 8 fails 6 is not larger than 11 and the computation then halts the three numbers have been sorted in ascending sequence i e 6 11 17 you should convince yourself by bench testing this algorithm with other values of a b and c that the computation described by the pseudocode will work correctly for any three numbers a flowchart of the sort ing algorithm is shown in the following diagram note that we decomposed the operation of interchanging two numbers into three more primitive instruc tions box t is used as temporary storage to hold intermediate results in order to interchange or switch the two numbers a and b we first temporarily store one of the numbers say a in t t f a next the other num ber is stored in a a f b and last the first number is placed in b b f t note that the instruction sequence a f b b f a will not interchange a and b because the first instruction effectively destroys the old value in a in computer terms the labeled boxes are analogous to memory or storage areas that can contain values next we code in java the pseudocode version of our sorting program sortinput java sort three numbers import java util class sortinput public static void main string args int a b c t scanner scan new scanner system in system out println type three integers a scan nextint b scan nextint c scan nextint if a b t a a b b t if b c t b b c c t if a b t a a b b t system out print the sorted order is system out println a b c dissection of the sortinput program int a b c t this program declares four integer variables the variables a b and c are inputs to be sorted and t is to be used for temporary purposes as described in the pseudocode system out println type three integers this line is used to prompt the user to type the three numbers to be sorted whenever a program is expecting the user to do something it should print out a prompt telling the user what to do a scan nextint b scan nextint c scan nextint the method call expression scan nextint is used to obtain the input from the keyboard three sepa rate integers need to be typed the values read will be stored in the three variables if a b t a a b b t if b c t b b c c t if a b t a a b b t the if statements and resulting assignments are java notation for the same actions described in the sort flow chart to comprehend these actions you need to understand why the interchange or swapping of values between two variables such as a and b requires the use of the temporary t also note how the three assignments are grouped as a block allowing each if expression to control a group of actions 4 the if else statement closely related to the if statement is the if else statement an if else statement has the following gen eral form if booleanexpr else if the expression booleanexpr is true then is executed and is skipped if booleanexpr is false then is skipped and is executed is called the else statement after the if else statement has been executed control passes to the next statement the flow of execution branches and then rejoins as shown in the following diagram execution enters if else statement continue with rest of program consider the following code if x y min x else min y system out println min min if x y is true then min will be assigned the value of x if it is false then min will be assigned the value of y after the if else statement is executed min is printed as with the if statement either branch of an if else statement can contain a block as shown in the follow ing example if temperature system out println warning warning warning system out println temperature degrees below freezing system out println warning warning warning else system out println it temperature degrees fahrenheit 4 1 nested if else statements 3 4 1 nested if else statements the if statement is a full fledged statement and can be used anywhere a statement is expected so you could put another if statement in either branch of an if statement for example you could rewrite the earlier air fare checking statement as if ageofperson if ageofperson system out println full fare adult if the expression ageofperson is true then the statement if ageofperson system out println full fare adult is evaluated the true branch for the ageofperson if statement is itself an if statement in general if the true branch of an if statement contains only another if statement writing it as a single if statement is more efficient combining the expressions with the operator as we did in section 3 2 2 logical opera tors on page using in most cases is also clearer these nested if statements can t always be collapsed consider the following variation on our earlier exam ple involving temperature if temperature system out println warning warning warning if temperature system out println temperature degrees below zero else system out println temperature f below freezing system out println warning warning warning else system out println it is temperature degrees fahrenheit the then statement for the outer if else statement is a block containing another if else statement 3 4 2 if else if else if in the preceding example the nesting was all done in the then statement part of the if else statement now we nest in the else statement part of the if else statement if ageofperson system out println child fare else if ageofperson system out println adult fare else system out println senior fare the braces are not needed we added them only for clarity this form is so common that experienced pro grammers usually drop the braces in addition the else and the following if are usually placed on the same line as in if ageofperson system out println child fare else if ageofperson system out println adult fare else system out println senior fare note that the second if statement is a single statement that constitutes the else branch of the first state ment the two forms presented are equivalent you should be sure that you understand why they are if you need to look back at the general form of the if statement and recall that an entire if else statement is a statement itself this fact is illustrated in the following figure wherein each statement is surrounded by a box as you can see there are five different statements else if ageofperson system out println adult fare else system out println senior fare sometimes this chain of if else if else if else can get rather long tedious and inefficient for this reason a special construct can be used to deal with this situation when the condition being tested is of the right form if you want to do different things based on distinct values of a single integer expression then you can use the switch statement which we discuss in section 3 9 the switch statement on page 3 4 3 the dangling else problem when you use sequences of nested if else statements a potential problem can arise as to what if an else goes with as for example in if if else the indenting suggests that is executed whenever is false however that isn t the case rather is executed only when is true and is false the proper indenting is if if else the rule used in java is that an else is always matched with the nearest preceding if that doesn t have an else to cause to be executed whenever is false as suggested by the first indenting example you can use braces to group the statements as shown if if else the braces are like parentheses in arithmetic expressions that are used to override the normal precedence rules the nested or inner if statement is now inside a block that prevents it from being matched with the else we have added the ability to choose among alternatives but our programs still execute each instruction at most once and progress from the top to the bottom the while statement allows us to write programs that run repeatedly the general form of a while statement in java is while booleanexpr statement statement is executed repeatedly as long as the expression booleanexpr is true as shown in the following flowchart execution enters while statement continue with rest of program this flowchart is the same as the one for the if statement with the addition of an arrow returning to the test box note that statement may not be executed that will occur when booleanexpr evaluates to false the first time note also that like the description of the if else statement there are no semicolons in the general form of the while statement the semicolon if any would be part of the statement that follows the parenthesized boolean expression a simple example for use on valentine day is valentine java a simple while loop class valentine public static void main string args int howmuch while howmuch 5 system out println i love you the output is when the body of the loop is a block you get while booleanexpr statement2 modifying our earlier example gives int howmuch while howmuch 5 system out print i love you system out println my rose which results in the output most while statements are preceded by some initialization statements our example initialized the loop counting variable howmuch 3 5 1 problem solving with loops suppose that you wanted to have a program that could read in an arbitrary number of nonzero values and compute the average if you were to use pseudocode the program might look like the following pseudocode for average using goto 1 get a number 2 if the number is 0 go to step 6 3 add the number to the running total 4 increment the count of numbers read in 5 go back to step 1 6 divide the running total by the count of numbers in order to get the average 7 print the average in this pseudocode step 5 goes back to the beginning of the instruction sequence forming a loop in the and many programming languages used a goto statement to implement step 5 for coding such a loop these goto statements resulted in programs that were hard to follow because they could jump all over the place their use was sometimes called spaghetti code in java there is no goto statement java includes goto as a keyword that has no use so that the compiler can issue an error message if it is used inadvertently by a programmer familiar with c or c today structured control constructs can do all the good things but none of the bad things that you could do with goto statements a loop is a sequence of statements that are to be repeated possibly many times the preceding pseudocode has a loop that begins at step 1 and ends at step 5 modern programs require that you use a construct to indi cate explicitly the beginning and the end of the loop the structured equivalent still in pseudocode might look like the following pseudocode with loop pseudocode for average without using goto get a number while the number is not 0 do the following add the number to the running total increment the count of numbers read in get a number when the loop exits divide the running total by the count of numbers to get the average print the average the loop initialization in this case is reading in the first number somewhere within a loop typically at the end is something that prepares the next iteration of the loop getting a new number in our example so although not part of the required syntax while statements generally look like the following statementinit initialization for the loop while booleanexpr statement2 statementnext prepare for next iteration the while statement is most often used when the number of iterations isn t known in advance this situa tion might occur if the program was supposed to read in values processing them in some way until a special value called a sentinel was read in such is the case for our compute the average program it reads in num bers until the sentinel value 0 is read in we can now directly translate into java the pseudocode for computing an average 