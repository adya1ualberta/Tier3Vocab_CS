the problem of searching for patterns in data is a fundamental one and has a long and successful history for instance the extensive astronomical observations of tycho brahe in the century allowed johannes kepler to discover the empirical laws of planetary motion which in turn provided a springboard for the development of clas sical mechanics similarly the discovery of regularities in atomic spectra played a key role in the development and veriﬁcation of quantum physics in the early twenti eth century the ﬁeld of pattern recognition is concerned with the automatic discov ery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories consider the example of recognizing handwritten digits illustrated in figure each digit corresponds to a pixel image and so can be represented by a vector x comprising real numbers the goal is to build a machine that will take such a vector x as input and that will produce the identity of the digit as the output this is a nontrivial problem due to the wide variability of handwriting it could be figure examples of hand written dig its taken from us zip codes tackled using handcrafted rules or heuristics for distinguishing the digits based on the shapes of the strokes but in practice such an approach leads to a proliferation of rules and of exceptions to the rules and so on and invariably gives poor results far better results can be obtained by adopting a machine learning approach in which a large set of n digits xn called a training set is used to tune the parameters of an adaptive model the categories of the digits in the training set are known in advance typically by inspecting them individually and hand labelling them we can express the category of a digit using target vector t which represents the identity of the corresponding digit suitable techniques for representing cate gories in terms of vectors will be discussed later note that there is one such target vector t for each digit image x the result of running the machine learning algorithm can be expressed as a function y x which takes a new digit image x as input and that generates an output vector y encoded in the same way as the target vectors the precise form of the function y x is determined during the training phase also known as the learning phase on the basis of the training data once the model is trained it can then de termine the identity of new digit images which are said to comprise a test set the ability to categorize correctly new examples that differ from those used for train ing is known as generalization in practical applications the variability of the input vectors will be such that the training data can comprise only a tiny fraction of all possible input vectors and so generalization is a central goal in pattern recognition for most practical applications the original input variables are typically prepro cessed to transform them into some new space of variables where it is hoped the pattern recognition problem will be easier to solve for instance in the digit recogni tion problem the images of the digits are typically translated and scaled so that each digit is contained within a box of a ﬁxed size this greatly reduces the variability within each digit class because the location and scale of all the digits are now the same which makes it much easier for a subsequent pattern recognition algorithm to distinguish between the different classes this pre processing stage is sometimes also called feature extraction note that new test data must be pre processed using the same steps as the training data pre processing might also be performed in order to speed up computation for example if the goal is real time face detection in a high resolution video stream the computer must handle huge numbers of pixels per second and presenting these directly to a complex pattern recognition algorithm may be computationally infeasi ble instead the aim is to ﬁnd useful features that are fast to compute and yet that introduction also preserve useful discriminatory information enabling faces to be distinguished from non faces these features are then used as the inputs to the pattern recognition algorithm for instance the average value of the image intensity over a rectangular subregion can be evaluated extremely efﬁciently viola and jones and a set of such features can prove very effective in fast face detection because the number of such features is smaller than the number of pixels this kind of pre processing repre sents a form of dimensionality reduction care must be taken during pre processing because often information is discarded and if this information is important to the solution of the problem then the overall accuracy of the system can suffer applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as supervised learning prob lems cases such as the digit recognition example in which the aim is to assign each input vector to one of a ﬁnite number of discrete categories are called classiﬁcation problems if the desired output consists of one or more continuous variables then the task is called regression an example of a regression problem would be the pre diction of the yield in a chemical manufacturing process in which the inputs consist of the concentrations of reactants the temperature and the pressure in other pattern recognition problems the training data consists of a set of input vectors x without any corresponding target values the goal in such unsupervised learning problems may be to discover groups of similar examples within the data where it is called clustering or to determine the distribution of data within the input space known as density estimation or to project the data from a high dimensional space down to two or three dimensions for the purpose of visualization finally the technique of reinforcement learning sutton and barto is con cerned with the problem of ﬁnding suitable actions to take in a given situation in order to maximize a reward here the learning algorithm is not given examples of optimal outputs in contrast to supervised learning but must instead discover them by a process of trial and error typically there is a sequence of states and actions in which the learning algorithm is interacting with its environment in many cases the current action not only affects the immediate reward but also has an impact on the re ward at all subsequent time steps for example by using appropriate reinforcement learning techniques a neural network can learn to play the game of backgammon to a high standard tesauro here the network must learn to take a board position as input along with the result of a dice throw and produce a strong move as the output this is done by having the network play against a copy of itself for perhaps a million games a major challenge is that a game of backgammon can involve dozens of moves and yet it is only at the end of the game that the reward in the form of victory is achieved the reward must then be attributed appropriately to all of the moves that led to it even though some moves will have been good ones and others less so this is an example of a credit assignment problem a general feature of re inforcement learning is the trade off between exploration in which the system tries out new kinds of actions to see how effective they are and exploitation in which the system makes use of actions that are known to yield a high reward too strong a focus on either exploration or exploitation will yield poor results reinforcement learning continues to be an active area of machine learning research however a figure plot of a training data set of n points shown as blue circles each comprising an observation of the input variable x along with the corresponding target variable t t the green curve shows the function sin used to gener ate the data our goal is to pre dict the value of t for some new value of x without knowledge of the green curve x detailed treatment lies beyond the scope of this book although each of these tasks needs its own tools and techniques many of the key ideas that underpin them are common to all such problems one of the main goals of this chapter is to introduce in a relatively informal way several of the most important of these concepts and to illustrate them using simple examples later in the book we shall see these same ideas re emerge in the context of more sophisti cated models that are applicable to real world pattern recognition applications this chapter also provides a self contained introduction to three important tools that will be used throughout the book namely probability theory decision theory and infor mation theory although these might sound like daunting topics they are in fact straightforward and a clear understanding of them is essential if machine learning techniques are to be used to best effect in practical applications example polynomial curve fitting we begin by introducing a simple regression problem which we shall use as a run ning example throughout this chapter to motivate a number of key concepts sup pose we observe a real valued input variable x and we wish to use this observation to predict the value of a real valued target variable t for the present purposes it is in structive to consider an artiﬁcial example using synthetically generated data because we then know the precise process that generated the data for comparison against any learned model the data for this example is generated from the function sin with random noise included in the target values as described in detail in appendix a now suppose that we are given a training set comprising n observations of x written x xn t together with corresponding observations of the values of t denoted t tn t figure shows a plot of a training set comprising n data points the input data set x in figure was generated by choos ing values of xn for n n spaced uniformly in range and the target data set t was obtained by ﬁrst computing the corresponding values of the function sin and then adding a small level of random noise having a gaussian distri bution the gaussian distribution is discussed in section to each such point in order to obtain the corresponding value tn by generating data in this way we are capturing a property of many real data sets namely that they possess an underlying regularity which we wish to learn but that individual observations are corrupted by random noise this noise might arise from intrinsically stochastic i e random pro cesses such as radioactive decay but more typically is due to there being sources of variability that are themselves unobserved our goal is to exploit this training set in order to make predictions of the value t of the target variable for some new value x of the input variable as we shall see later this involves implicitly trying to discover the underlying function sin this is intrinsically a difﬁcult problem as we have to generalize from a ﬁnite data set furthermore the observed data are corrupted with noise and so for a given x there is uncertainty as to the appropriate value for t probability theory discussed in section provides a framework for expressing such uncertainty in a precise and quantitative manner and decision theory discussed in section allows us to exploit this probabilistic representation in order to make predictions that are optimal according to appropriate criteria for the moment however we shall proceed rather informally and consider a simple approach based on curve ﬁtting in particular we shall ﬁt the data using a polynomial function of the form m y x w wmxm wjxj j where m is the order of the polynomial and xj denotes x raised to the power of j the polynomial coefﬁcients wm are collectively denoted by the vector w note that although the polynomial function y x w is a nonlinear function of x it is a linear function of the coefﬁcients w functions such as the polynomial which are linear in the unknown parameters have important properties and are called linear models and will be discussed extensively in chapters and the values of the coefﬁcients will be determined by ﬁtting the polynomial to the training data this can be done by minimizing an error function that measures the misﬁt between the function y x w for any given value of w and the training set data points one simple choice of error function which is widely used is given by the sum of the squares of the errors between the predictions y xn w for each data point xn and the corresponding target values tn so that we minimize e w y x w t where the factor of is included for later convenience we shall discuss the mo tivation for this choice of error function later in this chapter for the moment we simply note that it is a nonnegative quantity that would be zero if and only if the figure the error function corre sponds to one half of the sum of t the squares of the displacements shown by the vertical green bars of each data point from the function y x w xn x exercise function y x w were to pass exactly through each training data point the geomet rical interpretation of the sum of squares error function is illustrated in figure we can solve the curve ﬁtting problem by choosing the value of w for which e w is as small as possible because the error function is a quadratic function of the coefﬁcients w its derivatives with respect to the coefﬁcients will be linear in the elements of w and so the minimization of the error function has a unique solution denoted by w which can be found in closed form the resulting polynomial is given by the function y x w there remains the problem of choosing the order m of the polynomial and as we shall see this will turn out to be an example of an important concept called model comparison or model selection in figure we show four examples of the results of ﬁtting polynomials having orders m and to the data set shown in figure we notice that the constant m and ﬁrst order m polynomials give rather poor ﬁts to the data and consequently rather poor representations of the function sin the third order m polynomial seems to give the best ﬁt to the function sin of the examples shown in figure when we go to a much higher order polynomial m we obtain an excellent ﬁt to the training data in fact the polynomial passes exactly through each data point and e w however the ﬁtted curve oscillates wildly and gives a very poor representation of the function sin this latter behaviour is known as over ﬁtting as we have noted earlier the goal is to achieve good generalization by making accurate predictions for new data we can obtain some quantitative insight into the dependence of the generalization performance on m by considering a separate test set comprising data points generated using exactly the same procedure used to generate the training set points but with new choices for the random noise values included in the target values for each choice of m we can then evaluate the residual value of e w given by for the training data and we can also evaluate e w for the test data set it is sometimes more convenient to use the root mean square t t x x t t x x figure plots of polynomials having various orders m shown as red curves ﬁtted to the data set shown in figure rms error deﬁned by erms w n in which the division by n allows us to compare different sizes of data sets on an equal footing and the square root ensures that erms is measured on the same scale and in the same units as the target variable t graphs of the training and test set rms errors are shown for various values of m in figure the test set error is a measure of how well we are doing in predicting the values of t for new data observations of x we note from figure that small values of m give relatively large values of the test set error and this can be attributed to the fact that the corresponding polynomials are rather inﬂexible and are incapable of capturing the oscillations in the function sin values of m in the range m give small values for the test set error and these also give reasonable representations of the generating function sin as can be seen for the case of m from figure figure graphs of the root mean square error deﬁned by evaluated on the training set and on an inde pendent test set for various values of m m for m the training set error goes to zero as we might expect because this polynomial contains degrees of freedom corresponding to the coefﬁcients and so can be tuned exactly to the data points in the training set however the test set error has become very large and as we saw in figure the corresponding function y x w exhibits wild oscillations this may seem paradoxical because a polynomial of given order contains all lower order polynomials as special cases the m polynomial is therefore capa ble of generating results at least as good as the m polynomial furthermore we might suppose that the best predictor of new data would be the function sin from which the data was generated and we shall see later that this is indeed the case we know that a power series expansion of the function sin contains terms of all orders so we might expect that results should improve monotonically as we increase m we can gain some insight into the problem by examining the values of the co efﬁcients w obtained from polynomials of various order as shown in table we see that as m increases the magnitude of the coefﬁcients typically gets larger in particular for the m polynomial the coefﬁcients have become ﬁnely tuned to the data by developing large positive and negative values so that the correspond table table of the coefﬁcients w for polynomials of various order observe how the typical mag nitude of the coefﬁcients in creases dramatically as the or der of the polynomial increases t t x x figure plots of the solutions obtained by minimizing the sum of squares error function using the m polynomial for n data points left plot and n data points right plot we see that increasing the size of the data set reduces the over ﬁtting problem section ing polynomial function matches each of the data points exactly but between data points particularly near the ends of the range the function exhibits the large oscilla tions observed in figure intuitively what is happening is that the more ﬂexible polynomials with larger values of m are becoming increasingly tuned to the random noise on the target values it is also interesting to examine the behaviour of a given model as the size of the data set is varied as shown in figure we see that for a given model complexity the over ﬁtting problem become less severe as the size of the data set increases another way to say this is that the larger the data set the more complex in other words more ﬂexible the model that we can afford to ﬁt to the data one rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple say or of the number of adaptive parameters in the model however as we shall see in chapter the number of parameters is not necessarily the most appropriate measure of model complexity also there is something rather unsatisfying about having to limit the number of parameters in a model according to the size of the available training set it would seem more reasonable to choose the complexity of the model according to the com plexity of the problem being solved we shall see that the least squares approach to ﬁnding the model parameters represents a speciﬁc case of maximum likelihood discussed in section and that the over ﬁtting problem can be understood as a general property of maximum likelihood by adopting a bayesian approach the over ﬁtting problem can be avoided we shall see that there is no difﬁculty from a bayesian perspective in employing models for which the number of parameters greatly exceeds the number of data points indeed in a bayesian model the effective number of parameters adapts automatically to the size of the data set for the moment however it is instructive to continue with the current approach and to consider how in practice we can apply it to data sets of limited size where we t t x x figure plots of m polynomials ﬁtted to the data set shown in figure using the regularized error function for two values of the regularization parameter λ corresponding to ln λ and ln λ the case of no regularizer i e λ corresponding to ln λ is shown at the bottom right of figure may wish to use relatively complex and ﬂexible models one technique that is often used to control the over ﬁtting phenomenon in such cases is that of regularization which involves adding a penalty term to the error function in order to discourage the coefﬁcients from reaching large values the simplest such penalty term takes the form of a sum of squares of all of the coefﬁcients leading to a modiﬁed error function of the form e w y x w t lwl where wtw and the coefﬁcient λ governs the rel exercise ative importance of the regularization term compared with the sum of squares error term note that often the coefﬁcient is omitted from the regularizer because its inclusion causes the results to depend on the choice of origin for the target variable hastie et al or it may be included but with its own regularization coefﬁcient we shall discuss this topic in more detail in section again the error function in can be minimized exactly in closed form techniques such as this are known in the statistics literature as shrinkage methods because they reduce the value of the coefﬁcients the particular case of a quadratic regularizer is called ridge regres sion hoerl and kennard in the context of neural networks this approach is known as weight decay figure shows the results of ﬁtting the polynomial of order m to the same data set as before but now using the regularized error function given by we see that for a value of ln λ the over ﬁtting has been suppressed and we now obtain a much closer representation of the underlying function sin if however we use too large a value for λ then we again obtain a poor ﬁt as shown in figure for ln λ the corresponding coefﬁcients from the ﬁtted polynomials are given in table showing that regularization has the desired effect of reducing table table of the coefﬁcients w for m polynomials with various values for the regularization parameter λ note that ln λ corresponds to a model with no regularization i e to the graph at the bottom right in fig ure we see that as the value of λ increases the typical magnitude of the coefﬁcients gets smaller section the magnitude of the coefﬁcients the impact of the regularization term on the generalization error can be seen by plotting the value of the rms error for both training and test sets against ln λ as shown in figure we see that in effect λ now controls the effective complexity of the model and hence determines the degree of over ﬁtting the issue of model complexity is an important one and will be discussed at length in section here we simply note that if we were trying to solve a practical application using this approach of minimizing an error function we would have to ﬁnd a way to determine a suitable value for the model complexity the results above suggest a simple way of achieving this namely by taking the available data and partitioning it into a training set used to determine the coefﬁcients w and a separate validation set also called a hold out set used to optimize the model complexity either m or λ in many cases however this will prove to be too wasteful of valuable training data and we have to seek more sophisticated approaches so far our discussion of polynomial curve ﬁtting has appealed largely to in tuition we now seek a more principled approach to solving problems in pattern recognition by turning to a discussion of probability theory as well as providing the foundation for nearly all of the subsequent developments in this book it will also figure graph of the root mean square er ror versus ln λ for the m polynomial ln λ give us some important insights into the concepts we have introduced in the con text of polynomial curve ﬁtting and will allow us to extend these to more complex situations probability theory a key concept in the ﬁeld of pattern recognition is that of uncertainty it arises both through noise on measurements as well as through the ﬁnite size of data sets prob ability theory provides a consistent framework for the quantiﬁcation and manipula tion of uncertainty and forms one of the central foundations for pattern recognition when combined with decision theory discussed in section it allows us to make optimal predictions given all the information available to us even though that infor mation may be incomplete or ambiguous we will introduce the basic concepts of probability theory by considering a sim ple example imagine we have two boxes one red and one blue and in the red box we have apples and oranges and in the blue box we have apples and orange this is illustrated in figure now suppose we randomly pick one of the boxes and from that box we randomly select an item of fruit and having observed which sort of fruit it is we replace it in the box from which it came we could imagine repeating this process many times let us suppose that in so doing we pick the red box of the time and we pick the blue box of the time and that when we remove an item of fruit from a box we are equally likely to select any of the pieces of fruit in the box in this example the identity of the box that will be chosen is a random variable which we shall denote by b this random variable can take one of two possible values namely r corresponding to the red box or b corresponding to the blue box similarly the identity of the fruit is also a random variable and will be denoted by f it can take either of the values a for apple or o for orange to begin with we shall deﬁne the probability of an event to be the fraction of times that event occurs out of the total number of trials in the limit that the total number of trials goes to inﬁnity thus the probability of selecting the red box is figure we use a simple example of two coloured boxes each containing fruit apples shown in green and or anges shown in orange to intro duce the basic ideas of probability figure we can derive the sum and product rules of probability by considering two random variables x which takes the values xi where i m and y which takes the values yj where j l in this illustration we have m and l if we consider a total number n of instances of these variables then we denote the number of instances where x xi and y yj by nij which is the number of yj points in the corresponding cell of the array the number of points in column i corresponding to x xi is denoted by ci and the number of points in row j corresponding to y yj is denoted by rj ci rj xi and the probability of selecting the blue box is we write these probabilities as p b r and p b b note that by deﬁnition probabilities must lie in the interval also if the events are mutually exclusive and if they include all possible outcomes for instance in this example the box must be either red or blue then we see that the probabilities for those events must sum to one we can now ask questions such as what is the overall probability that the se lection procedure will pick an apple or given that we have chosen an orange what is the probability that the box we chose was the blue one we can answer questions such as these and indeed much more complex questions associated with problems in pattern recognition once we have equipped ourselves with the two el ementary rules of probability known as the sum rule and the product rule having obtained these rules we shall then return to our boxes of fruit example in order to derive the rules of probability consider the slightly more general ex ample shown in figure involving two random variables x and y which could for instance be the box and fruit variables considered above we shall suppose that x can take any of the values xi where i m and y can take the values yj where j l consider a total of n trials in which we sample both of the variables x and y and let the number of such trials in which x xi and y yj be nij also let the number of trials in which x takes the value xi irrespective of the value that y takes be denoted by ci and similarly let the number of trials in which y takes the value yj be denoted by rj the probability that x will take the value xi and y will take the value yj is written p x xi y yj and is called the joint probability of x xi and y yj it is given by the number of points falling in the cell i j as a fraction of the total number of points and hence p x x y y nij i j n here we are implicitly considering the limit n similarly the probability that x takes the value xi irrespective of the value of y is written as p x xi and is given by the fraction of the total number of points that fall in column i so that p x x ci i n because the number of instances in column i in figure is just the sum of the number of instances in each cell of that column we have ci j nij and therefore from and we have l p x xi p x xi y yj j which is the sum rule of probability note that p x xi is sometimes called the marginal probability because it is obtained by marginalizing or summing out the other variables in this case y if we consider only those instances for which x xi then the fraction of such instances for which y yj is written p y yj x xi and is called the conditional probability of y yj given x xi it is obtained by ﬁnding the fraction of those points in column i that fall in cell i j and hence is given by p y y x x nij j i ci from and we can then derive the following relationship p x x y y nij nij ci i j n ci n p y yj x xi p x xi which is the product rule of probability so far we have been quite careful to make a distinction between a random vari able such as the box b in the fruit example and the values that the random variable can take for example r if the box were the red one thus the probability that b takes the value r is denoted p b r although this helps to avoid ambiguity it leads to a rather cumbersome notation and in many cases there will be no need for such pedantry instead we may simply write p b to denote a distribution over the ran dom variable b or p r to denote the distribution evaluated for the particular value r provided that the interpretation is clear from the context with this more compact notation we can write the two fundamental rules of probability theory in the following form the rules of probability sum rule p x p x y y product rule p x y p y x p x here p x y is a joint probability and is verbalized as the probability of x and y similarly the quantity p y x is a conditional probability and is verbalized as the probability of y given x whereas the quantity p x is a marginal probability and is simply the probability of x these two simple rules form the basis for all of the probabilistic machinery that we use throughout this book from the product rule together with the symmetry property p x y p y x we immediately obtain the following relationship between conditional probabilities p x y p y p x which is called bayes theorem and which plays a central role in pattern recognition and machine learning using the sum rule the denominator in bayes theorem can be expressed in terms of the quantities appearing in the numerator p x p x y p y y we can view the denominator in bayes theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left hand side of over all values of y equals one in figure we show a simple example involving a joint distribution over two variables to illustrate the concept of marginal and conditional distributions here a ﬁnite sample of n data points has been drawn from the joint distribution and is shown in the top left in the top right is a histogram of the fractions of data points having each of the two values of y from the deﬁnition of probability these fractions would equal the corresponding probabilities p y in the limit n we can view the histogram as a simple way to model a probability distribution given only a ﬁnite number of points drawn from that distribution modelling distributions from data lies at the heart of statistical pattern recognition and will be explored in great detail in this book the remaining two plots in figure show the corresponding histogram estimates of p x and p x y let us now return to our example involving boxes of fruit for the moment we shall once again be explicit about distinguishing between the random variables and their instantiations we have seen that the probabilities of selecting either the red or the blue boxes are given by p b r p b b respectively note that these satisfy p b r p b b now suppose that we pick a box at random and it turns out to be the blue box then the probability of selecting an apple is just the fraction of apples in the blue box which is and so p f a b b in fact we can write out all four conditional probabilities for the type of fruit given the selected box p f a b r p f o b r p f a b b p f o b b y y p x y x p x p y p x y x x figure an illustration of a distribution over two variables x which takes possible values and y which takes two possible values the top left ﬁgure shows a sample of points drawn from a joint probability distri bution over these variables the remaining ﬁgures show histogram estimates of the marginal distributions p x and p y as well as the conditional distribution p x y corresponding to the bottom row in the top left ﬁgure again note that these probabilities are normalized so that p f a b r p f o b r and similarly p f a b b p f o b b we can now use the sum and product rules of probability to evaluate the overall probability of choosing an apple p f a p f a b r p b r p f a b b p b b from which it follows using the sum rule that p f o suppose instead we are told that a piece of fruit has been selected and it is an orange and we would like to know which box it came from this requires that we evaluate the probability distribution over boxes conditioned on the identity of the fruit whereas the probabilities in give the probability distribution over the fruit conditioned on the identity of the box we can solve the problem of reversing the conditional probability by using bayes theorem to give p b r f o p f o b r p b r p f o from the sum rule it then follows that p b b f o we can provide an important interpretation of bayes theorem as follows if we had been asked which box had been chosen before being told the identity of the selected item of fruit then the most complete information we have available is provided by the probability p b we call this the prior probability because it is the probability available before we observe the identity of the fruit once we are told that the fruit is an orange we can then use bayes theorem to compute the probability p b f which we shall call the posterior probability because it is the probability obtained after we have observed f note that in this example the prior probability of selecting the red box was so that we were more likely to select the blue box than the red one however once we have observed that the piece of selected fruit is an orange we ﬁnd that the posterior probability of the red box is now so that it is now more likely that the box we selected was in fact the red one this result accords with our intuition as the proportion of oranges is much higher in the red box than it is in the blue box and so the observation that the fruit was an orange provides signiﬁcant evidence favouring the red box in fact the evidence is sufﬁciently strong that it outweighs the prior and makes it more likely that the red box was chosen rather than the blue one finally we note that if the joint distribution of two variables factorizes into the product of the marginals so that p x y p x p y then x and y are said to be independent from the product rule we see that p y x p y and so the conditional distribution of y given x is indeed independent of the value of x for instance in our boxes of fruit example if each box contained the same fraction of apples and oranges then p f b p f so that the probability of selecting say an apple is independent of which box is chosen probability densities as well as considering probabilities deﬁned over discrete sets of events we also wish to consider probabilities with respect to continuous variables we shall limit ourselves to a relatively informal discussion if the probability of a real valued variable x falling in the interval x x δx is given by p x δx for δx then p x is called the probability density over x this is illustrated in figure the probability that x will lie in an interval a b is then given by b figure the concept of probability for discrete variables can be ex tended to that of a probability density p x over a continuous variable x and is such that the probability of x lying in the inter val x x δx is given by p x δx for δx the probability density can be expressed as the derivative of a cumulative distri bution function p x δx x because probabilities are nonnegative and because the value of x must lie some where on the real axis the probability density p x must satisfy the two conditions p x p x dx under a nonlinear change of variable a probability density transforms differently from a simple function due to the jacobian factor for instance if we consider a change of variables x g y then a function f x becomes f y f g y now consider a probability density px x that corresponds to a density py y with respect to the new variable y where the sufﬁces denote the fact that px x and py y are different densities observations falling in the range x x δx will for small values of δx be transformed into the range y y δy where px x δx py y δy and hence py y px dx x l dy l exercise px g y gt y one consequence of this property is that the concept of the maximum of a probability density is dependent on the choice of variable the probability that x lies in the interval z is given by the cumulative distribution function deﬁned by z p z p x dx which satisﬁes p t x p x as shown in figure if we have several continuous variables xd denoted collectively by the vector x then we can deﬁne a joint probability density p x p xd such that the probability of x falling in an inﬁnitesimal volume δx containing the point x is given by p x δx this multivariate probability density must satisfy p x p x dx in which the integral is taken over the whole of x space we can also consider joint probability distributions over a combination of discrete and continuous variables note that if x is a discrete variable then p x is sometimes called a probability mass function because it can be regarded as a set of probability masses concentrated at the allowed values of x the sum and product rules of probability as well as bayes theorem apply equally to the case of probability densities or to combinations of discrete and con tinuous variables for instance if x and y are two real variables then the sum and product rules take the form p x p x y dy p x y p y x p x a formal justiﬁcation of the sum and product rules for continuous variables feller requires a branch of mathematics called measure theory and lies outside the scope of this book its validity can be seen informally however by dividing each real variable into intervals of width and considering the discrete probability dis tribution over these intervals taking the limit then turns sums into integrals and gives the desired result expectations and covariances one of the most important operations involving probabilities is that of ﬁnding weighted averages of functions the average value of some function f x under a probability distribution p x is called the expectation of f x and will be denoted by e f for a discrete distribution it is given by e f p x f x x so that the average is weighted by the relative probabilities of the different values of x in the case of continuous variables expectations are expressed in terms of an integration with respect to the corresponding probability density e f p x f x dx in either case if we are given a ﬁnite number n of points drawn from the probability distribution or probability density then the expectation can be approximated as a ﬁnite sum over these points e f n f xn n we shall make extensive use of this result when we discuss sampling methods in chapter the approximation in becomes exact in the limit n sometimes we will be considering expectations of functions of several variables in which case we can use a subscript to indicate which variable is being averaged over so that for instance ex f x y denotes the average of the function f x y with respect to the distribution of x note that ex f x y will be a function of y we can also consider a conditional expectation with respect to a conditional distribution so that exercise exercise ex f y p x y f x x with an analogous deﬁnition for continuous variables the variance of f x is deﬁned by var f e f x e f x and provides a measure of how much variability there is in f x around its mean value e f x expanding out the square we see that the variance can also be written in terms of the expectations of f x and f x var f e f x e f x in particular we can consider the variance of the variable x itself which is given by var x e e x for two random variables x and y the covariance is deﬁned by cov x y ex y x e x y e y ex y xy e x e y which expresses the extent to which x and y vary together if x and y are indepen dent then their covariance vanishes in the case of two vectors of random variables x and y the covariance is a matrix cov x y ex y x e x yt e yt ex y xyt e x e yt if we consider the covariance of the components of a vector x with each other then we use a slightly simpler notation cov x cov x x bayesian probabilities so far in this chapter we have viewed probabilities in terms of the frequencies of random repeatable events we shall refer to this as the classical or frequentist interpretation of probability now we turn to the more general bayesian view in which probabilities provide a quantiﬁcation of uncertainty consider an uncertain event for example whether the moon was once in its own orbit around the sun or whether the arctic ice cap will have disappeared by the end of the century these are not events that can be repeated numerous times in order to deﬁne a notion of probability as we did earlier in the context of boxes of fruit nevertheless we will generally have some idea for example of how quickly we think the polar ice is melting if we now obtain fresh evidence for instance from a new earth observation satellite gathering novel forms of diagnostic information we may revise our opinion on the rate of ice loss our assessment of such matters will affect the actions we take for instance the extent to which we endeavour to reduce the emission of greenhouse gasses in such circumstances we would like to be able to quantify our expression of uncertainty and make precise revisions of uncertainty in the light of new evidence as well as subsequently to be able to take optimal actions or decisions as a consequence this can all be achieved through the elegant and very general bayesian interpretation of probability the use of probability to represent uncertainty however is not an ad hoc choice but is inevitable if we are to respect common sense while making rational coherent inferences for instance cox showed that if numerical values are used to represent degrees of belief then a simple set of axioms encoding common sense properties of such beliefs leads uniquely to a set of rules for manipulating degrees of belief that are equivalent to the sum and product rules of probability this provided the ﬁrst rigorous proof that probability theory could be regarded as an extension of boolean logic to situations involving uncertainty jaynes numerous other authors have proposed different sets of properties or axioms that such measures of uncertainty should satisfy ramsey good savage definetti lindley in each case the resulting numerical quantities behave pre cisely according to the rules of probability it is therefore natural to refer to these quantities as bayesian probabilities in the ﬁeld of pattern recognition too it is helpful to have a more general no thomas bayes thomas bayes was born in tun bridge wells and was a clergyman as well as an amateur scientist and a mathematician he studied logic and theology at edinburgh univer sity and was elected fellow of the royal society in during the century is sues regarding probability arose in connection with gambling and with the new concept of insurance one particularly important problem concerned so called in verse probability a solution was proposed by thomas bayes in his paper essay towards solving a problem in the doctrine of chances which was published in some three years after his death in the philo sophical transactions of the royal society in fact bayes only formulated his theory for the case of a uni form prior and it was pierre simon laplace who inde pendently rediscovered the theory in general form and who demonstrated its broad applicability tion of probability consider the example of polynomial curve ﬁtting discussed in section it seems reasonable to apply the frequentist notion of probability to the random values of the observed variables tn however we would like to address and quantify the uncertainty that surrounds the appropriate choice for the model param eters w we shall see that from a bayesian perspective we can use the machinery of probability theory to describe the uncertainty in model parameters such as w or indeed in the choice of model itself bayes theorem now acquires a new signiﬁcance recall that in the boxes of fruit example the observation of the identity of the fruit provided relevant information that altered the probability that the chosen box was the red one in that example bayes theorem was used to convert a prior probability into a posterior probability by incorporating the evidence provided by the observed data as we shall see in detail later we can adopt a similar approach when making inferences about quantities such as the parameters w in the polynomial curve ﬁtting example we capture our assumptions about w before observing the data in the form of a prior probability distribution p w the effect of the observed data tn is expressed through the conditional probability p w and we shall see later in section how this can be represented explicitly bayes theorem which takes the form w p d w p w p d then allows us to evaluate the uncertainty in w after we have observed in the form of the posterior probability p w the quantity p w on the right hand side of bayes theorem is evaluated for the observed data set and can be viewed as a function of the parameter vector w in which case it is called the likelihood function it expresses how probable the observed data set is for different settings of the parameter vector w note that the likelihood is not a probability distribution over w and its integral with respect to w does not necessarily equal one given this deﬁnition of likelihood we can state bayes theorem in words posterior likelihood prior where all of these quantities are viewed as functions of w the denominator in is the normalization constant which ensures that the posterior distribution on the left hand side is a valid probability density and integrates to one indeed integrating both sides of with respect to w we can express the denominator in bayes theorem in terms of the prior distribution and the likelihood function p d p d w p w dw in both the bayesian and frequentist paradigms the likelihood function p w plays a central role however the manner in which it is used is fundamentally dif ferent in the two approaches in a frequentist setting w is considered to be a ﬁxed parameter whose value is determined by some form of estimator and error bars section section section on this estimate are obtained by considering the distribution of possible data sets by contrast from the bayesian viewpoint there is only a single data set namely the one that is actually observed and the uncertainty in the parameters is expressed through a probability distribution over w a widely used frequentist estimator is maximum likelihood in which w is set to the value that maximizes the likelihood function p w this corresponds to choosing the value of w for which the probability of the observed data set is maxi mized in the machine learning literature the negative log of the likelihood function is called an error function because the negative logarithm is a monotonically de creasing function maximizing the likelihood is equivalent to minimizing the error one approach to determining frequentist error bars is the bootstrap efron hastie et al in which multiple data sets are created as follows suppose our original data set consists of n data points x xn we can create a new data set xb by drawing n points at random from x with replacement so that some points in x may be replicated in xb whereas other points in x may be absent from xb this process can be repeated l times to generate l data sets each of size n and each obtained by sampling from the original data set x the statistical accuracy of parameter estimates can then be evaluated by looking at the variability of predictions between the different bootstrap data sets one advantage of the bayesian viewpoint is that the inclusion of prior knowl edge arises naturally suppose for instance that a fair looking coin is tossed three times and lands heads each time a classical maximum likelihood estimate of the probability of landing heads would give implying that all future tosses will land heads by contrast a bayesian approach with any reasonable prior will lead to a much less extreme conclusion there has been much controversy and debate associated with the relative mer its of the frequentist and bayesian paradigms which have not been helped by the fact that there is no unique frequentist or even bayesian viewpoint for instance one common criticism of the bayesian approach is that the prior distribution is of ten selected on the basis of mathematical convenience rather than as a reﬂection of any prior beliefs even the subjective nature of the conclusions through their de pendence on the choice of prior is seen by some as a source of difﬁculty reducing the dependence on the prior is one motivation for so called noninformative priors however these lead to difﬁculties when comparing different models and indeed bayesian methods based on poor choices of prior can give poor results with high conﬁdence frequentist evaluation methods offer some protection from such prob lems and techniques such as cross validation remain useful in areas such as model comparison this book places a strong emphasis on the bayesian viewpoint reﬂecting the huge growth in the practical importance of bayesian methods in the past few years while also discussing useful frequentist concepts as required although the bayesian framework has its origins in the century the prac tical application of bayesian methods was for a long time severely limited by the difﬁculties in carrying through the full bayesian procedure particularly the need to marginalize sum or integrate over the whole of parameter space which as we shall see is required in order to make predictions or to compare different models the development of sampling methods such as markov chain monte carlo discussed in chapter along with dramatic improvements in the speed and memory capacity of computers opened the door to the practical use of bayesian techniques in an im pressive range of problem domains monte carlo methods are very ﬂexible and can be applied to a wide range of models however they are computationally intensive and have mainly been used for small scale problems more recently highly efﬁcient deterministic approximation schemes such as variational bayes and expectation propagation discussed in chapter have been developed these offer a complementary alternative to sampling methods and have allowed bayesian techniques to be used in large scale applications blei et al the gaussian distribution we shall devote the whole of chapter to a study of various probability dis tributions and their key properties it is convenient however to introduce here one of the most important probability distributions for continuous variables called the normal or gaussian distribution we shall make extensive use of this distribution in the remainder of this chapter and indeed throughout much of the book for the case of a single real valued variable x the gaussian distribution is de ﬁned by n x µ exp x µ exercise which is governed by two parameters µ called the mean and called the vari ance the square root of the variance given by σ is called the standard deviation and the reciprocal of the variance written as β is called the precision we shall see the motivation for these terms shortly figure shows a plot of the gaussian distribution from the form of we see that the gaussian distribution satisﬁes n x µ also it is straightforward to show that the gaussian is normalized so that pierre simon laplace it is said that laplace was seri ously lacking in modesty and at one point declared himself to be the best mathematician in france at the time a claim that was arguably true as well as being proliﬁc in mathe matics he also made numerous contributions to as tronomy including the nebular hypothesis by which the earth is thought to have formed from the condensa tion and cooling of a large rotating disk of gas and dust in he published the ﬁrst edition of the orie analytique des probabilite in which laplace states that probability theory is nothing but common sense reduced to calculation this work included a discus sion of the inverse probability calculation later termed bayes theorem by poincare which he used to solve problems in life expectancy jurisprudence planetary masses triangulation and error estimation figure plot of the univariate gaussian showing the mean µ and the standard deviation σ µ x exercise exercise n x µ dx thus satisﬁes the two requirements for a valid probability density we can readily ﬁnd expectations of functions of x under the gaussian distribu tion in particular the average value of x is given by e x n x µ x dx µ because the parameter µ represents the average value of x under the distribution it is referred to as the mean similarly for the second order moment e n x µ dx from and it follows that the variance of x is given by var x e e x and hence is referred to as the variance parameter the maximum of a distribution is known as its mode for a gaussian the mode coincides with the mean we are also interested in the gaussian distribution deﬁned over a d dimensional vector x of continuous variables which is given by n x µ σ exp x µ tς x µ d σ where the d dimensional vector µ is called the mean the d d matrix σ is called the covariance and σ denotes the determinant of σ we shall make use of the multivariate gaussian distribution brieﬂy in this chapter although its properties will be studied in detail in section figure illustration of the likelihood function for a gaussian distribution shown by the red curve here the black points de note a data set of values xn and the likelihood function given by corresponds to the product of the blue values maximizing the likelihood in volves adjusting the mean and vari ance of the gaussian so as to maxi mize this product p x xn x now suppose that we have a data set of observations x xn t rep resenting n observations of the scalar variable x note that we are using the type face x to distinguish this from a single observation of the vector valued variable xd t which we denote by x we shall suppose that the observations are drawn independently from a gaussian distribution whose mean µ and variance are unknown and we would like to determine these parameters from the data set data points that are drawn independently from the same distribution are said to be independent and identically distributed which is often abbreviated to i i d we have seen that the joint probability of two independent events is given by the product of the marginal probabilities for each event separately because our data set x is i i d we can therefore write the probability of the data set given µ and in the form section n p x µ n xn µ n when viewed as a function of µ and this is the likelihood function for the gaus sian and is interpreted diagrammatically in figure one common criterion for determining the parameters in a probability distribu tion using an observed data set is to ﬁnd the parameter values that maximize the likelihood function this might seem like a strange criterion because from our fore going discussion of probability theory it would seem more natural to maximize the probability of the parameters given the data not the probability of the data given the parameters in fact these two criteria are related as we shall discuss in the context of curve ﬁtting for the moment however we shall determine values for the unknown parame ters µ and in the gaussian by maximizing the likelihood function in prac tice it is more convenient to maximize the log of the likelihood function because the logarithm is a monotonically increasing function of its argument maximization of the log of a function is equivalent to maximization of the function itself taking the log not only simpliﬁes the subsequent mathematical analysis but it also helps numerically because the product of a large number of small probabilities can easily underﬂow the numerical precision of the computer and this is resolved by computing instead the sum of the log probabilities from and the log likelihood function can be written in the form ln p x µ x µ ln σ ln exercise maximizing with respect to µ we obtain the maximum likelihood solution given by µml n n xn n which is the sample mean i e the mean of the observed values xn similarly maximizing with respect to we obtain the maximum likelihood solution for the variance in the form ml n n xn n µml section which is the sample variance measured with respect to the sample mean µml note that we are performing a joint maximization of with respect to µ and but in the case of the gaussian distribution the solution for µ decouples from that for so that we can ﬁrst evaluate and then subsequently use this result to evaluate later in this chapter and also in subsequent chapters we shall highlight the sig niﬁcant limitations of the maximum likelihood approach here we give an indication of the problem in the context of our solutions for the maximum likelihood param eter settings for the univariate gaussian distribution in particular we shall show that the maximum likelihood approach systematically underestimates the variance of the distribution this is an example of a phenomenon called bias and is related to the problem of over ﬁtting encountered in the context of polynomial curve ﬁtting we ﬁrst note that the maximum likelihood solutions µml and are functions of exercise the data set values xn consider the expectations of these quantities with respect to the data set values which themselves come from a gaussian distribution with parameters µ and it is straightforward to show that e µml µ e n so that on average the maximum likelihood estimate will obtain the correct mean but will underestimate the true variance by a factor n n the intuition behind this result is given by figure from it follows that the following estimate for the variance parameter is unbiased n x µ figure illustration of how bias arises in using max imum likelihood to determine the variance of a gaussian the green curve shows the true gaussian distribution from which data is generated and the three red curves show the gaussian distributions obtained by ﬁtting to three data sets each consist ing of two data points shown in blue us ing the maximum likelihood results and averaged across the three data sets the mean is correct but the variance is systematically under estimated because it is measured relative to the sample mean and not relative to the true mean a c section in section we shall see how this result arises automatically when we adopt a bayesian approach note that the bias of the maximum likelihood solution becomes less signiﬁcant as the number n of data points increases and in the limit n the maximum likelihood solution for the variance equals the true variance of the distribution that generated the data in practice for anything other than small n this bias will not prove to be a serious problem however throughout this book we shall be interested in more complex models with many parameters for which the bias problems asso ciated with maximum likelihood will be much more severe in fact as we shall see the issue of bias in maximum likelihood lies at the root of the over ﬁtting problem that we encountered earlier in the context of polynomial curve ﬁtting curve ﬁtting re visited we have seen how the problem of polynomial curve ﬁtting can be expressed in terms of error minimization here we return to the curve ﬁtting example and view it from a probabilistic perspective thereby gaining some insights into error functions and regularization as well as taking us towards a full bayesian treatment the goal in the curve ﬁtting problem is to be able to make predictions for the target variable t given some new value of the input variable x on the basis of a set of training data comprising n input values x xn t and their corresponding target values t tn t we can express our uncertainty over the value of the target variable using a probability distribution for this purpose we shall assume that given the value of x the corresponding value of t has a gaussian distribution with a mean equal to the value y x w of the polynomial curve given by thus we have p t x w β n t y x w β where for consistency with the notation in later chapters we have deﬁned a preci sion parameter β corresponding to the inverse variance of the distribution this is illustrated schematically in figure figure schematic illustration of a gaus sian conditional distribution for t given x given by in which the mean is given by the polyno mial function y x w and the precision is given by the parameter β which is related to the vari ance by β t y w x we now use the training data x t to determine the values of the unknown parameters w and β by maximum likelihood if the data are assumed to be drawn independently from the distribution then the likelihood function is given by n p t x w β n tn y xn w β n as we did in the case of the simple gaussian distribution earlier it is convenient to maximize the logarithm of the likelihood function substituting for the form of the gaussian distribution given by we obtain the log likelihood function in the form ln p t x w β β y x w t ln β ln consider ﬁrst the determination of the maximum likelihood solution for the polyno mial coefﬁcients which will be denoted by wml these are determined by maxi mizing with respect to w for this purpose we can omit the last two terms on the right hand side of because they do not depend on w also we note that scaling the log likelihood by a positive constant coefﬁcient does not alter the location of the maximum with respect to w and so we can replace the coefﬁcient β with finally instead of maximizing the log likelihood we can equivalently minimize the negative log likelihood we therefore see that maximizing likelihood is equivalent so far as determining w is concerned to minimizing the sum of squares error function deﬁned by thus the sum of squares error function has arisen as a consequence of maximizing likelihood under the assumption of a gaussian noise distribution we can also use maximum likelihood to determine the precision parameter β of the gaussian conditional distribution maximizing with respect to β gives y x w t section again we can ﬁrst determine the parameter vector wml governing the mean and sub sequently use this to ﬁnd the precision βml as was the case for the simple gaussian distribution having determined the parameters w and β we can now make predictions for new values of x because we now have a probabilistic model these are expressed in terms of the predictive distribution that gives the probability distribution over t rather than simply a point estimate and is obtained by substituting the maximum likelihood parameters into to give p t x wml βml n t y x wml β now let us take a step towards a more bayesian approach and introduce a prior distribution over the polynomial coefﬁcients w for simplicity let us consider a gaussian distribution of the form α m f α t l where α is the precision of the distribution and m is the total number of elements in the vector w for an m th order polynomial variables such as α which control the distribution of model parameters are called hyperparameters using bayes theorem the posterior distribution for w is proportional to the product of the prior distribution and the likelihood function p w x t α β p t x w β p w α we can now determine w by ﬁnding the most probable value of w given the data in other words by maximizing the posterior distribution this technique is called maximum posterior or simply map taking the negative logarithm of and combining with and we ﬁnd that the maximum of the posterior is given by the minimum of β y x w t w w thus we see that maximizing the posterior distribution is equivalent to minimizing the regularized sum of squares error function encountered earlier in the form with a regularization parameter given by λ α β bayesian curve ﬁtting although we have included a prior distribution p w α we are so far still mak ing a point estimate of w and so this does not yet amount to a bayesian treatment in a fully bayesian approach we should consistently apply the sum and product rules of probability which requires as we shall see shortly that we integrate over all val ues of w such marginalizations lie at the heart of bayesian methods for pattern recognition in the curve ﬁtting problem we are given the training data x and t along with a new test point x and our goal is to predict the value of t we therefore wish to evaluate the predictive distribution p t x x t here we shall assume that the parameters α and β are ﬁxed and known in advance in later chapters we shall discuss how such parameters can be inferred from data in a bayesian setting a bayesian treatment simply corresponds to a consistent application of the sum and product rules of probability which allow the predictive distribution to be written in the form p t x x t p t x w p w x t dw here p t x w is given by and we have omitted the dependence on α and β to simplify the notation here p w x t is the posterior distribution over param eters and can be found by normalizing the right hand side of we shall see in section that for problems such as the curve ﬁtting example this posterior distribution is a gaussian and can be evaluated analytically similarly the integra tion in can also be performed analytically with the result that the predictive distribution is given by a gaussian of the form p t x x t n t m x x where the mean and variance are given by n m x βφ x ts φ xn tn n x β φ x tsφ x here the matrix s is given by n s αi β φ xn φ x t n where i is the unit matrix and we have deﬁned the vector φ x with elements φi x xi for i m we see that the variance as well as the mean of the predictive distribution in is dependent on x the ﬁrst term in represents the uncertainty in the predicted value of t due to the noise on the target variables and was expressed already in the maximum likelihood predictive distribution through β however the second term arises from the uncertainty in the parameters w and is a consequence of the bayesian treatment the predictive distribution for the synthetic sinusoidal regression problem is illustrated in figure figure the predictive distribution result ing from a bayesian treatment of polynomial curve ﬁtting using an m polynomial with the ﬁxed parameters α and β t corresponding to the known noise variance in which the red curve denotes the mean of the predictive distribution and the red region corresponds to stan dard deviation around the mean x model selection in our example of polynomial curve ﬁtting using least squares we saw that there was an optimal order of polynomial that gave the best generalization the order of the polynomial controls the number of free parameters in the model and thereby governs the model complexity with regularized least squares the regularization coefﬁcient λ also controls the effective complexity of the model whereas for more complex models such as mixture distributions or neural networks there may be multiple pa rameters governing complexity in a practical application we need to determine the values of such parameters and the principal objective in doing so is usually to achieve the best predictive performance on new data furthermore as well as ﬁnd ing the appropriate values for complexity parameters within a given model we may wish to consider a range of different types of model in order to ﬁnd the best one for our particular application we have already seen that in the maximum likelihood approach the perfor mance on the training set is not a good indicator of predictive performance on un seen data due to the problem of over ﬁtting if data is plentiful then one approach is simply to use some of the available data to train a range of models or a given model with a range of values for its complexity parameters and then to compare them on independent data sometimes called a validation set and select the one having the best predictive performance if the model design is iterated many times using a lim ited size data set then some over ﬁtting to the validation data can occur and so it may be necessary to keep aside a third test set on which the performance of the selected model is ﬁnally evaluated in many applications however the supply of data for training and testing will be limited and in order to build good models we wish to use as much of the available data as possible for training however if the validation set is small it will give a relatively noisy estimate of predictive performance one solution to this dilemma is to use cross validation which is illustrated in figure this allows a proportion s s of the available data to be used for training while making use of all of the figure the technique of s fold cross validation illus trated here for the case of s involves tak ing the available data and partitioning it into s groups in the simplest case these are of equal size then s of the groups are used to train a set of models that are then evaluated on the re maining group this procedure is then repeated for all s possible choices for the held out group indicated here by the red blocks and the perfor mance scores from the s runs are then averaged run run run run data to assess performance when data is particularly scarce it may be appropriate to consider the case s n where n is the total number of data points which gives the leave one out technique one major drawback of cross validation is that the number of training runs that must be performed is increased by a factor of s and this can prove problematic for models in which the training is itself computationally expensive a further problem with techniques such as cross validation that use separate data to assess performance is that we might have multiple complexity parameters for a single model for in stance there might be several regularization parameters exploring combinations of settings for such parameters could in the worst case require a number of training runs that is exponential in the number of parameters clearly we need a better ap proach ideally this should rely only on the training data and should allow multiple hyperparameters and model types to be compared in a single training run we there fore need to ﬁnd a measure of performance which depends only on the training data and which does not suffer from bias due to over ﬁtting historically various information criteria have been proposed that attempt to correct for the bias of maximum likelihood by the addition of a penalty term to compensate for the over ﬁtting of more complex models for example the akaike information criterion or aic akaike chooses the model for which the quan tity ln p d wml m is largest here p wml is the best ﬁt log likelihood and m is the number of adjustable parameters in the model a variant of this quantity called the bayesian information criterion or bic will be discussed in section such criteria do not take account of the uncertainty in the model parameters however and in practice they tend to favour overly simple models we therefore turn in section to a fully bayesian approach where we shall see how complexity penalties arise in a natural and principled way the curse of dimensionality in the polynomial curve ﬁtting example we had just one input variable x for prac tical applications of pattern recognition however we will have to deal with spaces figure scatter plot of the oil ﬂow data for input variables and in which red denotes the homoge nous class green denotes the annular class and blue denotes the laminar class our goal is to classify the new test point de noted by of high dimensionality comprising many input variables as we now discuss this poses some serious challenges and is an important factor inﬂuencing the design of pattern recognition techniques in order to illustrate the problem we consider a synthetically generated data set representing measurements taken from a pipeline containing a mixture of oil wa ter and gas bishop and james these three materials can be present in one of three different geometrical conﬁgurations known as homogenous annular and laminar and the fractions of the three materials can also vary each data point com prises a dimensional input vector consisting of measurements taken with gamma ray densitometers that measure the attenuation of gamma rays passing along nar row beams through the pipe this data set is described in detail in appendix a figure shows points from this data set on a plot showing two of the mea surements and the remaining ten input values are ignored for the purposes of this illustration each data point is labelled according to which of the three geomet rical classes it belongs to and our goal is to use this data as a training set in order to be able to classify a new observation such as the one denoted by the cross in figure we observe that the cross is surrounded by numerous red points and so we might suppose that it belongs to the red class however there are also plenty of green points nearby so we might think that it could instead belong to the green class it seems unlikely that it belongs to the blue class the intuition here is that the identity of the cross should be determined more strongly by nearby points from the training set and less strongly by more distant points in fact this intuition turns out to be reasonable and will be discussed more fully in later chapters how can we turn this intuition into a learning algorithm one very simple ap proach would be to divide the input space into regular cells as indicated in fig ure when we are given a test point and we wish to predict its class we ﬁrst decide which cell it belongs to and we then ﬁnd all of the training data points that figure illustration of a simple approach to the solution of a classiﬁcation problem in which the input space is divided into cells and any new test point is assigned to the class that has a majority number of rep resentatives in the same cell as the test point as we shall see shortly this simplistic approach has some severe shortcomings section fall in the same cell the identity of the test point is predicted as being the same as the class having the largest number of training points in the same cell as the test point with ties being broken at random there are numerous problems with this naive approach but one of the most se vere becomes apparent when we consider its extension to problems having larger numbers of input variables corresponding to input spaces of higher dimensionality the origin of the problem is illustrated in figure which shows that if we divide a region of a space into regular cells then the number of such cells grows exponen tially with the dimensionality of the space the problem with an exponentially large number of cells is that we would need an exponentially large quantity of training data in order to ensure that the cells are not empty clearly we have no hope of applying such a technique in a space of more than a few variables and so we need to ﬁnd a more sophisticated approach we can gain further insight into the problems of high dimensional spaces by returning to the example of polynomial curve ﬁtting and considering how we would figure illustration of the curse of dimensionality showing how the number of regions of a regular grid grows exponentially with the dimensionality d of the space for clarity only a subset of the cubical regions are shown for d d d d extend this approach to deal with input spaces having several variables if we have d input variables then a general polynomial with coefﬁcients up to order would take the form d d d d d d y x w wixi wijxixj wijkxixjxk exercise exercise exercise as d increases so the number of independent coefﬁcients not all of the coefﬁcients are independent due to interchange symmetries amongst the x variables grows pro portionally to in practice to capture complex dependencies in the data we may need to use a higher order polynomial for a polynomial of order m the growth in the number of coefﬁcients is like dm although this is now a power law growth rather than an exponential growth it still points to the method becoming rapidly unwieldy and of limited practical utility our geometrical intuitions formed through a life spent in a space of three di mensions can fail badly when we consider spaces of higher dimensionality as a simple example consider a sphere of radius r in a space of d dimensions and ask what is the fraction of the volume of the sphere that lies between radius r e and r we can evaluate this fraction by noting that the volume of a sphere of radius r in d dimensions must scale as rd and so we write vd r kdrd where the constant kd depends only on d thus the required fraction is given by vd vd e d vd which is plotted as a function of e for various values of d in figure we see that for large d this fraction tends to even for small values of e thus in spaces of high dimensionality most of the volume of a sphere is concentrated in a thin shell near the surface as a further example of direct relevance to pattern recognition consider the behaviour of a gaussian distribution in a high dimensional space if we transform from cartesian to polar coordinates and then integrate out the directional variables we obtain an expression for the density p r as a function of radius r from the origin thus p r δr is the probability mass inside a thin shell of thickness δr located at radius r this distribution is plotted for various values of d in figure and we see that for large d the probability mass of the gaussian is concentrated in a thin shell the severe difﬁculty that can arise in spaces of many dimensions is sometimes called the curse of dimensionality bellman in this book we shall make ex tensive use of illustrative examples involving input spaces of one or two dimensions because this makes it particularly easy to illustrate the techniques graphically the reader should be warned however that not all intuitions developed in spaces of low dimensionality will generalize to spaces of many dimensions figure plot of the fraction of the volume of a sphere lying in the range r e to r for various values of the dimensionality d e although the curse of dimensionality certainly raises important issues for pat tern recognition applications it does not prevent us from ﬁnding effective techniques applicable to high dimensional spaces the reasons for this are twofold first real data will often be conﬁned to a region of the space having lower effective dimension ality and in particular the directions over which important variations in the target variables occur may be so conﬁned second real data will typically exhibit some smoothness properties at least locally so that for the most part small changes in the input variables will produce small changes in the target variables and so we can ex ploit local interpolation like techniques to allow us to make predictions of the target variables for new values of the input variables successful pattern recognition tech niques exploit one or both of these properties consider for example an application in manufacturing in which images are captured of identical planar objects on a con veyor belt in which the goal is to determine their orientation each image is a point figure plot of the probability density with respect to radius r of a gaus sian distribution for various values of the dimensionality d in a high dimensional space most of the probability mass of a gaussian is lo cated within a thin shell at a speciﬁc radius r in a high dimensional space whose dimensionality is determined by the number of pixels because the objects can occur at different positions within the image and in different orientations there are three degrees of freedom of variability between images and a set of images will live on a three dimensional manifold embedded within the high dimensional space due to the complex relationships between the object position or orientation and the pixel intensities this manifold will be highly nonlinear if the goal is to learn a model that can take an input image and output the orientation of the object irrespective of its position then there is only one degree of freedom of variability within the manifold that is signiﬁcant decision theory we have seen in section how probability theory provides us with a consistent mathematical framework for quantifying and manipulating uncertainty here we turn to a discussion of decision theory that when combined with probability theory allows us to make optimal decisions in situations involving uncertainty such as those encountered in pattern recognition suppose we have an input vector x together with a corresponding vector t of target variables and our goal is to predict t given a new value for x for regression problems t will comprise continuous variables whereas for classiﬁcation problems t will represent class labels the joint probability distribution p x t provides a complete summary of the uncertainty associated with these variables determination of p x t from a set of training data is an example of inference and is typically a very difﬁcult problem whose solution forms the subject of much of this book in a practical application however we must often make a speciﬁc prediction for the value of t or more generally take a speciﬁc action based on our understanding of the values t is likely to take and this aspect is the subject of decision theory consider for example a medical diagnosis problem in which we have taken an x ray image of a patient and we wish to determine whether the patient has cancer or not in this case the input vector x is the set of pixel intensities in the image and output variable t will represent the presence of cancer which we denote by the class or the absence of cancer which we denote by the class we might for instance choose t to be a binary variable such that t corresponds to class and t corresponds to class we shall see later that this choice of label values is particularly convenient for probabilistic models the general inference problem then involves determining the joint distribution p x k or equivalently p x t which gives us the most complete probabilistic description of the situation although this can be a very useful and informative quantity in the end we must decide either to give treatment to the patient or not and we would like this choice to be optimal in some appropriate sense duda and hart this is the decision step and it is the subject of decision theory to tell us how to make optimal decisions given the appropriate probabilities we shall see that the decision stage is generally very simple even trivial once we have solved the inference problem here we give an introduction to the key ideas of decision theory as required for the rest of the book further background as well as more detailed accounts can be found in berger and bather before giving a more detailed analysis let us ﬁrst consider informally how we might expect probabilities to play a role in making decisions when we obtain the x ray image x for a new patient our goal is to decide which of the two classes to assign to the image we are interested in the probabilities of the two classes given the image which are given by p k x using bayes theorem these probabilities can be expressed in the form p ck x p x ck p ck p x note that any of the quantities appearing in bayes theorem can be obtained from the joint distribution p x k by either marginalizing or conditioning with respect to the appropriate variables we can now interpret p k as the prior probability for the class k and p k x as the corresponding posterior probability thus p repre sents the probability that a person has cancer before we take the x ray measurement similarly p x is the corresponding probability revised using bayes theorem in light of the information contained in the x ray if our aim is to minimize the chance of assigning x to the wrong class then intuitively we would choose the class having the higher posterior probability we now show that this intuition is correct and we also discuss more general criteria for making decisions minimizing the misclassiﬁcation rate suppose that our goal is simply to make as few misclassiﬁcations as possible we need a rule that assigns each value of x to one of the available classes such a rule will divide the input space into regions k called decision regions one for each class such that all points in k are assigned to class k the boundaries between decision regions are called decision boundaries or decision surfaces note that each decision region need not be contiguous but could comprise some number of disjoint regions we shall encounter examples of decision boundaries and decision regions in later chapters in order to ﬁnd the optimal decision rule consider ﬁrst of all the case of two classes as in the cancer problem for instance a mistake occurs when an input vector belonging to class is assigned to class or vice versa the probability of this occurring is given by p mistake p x p x p x dx p x dx we are free to choose the decision rule that assigns each point x to one of the two classes clearly to minimize p mistake we should arrange that each x is assigned to whichever class has the smaller value of the integrand in thus if p x p x for a given value of x then we should assign that x to class from the product rule of probability we have p x k p k x p x because the factor p x is common to both terms we can restate this result as saying that the minimum figure schematic illustration of the joint probabilities p x k for each of two classes plotted against x together with the decision boundary x x values of x x are classiﬁed as class and hence belong to decision region whereas points x x are classiﬁed as and belong to errors arise from the blue green and red regions so that for the errors are due to points from class being misclassiﬁed as represented by x x the sum of the red and green regions and conversely for points in the region x x the errors are due to points from class being misclassiﬁed as represented by the blue region as we vary the location x of the decision boundary the combined areas of the blue and green regions remains constant whereas the size of the red region varies the optimal choice for x is where the curves for p x and p x cross corresponding to misclassiﬁcation rate decision rule which assigns each value of x to the class having the higher posterior probability p ck x probability of making a mistake is obtained if each value of x is assigned to the class for which the posterior probability p k x is largest this result is illustrated for two classes and a single input variable x in figure for the more general case of k classes it is slightly easier to maximize the probability of being correct which is given by p correct k p x rk ck k k rk p x ck dx which is maximized when the regions k are chosen such that each x is assigned to the class for which p x k is largest again using the product rule p x k p k x p x and noting that the factor of p x is common to all terms we see that each x should be assigned to the class having the largest posterior probability p ck x figure an example of a loss matrix with ele ments lkj for the cancer treatment problem the rows correspond to the true class whereas the columns cor cancer cancer normal respond to the assignment of class made by our deci sion criterion normal minimizing the expected loss for many applications our objective will be more complex than simply mini mizing the number of misclassiﬁcations let us consider again the medical diagnosis problem we note that if a patient who does not have cancer is incorrectly diagnosed as having cancer the consequences may be some patient distress plus the need for further investigations conversely if a patient with cancer is diagnosed as healthy the result may be premature death due to lack of treatment thus the consequences of these two types of mistake can be dramatically different it would clearly be better to make fewer mistakes of the second kind even if this was at the expense of making more mistakes of the ﬁrst kind we can formalize such issues through the introduction of a loss function also called a cost function which is a single overall measure of loss incurred in taking any of the available decisions or actions our goal is then to minimize the total loss incurred note that some authors consider instead a utility function whose value they aim to maximize these are equivalent concepts if we take the utility to be simply the negative of the loss and throughout this text we shall use the loss function convention suppose that for a new value of x the true class is k and that we assign x to class j where j may or may not be equal to k in so doing we incur some level of loss that we denote by lkj which we can view as the k j element of a loss matrix for instance in our cancer example we might have a loss matrix of the form shown in figure this particular loss matrix says that there is no loss incurred if the correct decision is made there is a loss of if a healthy patient is diagnosed as having cancer whereas there is a loss of if a patient having cancer is diagnosed as healthy the optimal solution is the one which minimizes the loss function however the loss function depends on the true class which is unknown for a given input vector x our uncertainty in the true class is expressed through the joint probability distribution p x k and so we seek instead to minimize the average loss where the average is computed with respect to this distribution which is given by e l lkjp x ck dx each x can be assigned independently to one of the decision regions rj our goal the product rule p x k p k x p x to eliminate the common factor of p x thus the decision rule that minimizes the expected loss is the one that assigns each figure illustration of the reject option inputs x such that the larger of the two poste rior probabilities is less than or equal to some threshold θ will be rejected θ p x p x x reject region exercise new x to the class j for which the quantity lkjp ck x k is a minimum this is clearly trivial to do once we know the posterior class proba bilities p ck x the reject option we have seen that classiﬁcation errors arise from the regions of input space where the largest of the posterior probabilities p k x is signiﬁcantly less than unity or equivalently where the joint distributions p x k have comparable values these are the regions where we are relatively uncertain about class membership in some applications it will be appropriate to avoid making decisions on the difﬁcult cases in anticipation of a lower error rate on those examples for which a classiﬁcation de cision is made this is known as the reject option for example in our hypothetical medical illustration it may be appropriate to use an automatic system to classify those x ray images for which there is little doubt as to the correct class while leav ing a human expert to classify the more ambiguous cases we can achieve this by introducing a threshold θ and rejecting those inputs x for which the largest of the posterior probabilities p k x is less than or equal to θ this is illustrated for the case of two classes and a single continuous input variable x in figure note that setting θ will ensure that all examples are rejected whereas if there are k classes then setting θ k will ensure that no examples are rejected thus the fraction of examples that get rejected is controlled by the value of θ we can easily extend the reject criterion to minimize the expected loss when a loss matrix is given taking account of the loss incurred when a reject decision is made inference and decision we have broken the classiﬁcation problem down into two separate stages the inference stage in which we use training data to learn a model for p ck x and the subsequent decision stage in which we use these posterior probabilities to make op timal class assignments an alternative possibility would be to solve both problems together and simply learn a function that maps inputs x directly into decisions such a function is called a discriminant function in fact we can identify three distinct approaches to solving decision problems all of which have been used in practical applications these are given in decreasing order of complexity by a first solve the inference problem of determining the class conditional densities p x ck for each class ck individually also separately infer the prior class probabilities p ck then use bayes theorem in the form p ck x p x ck p ck p x to ﬁnd the posterior class probabilities p k x as usual the denominator in bayes theorem can be found in terms of the quantities appearing in the numerator because p x p x ck p ck k equivalently we can model the joint distribution p x k directly and then normalize to obtain the posterior probabilities having found the posterior probabilities we use decision theory to determine class membership for each new input x approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models because by sampling from them it is possible to generate synthetic data points in the input space b first solve the inference problem of determining the posterior class probabilities p k x and then subsequently use decision theory to assign each new x to one of the classes approaches that model the posterior probabilities directly are called discriminative models c find a function f x called a discriminant function which maps each input x directly onto a class label for instance in the case of two class problems f might be binary valued and such that f represents class and f represents class in this case probabilities play no role let us consider the relative merits of these three alternatives approach a is the most demanding because it involves ﬁnding the joint distribution over both x and k for many applications x will have high dimensionality and consequently we may need a large training set in order to be able to determine the class conditional densities to reasonable accuracy note that the class priors p k can often be esti mated simply from the fractions of the training set data points in each of the classes one advantage of approach a however is that it also allows the marginal density of data p x to be determined from this can be useful for detecting new data points that have low probability under the model and for which the predictions may x x figure example of the class conditional densities for two classes having a single input variable x left plot together with the corresponding posterior probabilities right plot note that the left hand mode of the class conditional density p x shown in blue on the left plot has no effect on the posterior probabilities the vertical green line in the right plot shows the decision boundary in x that gives the minimum misclassiﬁcation rate be of low accuracy which is known as outlier detection or novelty detection bishop tarassenko however if we only wish to make classiﬁcation decisions then it can be waste ful of computational resources and excessively demanding of data to ﬁnd the joint distribution p x k when in fact we only really need the posterior probabilities p k x which can be obtained directly through approach b indeed the class conditional densities may contain a lot of structure that has little effect on the pos terior probabilities as illustrated in figure there has been much interest in exploring the relative merits of generative and discriminative approaches to machine learning and in ﬁnding ways to combine them jebara lasserre et al an even simpler approach is c in which we use the training data to ﬁnd a discriminant function f x that maps each x directly onto a class label thereby combining the inference and decision stages into a single learning problem in the example of figure this would correspond to ﬁnding the value of x shown by the vertical green line because this is the decision boundary giving the minimum probability of misclassiﬁcation with option c however we no longer have access to the posterior probabilities p k x there are many powerful reasons for wanting to compute the posterior probabilities even if we subsequently use them to make decisions these include minimizing risk consider a problem in which the elements of the loss matrix are subjected to revision from time to time such as might occur in a ﬁnancial application if we know the posterior probabilities we can trivially revise the minimum risk decision criterion by modifying appropriately if we have only a discriminant function then any change to the loss matrix would require that we return to the training data and solve the classiﬁcation problem afresh reject option posterior probabilities allow us to determine a rejection criterion that will minimize the misclassiﬁcation rate or more generally the expected loss for a given fraction of rejected data points compensating for class priors consider our medical x ray problem again and suppose that we have collected a large number of x ray images from the gen eral population for use as training data in order to build an automated screening system because cancer is rare amongst the general population we might ﬁnd that say only in every examples corresponds to the presence of can cer if we used such a data set to train an adaptive model we could run into severe difﬁculties due to the small proportion of the cancer class for instance a classiﬁer that assigned every point to the normal class would already achieve accuracy and it would be difﬁcult to avoid this trivial solution also even a large data set will contain very few examples of x ray images corre sponding to cancer and so the learning algorithm will not be exposed to a broad range of examples of such images and hence is not likely to generalize well a balanced data set in which we have selected equal numbers of exam ples from each of the classes would allow us to ﬁnd a more accurate model however we then have to compensate for the effects of our modiﬁcations to the training data suppose we have used such a modiﬁed data set and found models for the posterior probabilities from bayes theorem we see that the posterior probabilities are proportional to the prior probabilities which we can interpret as the fractions of points in each class we can therefore simply take the posterior probabilities obtained from our artiﬁcially balanced data set and ﬁrst divide by the class fractions in that data set and then multiply by the class fractions in the population to which we wish to apply the model finally we need to normalize to ensure that the new posterior probabilities sum to one note that this procedure cannot be applied if we have learned a discriminant function directly instead of determining posterior probabilities combining models for complex applications we may wish to break the problem into a number of smaller subproblems each of which can be tackled by a sep arate module for example in our hypothetical medical diagnosis problem we may have information available from say blood tests as well as x ray im ages rather than combine all of this heterogeneous information into one huge input space it may be more effective to build one system to interpret the x ray images and a different one to interpret the blood data as long as each of the two models gives posterior probabilities for the classes we can combine the outputs systematically using the rules of probability one simple way to do this is to assume that for each class separately the distributions of inputs for the x ray images denoted by xi and the blood data denoted by xb are independent so that p xi xb ck p xi ck p xb ck section this is an example of conditional independence property because the indepen dence holds when the distribution is conditioned on the class k the posterior probability given both the x ray and blood data is then given by p ck xi xb p xi xb ck p ck p xi ck p xb ck p ck p ck xi p ck xb p ck section section appendix d thus we need the class prior probabilities p k which we can easily estimate from the fractions of data points in each class and then we need to normalize the resulting posterior probabilities so they sum to one the particular condi tional independence assumption is an example of the naive bayes model note that the joint marginal distribution p xi xb will typically not factorize under this model we shall see in later chapters how to construct models for combining data that do not require the conditional independence assumption loss functions for regression so far we have discussed decision theory in the context of classiﬁcation prob lems we now turn to the case of regression problems such as the curve ﬁtting example discussed earlier the decision stage consists of choosing a speciﬁc esti mate y x of the value of t for each input x suppose that in doing so we incur a loss l t y x the average or expected loss is then given by e l l t y x p x t dx dt a common choice of loss function in regression problems is the squared loss given by l t y x y x t in this case the expected loss can be written e l y x t x t dx dt our goal is to choose y x so as to minimize e l if we assume a completely ﬂexible function y x we can do this formally using the calculus of variations to give δe l y x t p x t dt solving for y x and using the sum and product rules of probability we obtain y x tp x t dt p x tp t x dt et t x figure the regression function y x which minimizes the expected squared loss is given by the mean of the conditional distri bution p t x t y x exercise which is the conditional average of t conditioned on x and is known as the regression function this result is illustrated in figure it can readily be extended to mul tiple target variables represented by the vector t in which case the optimal solution is the conditional average y x et t x we can also derive this result in a slightly different way which will also shed light on the nature of the regression problem armed with the knowledge that the optimal solution is the conditional expectation we can expand the square term as follows y x t y x e t x e t x t y x e t x y x e t x e t x t e t x t where to keep the notation uncluttered we use e t x to denote et t x substituting into the loss function and performing the integral over t we see that the cross term vanishes and we obtain an expression for the loss function in the form e l y x e t x p x dx e t x t x dx the function y x we seek to determine enters only in the ﬁrst term which will be minimized when y x is equal to e t x in which case this term will vanish this is simply the result that we derived previously and that shows that the optimal least squares predictor is given by the conditional mean the second term is the variance of the distribution of t averaged over x it represents the intrinsic variability of the target data and can be regarded as noise because it is independent of y x it represents the irreducible minimum value of the loss function as with the classiﬁcation problem we can either determine the appropriate prob abilities and then use these to make optimal decisions or we can build models that make decisions directly indeed we can identify three distinct approaches to solving regression problems given in order of decreasing complexity by a first solve the inference problem of determining the joint density p x t then normalize to ﬁnd the conditional density p t x and ﬁnally marginalize to ﬁnd the conditional mean given by section b first solve the inference problem of determining the conditional density p t x and then subsequently marginalize to ﬁnd the conditional mean given by c find a regression function y x directly from the training data the relative merits of these three approaches follow the same lines as for classiﬁca tion problems above the squared loss is not the only possible choice of loss function for regression indeed there are situations in which squared loss can lead to very poor results and where we need to develop more sophisticated approaches an important example concerns situations in which the conditional distribution p t x is multimodal as often arises in the solution of inverse problems here we consider brieﬂy one simple generalization of the squared loss called the minkowski loss whose expectation is given by exercise e lq y x t qp x t dx dt which reduces to the expected squared loss for q the function y t q is plotted against y t for various values of q in figure the minimum of e lq is given by the conditional mean for q the conditional median for q and the conditional mode for q information theory exercise in this chapter we have discussed a variety of concepts from probability theory and decision theory that will form the foundations for much of the subsequent discussion in this book we close this chapter by introducing some additional concepts from the ﬁeld of information theory which will also prove useful in our development of pattern recognition and machine learning techniques again we shall focus only on the key concepts and we refer the reader elsewhere for more detailed discussions viterbi and omura cover and thomas mackay we begin by considering a discrete random variable x and we ask how much information is received when we observe a speciﬁc value for this variable the amount of information can be viewed as the degree of surprise on learning the value of x if we are told that a highly improbable event has just occurred we will have received more information than if we were told that some very likely event has just occurred and if we knew that the event was certain to happen we would receive no information our measure of information content will therefore depend on the probability distribution p x and we therefore look for a quantity h x that is a monotonic function of the probability p x and that expresses the information content the form of h can be found by noting that if we have two events x and y that are unrelated then the information gain from observing both of them should be the sum of the information gained from each of them separately so that h x y h x h y two unrelated events will be statistically independent and so p x y p x p y from these two relationships it is easily shown that h x must be given by the logarithm of p x and so we have y t y t figure plots of the quantity lq y t q for various values of q h x p x where the negative sign ensures that information is positive or zero note that low probability events x correspond to high information content the choice of basis for the logarithm is arbitrary and for the moment we shall adopt the convention prevalent in information theory of using logarithms to the base of in this case as we shall see shortly the units of h x are bits binary digits now suppose that a sender wishes to transmit the value of a random variable to a receiver the average amount of information that they transmit in the process is obtained by taking the expectation of with respect to the distribution p x and is given by h x p x p x x this important quantity is called the entropy of the random variable x note that limp p ln p and so we shall take p x ln p x whenever we encounter a value for x such that p x so far we have given a rather heuristic motivation for the deﬁnition of informa tion and the corresponding entropy we now show that these deﬁnitions indeed possess useful properties consider a random variable x having possible states each of which is equally likely in order to communicate the value of x to a receiver we would need to transmit a message of length bits notice that the entropy of this variable is given by h x log bits now consider an example cover and thomas of a variable having pos sible states a b c d e f g h for which the respective probabilities are given by the entropy in this case is given by h x log log log log log bits we see that the nonuniform distribution has a smaller entropy than the uniform one and we shall gain some insight into this shortly when we discuss the interpretation of entropy in terms of disorder for the moment let us consider how we would transmit the identity of the variable state to a receiver we could do this as before using a bit number however we can take advantage of the nonuniform distribution by using shorter codes for the more probable events at the expense of longer codes for the less probable events in the hope of getting a shorter average code length this can be done by representing the states a b c d e f g h using for instance the following set of code strings 111101 the average length of the code that has to be transmitted is then average code length bits which again is the same as the entropy of the random variable note that shorter code strings cannot be used because it must be possible to disambiguate a concatenation of such strings into its component parts for instance decodes uniquely into the state sequence c a d this relation between entropy and shortest coding length is a general one the noiseless coding theorem shannon states that the entropy is a lower bound on the number of bits needed to transmit the state of a random variable from now on we shall switch to the use of natural logarithms in deﬁning en tropy as this will provide a more convenient link with ideas elsewhere in this book in this case the entropy is measured in units of nats instead of bits which differ simply by a factor of ln we have introduced the concept of entropy in terms of the average amount of information needed to specify the state of a random variable in fact the concept of entropy has much earlier origins in physics where it was introduced in the context of equilibrium thermodynamics and later given a deeper interpretation as a measure of disorder through developments in statistical mechanics we can understand this alternative view of entropy by considering a set of n identical objects that are to be divided amongst a set of bins such that there are ni objects in the ith bin consider the number of different ways of allocating the objects to the bins there are n ways to choose the ﬁrst object n ways to choose the second object and so on leading to a total of n ways to allocate all n objects to the bins where n pronounced factorial n denotes the product n n however we don t wish to distinguish between rearrangements of objects within each bin in the ith bin there are ni ways of reordering the objects and so the total number of ways of allocating the n objects to the bins is given by n i ni which is called the multiplicity the entropy is then deﬁned as the logarithm of the multiplicity scaled by an appropriate constant h n ln w n ln n n ln ni i we now consider the limit n in which the fractions ni n are held ﬁxed and apply stirling approximation which gives ln n n ln n n h lim ni ln ni p ln pi n i n n i where we have used i ni n here pi limn ni n is the probability of an object being assigned to the ith bin in physics terminology the speciﬁc ar rangements of objects in the bins is called a microstate and the overall distribution of occupation numbers expressed through the ratios ni n is called a macrostate the multiplicity w is also known as the weight of the macrostate we can interpret the bins as the states xi of a discrete random variable x where p x xi pi the entropy of the random variable x is then h p p xi ln p xi i distributions p xi that are sharply peaked around a few values will have a relatively low entropy whereas those that are spread more evenly across many values will have higher entropy as illustrated in figure because pi the entropy is nonnegative and it will equal its minimum value of when one of the pi and all other pj i the maximum entropy conﬁguration can be found by appendix e maximizing h using a lagrange multiplier to enforce the normalization constraint on the probabilities thus we maximize h p xi ln p xi λ p xi figure histograms of two probability distributions over bins illustrating the higher value of the entropy h for the broader distribution the largest entropy would arise from a uniform distribution that would give h ln exercise from which we ﬁnd that all of the p xi are equal and are given by p xi m where m is the total number of states xi the corresponding value of the entropy is then h ln m this result can also be derived from jensen inequality to be discussed shortly to verify that the stationary point is indeed a maximum we can evaluate the second derivative of the entropy which gives h i p xi p xj where iij are the elements of the identity matrix ij pi we can extend the deﬁnition of entropy to include distributions p x over con tinuous variables x as follows first divide x into bins of width then assuming p x is continuous the mean value theorem weisstein tells us that for each such bin there must exist a value xi such that i we can now quantize the continuous variable x by assigning any value x to the value xi whenever x falls in the ith bin the probability of observing the value xi is then p xi this gives a discrete distribution for which the entropy takes the form h p xi ln p xi p xi ln p xi ln where we have used i p xi which follows from we now omit the second term ln on the right hand side of and then consider the limit the ﬁrst term on the right hand side of will approach the integral of p x ln p x in this limit so that lim p xi ln p xi p x ln p x dx where the quantity on the right hand side is called the differential entropy we see that the discrete and continuous forms of the entropy differ by a quantity ln which diverges in the limit this reﬂects the fact that to specify a continuous variable very precisely requires a large number of bits for a density deﬁned over multiple continuous variables denoted collectively by the vector x the differential entropy is given by h x p x ln p x dx in the case of discrete distributions we saw that the maximum entropy con ﬁguration corresponded to an equal distribution of probabilities across the possible states of the variable let us now consider the maximum entropy conﬁguration for a continuous variable in order for this maximum to be well deﬁned it will be nec essary to constrain the ﬁrst and second moments of p x as well as preserving the normalization constraint we therefore maximize the differential entropy with the ludwig boltzmann ludwig eduard boltzmann was an austrian physicist who created the ﬁeld of statistical mechanics prior to boltzmann the concept of en tropy was already known from classical thermodynamics where it quantiﬁes the fact that when we take energy from a system not all of that energy is typically available to do useful work boltzmann showed that the ther modynamic entropy s a macroscopic quantity could be related to the statistical properties at the micro scopic level this is expressed through the famous equation s k ln w in which w represents the number of possible microstates in a macrostate and k in units of joules per kelvin is known as boltzmann constant boltzmann ideas were disputed by many scientists of they day one dif ﬁculty they saw arose from the second law of thermo dynamics which states that the entropy of a closed system tends to increase with time by contrast at the microscopic level the classical newtonian equa tions of physics are reversible and so they found it difﬁcult to see how the latter could explain the for mer they didn t fully appreciate boltzmann argu ments which were statistical in nature and which con cluded not that entropy could never decrease over time but simply that with overwhelming probability it would generally increase boltzmann even had a long running dispute with the editor of the leading german physics journal who refused to let him refer to atoms and molecules as anything other than convenient the oretical constructs the continued attacks on his work lead to bouts of depression and eventually he com mitted suicide shortly after boltzmann death new experiments by perrin on colloidal suspensions veri ﬁed his theories and conﬁrmed the value of the boltz mann constant the equation s k ln w is carved on boltzmann tombstone three constraints p x dx xp x dx µ x µ x dx appendix e the constrained maximization can be performed using lagrange multipliers so that we maximize the following functional with respect to p x p x ln p x dx λ p x dx xp x dx µ λ x µ x dx appendix d exercise using the calculus of variations we set the derivative of this functional to zero giving p x exp x µ the lagrange multipliers can be found by back substitution of this result into the three constraint equations leading ﬁnally to the result p x exp x µ exercise and so the distribution that maximizes the differential entropy is the gaussian note that we did not constrain the distribution to be nonnegative when we maximized the entropy however because the resulting distribution is indeed nonnegative we see with hindsight that such a constraint is not necessary if we evaluate the differential entropy of the gaussian we obtain h x ln thus we see again that the entropy increases as the distribution becomes broader i e as increases this result also shows that the differential entropy unlike the discrete entropy can be negative because h x in for suppose we have a joint distribution p x y from which we draw pairs of values of x and y if a value of x is already known then the additional information needed to specify the corresponding value of y is given by ln p y x thus the average additional information needed to specify y can be written as h y x p y x ln p y x dy dx exercise which is called the conditional entropy of y given x it is easily seen using the product rule that the conditional entropy satisﬁes the relation h x y h y x h x where h x y is the differential entropy of p x y and h x is the differential en tropy of the marginal distribution p x thus the information needed to describe x and y is given by the sum of the information needed to describe x alone plus the additional information required to specify y given x relative entropy and mutual information so far in this section we have introduced a number of concepts from information theory including the key notion of entropy we now start to relate these ideas to pattern recognition consider some unknown distribution p x and suppose that we have modelled this using an approximating distribution q x if we use q x to construct a coding scheme for the purpose of transmitting values of x to a receiver then the average additional amount of information in nats required to specify the value of x assuming we choose an efﬁcient coding scheme as a result of using q x instead of the true distribution p x is given by kl plq p x ln q x dx p x ln p x dx p x ln q x dx this is known as the relative entropy or kullback leibler divergence or kl diver gence kullback and leibler between the distributions p x and q x note that it is not a symmetrical quantity that is to say kl p q kl q p we now show that the kullback leibler divergence satisﬁes kl p q with equality if and only if p x q x to do this we ﬁrst introduce the concept of convex functions a function f x is said to be convex if it has the property that every chord lies on or above the function as shown in figure any value of x in the interval from x a to x b can be written in the form λa λ b where λ the corresponding point on the chord is given by λf a λ f b claude shannon after graduating from michigan and mit shannon joined the at t bell telephone laboratories in his paper a mathematical theory of communication published in the bell system technical journal in laid the foundations for modern information the ory this paper introduced the word bit and his con cept that information could be sent as a stream of and paved the way for the communications revo lution it is said that von neumann recommended to shannon that he use the term entropy not only be cause of its similarity to the quantity used in physics but also because nobody knows what entropy really is so in any discussion you will always have an advan tage figure a convex function f x is one for which ev ery chord shown in blue lies on or above the function shown in red a xλ b x and the corresponding value of the function is f λa λ b convexity then implies exercise exercise f λa λ b λf a λ f b this is equivalent to the requirement that the second derivative of the function be everywhere positive examples of convex functions are x ln x for x and a function is called strictly convex if the equality is satisﬁed only for λ and λ if a function has the opposite property namely that every chord lies on or below the function it is called concave with a corresponding deﬁnition for strictly concave if a function f x is convex then f x will be concave using the technique of proof by induction we can show from that a convex function f x satisﬁes m f i λixi i λif xi where λi and i λi for any set of points xi the result is known as jensen inequality if we interpret the λi as the probability distribution over a discrete variable x taking the values xi then can be written f e x e f x where e denotes the expectation for continuous variables jensen inequality takes the form f xp x dx f x p x dx we can apply jensen inequality in the form to the kullback leibler divergence to give kl plq p x ln q x dx ln q x dx so the equality will hold if and only if q x p x for all x thus we can in terpret the kullback leibler divergence as a measure of the dissimilarity of the two distributions p x and q x we see that there is an intimate relationship between data compression and den sity estimation i e the problem of modelling an unknown probability distribution because the most efﬁcient compression is achieved when we know the true distri bution if we use a distribution that is different from the true one then we must necessarily have a less efﬁcient coding and on average the additional information that must be transmitted is at least equal to the kullback leibler divergence be tween the two distributions suppose that data is being generated from an unknown distribution p x that we wish to model we can try to approximate this distribution using some parametric distribution q x θ governed by a set of adjustable parameters θ for example a multivariate gaussian one way to determine θ is to minimize the kullback leibler divergence between p x and q x θ with respect to θ we cannot do this directly because we don t know p x suppose however that we have observed a ﬁnite set of training points xn for n n drawn from p x then the expectation with respect to p x can be approximated by a ﬁnite sum over these points using so that exercise n kl plq ln q xn θ ln p xn n the second term on the right hand side of is independent of θ and the ﬁrst term is the negative log likelihood function for θ under the distribution q x θ eval uated using the training set thus we see that minimizing this kullback leibler divergence is equivalent to maximizing the likelihood function now consider the joint distribution between two sets of variables x and y given by p x y if the sets of variables are independent then their joint distribution will factorize into the product of their marginals p x y p x p y if the variables are not independent we can gain some idea of whether they are close to being indepen dent by considering the kullback leibler divergence between the joint distribution and the product of the marginals given by i x y kl p x y lp x p y x y ln p x p y dx dy p x y which is called the mutual information between the variables x and y from the properties of the kullback leibler divergence we see that i x y with equal ity if and only if x and y are independent using the sum and product rules of probability we see that the mutual information is related to the conditional entropy through i x y h x h x y h y h y x thus we can view the mutual information as the reduction in the uncertainty about x by virtue of being told the value of y or vice versa from a bayesian perspective we can view p x as the prior distribution for x and p x y as the posterior distribu tion after we have observed new data y the mutual information therefore represents the reduction in uncertainty about x as a consequence of the new observation y exercises www consider the sum of squares error function given by in which the function y x w is given by the polynomial show that the coefﬁcients w wi that minimize this error function are given by the solution to the following set of linear equations where m aijwj ti j n n aij xn i j ti xn itn here a sufﬁx i or j denotes the index of a component whereas x i denotes x raised to the power of i write down the set of coupled linear equations analogous to satisﬁed by the coefﬁcients wi which minimize the regularized sum of squares error function given by suppose that we have three coloured boxes r red b blue and g green box r contains apples oranges and limes box b contains apple orange and limes and box g contains apples oranges and limes if a box is chosen at random with probabilities p r p b p g and a piece of fruit is removed from the box with equal probability of selecting any of the items in the box then what is the probability of selecting an apple if we observe that the selected fruit is in fact an orange what is the probability that it came from the green box www consider a probability density px x deﬁned over a continuous vari able x and suppose that we make a nonlinear change of variable using x g y so that the density transforms according to by differentiating show that the location y of the maximum of the density in y is not in general related to the location x of the maximum of the density over x by the simple functional relation g y as a consequence of the jacobian factor this shows that the maximum x of a probability density in contrast to a simple function is dependent on the choice of variable verify that in the case of a linear transformation the location of the maximum transforms in the same way as the variable itself using the deﬁnition show that var f x satisﬁes show that if two variables x and y are independent then their covariance is zero www in this exercise we prove the normalization condition for the univariate gaussian to do this consider the integral i exp dx which we can evaluate by ﬁrst writing its square in the form exp dx dy now make the transformation from cartesian coordinates x y to polar coordinates r θ and then substitute u show that by performing the integrals over θ and u and then taking the square root of both sides we obtain i finally use this result to show that the gaussian distribution x µ is normal ized www by using a change of variables verify that the univariate gaussian distribution given by satisﬁes next by differentiating both sides of the normalization condition n x µ dx with respect to verify that the gaussian satisﬁes finally show that holds www show that the mode i e the maximum of the gaussian distribution is given by µ similarly show that the mode of the multivariate gaussian is given by µ www suppose that the two variables x and z are statistically independent show that the mean and variance of their sum satisﬁes e x z e x e z var x z var x var z by setting the derivatives of the log likelihood function with respect to µ and equal to zero verify the results and www using the results and show that e xnxm where xn and xm denote data points sampled from a gaussian distribution with mean µ and variance and inm satisﬁes inm if n m and inm otherwise hence prove the results and suppose that the variance of a gaussian is estimated using the result but with the maximum likelihood estimate µml replaced with the true value µ of the mean show that this estimator has the property that its expectation is given by the true variance show that an arbitrary square matrix with elements wij can be written in the form wij ws wa where ws and wa are symmetric and anti symmetric ij ij ij ij matrices respectively satisfying ws s and wa wa for all i and j now consider the second order term in a higher order polynomial in d dimensions given by d d show that wijxixj i j d d d d wijxixj ws xixj i j i j so that the contribution from the anti symmetric matrix vanishes we therefore see that without loss of generality the matrix of coefﬁcients wij can be chosen to be symmetric and so not all of the elements of this matrix can be chosen indepen dently show that the number of independent parameters in the matrix ws is given by d d www in this exercise and the next we explore how the number of indepen dent parameters in a polynomial grows with the order m of the polynomial and with the dimensionality d of the input space we start by writing down the m th order term for a polynomial in d dimensions in the form d d d im xim the coefﬁcients im comprise d elements but the number of independent m parameters is signiﬁcantly fewer due to the many interchange symmetries of the factor xim begin by showing that the redundancy in the coefﬁcients can be removed by rewriting this m th order term in the form d im w im xim note that the precise relationship between the w coefﬁcients and w coefﬁcients need not be made explicit use this result to show that the number of independent param eters n d m which appear at order m satisﬁes the following recursion relation d n d m n i m i next use proof by induction to show that the following result holds i m d m which can be done by ﬁrst proving the result for d and arbitrary m by making use of the result then assuming it is correct for dimension d and verifying that it is correct for dimension d finally use the two previous results together with proof by induction to show n d m d m d m to do this ﬁrst show that the result is true for m and any value of d by comparison with the result of exercise then make use of together with to show that if the result holds at order m then it will also hold at order m in exercise we proved the result for the number of independent parameters in the m th order term of a d dimensional polynomial we now ﬁnd an expression for the total number n d m of independent parameters in all of the terms up to and including the m order first show that n d m satisﬁes m n d m n d m m where n d m is the number of independent parameters in the term of order m now make use of the result together with proof by induction to show that d m n d m d m this can be done by ﬁrst proving that the result holds for m and arbitrary d then assuming that it holds at order m and hence showing that it holds at order m finally make use of stirling approximation in the form n nne n for large n to show that for d m the quantity n d m grows like dm and for m d it grows like md consider a cubic m polynomial in d dimensions and evaluate numerically the total number of independent parameters for i d and ii d which correspond to typical small scale and medium scale machine learning applications www the gamma function is deﬁned by γ x ux u du using integration by parts prove the relation γ x xγ x show also that γ and hence that γ x x when x is an integer www we can use the result to derive an expression for the surface area sd and the volume vd of a sphere of unit radius in d dimensions to do this consider the following result which is obtained by transforming from cartesian to polar coordinates tli e xi dxi sd e r rd dr using the deﬁnition of the gamma function together with evaluate both sides of this equation and hence show that sd γ d next by integrating with respect to radius from to show that the volume of the unit sphere in d dimensions is given by v sd d d finally use the results γ and γ π to show that and reduce to the usual expressions for d and d consider a sphere of radius a in d dimensions together with the concentric hypercube of side so that the sphere touches the hypercube at the centres of each of its sides by using the results of exercise show that the ratio of the volume of the sphere to the volume of the cube is given by volume of sphere πd volume of cube d now make use of stirling formula in the form γ x xxx which is valid for x to show that as d the ratio goes to zero show also that the ratio of the distance from the centre of the hypercub e to one of the corners divided by the perpendicular distance to one of the sides is d which therefore goes to as d from these results we see that in a space of high dimensionality most of the volume of a cube is concentrated in the large number of corners which themselves become very long spikes www in this exercise we explore the behaviour of the gaussian distribution in high dimensional spaces consider a gaussian distribution in d dimensions given by we wish to ﬁnd the density with respect to radius in polar coordinates in which the direction variables have been integrated out to do this show that the integral of the probability density over a thin shell of radius r and thickness e where e is given by p r e where sdrd where sd is the surface area of a unit sphere in d dimensions show that the function p r has a single stationary point located for large d at r dσ by considering p r e where e r show that for large d which shows that r is a maxi mum of the radial probability density and also that p r decays exponentially away from its maximum at r with length scale σ we have already seen that σ r for large d and so we see that most of the probability mass is concentrated in a thin shell at large radius finally show that the probability density p x is larger at the origin than at the radius r by a factor of exp d we therefore see that most of the probability mass in a high dimensional gaussian distribution is located at a different radius from the region of high probability density this property of distributions in spaces of high dimensionality will have important consequences when we consider bayesian inference of model parameters in later chapters consider two nonnegative numbers a and b and show that if a b then a ab use this result to show that if the decision regions of a two class classiﬁcation problem are chosen to minimize the probability of misclassiﬁcation this probability will satisfy p mistake p x p x dx www given a loss matrix with elements lkj the expected risk is minimized if for each x we choose the class that minimizes verify that when the loss matrix is given by lkj ikj where ikj are the elements of the identity matrix this reduces to the criterion of choosing the class having the largest posterior probability what is the interpretation of this form of loss matrix derive the criterion for minimizing the expected loss when there is a general loss matrix and general prior probabilities for the classes www consider a classiﬁcation problem in which the loss incurred when an input vector from class k is classiﬁed as belonging to class j is given by the loss matrix lkj and for which the loss incurred in selecting the reject option is λ find the decision criterion that will give the minimum expected loss verify that this reduces to the reject criterion discussed in section when the loss matrix is given by lkj ikj what is the relationship between λ and the rejection threshold θ www consider the generalization of the squared loss function for a single target variable t to the case of multiple target variables described by the vector t given by e l t y x ly x x t dx dt using the calculus of variations show that the function y x for which this expected loss is minimized is given by y x et t x show that this result reduces to for the case of a single target variable t by expansion of the square in derive a result analogous to and hence show that the function y x that minimizes the expected squared loss for the case of a vector t of target variables is again given by the conditional expectation of t www consider the expected loss for regression problems under the lq loss function given by write down the condition that y x must satisfy in order to minimize e lq show that for q this solution represents the conditional median i e the function y x such that the probability mass for t y x is the same as for t y x also show that the minimum expected lq loss for q is given by the conditional mode i e by the function y x equal to the value of t that maximizes p t x for each x in section we introduced the idea of entropy h x as the information gained on observing the value of a random variable x having distribution p x we saw that for independent variables x and y for which p x y p x p y the entropy functions are additive so that h x y h x h y in this exercise we derive the relation between h and p in the form of a function h p first show that h p and hence by induction that h pn nh p where n is a positive integer hence show that h pn m n m h p where m is also a positive integer this implies that h px xh p where x is a positive rational number and hence by continuity when it is a positive real number finally show that this implies h p must take the form h p ln p www consider an m state discrete random variable x and use jensen in equality in the form to show that the entropy of its distribution p x satisﬁes h x ln m evaluate the kullback leibler divergence between two gaussians p x n x µ and q x n x m table the joint distribution p x y for two binary variables y x and y used in exercise x www consider two variables x and y having joint distribution p x y show that the differential entropy of this pair of variables satisﬁes h x y h x h y with equality if and only if x and y are statistically independent consider a vector x of continuous variables with distribution p x and corre sponding entropy h x suppose that we make a nonsingular linear transformation of x to obtain a new variable y ax show that the corresponding entropy is given by h y h x ln a where a denotes the determinant of a suppose that the conditional entropy h y x between two discrete random variables x and y is zero show that for all values of x such that p x the variable y must be a function of x in other words for each x there is only one value of y such that p y x www use the calculus of variations to show that the stationary point of the functional is given by then use the constraints and to eliminate the lagrange multipliers and hence show that the maximum entropy solution is given by the gaussian www use the results and to show that the entropy of the univariate gaussian is given by a strictly convex function is deﬁned as one for which every chord lies above the function show that this is equivalent to the condition that the second derivative of the function be positive using the deﬁnition together with the product rule of probability prove the result www using proof by induction show that the inequality for convex functions implies the result consider two binary variables x and y having the joint distribution given in table evaluate the following quantities a h x c h y x e h x y b h y d h x y f i x y draw a diagram to show the relationship between these various quantities by applying jensen inequality with f x ln x show that the arith metic mean of a set of real numbers is never less than their geometrical mean www using the sum and product rules of probability show that the mutual information i x y satisﬁes the relation in chapter we emphasized the central role played by probability theory in the solution of pattern recognition problems we turn now to an exploration of some particular examples of probability distributions and their properties as well as be ing of great interest in their own right these distributions can form building blocks for more complex models and will be used extensively throughout the book the distributions introduced in this chapter will also serve another important purpose namely to provide us with the opportunity to discuss some key statistical concepts such as bayesian inference in the context of simple models before we encounter them in more complex situations in later chapters one role for the distributions discussed in this chapter is to model the prob ability distribution p x of a random variable x given a ﬁnite set xn of observations this problem is known as density estimation for the purposes of this chapter we shall assume that the data points are independent and identically distributed it should be emphasized that the problem of density estimation is fun damentally ill posed because there are inﬁnitely many probability distributions that could have given rise to the observed ﬁnite data set indeed any distribution p x that is nonzero at each of the data points xn is a potential candidate the issue of choosing an appropriate distribution relates to the problem of model selec tion that has already been encountered in the context of polynomial curve ﬁtting in chapter and that is a central issue in pattern recognition we begin by considering the binomial and multinomial distributions for discrete random variables and the gaussian distribution for continuous random variables these are speciﬁc examples of parametric distributions so called because they are governed by a small number of adaptive parameters such as the mean and variance in the case of a gaussian for example to apply such models to the problem of density estimation we need a procedure for determining suitable values for the parameters given an observed data set in a frequentist treatment we choose speciﬁc values for the parameters by optimizing some criterion such as the likelihood function by contrast in a bayesian treatment we introduce prior distributions over the parameters and then use bayes theorem to compute the corresponding posterior distribution given the observed data we shall see that an important role is played by conjugate priors that lead to posterior distributions having the same functional form as the prior and that there fore lead to a greatly simpliﬁed bayesian analysis for example the conjugate prior for the parameters of the multinomial distribution is called the dirichlet distribution while the conjugate prior for the mean of a gaussian is another gaussian all of these distributions are examples of the exponential family of distributions which possess a number of important properties and which will be discussed in some detail one limitation of the parametric approach is that it assumes a speciﬁc functional form for the distribution which may turn out to be inappropriate for a particular application an alternative approach is given by nonparametric density estimation methods in which the form of the distribution typically depends on the size of the data set such models still contain parameters but these control the model complexity rather than the form of the distribution we end this chapter by considering three nonparametric methods based respectively on histograms nearest neighbours and kernels binary variables we begin by considering a single binary random variable x for example x might describe the outcome of ﬂipping a coin with x representing heads and x representing tails we can imagine that this is a damaged coin so that the probability of landing heads is not necessarily the same as that of landing tails the probability of x will be denoted by the parameter µ so that p x µ µ exercise where µ from which it follows that p x µ µ the probability distribution over x can therefore be written in the form bern x µ µx µ x which is known as the bernoulli distribution it is easily veriﬁed that this distribution is normalized and that it has mean and variance given by e x µ var x µ µ now suppose we have a data set xn of observed values of x we can construct the likelihood function which is a function of µ on the assumption that the observations are drawn independently from p x µ so that n n p d µ tt p xn µ tt µxn µ xn in a frequentist setting we can estimate a value for µ by maximizing the likelihood function or equivalently by maximizing the logarithm of the likelihood in the case of the bernoulli distribution the log likelihood function is given by n n ln p d µ ln p xn µ xn ln µ xn ln µ section at this point it is worth noting that the log likelihood function depends on the n observations xn only through their sum n xn this sum provides an example of a sufﬁcient statistic for the data under this distribution and we shall study the impor tant role of sufﬁcient statistics in some detail if we set the derivative of ln p µ with respect to µ equal to zero we obtain the maximum likelihood estimator µml n n xn n jacob bernoulli jacob bernoulli also known as jacques or james bernoulli was a swiss mathematician and was the ﬁrst of many in the bernoulli family to pursue a career in science and mathematics although compelled to study philosophy and theology against his will by his parents he travelled extensively after graduating in order to meet with many of the leading scientists of his time including boyle and hooke in england when he returned to switzerland he taught mechanics and became professor of mathematics at basel in unfortunately rivalry between jacob and his younger brother johann turned an initially productive collabora tion into a bitter and public dispute jacob most sig niﬁcant contributions to mathematics appeared in the art of conjecture published in eight years after his death which deals with topics in probability the ory including what has become known as the bernoulli distribution figure histogram plot of the binomial dis tribution as a function of m for n and µ m which is also known as the sample mean if we denote the number of observations of x heads within this data set by m then we can write in the form m µml n so that the probability of landing heads is given in this maximum likelihood frame work by the fraction of observations of heads in the data set now suppose we ﬂip a coin say times and happen to observe heads then n m and µml in this case the maximum likelihood result would predict that all future observations should give heads common sense tells us that this is unreasonable and in fact this is an extreme example of the over ﬁtting associ ated with maximum likelihood we shall see shortly how to arrive at more sensible conclusions through the introduction of a prior distribution over µ we can also work out the distribution of the number m of observations of x given that the data set has size n this is called the binomial distribution and from we see that it is proportional to µm µ n m in order to obtain the normalization coefﬁcient we note that out of n coin ﬂips we have to add up all of the possible ways of obtaining m heads so that the binomial distribution can be written where bin m n µ n µm µ n m n n m n m m exercise is the number of ways of choosing m objects out of a total of n identical objects figure shows a plot of the binomial distribution for n and µ the mean and variance of the binomial distribution can be found by using the result of exercise which shows that for independent events the mean of the sum is the sum of the means and the variance of the sum is the sum of the variances because m xn and for each observation the mean and variance are given by and respectively we have n e m mbin m n µ nµ m n exercise exercise var m m e m bin m n µ n µ µ m these results can also be proved directly using calculus the beta distribution we have seen in that the maximum likelihood setting for the parameter µ in the bernoulli distribution and hence in the binomial distribution is given by the fraction of the observations in the data set having x as we have already noted this can give severely over ﬁtted results for small data sets in order to develop a bayesian treatment for this problem we need to introduce a prior distribution p µ over the parameter µ here we consider a form of prior distribution that has a simple interpretation as well as some useful analytical properties to motivate this prior we note that the likelihood function takes the form of the product of factors of the form µx µ x if we choose a prior to be proportional to powers of µ and µ then the posterior distribution which is proportional to the product of the prior and the likelihood function will have the same functional form as the prior this property is called conjugacy and we will see several examples of it later in this chapter we therefore choose a prior called the beta distribution given by beta γ a b a b γ a γ b where γ x is the gamma function deﬁned by and the coefﬁcient in ensures that the beta distribution is normalized so that r exercise the mean and variance of the beta distribution are given by e µ a a b var µ ab a b a b the parameters a and b are often called hyperparameters because they control the distribution of the parameter µ figure shows plots of the beta distribution for various values of the hyperparameters the posterior distribution of µ is now obtained by multiplying the beta prior by the binomial likelihood function and normalizing keeping only the factors that depend on µ we see that this posterior distribution has the form p µ m l a b µm a µ l b µ µ µ µ figure plots of the beta distribution beta µ a b given by as a function of µ for various values of the hyperparameters a and b where l n m and therefore corresponds to the number of tails in the coin example we see that has the same functional dependence on µ as the prior distribution reﬂecting the conjugacy properties of the prior with respect to the like lihood function indeed it is simply another beta distribution and its normalization coefﬁcient can therefore be obtained by comparison with to give p µ m l a b γ m a l b µm a µ l b we see that the effect of observing a data set of m observations of x and l observations of x has been to increase the value of a by m and the value of b by l in going from the prior distribution to the posterior distribution this allows us to provide a simple interpretation of the hyperparameters a and b in the prior as an effective number of observations of x and x respectively note that a and b need not be integers furthermore the posterior distribution can act as the prior if we subsequently observe additional data to see this we can imagine taking observations one at a time and after each observation updating the current posterior figure illustration of one step of sequential bayesian inference the prior is given by a beta distribution with parameters a b and the likelihood function given by with n m corresponds to a single observation of x so that the posterior is given by a beta distribution with parameters a b section distribution by multiplying by the likelihood function for the new observation and then normalizing to obtain the new revised posterior distribution at each stage the posterior is a beta distribution with some total number of prior and actual observed values for x and x given by the parameters a and b incorporation of an additional observation of x simply corresponds to incrementing the value of a by whereas for an observation of x we increment b by figure illustrates one step in this process we see that this sequential approach to learning arises naturally when we adopt a bayesian viewpoint it is independent of the choice of prior and of the likelihood function and depends only on the assumption of i i d data sequential methods make use of observations one at a time or in small batches and then discard them before the next observations are used they can be used for example in real time learning scenarios where a steady stream of data is arriving and predictions must be made before all of the data is seen because they do not require the whole data set to be stored or loaded into memory sequential methods are also useful for large data sets maximum likelihood methods can also be cast into a sequential framework if our goal is to predict as best we can the outcome of the next trial then we must evaluate the predictive distribution of x given the observed data set from the sum and product rules of probability this takes the form p x d p x µ p µ d dµ µp µ d dµ e µ d using the result for the posterior distribution p µ together with the result for the mean of the beta distribution we obtain m a p x d m a l b which has a simple interpretation as the total fraction of observations both real ob servations and ﬁctitious prior observations that correspond to x note that in the limit of an inﬁnitely large data set m l the result reduces to the maximum likelihood result as we shall see it is a very general property that the bayesian and maximum likelihood results will agree in the limit of an inﬁnitely exercise exercise large data set for a ﬁnite data set the posterior mean for µ always lies between the prior mean and the maximum likelihood estimate for µ corresponding to the relative frequencies of events given by from figure we see that as the number of observations increases so the posterior distribution becomes more sharply peaked this can also be seen from the result for the variance of the beta distribution in which we see that the variance goes to zero for a or b in fact we might wonder whether it is a general property of bayesian learning that as we observe more and more data the uncertainty represented by the posterior distribution will steadily decrease to address this we can take a frequentist view of bayesian learning and show that on average such a property does indeed hold consider a general bayesian inference problem for a parameter θ for which we have observed a data set d de scribed by the joint distribution p θ d the following result eθ θ ed eθ θ d where eθ θ r p θ θ dθ ed eθ θ d r rr θp θ d p d dd says that the posterior mean of θ averaged over the distribution generating the data is equal to the prior mean of θ similarly we can show that varθ θ ed varθ θ d vard eθ θ d the term on the left hand side of is the prior variance of θ on the right hand side the ﬁrst term is the average posterior variance of θ and the second term measures the variance in the posterior mean of θ because this variance is a positive quantity this result shows that on average the posterior variance of θ is smaller than the prior variance the reduction in variance is greater if the variance in the posterior mean is greater note however that this result only holds on average and that for a particular observed data set it is possible for the posterior variance to be larger than the prior variance multinomial variables binary variables can be used to describe quantities that can take one of two possible values often however we encounter discrete variables that can take on one of k possible mutually exclusive states although there are various alternative ways to express such variables we shall see shortly that a particularly convenient represen tation is the of k scheme in which the variable is represented by a k dimensional vector x in which one of the elements xk equals and all remaining elements equal so for instance if we have a variable that can take k states and a particular observation of the variable happens to correspond to the state where then x will be represented by x t note that such vectors satisfy k xk if we denote the probability of xk by the parameter µk then the distribution of x is given k xk k where µ µk t and the parameters µk are constrained to satisfy µk and k µk because they represent probabilities the distribution can be regarded as a generalization of the bernoulli distribution to more than two outcomes it is easily seen that the distribution is normalized p x µ µk and that x k e x µ p x µ x µm t µ x now consider a data set of n independent observations xn the corresponding likelihood function takes the form n k k k p d µ tt tt µxnk tt pn xnk tt µmk we see that the likelihood function depends on the n data points only through the k quantities section appendix e mk xnk n which represent the number of observations of xk these are called the sufﬁcient statistics for this distribution in order to ﬁnd the maximum likelihood solution for µ we need to maximize ln p µ with respect to µk taking account of the constraint that the µk must sum to one this can be achieved using a lagrange multiplier λ and maximizing k mk ln µk λ k k µk setting the derivative of with respect to µk to zero we obtain µk mk λ we can solve for the lagrange multiplier λ by substituting into the constraint the form µml mk n which is the fraction of the n observations for which xk we can consider the joint distribution of the quantities mk conditioned on the parameters µ and on the total number n of observations from this takes the form mult mk µ n n mk k mk k k which is known as the multinomial distribution the normalization coefﬁcient is the number of ways of partitioning n objects into k groups of size mk and is given by n mk n mk note that the variables mk are subject to the constraint k mk n k the dirichlet distribution we now introduce a family of prior distributions for the parameters µk of the multinomial distribution by inspection of the form of the multinomial distribution we see that the conjugate prior is given by k αk k exercise k where µk and k µk here αk are the parameters of the distribution and α denotes αk t note that because of the summation constraint the distribution over the space of the µk is conﬁned to a simplex of dimensionality k as illustrated for k in figure the normalized form for this distribution is by dir µ α γ tt µαk which is called the dirichlet distribution here γ x is the gamma function deﬁned by while k αk k figure the dirichlet distribution over three variables is conﬁned to a simplex a bounded linear manifold of the form shown as a consequence of the constraints µk and k µk plots of the dirichlet distribution over the simplex for various settings of the param eters αk are shown in figure multiplying the prior by the likelihood function we obtain the posterior distribution for the parameters µk in the form p µ d α p d µ p µ α tt µαk mk k we see that the posterior distribution again takes the form of a dirichlet distribution conﬁrming that the dirichlet is indeed a conjugate prior for the multinomial this allows us to determine the normalization coefﬁcient by comparison with so that p µ d α dir µ α m k γ n tt µαk mk where we have denoted m mk t as for the case of the binomial distribution with its beta prior we can interpret the parameters αk of the dirichlet prior as an effective number of observations of xk note that two state quantities can either be represented as binary variables and lejeune dirichlet johann peter gustav lejeune dirichlet was a modest and re served mathematician who made contributions in number theory me chanics and astronomy and who gave the ﬁrst rigorous analysis of fourier series his family originated from richelet in belgium and the name lejeune dirichlet comes from le jeune de richelet the young person from richelet dirichlet ﬁrst paper which was published in brought him instant fame it concerned fer mat last theorem which claims that there are no positive integer solutions to xn yn zn for n dirichlet gave a partial proof for the case n which was sent to legendre for review and who in turn com pleted the proof later dirichlet gave a complete proof for n although a full proof of fermat last theo rem for arbitrary n had to wait until the work of andrew wiles in the closing years of the century figure plots of the dirichlet distribution over three variables where the two horizontal axes are coordinates in the plane of the simplex and the vertical axis corresponds to the value of the density here αk on the left plot αk in the centre plot and αk in the right plot modelled using the binomial distribution or as of variables and modelled using the multinomial distribution with k the gaussian distribution the gaussian also known as the normal distribution is a widely used model for the distribution of continuous variables in the case of a single variable x the gaussian distribution can be written in the form n x µ σ exp x µ where µ is the mean and is the variance for a d dimensional vector x the multivariate gaussian distribution takes the form n x µ σ exp x µ tς x µ d σ section exercise where µ is a d dimensional mean vector σ is a d d covariance matrix and σ denotes the determinant of σ the gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives for example we have already seen that for a single real variable the distribution that maximizes the entropy is the gaussian this property applies also to the multivariate gaussian another situation in which the gaussian distribution arises is when we consider the sum of multiple random variables the central limit theorem due to laplace tells us that subject to certain mild conditions the sum of a set of random variables which is of course itself a random variable has a distribution that becomes increas ingly gaussian as the number of terms in the sum increases walker we can figure histogram plots of the mean of n uniformly distributed numbers for various values of n we observe that as n increases the distribution tends towards a gaussian appendix c illustrate this by considering n variables xn each of which has a uniform distribution over the interval and then considering the distribution of the mean xn n for large n this distribution tends to a gaussian as illustrated in figure in practice the convergence to a gaussian as n increases can be very rapid one consequence of this result is that the binomial distribution which is a distribution over m deﬁned by the sum of n observations of the random binary variable x will tend to a gaussian as n see figure for the case of n the gaussian distribution has many important analytical properties and we shall consider several of these in detail as a result this section will be rather more tech nically involved than some of the earlier sections and will require familiarity with various matrix identities however we strongly encourage the reader to become pro ﬁcient in manipulating gaussian distributions using the techniques presented here as this will prove invaluable in understanding the more complex models presented in later chapters we begin by considering the geometrical form of the gaussian distribution the carl friedrich gauss it is said that when gauss went to elementary school at age his teacher bu ttner trying to keep the class occupied asked the pupils to sum the integers from to to the teacher amazement gauss arrived at the answer in a matter of moments by noting that the sum can be represented as pairs etc each of which added to giving the an swer it is now believed that the problem which was actually set was of the same form but somewhat harder in that the sequence had a larger starting value and a larger increment gauss was a german math ematician and scientist with a reputation for being a hard working perfectionist one of his many contribu tions was to show that least squares can be derived under the assumption of normally distributed errors he also created an early formulation of non euclidean geometry a self consistent geometrical theory that vi olates the axioms of euclid but was reluctant to dis cuss it openly for fear that his reputation might suffer if it were seen that he believed in such a geometry at one point gauss was asked to conduct a geodetic survey of the state of hanover which led to his for mulation of the normal distribution now also known as the gaussian after his death a study of his di aries revealed that he had discovered several impor tant mathematical results years or even decades be fore they were published by others exercise exercise functional dependence of the gaussian on x is through the quadratic form x µ tς x µ which appears in the exponent the quantity is called the mahalanobis distance from µ to x and reduces to the euclidean distance when σ is the identity matrix the gaussian distribution will be constant on surfaces in x space for which this quadratic form is constant first of all we note that the matrix σ can be taken to be symmetric without loss of generality because any antisymmetric component would disappear from the exponent now consider the eigenvector equation for the covariance matrix σui λiui where i d because σ is a real symmetric matrix its eigenvalues will be real and its eigenvectors can be chosen to form an orthonormal set so that utuj iij where iij is the i j element of the identity matrix and satisﬁes if i j otherwise exercise the covariance matrix σ can be expressed as an expansion in terms of its eigenvec tors in the form d t i and similarly the inverse covariance matrix σ can be expressed as σ u ut substituting into the quadratic form becomes d yi λi where we have deﬁned i yi ut x µ we can interpret yi as a new coordinate system deﬁned by the orthonormal vectors ui that are shifted and rotated with respect to the original xi coordinates forming the vector y yd t we have y u x µ figure the red curve shows the ellip tical surface of constant proba bility density for a gaussian in a two dimensional space x on which the density is exp of its value at x µ the major axes of the ellipse are deﬁned by the eigenvectors ui of the covari ance matrix with correspond ing eigenvalues λi appendix c where u is a matrix whose rows are given by ut from it follows that u is an orthogonal matrix i e it satisﬁes uut i and hence also utu i where i is the identity matrix the quadratic form and hence the gaussian density will be constant on surfaces for which is constant if all of the eigenvalues λi are positive then these surfaces represent ellipsoids with their centres at µ and their axes oriented along ui and with scaling factors in the directions of the axes given by as illustrated in figure for the gaussian distribution to be well deﬁned it is necessary for all of the eigenvalues λi of the covariance matrix to be strictly positive otherwise the dis tribution cannot be properly normalized a matrix whose eigenvalues are strictly positive is said to be positive deﬁnite in chapter we will encounter gaussian distributions for which one or more of the eigenvalues are zero in which case the distribution is singular and is conﬁned to a subspace of lower dimensionality if all of the eigenvalues are nonnegative then the covariance matrix is said to be positive semideﬁnite now consider the form of the gaussian distribution in the new coordinate system deﬁned by the yi in going from the x to the y coordinate system we have a jacobian matrix j with elements given by jij xi u yj ji where uji are the elements of the matrix ut using the orthonormality property of the matrix u we see that the square of the determinant of the jacobian matrix is j ut ut u utu i and hence j also the determinant σ of the covariance matrix can be written as the product of its eigenvalues and hence d j j thus in the yj coordinate system the gaussian distribution takes the form p y p x j jtt exp which is the product of d independent univariate gaussian distributions the eigen vectors therefore deﬁne a new set of shifted and rotated coordinates with respect to which the joint probability distribution factorizes into a product of independent distributions the integral of the distribution in the y coordinate system is then r p y dy tt r exp r dyj j 2λj where we have used the result for the normalization of the univariate gaussian this conﬁrms that the multivariate gaussian is indeed normalized we now look at the moments of the gaussian distribution and thereby provide an interpretation of the parameters µ and σ the expectation of x under the gaussian distribution is given by e x r exp r x µ tς x µ x dx r exp r ztς z µ dz where we have changed variables using z x µ we now note that the exponent is an even function of the components of z and because the integrals over these are taken over the range the term in z in the factor z µ will vanish by symmetry thus e x µ and so we refer to µ as the mean of the gaussian distribution we now consider second order moments of the gaussian in the univariate case we considered the second order moment given by e for the multivariate gaus sian there are second order moments given by e xixj which we can group together to form the matrix e xxt this matrix can be written as e xxt r exp r x µ tς x µ xxt dx r exp r ztς z µ z µ t dz where again we have changed variables using z x µ note that the cross terms involving µzt and µtz will again vanish by symmetry the term µµt is constant and can be taken outside the integral which itself is unity because the gaussian distribution is normalized consider the term involving zzt again we can make use of the eigenvector expansion of the covariance matrix given by together with the completeness of the set of eigenvectors to write where yj utz which gives d z yjuj j r exp r ztς zzt dz d σ r π d σ d j i j λk k uiutλi σ i where we have made use of the eigenvector equation together with the fact that the integral on the right hand side of the middle line vanishes by symmetry unless i j and in the ﬁnal line we have made use of the results and together with thus we have e xxt µµt σ for single random variables we subtracted the mean before taking second mo ments in order to deﬁne a variance similarly in the multivariate case it is again convenient to subtract off the mean giving rise to the covariance of a random vector x deﬁned by exercise cov x e x e x x e x t for the speciﬁc case of a gaussian distribution we can make use of e x µ together with the result to give cov x σ because the parameter matrix σ governs the covariance of x under the gaussian distribution it is called the covariance matrix although the gaussian distribution is widely used as a density model it suffers from some signiﬁcant limitations consider the number of free parameters in the distribution a general symmetric covariance matrix σ will have d d independent parameters and there are another d independent parameters in µ giv ing d d parameters in total for large d the total number of parameters figure contours of constant probability density for a gaussian distribution in two dimensions in which the covariance matrix is a of general form b diagonal in which the elliptical contours are aligned with the coordinate axes and c proportional to the identity matrix in which the contours are concentric circles a b c section section therefore grows quadratically with d and the computational task of manipulating and inverting large matrices can become prohibitive one way to address this prob lem is to use restricted forms of the covariance matrix if we consider covariance matrices that are diagonal so that σ diag we then have a total of inde pendent parameters in the density model the corresponding contours of constant density are given by axis aligned ellipsoids we could further restrict the covariance matrix to be proportional to the identity matrix σ known as an isotropic co variance giving d independent parameters in the model and spherical surfaces of constant density the three possibilities of general diagonal and isotropic covari ance matrices are illustrated in figure unfortunately whereas such approaches limit the number of degrees of freedom in the distribution and make inversion of the covariance matrix a much faster operation they also greatly restrict the form of the probability density and limit its ability to capture interesting correlations in the data a further limitation of the gaussian distribution is that it is intrinsically uni modal i e has a single maximum and so is unable to provide a good approximation to multimodal distributions thus the gaussian distribution can be both too ﬂexible in the sense of having too many parameters while also being too limited in the range of distributions that it can adequately represent we will see later that the introduc tion of latent variables also called hidden variables or unobserved variables allows both of these problems to be addressed in particular a rich family of multimodal distributions is obtained by introducing discrete latent variables leading to mixtures of gaussians as discussed in section similarly the introduction of continuous latent variables as described in chapter leads to models in which the number of free parameters can be controlled independently of the dimensionality d of the data space while still allowing the model to capture the dominant correlations in the data set indeed these two approaches can be combined and further extended to derive a very rich set of hierarchical models that can be adapted to a broad range of prac tical applications for instance the gaussian version of the markov random ﬁeld which is widely used as a probabilistic model of images is a gaussian distribution over the joint space of pixel intensities but rendered tractable through the imposition of considerable structure reﬂecting the spatial organization of the pixels similarly the linear dynamical system used to model time series data for applications such as tracking is also a joint gaussian distribution over a potentially large number of observed and latent variables and again is tractable due to the structure imposed on the distribution a powerful framework for expressing the form and properties of such complex distributions is that of probabilistic graphical models which will form the subject of chapter conditional gaussian distributions an important property of the multivariate gaussian distribution is that if two sets of variables are jointly gaussian then the conditional distribution of one set conditioned on the other is again gaussian similarly the marginal distribution of either set is also gaussian consider ﬁrst the case of conditional distributions suppose x is a d dimensional vector with gaussian distribution x µ σ and that we partition x into two dis joint subsets xa and xb without loss of generality we can take xa to form the ﬁrst m components of x with xb comprising the remaining d m components so that x xa we also deﬁne corresponding partitions of the mean vector µ given by µ µa µb and of the covariance matrix σ given by σ σaa σab note that the symmetry σt σ of the covariance matrix implies that σaa and σbb are symmetric while σba σt in many situations it will be convenient to work with the inverse of the covari ance matrix exercise λ σ which is known as the precision matrix in fact we shall see that some properties of gaussian distributions are most naturally expressed in terms of the covariance whereas others take a simpler form when viewed in terms of the precision we therefore also introduce the partitioned form of the precision matrix λ λaa λab corresponding to the partitioning of the vector x because the inverse of a symmetric matrix is also symmetric we see that λaa and λbb are symmetric while t λba it should be stressed at this point that for instance λaa is not simply given by the inverse of σaa in fact we shall shortly examine the relation between the inverse of a partitioned matrix and the inverses of its partitions let us begin by ﬁnding an expression for the conditional distribution p xa xb from the product rule of probability we see that this conditional distribution can be evaluated from the joint distribution p x p xa xb simply by ﬁxing xb to the observed value and normalizing the resulting expression to obtain a valid probability distribution over xa instead of performing this normalization explicitly we can obtain the solution more efﬁciently by considering the quadratic form in the exponent of the gaussian distribution given by and then reinstating the normalization coefﬁcient at the end of the calculation if we make use of the partitioning and we obtain x µ tς x µ xa µa tλ x µ x µ tλab xb µb xb µb tλ x µ x µ tλbb xb µb we see that as a function of xa this is again a quadratic form and hence the cor responding conditional distribution p xa xb will be gaussian because this distri bution is completely characterized by its mean and its covariance our goal will be to identify expressions for the mean and covariance of p xa xb by inspection of this is an example of a rather common operation associated with gaussian distributions sometimes called completing the square in which we are given a quadratic form deﬁning the exponent terms in a gaussian distribution and we need to determine the corresponding mean and covariance such problems can be solved straightforwardly by noting that the exponent in a general gaussian distribution n x µ σ can be written x µ tς x µ xtς xtς const where const denotes terms which are independent of x and we have made use of the symmetry of σ thus if we take our general quadratic form and express it in the form given by the right hand side of then we can immediately equate the matrix of coefﬁcients entering the second order term in x to the inverse covariance matrix σ and the coefﬁcient of the linear term in x to σ from which we can obtain µ now let us apply this procedure to the conditional gaussian distribution p xa xb for which the quadratic form in the exponent is given by we will denote the mean and covariance of this distribution by µa b and σa b respectively consider the functional dependence of on xa in which xb is regarded as a constant if we pick out all terms that are second order in xa we have t xa λaaxa from which we can immediately conclude that the covariance inverse precision of p xa xb is given by σ a b λ now consider all of the terms in that are linear in xa xt λaaµa λab xb µb where we have used λt λab from our discussion of the general form the coefﬁcient of xa in this expression must equal σ b and hence µa b σa b λaaµa λab xb µb µa λ xb µb exercise where we have made use of the results and are expressed in terms of the partitioned precision matrix of the original joint distribution p xa xb we can also express these results in terms of the corresponding partitioned covariance matrix to do this we make use of the following identity for the inverse of a partitioned matrix a b c d where we have deﬁned m mbd d d d m a bd the quantity m is known as the schur complement of the matrix on the left hand side of with respect to the submatrix d using the deﬁnition σaa σab λaa λab σba σbb and making use of we have λba λbb λaa σaa σabς λab σaa σabς section from these we obtain the following expressions for the mean and covariance of the conditional distribution p xa xb µa b µa σabς xb µb σa b σaa σabς comparing and we see that the conditional distribution p xa xb takes a simpler form when expressed in terms of the partitioned precision matrix than when it is expressed in terms of the partitioned covariance matrix note that the mean of the conditional distribution p xa xb given by is a linear function of xb and that the covariance given by is independent of xa this represents an example of a linear gaussian model marginal gaussian distributions we have seen that if a joint distribution p xa xb is gaussian then the condi tional distribution p xa xb will again be gaussian now we turn to a discussion of the marginal distribution given by p xa r p xa xb dxb which as we shall see is also gaussian once again our strategy for evaluating this distribution efﬁciently will be to focus on the quadratic form in the exponent of the joint distribution and thereby to identify the mean and covariance of the marginal distribution p xa the quadratic form for the joint distribution can be expressed using the par titioned precision matrix in the form because our goal is to integrate out xb this is most easily achieved by ﬁrst considering the terms involving xb and then completing the square in order to facilitate integration picking out just those terms that involve xb we have x λ x xt m x λ tλ x λ mtλ where we have deﬁned m λbbµb λba xa µa we see that the dependence on xb has been cast into the standard quadratic form of a gaussian distribution corresponding to the ﬁrst term on the right hand side of plus a term that does not depend on xb but that does depend on xa thus when we take the exponential of this quadratic form we see that the integration over xb required by will take the form r exp r x λ tλ x λ dx this integration is easily performed by noting that it is the integral over an unnor malized gaussian and so the result will be the reciprocal of the normalization co efﬁcient we know from the form of the normalized gaussian given by that this coefﬁcient is independent of the mean and depends only on the determinant of the covariance matrix thus by completing the square with respect to xb we can integrate out xb and the only term remaining from the contributions on the left hand side of that depends on xa is the last term on the right hand side of in which m is given by combining this term with the remaining terms from that depend on xa we obtain λ µ bb b λba xa µa t λ λbbµ λba xa µa t t xa λaaxa xa λaaµa λabµb const xt λ λ λ x xt λaa λabλ const where const denotes quantities independent of xa again by comparison with we see that the covariance of the marginal distribution of p xa is given by σa λaa λabλ similarly the mean is given by σa λaa λabλ µa µa where we have used the covariance in is expressed in terms of the partitioned precision matrix given by we can rewrite this in terms of the corresponding partitioning of the covariance matrix given by as we did for the conditional distribution these partitioned matrices are related by λaa λab σaa σab λba λbb making use of we then have σba σbb λaa λab λ σ thus we obtain the intuitively satisfying result that the marginal distribution p xa has mean and covariance given by e xa µa cov xa σaa we see that for a marginal distribution the mean and covariance are most simply ex pressed in terms of the partitioned covariance matrix in contrast to the conditional distribution for which the partitioned precision matrix gives rise to simpler expres sions our results for the marginal and conditional distributions of a partitioned gaus sian are summarized below partitioned gaussians given a joint gaussian distribution n x µ σ with λ σ and x xa µ µa xb xa xa figure the plot on the left shows the contours of a gaussian distribution p xa xb over two variables and the plot on the right shows the marginal distribution p xa blue curve and the conditional distribution p xa xb for xb red curve σba σbb conditional distribution λba λbb p xa xb n x µa b λ µa b µa λ xb µb marginal distribution p xa n xa µa σaa we illustrate the idea of conditional and marginal distributions associated with a multivariate gaussian using an example involving two variables in figure bayes theorem for gaussian variables in sections and we considered a gaussian p x in which we parti tioned the vector x into two subvectors x xa xb and then found expressions for the conditional distribution p xa xb and the marginal distribution p xa we noted that the mean of the conditional distribution p xa xb was a linear function of xb here we shall suppose that we are given a gaussian marginal distribution p x and a gaussian conditional distribution p y x in which p y x has a mean that is a linear function of x and a covariance which is independent of x this is an example of a linear gaussian model roweis and ghahramani which we shall study in greater generality in section we wish to ﬁnd the marginal distribution p y and the conditional distribution p x y this is a problem that will arise frequently in subsequent chapters and it will prove convenient to derive the general results here we shall take the marginal and conditional distributions to be p x n x µ λ where µ a and b are parameters governing the means and λ and l are precision matrices if x has dimensionality m and y has dimensionality d then the matrix a has size d m first we ﬁnd an expression for the joint distribution over x and y to do this we deﬁne z x and then consider the log of the joint distribution ln p z ln p x ln p y x x µ tλ x µ y ax b t l y ax b const where const denotes terms independent of x and y as before we see that this is a quadratic function of the components of z and hence p z is gaussian distribution to ﬁnd the precision of this gaussian we consider the second order terms in which can be written as xt λ at la x yt ly yt lax xt atly x t λ atla atl x and so the gaussian distribution over z has precision inverse covariance matrix given by λ atla atl la l exercise the covariance matrix is found by taking the inverse of the precision which can be done using the matrix inversion formula to give λ λ aλ l aλ similarly we can ﬁnd the mean of the gaussian distribution over z by identify ing the linear terms in which are given by x t λµ atlb using our earlier result obtained by completing the square over the quadratic form of a multivariate gaussian we ﬁnd that the mean of z is given by e z r λµ atlb exercise making use of we then obtain e z a µ b section next we ﬁnd an expression for the marginal distribution p y in which we have marginalized over x recall that the marginal distribution over a subset of the com ponents of a gaussian random vector takes a particularly simple form when ex pressed in terms of the partitioned covariance matrix speciﬁcally its mean and covariance are given by and respectively making use of and we see that the mean and covariance of the marginal distribution p y are given by section e y aµ b cov y l aλ a special case of this result is when a i in which case it reduces to the convolu tion of two gaussians for which we see that the mean of the convolution is the sum of the mean of the two gaussians and the covariance of the convolution is the sum of their covariances finally we seek an expression for the conditional p x y recall that the results for the conditional distribution are most easily expressed in terms of the partitioned precision matrix using and applying these results to and we see that the conditional distribution p x y has mean and covariance given by e x y λ atla atl y b λµ cov x y λ atla the evaluation of this conditional can be seen as an example of bayes theorem we can interpret the distribution p x as a prior distribution over x if the variable y is observed then the conditional distribution p x y represents the corresponding posterior distribution over x having found the marginal and conditional distribu tions we effectively expressed the joint distribution p z p x p y x in the form p x y p y these results are summarized below marginal and conditional gaussians given a marginal gaussian distribution for x and a conditional gaussian distri bution for y given x in the form p x n x µ λ p y x n y ax b l the marginal distribution of y and the conditional distribution of x given y are given by where p y n y aµ b l aλ p x y n x σ atl y b λµ σ σ λ atla maximum likelihood for the gaussian given a data set x xn t in which the observations xn are as sumed to be drawn independently from a multivariate gaussian distribution we can estimate the parameters of the distribution by maximum likelihood the log likeli hood function is given by ln p x µ σ nd ln n ln σ x µ tς x µ by simple rearrangement we see that the likelihood function depends on the data set only through the two quantities xn t n n appendix c these are known as the sufﬁcient statistics for the gaussian distribution using c the derivative of the log likelihood with respect to µ is given by n ln p x µ σ σ x µ n n µ and setting this derivative to zero we obtain the solution for the maximum likelihood estimate of the mean given by µml n n xn n exercise which is the mean of the observed set of data points the maximization of with respect to σ is rather more involved the simplest approach is to ignore the symmetry constraint and show that the resulting solution is symmetric as required alternative derivations of this result which impose the symmetry and positive deﬁ niteness constraints explicitly can be found in magnus and neudecker the result is as expected and takes the form σml n n xn n µml xn µml t exercise which involves µml because this is the result of a joint maximization with respect to µ and σ note that the solution for µml does not depend on σml and so we can ﬁrst evaluate µml and then use this to evaluate σml if we evaluate the expectations of the maximum likelihood solutions under the true distribution we obtain the following results e µml µ e σml n σ n we see that the expectation of the maximum likelihood estimate for the mean is equal to the true mean however the maximum likelihood estimate for the covariance has an expectation that is less than the true value and hence it is biased we can correct this bias by deﬁning a different estimator σ given by n σ x n n µml xn µml t clearly from and the expectation of σ is equal to σ sequential estimation our discussion of the maximum likelihood solution for the parameters of a gaus sian distribution provides a convenient opportunity to give a more general discussion of the topic of sequential estimation for maximum likelihood sequential methods allow data points to be processed one at a time and then discarded and are important for on line applications and also where large data sets are involved so that batch processing of all data points at once is infeasible consider the result for the maximum likelihood estimator of the mean µml which we will denote by µ n when it is based on n observations if we figure a schematic illustration of two correlated ran dom variables z and θ together with the regression function f θ given by the con ditional expectation e z θ the robbins monro algorithm provides a general sequen tial procedure for ﬁnding the root θ of such functions dissect out the contribution from the ﬁnal data point xn we obtain n ml n n xn n n x n n n xn n x n µ n n n n ml µ n x µ n this result has a nice interpretation as follows after observing n data points we have estimated µ by µ n we now observe data point xn and we obtain our revised estimate µ n by moving the old estimate a small amount proportional to n in the direction of the error signal xn µ n note that as n increases so the contribution from successive data points gets smaller the result will clearly give the same answer as the batch result because the two formulae are equivalent however we will not always be able to de rive a sequential algorithm by this route and so we seek a more general formulation of sequential learning which leads us to the robbins monro algorithm consider a pair of random variables θ and z governed by a joint distribution p z θ the con ditional expectation of z given θ deﬁnes a deterministic function f θ that is given by f θ e z θ r zp z θ dz and is illustrated schematically in figure functions deﬁned in this way are called regression functions our goal is to ﬁnd the root θ at which f θ if we had a large data set of observations of z and θ then we could model the regression function directly and then obtain an estimate of its root suppose however that we observe values of z one at a time and we wish to ﬁnd a corresponding sequential estimation scheme for θ the following general procedure for solving such problems was given by robbins and monro we shall assume that the conditional variance of z is ﬁnite so that e z f θ and we shall also without loss of generality consider the case where f θ for θ θ and f θ for θ θ as is the case in figure the robbins monro procedure then deﬁnes a sequence of successive estimates of the root θ given by θ n θ n an θ n where z θ n is an observed value of z when θ takes the value θ n the coefﬁcients an represent a sequence of positive numbers that satisfy the conditions lim n an an n n it can then be shown robbins and monro fukunaga that the sequence of estimates given by does indeed converge to the root with probability one note that the ﬁrst condition ensures that the successive corrections decrease in magnitude so that the process can converge to a limiting value the second con dition is required to ensure that the algorithm does not converge short of the root and the third condition is needed to ensure that the accumulated noise has ﬁnite variance and hence does not spoil convergence now let us consider how a general maximum likelihood problem can be solved sequentially using the robbins monro algorithm by deﬁnition the maximum like lihood solution θml is a stationary point of the log likelihood function and hence satisﬁes θ n n ln p xn θ θml exchanging the derivative and the summation and taking the limit n we have lim ln p xn θ ex ln p x θ n n n θ θ and so we see that ﬁnding the maximum likelihood solution corresponds to ﬁnd ing the root of a regression function we can therefore apply the robbins monro procedure which now takes the form θ n θ n a ln p x θ n n θ n n figure in the case of a gaussian distribution with θ z corresponding to the mean µ the regression function illustrated in figure takes the form of a straight line as shown in red in this case the random variable z corresponds to the derivative of the log likelihood function and is given by x µml and its expectation that deﬁnes the regression function is a straight line given by µ µml the root of the regres sion function corresponds to the maximum like lihood estimator µml µml p z µ µ as a speciﬁc example we consider once again the sequential estimation of the mean of a gaussian distribution in which case the parameter θ n is the estimate µ n of the mean of the gaussian and the random variable z is given by z ln p x µ x µ µml ml ml thus the distribution of z is gaussian with mean µ µml as illustrated in fig ure substituting into we obtain the univariate form of provided we choose the coefﬁcients an to have the form an n note that although we have focussed on the case of a single variable the same technique together with the same restrictions 132 on the coefﬁcients an apply equally to the multivariate case blum bayesian inference for the gaussian the maximum likelihood framework gave point estimates for the parameters µ and σ now we develop a bayesian treatment by introducing prior distributions over these parameters let us begin with a simple example in which we consider a single gaussian random variable x we shall suppose that the variance is known and we consider the task of inferring the mean µ given a set of n observations x xn the likelihood function that is the probability of the observed data given µ viewed as a function of µ is given by tt again we emphasize that the likelihood function p x µ is not a probability distri bution over µ and is not normalized we see that the likelihood function takes the form of the exponential of a quad ratic form in µ thus if we choose a prior p µ given by a gaussian it will be a conjugate distribution for this likelihood function because the corresponding poste rior will be a product of two exponentials of quadratic functions of µ and hence will also be gaussian we therefore take our prior distribution to be p µ n µ exercise and the posterior distribution is given by p µ x p x µ p µ simple manipulation involving completing the square in the exponent shows that the posterior distribution is given by p µ x n µ µn where µn µml n n in which µml is the maximum likelihood solution for µ given by the sample mean µml n n xn n it is worth spending a moment studying the form of the posterior mean and variance first of all we note that the mean of the posterior distribution given by is a compromise between the prior mean and the maximum likelihood solution µml if the number of observed data points n then reduces to the prior mean as expected for n the posterior mean is given by the maximum likelihood solution similarly consider the result for the variance of the posterior distribution we see that this is most naturally expressed in terms of the inverse variance which is called the precision furthermore the precisions are additive so that the precision of the posterior is given by the precision of the prior plus one contribution of the data precision from each of the observed data points as we increase the number of observed data points the precision steadily increases corresponding to a posterior distribution with steadily decreasing variance with no observed data points we have the prior variance whereas if the number of data points n the variance goes to zero and the posterior distribution becomes inﬁnitely peaked around the maximum likelihood solution we therefore see that the maximum likelihood result of a point estimate for µ given by is recovered precisely from the bayesian formalism in the limit of an inﬁnite number of observations note also that for ﬁnite n if we take the limit in which the prior has inﬁnite variance then the posterior mean reduces to the maximum likelihood result while from the posterior variance is given by n figure illustration of bayesian inference for the mean µ of a gaussian distri bution in which the variance is as sumed to be known the curves show the prior distribution over µ the curve labelled n which in this case is itself gaussian along with the posterior distribution given by for increasing numbers n of data points the data points are generated from a gaussian of mean and variance and the prior is chosen to have mean in both the prior and the likelihood function the variance is set to the true value exercise section we illustrate our analysis of bayesian inference for the mean of a gaussian distribution in figure the generalization of this result to the case of a d dimensional gaussian random variable x with known covariance and unknown mean is straightforward we have already seen how the maximum likelihood expression for the mean of a gaussian can be re cast as a sequential update formula in which the mean after observing n data points was expressed in terms of the mean after observing n data points together with the contribution from data point xn in fact the bayesian paradigm leads very naturally to a sequential view of the inference problem to see this in the context of the inference of the mean of a gaussian we write the posterior distribution with the contribution from the ﬁnal data point xn separated out so that p µ d p µ n n p xn µ l p xn µ the term in square brackets is up to a normalization coefﬁcient just the posterior distribution after observing n data points we see that this can be viewed as a prior distribution which is combined using bayes theorem with the likelihood function associated with data point xn to arrive at the posterior distribution after observing n data points this sequential view of bayesian inference is very general and applies to any problem in which the observed data are assumed to be independent and identically distributed so far we have assumed that the variance of the gaussian distribution over the data is known and our goal is to infer the mean now let us suppose that the mean is known and we wish to infer the variance again our calculations will be greatly simpliﬁed if we choose a conjugate form for the prior distribution it turns out to be most convenient to work with the precision λ the likelihood function for λ takes the form p x λ ntt n xn µ λ λn exp n λ n xn µ λ λ λ figure plot of the gamma distribution gam λ a b deﬁned by for various values of the parameters a and b exercise exercise the corresponding conjugate prior should therefore be proportional to the product of a power of λ and the exponential of a linear function of λ this corresponds to the gamma distribution which is deﬁned by gam λ a b baλa exp bλ here γ a is the gamma function that is deﬁned by and that ensures that is correctly normalized the gamma distribution has a ﬁnite integral if a and the distribution itself is ﬁnite if a it is plotted for various values of a and b in figure the mean and variance of the gamma distribution are given by e λ a b var λ a consider a prior distribution gam λ if we multiply by the likelihood function then we obtain a posterior distribution λ which we recognize as a gamma distribution of the form gam λ an bn where n an n b b x µ b n where is the maximum likelihood estimator of the variance note that in there is no need to keep track of the normalization constants in the prior and the likelihood function because if required the correct coefﬁcient can be found at the end using the normalized form for the gamma distribution from we see that the effect of observing n data points is to increase the value of the coefﬁcient a by n thus we can interpret the parameter in the prior in terms of effective prior observations similarly from we see that the n data points contribute to the parameter b where is section the variance and so we can interpret the parameter in the prior as arising from the effective prior observations having variance recall that we made an analogous interpretation for the dirichlet prior these distributions are examples of the exponential family and we shall see that the interpretation of a conjugate prior in terms of effective ﬁctitious data points is a general one for the exponential family of distributions instead of working with the precision we can consider the variance itself the conjugate prior in this case is called the inverse gamma distribution although we shall not discuss this further because we will ﬁnd it more convenient to work with the precision now suppose that both the mean and the precision are unknown to ﬁnd a conjugate prior we consider the dependence of the likelihood function on µ and λ tt λ r λ exp n exp λµ n n λ n n n we now wish to identify a prior distribution p µ λ that has the same functional dependence on µ and λ as the likelihood function and that should therefore take the form p µ λ exp β exp cλµ dλ exp r βλ β exp r where c d and β are constants since we can always write p µ λ p µ λ p λ we can ﬁnd p µ λ and p λ by inspection in particular we see that p µ λ is a gaussian whose precision is a linear function of λ and that p λ is a gamma distri bution so that the normalized prior takes the form p µ λ n µ βλ gam λ a b where we have deﬁned new constants given by c β a β b d the distribution is called the normal gamma or gaussian gamma distribution and is plotted in figure note that this is not simply the product of an independent gaussian prior over µ and a gamma prior over λ because the precision of µ is a linear function of λ even if we chose a prior in which µ and λ were independent the posterior distribution would exhibit a coupling between the precision of µ and the value of λ figure contour plot of the normal gamma distribution for parameter values β a and b λ exercise µ in the case of the multivariate gaussian distribution x µ λ for a d dimensional variable x the conjugate prior distribution for the mean µ assuming the precision is known is again a gaussian for known mean and unknown precision matrix λ the conjugate prior is the wishart distribution given by w λ w ν b λ ν d exp tr w where ν is called the number of degrees of freedom of the distribution w is a d d scale matrix and tr denotes the trace the normalization constant b is given by b w ν w ν πd d tti ν i section exercise again it is also possible to deﬁne a conjugate prior over the covariance matrix itself rather than over the precision matrix which leads to the inverse wishart distribu tion although we shall not discuss this further if both the mean and the precision are unknown then following a similar line of reasoning to the univariate case the conjugate prior is given by p µ λ β w ν n µ βλ w λ w ν which is known as the normal wishart or gaussian wishart distribution student t distribution we have seen that the conjugate prior for the precision of a gaussian is given by a gamma distribution if we have a univariate gaussian x µ τ together with a gamma prior gam τ a b and we integrate out the precision we obtain the marginal distribution of x in the form figure plot of student t distribution for µ and λ for various values of ν the limit ν corresponds to a gaussian distribution with mean µ and precision λ p x µ a b r n x µ τ gam τ a b dτ r bae bτ τa τ exp j τ d ba x µ a where we have made the change of variable z τ b x µ by convention we deﬁne new parameters given by ν and λ a b in terms of which the distribution p x µ a b takes the form γ ν λ λ x µ ν exercise exercise which is known as student t distribution the parameter λ is sometimes called the precision of the t distribution even though it is not in general equal to the inverse of the variance the parameter ν is called the degrees of freedom and its effect is illustrated in figure for the particular case of ν the t distribution reduces to the cauchy distribution while in the limit ν the t distribution st x µ λ ν becomes a gaussian x µ λ with mean µ and precision λ from we see that student t distribution is obtained by adding up an inﬁnite number of gaussian distributions having the same mean but different preci sions this can be interpreted as an inﬁnite mixture of gaussians gaussian mixtures will be discussed in detail in section the result is a distribution that in gen eral has longer tails than a gaussian as was seen in figure this gives the t distribution an important property called robustness which means that it is much less sensitive than the gaussian to the presence of a few data points which are outliers the robustness of the t distribution is illustrated in figure which compares the maximum likelihood solutions for a gaussian and a t distribution note that the max imum likelihood solution for the t distribution can be found using the expectation maximization em algorithm here we see that the effect of a small number of a b figure illustration of the robustness of student t distribution compared to a gaussian a histogram distribution of data points drawn from a gaussian distribution together with the maximum likelihood ﬁt ob tained from a t distribution red curve and a gaussian green curve largely hidden by the red curve because the t distribution contains the gaussian as a special case it gives almost the same solution as the gaussian b the same data set but with three additional outlying data points showing how the gaussian green curve is strongly distorted by the outliers whereas the t distribution red curve is relatively unaffected exercise outliers is much less signiﬁcant for the t distribution than for the gaussian outliers can arise in practical applications either because the process that generates the data corresponds to a distribution having a heavy tail or simply through mislabelled data robustness is also an important property for regression problems unsurprisingly the least squares approach to regression does not exhibit robustness because it cor responds to maximum likelihood under a conditional gaussian distribution by basing a regression model on a heavy tailed distribution such as a t distribution we obtain a more robust model if we go back to and substitute the alternative parameters ν λ a b and η τb a we see that the t distribution can be written in the form st x µ λ ν r n x µ ηλ gam η ν ν dη we can then generalize this to a multivariate gaussian x µ λ to obtain the cor responding multivariate student t distribution in the form st x µ λ ν r n x µ ηλ gam η ν ν dη using the same technique as for the univariate case we can evaluate this integral to give st x µ λ ν γ d ν γ ν λ πν d d ν ν where d is the dimensionality of x and is the squared mahalanobis distance deﬁned by exercise x µ tλ x µ this is the multivariate form of student t distribution and satisﬁes the following properties e x µ if ν cov x ν ν λ if ν mode x µ with corresponding results for the univariate case periodic variables although gaussian distributions are of great practical signiﬁcance both in their own right and as building blocks for more complex probabilistic models there are situations in which they are inappropriate as density models for continuous vari ables one important case which arises in practical applications is that of periodic variables an example of a periodic variable would be the wind direction at a particular geographical location we might for instance measure values of wind direction on a number of days and wish to summarize this using a parametric distribution another example is calendar time where we may be interested in modelling quantities that are believed to be periodic over hours or over an annual cycle such quantities can conveniently be represented using an angular polar coordinate θ we might be tempted to treat periodic variables by choosing some direction as the origin and then applying a conventional distribution such as the gaussian such an approach however would give results that were strongly dependent on the arbitrary choice of origin suppose for instance that we have two observations at and and we model them using a standard univariate gaussian distribution if we choose the origin at then the sample mean of this data set will be with standard deviation whereas if we choose the origin at then the mean will be and the standard deviation will be we clearly need to develop a special approach for the treatment of periodic variables let us consider the problem of evaluating the mean of a set of observations θn of a periodic variable from now on we shall assume that θ is measured in radians we have already seen that the simple average θn n will be strongly coordinate dependent to ﬁnd an invariant measure of the mean we note that the observations can be viewed as points on the unit circle and can therefore be described instead by two dimensional unit vectors xn where lxnl for n n as illustrated in figure we can average the vectors xn figure illustration of the representation of val ues θn of a periodic variable as two dimensional vectors xn living on the unit circle also shown is the average x of those vectors instead to give x n n xn n and then ﬁnd the corresponding angle θ of this average clearly this deﬁnition will ensure that the location of the mean is independent of the origin of the angular coor dinate note that x will typically lie inside the unit circle the cartesian coordinates of the observations are given by xn cos θn sin θn and we can write the carte sian coordinates of the sample mean in the form x r cos θ r sin θ substituting into and equating the and components then gives r cos θ n n cos θn n r sin θ n n sin θn n taking the ratio and using the identity tan θ sin θ cos θ we can solve for θ to give tan n sin θn n cos θn shortly we shall see how this result arises naturally as the maximum likelihood estimator for an appropriately deﬁned distribution over a periodic variable we now consider a periodic generalization of the gaussian called the von mises distribution here we shall limit our attention to univariate distributions although periodic distributions can also be found over hyperspheres of arbitrary dimension for an extensive discussion of periodic distributions see mardia and jupp by convention we will consider distributions p θ that have period any probability density p θ deﬁned over θ must not only be nonnegative and integrate figure the von mises distribution can be derived by considering a two dimensional gaussian of the form whose density contours are shown in blue and conditioning on the unit circle shown in red to one but it must also be periodic thus p θ must satisfy the three conditions p θ p θ dθ p θ p θ from it follows that p θ m p θ for any integer m we can easily obtain a gaussian like distribution that satisﬁes these three prop erties as follows consider a gaussian distribution over two variables x having mean µ and a covariance matrix σ where i is the identity matrix so that p exp r the contours of constant p x are circles as illustrated in figure now suppose we consider the value of this distribution along a circle of ﬁxed radius then by con struction this distribution will be periodic although it will not be normalized we can determine the form of this distribution by transforming from cartesian coordinates to polar coordinates r θ so that r cos θ r sin θ we also map the mean µ into polar coordinates by writing cos sin next we substitute these transformations into the two dimensional gaussian distribu tion and then condition on the unit circle r noting that we are interested only in the dependence on θ focussing on the exponent in the gaussian distribution we have r cos θ cos r sin θ sin cos θ cos 2r0 sin θ sin cos θ θ const figure the von mises distribution plotted for two different parameter values shown as a cartesian plot on the left and as the corresponding polar plot on the right exercise where const denotes terms independent of θ and we have made use of the following trigonometrical identities a a cos a cos b sin a sin b cos a b if we now deﬁne m we obtain our ﬁnal expression for the distribution of p θ along the unit circle r in the form p θ m m exp m cos θ which is called the von mises distribution or the circular normal here the param eter corresponds to the mean of the distribution while m which is known as the concentration parameter is analogous to the inverse variance precision for the gaussian the normalization coefﬁcient in is expressed in terms of m which is the zeroth order bessel function of the ﬁrst kind abramowitz and stegun and is deﬁned by m exp m cos θ dθ exercise for large m the distribution becomes approximately gaussian the von mises dis tribution is plotted in figure and the function m is plotted in figure now consider the maximum likelihood estimators for the parameters and m for the von mises distribution the log likelihood function is given by n ln p d m n ln n ln m m cos θn n m a m m m figure plot of the bessel function m deﬁned by together with the function a m deﬁned by exercise setting the derivative with respect to equal to zero gives n sin θn n to solve for we make use of the trigonometric identity sin a b cos b sin a cos a sin b from which we obtain θml tan r n sin θn which we recognize as the result obtained earlier for the mean of the obser vations viewed in a two dimensional cartesian space similarly maximizing with respect to m and making use of m m abramowitz and stegun we have n a m n cos θn n θml where we have substituted for the maximum likelihood solution for θml recalling that we are performing a joint optimization over θ and m and we have deﬁned m a m m the function a m is plotted in figure making use of the trigonometric iden tity we can write in the form a mml n n cos θn cos θml n n sin θn sin θml figure plots of the old faith ful data in which the blue curves show contours of constant proba bility density on the left is a single gaussian distribution which has been ﬁtted to the data us ing maximum likelihood note that this distribution fails to capture the two clumps in the data and indeed places much of its probability mass in the central region between the clumps where the data are relatively sparse on the right the distribution is given by a linear combination of two gaussians which has been ﬁtted to the data by maximum likelihood using techniques discussed chap ter and which gives a better rep resentation of the data appendix a the right hand side of is easily evaluated and the function a m can be inverted numerically for completeness we mention brieﬂy some alternative techniques for the con struction of periodic distributions the simplest approach is to use a histogram of observations in which the angular coordinate is divided into ﬁxed bins this has the virtue of simplicity and ﬂexibility but also suffers from signiﬁcant limitations as we shall see when we discuss histogram methods in more detail in section another approach starts like the von mises distribution from a gaussian distribution over a euclidean space but now marginalizes onto the unit circle rather than conditioning mardia and jupp however this leads to more complex forms of distribution and will not be discussed further finally any valid distribution over the real axis such as a gaussian can be turned into a periodic distribution by mapping succes sive intervals of width onto the periodic variable which corresponds to wrapping the real axis around unit circle again the resulting distribution is more complex to handle than the von mises distribution one limitation of the von mises distribution is that it is unimodal by forming mixtures of von mises distributions we obtain a ﬂexible framework for modelling periodic variables that can handle multimodality for an example of a machine learn ing application that makes use of von mises distributions see lawrence et al and for extensions to modelling conditional densities for regression problems see bishop and nabney mixtures of gaussians while the gaussian distribution has some important analytical properties it suf fers from signiﬁcant limitations when it comes to modelling real data sets consider the example shown in figure this is known as the old faithful data set and comprises measurements of the eruption of the old faithful geyser at yel lowstone national park in the usa each measurement comprises the duration of figure example of a gaussian mixture distribution in one dimension showing three gaussians each scaled by a coefﬁcient in blue and their sum in red p x x the eruption in minutes horizontal axis and the time in minutes to the next erup tion vertical axis we see that the data set forms two dominant clumps and that a simple gaussian distribution is unable to capture this structure whereas a linear superposition of two gaussians gives a better characterization of the data set such superpositions formed by taking linear combinations of more basic dis tributions such as gaussians can be formulated as probabilistic models known as mixture distributions mclachlan and basford mclachlan and peel in figure we see that a linear combination of gaussians can give rise to very complex densities by using a sufﬁcient number of gaussians and by adjusting their means and covariances as well as the coefﬁcients in the linear combination almost any continuous density can be approximated to arbitrary accuracy we therefore consider a superposition of k gaussian densities of the form section k p x πkn x µk σk k which is called a mixture of gaussians each gaussian density x µk σk is called a component of the mixture and has its own mean µk and covariance σk contour and surface plots for a gaussian mixture having components are shown in figure in this section we shall consider gaussian components to illustrate the frame work of mixture models more generally mixture models can comprise linear com binations of other distributions for instance in section we shall consider mixtures of bernoulli distributions as an example of a mixture model for discrete variables the parameters πk in are called mixing coefﬁcients if we integrate both sides of with respect to x and note that both p x and the individual gaussian components are normalized we obtain k πk k also the requirement that p x together with x µk σk implies πk for all k combining this with the condition we obtain πk figure illustration of a mixture of gaussians in a two dimensional space a contours of constant density for each of the mixture components in which the components are denoted red blue and green and the values of the mixing coefﬁcients are shown below each component b contours of the marginal probability density p x of the mixture distribution c a surface plot of the distribution p x we therefore see that the mixing coefﬁcients satisfy the requirements to be probabil ities from the sum and product rules the marginal density is given by k p x p k p x k k which is equivalent to in which we can view πk p k as the prior prob ability of picking the kth component and the density x µk σk p x k as the probability of x conditioned on k as we shall see in later chapters an impor tant role is played by the posterior probabilities p k x which are also known as responsibilities from bayes theorem these are given by γk x p k x p k p x k l p l p x l πkn x µk σk l πln x µl σl we shall discuss the probabilistic interpretation of the mixture distribution in greater detail in chapter the form of the gaussian mixture distribution is governed by the parameters π µ and σ where we have used the notation π πk µ µk and σ σk one way to set the values of these parameters is to use maximum likelihood from the log of the likelihood function is given by n ln p x π µ σ ln n k k πkn xn µk σk where x xn we immediately see that the situation is now much more complex than with a single gaussian due to the presence of the summation over k inside the logarithm as a result the maximum likelihood solution for the parameters no longer has a closed form analytical solution one approach to maxi mizing the likelihood function is to use iterative numerical optimization techniques fletcher nocedal and wright bishop and nabney alterna tively we can employ a powerful framework called expectation maximization which will be discussed at length in chapter the exponential family the probability distributions that we have studied so far in this chapter with the exception of the gaussian mixture are speciﬁc examples of a broad class of distri butions called the exponential family duda and hart bernardo and smith members of the exponential family have many important properties in com mon and it is illuminating to discuss these properties in some generality the exponential family of distributions over x given parameters η is deﬁned to be the set of distributions of the form p x η h x g η exp ηtu x where x may be scalar or vector and may be discrete or continuous here η are called the natural parameters of the distribution and u x is some function of x the function g η can be interpreted as the coefﬁcient that ensures that the distribu tion is normalized and therefore satisﬁes g η r h x exp ηtu x dx where the integration is replaced by summation if x is a discrete variable we begin by taking some examples of the distributions introduced earlier in the chapter and showing that they are indeed members of the exponential family consider ﬁrst the bernoulli distribution p x µ bern x µ µx µ x expressing the right hand side as the exponential of the logarithm we have p x µ exp x ln µ x ln µ exp ln µ µ comparison with allows us to identify ln µ µ which we can solve for µ to give µ σ η where σ η exp η is called the logistic sigmoid function thus we can write the bernoulli distribution using the standard representation in the form p x η σ η exp ηx where we have used σ η σ η which is easily proved from com parison with shows that u x x h x g η σ η next consider the multinomial distribution that for a single observation x takes the form p x µ tt µxk exp m xk ln µk k k where x xn t again we can write this in the standard representation so that p x η exp ηtx where ηk ln µk and we have deﬁned η ηm t again comparing with we have u x x h x g η note that the parameters ηk are not independent because the parameters µk are sub ject to the constraint m µk k so that given any m of the parameters µk the value of the remaining parameter is ﬁxed in some circumstances it will be convenient to remove this constraint by expressing the distribution in terms of only m parameters this can be achieved by using the relationship to eliminate µm by expressing it in terms of the remaining µk where k m thereby leaving m parameters note that these remaining parameters are still subject to the constraints m µk µk k making use of the constraint the multinomial distribution in this representa tion then becomes exp m k xk ln µk m m m exp m xk ln µk ln m µk we now identify k m j k ln µk η which we can solve for µk by ﬁrst summing both sides over k and then rearranging and back substituting to give exp ηk j exp η this is called the softmax function or the normalized exponential in this represen tation the multinomial distribution therefore takes the form p x η m k exp ηk exp ηtx this is the standard form of the exponential family with parameter vector η ηm t in which u x x h x g η m k exp ηk finally let us consider the gaussian distribution for the univariate gaussian we have p x µ exp r x µ exp r µ x 219 exercise which after some simple rearrangement can be cast in the standard exponential family form with µ η 220 u x x h x exp exercise maximum likelihood and sufﬁcient statistics let us now consider the problem of estimating the parameter vector η in the gen eral exponential family distribution using the technique of maximum likeli hood taking the gradient of both sides of with respect to η we have g η r h x exp ηtu x dx g η r h x exp ηtu x u x dx rearranging and making use again of then gives x exp tu x u x dx u x g η where we have used we therefore obtain the result ln g η e u x note that the covariance of u x can be expressed in terms of the second derivatives of g η and similarly for higher order moments thus provided we can normalize a distribution from the exponential family we can always ﬁnd its moments by simple differentiation now consider a set of independent identically distributed data denoted by x xn for which the likelihood function is given by n p x η n h xn g η n exp ηt n u xn setting the gradient of ln p x η with respect to η to zero we get the following condition to be satisﬁed by the maximum likelihood estimator ηml ln g ηml n n u xn n which can in principle be solved to obtain ηml we see that the solution for the maximum likelihood estimator depends on the data only through n u xn which is therefore called the sufﬁcient statistic of the distribution we do not need to store the entire data set itself but only the value of the sufﬁcient statistic for the bernoulli distribution for example the function u x is given just by x and so we need only keep the sum of the data points xn whereas for the gaussian u x x t and so we should keep both the sum of xn and the sum of if we consider the limit n then the right hand side of becomes e u x and so by comparing with we see that in this limit ηml will equal the true value η in fact this sufﬁciency property holds also for bayesian inference although we shall defer discussion of this until chapter when we have equipped ourselves with the tools of graphical models and can thereby gain a deeper insight into these important concepts conjugate priors we have already encountered the concept of a conjugate prior several times for example in the context of the bernoulli distribution for which the conjugate prior is the beta distribution or the gaussian where the conjugate prior for the mean is a gaussian and the conjugate prior for the precision is the wishart distribution in general for a given probability distribution p x η we can seek a prior p η that is conjugate to the likelihood function so that the posterior distribution has the same functional form as the prior for any member of the exponential family there exists a conjugate prior that can be written in the form p η χ ν f χ ν g η ν exp νηtχ where f χ ν is a normalization coefﬁcient and g η is the same function as ap pears in to see that this is indeed conjugate let us multiply the prior by the likelihood function to obtain the posterior distribution up to a nor malization coefﬁcient in the form p η x χ ν g η ν n exp ηt n n u xn νχ this again takes the same functional form as the prior conﬁrming conjugacy furthermore we see that the parameter ν can be interpreted as a effective number of pseudo observations in the prior each of which has a value for the sufﬁcient statistic u x given by χ noninformative priors in some applications of probabilistic inference we may have prior knowledge that can be conveniently expressed through the prior distribution for example if the prior assigns zero probability to some value of variable then the posterior dis tribution will necessarily also assign zero probability to that value irrespective of any subsequent observations of data in many cases however we may have little idea of what form the distribution should take we may then seek a form of prior distribution called a noninformative prior which is intended to have as little inﬂu ence on the posterior distribution as possible jeffries box and tao bernardo and smith this is sometimes referred to as letting the data speak for themselves if we have a distribution p x λ governed by a parameter λ we might be tempted to propose a prior distribution p λ const as a suitable prior if λ is a discrete variable with k states this simply amounts to setting the prior probability of each state to k in the case of continuous parameters however there are two potential difﬁculties with this approach the ﬁrst is that if the domain of λ is unbounded this prior distribution cannot be correctly normalized because the integral over λ diverges such priors are called improper in practice improper priors can often be used provided the corresponding posterior distribution is proper i e that it can be correctly normalized for instance if we put a uniform prior distribution over the mean of a gaussian then the posterior distribution for the mean once we have observed at least one data point will be proper a second difﬁculty arises from the transformation behaviour of a probability density under a nonlinear change of variables given by if a function h λ is constant and we change variables to λ then h η h will also be constant however if we choose the density pλ λ to be constant then the density of η will be given from by pη η pλ dλ λ dη pλ η and so the density over η will not be constant this issue does not arise when we use maximum likelihood because the likelihood function p x λ is a simple function of λ and so we are free to use any convenient parameterization if however we are to choose a prior distribution that is constant we must take care to use an appropriate representation for the parameters here we consider two simple examples of noninformative priors berger first of all if a density takes the form p x µ f x µ then the parameter µ is known as a location parameter this family of densities exhibits translation invariance because if we shift x by a constant to give x x c then p x µ f x µ where we have deﬁned µ µ c thus the density takes the same form in the new variable as in the original one and so the density is independent of the choice of origin we would like to choose a prior distribution that reﬂects this translation invariance property and so we choose a prior that assigns equal probability mass to an interval a µ b as to the shifted interval a c µ b c this implies b p µ dµ a b c a c p µ dµ b p µ c dµ a and because this must hold for all choices of a and b we have p µ c p µ which implies that p µ is constant an example of a location parameter would be the mean µ of a gaussian distribution as we have seen the conjugate prior distri bution for µ in this case is a gaussian p µ n µ and we obtain a noninformative prior by taking the limit indeed from and we see that this gives a posterior distribution over µ in which the contributions from the prior vanish as a second example consider a density of the form p x σ f x exercise where σ note that this will be a normalized density provided f x is correctly normalized the parameter σ is known as a scale parameter and the density exhibits scale invariance because if we scale x by a constant to give x cx then p x σ f x σ σ where we have deﬁned σ cσ this transformation corresponds to a change of scale for example from meters to kilometers if x is a length and we would like to choose a prior distribution that reﬂects this scale invariance if we consider an interval a σ b and a scaled interval a c σ b c then the prior should assign equal probability mass to these two intervals thus we have b p σ dσ a b c a c b p σ dσ p σ a c dσ c and because this must hold for choices of a and b we have p σ p σ and hence p σ σ note that again this is an improper prior because the integral of the distribution over σ is divergent it is sometimes also convenient to think of the prior distribution for a scale parameter in terms of the density of the log of the parameter using the transformation rule for densities we see that p ln σ const thus for this prior there is the same probability mass in the range σ as in the range σ and in σ section an example of a scale parameter would be the standard deviation σ of a gaussian distribution after we have taken account of the location parameter µ because n x µ σ exp x σ where x x µ as discussed earlier it is often more convenient to work in terms densities we see that a distribution p σ σ corresponds to a distribution over λ of the form p λ λ we have seen that the conjugate prior for λ was the gamma distribution gam λ given by the noninformative prior is obtained as the special case again if we examine the results and for the posterior distribution of λ we see that for the posterior depends only on terms arising from the data and not from the prior nonparametric methods throughout this chapter we have focussed on the use of probability distributions having speciﬁc functional forms governed by a small number of parameters whose values are to be determined from a data set this is called the parametric approach to density modelling an important limitation of this approach is that the chosen density might be a poor model of the distribution that generates the data which can result in poor predictive performance for instance if the process that generates the data is multimodal then this aspect of the distribution can never be captured by a gaussian which is necessarily unimodal in this ﬁnal section we consider some nonparametric approaches to density es timation that make few assumptions about the form of the distribution here we shall focus mainly on simple frequentist methods the reader should be aware however that nonparametric bayesian methods are attracting increasing interest walker et al neal mu ller and quintana teh et al let us start with a discussion of histogram methods for density estimation which we have already encountered in the context of marginal and conditional distributions in figure and in the context of the central limit theorem in figure here we explore the properties of histogram density models in more detail focussing on the case of a single continuous variable x standard histograms simply partition x into distinct bins of width i and then count the number ni of observations of x falling in bin i in order to turn this count into a normalized probability density we simply divide by the total number n of observations and by the width i of the bins to obtain probability values for each bin given by p ni i n i for which it is easily seen that p x dx this gives a model for the density p x that is constant over the width of each bin and often the bins are chosen to have the same width i figure an illustration of the histogram approach to density estimation in which a data set of data points is generated from the distribution shown by the green curve histogram density estimates based on with a common bin width are shown for various values of section in figure we show an example of histogram density estimation here the data is drawn from the distribution corresponding to the green curve which is formed from a mixture of two gaussians also shown are three examples of his togram density estimates corresponding to three different choices for the bin width we see that when is very small top ﬁgure the resulting density model is very spiky with a lot of structure that is not present in the underlying distribution that generated the data set conversely if is too large bottom ﬁgure then the result is a model that is too smooth and that consequently fails to capture the bimodal prop erty of the green curve the best results are obtained for some intermediate value of middle ﬁgure in principle a histogram density model is also dependent on the choice of edge location for the bins though this is typically much less signiﬁcant than the value of note that the histogram method has the property unlike the methods to be dis cussed shortly that once the histogram has been computed the data set itself can be discarded which can be advantageous if the data set is large also the histogram approach is easily applied if the data points are arriving sequentially in practice the histogram technique can be useful for obtaining a quick visual ization of data in one or two dimensions but is unsuited to most density estimation applications one obvious problem is that the estimated density has discontinuities that are due to the bin edges rather than any property of the underlying distribution that generated the data another major limitation of the histogram approach is its scaling with dimensionality if we divide each variable in a d dimensional space into m bins then the total number of bins will be md this exponential scaling with d is an example of the curse of dimensionality in a space of high dimensional ity the quantity of data needed to provide meaningful estimates of local probability density would be prohibitive the histogram approach to density estimation does however teach us two im portant lessons first to estimate the probability density at a particular location we should consider the data points that lie within some local neighbourhood of that point note that the concept of locality requires that we assume some form of dis tance measure and here we have been assuming euclidean distance for histograms this neighbourhood property was deﬁned by the bins and there is a natural smooth ing parameter describing the spatial extent of the local region in this case the bin width second the value of the smoothing parameter should be neither too large nor too small in order to obtain good results this is reminiscent of the choice of model complexity in polynomial curve ﬁtting discussed in chapter where the degree m of the polynomial or alternatively the value α of the regularization parameter was optimal for some intermediate value neither too large nor too small armed with these insights we turn now to a discussion of two widely used nonparametric tech niques for density estimation kernel estimators and nearest neighbours which have better scaling with dimensionality than the simple histogram model kernel density estimators let us suppose that observations are being drawn from some unknown probabil ity density p x in some d dimensional space which we shall take to be euclidean and we wish to estimate the value of p x from our earlier discussion of locality let us consider some small region containing x the probability mass associated with this region is given by p p x dx r section now suppose that we have collected a data set comprising n observations drawn from p x because each data point has a probability p of falling within the total number k of points that lie inside will be distributed according to the binomial distribution bin k n p n pk p k using we see that the mean fraction of points falling inside the region is e k n p and similarly using we see that the variance around this mean is var k n p p n for large n this distribution will be sharply peaked around the mean and so k np if however we also assume that the region is sufﬁciently small that the probability density p x is roughly constant over the region then we have p p x v where v is the volume of combining and we obtain our density estimate in the form p x k nv note that the validity of depends on two contradictory assumptions namely that the region be sufﬁciently small that the density is approximately constant over the region and yet sufﬁciently large in relation to the value of that density that the number k of points falling inside the region is sufﬁcient for the binomial distribution to be sharply peaked we can exploit the result in two different ways either we can ﬁx k and determine the value of v from the data which gives rise to the k nearest neighbour technique discussed shortly or we can ﬁx v and determine k from the data giv ing rise to the kernel approach it can be shown that both the k nearest neighbour density estimator and the kernel density estimator converge to the true probability density in the limit n provided v shrinks suitably with n and k grows with n duda and hart we begin by discussing the kernel method in detail and to start with we take the region to be a small hypercube centred on the point x at which we wish to determine the probability density in order to count the number k of points falling within this region it is convenient to deﬁne the following function ui i d otherwise which represents a unit cube centred on the origin the function k u is an example of a kernel function and in this context is also called a parzen window from the quantity k x xn h will be one if the data point xn lies inside a cube of side h centred on x and zero otherwise the total number of data points lying inside this cube will therefore be k k n x xn substituting this expression into then gives the following result for the esti mated density at x p x n n k x xn hd where we have used v hd for the volume of a hypercube of side h in d di mensions using the symmetry of the function k u we can now re interpret this equation not as a single cube centred on x but as the sum over n cubes centred on the n data points xn as it stands the kernel density estimator will suffer from one of the same problems that the histogram method suffered from namely the presence of artiﬁcial discontinuities in this case at the boundaries of the cubes we can obtain a smoother density model if we choose a smoother kernel function and a common choice is the gaussian which gives rise to the following kernel density model p x n n exp lx xn where h represents the standard deviation of the gaussian components thus our density model is obtained by placing a gaussian over each data point and then adding up the contributions over the whole data set and then dividing by n so that the den sity is correctly normalized in figure we apply the model to the data figure illustration of the kernel density model applied to the same data set used to demonstrate the histogram approach in figure we see that h acts as a smoothing parameter and that if it is set too small top panel the result is a very noisy density model whereas if it is set too large bottom panel then the bimodal nature of the underlying distribution from which the data is generated shown by the green curve is washed out the best den sity model is obtained for some intermedi ate value of h middle panel set used earlier to demonstrate the histogram technique we see that as expected the parameter h plays the role of a smoothing parameter and there is a trade off between sensitivity to noise at small h and over smoothing at large h again the optimization of h is a problem in model complexity analogous to the choice of bin width in histogram density estimation or the degree of the polynomial used in curve ﬁtting we can choose any other kernel function k u in subject to the condi tions k u r k u du which ensure that the resulting probability distribution is nonnegative everywhere and integrates to one the class of density model given by is called a kernel density estimator or parzen estimator it has a great merit that there is no compu tation involved in the training phase because this simply requires storage of the training set however this is also one of its great weaknesses because the computa tional cost of evaluating the density grows linearly with the size of the data set nearest neighbour methods one of the difﬁculties with the kernel approach to density estimation is that the parameter h governing the kernel width is ﬁxed for all kernels in regions of high data density a large value of h may lead to over smoothing and a washing out of structure that might otherwise be extracted from the data however reducing h may lead to noisy estimates elsewhere in data space where the density is smaller thus the optimal choice for h may be dependent on location within the data space this issue is addressed by nearest neighbour methods for density estimation we therefore return to our general result for local density estimation and instead of ﬁxing v and determining the value of k from the data we consider a ﬁxed value of k and use the data to ﬁnd an appropriate value for v to do this we consider a small sphere centred on the point x at which we wish to estimate the figure illustration of k nearest neighbour den sity estimation using the same data set as in figures and we see that the parameter k governs the degree of smoothing so that a small value of k leads to a very noisy density model top panel whereas a large value bot tom panel smoothes out the bimodal na ture of the true distribution shown by the green curve from which the data set was generated exercise density p x and we allow the radius of the sphere to grow until it contains precisely k data points the estimate of the density p x is then given by with v set to the volume of the resulting sphere this technique is known as k nearest neighbours and is illustrated in figure for various choices of the parameter k using the same data set as used in figure and figure we see that the value of k now governs the degree of smoothing and that again there is an optimum choice for k that is neither too large nor too small note that the model produced by k nearest neighbours is not a true density model because the integral over all space diverges we close this chapter by showing how the k nearest neighbour technique for density estimation can be extended to the problem of classiﬁcation to do this we apply the k nearest neighbour density estimation technique to each class separately and then make use of bayes theorem let us suppose that we have a data set com prising nk points in class k with n points in total so that k nk n if we wish to classify a new point x we draw a sphere centred on x containing precisely k points irrespective of their class suppose this sphere has volume v and contains kk points from class k then provides an estimate of the density associated with each class p x ck kk nkv similarly the unconditional density is given by p x nv while the class priors are given by p ck nk n we can now combine 254 and using bayes theorem to obtain the posterior probability of class membership p c x p x ck p ck kk figure a in the k nearest neighbour classiﬁer a new point shown by the black diamond is clas siﬁed according to the majority class membership of the k closest train ing data points in this case k b in the nearest neighbour k approach to classiﬁcation the resulting decision boundary is composed of hyperplanes that form perpendicular bisectors of pairs of points from different classes a b if we wish to minimize the probability of misclassiﬁcation this is done by assigning the test point x to the class having the largest posterior probability corresponding to the largest value of kk k thus to classify a new point we identify the k nearest points from the training data set and then assign the new point to the class having the largest number of representatives amongst this set ties can be broken at random the particular case of k is called the nearest neighbour rule because a test point is simply assigned to the same class as the nearest point from the training set these concepts are illustrated in figure in figure we show the results of applying the k nearest neighbour algo rithm to the oil ﬂow data introduced in chapter for various values of k as expected we see that k controls the degree of smoothing so that small k produces many small regions of each class whereas large k leads to fewer larger regions k k k x7 figure plot of data points from the oil data set showing values of x6 plotted against x7 where the red green and blue points correspond to the laminar annular and homogeneous classes respectively also shown are the classiﬁcations of the input space given by the k nearest neighbour algorithm for various values of k an interesting property of the nearest neighbour k classiﬁer is that in the limit n the error rate is never more than twice the minimum achievable error rate of an optimal classiﬁer i e one that uses the true class distributions cover and hart as discussed so far both the k nearest neighbour method and the kernel den sity estimator require the entire training data set to be stored leading to expensive computation if the data set is large this effect can be offset at the expense of some additional one off computation by constructing tree based search structures to allow approximate near neighbours to be found efﬁciently without doing an exhaustive search of the data set nevertheless these nonparametric methods are still severely limited on the other hand we have seen that simple parametric models are very restricted in terms of the forms of distribution that they can represent we therefore need to ﬁnd density models that are very ﬂexible and yet for which the complexity of the models can be controlled independently of the size of the training set and we shall see in subsequent chapters how to achieve this exercises www verify that the bernoulli distribution satisﬁes the following prop erties p x µ x e x µ var x µ µ show that the entropy h x of a bernoulli distributed random binary variable x is given by h x µ ln µ µ ln µ the form of the bernoulli distribution given by is not symmetric be tween the two values of x in some situations it will be more convenient to use an equivalent formulation for which x in which case the distribution can be written µ x µ x where µ show that the distribution is normalized and evaluate its mean variance and entropy www in this exercise we prove that the binomial distribution is nor malized first use the deﬁnition of the number of combinations of m identical objects chosen from a total of n to show that n n n m m m use this result to prove by induction the following result x n m n xm which is known as the binomial theorem and which is valid for all real values of x finally show that the binomial distribution is normalized so that n µm µ n m m m which can be done by ﬁrst pulling out a factor µ n out of the summation and then making use of the binomial theorem show that the mean of the binomial distribution is given by to do this differentiate both sides of the normalization condition with respect to µ and then rearrange to obtain an expression for the mean of n similarly by differentiating twice with respect to µ and making use of the result for the mean of the binomial distribution prove the result for the variance of the binomial www in this exercise we prove that the beta distribution given by is correctly normalized so that holds this is equivalent to showing that r γ a γ b from the deﬁnition of the gamma function we have γ a γ b r exp x xa dx r exp y yb dy use this expression to prove as follows first bring the integral over y inside the integrand of the integral over x next make the change of variable t y x where x is ﬁxed then interchange the order of the x and t integrations and ﬁnally make the change of variable x tµ where t is ﬁxed make use of the result to show that the mean variance and mode of the beta distribution are given respectively by e µ a a b var µ ab a b a b mode µ a a b consider a binomial random variable x given by with prior distribution for µ given by the beta distribution and suppose we have observed m occur rences of x and l occurrences of x show that the posterior mean value of x lies between the prior mean and the maximum likelihood estimate for µ to do this show that the posterior mean can be written as λ times the prior mean plus λ times the maximum likelihood estimate where λ this illustrates the con cept of the posterior distribution being a compromise between the prior distribution and the maximum likelihood solution consider two variables x and y with joint distribution p x y prove the follow ing two results e x ey ex x y var x ey varx x y vary ex x y here ex x y denotes the expectation of x under the conditional distribution p x y with a similar notation for the conditional variance www in this exercise we prove the normalization of the dirichlet dis tribution using induction we have already shown in exercise that the beta distribution which is a special case of the dirichlet for m is normalized we now assume that the dirichlet distribution is normalized for m variables and prove that it is normalized for m variables to do this consider the dirichlet distribution over m variables and take account of the constraint m µk by eliminating µm so that the dirichlet is written m m αm and our goal is to ﬁnd an expression for cm to do this integrate over µm taking care over the limits of integration and then make a change of variable so that this integral has limits and by assuming the correct result for cm and making use of derive the expression for cm using the property γ x xγ x of the gamma function derive the following results for the mean variance and covariance of the dirichlet distribution given by e µ αj var µ αj αj j α cov µ µ αjαl j l where is deﬁned by www by expressing the expectation of ln µj under the dirichlet distribution as a derivative with respect to αj show that e ln µj ψ αj ψ where is given by and is the digamma function ψ a da ln γ a the uniform distribution for a continuous variable x is deﬁned by u x a b a x b b a verify that this distribution is normalized and ﬁnd expressions for its mean and variance evaluate the kullback leibler divergence between two gaussians p x n x µ σ and q x n x m l www this exercise demonstrates that the multivariate distribution with max imum entropy for a given covariance is a gaussian the entropy of a distribution p x is given by h x r p x ln p x dx we wish to maximize h x over all distributions p x subject to the constraints that p x be normalized and that it have a speciﬁc mean and covariance so that r p x dx r p x x dx µ r p x x µ x µ t dx σ by performing a variational maximization of and using lagrange multipliers to enforce the constraints 281 and show that the maximum likelihood distribution is given by the gaussian show that the entropy of the multivariate gaussian n x µ σ is given by h x ln σ d ln where d is the dimensionality of x www consider two random variables and having gaussian distri butions with means and precisions respectively derive an expression for the differential entropy of the variable x to do this ﬁrst ﬁnd the distribution of x by using the relation p x r p x x p x dx and completing the square in the exponent then observe that this represents the convolution of two gaussian distributions which itself will be gaussian and ﬁnally make use of the result for the entropy of the univariate gaussian www consider the multivariate gaussian distribution given by by writing the precision matrix inverse covariance matrix σ as the sum of a sym metric and an anti symmetric matrix show that the anti symmetric term does not appear in the exponent of the gaussian and hence that the precision matrix may be taken to be symmetric without loss of generality because the inverse of a symmetric matrix is also symmetric see exercise it follows that the covariance matrix may also be chosen to be symmetric without loss of generality consider a real symmetric matrix σ whose eigenvalue equation is given by by taking the complex conjugate of this equation and subtracting the original equation and then forming the inner product with eigenvector ui show that the eigenvalues λi are real similarly use the symmetry property of σ to show that two eigenvectors ui and uj will be orthogonal provided λj λi finally show that without loss of generality the set of eigenvectors can be chosen to be orthonormal so that they satisfy even if some of the eigenvalues are zero show that a real symmetric matrix σ having the eigenvector equation can be expressed as an expansion in the eigenvectors with coefﬁcients given by the eigenvalues of the form similarly show that the inverse matrix σ has a representation of the form www a positive deﬁnite matrix σ can be deﬁned as one for which the quadratic form atσa is positive for any real value of the vector a show that a necessary and sufﬁcient condition for σ to be positive deﬁnite is that all of the eigenvalues λi of σ deﬁned by are positive show that a real symmetric matrix of size d d has d d independent parameters www show that the inverse of a symmetric matrix is itself symmetric by diagonalizing the coordinate system using the eigenvector expansion show that the volume contained within the hyperellipsoid corresponding to a constant mahalanobis distance is given by vd σ d where vd is the volume of the unit sphere in d dimensions and the mahalanobis distance is deﬁned by www prove the identity by multiplying both sides by the matrix a b and making use of the deﬁnition in sections and we considered the conditional and marginal distri butions for a multivariate gaussian more generally we can consider a partitioning of the components of x into three groups xa xb and xc with a corresponding par titioning of the mean vector µ and of the covariance matrix σ in the form µa µ µb µc σ σaa σab σac σba σbb σbc σca σcb σcc by making use of the results of section ﬁnd an expression for the conditional distribution p xa xb in which xc has been marginalized out a very useful result from linear algebra is the woodbury matrix inversion formula given by a bcd a a c da by multiplying both sides by a bcd prove the correctness of this result let x and z be two independent random vectors so that p x z p x p z show that the mean of their sum y x z is given by the sum of the means of each of the variable separately similarly show that the covariance matrix of y is given by the sum of the covariance matrices of x and z conﬁrm that this result agrees with that of exercise www consider a joint distribution over the variable z x whose mean and covariance are given by and respectively by mak ing use of the results and show that the marginal distribution p x is given similarly by making use of the results and show that the conditional distribution p y x is given by using the partitioned matrix inversion formula show that the inverse of the precision matrix is given by the covariance matrix by starting from and making use of the result verify the result consider two multidimensional random vectors x and z having gaussian distributions p x x µx σx and p z z µz σz respectively together with their sum y x z use the results and to ﬁnd an expression for the marginal distribution p y by considering the linear gaussian model comprising the product of the marginal distribution p x and the conditional distribution p y x www this exercise and the next provide practice at manipulating the quadratic forms that arise in linear gaussian models as well as giving an indepen dent check of results derived in the main text consider a joint distribution p x y deﬁned by the marginal and conditional distributions given by and by examining the quadratic form in the exponent of the joint distribution and using the technique of completing the square discussed in section ﬁnd expressions for the mean and covariance of the marginal distribution p y in which the variable x has been integrated out to do this make use of the woodbury matrix inversion formula verify that these results agree with and obtained using the results of chapter consider the same joint distribution as in exercise but now use the technique of completing the square to ﬁnd expressions for the mean and covariance of the conditional distribution p x y again verify that these agree with the corre sponding expressions and www to ﬁnd the maximum likelihood solution for the covariance matrix of a multivariate gaussian we need to maximize the log likelihood function with respect to σ noting that the covariance matrix must be symmetric and positive deﬁnite here we proceed by ignoring these constraints and doing a straightforward maximization using the results c c and c from appendix c show that the covariance matrix σ that maximizes the log likelihood function is given by the sample covariance we note that the ﬁnal result is necessarily symmetric and positive deﬁnite provided the sample covariance is nonsingular use the result to prove now using the results and show that e xnxm µµt inmς where xn denotes a data point sampled from a gaussian distribution with mean µ and covariance σ and inm denotes the n m element of the identity matrix hence prove the result www using an analogous procedure to that used to obtain derive an expression for the sequential estimation of the variance of a univariate gaussian distribution by starting with the maximum likelihood expression ml n n xn n µ verify that substituting the expression for a gaussian distribution into the robbins monro sequential estimation formula gives a result of the same form and hence obtain an expression for the corresponding coefﬁcients an using an analogous procedure to that used to obtain derive an ex pression for the sequential estimation of the covariance of a multivariate gaussian distribution by starting with the maximum likelihood expression verify that substituting the expression for a gaussian distribution into the robbins monro se quential estimation formula gives a result of the same form and hence obtain an expression for the corresponding coefﬁcients an use the technique of completing the square for the quadratic form in the expo nent to derive the results and starting from the results and for the posterior distribution of the mean of a gaussian random variable dissect out the contributions from the ﬁrst n data points and hence obtain expressions for the sequential update of µn and now derive the same results starting from the posterior distribution p µ xn n µ µn and multiplying by the likelihood func tion p n xn µ n xn µ σ and then completing the square and normalizing to obtain the posterior distribution after n observations www consider a d dimensional gaussian random variable x with distribu tion n x µ σ in which the covariance σ is known and for which we wish to infer the mean µ from a set of observations x xn given a prior distribution p µ n µ ﬁnd the corresponding posterior distribution p µ x use the deﬁnition of the gamma function to show that the gamma dis tribution is normalized evaluate the mean variance and mode of the gamma distribution the following distribution q x q is a generalization of the univariate gaussian distribution show that this distribution is normalized so that p x q dx and that it reduces to the gaussian when q consider a regression model in which the target variable is given by t y x w e and e is a random noise variable drawn from the distribution show that the log likelihood function over w and for an observed data set of input vectors x xn and corresponding target variables t tn t is given by n ln t x w xn n w tn q ln const 295 where const denotes terms independent of both w and note that as a function of w this is the lq error function considered in section consider a univariate gaussian distribution x µ τ having conjugate gaussian gamma prior given by and a data set x xn of i i d observations show that the posterior distribution is also a gaussian gamma distri bution of the same functional form as the prior and write down expressions for the parameters of this posterior distribution verify that the wishart distribution deﬁned by 155 is indeed a conjugate prior for the precision matrix of a multivariate gaussian www verify that evaluating the integral in leads to the result 159 www show that in the limit ν the t distribution 159 becomes a gaussian hint ignore the normalization coefﬁcient and simply look at the depen dence on x by following analogous steps to those used to derive the univariate student t distribution 159 verify the result 162 for the multivariate form of the stu dent t distribution by marginalizing over the variable η in using the deﬁnition 161 show by exchanging integration variables that the multivariate t distribution is correctly normalized by using the deﬁnition 161 of the multivariate student t distribution as a convolution of a gaussian with a gamma distribution verify the properties 164 165 and 166 for the multivariate t distribution deﬁned by 162 show that in the limit ν the multivariate student t distribution 162 reduces to a gaussian with mean µ and precision λ www the various trigonometric identities used in the discussion of periodic variables in this chapter can be proven easily from the relation exp ia cos a i sin a 296 in which i is the square root of minus one by considering the identity exp ia exp ia prove the result 177 similarly using the identity cos a b r exp i a b 298 where r denotes the real part prove 178 finally by using sin a b exp i a b where denotes the imaginary part prove the result 183 for large m the von mises distribution becomes sharply peaked around the mode by deﬁning ξ θ and making the taylor ex pansion of the cosine function given by cos α o α show that as m the von mises distribution tends to a gaussian using the trigonometric identity 183 show that solution of 182 for is given by 184 by computing ﬁrst and second derivatives of the von mises distribution 179 and using m for m show that the maximum of the distribution occurs when θ and that the minimum occurs when θ π mod by making use of the result 168 together with 184 and the trigonometric identity 178 show that the maximum likelihood solution mml for the concentra tion of the von mises distribution satisﬁes a mml r where r is the radius of the mean of the observations viewed as unit vectors in the two dimensional euclidean plane as illustrated in figure www express the beta distribution the gamma distribution 146 and the von mises distribution 179 as members of the exponential family 194 and thereby identify their natural parameters verify that the multivariate gaussian distribution can be cast in exponential family form 194 and derive expressions for η u x h x and g η analogous to 220 223 the result 226 showed that the negative gradient of ln g η for the exponen tial family is given by the expectation of u x by taking the second derivatives of 195 show that ln g η e u x u x t e u x e u x t cov u x 300 by changing variables using y x σ show that the density 236 will be correctly normalized provided f x is correctly normalized 60 www consider a histogram like density model in which the space x is di vided into ﬁxed regions for which the density p x takes the constant value hi over the ith region and that the volume of region i is denoted i suppose we have a set of n observations of x such that ni of these observations fall in region i using a lagrange multiplier to enforce the normalization constraint on the density derive an expression for the maximum likelihood estimator for the hi show that the k nearest neighbour density model deﬁnes an improper distribu tion whose integral over all space is divergent the focus so far in this book has been on unsupervised learning including topics such as density estimation and data clustering we turn now to a discussion of super vised learning starting with regression the goal of regression is to predict the value of one or more continuous target variables t given the value of a d dimensional vec tor x of input variables we have already encountered an example of a regression problem when we considered polynomial curve ﬁtting in chapter the polynomial is a speciﬁc example of a broad class of functions called linear regression models which share the property of being linear functions of the adjustable parameters and which will form the focus of this chapter the simplest form of linear regression models are also linear functions of the input variables however we can obtain a much more useful class of functions by taking linear combinations of a ﬁxed set of nonlinear functions of the input variables known as basis functions such models are linear functions of the parameters which gives them simple analytical properties and yet can be nonlinear with respect to the input variables given a training data set comprising n observations xn where n n together with corresponding target values tn the goal is to predict the value of t for a new value of x in the simplest approach this can be done by directly con structing an appropriate function y x whose values for new inputs x constitute the predictions for the corresponding values of t more generally from a probabilistic perspective we aim to model the predictive distribution p t x because this expresses our uncertainty about the value of t for each value of x from this conditional dis tribution we can make predictions of t for any new value of x in such a way as to minimize the expected value of a suitably chosen loss function as discussed in sec tion a common choice of loss function for real valued variables is the squared loss for which the optimal solution is given by the conditional expectation of t although linear models have signiﬁcant limitations as practical techniques for pattern recognition particularly for problems involving input spaces of high dimen sionality they have nice analytical properties and form the foundation for more so phisticated models to be discussed in later chapters linear basis function models the simplest linear model for regression is one that involves a linear combination of the input variables y x w wdxd where x xd t this is often simply known as linear regression the key property of this model is that it is a linear function of the parameters wd it is also however a linear function of the input variables xi and this imposes signiﬁcant limitations on the model we therefore extend the class of models by considering linear combinations of ﬁxed nonlinear functions of the input variables of the form m y x w wjφj x j where φj x are known as basis functions by denoting the maximum value of the index j by m the total number of parameters in this model will be m the parameter allows for any ﬁxed offset in the data and is sometimes called a bias parameter not to be confused with bias in a statistical sense it is often convenient to deﬁne an additional dummy basis function x so that m y x w wjφj x wtφ x j where w wm t and φ φm t in many practical ap plications of pattern recognition we will apply some form of ﬁxed pre processing or feature extraction to the original data variables if the original variables com prise the vector x then the features can be expressed in terms of the basis functions φj x by using nonlinear basis functions we allow the function y x w to be a non linear function of the input vector x functions of the form are called linear models however because this function is linear in w it is this linearity in the pa rameters that will greatly simplify the analysis of this class of models however it also leads to some signiﬁcant limitations as we discuss in section the example of polynomial regression considered in chapter is a particular example of this model in which there is a single input variable x and the basis func tions take the form of powers of x so that φj x xj one limitation of polynomial basis functions is that they are global functions of the input variable so that changes in one region of input space affect all other regions this can be resolved by dividing the input space up into regions and ﬁt a different polynomial in each region leading to spline functions hastie et al there are many other possible choices for the basis functions for example φj x exp x µ 2s2 where the µj govern the locations of the basis functions in input space and the pa rameter governs their spatial scale these are usually referred to as gaussian basis functions although it should be noted that they are not required to have a prob abilistic interpretation and in particular the normalization coefﬁcient is unimportant because these basis functions will be multiplied by adaptive parameters wj another possibility is the sigmoidal basis function of the form φ x σ x µj where σ a is the logistic sigmoid function deﬁned by σ a exp a equivalently we can use the tanh function because this is related to the logistic sigmoid by tanh a a and so a general linear combination of logistic sigmoid functions is equivalent to a general linear combination of tanh functions these various choices of basis function are illustrated in figure yet another possible choice of basis function is the fourier basis which leads to an expansion in sinusoidal functions each basis function represents a speciﬁc fre quency and has inﬁnite spatial extent by contrast basis functions that are localized to ﬁnite regions of input space necessarily comprise a spectrum of different spatial frequencies in many signal processing applications it is of interest to consider ba sis functions that are localized in both space and frequency leading to a class of functions known as wavelets these are also deﬁned to be mutually orthogonal to simplify their application wavelets are most applicable when the input values live figure examples of basis functions showing polynomials on the left gaussians of the form in the centre and sigmoidal of the form on the right on a regular lattice such as the successive time points in a temporal sequence or the pixels in an image useful texts on wavelets include ogden mallat and vidakovic most of the discussion in this chapter however is independent of the particular choice of basis function set and so for most of our discussion we shall not specify the particular form of the basis functions except for the purposes of numerical il lustration indeed much of our discussion will be equally applicable to the situation in which the vector φ x of basis functions is simply the identity φ x x fur thermore in order to keep the notation simple we shall focus on the case of a single target variable t however in section we consider brieﬂy the modiﬁcations needed to deal with multiple target variables maximum likelihood and least squares in chapter we ﬁtted polynomial functions to data sets by minimizing a sum of squares error function we also showed that this error function could be motivated as the maximum likelihood solution under an assumed gaussian noise model let us return to this discussion and consider the least squares approach and its relation to maximum likelihood in more detail as before we assume that the target variable t is given by a deterministic func tion y x w with additive gaussian noise so that t y x w e where e is a zero mean gaussian random variable with precision inverse variance β thus we can write section p t x w β n t y x w β recall that if we assume a squared loss function then the optimal prediction for a new value of x will be given by the conditional mean of the target variable in the case of a gaussian conditional distribution of the form the conditional mean will be simply e t x r tp t x dt y x w note that the gaussian noise assumption implies that the conditional distribution of t given x is unimodal which may be inappropriate for some applications an ex tension to mixtures of conditional gaussian distributions which permit multimodal conditional distributions will be discussed in section now consider a data set of inputs x xn with corresponding target values tn we group the target variables tn into a column vector that we denote by t where the typeface is chosen to distinguish it from a single observation of a multivariate target which would be denoted t making the assumption that these data points are drawn independently from the distribution we obtain the following expression for the likelihood function which is a function of the adjustable parameters w and β in the form n p t x w β n tn wtφ xn β n where we have used note that in supervised learning problems such as regres sion and classiﬁcation we are not seeking to model the distribution of the input variables thus x will always appear in the set of conditioning variables and so from now on we will drop the explicit x from expressions such as p t x w β in or der to keep the notation uncluttered taking the logarithm of the likelihood function and making use of the standard form for the univariate gaussian we have ln p t w β n ln n tn wtφ xn β n n ln β n ln βe w where the sum of squares error function is deﬁned by n ed w n n wtφ xn having written down the likelihood function we can use maximum likelihood to determine w and β consider ﬁrst the maximization with respect to w as observed already in section we see that maximization of the likelihood function under a conditional gaussian noise distribution for a linear model is equivalent to minimizing a sum of squares error function given by ed w the gradient of the log likelihood function takes the form n ln p t w β tn wtφ xn φ xn t n setting this gradient to zero gives n solving for w we obtain wml φtφ φtt which are known as the normal equations for the least squares problem here φ is an n m matrix called the design matrix whose elements are given by φnj φj xn so that φ φm φm the quantity xn xn φm xn φ φtφ φt is known as the moore penrose pseudo inverse of the matrix φ rao and mitra golub and van loan it can be regarded as a generalization of the notion of matrix inverse to nonsquare matrices indeed if φ is square and invertible then using the property ab b we see that φ φ at this point we can gain some insight into the role of the bias parameter if we make the bias parameter explicit then the error function becomes m e w t w w φ x setting the derivative with respect to equal to zero and solving for we obtain m t wjφj j where we have deﬁned t n n tn n φj n n φj xn n thus the bias compensates for the difference between the averages over the training set of the target values and the weighted sum of the averages of the basis function values we can also maximize the log likelihood function with respect to the noise precision parameter β giving t wt φ x figure geometrical interpretation of the least squares solution in an n dimensional space whose axes are the values of tn the least squares regression function is obtained by ﬁnding the or thogonal projection of the data vector t onto the subspace spanned by the basis functions φj x in which each basis function is viewed as a vec tor ϕj of length n with elements φj xn exercise and so we see that the inverse of the noise precision is given by the residual variance of the target values around the regression function geometry of least squares at this point it is instructive to consider the geometrical interpretation of the least squares solution to do this we consider an n dimensional space whose axes are given by the tn so that t tn t is a vector in this space each basis function φj xn evaluated at the n data points can also be represented as a vector in the same space denoted by ϕj as illustrated in figure note that ϕj corresponds to the jth column of φ whereas φ xn corresponds to the nth row of φ if the number m of basis functions is smaller than the number n of data points then the m vectors φj xn will span a linear subspace of dimensionality m we deﬁne y to be an n dimensional vector whose nth element is given by y xn w where n n because y is an arbitrary linear combination of the vectors ϕj it can live anywhere in the m dimensional subspace the sum of squares error is then equal up to a factor of to the squared euclidean distance between y and t thus the least squares solution for w corresponds to that choice of y that lies in subspace and that is closest to t intuitively from figure we anticipate that this solution corresponds to the orthogonal projection of t onto the subspace this is indeed the case as can easily be veriﬁed by noting that the solution for y is given by φwml and then conﬁrming that this takes the form of an orthogonal projection in practice a direct solution of the normal equations can lead to numerical difﬁ culties when φtφ is close to singular in particular when two or more of the basis vectors ϕj are co linear or nearly so the resulting parameter values can have large magnitudes such near degeneracies will not be uncommon when dealing with real data sets the resulting numerical difﬁculties can be addressed using the technique of singular value decomposition or svd press et al bishop and nabney note that the addition of a regularization term ensures that the matrix is non singular even in the presence of degeneracies sequential learning batch techniques such as the maximum likelihood solution which in volve processing the entire training set in one go can be computationally costly for large data sets as we have discussed in chapter if the data set is sufﬁciently large it may be worthwhile to use sequential algorithms also known as on line algorithms in which the data points are considered one at a time and the model parameters up dated after each such presentation sequential learning is also appropriate for real time applications in which the data observations are arriving in a continuous stream and predictions must be made before all of the data points are seen we can obtain a sequential learning algorithm by applying the technique of stochastic gradient descent also known as sequential gradient descent as follows if the error function comprises a sum over data points e n en then after presen tation of pattern n the stochastic gradient descent algorithm updates the parameter vector w using w τ w τ η en where τ denotes the iteration number and η is a learning rate parameter we shall discuss the choice of value for η shortly the value of w is initialized to some starting vector w for the case of the sum of squares error function this gives w τ w τ η tn w τ tφn φn where φn φ xn this is known as least mean squares or the lms algorithm the value of η needs to be chosen with care to ensure that the algorithm converges bishop and nabney regularized least squares in section we introduced the idea of adding a regularization term to an error function in order to control over ﬁtting so that the total error function to be minimized takes the form ed w λew w where λ is the regularization coefﬁcient that controls the relative importance of the data dependent error ed w and the regularization term ew w one of the sim plest forms of regularizer is given by the sum of squares of the weight vector ele ments e w wtw w if we also consider the sum of squares error function given by n w n then the total error function becomes n wtφ xn n n n wtφ xn λ wtw this particular choice of regularizer is known in the machine learning literature as weight decay because in sequential learning algorithms it encourages weight values to decay towards zero unless supported by the data in statistics it provides an ex ample of a parameter shrinkage method because it shrinks parameter values towards q q q q figure contours of the regularization term in for various values of the parameter q zero it has the advantage that the error function remains a quadratic function of w and so its exact minimizer can be found in closed form speciﬁcally setting the gradient of with respect to w to zero and solving for w as before we obtain w λi φtφ φtt this represents a simple extension of the least squares solution a more general regularizer is sometimes used for which the regularized error takes the form t wtφ x λ w q exercise where q corresponds to the quadratic regularizer figure shows con tours of the regularization function for different values of q the case of q is know as the lasso in the statistics literature tibshirani it has the property that if λ is sufﬁciently large some of the coefﬁcients wj are driven to zero leading to a sparse model in which the corresponding basis functions play no role to see this we ﬁrst note that minimizing is equivalent to minimizing the unregularized sum of squares error subject to the constraint appendix e m wj q η j for an appropriate value of the parameter η where the two approaches can be related using lagrange multipliers the origin of the sparsity can be seen from figure which shows that the minimum of the error function subject to the constraint as λ is increased so an increasing number of parameters are driven to zero regularization allows complex models to be trained on data sets of limited size without severe over ﬁtting essentially by limiting the effective model complexity however the problem of determining the optimal model complexity is then shifted from one of ﬁnding the appropriate number of basis functions to one of determining a suitable value of the regularization coefﬁcient λ we shall return to the issue of model complexity later in this chapter figure plot of the contours of the unregularized error function blue along with the constraint re gion for the quadratic regular izer q on the left and the lasso regularizer q on the right in which the optimum value for the pa rameter vector w is denoted by w the lasso gives a sparse solution in which w w w for the remainder of this chapter we shall focus on the quadratic regularizer both for its practical importance and its analytical tractability multiple outputs so far we have considered the case of a single target variable t in some applica tions we may wish to predict k target variables which we denote collectively by the target vector t this could be done by introducing a different set of basis func tions for each component of t leading to multiple independent regression problems however a more interesting and more common approach is to use the same set of basis functions to model all of the components of the target vector so that y x w wtφ x where y is a k dimensional column vector w is an m k matrix of parameters and φ x is an m dimensional column vector with elements φj x with x as before suppose we take the conditional distribution of the target vector to be an isotropic gaussian of the form p t x w β n t wtφ x β if we have a set of observations tn we can combine these into a matrix t of size n k such that the nth row is given by tt similarly we can combine the input vectors xn into a matrix x the log likelihood function is then given by ln p t x w β n ln n tn wtφ xn β n nk β β i as before we can maximize this function with respect to w giving wml φtφ φtt if we examine this result for each target variable tk we have w φtφ φtt φ tk 35 exercise where tk is an n dimensional column vector with components tnk for n n thus the solution to the regression problem decouples between the different target variables and we need only compute a single pseudo inverse matrix φ which is shared by all of the vectors wk the extension to general gaussian noise distributions having arbitrary covari ance matrices is straightforward again this leads to a decoupling into k inde pendent regression problems this result is unsurprising because the parameters w deﬁne only the mean of the gaussian noise distribution and we know from sec tion that the maximum likelihood solution for the mean of a multivariate gaus sian is independent of the covariance from now on we shall therefore consider a single target variable t for simplicity the bias variance decomposition so far in our discussion of linear models for regression we have assumed that the form and number of basis functions are both ﬁxed as we have seen in chapter the use of maximum likelihood or equivalently least squares can lead to severe over ﬁtting if complex models are trained using data sets of limited size however limiting the number of basis functions in order to avoid over ﬁtting has the side effect of limiting the ﬂexibility of the model to capture interesting and important trends in the data although the introduction of regularization terms can control over ﬁtting for models with many parameters this raises the question of how to determine a suitable value for the regularization coefﬁcient λ seeking the solution that minimizes the regularized error function with respect to both the weight vector w and the regularization coefﬁcient λ is clearly not the right approach since this leads to the unregularized solution with λ as we have seen in earlier chapters the phenomenon of over ﬁtting is really an unfortunate property of maximum likelihood and does not arise when we marginalize over parameters in a bayesian setting in this chapter we shall consider the bayesian view of model complexity in some depth before doing so however it is instructive to consider a frequentist viewpoint of the model complexity issue known as the bias variance trade off although we shall introduce this concept in the context of linear basis function models where it is easy to illustrate the ideas using simple examples the discussion has more general applicability in section when we discussed decision theory for regression problems we considered various loss functions each of which leads to a corresponding optimal prediction once we are given the conditional distribution p t x a popular choice is the squared loss function for which the optimal prediction is given by the conditional expectation which we denote by h x and which is given by h x e t x r tp t x dt 36 at this point it is worth distinguishing between the squared loss function arising from decision theory and the sum of squares error function that arose in the maxi mum likelihood estimation of model parameters we might use more sophisticated techniques than least squares for example regularization or a fully bayesian ap proach to determine the conditional distribution p t x these can all be combined with the squared loss function for the purpose of making predictions we showed in section that the expected squared loss can be written in the form e l r y x h x p x dx r h x t x t dx dt recall that the second term which is independent of y x arises from the intrinsic noise on the data and represents the minimum achievable value of the expected loss the ﬁrst term depends on our choice for the function y x and we will seek a so lution for y x which makes this term a minimum because it is nonnegative the smallest that we can hope to make this term is zero if we had an unlimited supply of data and unlimited computational resources we could in principle ﬁnd the regres sion function h x to any desired degree of accuracy and this would represent the optimal choice for y x however in practice we have a data set containing only a ﬁnite number n of data points and consequently we do not know the regression function h x exactly if we model the h x using a parametric function y x w governed by a pa rameter vector w then from a bayesian perspective the uncertainty in our model is expressed through a posterior distribution over w a frequentist treatment however involves making a point estimate of w based on the data set and tries instead to interpret the uncertainty of this estimate through the following thought experi ment suppose we had a large number of data sets each of size n and each drawn independently from the distribution p t x for any given data set we can run our learning algorithm and obtain a prediction function y x different data sets from the ensemble will give different functions and consequently different values of the squared loss the performance of a particular learning algorithm is then assessed by taking the average over this ensemble of data sets consider the integrand of the ﬁrst term in which for a particular data set d takes the form y x d h x 38 because this quantity will be dependent on the particular data set d we take its aver age over the ensemble of data sets if we add and subtract the quantity ed y x d inside the braces and then expand we obtain y x d ed y x d ed y x d h x y x d ed y x d ed y x d h x y x d ed y x d ed y x d h x 39 we now take the expectation of this expression with respect to and note that the ﬁnal term will vanish giving e y x h x biva varvia nce we see that the expected squared difference between y x and the regression function h x can be expressed as the sum of two terms the ﬁrst term called the squared bias represents the extent to which the average prediction over all data sets differs from the desired regression function the second term called the variance measures the extent to which the solutions for individual data sets vary around their average and hence this measures the extent to which the function y x is sensitive to the particular choice of data set we shall provide some intuition to support these deﬁnitions shortly when we consider a simple example so far we have considered a single input value x if we substitute this expansion back into we obtain the following decomposition of the expected squared loss expected loss bias variance noise where bias r ed y x d h x x dx variance r ed y x d ed y x d p x dx noise r h x t x t dx dt appendix a and the bias and variance terms now refer to integrated quantities our goal is to minimize the expected loss which we have decomposed into the sum of a squared bias a variance and a constant noise term as we shall see there is a trade off between bias and variance with very ﬂexible models having low bias and high variance and relatively rigid models having high bias and low variance the model with the optimal predictive capability is the one that leads to the best balance between bias and variance this is illustrated by considering the sinusoidal data set from chapter here we generate data sets each containing n data points independently from the sinusoidal curve h x sin the data sets are indexed by l l where l and for each data set d l we t t x x t t x x t t x x figure illustration of the dependence of bias and variance on model complexity governed by a regulariza tion parameter λ using the sinusoidal data set from chapter there are l data sets each having n data points and there are gaussian basis functions in the model so that the total number of parameters is m including the bias parameter the left column shows the result of ﬁtting the model to the data sets for various values of ln λ for clarity only of the ﬁts are shown the right column shows the corresponding average of the ﬁts red along with the sinusoidal function from which the data sets were generated green figure plot of squared bias and variance together with their sum correspond ing to the results shown in fig ure also shown is the average test set error for a test data set size of points the minimum value of bias variance occurs around ln λ 31 which is close to the value that gives the minimum error on the test data 06 ln λ ﬁt a model with 24 gaussian basis functions by minimizing the regularized error function to give a prediction function y l x as shown in figure the top row corresponds to a large value of the regularization coefﬁcient λ that gives low variance because the red curves in the left plot look similar but high bias because the two curves in the right plot are very different conversely on the bottom row for which λ is small there is large variance shown by the high variability between the red curves in the left plot but low bias shown by the good ﬁt between the average model ﬁt and the original sinusoidal function note that the result of averaging many solutions for the complex model with m is a very good ﬁt to the regression function which suggests that averaging may be a beneﬁcial procedure indeed a weighted averaging of multiple solutions lies at the heart of a bayesian approach although the averaging is with respect to the posterior distribution of parameters not with respect to multiple data sets we can also examine the bias variance trade off quantitatively for this example the average prediction is estimated from l y x y l x l l and the integrated squared bias and integrated variance are then given by bias n n y xn n h xn n l variance y l x y x 47 where the integral over x weighted by the distribution p x is approximated by a ﬁnite sum over data points drawn from that distribution these quantities along with their sum are plotted as a function of ln λ in figure we see that small values of λ allow the model to become ﬁnely tuned to the noise on each individual data set leading to large variance conversely a large value of λ pulls the weight parameters towards zero leading to large bias although the bias variance decomposition may provide some interesting in sights into the model complexity issue from a frequentist perspective it is of lim ited practical value because the bias variance decomposition is based on averages with respect to ensembles of data sets whereas in practice we have only the single observed data set if we had a large number of independent training sets of a given size we would be better off combining them into a single large training set which of course would reduce the level of over ﬁtting for a given model complexity given these limitations we turn in the next section to a bayesian treatment of linear basis function models which not only provides powerful insights into the issues of over ﬁtting but which also leads to practical techniques for addressing the question model complexity bayesian linear regression in our discussion of maximum likelihood for setting the parameters of a linear re gression model we have seen that the effective model complexity governed by the number of basis functions needs to be controlled according to the size of the data set adding a regularization term to the log likelihood function means the effective model complexity can then be controlled by the value of the regularization coefﬁ cient although the choice of the number and form of the basis functions is of course still important in determining the overall behaviour of the model this leaves the issue of deciding the appropriate model complexity for the par ticular problem which cannot be decided simply by maximizing the likelihood func tion because this always leads to excessively complex models and over ﬁtting in dependent hold out data can be used to determine model complexity as discussed in section but this can be both computationally expensive and wasteful of valu able data we therefore turn to a bayesian treatment of linear regression which will avoid the over ﬁtting problem of maximum likelihood and which will also lead to automatic methods of determining model complexity using the training data alone again for simplicity we will focus on the case of a single target variable t ex tension to multiple target variables is straightforward and follows the discussion of section parameter distribution we begin our discussion of the bayesian treatment of linear regression by in troducing a prior probability distribution over the model parameters w for the mo ment we shall treat the noise precision parameter β as a known constant first note that the likelihood function p t w deﬁned by is the exponential of a quadratic function of w the corresponding conjugate prior is therefore given by a gaussian distribution of the form p w n w having mean and covariance exercise next we compute the posterior distribution which is proportional to the product of the likelihood function and the prior due to the choice of a conjugate gaus sian prior distribution the posterior will also be gaussian we can evaluate this distribution by the usual procedure of completing the square in the exponential and then ﬁnding the normalization coefﬁcient using the standard result for a normalized gaussian however we have already done the necessary work in deriving the gen eral result 116 which allows us to write down the posterior distribution directly in the form where p w t n w mn sn mn sn s βφtt 50 s s βφtφ exercise note that because the posterior distribution is gaussian its mode coincides with its mean thus the maximum posterior weight vector is simply given by wmap mn if we consider an inﬁnitely broad prior α with α the mean mn of the posterior distribution reduces to the maximum likelihood value wml given by similarly if n then the posterior distribution reverts to the prior furthermore if data points arrive sequentially then the posterior distribution at any stage acts as the prior distribution for the subsequent data point such that the new posterior distribution is again given by for the remainder of this chapter we shall consider a particular form of gaus sian prior in order to simplify the treatment speciﬁcally we consider a zero mean isotropic gaussian governed by a single precision parameter α so that p w α n w α and the corresponding posterior distribution over w is then given by with mn βsnφtt s αi βφtφ the log of the posterior distribution is given by the sum of the log likelihood and the log of the prior and as a function of w takes the form n ln w t β n n wtφ xn α wtw const maximization of this posterior distribution with respect to w is therefore equiva lent to the minimization of the sum of squares error function with the addition of a quadratic regularization term corresponding to with λ α β we can illustrate bayesian learning in a linear basis function model as well as the sequential update of a posterior distribution using a simple example involving straight line ﬁtting consider a single input variable x a single target variable t and a linear model of the form y x w because this has just two adap tive parameters we can plot the prior and posterior distributions directly in parameter space we generate synthetic data from the function f x a with param eter values and by ﬁrst choosing values of xn from the uniform distribution u x then evaluating f xn a and ﬁnally adding gaussian noise with standard deviation of to obtain the target values tn our goal is to recover the values of and from such data and we will explore the dependence on the size of the data set we assume here that the noise variance is known and hence we set the precision parameter to its true value β similarly we ﬁx the parameter α to we shall shortly discuss strategies for determining α and β from the training data figure shows the results of bayesian learning in this model as the size of the data set is increased and demonstrates the sequential nature of bayesian learning in which the current posterior distribution forms the prior when a new data point is observed it is worth taking time to study this ﬁgure in detail as it illustrates several important aspects of bayesian inference the ﬁrst row of this ﬁgure corresponds to the situation before any data points are observed and shows a plot of the prior distribution in w space together with six samples of the function y x w in which the values of w are drawn from the prior in the second row we see the situation after observing a single data point the location x t of the data point is shown by a blue circle in the right hand column in the left hand column is a plot of the likelihood function p t x w for this data point as a function of w note that the likelihood function provides a soft constraint that the line must pass close to the data point where close is determined by the noise precision β for comparison the true parameter values and used to generate the data set are shown by a white cross in the plots in the left column of figure when we multiply this likelihood function by the prior from the top row and normalize we obtain the posterior distribution shown in the middle plot on the second row sam ples of the regression function y x w obtained by drawing samples of w from this posterior distribution are shown in the right hand plot note that these sample lines all pass close to the data point the third row of this ﬁgure shows the effect of ob serving a second data point again shown by a blue circle in the plot in the right hand column the corresponding likelihood function for this second data point alone is shown in the left plot when we multiply this likelihood function by the posterior distribution from the second row we obtain the posterior distribution shown in the middle plot of the third row note that this is exactly the same posterior distribution as would be obtained by combining the original prior with the likelihood function for the two data points this posterior has now been inﬂuenced by two data points and because two points are sufﬁcient to deﬁne a line this already gives a relatively compact posterior distribution samples from this posterior distribution give rise to the functions shown in red in the third column and we see that these functions pass close to both of the data points the fourth row shows the effect of observing a total of data points the left hand plot shows the likelihood function for the data point alone and the middle plot shows the resulting posterior distribution that has now absorbed information from all observations note how the posterior is much sharper than in the third row in the limit of an inﬁnite number of data points the figure illustration of sequential bayesian learning for a simple linear model of the form y x w a detailed description of this ﬁgure is given in the text posterior distribution would become a delta function centred on the true parameter values shown by the white cross other forms of prior over the parameters can be considered for instance we can generalize the gaussian prior to give q α q m α exercise in which q corresponds to the gaussian distribution and only in this case is the prior conjugate to the likelihood function finding the maximum of the poste rior distribution over w corresponds to minimization of the regularized error function in the case of the gaussian prior the mode of the posterior distribution was equal to the mean although this will no longer hold if q predictive distribution in practice we are not usually interested in the value of w itself but rather in making predictions of t for new values of x this requires that we evaluate the predictive distribution deﬁned by p t t α β r p t w β p w t α β dw in which t is the vector of target values from the training set and we have omitted the corresponding input vectors from the right hand side of the conditioning statements to simplify the notation the conditional distribution p t x w β of the target vari able is given by and the posterior weight distribution is given by we see that involves the convolution of two gaussian distributions and so making use of the result 115 from section we see that the predictive distribution takes the form p t x t α β n t mt φ x x where the variance x of the predictive distribution is given by x β φ x tsn φ x the ﬁrst term in represents the noise on the data whereas the second term reﬂects the uncertainty associated with the parameters w because the noise process and the distribution of w are independent gaussians their variances are additive note that as additional data points are observed the posterior distribution becomes narrower as a consequence it can be shown qazaz et al that x exercise x in the limit n the second term in goes to zero and the variance of the predictive distribution arises solely from the additive noise governed by the parameter β as an illustration of the predictive distribution for bayesian linear regression models let us return to the synthetic sinusoidal data set of section in figure t t x x t t x x figure examples of the predictive distribution for a model consisting of gaussian basis functions of the form using the synthetic sinusoidal data set of section see the text for a detailed discussion we ﬁt a model comprising a linear combination of gaussian basis functions to data sets of various sizes and then look at the corresponding posterior distributions here the green curves correspond to the function sin from which the data points were generated with the addition of gaussian noise data sets of size n n n and n are shown in the four plots by the blue circles for each plot the red curve shows the mean of the corresponding gaussian predictive distribution and the red shaded region spans one standard deviation either side of the mean note that the predictive uncertainty depends on x and is smallest in the neighbourhood of the data points also note that the level of uncertainty decreases as more data points are observed the plots in figure only show the point wise predictive variance as a func tion of x in order to gain insight into the covariance between the predictions at different values of x we can draw samples from the posterior distribution over w and then plot the corresponding functions y x w as shown in figure t t x x t t x x figure plots of the function y x w using samples from the posterior distributions over w corresponding to the plots in figure section exercise exercise if we used localized basis functions such as gaussians then in regions away from the basis function centres the contribution from the second term in the predic tive variance will go to zero leaving only the noise contribution β thus the model becomes very conﬁdent in its predictions when extrapolating outside the region occupied by the basis functions which is generally an undesirable behaviour this problem can be avoided by adopting an alternative bayesian approach to re gression known as a gaussian process note that if both w and β are treated as unknown then we can introduce a conjugate prior distribution p w β that from the discussion in section will be given by a gaussian gamma distribution denison et al in this case the predictive distribution is a student t distribution figure the equivalent ker nel k x xt for the gaussian basis functions in figure shown as a plot of x versus xt together with three slices through this matrix cor responding to three different values of x the data set used to generate this kernel comprised values of x equally spaced over the interval chapter equivalent kernel the posterior mean solution for the linear basis function model has an in teresting interpretation that will set the stage for kernel methods including gaussian processes if we substitute into the expression we see that the predictive mean can be written in the form y x mn mt φ x βφ x tsnφtt βφ x tsnφ xn tn 60 n where sn is deﬁned by thus the mean of the predictive distribution at a point x is given by a linear combination of the training set target variables tn so that we can write where the function n y x mn k x xn tn n k x xi βφ x tsnφ xi is known as the smoother matrix or the equivalent kernel regression functions such as this which make predictions by taking linear combinations of the training set target values are known as linear smoothers note that the equivalent kernel depends on the input values xn from the data set because these appear in the deﬁnition of sn the equivalent kernel is illustrated for the case of gaussian basis functions in figure in which the kernel functions k x xi have been plotted as a function of xi for three different values of x we see that they are localized around x and so the mean of the predictive distribution at x given by y x mn is obtained by forming a weighted combination of the target values in which data points close to x are given higher weight than points further removed from x intuitively it seems reasonable that we should weight local evidence more strongly than distant evidence note that this localization property holds not only for the localized gaussian basis functions but also for the nonlocal polynomial and sigmoidal basis functions as illustrated in figure figure examples of equiva lent kernels k x xt for x plotted as a function of xt corre sponding left to the polynomial ba sis functions and right to the sig moidal basis functions shown in fig ure note that these are local ized functions of xt even though the corresponding basis functions are nonlocal further insight into the role of the equivalent kernel can be obtained by consid ering the covariance between y x and y xi which is given by cov y x y xi cov φ x tw wtφ xi φ x tsnφ xi β x xi where we have made use of and from the form of the equivalent kernel we see that the predictive mean at nearby points will be highly correlated whereas for more distant pairs of points the correlation will be smaller the predictive distribution shown in figure allows us to visualize the point wise uncertainty in the predictions governed by however by drawing sam ples from the posterior distribution over w and plotting the corresponding model functions y x w as in figure we are visualizing the joint uncertainty in the posterior distribution between the y values at two or more x values as governed by the equivalent kernel the formulation of linear regression in terms of a kernel function suggests an alternative approach to regression as follows instead of introducing a set of basis functions which implicitly determines an equivalent kernel we can instead deﬁne a localized kernel directly and use this to make predictions for new input vectors x given the observed training set this leads to a practical framework for regression and classiﬁcation called gaussian processes which will be discussed in detail in section we have seen that the effective kernel deﬁnes the weights by which the training set target values are combined in order to make a prediction at a new value of x and it can be shown that these weights sum to one in other words exercise n k x xn n for all values of x this intuitively pleasing result can easily be proven informally by noting that the summation is equivalent to considering the predictive mean y x for a set of target data in which tn for all n provided the basis functions are linearly independent that there are more data points than basis functions and that one of the basis functions is constant corresponding to the bias parameter then it is clear that we can ﬁt the training data exactly and hence that the predictive mean will chapter be simply y x from which we obtain 64 note that the kernel function can be negative as well as positive so although it satisﬁes a summation constraint the corresponding predictions are not necessarily convex combinations of the training set target variables finally we note that the equivalent kernel satisﬁes an important property shared by kernel functions in general namely that it can be expressed in the form an inner product with respect to a vector ψ x of nonlinear functions so that k x z ψ x tψ z where ψ x x bayesian model comparison section in chapter we highlighted the problem of over ﬁtting as well as the use of cross validation as a technique for setting the values of regularization parameters or for choosing between alternative models here we consider the problem of model se lection from a bayesian perspective in this section our discussion will be very general and then in section we shall see how these ideas can be applied to the determination of regularization parameters in linear regression as we shall see the over ﬁtting associated with maximum likelihood can be avoided by marginalizing summing or integrating over the model parameters in stead of making point estimates of their values models can then be compared di rectly on the training data without the need for a validation set this allows all available data to be used for training and avoids the multiple training runs for each model associated with cross validation it also allows multiple complexity parame ters to be determined simultaneously as part of the training process for example in chapter we shall introduce the relevance vector machine which is a bayesian model having one complexity parameter for every training data point the bayesian view of model comparison simply involves the use of probabilities to represent uncertainty in the choice of model along with a consistent application of the sum and product rules of probability suppose we wish to compare a set of l models i where i l here a model refers to a probability distribution over the observed data in the case of the polynomial curve ﬁtting problem the distribution is deﬁned over the set of target values t while the set of input values x is assumed to be known other types of model deﬁne a joint distributions over x and t we shall suppose that the data is generated from one of these models but we are uncertain which one our uncertainty is expressed through a prior probability distribution p i given a training set we then wish to evaluate the posterior distribution p mi d p mi p d mi the prior allows us to express a preference for different models let us simply assume that all models are given equal prior probability the interesting term is the model evidence p d mi which expresses the preference shown by the data for different models and we shall examine this term in more detail shortly the model evidence is sometimes also called the marginal likelihood because it can be viewed as a likelihood function over the space of models in which the parameters have been marginalized out the ratio of model evidences p i p j for two models is known as a bayes factor kass and raftery once we know the posterior distribution over models the predictive distribution is given from the sum and product rules by chapter l p t x d p t x mi d p mi d i this is an example of a mixture distribution in which the overall predictive distribu tion is obtained by averaging the predictive distributions p t x i of individual models weighted by the posterior probabilities p i of those models for in stance if we have two models that are a posteriori equally likely and one predicts a narrow distribution around t a while the other predicts a narrow distribution around t b the overall predictive distribution will be a bimodal distribution with modes at t a and t b not a single model at t a b a simple approximation to model averaging is to use the single most probable model alone to make predictions this is known as model selection for a model governed by a set of parameters w the model evidence is given from the sum and product rules of probability by p d mi r p d w mi p w mi dw from a sampling perspective the marginal likelihood can be viewed as the proba bility of generating the data set from a model whose parameters are sampled at random from the prior it is also interesting to note that the evidence is precisely the normalizing term that appears in the denominator in bayes theorem when evaluating the posterior distribution over parameters because p w d m p d w mi p w mi p d mi we can obtain some insight into the model evidence by making a simple approx imation to the integral over parameters consider ﬁrst the case of a model having a single parameter w the posterior distribution over parameters is proportional to p w p w where we omit the dependence on the model i to keep the notation uncluttered if we assume that the posterior distribution is sharply peaked around the most probable value wmap with width wposterior then we can approximate the in tegral by the value of the integrand at its maximum times the width of the peak if we further assume that the prior is ﬂat with width wprior so that p w wprior then we have p d r p d w p w dw p d wmap wposterior wprior figure we can obtain a rough approximation to the model evidence if we assume that the posterior distribution over parame ters is sharply peaked around its mode wmap wposterior wmap w wprior and so taking logs we obtain ln p d ln p d wmap ln wposterior wprior this approximation is illustrated in figure the ﬁrst term represents the ﬁt to the data given by the most probable parameter values and for a ﬂat prior this would correspond to the log likelihood the second term penalizes the model according to its complexity because wposterior wprior this term is negative and it increases in magnitude as the ratio wposterior wprior gets smaller thus if parameters are ﬁnely tuned to the data in the posterior distribution then the penalty term is large for a model having a set of m parameters we can make a similar approximation for each parameter in turn assuming that all parameters have the same ratio of wposterior wprior we obtain ln p d ln p d w map ln wposterior wprior section thus in this very simple approximation the size of the complexity penalty increases linearly with the number m of adaptive parameters in the model as we increase the complexity of the model the ﬁrst term will typically decrease because a more complex model is better able to ﬁt the data whereas the second term will increase due to the dependence on m the optimal model complexity as determined by the maximum evidence will be given by a trade off between these two competing terms we shall later develop a more reﬁned version of this approximation based on a gaussian approximation to the posterior distribution we can gain further insight into bayesian model comparison and understand how the marginal likelihood can favour models of intermediate complexity by con sidering figure here the horizontal axis is a one dimensional representation of the space of possible data sets so that each point on this axis corresponds to a speciﬁc data set we now consider three models and of successively increasing complexity imagine running these models generatively to produce exam ple data sets and then looking at the distribution of data sets that result any given figure schematic illustration of the distribution of data sets for three models of different com plexity in which is the simplest and is the most complex note that the dis tributions are normalized in this example for the partic ular observed data set the model with intermedi ate complexity has the largest evidence p d d model can generate a variety of different data sets since the parameters are governed by a prior probability distribution and for any choice of the parameters there may be random noise on the target variables to generate a particular data set from a spe ciﬁc model we ﬁrst choose the values of the parameters from their prior distribution p w and then for these parameter values we sample the data from p w a sim ple model for example based on a ﬁrst order polynomial has little variability and so will generate data sets that are fairly similar to each other its distribution p is therefore conﬁned to a relatively small region of the horizontal axis by contrast a complex model such as a ninth order polynomial can generate a great variety of different data sets and so its distribution p is spread over a large region of the space of data sets because the distributions p i are normalized we see that the particular data set can have the highest value of the evidence for the model of intermediate complexity essentially the simpler model cannot ﬁt the data well whereas the more complex model spreads its predictive probability over too broad a range of data sets and so assigns relatively small probability to any one of them implicit in the bayesian model comparison framework is the assumption that the true distribution from which the data are generated is contained within the set of models under consideration provided this is so we can show that bayesian model comparison will on average favour the correct model to see this consider two models and in which the truth corresponds to for a given ﬁnite data set it is possible for the bayes factor to be larger for the incorrect model however if we average the bayes factor over the distribution of data sets we obtain the expected bayes factor in the form r p d m ln p d dd section where the average has been taken with respect to the true distribution of the data this quantity is an example of the kullback leibler divergence and satisﬁes the prop erty of always being positive unless the two distributions are equal in which case it is zero thus on average the bayes factor will always favour the correct model we have seen that the bayesian framework avoids the problem of over ﬁtting and allows models to be compared on the basis of the training data alone however a bayesian approach like any approach to pattern recognition needs to make as sumptions about the form of the model and if these are invalid then the results can be misleading in particular we see from figure that the model evidence can be sensitive to many aspects of the prior such as the behaviour in the tails indeed the evidence is not deﬁned if the prior is improper as can be seen by noting that an improper prior has an arbitrary scaling factor in other words the normalization coefﬁcient is not deﬁned because the distribution cannot be normalized if we con sider a proper prior and then take a suitable limit in order to obtain an improper prior for example a gaussian prior in which we take the limit of inﬁnite variance then the evidence will go to zero as can be seen from and figure it may however be possible to consider the evidence ratio between two models ﬁrst and then take a limit to obtain a meaningful answer in a practical application therefore it will be wise to keep aside an independent test set of data on which to evaluate the overall performance of the ﬁnal system the evidence approximation in a fully bayesian treatment of the linear basis function model we would intro duce prior distributions over the hyperparameters α and β and make predictions by marginalizing with respect to these hyperparameters as well as with respect to the parameters w however although we can integrate analytically over either w or over the hyperparameters the complete marginalization over all of these variables is analytically intractable here we discuss an approximation in which we set the hyperparameters to speciﬁc values determined by maximizing the marginal likeli hood function obtained by ﬁrst integrating over the parameters w this framework is known in the statistics literature as empirical bayes bernardo and smith gelman et al or type maximum likelihood berger or generalized maximum likelihood wahba and in the machine learning literature is also called the evidence approximation gull mackay if we introduce hyperpriors over α and β the predictive distribution is obtained by marginalizing over w α and β so that p t t rrr p t w β p w t α β p α β t dw dα dβ where p t w β is given by and p w t α β is given by with mn and sn deﬁned by and respectively here we have omitted the dependence on the input variable x to keep the notation uncluttered if the posterior distribution p α β t is sharply peaked around values α and β then the predictive distribution is obtained simply by marginalizing over w in which α and β are ﬁxed to the values α and β so that p t t p t t α β p t w β p w t α β dw 75 exercise exercise from bayes theorem the posterior distribution for α and β is given by p α β t p t α β p α β if the prior is relatively ﬂat then in the evidence framework the values of α and β are obtained by maximizing the marginal likelihood function p t α β we shall proceed by evaluating the marginal likelihood for the linear basis function model and then ﬁnding its maxima this will allow us to determine values for these hyperpa rameters from the training data alone without recourse to cross validation recall that the ratio α β is analogous to a regularization parameter as an aside it is worth noting that if we deﬁne conjugate gamma prior distri butions over α and β then the marginalization over these hyperparameters in can be performed analytically to give a student t distribution over w see sec tion although the resulting integral over w is no longer analytically tractable it might be thought that approximating this integral for example using the laplace approximation discussed section which is based on a local gaussian approxi mation centred on the mode of the posterior distribution might provide a practical alternative to the evidence framework buntine and weigend however the integrand as a function of w typically has a strongly skewed mode so that the laplace approximation fails to capture the bulk of the probability mass leading to poorer re sults than those obtained by maximizing the evidence mackay returning to the evidence framework we note that there are two approaches that we can take to the maximization of the log evidence we can evaluate the evidence function analytically and then set its derivative equal to zero to obtain re estimation equations for α and β which we shall do in section alternatively we use a technique called the expectation maximization em algorithm which will be dis cussed in section where we shall also show that these two approaches converge to the same solution evaluation of the evidence function the marginal likelihood function p t α β is obtained by integrating over the weight parameters w so that p t α β r p t w β p w α dw one way to evaluate this integral is to make use once again of the result 115 for the conditional distribution in a linear gaussian model here we shall evaluate the integral instead by completing the square in the exponent and making use of the standard form for the normalization coefﬁcient of a gaussian from and we can write the evidence function in the form β n α m r where m is the dimensionality of w and we have deﬁned e w βed w αew w β α wtw exercise we recognize as being equal up to a constant of proportionality to the reg ularized sum of squares error function we now complete the square over w giving e w e mn where we have introduced w mn ta w mn a αi βφtφ together with e m β φm m m exercise note that a corresponds to the matrix of second derivatives of the error function a e w and is known as the hessian matrix here we have also deﬁned mn given by mn βa using we see that a s and hence is equivalent to the previous deﬁnition and therefore represents the mean of the posterior distribution the integral over w can now be evaluated simply by appealing to the standard result for the normalization coefﬁcient of a multivariate gaussian giving r exp e w dw exp e mn r exp w m ta w m dw exp e mn m a 85 using we can then write the log of the marginal likelihood in the form ln p t α β m ln α n ln β e m ln a n ln which is the required expression for the evidence function returning to the polynomial regression problem we can plot the model evidence against the order of the polynomial as shown in figure here we have assumed a prior of the form with the parameter α ﬁxed at α the form of this plot is very instructive referring back to figure we see that the m polynomial has very poor ﬁt to the data and consequently gives a relatively low value figure plot of the model evidence versus the order m for the polynomial re gression model showing that the evidence favours the model with m 24 m for the evidence going to the m polynomial greatly improves the data ﬁt and hence the evidence is signiﬁcantly higher however in going to m the data ﬁt is improved only very marginally due to the fact that the underlying sinusoidal function from which the data is generated is an odd function and so has no even terms in a polynomial expansion indeed figure shows that the residual data error is reduced only slightly in going from m to m because this richer model suffers a greater complexity penalty the evidence actually falls in going from m to m when we go to m we obtain a signiﬁcant further improvement in data ﬁt as seen in figure and so the evidence is increased again giving the highest overall evidence for any of the polynomials further increases in the value of m produce only small improvements in the ﬁt to the data but suffer increasing complexity penalty leading overall to a decrease in the evidence values looking again at figure we see that the generalization error is roughly constant between m and m and it would be difﬁcult to choose between these models on the basis of this plot alone the evidence values however show a clear preference for m since this is the simplest model which gives a good explanation for the observed data maximizing the evidence function let us ﬁrst consider the maximization of p t α β with respect to α this can be done by ﬁrst deﬁning the following eigenvector equation βφtφ ui λiui from it then follows that a has eigenvalues α λi now consider the deriva tive of the term involving ln a in with respect to α we have d ln a d dα dα ln tt λi α d dα ln λi i λi α i thus the stationary points of with respect to α satisfy m m m multiplying through by and rearranging we obtain αmt mn λi α i since there are m terms in the sum over i the quantity γ can be written λi α λi exercise i the interpretation of the quantity γ will be discussed shortly from we see that the value of α that maximizes the marginal likelihood satisﬁes γ α mt mn note that this is an implicit solution for α not only because γ depends on α but also because the mode mn of the posterior distribution itself depends on the choice of α we therefore adopt an iterative procedure in which we make an initial choice for α and use this to ﬁnd mn which is given by 53 and also to evaluate γ which is given by these values are then used to re estimate α using and the process repeated until convergence note that because the matrix φtφ is ﬁxed we can compute its eigenvalues once at the start and then simply multiply these by β to obtain the λi it should be emphasized that the value of α has been determined purely by look ing at the training data in contrast to maximum likelihood methods no independent data set is required in order to optimize the model complexity we can similarly maximize the log marginal likelihood with respect to β to do this we note that the eigenvalues λi deﬁned by are proportional to β and hence dλi dβ λi β giving d ln a d ln λ α λi γ the stationary point of the marginal likelihood therefore satisﬁes exercise n and rearranging we obtain n n n mt φ xn γ 94 t mt φ x again this is an implicit solution for β and can be solved by choosing an initial value for β and then using this to calculate mn and γ and then re estimate β using repeating until convergence if both α and β are to be determined from the data then their values can be re estimated together after each update of γ figure contours of the likelihood function red and the prior green in which the axes in parameter space have been rotated to align with the eigenvectors ui of the hessian for α the mode of the poste rior is given by the maximum likelihood solution wml whereas for nonzero α the mode is at wmap mn in the direction the eigenvalue deﬁned by is small compared with α and so the quantity α is close to zero and the corresponding map value of is also close to zero by contrast in the direction the eigenvalue is large compared with α and so the quantity α is close to unity and the map value of is close to its maximum likelihood value wmap wml effective number of parameters the result has an elegant interpretation mackay which provides insight into the bayesian solution for α to see this consider the contours of the like lihood function and the prior as illustrated in figure here we have implicitly transformed to a rotated set of axes in parameter space aligned with the eigenvec tors ui deﬁned in contours of the likelihood function are then axis aligned ellipses the eigenvalues λi measure the curvature of the likelihood function and so in figure the eigenvalue is small compared with because a smaller curvature corresponds to a greater elongation of the contours of the likelihood func tion because βφtφ is a positive deﬁnite matrix it will have positive eigenvalues and so the ratio λi λi α will lie between and consequently the quantity γ deﬁned by will lie in the range γ m for directions in which λi α the corresponding parameter wi will be close to its maximum likelihood value and the ratio λi λi α will be close to such parameters are called well determined because their values are tightly constrained by the data conversely for directions in which λi α the corresponding parameters wi will be close to zero as will the ratios λi λi α these are directions in which the likelihood function is relatively insensitive to the parameter value and so the parameter has been set to a small value by the prior the quantity γ deﬁned by therefore measures the effective total number of well determined parameters we can obtain some insight into the result for re estimating β by com paring it with the corresponding maximum likelihood result given by both of these formulae express the variance the inverse precision as an average of the squared differences between the targets and the model predictions however they differ in that the number of data points n in the denominator of the maximum like lihood result is replaced by n γ in the bayesian result we recall from that the maximum likelihood estimate of the variance for a gaussian distribution over a single variable x is given by ml n n xn n µml and that this estimate is biased because the maximum likelihood solution µml for the mean has ﬁtted some of the noise on the data in effect this has used up one degree of freedom in the model the corresponding unbiased estimate is given by and takes the form x µ we shall see in section that this result can be obtained from a bayesian treat ment in which we marginalize over the unknown mean the factor of n in the denominator of the bayesian result takes account of the fact that one degree of free dom has been used in ﬁtting the mean and removes the bias of maximum likelihood now consider the corresponding results for the linear regression model the mean of the target distribution is now given by the function wtφ x which contains m parameters however not all of these parameters are tuned to the data the effective number of parameters that are determined by the data is γ with the remaining m γ parameters set to small values by the prior this is reﬂected in the bayesian result for the variance that has a factor n γ in the denominator thereby correcting for the bias of the maximum likelihood result we can illustrate the evidence framework for setting hyperparameters using the sinusoidal synthetic data set from section together with the gaussian basis func tion model comprising basis functions so that the total number of parameters in the model is given by m including the bias here for simplicity of illustra tion we have set β to its true value of and then used the evidence framework to determine α as shown in figure we can also see how the parameter α controls the magnitude of the parameters wi by plotting the individual parameters versus the effective number γ of param eters as shown in figure if we consider the limit n m in which the number of data points is large in relation to the number of parameters then from all of the parameters will be well determined by the data because φtφ involves an implicit sum over data points and so the eigenvalues λi increase with the size of the data set in this case γ m and the re estimation equations for α and β become m α mn β n d mn where ew and ed are deﬁned by 25 and respectively these results can be used as an easy to compute approximation to the full evidence re estimation ln α ln α figure the left plot shows γ red curve and mn blue curve versus ln α for the sinusoidal synthetic data set it is the intersection of these two curves that deﬁnes the optimum value for α given by the evidence procedure the right plot shows the corresponding graph of log evidence ln p t α β versus ln α red curve showing that the peak coincides with the crossing point of the curves in the left plot also shown is the test set error blue curve showing that the evidence maximum occurs close to the point of best generalization formulae because they do not require evaluation of the eigenvalue spectrum of the hessian figure plot of the parameters wi from the gaussian basis function model versus the effective num ber of parameters γ in which the wi hyperparameter α is varied in the range α causing γ to vary in the range γ m γ limitations of fixed basis functions throughout this chapter we have focussed on models comprising a linear combina tion of ﬁxed nonlinear basis functions we have seen that the assumption of linearity in the parameters led to a range of useful properties including closed form solutions to the least squares problem as well as a tractable bayesian treatment furthermore for a suitable choice of basis functions we can model arbitrary nonlinearities in the mapping from input variables to targets in the next chapter we shall study an anal ogous class of models for classiﬁcation it might appear therefore that such linear models constitute a general purpose framework for solving problems in pattern recognition unfortunately there are some signiﬁcant shortcomings with linear models which will cause us to turn in later chapters to more complex models such as support vector machines and neural networks the difﬁculty stems from the assumption that the basis functions φj x are ﬁxed before the training data set is observed and is a manifestation of the curse of dimen sionality discussed in section as a consequence the number of basis functions needs to grow rapidly often exponentially with the dimensionality d of the input space fortunately there are two properties of real data sets that we can exploit to help alleviate this problem first of all the data vectors xn typically lie close to a non linear manifold whose intrinsic dimensionality is smaller than that of the input space as a result of strong correlations between the input variables we will see an example of this when we consider images of handwritten digits in chapter if we are using localized basis functions we can arrange that they are scattered in input space only in regions containing data this approach is used in radial basis function networks and also in support vector and relevance vector machines neural network models which use adaptive basis functions having sigmoidal nonlinearities can adapt the parameters so that the regions of input space over which the basis functions vary corresponds to the data manifold the second property is that target variables may have signiﬁcant dependence on only a small number of possible directions within the data manifold neural networks can exploit this property by choosing the directions in input space to which the basis functions respond exercises www show that the tanh function and the logistic sigmoid function are related by tanh a hence show that a general linear combination of logistic sigmoid functions of the form y x w j wjσ x µj is equivalent to a linear combination of tanh functions of the form y x u uj j tanh x µj and ﬁnd expressions to relate the new parameters um to the original pa rameters wm show that the matrix φ φtφ takes any vector v and projects it onto the space spanned by the columns of φ use this result to show that the least squares solution corresponds to an orthogonal projection of the vector t onto the manifold s as shown in figure consider a data set in which each data point tn is associated with a weighting factor rn so that the sum of squares error function becomes e w r t wtφ x find an expression for the solution w that minimizes this error function give two alternative interpretations of the weighted sum of squares error function in terms of i data dependent noise variance and ii replicated data points www consider a linear model of the form d y x w wixi i together with a sum of squares error function of the form e w y x w t now suppose that gaussian noise ei with zero mean and variance is added in dependently to each of the input variables xi by making use of e ei and e eiej show that minimizing ed averaged over the noise distribution is equivalent to minimizing the sum of squares error for noise free input variables with the addition of a weight decay regularization term in which the bias parameter is omitted from the regularizer 5 www using the technique of lagrange multipliers discussed in appendix e show that minimization of the regularized error function is equivalent to mini mizing the unregularized sum of squares error 12 subject to the constraint discuss the relationship between the parameters η and λ www consider a linear basis function regression model for a multivariate target variable t having a gaussian distribution of the form p t w σ n t y x w σ where y x w wtφ x together with a training data set comprising input basis vectors φ xn and corre sponding target vectors tn with n n show that the maximum likelihood solution wml for the parameter matrix w has the property that each column is given by an expression of the form which was the solution for an isotropic noise distribution note that this is independent of the covariance matrix σ show that the maximum likelihood solution for σ is given by σ n n tn n t φ xn tn t φ xn t by using the technique of completing the square verify the result 49 for the posterior distribution of the parameters w in the linear basis function model in which mn and sn are deﬁned by 50 and 51 respectively www consider the linear basis function model in section and suppose that we have already observed n data points so that the posterior distribution over w is given by 49 this posterior can be regarded as the prior for the next obser vation by considering an additional data point xn tn and by completing the square in the exponential show that the resulting posterior distribution is again given by 49 but with sn replaced by sn and mn replaced by mn repeat the previous exercise but instead of completing the square by hand make use of the general result for linear gaussian models given by 116 www by making use of the result 115 to evaluate the integral in verify that the predictive distribution for the bayesian linear regression model is given by in which the input dependent variance is given by we have seen that as the size of a data set increases the uncertainty associated with the posterior distribution over model parameters decreases make use of the matrix identity appendix c m vtm to show that the uncertainty x associated with the linear regression function given by satisﬁes n x x 12 we saw in section that the conjugate prior for a gaussian distribution with unknown mean and unknown precision inverse variance is a normal gamma distribution this property also holds for the case of the conditional gaussian dis tribution p t x w β of the linear regression model if we consider the likelihood function then the conjugate prior for w and β is given by p w β n w β gam β b0 show that the corresponding posterior distribution takes the same functional form so that p w β t n w mn β gam β an bn 113 and ﬁnd expressions for the posterior parameters mn sn an and bn show that the predictive distribution p t x t for the model discussed in ex ercise 12 is given by a student t distribution of the form p t x t st t µ λ ν and obtain expressions for µ λ and ν in this exercise we explore in more detail the properties of the equivalent kernel deﬁned by where sn is deﬁned by 54 suppose that the basis functions φj x are linearly independent and that the number n of data points is greater than the number m of basis functions furthermore let one of the basis functions be constant say x by taking suitable linear combinations of these basis functions we can construct a new basis set ψj x spanning the same space but that are orthonormal so that n ψj xn ψk xn ijk 115 n where ijk is deﬁned to be if j k and otherwise and we take x show that for α the equivalent kernel can be written as k x xi ψ x tψ xi where ψ ψm t use this result to show that the kernel satisﬁes the summation constraint n k x xn 116 n 15 www consider a linear basis function model for regression in which the pa rameters α and β are set using the evidence framework show that the function e mn deﬁned by satisﬁes the relation mn n 16 derive the result 86 for the log evidence function p t α β of the linear regression model by making use of 115 to evaluate the integral directly show that the evidence function for the bayesian linear regression model can be written in the form 78 in which e w is deﬁned by 18 www by completing the square over w show that the error function 79 in bayesian linear regression can be written in the form 80 19 show that the integration over w in the bayesian linear regression model gives the result 85 hence show that the log marginal likelihood is given by 86 www starting from 86 verify all of the steps needed to show that maxi mization of the log marginal likelihood function 86 with respect to α leads to the re estimation equation an alternative way to derive the result 92 for the optimal value of α in the evidence framework is to make use of the identity d ln a tr a d a 117 prove this identity by considering the eigenvalue expansion of a real symmetric matrix a and making use of the standard results for the determinant and trace of a expressed in terms of its eigenvalues appendix c then make use of 117 to derive 92 starting from 86 22 starting from 86 verify all of the steps needed to show that maximiza tion of the log marginal likelihood function 86 with respect to β leads to the re estimation equation www show that the marginal probability of the data in other words the model evidence for the model described in exercise 12 is given by p t γ an sn n ban γ by ﬁrst marginalizing with respect to w and then with respect to β 24 repeat the previous exercise but now use bayes theorem in the form p t p t w β p w β p w β t 119 and then substitute for the prior and posterior distributions and the likelihood func tion in order to derive the result 118 in the previous chapter we explored a class of regression models having particularly simple analytical and computational properties we now discuss an analogous class of models for solving classiﬁcation problems the goal in classiﬁcation is to take an input vector x and to assign it to one of k discrete classes k where k k in the most common scenario the classes are taken to be disjoint so that each input is assigned to one and only one class the input space is thereby divided into decision regions whose boundaries are called decision boundaries or decision surfaces in this chapter we consider linear models for classiﬁcation by which we mean that the decision surfaces are linear functions of the input vector x and hence are deﬁned by d dimensional hyperplanes within the d dimensional input space data sets whose classes can be separated exactly by linear decision surfaces are said to be linearly separable for regression problems the target variable t was simply the vector of real num bers whose values we wish to predict in the case of classiﬁcation there are various 179 ways of using target values to represent class labels for probabilistic models the most convenient in the case of two class problems is the binary representation in which there is a single target variable t such that t represents class and t represents class we can interpret the value of t as the probability that the class is with the values of probability taking only the extreme values of and for k classes it is convenient to use a of k coding scheme in which t is a vector of length k such that if the class is j then all elements tk of t are zero except element tj which takes the value for instance if we have k 5 classes then a pattern from class would be given the target vector t t again we can interpret the value of tk as the probability that the class is k for nonprobabilistic models alternative choices of target variable representation will sometimes prove convenient in chapter we identiﬁed three distinct approaches to the classiﬁcation prob lem the simplest involves constructing a discriminant function that directly assigns each vector x to a speciﬁc class a more powerful approach however models the conditional probability distribution p k x in an inference stage and then subse quently uses this distribution to make optimal decisions by separating inference and decision we gain numerous beneﬁts as discussed in section 5 there are two different approaches to determining the conditional probabilities p k x one technique is to model them directly for example by representing them as parametric models and then optimizing the parameters using a training set alternatively we can adopt a generative approach in which we model the class conditional densities given by p x k together with the prior probabilities p k for the classes and then we compute the required posterior probabilities using bayes theorem p ck x p x ck p ck p x we shall discuss examples of all three approaches in this chapter in the linear regression models considered in chapter the model prediction y x w was given by a linear function of the parameters w in the simplest case the model is also linear in the input variables and therefore takes the form y x wtx so that y is a real number for classiﬁcation problems however we wish to predict discrete class labels or more generally posterior probabilities that lie in the range to achieve this we consider a generalization of this model in which we transform the linear function of w using a nonlinear function f so that y x f wtx in the machine learning literature f is known as an activation function whereas its inverse is called a link function in the statistics literature the decision surfaces correspond to y x constant so that wtx constant and hence the deci sion surfaces are linear functions of x even if the function f is nonlinear for this reason the class of models described by are called generalized linear models mccullagh and nelder note however that in contrast to the models used for regression they are no longer linear in the parameters due to the presence of the nonlinear function f this will lead to more complex analytical and computa tional properties than for linear regression models nevertheless these models are still relatively simple compared to the more general nonlinear models that will be studied in subsequent chapters the algorithms discussed in this chapter will be equally applicable if we ﬁrst make a ﬁxed nonlinear transformation of the input variables using a vector of basis functions φ x as we did for regression models in chapter we begin by consider ing classiﬁcation directly in the original input space x while in section we shall ﬁnd it convenient to switch to a notation involving basis functions for consistency with later chapters discriminant functions a discriminant is a function that takes an input vector x and assigns it to one of k classes denoted k in this chapter we shall restrict attention to linear discriminants namely those for which the decision surfaces are hyperplanes to simplify the dis cussion we consider ﬁrst the case of two classes and then investigate the extension to k classes two classes the simplest representation of a linear discriminant function is obtained by tak ing a linear function of the input vector so that y x wtx where w is called a weight vector and is a bias not to be confused with bias in the statistical sense the negative of the bias is sometimes called a threshold an input vector x is assigned to class if y x and to class otherwise the cor responding decision boundary is therefore deﬁned by the relation y x which corresponds to a d dimensional hyperplane within the d dimensional input space consider two points xa and xb both of which lie on the decision surface because y xa y xb we have wt xa xb and hence the vector w is orthogonal to every vector lying within the decision surface and so w determines the orientation of the decision surface similarly if x is a point on the decision surface then y x and so the normal distance from the origin to the decision surface is given by wtx lwl w lwl 5 we therefore see that the bias parameter determines the location of the decision surface these properties are illustrated for the case of d in figure furthermore we note that the value of y x gives a signed measure of the per pendicular distance r of the point x from the decision surface to see this consider figure illustration of the geometry of a linear discriminant function in two dimensions the decision surface shown in red is perpen y dicular to w and its displacement from the y origin is controlled by the bias parameter also the signed orthogonal distance of a gen eral point x from the decision surface is given by y x w an arbitrary point x and let x be its orthogonal projection onto the decision surface so that x x r w lwl multiplying both sides of this result by wt and adding and making use of y x wtx and y x wtx we have y x r this result is illustrated in figure lwl as with the linear regression models in chapter it is sometimes convenient to use a more compact notation in which we introduce an additional dummy input value x and then deﬁne w w w and x x x so that y x wtx in this case the decision surfaces are d dimensional hyperplanes passing through the origin of the d dimensional expanded input space multiple classes now consider the extension of linear discriminants to k classes we might be tempted be to build a k class discriminant by combining a number of two class discriminant functions however this leads to some serious difﬁculties duda and hart as we now show consider the use of k classiﬁers each of which solves a two class problem of separating points in a particular class k from points not in that class this is known as a one versus the rest classiﬁer the left hand example in figure shows an not c not figure attempting to construct a k class discriminant from a set of two class discriminants leads to am biguous regions shown in green on the left is an example involving the use of two discriminants designed to distinguish points in class ck from points not in class ck on the right is an example involving three discriminant functions each of which is used to separate a pair of classes ck and cj example involving three classes where this approach leads to regions of input space that are ambiguously classiﬁed an alternative is to introduce k k binary discriminant functions one for every possible pair of classes this is known as a one versus one classiﬁer each point is then classiﬁed according to a majority vote amongst the discriminant func tions however this too runs into the problem of ambiguous regions as illustrated in the right hand diagram of figure we can avoid these difﬁculties by considering a single k class discriminant comprising k linear functions of the form yk x wtx and then assigning a point x to class ck if yk x yj x for all j k the decision boundary between class ck and class cj is therefore given by yk x yj x and hence corresponds to a d dimensional hyperplane deﬁned by wk wj tx this has the same form as the decision boundary for the two class case discussed in section and so analogous geometrical properties apply the decision regions of such a discriminant are always singly connected and convex to see this consider two points xa and xb both of which lie inside decision region k as illustrated in figure any point x that lies on the line connecting xa and xb can be expressed in the form x λxa λ xb figure illustration of the decision regions for a mul ticlass linear discriminant with the decision boundaries shown in red if two points xa and xb both lie inside the same decision re j connecting these two pobints must also lie in ri singly connected and convex k xb xa xˆ where λ from the linearity of the discriminant functions it follows that yk x λyk xa λ yk xb 12 because both xa and xb lie inside k it follows that yk xa yj xa and yk xb yj xb for all j k and hence yk x yj x and so x also lies inside k thus k is singly connected and convex note that for two classes we can either employ the formalism discussed here based on two discriminant functions x and x or else use the simpler but equivalent formulation described in section based on a single discriminant function y x we now explore three approaches to learning the parameters of linear discrimi nant functions based on least squares fisher linear discriminant and the percep tron algorithm least squares for classiﬁcation in chapter we considered models that were linear functions of the parame ters and we saw that the minimization of a sum of squares error function led to a simple closed form solution for the parameter values it is therefore tempting to see if we can apply the same formalism to classiﬁcation problems consider a general classiﬁcation problem with k classes with a of k binary coding scheme for the target vector t one justiﬁcation for using least squares in such a context is that it approximates the conditional expectation e t x of the target values given the input vector for the binary coding scheme this conditional expectation is given by the vector of posterior class probabilities unfortunately however these probabilities are typically approximated rather poorly indeed the approximations can have values outside the range due to the limited ﬂexibility of a linear model as we shall see shortly each class ck is described by its own linear model so that yk x wtx where k k we can conveniently group these together using vector nota tion so that y x w tx k k new input x is then assigned to the class for which the output yk wtx is largest we now determine the parameter matrix w by minimizing a sum of squares error function as we did for regression in chapter consider a training data set xn tn where n n and deﬁne a matrix t whose nth row is the vector tt together with a matrix x whose nth row is x t the sum of squares error function can then be written as ed w tr j x w t t x w t l 15 setting the derivative with respect to w to zero and rearranging we then obtain the solution for w in the form w xtx x t 16 where x is the pseudo inverse of the matrix x as discussed in section we then obtain the discriminant function in the form y x w tx tt x t x 17 exercise an interesting property of least squares solutions with multiple target variables is that if every target vector in the training set satisﬁes some linear constraint attn b 18 for some constants a and b then the model prediction for any value of x will satisfy the same constraint so that section aty x b 19 thus if we use a of k coding scheme for k classes then the predictions made by the model will have the property that the elements of y x will sum to for any value of x however this summation constraint alone is not sufﬁcient to allow the model outputs to be interpreted as probabilities because they are not constrained to lie within the interval the least squares approach gives an exact closed form solution for the discrimi nant function parameters however even as a discriminant function where we use it to make decisions directly and dispense with any probabilistic interpretation it suf fers from some severe problems we have already seen that least squares solutions lack robustness to outliers and this applies equally to the classiﬁcation application as illustrated in figure here we see that the additional data points in the right hand ﬁgure produce a signiﬁcant change in the location of the decision boundary even though these point would be correctly classiﬁed by the original decision bound ary in the left hand ﬁgure the sum of squares error function penalizes predictions that are too correct in that they lie a long way on the correct side of the decision figure the left plot shows data from two classes denoted by red crosses and blue circles together with the decision boundary found by least squares magenta curve and also by the logistic regression model green curve which is discussed later in section the right hand plot shows the corresponding results obtained when extra data points are added at the bottom left of the diagram showing that least squares is highly sensitive to outliers unlike logistic regression boundary in section we shall consider several alternative error functions for classiﬁcation and we shall see that they do not suffer from this difﬁculty however problems with least squares can be more severe than simply lack of robustness as illustrated in figure 5 this shows a synthetic data set drawn from three classes in a two dimensional input space having the property that lin ear decision boundaries can give excellent separation between the classes indeed the technique of logistic regression described later in this chapter gives a satisfac tory solution as seen in the right hand plot however the least squares solution gives poor results with only a small region of the input space assigned to the green class the failure of least squares should not surprise us when we recall that it cor responds to maximum likelihood under the assumption of a gaussian conditional distribution whereas binary target vectors clearly have a distribution that is far from gaussian by adopting more appropriate probabilistic models we shall obtain clas siﬁcation techniques with much better properties than least squares for the moment however we continue to explore alternative nonprobabilistic methods for setting the parameters in the linear classiﬁcation models fisher linear discriminant one way to view a linear classiﬁcation model is in terms of dimensionality reduction consider ﬁrst the case of two classes and suppose we take the d 6 6 6 6 6 figure 5 example of a synthetic data set comprising three classes with training data points denoted in red green and blue lines denote the decision boundaries and the background colours denote the respective classes of the decision regions on the left is the result of using a least squares discriminant we see that the region of input space assigned to the green class is too small and so most of the points from this class are misclassiﬁed on the right is the result of using logistic regressions as described in section showing correct classiﬁcation of the training data dimensional input vector x and project it down to one dimension using y wtx if we place a threshold on y and classify y as class and otherwise class then we obtain our standard linear classiﬁer discussed in the previous section in general the projection onto one dimension leads to a considerable loss of infor mation and classes that are well separated in the original d dimensional space may become strongly overlapping in one dimension however by adjusting the com ponents of the weight vector w we can select a projection that maximizes the class separation to begin with consider a two class problem in which there are points of class and points of class so that the mean vectors of the two classes are given by m n xn n xn n n the simplest measure of the separation of the classes when projected onto w is the separation of the projected class means this suggests that we might choose w so as to maximize where wt 22 mk wtmk 23 6 6 figure 6 the left plot shows samples from two classes depicted in red and blue along with the histograms resulting from projection onto the line joining the class means note that there is considerable class overlap in the projected space the right plot shows the corresponding projection based on the fisher linear discriminant showing the greatly improved class separation is the mean of the projected data from class k however this expression can be made arbitrarily large simply by increasing the magnitude of w to solve this problem we could constrain w to have unit length so that i using appendix e exercise a lagrange multiplier to perform the constrained maximization we then ﬁnd that w there is still a problem with this approach however as illustrated in figure 6 this shows two classes that are well separated in the original two dimensional space x2 but that have considerable overlap when projected onto the line joining their means this difﬁculty arises from the strongly nondiagonal covariances of the class distributions the idea proposed by fisher is to maximize a function that will give a large separation between the projected class means while also giving a small variance within each class thereby minimizing the class overlap the projection formula transforms the set of labelled data points in x into a labelled set in the one dimensional space y the within class variance of the transformed data from class ck is therefore given by yn mk 24 n ck where yn wtxn we can deﬁne the total within class variance for the whole data set to be simply the fisher criterion is deﬁned to be the ratio of the between class variance to the within class variance and is given by j w s2 25 exercise 5 we can make the dependence on w explicit by using 23 and 24 to rewrite the fisher criterion in the form j w wtsbw wtsww where sb is the between class covariance matrix and is given by sb t and sw is the total within class covariance matrix given by sw xn xn t xn xn t differentiating 26 with respect to w we ﬁnd that j w is maximized when wtsbw sww wtsww sbw from we see that sbw is always in the direction of furthermore we do not care about the magnitude of w only its direction and so we can drop the scalar factors wtsbw and wtsww multiplying both sides of by s we then obtain w s 30 note that if the within class covariance is isotropic so that sw is proportional to the unit matrix we ﬁnd that w is proportional to the difference of the class means as discussed above the result 30 is known as fisher linear discriminant although strictly it is not a discriminant but rather a speciﬁc choice of direction for projection of the data down to one dimension however the projected data can subsequently be used to construct a discriminant by choosing a threshold so that we classify a new point as belonging to if y x and classify it as belonging to otherwise for example we can model the class conditional densities p y k using gaussian distributions and then use the techniques of section to ﬁnd the parameters of the gaussian distributions by maximum likelihood having found gaussian ap proximations to the projected classes the formalism of section 5 then gives an expression for the optimal threshold some justiﬁcation for the gaussian assumption comes from the central limit theorem by noting that y wtx is the sum of a set of random variables 5 relation to least squares the least squares approach to the determination of a linear discriminant was based on the goal of making the model predictions as close as possible to a set of target values by contrast the fisher criterion was derived by requiring maximum class separation in the output space it is interesting to see the relationship between these two approaches in particular we shall show that for the two class problem the fisher criterion can be obtained as a special case of least squares so far we have considered of k coding for the target values if however we adopt a slightly different target coding scheme then the least squares solution for the weights becomes equivalent to the fisher solution duda and hart in particular we shall take the targets for class to be n where is the number of patterns in class and n is the total number of patterns this target value approximates the reciprocal of the prior probability for class for class we shall take the targets to be n where is the number of patterns in class the sum of squares error function can be written n wtxn n tn 31 setting the derivatives of e with respect to and w to zero we obtain respectively n wtxn tn n n wtxn tn xn n from 32 and making use of our choice of target coding scheme for the tn we obtain an expression for the bias in the form wtm where we have used n tn n n n 35 and where m is the mean of the total data set and is given by m x n m n m 36 exercise 6 after some straightforward algebra and again making use of the choice of tn the second equation 33 becomes sw s n b w n where sw is deﬁned by sb is deﬁned by and we have substituted for the bias using using we note that sbw is always in the direction of m2 thus we can write w s m2 m1 38 where we have ignored irrelevant scale factors thus the weight vector coincides with that found from the fisher criterion in addition we have also found an expres sion for the bias value given by 34 this tells us that a new vector x should be classiﬁed as belonging to class if y x wt x m and class otherwise 6 fisher discriminant for multiple classes we now consider the generalization of the fisher discriminant to k classes and we shall assume that the dimensionality d of the input space is greater than the number k of classes next we introduce di linear features yk wtx where k di these feature values can conveniently be grouped together to form a vector y similarly the weight vectors wk can be considered to be the columns of a matrix w so that y wtx 39 note that again we are not including any bias parameters in the deﬁnition of y the generalization of the within class covariance matrix to the case of k classes follows from 28 to give where k sw sk k sk xn mk xn mk t n ck m k nk xn n ck and nk is the number of patterns in class k in order to ﬁnd a generalization of the between class covariance matrix we follow duda and hart and consider ﬁrst the total covariance matrix n st xn m xn m t 43 n where m is the mean of the total data set m n n xn n n k nkmk k 44 and n k nk is the total number of data points the total covariance matrix can be decomposed into the sum of the within class covariance matrix given by and plus an additional matrix sb which we identify as a measure of the between class covariance where st sw sb 45 k sb nk mk m mk m t 46 k these covariance matrices have been deﬁned in the original x space we can now deﬁne similar matrices in the projected di dimensional y space and where k sw yn µk yn µk t 47 k n ck k sb nk µk µ µk µ t 48 k µ k nk yn n ck µ n k nk k µk 49 again we wish to construct a scalar that is large when the between class covariance is large and when the within class covariance is small there are now many possible choices of criterion fukunaga one example is given by w this criterion can then be rewritten as an explicit function of the projection matrix w in the form j w tr wswwt wsbwt 51 maximization of such criteria is straightforward though somewhat involved and is discussed at length in fukunaga the weight values are determined by those eigenvectors of s that correspond to the di largest eigenvalues there is one important result that is common to all such criteria which is worth emphasizing we ﬁrst note from 46 that sb is composed of the sum of k ma trices each of which is an outer product of two vectors and therefore of rank in addition only k of these matrices are independent as a result of the constraint 44 thus sb has rank at most equal to k and so there are at most k nonzero eigenvalues this shows that the projection onto the k dimensional subspace spanned by the eigenvectors of sb does not alter the value of j w and so we are therefore unable to ﬁnd more than k linear features by this means fukunaga the perceptron algorithm another example of a linear discriminant model is the perceptron of rosenblatt which occupies an important place in the history of pattern recognition al gorithms it corresponds to a two class model in which the input vector x is ﬁrst transformed using a ﬁxed nonlinear transformation to give a feature vector φ x and this is then used to construct a generalized linear model of the form y x f wtφ x where the nonlinear activation function f is given by a step function of the form a a 53 the vector φ x will typically include a bias component x in earlier discussions of two class classiﬁcation problems we have focussed on a target coding scheme in which t which is appropriate in the context of probabilistic models for the perceptron however it is more convenient to use target values t for class and t for class which matches the choice of activation function the algorithm used to determine the parameters w of the perceptron can most easily be motivated by error function minimization a natural choice of error func tion would be the total number of misclassiﬁed patterns however this does not lead to a simple learning algorithm because the error is a piecewise constant function of w with discontinuities wherever a change in w causes the decision boundary to move across one of the data points methods based on changing w using the gradi ent of the error function cannot then be applied because the gradient is zero almost everywhere we therefore consider an alternative error function known as the perceptron cri terion to derive this we note that we are seeking a weight vector w such that patterns xn in class will have wtφ xn whereas patterns xn in class have wtφ xn using the t target coding scheme it follows that we would like all patterns to satisfy wtφ xn tn the perceptron criterion associates zero error with any pattern that is correctly classiﬁed whereas for a mis classiﬁed pattern xn it tries to minimize the quantity wtφ xn tn the perceptron criterion is therefore given by ep w wtφntn 54 n m frank rosenblatt rosenblatt perceptron played an important role in the history of ma chine learning initially rosenblatt simulated the perceptron on an ibm computer at cornell in but by the early he had built special purpose hardware that provided a direct par allel implementation of perceptron learning many of his ideas were encapsulated in principles of neuro dynamics perceptrons and the theory of brain mech anisms published in rosenblatt work was criticized by marvin minksy whose objections were published in the book perceptrons co authored with seymour papert this book was widely misinter preted at the time as showing that neural networks were fatally ﬂawed and could only learn solutions for linearly separable problems in fact it only proved such limitations in the case of single layer networks such as the perceptron and merely conjectured in correctly that they applied to more general network models unfortunately however this book contributed to the substantial decline in research funding for neu ral computing a situation that was not reversed un til the mid today there are many hundreds if not thousands of applications of neural networks in widespread use with examples in areas such as handwriting recognition and information retrieval be ing used routinely by millions of people section where denotes the set of all misclassiﬁed patterns the contribution to the error associated with a particular misclassiﬁed pattern is a linear function of w in regions of w space where the pattern is misclassiﬁed and zero in regions where it is correctly classiﬁed the total error function is therefore piecewise linear we now apply the stochastic gradient descent algorithm to this error function the change in the weight vector w is then given by w τ w τ η ep w w τ ηφntn where η is the learning rate parameter and τ is an integer that indexes the steps of the algorithm because the perceptron function y x w is unchanged if we multiply w by a constant we can set the learning rate parameter η equal to without of generality note that as the weight vector evolves during training the set of patterns that are misclassiﬁed will change the perceptron learning algorithm has a simple interpretation as follows we cycle through the training patterns in turn and for each pattern xn we evaluate the perceptron function 52 if the pattern is correctly classiﬁed then the weight vector remains unchanged whereas if it is incorrectly classiﬁed then for class we add the vector φ xn onto the current estimate of weight vector w while for class we subtract the vector φ xn from w the perceptron learning algorithm is illustrated in figure 7 if we consider the effect of a single update in the perceptron learning algorithm we see that the contribution to the error from a misclassiﬁed pattern will be reduced because from 55 we have w τ tφntn w τ tφntn φntn tφntn w τ tφntn 56 where we have set η and made use of φntn of course this does not imply that the contribution to the error function from the other misclassiﬁed patterns will have been reduced furthermore the change in weight vector may have caused some previously correctly classiﬁed patterns to become misclassiﬁed thus the perceptron learning rule is not guaranteed to reduce the total error function at each stage however the perceptron convergence theorem states that if there exists an ex act solution in other words if the training data set is linearly separable then the perceptron learning algorithm is guaranteed to ﬁnd an exact solution in a ﬁnite num ber of steps proofs of this theorem can be found for example in rosenblatt block nilsson minsky and papert hertz et al and bishop note however that the number of steps required to achieve con vergence could still be substantial and in practice until convergence is achieved we will not be able to distinguish between a nonseparable problem and one that is simply slow to converge even when the data set is linearly separable there may be many solutions and which one is found will depend on the initialization of the parameters and on the or der of presentation of the data points furthermore for data sets that are not linearly separable the perceptron learning algorithm will never converge 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 figure 7 illustration of the convergence of the perceptron learning algorithm showing data points from two classes red and blue in a two dimensional feature space the top left plot shows the initial parameter vector w shown as a black arrow together with the corresponding decision boundary black line in which the arrow points towards the decision region which classiﬁed as belonging to the red class the data point circled in green is misclassiﬁed and so its feature vector is added to the current weight vector giving the new decision boundary shown in the top right plot the bottom left plot shows the next misclassiﬁed point to be considered indicated by the green circle and its feature vector is again added to the weight vector giving the decision boundary shown in the bottom right plot for which all data points are correctly classiﬁed figure illustration of the mark perceptron hardware the photograph on the left shows how the inputs were obtained using a simple camera system in which an input scene in this case a printed character was illuminated by powerful lights and an image focussed onto a 20 20 array of cadmium sulphide photocells giving a primitive pixel image the perceptron also had a patch board shown in the middle photograph which allowed different conﬁgurations of input features to be tried often these were wired up at random to demonstrate the ability of the perceptron to learn without the need for precise wiring in contrast to a modern digital computer the photograph on the right shows one of the racks of adaptive weights each weight was implemented using a rotary variable resistor also called a potentiometer driven by an electric motor thereby allowing the value of the weight to be adjusted automatically by the learning algorithm aside from difﬁculties with the learning algorithm the perceptron does not pro vide probabilistic outputs nor does it generalize readily to k classes the most important limitation however arises from the fact that in common with all of the models discussed in this chapter and the previous one it is based on linear com binations of ﬁxed basis functions more detailed discussions of the limitations of perceptrons can be found in minsky and papert and bishop analogue hardware implementations of the perceptron were built by rosenblatt based on motor driven variable resistors to implement the adaptive parameters wj these are illustrated in figure the inputs were obtained from a simple camera system based on an array of photo sensors while the basis functions φ could be chosen in a variety of ways for example based on simple ﬁxed functions of randomly chosen subsets of pixels from the input image typical applications involved learning to discriminate simple shapes or characters at the same time that the perceptron was being developed a closely related system called the adaline which is short for adaptive linear element was being explored by widrow and co workers the functional form of the model was the same as for the perceptron but a different approach to training was adopted widrow and hoff widrow and lehr probabilistic generative models we turn next to a probabilistic view of classiﬁcation and show how models with linear decision boundaries arise from simple assumptions about the distribution of the data in section 5 we discussed the distinction between the discriminative and the generative approaches to classiﬁcation here we shall adopt a generative figure plot of the logistic sigmoid function σ a deﬁned by 59 shown in red together with the scaled pro bit function φ λa for π shown in dashed blue where φ a is deﬁned by 114 the scal ing factor π is chosen so that the derivatives of the two curves are equal for a 5 5 5 approach in which we model the class conditional densities p x k as well as the class priors p k and then use these to compute posterior probabilities p k x through bayes theorem consider ﬁrst of all the case of two classes the posterior probability for class can be written as p c x p x p p x c p c p x c p c where we have deﬁned exp a σ a a ln p x p p x p and σ a is the logistic sigmoid function deﬁned by 58 σ a exp a 59 which is plotted in figure the term sigmoid means s shaped this type of function is sometimes also called a squashing function because it maps the whole real axis into a ﬁnite interval the logistic sigmoid has been encountered already in earlier chapters and plays an important role in many classiﬁcation algorithms it satisﬁes the following symmetry property σ a σ a 60 as is easily veriﬁed the inverse of the logistic sigmoid is given by ln σ 61 σ and is known as the logit function it represents the log of the ratio of probabilities ln p x p x for the two classes also known as the log odds note that in we have simply rewritten the posterior probabilities in an equivalent form and so the appearance of the logistic sigmoid may seem rather vac uous however it will have signiﬁcance provided a x takes a simple functional form we shall shortly consider situations in which a x is a linear function of x in which case the posterior probability is governed by a generalized linear model for the case of k classes we have c x p x ck p ck p x c p c exp ak j exp aj which is known as the normalized exponential and can be regarded as a multiclass generalization of the logistic sigmoid here the quantities ak are deﬁned by ak ln p x ck p ck the normalized exponential is also known as the softmax function as it represents a smoothed version of the max function because if ak aj for all j k then p k x and p j x we now investigate the consequences of choosing speciﬁc forms for the class conditional densities looking ﬁrst at continuous input variables x and then dis cussing brieﬂy the case of discrete inputs continuous inputs let us assume that the class conditional densities are gaussian and then explore the resulting form for the posterior probabilities to start with we shall assume that all classes share the same covariance matrix thus the density for class k is given by p x c exp x µ tς x µ 64 d σ k k consider ﬁrst the case of two classes from 57 and 58 we have p x σ wtx where we have deﬁned w σ µ µ 66 w µtς µtς ln p p we see that the quadratic terms in x from the exponents of the gaussian densities have cancelled due to the assumption of common covariance matrices leading to a linear function of x in the argument of the logistic sigmoid this result is illus trated for the case of a two dimensional input space x in figure the resulting figure the left hand plot shows the class conditional densities for two classes denoted red and blue on the right is the corresponding posterior probability p x which is given by a logistic sigmoid of a linear function of x the surface in the right hand plot is coloured using a proportion of red ink given by p x and a proportion of blue ink given by p x p x decision boundaries correspond to surfaces along which the posterior probabilities p k x are constant and so will be given by linear functions of x and therefore the decision boundaries are linear in input space the prior probabilities p k enter only through the bias parameter so that changes in the priors have the effect of making parallel shifts of the decision boundary and more generally of the parallel contours of constant posterior probability for the general case of k classes we have from 62 and ak x wtx where we have deﬁned wk σ w µtς ln p c we see that the ak x are again linear functions of x as a consequence of the cancel lation of the quadratic terms due to the shared covariances the resulting decision boundaries corresponding to the minimum misclassiﬁcation rate will occur when two of the posterior probabilities the two largest are equal and so will be deﬁned by linear functions of x and so again we have a generalized linear model if we relax the assumption of a shared covariance matrix and allow each class conditional density p x k to have its own covariance matrix σk then the earlier cancellations will no longer occur and we will obtain quadratic functions of x giv ing rise to a quadratic discriminant the linear and quadratic decision boundaries are illustrated in figure 5 5 5 5 5 5 figure the left hand plot shows the class conditional densities for three classes each having a gaussian distribution coloured red green and blue in which the red and green classes have the same covariance matrix the right hand plot shows the corresponding posterior probabilities in which the rgb colour vector represents the posterior probabilities for the respective three classes the decision boundaries are also shown notice that the boundary between the red and green classes which have the same covariance matrix is linear whereas those between the other pairs of classes are quadratic maximum likelihood solution once we have speciﬁed a parametric functional form for the class conditional densities p x k we can then determine the values of the parameters together with the prior class probabilities p k using maximum likelihood this requires a data set comprising observations of x along with their corresponding class labels consider ﬁrst the case of two classes each having a gaussian class conditional density with a shared covariance matrix and suppose we have a data set xn tn where n n here tn denotes class and tn denotes class we denote the prior class probability p π so that p π for a data point xn from class we have tn and hence p xn p p xn πn xn σ similarly for class we have tn and hence p xn p p xn π n xn σ thus the likelihood function is given by p t π µ µ σ tt πn xn µ σ tn π n xn µ σ tn where t tn t as usual it is convenient to maximize the log of the likelihood function consider ﬁrst the maximization with respect to π the terms in the log likelihood function that depend on π are n tn ln π tn ln π n setting the derivative with respect to π equal to zero and rearranging we obtain π t exercise where denotes the total number of data points in class and denotes the total number of data points in class thus the maximum likelihood estimate for π is simply the fraction of points in class as expected this result is easily generalized to the multiclass case where again the maximum likelihood estimate of the prior probability associated with class k is given by the fraction of the training set points assigned to that class now consider the maximization with respect to again we can pick out of the log likelihood function those terms that depend on giving t ln n x µ σ t x µ tς x µ const setting the derivative with respect to to zero and rearranging we obtain n tnxn n 75 which is simply the mean of all the input vectors xn assigned to class by a similar argument the corresponding result for is given by n tn n xn which again is the mean of all the input vectors xn assigned to class finally consider the maximum likelihood solution for the shared covariance matrix σ picking out the terms in the log likelihood function that depend on σ we have n n n n n ln σ n n xn n µ tς xn n ln σ n n n xn µ tς xn n ln σ n tr σ 77 where we have deﬁned s s n s n 78 s x n s x n2 n xn xn t 79 µ2 t 80 exercise section 7 section using the standard result for the maximum likelihood solution for a gaussian distri bution we see that σ s which represents a weighted average of the covariance matrices associated with each of the two classes separately this result is easily extended to the k class problem to obtain the corresponding maximum likelihood solutions for the parameters in which each class conditional density is gaussian with a shared covariance matrix note that the approach of ﬁtting gaussian distributions to the classes is not robust to outliers because the maximum likelihood estimation of a gaussian is not robust 2 discrete features let us now consider the case of discrete feature values xi for simplicity we begin by looking at binary feature values xi and discuss the extension to more general discrete features shortly if there are d inputs then a general distribu tion would correspond to a table of numbers for each class containing independent variables due to the summation constraint because this grows expo nentially with the number of features we might seek a more restricted representa tion here we will make the naive bayes assumption in which the feature values are treated as independent conditioned on the class k thus we have class conditional distributions of the form d xi xi ki i which contain d independent parameters for each class substituting into then gives exercise 11 d ak x xi ln µki xi ln µki ln p ck 82 i which again are linear functions of the input values xi for the case of k 2 classes we can alternatively consider the logistic sigmoid formulation given by 57 anal ogous results are obtained for discrete variables each of which can take m 2 states 2 exponential family as we have seen for both gaussian distributed and discrete inputs the posterior class probabilities are given by generalized linear models with logistic sigmoid k 2 classes or softmax k 2 classes activation functions these are particular cases of a more general result obtained by assuming that the class conditional densities p x k are members of the exponential family of distributions using the form 2 194 for members of the exponential family we see that the distribution of x can be written in the form p x λk h x g λk exp λtu x we now restrict attention to the subclass of such distributions for which u x x then we make use of 2 236 to introduce a scaling parameter so that we obtain the restricted set of exponential family class conditional densities of the form p x λ h x g λ exp λtx note that we are allowing each class to have its own parameter vector λk but we are assuming that the classes share the same scale parameter for the two class problem we substitute this expression for the class conditional densities into 58 and we see that the posterior class probability is again given by a logistic sigmoid acting on a linear function a x which is given by a x tx ln g ln g ln p ln p 85 similarly for the k class problem we substitute the class conditional density ex pression into to give ak x λtx ln g λk ln p ck 86 and so again is a linear function of x probabilistic discriminative models for the two class classiﬁcation problem we have seen that the posterior probability of class can be written as a logistic sigmoid acting on a linear function of x for a wide choice of class conditional distributions p x k similarly for the multiclass case the posterior probability of class k is given by a softmax transformation of a linear function of x for speciﬁc choices of the class conditional densities p x k we have used maximum likelihood to determine the parameters of the densities as well as the class priors p k and then used bayes theorem to ﬁnd the posterior class probabilities however an alternative approach is to use the functional form of the generalized linear model explicitly and to determine its parameters directly by using maximum likelihood we shall see that there is an efﬁcient algorithm ﬁnding such solutions known as iterative reweighted least squares or irls the indirect approach to ﬁnding the parameters of a generalized linear model by ﬁtting class conditional densities and class priors separately and then applying x2 5 x1 5 figure 12 illustration of the role of nonlinear basis functions in linear classiﬁcation models the left plot shows the original input space x1 x2 together with data points from two classes labelled red and blue two gaussian basis functions x and x are deﬁned in this space with centres shown by the green crosses and with contours shown by the green circles the right hand plot shows the corresponding feature space together with the linear decision boundary obtained given by a logistic regression model of the form discussed in section 2 this corresponds to a nonlinear decision boundary in the original input space shown by the black curve in the left hand plot bayes theorem represents an example of generative modelling because we could take such a model and generate synthetic data by drawing values of x from the marginal distribution p x in the direct approach we are maximizing a likelihood function deﬁned through the conditional distribution p k x which represents a form of discriminative training one advantage of the discriminative approach is that there will typically be fewer adaptive parameters to be determined as we shall see shortly it may also lead to improved predictive performance particularly when the class conditional density assumptions give a poor approximation to the true dis tributions fixed basis functions so far in this chapter we have considered classiﬁcation models that work di rectly with the original input vector x however all of the algorithms are equally applicable if we ﬁrst make a ﬁxed nonlinear transformation of the inputs using a vector of basis functions φ x the resulting decision boundaries will be linear in the feature space φ and these correspond to nonlinear decision boundaries in the original x space as illustrated in figure 12 classes that are linearly separable in the feature space φ x need not be linearly separable in the original observation space x note that as in our discussion of linear models for regression one of the section 6 basis functions is typically set to a constant say x so that the correspond ing parameter plays the role of a bias for the remainder of this chapter we shall include a ﬁxed basis function transformation φ x as this will highlight some useful similarities to the regression models discussed in chapter for many problems of practical interest there is signiﬁcant overlap between the class conditional densities p x k this corresponds to posterior probabilities p k x which for at least some values of x are not 0 or in such cases the opti mal solution is obtained by modelling the posterior probabilities accurately and then applying standard decision theory as discussed in chapter note that nonlinear transformations φ x cannot remove such class overlap indeed they can increase the level of overlap or create overlap where none existed in the original observation space however suitable choices of nonlinearity can make the process of modelling the posterior probabilities easier such ﬁxed basis function models have important limitations and these will be resolved in later chapters by allowing the basis functions themselves to adapt to the data notwithstanding these limitations models with ﬁxed nonlinear basis functions play an important role in applications and a discussion of such models will intro duce many of the key concepts needed for an understanding of their more complex counterparts 2 logistic regression we begin our treatment of generalized linear models by considering the problem of two class classiﬁcation in our discussion of generative approaches in section 2 we saw that under rather general assumptions the posterior probability of class can be written as a logistic sigmoid acting on a linear function of the feature vector φ so that exercise 12 p φ y φ σ wtφ with p 2 φ p φ here σ is the logistic sigmoid function deﬁned by 59 in the terminology of statistics this model is known as logistic regression although it should be emphasized that this is a model for classiﬁcation rather than regression for an m dimensional feature space φ this model has m adjustable parameters by contrast if we had ﬁtted gaussian class conditional densities using maximum likelihood we would have used parameters for the means and m m 2 parameters for the shared covariance matrix together with the class prior p this gives a total of m m 5 2 parameters which grows quadratically with m in contrast to the linear dependence on m of the number of parameters in logistic regression for large values of m there is a clear advantage in working with the logistic regression model directly we now use maximum likelihood to determine the parameters of the logistic regression model to do this we shall make use of the derivative of the logistic sig moid function which can conveniently be expressed in terms of the sigmoid function itself dσ σ σ da for a data set φn tn where tn 0 and φn φ xn with n n tn tn n n where t tn t and yn p φn as usual we can deﬁne an error function by taking the negative logarithm of the likelihood which gives the cross entropy error function in the form exercise 13 n e w ln p t w tn ln yn tn ln yn n where yn σ an and an wtφn taking the gradient of the error function with respect to w we obtain section exercise 14 n e w yn tn φn n where we have made use of we see that the factor involving the derivative of the logistic sigmoid has cancelled leading to a simpliﬁed form for the gradient of the log likelihood in particular the contribution to the gradient from data point n is given by the error yn tn between the target value and the prediction of the model times the basis function vector φn furthermore comparison with 13 shows that this takes precisely the same form as the gradient of the sum of squares error function for the linear regression model if desired we could make use of the result to give a sequential algorithm in which patterns are presented one at a time in which each of the weight vectors is updated using 22 in which en is the nth term in it is worth noting that maximum likelihood can exhibit severe over ﬁtting for data sets that are linearly separable this arises because the maximum likelihood so lution occurs when the hyperplane corresponding to σ 0 5 equivalent to wtφ 0 separates the two classes and the magnitude of w goes to inﬁnity in this case the logistic sigmoid function becomes inﬁnitely steep in feature space corresponding to a heaviside step function so that every training point from each class k is assigned a posterior probability p k x furthermore there is typically a continuum of such solutions because any separating hyperplane will give rise to the same pos terior probabilities at the training data points as will be seen later in figure 10 13 maximum likelihood provides no way to favour one such solution over another and which solution is found in practice will depend on the choice of optimization algo rithm and on the parameter initialization note that the problem will arise even if the number of data points is large compared with the number of parameters in the model so long as the training data set is linearly separable the singularity can be avoided by inclusion of a prior and ﬁnding a map solution for w or equivalently by adding a regularization term to the error function iterative reweighted least squares in the case of the linear regression models discussed in chapter the maxi mum likelihood solution on the assumption of a gaussian noise model leads to a closed form solution this was a consequence of the quadratic dependence of the log likelihood function on the parameter vector w for logistic regression there is no longer a closed form solution due to the nonlinearity of the logistic sigmoid function however the departure from a quadratic form is not substantial to be precise the error function is concave as we shall see shortly and hence has a unique minimum furthermore the error function can be minimized by an efﬁcient iterative technique based on the newton raphson iterative optimization scheme which uses a local quadratic approximation to the log likelihood function the newton raphson update for minimizing a function e w takes the form fletcher bishop and nabney w new w old h e w 92 where h is the hessian matrix whose elements comprise the second derivatives of e w with respect to the components of w let us ﬁrst of all apply the newton raphson method to the linear regression model with the sum of squares error function 12 the gradient and hessian of this error function are given by e w h e w n wtφn tn φn φtφw φtt n n t t n section n where φ is the n m design matrix whose nth row is given by φt the newton raphson update then takes the form w new w old φtφ φtφw old φtt φtφ 95 which we recognize as the standard least squares solution note that the error func tion in this case is quadratic and hence the newton raphson formula gives the exact solution in one step now let us apply the newton raphson update to the cross entropy error function for the logistic regression model from we see that the gradient and hessian of this error function are given by e w n yn tn φn φt y t 96 n n h e w yn yn φn n φt φtrφ 97 where we have made use of also we have introduced the n n diagonal matrix r with elements exercise 15 rnn yn yn we see that the hessian is no longer constant but depends on w through the weight ing matrix r corresponding to the fact that the error function is no longer quadratic using the property 0 yn which follows from the form of the logistic sigmoid function we see that uthu 0 for an arbitrary vector u and so the hessian matrix h is positive deﬁnite it follows that the error function is a concave function of w and hence has a unique minimum the newton raphson update formula for the logistic regression model then be comes w new w old φtrφ y t φ rφ φ rφw old φ y t φtrφ where z is an n dimensional vector with elements z φw old r y t 100 we see that the update formula takes the form of a set of normal equations for a weighted least squares problem because the weighing matrix r is not constant but depends on the parameter vector w we must apply the normal equations iteratively each time using the new weight vector w to compute a revised weighing matrix r for this reason the algorithm is known as iterative reweighted least squares or irls rubin as in the weighted least squares problem the elements of the diagonal weighting matrix r can be interpreted as variances because the mean and variance of t in the logistic regression model are given by e t σ x y var t e e t 2 σ x σ x 2 y y where we have used the property t for t 0 in fact we can interpret irls as the solution to a linearized problem in the space of the variable a wtφ the quantity zn which corresponds to the nth element of z can then be given a simple interpretation as an effective target value in this space obtained by making a local linear approximation to the logistic sigmoid function around the current operating point w old a w a w old dan w old tn yn φtw old yn tn z n yn yn section 2 multiclass logistic regression in our discussion of generative models for multiclass classiﬁcation we have seen that for a large class of distributions the posterior probabilities are given by a softmax transformation of linear functions of the feature variables so that φ exp ak 104 p ck φ yk j exp aj exercise 17 where the activations ak are given by ak wtφ there we used maximum likelihood to determine separately the class conditional densities and the class priors and then found the corresponding posterior probabilities using bayes theorem thereby implicitly determining the parameters wk here we consider the use of maximum likelihood to determine the parameters wk of this model directly to do this we will require the derivatives of yk with respect to all of the activations aj these are given by yk y aj k ikj yj where ikj are the elements of the identity matrix next we write down the likelihood function this is most easily done using the of k coding scheme in which the target vector tn for a feature vector φn belonging to class k is a binary vector with all elements zero except for element k which equals one the likelihood function is then given by n k n k p t wk tt tt p ck φn tnk tt tt ytnk 107 n k n k where ynk yk φn and t is an n k matrix of target variables with elements tnk taking the negative logarithm then gives n k exercise 18 e wk ln p t wk tnk ln ynk n k which is known as the cross entropy error function for the multiclass classiﬁcation problem we now take the gradient of the error function with respect to one of the param eter vectors wj making use of the result for the derivatives of the softmax function we obtain n wj e wk ynj tnj φn 109 n where we have made use of k tnk once again we see the same form arising for the gradient as was found for the sum of squares error function with the linear model and the cross entropy error for the logistic regression model namely the prod uct of the error ynj tnj times the basis function φn again we could use this to formulate a sequential algorithm in which patterns are presented one at a time in which each of the weight vectors is updated using 22 we have seen that the derivative of the log likelihood function for a linear regres sion model with respect to the parameter vector w for a data point n took the form of the error yn tn times the feature vector φn similarly for the combination of logistic sigmoid activation function and cross entropy error function and for the softmax activation function with the multiclass cross entropy error function 108 we again obtain this same simple form this is an example of a more general result as we shall see in section 6 to ﬁnd a batch algorithm we again appeal to the newton raphson update to obtain the corresponding irls algorithm for the multiclass problem this requires evaluation of the hessian matrix that comprises blocks of size m m in which block j k is given by w w e wk ynk ikj ynj φnφt exercise 20 as with the two class problem the hessian matrix for the multiclass logistic regres sion model is positive deﬁnite and so the error function again has a unique minimum practical details of irls for the multiclass case can be found in bishop and nabney 5 probit regression we have seen that for a broad range of class conditional distributions described by the exponential family the resulting posterior class probabilities are given by a logistic or softmax transformation acting on a linear function of the feature vari ables however not all choices of class conditional density give rise to such a simple form for the posterior probabilities for instance if the class conditional densities are modelled using gaussian mixtures this suggests that it might be worth exploring other types of discriminative probabilistic model for the purposes of this chapter however we shall return to the two class case and again remain within the frame work of generalized linear models so that p t a f a 111 where a wtφ and f is the activation function one way to motivate an alternative choice for the link function is to consider a noisy threshold model as follows for each input φn we evaluate an wtφn and then we set the target value according to tn if an θ tn 0 otherwise figure 13 schematic example of a probability density p θ shown by the blue curve given in this example by a mixture of two gaussians along with its cumulative distribution function f a shown by the red curve note that the value of the blue curve at any point such as that indicated by the vertical green line corresponds to the slope of the red curve at the same point conversely the value of the red curve at this point corresponds to the area under the blue curve indicated by the shaded green region in the stochastic threshold model the class label takes the value t if the value of a wtφ exceeds a threshold oth erwise it takes the value t 0 this is equivalent to an activation function given by the cumulative distribution function f a 0 8 0 6 0 0 2 0 0 2 3 if the value of θ is drawn from a probability density p θ then the corresponding activation function will be given by the cumulative distribution function as illustrated in figure 13 f a p θ dθ 113 as a speciﬁc example suppose that the density p θ is given by a zero mean unit variance gaussian the corresponding cumulative distribution function is given by a φ a n θ 0 dθ 114 which is known as the probit function it has a sigmoidal shape and is compared with the logistic sigmoid function in figure 9 note that the use of a more gen eral gaussian distribution does not change the model because this is equivalent to a re scaling of the linear coefﬁcients w many numerical packages provide for the evaluation of a closely related function deﬁned by 2 f a exercise 21 and known as the erf function or error function not to be confused with the error function of a machine learning model it is related to the probit function by the generalized linear model based on a probit activation function is known as probit regression we can determine the parameters of this model using maximum likelihood by a straightforward extension of the ideas discussed earlier in practice the results found using probit regression tend to be similar to those of logistic regression we shall however ﬁnd another use for the probit model when we discuss bayesian treatments of logistic regression in section 5 one issue that can occur in practical applications is that of outliers which can arise for instance through errors in measuring the input vector x or through misla belling of the target value t because such points can lie a long way to the wrong side of the ideal decision boundary they can seriously distort the classiﬁer note that the logistic and probit regression models behave differently in this respect because the tails of the logistic sigmoid decay asymptotically like exp x for x whereas for the probit activation function they decay like exp x2 and so the probit model can be signiﬁcantly more sensitive to outliers however both the logistic and the probit models assume the data is correctly labelled the effect of mislabelling is easily incorporated into a probabilistic model by introducing a probability e that the target value t has been ﬂipped to the wrong value opper and winther leading to a target value distribution for data point x of the form p t x e σ x e σ x e 2e σ x 117 where σ x is the activation function with input vector x here e may be set in advance or it may be treated as a hyperparameter whose value is inferred from the data 3 6 canonical link functions for the linear regression model with a gaussian noise distribution the error function corresponding to the negative log likelihood is given by 3 12 if we take the derivative with respect to the parameter vector w of the contribution to the error function from a data point n this takes the form of the error yn tn times the feature vector φn where yn wtφn similarly for the combination of the logistic sigmoid activation function and the cross entropy error function 90 and for the softmax activation function with the multiclass cross entropy error function 108 we again obtain this same simple form we now show that this is a general result of assuming a conditional distribution for the target variable from the exponential family along with a corresponding choice for the activation function known as the canonical link function we again make use of the restricted form 84 of exponential family distribu tions note that here we are applying the assumption of exponential family distribu tion to the target variable t in contrast to section 2 where we applied it to the input vector x we therefore consider conditional distributions of the target variable of the form p t η h t g η exp j ηtl 118 using the same line of argument as led to the derivation of the result 2 226 we see that the conditional mean of t which we denote by y is given by y e t η dη ln g η 119 thus y and η must related and we denote this relation through η ψ y following nelder and wedderburn we deﬁne a generalized linear model to be one for which y is a nonlinear function of a linear combination of the input or feature variables so that y f wtφ 120 where f is known as the activation function in the machine learning literature and f is known as the link function in statistics now consider the log likelihood function for this model which as a function of η is given by ln p t η n ln p tn η n jln g ηn ηntn const 121 where we are assuming that all observations share a common scale parameter which corresponds to the noise variance for a gaussian distribution for instance and so is independent of n the derivative of the log likelihood with respect to the model parameters w is then given by d t dη dy w ln p t η n n dηn ln g ηn n n dyn n dan an t yn ψi yn f i an φn 122 n where an wtφn and we have used yn f an together with the result 119 for e t η we now see that there is a considerable simpliﬁcation if we choose a particular form for the link function f 1 y given by f 1 y ψ y 123 which gives f ψ y y and hence f i ψ ψi y 1 also because a f 1 y we have a ψ and hence f i a ψi y 1 in this case the gradient of the error function reduces to n ln e w yn n 1 tn φn 124 for the gaussian β 1 whereas for the logistic model 1 the laplace approximation in section 5 we shall discuss the bayesian treatment of logistic regression as we shall see this is more complex than the bayesian treatment of linear regression models discussed in sections 3 3 and 3 5 in particular we cannot integrate exactly chapter 10 chapter 11 over the parameter vector w since the posterior distribution is no longer gaussian it is therefore necessary to introduce some form of approximation later in the book we shall consider a range of techniques based on analytical approximations and numerical sampling here we introduce a simple but widely used framework called the laplace ap proximation that aims to ﬁnd a gaussian approximation to a probability density deﬁned over a set of continuous variables consider ﬁrst the case of a single contin uous variable z and suppose the distribution p z is deﬁned by 1 p z z f z 125 where z f z dz is the normalization coefﬁcient we shall suppose that the value of z is unknown in the laplace method the goal is to ﬁnd a gaussian approx imation q z which is centred on a mode of the distribution p z the ﬁrst step is to ﬁnd a mode of p z in other words a point such that pi 0 or equivalently df z dz 1 z 0 a gaussian distribution has the property that its logarithm is a quadratic function of the variables we therefore consider a taylor expansion of ln f z centred on the mode so that ln f z ln f z 1 a z z 2 where 0 2 0 2 1 note that the ﬁrst order term in the taylor expansion does not appear since is a local maximum of the distribution taking the exponential we obtain f z f z exp a z z 2 we can then obtain a normalized distribution q z by making use of the standard result for the normalization of a gaussian so that a 1 2 a the laplace approximation is illustrated in figure 14 note that the gaussian approximation will only be well deﬁned if its precision a 0 in other words the stationary point must be a local maximum so that the second derivative of f z at the point is negative 0 8 0 6 30 0 20 0 2 10 0 2 1 0 1 2 3 0 2 1 0 1 2 3 figure 14 illustration of the laplace approximation applied to the distribution p z exp 2 σ where σ z is the logistic sigmoid function deﬁned by σ z 1 e z 1 the left plot shows the normalized distribution p z in yellow together with the laplace approximation centred on the mode of p z in red the right plot shows the negative logarithms of the corresponding curves we can extend the laplace method to approximate a distribution p z f z z deﬁned over an m dimensional space z at a stationary point the gradient f z will vanish expanding around this stationary point we have ln f z ln f z 1 z z ta z z 131 where the m m hessian matrix a is deﬁned by a ln f z z 132 and is the gradient operator taking the exponential of both sides we obtain f z f z exp 1 z z ta z z 133 the distribution q z is proportional to f z and the appropriate normalization coef ﬁcient can be found by inspection using the standard result 2 43 for a normalized multivariate gaussian giving a 1 2 1 t 1 where a denotes the determinant of a this gaussian distribution will be well deﬁned provided its precision matrix given by a is positive deﬁnite which implies that the stationary point must be a local maximum not a minimum or a saddle point in order to apply the laplace approximation we ﬁrst need to ﬁnd the mode and then evaluate the hessian matrix at that mode in practice a mode will typi cally be found by running some form of numerical optimization algorithm bishop and nabney many of the distributions encountered in practice will be mul timodal and so there will be different laplace approximations according to which mode is being considered note that the normalization constant z of the true distri bution does not need to be known in order to apply the laplace method as a result of the central limit theorem the posterior distribution for a model is expected to become increasingly better approximated by a gaussian as the number of observed data points is increased and so we would expect the laplace approximation to be most useful in situations where the number of data points is relatively large one major weakness of the laplace approximation is that since it is based on a gaussian distribution it is only directly applicable to real variables in other cases it may be possible to apply the laplace approximation to a transformation of the variable for instance if 0 τ then we can consider a laplace approximation of ln τ the most serious limitation of the laplace framework however is that it is based purely on the aspects of the true distribution at a speciﬁc value of the variable and so can fail to capture important global properties in chapter 10 we shall consider alternative approaches which adopt a more global perspective 1 model comparison and bic as well as approximating the distribution p z we can also obtain an approxi mation to the normalization constant z using the approximation 133 we have z f f z dz 0 f 2 0 0 2π m 2 a 1 2 135 where we have noted that the integrand is gaussian and made use of the standard result 2 43 for a normalized gaussian distribution we can use the result 135 to obtain an approximation to the model evidence which as discussed in section 3 plays a central role in bayesian model comparison consider a data set and a set of models i having parameters θi for each model we deﬁne a likelihood function p θi i if we introduce a prior p θi i over the parameters then we are interested in computing the model evi dence p i for the various models from now on we omit the conditioning on i to keep the notation uncluttered from bayes theorem the model evidence is given by exercise 22 p d f p d θ p θ dθ 136 identifying f θ p θ p θ and z p and applying the result 135 we obtain ln p d ln p d θ ln p θ m ln 2π 1 ln a 137 occam factor exercise 23 where θmap is the value of θ at the mode of the posterior distribution and a is the hessian matrix of second derivatives of the negative log posterior a ln p d θmap p θmap ln p θmap d 138 the ﬁrst term on the right hand side of 137 represents the log likelihood evalu ated using the optimized parameters while the remaining three terms comprise the occam factor which penalizes model complexity if we assume that the gaussian prior distribution over parameters is broad and that the hessian has full rank then we can approximate 137 very roughly using ln p d ln p d θ map 1 m ln n 139 section 3 5 3 where n is the number of data points m is the number of parameters in θ and we have omitted additive constants this is known as the bayesian information criterion bic or the schwarz criterion schwarz note that compared to aic given by 1 73 this penalizes model complexity more heavily complexity measures such as aic and bic have the virtue of being easy to evaluate but can also give misleading results in particular the assumption that the hessian matrix has full rank is often not valid since many of the parameters are not well determined we can use the result 137 to obtain a more accurate estimate of the model evidence starting from the laplace approximation as we illustrate in the context of neural networks in section 5 7 5 bayesian logistic regression we now turn to a bayesian treatment of logistic regression exact bayesian infer ence for logistic regression is intractable in particular evaluation of the posterior distribution would require normalization of the product of a prior distribution and a likelihood function that itself comprises a product of logistic sigmoid functions one for every data point evaluation of the predictive distribution is similarly intractable here we consider the application of the laplace approximation to the problem of bayesian logistic regression spiegelhalter and lauritzen mackay 5 1 laplace approximation recall from section that the laplace approximation is obtained by ﬁnding the mode of the posterior distribution and then ﬁtting a gaussian centred at that mode this requires evaluation of the second derivatives of the log posterior which is equivalent to ﬁnding the hessian matrix because we seek a gaussian representation for the posterior distribution it is natural to begin with a gaussian prior which we write in the general form p w n w where and s0 are ﬁxed hyperparameters the posterior distribution over w is given by p w t p w p t w 141 where t tn t taking the log of both sides and substituting for the prior distribution using 140 and for the likelihood function using 89 we obtain ln p w t 1 w m ts 1 w m 2 0 0 0 n tn ln yn 1 tn ln 1 yn const 142 n 1 where yn σ wtφn to obtain a gaussian approximation to the posterior dis tribution we ﬁrst maximize the posterior distribution to give the map maximum posterior solution wmap which deﬁnes the mean of the gaussian the covariance is then given by the inverse of the matrix of second derivatives of the negative log likelihood which takes the form n 1 0 n 1 φt 143 the gaussian approximation to the posterior distribution therefore takes the form q w n w wmap sn 144 having obtained a gaussian approximation to the posterior distribution there remains the task of marginalizing with respect to this distribution in order to make predictions 5 2 predictive distribution the predictive distribution for class 1 given a new feature vector φ x is obtained by marginalizing with respect to the posterior distribution p w t which is itself approximated by a gaussian distribution q w so that p φ t f p φ w p w t dw f σ wtφ q w dw 145 with the corresponding probability for class 2 given by p 2 φ t 1 p 1 φ t to evaluate the predictive distribution we ﬁrst note that the function σ wtφ de pends on w only through its projection onto φ denoting a wtφ we have σ wtφ f δ a wtφ σ a da 146 where δ is the dirac delta function from this we obtain f σ wtφ q w dw f σ a p a da 147 where p a f δ a wtφ q w dw 148 we can evaluate p a by noting that the delta function imposes a linear constraint on w and so forms a marginal distribution from the joint distribution q w by inte grating out all directions orthogonal to φ because q w is gaussian we know from section 2 3 2 that the marginal distribution will also be gaussian we can evaluate the mean and covariance of this distribution by taking moments and interchanging the order of integration over a and w so that µa e a f p a a da f q w wtφ dw wt φ 149 where we have used the result 144 for the variational posterior distribution q w similarly 2 var a f p a e a 21 da f q w wtφ 2 mt φ 21 dw φtsnφ 150 note that the distribution of a takes the same form as the predictive distribution 3 58 for the linear regression model with the noise variance set to zero thus our variational approximation to the predictive distribution becomes p t f σ a p a da f σ a n a µa da 151 exercise 24 exercise 25 exercise 26 this result can also be derived directly by making use of the results for the marginal of a gaussian distribution given in section 2 3 2 the integral over a represents the convolution of a gaussian with a logistic sig moid and cannot be evaluated analytically we can however obtain a good approx imation spiegelhalter and lauritzen mackay barber and bishop by making use of the close similarity between the logistic sigmoid function σ a deﬁned by 59 and the probit function φ a deﬁned by 114 in order to obtain the best approximation to the logistic function we need to re scale the hori zontal axis so that we approximate σ a by φ λa we can ﬁnd a suitable value of λ by requiring that the two functions have the same slope at the origin which gives λ2 π 8 the similarity of the logistic sigmoid and the probit function for this choice of λ is illustrated in figure 9 the advantage of using a probit function is that its convolution with a gaussian can be expressed analytically in terms of another probit function speciﬁcally we can show that f φ λa n a µ da φ µ 152 we now apply the approximation σ a φ λa to the probit functions appearing on both sides of this equation leading to the following approximation for the convo lution of a logistic sigmoid with a gaussian f σ a n a µ da σ κ µ 153 where we have deﬁned κ 1 8 1 2 154 applying this result to 151 we obtain the approximate predictive distribution in the form 2 a where µa and are deﬁned by 149 and 150 respectively and κ σ2 is de a a ﬁned by 154 note that the decision boundary corresponding to p 1 φ t 0 5 is given by µa 0 which is the same as the decision boundary obtained by using the map value for w thus if the decision criterion is based on minimizing misclassiﬁca tion rate with equal prior probabilities then the marginalization over w has no ef fect however for more complex decision criteria it will play an important role marginalization of the logistic sigmoid model under a gaussian approximation to the posterior distribution will be illustrated in the context of variational inference in figure 10 13 exercises 1 given a set of data points xn we can deﬁne the convex hull to be the set of all points x given by x αnxn 156 n where αn 0 and n αn 1 consider a second set of points yn together with their corresponding convex hull by deﬁnition the two sets of points will be linearly separable if there exists a vector w and a scalar w such that wtx w 0 for all x and wty 0 for all y 0 n 0 n n n show that if their convex hulls intersect the two sets of points cannot be linearly separable and conversely that if they are linearly separable their convex hulls do not intersect 2 www consider the minimization of a sum of squares error function 15 and suppose that all of the target vectors in the training set satisfy a linear constraint attn b 0 where tn corresponds to the nth row of the matrix t in 15 show that as a consequence of this constraint the elements of the model prediction y x given by the least squares solution 17 also satisfy this constraint so that aty x b 0 158 to do so assume that one of the basis functions x 1 so that the corresponding parameter plays the role of a bias 3 extend the result of exercise 2 to show that if multiple linear constraints are satisﬁed simultaneously by the target vectors then the same constraints will also be satisﬁed by the least squares prediction of a linear model www show that maximization of the class separation criterion given by 23 with respect to w using a lagrange multiplier to enforce the constraint wtw 1 leads to the result that w m2 m1 5 by making use of 20 23 and 24 show that the fisher criterion 25 can be written in the form 26 6 using the deﬁnitions of the between class and within class covariance matrices given by 27 and 28 respectively together with 34 and 36 and the choice of target values described in section 1 5 show that the expression 33 that minimizes the sum of squares error function can be written in the form 37 7 www show that the logistic sigmoid function 4 59 satisﬁes the property σ a 1 σ a and that its inverse is given by σ 1 y ln y 1 y 4 8 using 4 57 and 4 58 derive the result 4 65 for the posterior class probability in the two class generative model with gaussian densities and verify the results 4 66 and 4 67 for the parameters w and 4 9 www consider a generative classiﬁcation model for k classes deﬁned by prior class probabilities p k πk and general class conditional densities p φ k where φ is the input feature vector suppose we are given a training data set φn tn where n 1 n and tn is a binary target vector of length k that uses the 1 of k coding scheme so that it has components tnj ijk if pattern n is from class k assuming that the data points are drawn independently from this model show that the maximum likelihood solution for the prior probabilities is given by π nk k n 4 159 where nk is the number of data points assigned to class ck 4 10 consider the classiﬁcation model of exercise 4 9 and now suppose that the class conditional densities are given by gaussian distributions with a shared covari ance matrix so that p φ ck n φ µk σ 4 160 show that the maximum likelihood solution for the mean of the gaussian distribution for class ck is given by n µk tnkφn nk n 1 4 161 which represents the mean of those feature vectors assigned to class k similarly show that the maximum likelihood solution for the shared covariance matrix is given by k σ nk s n k 4 162 where n sk tnk nk n 1 k 1 φn µk φn µk t 4 163 thus σ is given by a weighted average of the covariances of the data associated with each class in which the weighting coefﬁcients are given by the prior probabilities of the classes 4 11 consider a classiﬁcation problem with k classes for which the feature vector φ has m components each of which can take l discrete states let the values of the components be represented by a 1 of l binary coding scheme further suppose that conditioned on the class k the m components of φ are independent so that the class conditional density factorizes with respect to the feature vector components show that the quantities ak given by 4 63 which appear in the argument to the softmax function describing the posterior class probabilities are linear functions of the components of φ note that this represents an example of the naive bayes model which is discussed in section 8 2 2 introduction what is computer vision a brief history book overview sample syllabus a note on notation additional reading figure the human visual system has no problem interpreting the subtle variations in translucency and shading in this photograph and correctly segmenting the object from its background a b c d figure some examples of computer vision algorithms and applications a structure from motion algorithms can reconstruct a sparse point model of a large complex scene from hundreds of partially overlapping photographs snavely seitz and szeliski qc acm b stereo matching algorithms can build a detailed model of a building fac ade from hundreds of differently exposed photographs taken from the internet goesele snavely curless et al qc ieee c person tracking algorithms can track a person walking in front of a cluttered background sidenbladh black and fleet qc springer d face detection algorithms coupled with color based clothing and hair detection algorithms can locate and recognize the individuals in this image sivic zitnick and szeliski qc springer what is computer vision as humans we perceive the three dimensional structure of the world around us with apparent ease think of how vivid the three dimensional percept is when you look at a vase of flowers sitting on the table next to you you can tell the shape and translucency of each petal through the subtle patterns of light and shading that play across its surface and effortlessly segment each flower from the background of the scene figure looking at a framed group por trait you can easily count and name all of the people in the picture and even guess at their emotions from their facial appearance perceptual psychologists have spent decades trying to understand how the visual system works and even though they can devise optical to tease apart some of its principles figure a complete solution to this puzzle remains elusive marr palmer livingstone researchers in computer vision have been developing in parallel mathematical tech niques for recovering the three dimensional shape and appearance of objects in imagery we now have reliable techniques for accurately computing a partial model of an environment from thousands of partially overlapping photographs figure given a large enough set of views of a particular object or fac ade we can create accurate dense surface mod els using stereo matching figure we can track a person moving against a complex background figure we can even with moderate success attempt to find and name all of the people in a photograph using a combination of face clothing and hair detection and recognition figure however despite all of these advances the dream of having a computer interpret an image at the same level as a two year old for example counting all of the animals in a picture remains elusive why is vision so difficult in part it is because vision is an inverse problem in which we seek to recover some unknowns given insufficient information to fully specify the solution we must therefore resort to physics based and prob abilistic models to disambiguate between potential solutions however modeling the visual world in all of its rich complexity is far more difficult than say modeling the vocal tract that produces spoken sounds the forward models that we use in computer vision are usually developed in physics ra diometry optics and sensor design and in computer graphics both of these fields model how objects move and animate how light reflects off their surfaces is scattered by the at mosphere refracted through camera lenses or human eyes and finally projected onto a flat or curved image plane while computer graphics are not yet perfect no fully computer animated movie with human characters has yet succeeded at crossing the uncanny that separates real humans from android robots and computer animated humans in limited http www michaelbach de ot sze muelue the term uncanny valley was originally coined by roboticist masahiro mori as applied to robotics mori it is also commonly applied to computer animated films such as final fantasy and polar express geller a b x x x x x x x o x o x o x x x x x x x x x x o x x x o x x x x x x x x o x x o x x o x x x x x x x x x o x o o x x x x x x x x o x x o x x x x x x x x x x x o x x x o x x x x x x x x o x x o x x o x x x x x x x x o x x x o x x x x x x x x x x x o o x x x x x x x x x x o x x x o x c d figure some common optical illusions and what they might tell us about the visual sys tem a the classic mu ller lyer illusion where the length of the two horizontal lines appear different probably due to the imagined perspective effects b the white square b in the shadow and the black square a in the light actually have the same absolute intensity value the percept is due to brightness constancy the visual system attempt to discount illumi nation when interpreting colors image courtesy of ted adelson http web mit edu persci people adelson checkershadow illusion html c a variation of the hermann grid illusion courtesy of hany farid http www cs dartmouth edu farid illusions hermann html as you move your eyes over the figure gray spots appear at the intersections d count the red xs in the left half of the figure now count them in the right half is it significantly harder the explanation has to do with a pop out effect treisman which tells us about the operations of parallel perception and integration pathways in the brain domains such as rendering a still scene composed of everyday objects or animating extinct creatures such as dinosaurs the illusion of reality is perfect in computer vision we are trying to do the inverse i e to describe the world that we see in one or more images and to reconstruct its properties such as shape illumination and color distributions it is amazing that humans and animals do this so effortlessly while computer vision algorithms are so error prone people who have not worked in the field often under estimate the difficulty of the problem colleagues at work often ask me for software to find and name all the people in photos so they can get on with the more interesting work this misperception that vision should be easy dates back to the early days of artificial intelligence see section when it was initially believed that the cognitive logic proving and plan ning parts of intelligence were intrinsically more difficult than the perceptual components boden the good news is that computer vision is being used today in a wide variety of real world applications which include optical character recognition ocr reading handwritten postal codes on letters figure and automatic number plate recognition anpr machine inspection rapid parts inspection for quality assurance using stereo vision with specialized illumination to measure tolerances on aircraft wings or auto body parts figure or looking for defects in steel castings using x ray vision retail object recognition for automated checkout lanes figure model building photogrammetry fully automated construction of models from aerial photographs used in systems such as bing maps medical imaging registering pre operative and intra operative imagery figure or performing long term studies of people brain morphology as they age automotive safety detecting unexpected obstacles such as pedestrians on the street under conditions where active vision techniques such as radar or lidar do not work well figure see also miller campbell huttenlocher et al montemerlo becker bhat et al urmson anhalt bagnell et al for examples of fully automated driving match move merging computer generated imagery cgi with live action footage by tracking feature points in the source video to estimate the camera motion and shape of the environment such techniques are widely used in hollywood e g in movies such as jurassic park roble roble and zafar they also require the use of precise matting to insert new elements between foreground and background elements chuang agarwala curless et al a b c d e f figure some industrial applications of computer vision a optical character recognition ocr http yann lecun com exdb lenet b mechanical inspection http www cognitens com c retail http www evoretail com d medical imaging http www clarontech com e automotive safety http www mobileye com f surveillance and traffic monitoring http www honeywellvideo com courtesy of honeywell international inc motion capture mocap using retro reflective markers viewed from multiple cam eras or other vision based techniques to capture actors for computer animation surveillance monitoring for intruders analyzing highway traffic figure and monitoring pools for drowning victims fingerprint recognition and biometrics for automatic access authentication as well as forensic applications david lowe web site of industrial vision applications http www cs ubc ca spider lowe vision html lists many other interesting industrial applications of computer vision while the above applications are all extremely important they mostly pertain to fairly specialized kinds of imagery and narrow domains in this book we focus more on broader consumer level applications such as fun things you can do with your own personal photographs and video these include stitching turning overlapping photos into a single seamlessly stitched panorama fig ure as described in chapter exposure bracketing merging multiple exposures taken under challenging lighting conditions strong sunlight and shadows into a single perfectly exposed image fig ure as described in section morphing turning a picture of one of your friends into another using a seamless morph transition figure modeling converting one or more snapshots into a model of the object or person you are photographing figure as described in section video match move and stabilization inserting pictures or models into your videos by automatically tracking nearby reference points see section or using motion estimates to remove shake from your videos see section photo based walkthroughs navigating a large collection of photographs such as the interior of your house by flying between different photos in see sections and face detection for improved camera focusing as well as more relevant image search ing see section visual authentication automatically logging family members onto your home com puter as they sit down in front of the webcam see section a b c d figure some consumer applications of computer vision a image stitching merging different views szeliski and shum qc acm b exposure bracketing merging different exposures c morphing blending between two photographs gomes darsa costa et al qc morgan kaufmann d turning a collection of photographs into a model sinha steedly szeliski et al qc acm the great thing about these applications is that they are already familiar to most students they are at least technologies that students can immediately appreciate and use with their own personal media since computer vision is a challenging topic given the wide range of mathematics being and the intrinsically difficult nature of the problems being solved having fun and relevant problems to work on can be highly motivating and inspiring the other major reason why this book has a strong focus on applications is that they can be used to formulate and constrain the potentially open ended problems endemic in vision for example if someone comes to me and asks for a good edge detector my first question is usually to ask why what kind of problem are they trying to solve and why do they believe that edge detection is an important component if they are trying to locate faces i usually point out that most successful face detectors use a combination of skin color detection exer cise and simple blob features section they do not rely on edge detection if they are trying to match door and window edges in a building for the purpose of reconstruction i tell them that edges are a fine idea but it is better to tune the edge detector for long edges see sections and and link them together into straight lines with common vanishing points before matching see section thus it is better to think back from the problem at hand to suitable techniques rather than to grab the first technique that you may have heard of this kind of working back from problems to solutions is typical of an engineering approach to the study of vision and reflects my own background in the field first i come up with a detailed problem definition and decide on the constraints and specifications for the problem then i try to find out which techniques are known to work implement a few of these evaluate their performance and finally make a selection in order for this process to work it is important to have realistic test data both synthetic which can be used to verify correctness and analyze noise sensitivity and real world data typical of the way the system will finally be used however this book is not just an engineering text a source of recipes it also takes a scientific approach to basic vision problems here i try to come up with the best possible models of the physics of the system at hand how the scene is created how light interacts with the scene and atmospheric effects and how the sensors work including sources of noise and uncertainty the task is then to try to invert the acquisition process to come up with the best possible description of the scene the book often uses a statistical approach to formulating and solving computer vision problems where appropriate probability distributions are used to model the scene and the noisy image acquisition process the association of prior distributions with unknowns is often for a fun student project on this topic see the photobook project at http www cc gatech edu dvfx videos html these techniques include physics euclidean and projective geometry statistics and optimization they make computer vision a fascinating field to study and a great way to learn techniques widely applicable in other fields called bayesian modeling appendix b it is possible to associate a risk or loss function with mis estimating the answer section b and to set up your inference algorithm to minimize the expected risk consider a robot trying to estimate the distance to an obstacle it is usually safer to underestimate than to overestimate with statistical techniques it often helps to gather lots of training data from which to learn probabilistic models finally statistical approaches enable you to use proven inference techniques to estimate the best answer or distribution of answers and to quantify the uncertainty in the resulting estimates because so much of computer vision involves the solution of inverse problems or the esti mation of unknown quantities my book also has a heavy emphasis on algorithms especially those that are known to work well in practice for many vision problems it is all too easy to come up with a mathematical description of the problem that either does not match realistic real world conditions or does not lend itself to the stable estimation of the unknowns what we need are algorithms that are both robust to noise and deviation from our models and rea sonably efficient in terms of run time resources and space in this book i go into these issues in detail using bayesian techniques where applicable to ensure robustness and efficient search minimization and linear system solving algorithms to ensure efficiency most of the algorithms described in this book are at a high level being mostly a list of steps that have to be filled in by students or by reading more detailed descriptions elsewhere in fact many of the algorithms are sketched out in the exercises now that i ve described the goals of this book and the frameworks that i use i devote the rest of this chapter to two additional topics section is a brief synopsis of the history of computer vision it can easily be skipped by those who want to get to the meat of the new material in this book and do not care as much about who invented what when the second is an overview of the book contents section which is useful reading for everyone who intends to make a study of this topic or to jump in partway since it describes chapter inter dependencies this outline is also useful for instructors looking to structure one or more courses around this topic as it provides sample curricula based on the book contents a brief history in this section i provide a brief personal synopsis of the main developments in computer vision over the last years figure at least those that i find personally interesting and which appear to have stood the test of time readers not interested in the provenance of various ideas and the evolution of this field should skip ahead to the book overview in section figure a rough timeline of some of the most active topics of research in computer vision when computer vision first started out in the early it was viewed as the visual perception component of an ambitious agenda to mimic human intelligence and to endow robots with intelligent behavior at the time it was believed by some of the early pioneers of artificial intelligence and robotics at places such as mit stanford and cmu that solving the visual input problem would be an easy step along the path to solving more difficult problems such as higher level reasoning and planning according to one well known story in marvin minsky at mit asked his undergraduate student gerald jay sussman to spend the summer linking a camera to a computer and getting the computer to describe what it saw boden p we now know that the problem is slightly more difficult than that what distinguished computer vision from the already existing field of digital image pro cessing rosenfeld and pfaltz rosenfeld and kak was a desire to recover the three dimensional structure of the world from images and to use this as a stepping stone to wards full scene understanding winston and hanson and riseman provide two nice collections of classic papers from this early period early attempts at scene understanding involved extracting edges and then inferring the structure of an object or a blocks world from the topological structure of the lines roberts several line labeling algorithms figure were developed at that time huffman clowes waltz rosenfeld hummel and zucker kanade nalwa gives a nice review of this area the topic of edge detection was also boden cites crevier as the original source the actual vision memo was authored by seymour papert and involved a whole cohort of students to see how far robotic vision has come in the last four decades have a look at the towel folding robot at http rll eecs berkeley edu pr maitin shepard cusumano towner lei et al a b c d e f figure some early examples of computer vision algorithms a line label ing nalwa qc addison wesley b pictorial structures fischler and elschlager qc ieee c articulated body model marr qc david marr d intrin sic images barrow and tenenbaum qc ieee e stereo correspondence marr qc david marr f optical flow nagel and enkelmann qc ieee an active area of research a nice survey of contemporaneous work can be found in davis three dimensional modeling of non polyhedral objects was also being studied baum gart baker one popular approach used generalized cylinders i e solids of revolution and swept closed curves agin and binford nevatia and binford of ten arranged into parts hinton marr figure fischler and elschlager called such elastic arrangements of parts pictorial structures figure this is currently one of the favored approaches being used in object recognition see sec tion and felzenszwalb and huttenlocher a qualitative approach to understanding intensities and shading variations and explaining them by the effects of image formation phenomena such as surface orientation and shadows was championed by barrow and tenenbaum in their paper on intrinsic images fig ure along with the related d sketch ideas of marr this approach is again seeing a bit of a revival in the work of tappen freeman and adelson more quantitative approaches to computer vision were also developed at the time in cluding the first of many feature based stereo correspondence algorithms figure dev in robotics and computer animation these linked part graphs are often called kinematic chains marr and poggio moravec marr and poggio mayhew and frisby baker barnard and fischler ohta and kanade grimson pol lard mayhew and frisby prazdny and intensity based optical flow algorithms figure horn and schunck huang lucas and kanade nagel the early work in simultaneously recovering structure and camera motion see chapter also began around this time ullman longuet higgins a lot of the philosophy of how vision was believed to work at the time is summarized in david marr book in particular marr introduced his notion of the three levels of description of a visual information processing system these three levels very loosely paraphrased according to my own interpretation are computational theory what is the goal of the computation task and what are the constraints that are known or can be brought to bear on the problem representations and algorithms how are the input output and intermediate infor mation represented and which algorithms are used to calculate the desired result hardware implementation how are the representations and algorithms mapped onto actual hardware e g a biological vision system or a specialized piece of silicon con versely how can hardware constraints be used to guide the choice of representation and algorithm with the increasing use of graphics chips gpus and many core ar chitectures for computer vision see section c this question is again becoming quite relevant as i mentioned earlier in this introduction it is my conviction that a careful analysis of the problem specification and known constraints from image formation and priors the scientific and statistical approaches must be married with efficient and robust algorithms the engineer ing approach to design successful vision algorithms thus it seems that marr philosophy is as good a guide to framing and solving problems in our field today as it was years ago in the a lot of attention was focused on more sophisticated mathematical techniques for performing quantitative image and scene analysis image pyramids see section started being widely used to perform tasks such as im age blending figure and coarse to fine correspondence search rosenfeld burt and adelson b rosenfeld quam anandan continuous versions of pyramids using the concept of scale space processing were also developed witkin witkin terzopoulos and kass lindeberg in the late wavelets see sec tion started displacing or augmenting regular image pyramids in some applications more recent developments in visual perception theory are covered in palmer livingstone a b c d e f figure examples of computer vision algorithms from the a pyramid blending burt and adelson qc acm b shape from shading freeman and adelson qc ieee c edge detection freeman and adelson qc ieee d physically based models terzopoulos and witkin qc ieee e regularization based surface reconstruction terzopoulos qc ieee f range data acquisition and merging banno masuda oishi et al qc springer adelson simoncelli and hingorani mallat simoncelli and adelson b simoncelli freeman adelson et al the use of stereo as a quantitative shape cue was extended by a wide variety of shape from x techniques including shape from shading figure see section and horn pentland blake zimmerman and knowles horn and brooks photometric stereo see section and woodham shape from texture see sec tion and witkin pentland malik and rosenholtz and shape from focus see section and nayar watanabe and noguchi horn has a nice discussion of most of these techniques research into better edge and contour detection figure see section was also active during this period canny nalwa and binford including the introduc tion of dynamically evolving contour trackers section such as snakes kass witkin and terzopoulos as well as three dimensional physically based models figure terzopoulos witkin and kass kass witkin and terzopoulos terzopoulos and fleischer terzopoulos witkin and kass researchers noticed that a lot of the stereo flow shape from x and edge detection al gorithms could be unified or at least described using the same mathematical framework if they were posed as variational optimization problems see section and made more ro bust well posed using regularization figure see section and terzopoulos poggio torre and koch terzopoulos blake and zisserman bertero pog gio and torre terzopoulos around the same time geman and geman pointed out that such problems could equally well be formulated using discrete markov ran dom field mrf models see section which enabled the use of better global search and optimization algorithms such as simulated annealing online variants of mrf algorithms that modeled and updated uncertainties using the kalman filter were introduced a little later dickmanns and graefe matthies kanade and szeliski szeliski attempts were also made to map both regularized and mrf algorithms onto parallel hardware poggio and koch poggio little gamble et al fischler firschein barnard et al the book by fischler and firschein contains a nice collection of articles focusing on all of these topics stereo flow regularization mrfs and even higher level vision three dimensional range data processing acquisition merging modeling and recogni tion see figure continued being actively explored during this decade agin and binford besl and jain faugeras and hebert curless and levoy the compi lation by kanade contains a lot of the interesting papers in this area while a lot of the previously mentioned topics continued to be explored a few of them became significantly more active a burst of activity in using projective invariants for recognition mundy and zisserman evolved into a concerted effort to solve the structure from motion problem see chap ter a lot of the initial activity was directed at projective reconstructions which did not require knowledge of camera calibration faugeras hartley gupta and chang hartley faugeras and luong hartley and zisserman simultaneously fac torization techniques section were developed to solve efficiently problems for which or thographic camera approximations were applicable figure tomasi and kanade poelman and kanade anandan and irani and then later extended to the perspec tive case christy and horaud triggs eventually the field started using full global optimization see section and taylor kriegman and anandan szeliski and kang azarbayejani and pentland which was later recognized as being the same as the bundle adjustment techniques traditionally used in photogrammetry triggs mclauch lan hartley et al fully automated sparse modeling systems were built using such techniques beardsley torr and zisserman schaffalitzky and zisserman brown and lowe snavely seitz and szeliski work begun in the on using detailed measurements of color and intensity combined a b c d e f figure examples of computer vision algorithms from the a factorization based structure from motion tomasi and kanade qc springer b dense stereo match ing boykov veksler and zabih c multi view reconstruction seitz and dyer qc springer d face tracking matthews xiao and baker e image segmenta tion belongie fowlkes chung et al qc pentland springer f face recognition turk and with accurate physical models of radiance transport and color image formation created its own subfield known as physics based vision a good survey of the field can be found in the three volume collection on this topic wolff shafer and healey healey and shafer shafer healey and wolff optical flow methods see chapter continued to be improved nagel and enkelmann bolles baker and marimont horn and weldon jr anandan bergen anandan hanna et al black and anandan bruhn weickert and schno rr papenberg bruhn brox et al with nagel barron fleet and beauchemin baker black lewis et al being good surveys similarly a lot of progress was made on dense stereo correspondence algorithms see chapter okutomi and kanade boykov veksler and zabih birchfield and tomasi boykov veksler and zabih and the survey and comparison in scharstein and szeliski with the biggest breakthrough being perhaps global optimization using graph cut techniques fig ure boykov veksler and zabih multi view stereo algorithms figure that produce complete surfaces see sec tion were also an active topic of research seitz and dyer kutulakos and seitz that continues to be active today seitz curless diebel et al techniques for producing volumetric descriptions from binary silhouettes see section continued to be developed potmesil srivasan liang and hackwood szeliski lau rentini along with techniques based on tracking and reconstructing smooth occluding contours see section and cipolla and blake vaillant and faugeras zheng boyer and berger szeliski and weiss cipolla and giblin tracking algorithms also improved a lot including contour tracking using active contours see section such as snakes kass witkin and terzopoulos particle filters blake and isard and level sets malladi sethian and vemuri as well as intensity based direct techniques lucas and kanade shi and tomasi rehg and kanade often applied to tracking faces figure lanitis taylor and cootes matthews and baker matthews xiao and baker and whole bodies sidenbladh black and fleet hilton fua and ronfard moeslund hilton and kru ger image segmentation see chapter figure a topic which has been active since the earliest days of computer vision brice and fennema horowitz and pavlidis riseman and arbib rosenfeld and davis haralick and shapiro pavlidis and liow was also an active topic of research producing techniques based on min imum energy mumford and shah and minimum description length leclerc normalized cuts shi and malik and mean shift comaniciu and meer statistical learning techniques started appearing first in the application of principal com ponent eigenface analysis to face recognition figure see section and turk and pentland and linear dynamical systems for curve tracking see section and blake and isard perhaps the most notable development in computer vision during this decade was the increased interaction with computer graphics seitz and szeliski especially in the cross disciplinary area of image based modeling and rendering see chapter the idea of manipulating real world imagery directly to create new animations first came to prominence with image morphing techniques see section and beier and neely and was later applied to view interpolation chen and williams seitz and dyer panoramic image stitching see chapter and mann and picard chen szeliski szeliski and shum szeliski and full light field rendering figure see section and gortler grzeszczuk szeliski et al levoy and hanrahan shade gortler he et al at the same time image based modeling techniques figure for automatically creating realistic models from collections of images were also being introduced beardsley torr and zisserman debevec taylor and malik taylor debevec and malik a b c d e f figure recent examples of computer vision algorithms a image based rendering gortler grzeszczuk szeliski et al b image based modeling debevec taylor and malik qc acm c interactive tone mapping lischinski farbman uyttendaele et al d texture synthesis efros and freeman e feature based recognition fergus perona and zisserman f region based recognition mori ren efros et al qc ieee this past decade has continued to see a deepening interplay between the vision and graphics fields in particular many of the topics introduced under the rubric of image based rendering such as image stitching see chapter light field capture and rendering see section and high dynamic range hdr image capture through exposure bracketing see section and mann and picard debevec and malik were re christened as computational photography see chapter to acknowledge the increased use of such techniques in everyday digital photography for example the rapid adoption of exposure bracketing to create high dynamic range images necessitated the development of tone mapping algorithms figure see section to convert such images back to displayable results fattal lischinski and werman durand and dorsey rein hard stark shirley et al lischinski farbman uyttendaele et al in addition to merging multiple exposures techniques were developed to merge flash images with non flash counterparts eisemann and durand petschnigg agrawala hoppe et al and to interactively or automatically select different regions from overlapping images agarwala dontcheva agrawala et al texture synthesis figure see section quilting efros and leung efros and freeman kwatra scho dl essa et al and inpainting bertalmio sapiro caselles et al bertalmio vese sapiro et al criminisi pe rez and toyama are additional topics that can be classified as computational photography techniques since they re combine input image samples to produce new photographs a second notable trend during this past decade has been the emergence of feature based techniques combined with learning for object recognition see section and ponce hebert schmid et al some of the notable papers in this area include the constellation model of fergus perona and zisserman figure and the pictorial structures of felzenszwalb and huttenlocher feature based techniques also dominate other recognition tasks such as scene recognition zhang marszalek lazebnik et al and panorama and location recognition brown and lowe schindler brown and szeliski and while interest point patch based features tend to dominate current research some groups are pursuing recognition based on contours belongie malik and puzicha and region segmentation figure mori ren efros et al another significant trend from this past decade has been the development of more efficient algorithms for complex global optimization problems see sections and b and szeliski zabih scharstein et al blake kohli and rother while this trend began with work on graph cuts boykov veksler and zabih kohli and torr a lot of progress has also been made in message passing algorithms such as loopy belief propagation lbp yedidia freeman and weiss kumar and torr the final trend which now dominates a lot of the visual recognition research in our com munity is the application of sophisticated machine learning techniques to computer vision problems see section and freeman perona and scho lkopf this trend coin cides with the increased availability of immense quantities of partially labelled data on the internet which makes it more feasible to learn object categories without the use of careful human supervision book overview in the final part of this introduction i give a brief tour of the material in this book as well as a few notes on notation and some additional general references since computer vision is such a broad field it is possible to study certain aspects of it e g geometric image formation and structure recovery without engaging other parts e g the modeling of reflectance and shading some of the chapters in this book are only loosely coupled with others and it is not strictly necessary to read all of the material in sequence image processing vision graphics figure relationship between images geometry and photometry as well as a taxonomy of the topics covered in this book topics are roughly positioned along the left right axis depending on whether they are more closely related to image based left geometry based middle or appearance based right representations and on the vertical axis by increasing level of abstraction the whole figure should be taken with a large grain of salt as there are many additional subtle connections between topics not illustrated here figure shows a rough layout of the contents of this book since computer vision involves going from images to a structural description of the scene and computer graphics the converse i have positioned the chapters horizontally in terms of which major component they address in addition to vertically according to their dependence going from left to right we see the major column headings as images which are in nature geometry which encompasses descriptions and photometry which encom passes object appearance an alternative labeling for these latter two could also be shape and appearance see e g chapter and kang szeliski and anandan going from top to bottom we see increasing levels of modeling and abstraction as well as tech niques that build on previously developed algorithms of course this taxonomy should be taken with a large grain of salt as the processing and dependencies in this diagram are not strictly sequential and subtle additional dependencies and relationships also exist e g some recognition techniques make use of information the placement of topics along the hor izontal axis should also be taken lightly as most vision algorithms involve mapping between at least two different representations interspersed throughout the book are sample applications which relate the algorithms and mathematical material being presented in various chapters to useful real world applica tions many of these applications are also presented in the exercises sections so that students can write their own at the end of each section i provide a set of exercises that the students can use to imple ment test and refine the algorithms and techniques presented in each section some of the exercises are suitable as written homework assignments others as shorter one week projects and still others as open ended research problems that make for challenging final projects motivated students who implement a reasonable subset of these exercises will by the end of the book have a computer vision software library that can be used for a variety of interesting tasks and projects as a reference book i try wherever possible to discuss which techniques and algorithms work well in practice as well as providing up to date pointers to the latest research results in the areas that i cover the exercises can be used to build up your own personal library of self tested and validated vision algorithms which is more worthwhile in the long term assuming you have the time than simply pulling algorithms out of a library whose performance you do not really understand the book begins in chapter with a review of the image formation processes that create the images that we see and capture understanding this process is fundamental if you want to take a scientific model based approach to computer vision students who are eager to just start implementing algorithms or courses that have limited time can skip ahead to the for an interesting comparison with what is known about the human visual system e g the largely parallel what and where pathways see some textbooks on human perception palmer livingstone n image formation image processing features segmentation structure from motion motion stitching computational photography stereo shape image based rendering recognition figure a pictorial summary of the chapter contents sources brown szeliski and winder comaniciu and meer snavely seitz and szeliski nagel and enkelmann szeliski and shum debevec and malik gortler grzeszczuk szeliski et al viola and jones see the figures in the respec tive chapters for copyright information next chapter and dip into this material later in chapter we break down image formation into three major components geometric image formation section deals with points lines and planes and how these are mapped onto images using projective geometry and other models including radial lens distortion photometric image formation section covers radiometry which describes how light interacts with surfaces in the world and optics which projects light onto the sensor plane finally section covers how sensors work including topics such as sampling and aliasing color sensing and in camera compression chapter covers image processing which is needed in almost all computer vision appli cations this includes topics such as linear and non linear filtering section the fourier transform section image pyramids and wavelets section geometric transforma tions such as image warping section and global optimization techniques such as regu larization and markov random fields mrfs section while most of this material is covered in courses and textbooks on image processing the use of optimization techniques is more typically associated with computer vision although mrfs are now being widely used in image processing as well the section on mrfs is also the first introduction to the use of bayesian inference techniques which are covered at a more abstract level in appendix b chapter also presents applications such as seamless image blending and image restoration in chapter we cover feature detection and matching a lot of current reconstruction and recognition techniques are built on extracting and matching feature points section so this is a fundamental technique required by many subsequent chapters chapters and we also cover edge and straight line detection in sections and chapter covers region segmentation techniques including active contour detection and tracking section segmentation techniques include top down split and bottom up merge techniques mean shift techniques that find modes of clusters and various graph based segmentation approaches all of these techniques are essential building blocks that are widely used in a variety of applications including performance driven animation interactive image editing and recognition in chapter we cover geometric alignment and camera calibration we introduce the basic techniques of feature based alignment in section and show how this problem can be solved using either linear or non linear least squares depending on the motion involved we also introduce additional concepts such as uncertainty weighting and robust regression which are essential to making real world systems work feature based alignment is then used as a building block for pose estimation extrinsic calibration in section and camera intrinsic calibration in section chapter also describes applications of these techniques to photo alignment for flip book animations pose estimation from a hand held camera and single view reconstruction of building models chapter covers the topic of structure from motion which involves the simultaneous recovery of camera motion and scene structure from a collection of tracked fea tures this chapter begins with the easier problem of point triangulation section which is the reconstruction of points from matched features when the camera positions are known it then describes two frame structure from motion section for which al gebraic techniques exist as well as robust sampling techniques such as ransac that can discount erroneous feature matches the second half of chapter describes techniques for multi frame structure from motion including factorization section bundle adjustment section and constrained motion and structure models section it also presents applications in view morphing sparse model construction and match move in chapter we go back to a topic that deals directly with image intensities as op posed to feature tracks namely dense intensity based motion estimation optical flow we start with the simplest possible motion models translational motion section and cover topics such as hierarchical coarse to fine motion estimation fourier based techniques and iterative refinement we then present parametric motion models which can be used to com pensate for camera rotation and zooming as well as affine or planar perspective motion sec tion this is then generalized to spline based motion models section and finally to general per pixel optical flow section including layered and learned motion models section applications of these techniques include automated morphing frame interpo lation slow motion and motion based user interfaces chapter is devoted to image stitching i e the construction of large panoramas and com posites while stitching is just one example of computation photography see chapter there is enough depth here to warrant a separate chapter we start by discussing various pos sible motion models section including planar motion and pure camera rotation we then discuss global alignment section which is a special simplified case of general bundle adjustment and then present panorama recognition i e techniques for automatically discovering which images actually form overlapping panoramas finally we cover the topics of image compositing and blending section which involve both selecting which pixels from which images to use and blending them together so as to disguise exposure differences image stitching is a wonderful application that ties together most of the material covered in earlier parts of this book it also makes for a good mid term course project that can build on previously developed techniques such as image warping and feature detection and match ing chapter also presents more specialized variants of stitching such as whiteboard and document scanning video summarization panography full spherical panoramas and interactive photomontage for blending repeated action shots together chapter presents additional examples of computational photography which is the pro cess of creating new images from one or more input photographs often based on the careful modeling and calibration of the image formation process section computational pho tography techniques include merging multiple exposures to create high dynamic range images section increasing image resolution through blur removal and super resolution sec tion and image editing and compositing operations section we also cover the topics of texture analysis synthesis and inpainting hole filling in section as well as non photorealistic rendering section in chapter we turn to the issue of stereo correspondence which can be thought of as a special case of motion estimation where the camera positions are already known sec tion this additional knowledge enables stereo algorithms to search over a much smaller space of correspondences and in many cases to produce dense depth estimates that can be converted into visible surface models section we also cover multi view stereo algorithms that build a true surface representation instead of just a single depth map section applications of stereo matching include head and gaze tracking as well as depth based background replacement z keying chapter covers additional shape and appearance modeling techniques these in clude classic shape from x techniques such as shape from shading shape from texture and shape from focus section as well as shape from smooth occluding contours sec tion and silhouettes section an alternative to all of these passive computer vision techniques is to use active rangefinding section i e to project patterned light onto scenes and recover the geometry through triangulation processing all of these representations often involves interpolating or simplifying the geometry section or using alternative representations such as surface point sets section the collection of techniques for going from one or more images to partial or full models is often called image based modeling or photography section examines three more specialized application areas architecture faces and human bodies which can use model based reconstruction to fit parameterized models to the sensed data section examines the topic of appearance modeling i e techniques for estimating the texture maps albedos or even sometimes complete bi directional reflectance distribution functions brdfs that describe the appearance of surfaces in chapter we discuss the large number of image based rendering techniques that have been developed in the last two decades including simpler techniques such as view in terpolation section layered depth images section and sprites and layers sec tion as well as the more general framework of light fields and lumigraphs sec tion and higher order fields such as environment mattes section applications of these techniques include navigating collections of photographs using photo tourism and viewing models as object movies in chapter we also discuss video based rendering which is the temporal extension of image based rendering the topics we cover include video based animation section periodic video turned into video textures section and video constructed from multiple video streams section applications of these techniques include video de noising morphing and tours based on video week material project chapter image formation chapter image processing chapter feature detection and matching chapter feature based alignment chapter image stitching chapter dense motion estimation chapter structure from motion pp chapter recognition chapter computational photography chapter stereo correspondence chapter reconstruction chapter image based rendering final project presentations fp table sample syllabi for week and week courses the weeks in parentheses are not used in the shorter version and are two early term mini projects pp is when the student selected final project proposals are due and fp is the final project presentations chapter describes different approaches to recognition it begins with techniques for detecting and recognizing faces sections and then looks at techniques for finding and recognizing particular objects instance recognition in section next we cover the most difficult variant of recognition namely the recognition of broad categories such as cars motorcycles horses and other animals section and the role that scene context plays in recognition section to support the book use as a textbook the appendices and associated web site contain more detailed mathematical topics and additional material appendix a covers linear algebra and numerical techniques including matrix algebra least squares and iterative techniques appendix b covers bayesian estimation theory including maximum likelihood estimation robust statistics markov random fields and uncertainty modeling appendix c describes the supplementary material available to complement this book including images and data sets pointers to software course slides and an on line bibliography sample syllabus teaching all of the material covered in this book in a single quarter or semester course is a herculean task and likely one not worth attempting it is better to simply pick and choose a note on notation topics related to the lecturer preferred emphasis and tailored to the set of mini projects envisioned for the students steve seitz and i have successfully used a week syllabus similar to the one shown in table omitting the parenthesized weeks as both an undergraduate and a graduate level course in computer vision the undergraduate tends to go lighter on the mathematics and takes more time reviewing basics while the graduate level dives more deeply into techniques and assumes the students already have a decent grounding in either vision or related mathematical techniques see also the introduction to computer vision course at stanford which uses a similar curriculum related courses have also been taught on the topics of and computational photography when steve and i teach the course we prefer to give the students several small program ming projects early in the course rather than focusing on written homework or quizzes with a suitable choice of topics it is possible for these projects to build on each other for exam ple introducing feature matching early on can be used in a second assignment to do image alignment and stitching alternatively direct optical flow techniques can be used to do the alignment and more focus can be put on either graph cut seam selection or multi resolution blending techniques we also ask the students to propose a final project we provide a set of suggested topics for those who need ideas by the middle of the course and reserve the last week of the class for student presentations with any luck some of these final projects can actually turn into conference submissions no matter how you decide to structure the course or how you choose to use this book i encourage you to try at least a few small programming tasks to get a good feel for how vision techniques work and when they do not better yet pick topics that are fun and can be used on your own photographs and try to push your creative boundaries to come up with surprising results a note on notation for better or worse the notation found in computer vision and multi view geometry textbooks tends to vary all over the map faugeras hartley and zisserman girod greiner and niemann faugeras and luong forsyth and ponce in this book i use the convention i first learned in my high school physics class and later multi variate http www cs washington edu education courses http www cs washington edu education courses vision stanford edu teaching http www cs washington edu education courses http graphics cs cmu edu courses calculus and computer graphics courses which is that vectors v are lower case bold matrices m are upper case bold and scalars t are mixed case italic unless otherwise noted vectors operate as column vectors i e they post multiply matrices m v although they are sometimes written as comma separated parenthesized lists x x y instead of bracketed column vectors x x y t some commonly used matrices are r for rotations k for calibration matrices and i for the identity matrix homogeneous coordinates section are denoted with a tilde over the vector e g x x y w w x y w x in the cross product operator in matrix form is denoted by additional reading this book attempts to be self contained so that students can implement the basic assignments and algorithms described here without the need for outside references however it does pre suppose a general familiarity with basic concepts in linear algebra and numerical techniques which are reviewed in appendix a and image processing which is reviewed in chapter students who want to delve more deeply into these topics can look in golub and van loan for matrix algebra and strang for linear algebra in image processing there are a number of popular textbooks including crane gomes and velho ja hne pratt russ burger and burge gonzales and woods for computer graphics popular texts include foley van dam feiner et al watt with glassner providing a more in depth look at image formation and rendering for statistics and machine learning chris bishop book is a wonderful and comprehen sive introduction with a wealth of exercises students may also want to look in other textbooks on computer vision for material that we do not cover here as well as for additional project ideas ballard and brown faugeras nalwa trucco and verri forsyth and ponce there is however no substitute for reading the latest research literature both for the lat est ideas and techniques and for the most up to date references to related literature in this book i have attempted to cite the most recent work in each field so that students can read them directly and use them as inspiration for their own work browsing the last few years con ference proceedings from the major vision and graphics conferences such as cvpr eccv iccv and siggraph will provide a wealth of new ideas the tutorials offered at these conferences for which slides or notes are often available on line are also an invaluable re source for a comprehensive bibliography and taxonomy of computer vision research keith price annotated com puter vision bibliography http www visionbib com bibliography contents html is an invaluable resource chapter image formation geometric primitives and transformations geometric primitives transformations transformations rotations to projections lens distortions photometric image formation lighting reflectance and shading optics the digital camera sampling and aliasing color compression additional reading exercises a b zi zo c d figure a few components of the image formation process a perspective projection b light scattering when hitting a surface c lens optics d bayer color filter array before we can intelligently analyze and manipulate images we need to establish a vocabulary for describing the geometry of a scene we also need to understand the image formation process that produced a particular image given a set of lighting conditions scene geometry surface properties and camera optics in this chapter we present a simplified model of such an image formation process section introduces the basic geometric primitives used throughout the book points lines and planes and the geometric transformations that project these quantities into image features figure section describes how lighting surface properties fig ure and camera optics figure interact in order to produce the color values that fall onto the image sensor section describes how continuous color images are turned into discrete digital samples inside the image sensor figure and how to avoid or at least characterize sampling deficiencies such as aliasing the material covered in this chapter is but a brief summary of a very rich and deep set of topics traditionally covered in a number of separate fields a more thorough introduction to the geometry of points lines planes and projections can be found in textbooks on multi view geometry hartley and zisserman faugeras and luong and computer graphics foley van dam feiner et al the image formation synthesis process is traditionally taught as part of a computer graphics curriculum foley van dam feiner et al glass ner watt shirley but it is also studied in physics based computer vision wolff shafer and healey the behavior of camera lens systems is studied in optics mo ller hecht ray two good books on color theory are wyszecki and stiles healey and shafer with livingstone providing a more fun and in formal introduction to the topic of color perception topics relating to sampling and aliasing are covered in textbooks on signal and image processing crane ja hne oppen heim and schafer oppenheim schafer and buck pratt russ burger and burge gonzales and woods a note to students if you have already studied computer graphics you may want to skim the material in section although the sections on projective depth and object centered projection near the end of section may be new to you similarly physics students as well as computer graphics students will mostly be familiar with section finally students with a good background in image processing will already be familiar with sampling issues section as well as some of the material in chapter geometric primitives and transformations in this section we introduce the basic and primitives used in this textbook namely points lines and planes we also describe how features are projected into features more detailed descriptions of these topics along with a gentler and more intuitive introduc tion can be found in textbooks on multiple view geometry hartley and zisserman faugeras and luong geometric primitives geometric primitives form the basic building blocks used to describe three dimensional shapes in this section we introduce points lines and planes later sections of the book discuss curves sections and surfaces section and volumes section points points pixel coordinates in an image can be denoted using a pair of values x x y or alternatively x r x l as stated in the introduction we use the notation to denote column vectors points can also be represented using homogeneous coordinates x x y w where vectors that differ only by scale are considered to be equivalent is called the projective space a homogeneous vector x can be converted back into an inhomogeneous vector x by dividing through by the last element w i e x x y w w x y w x where x x y is the augmented vector homogeneous points whose last element is w are called ideal points or points at infinity and do not have an equivalent inhomogeneous representation lines lines can also be represented using homogeneous coordinates l a b c the corresponding line equation is x l ax by c we can normalize the line equation vector so that l nˆx nˆy d nˆ d with in this case nˆ is the normal vector perpendicular to the line and d is its distance to the origin figure the one exception to this normalization is the line at infinity l which includes all ideal points at infinity we can also express nˆ as a function of rotation angle θ nˆ nˆx nˆy cos θ sin θ figure this representation is commonly used in the hough transform line finding a b figure a line equation and b plane equation expressed in terms of the normal nˆ and distance to the origin d algorithm which is discussed in section the combination θ d is also known as polar coordinates when using homogeneous coordinates we can compute the intersection of two lines as x where is the cross product operator similarly the line joining two points can be written as l x x when trying to fit an intersection point to multiple lines or conversely a line to multiple points least squares techniques section and appendix a can be used as discussed in exercise conics there are other algebraic curves that can be expressed with simple polynomial homogeneous equations for example the conic sections so called because they arise as the intersection of a plane and a cone can be written using a quadric equation x t qx quadric equations play useful roles in the study of multi view geometry and camera calibra tion hartley and zisserman faugeras and luong but are not used extensively in this book points point coordinates in three dimensions can be written using inhomogeneous co ordinates x x y z or homogeneous coordinates x x y z w as before it is sometimes useful to denote a point using the augmented vector x x y z with x w x z p λ r λ p λq q x y figure line equation r λ p λq planes planes can also be represented as homogeneous coordinates m with a corresponding plane equation a b c d x m ax by cz d we can also normalize the plane equation as m nˆx nˆy nˆz d nˆ d with in this case nˆ is the normal vector perpendicular to the plane and d is its distance to the origin figure as with the case of lines the plane at infinity m which contains all the points at infinity cannot be normalized i e it does not have a unique normal or a finite distance we can express nˆ as a function of two angles θ φ nˆ cos θ cos φ sin θ cos φ sin φ i e using spherical coordinates but these are less commonly used than polar coordinates since they do not uniformly sample the space of possible normal vectors lines lines in are less elegant than either lines in or planes in one possible representation is to use two points on the line p q any other point on the line can be expressed as a linear combination of these two points r λ p λq as shown in figure if we restrict λ we get the line segment joining p and q if we use homogeneous coordinates we can write the line as r µp λq a special case of this is when the second point is at infinity i e q dˆx dˆy dˆz dˆ here we see that dˆis the direction of the line we can then re write the inhomogeneous line equation as r p λdˆ a disadvantage of the endpoint representation for lines is that it has too many degrees of freedom i e six three for each endpoint instead of the four degrees that a line truly has however if we fix the two points on the line to lie in specific planes we obtain a rep resentation with four degrees of freedom for example if we are representing nearly vertical lines then z and z form two suitable planes i e the x y coordinates in both planes provide the four coordinates describing the line this kind of two plane parameteri zation is used in the light field and lumigraph image based rendering systems described in chapter to represent the collection of rays seen by a camera as it moves in front of an object the two endpoint representation is also useful for representing line segments even when their exact endpoints cannot be seen only guessed at if we wish to represent all possible lines without bias towards any particular orientation we can use plu cker coordinates hartley and zisserman chapter faugeras and luong chapter these coordinates are the six independent non zero entries in the skew symmetric matrix l p q t q p t where p and q are any two non identical points on the line this representation has only four degrees of freedom since l is homogeneous and also satisfies det l which results in a quadratic constraint on the plu cker coordinates in practice the minimal representation is not essential for most applications an ade quate model of lines can be obtained by estimating their direction which may be known ahead of time e g for architecture and some point within the visible portion of the line see section or by using the two endpoints since lines are most often visible as finite line segments however if you are interested in more details about the topic of minimal line parameterizations fo rstner discusses various ways to infer and model lines in projective geometry as well as how to estimate the uncertainty in such fitted models quadrics the analog of a conic section is a quadric surface x t qx hartley and zisserman chapter again while quadric surfaces are useful in the study of multi view geometry and can also serve as useful modeling primitives spheres ellipsoids cylinders we do not study them in great detail in this book transformations having defined our basic primitives we can now turn our attention to how they can be trans formed the simplest transformations occur in the plane and are illustrated in figure figure basic set of planar transformations translation translations can be written as xi x t or xi i t l x where i is the identity matrix or i i t l x where is the zero vector using a matrix results in a more compact notation whereas using a full rank matrix which can be obtained from the matrix by appending a row makes it possible to chain transformations using matrix multiplication note that in any equation where an augmented vector such as x appears on both sides it can always be replaced with a full homogeneous vector x rotation translation this transformation is also known as rigid body motion or the euclidean transformation since euclidean distances are preserved it can be written as xi rx t or where xi r t l x cos θ sin θ sin θ cos θ l is an orthonormal rotation matrix with rrt i and r scaled rotation also known as the similarity transform this transformation can be ex pressed as xi srx t where is an arbitrary scale factor it can also be written as xi sr t x a b tx b a ty l x where we no longer require that the similarity transform preserves angles between lines affine the affine transformation is written as xi ax where a is an arbitrary matrix i e xi l x parallel lines remain parallel under affine transformations projective this transformation also known as a perspective transform or homography operates on homogeneous coordinates x i h x where h is an arbitrary matrix note that h is homogeneous i e it is only defined up to a scale and that two h matrices that differ only by scale are equivalent the resulting homogeneous coordinate x i must be normalized in order to obtain an inhomogeneous result x i e xi h20x and yi h20x perspective transformations preserve straight lines i e they remain straight after the trans formation hierarchy of transformations the preceding set of transformations are illustrated in figure and summarized in table the easiest way to think of them is as a set of potentially restricted matrices operating on homogeneous coordinate vectors hartley and zisserman contains a more detailed description of the hierarchy of planar transformations the above transformations form a nested set of groups i e they are closed under com position and have an inverse that is a member of the same group this will be important later when applying these transformations to images in section each simpler group is a subset of the more complex group below it co vectors while the above transformations can be used to transform points in a plane can they also be used directly to transform a line equation consider the homogeneous equa tion l x if we transform xi h x we obtain li x i it t i t x l x i e li h t l thus the action of a projective transformation on a co vector such as a line or normal can be represented by the transposed inverse of the matrix which is equiv alent to the adjoint of h since projective transformation matrices are homogeneous jim transformation matrix dof preserves icon translation i t orientation rigid euclidean r t l lengths similarity sr t l angles affine a l parallelism projective h straight lines table hierarchy of coordinate transformations each transformation also preserves the properties listed in the rows below it i e similarity preserves not only angles but also parallelism and straight lines the matrices are extended with a third row to form a full matrix for homogeneous coordinate transformations blinn describes in chapters and the ins and outs of notating and manipulating co vectors while the above transformations are the ones we use most extensively a number of addi tional transformations are sometimes used stretch squash this transformation changes the aspect ratio of an image xi sxx tx yi syy ty and is a restricted form of an affine transformation unfortunately it does not nest cleanly with the groups listed in table planar surface flow this eight parameter transformation horn bergen anandan hanna et al girod greiner and niemann xi a6x2 yi a7x2 arises when a planar surface undergoes a small motion it can thus be thought of as a small motion approximation to a full homography its main attraction is that it is linear in the motion parameters ak which are often the quantities being estimated transformation matrix dof preserves icon translation i t orientation rigid euclidean r t l lengths similarity sr t l angles affine a l parallelism projective h straight lines table hierarchy of coordinate transformations each transformation also preserves the properties listed in the rows below it i e similarity preserves not only angles but also parallelism and straight lines the matrices are extended with a fourth row to form a full matrix for homogeneous coordinate transformations the mnemonic icons are drawn in but are meant to suggest transformations occurring in a full cube bilinear interpolant this eight parameter transform wolberg xi a6xy yi a7xy can be used to interpolate the deformation due to the motion of the four corner points of a square in fact it can interpolate the motion of any four non collinear points while the deformation is linear in the motion parameters it does not generally preserve straight lines only lines parallel to the square axes however it is often quite useful e g in the interpolation of sparse grids using splines section transformations the set of three dimensional coordinate transformations is very similar to that available for transformations and is summarized in table as in these transformations form a nested set of groups hartley and zisserman section give a more detailed descrip tion of this hierarchy translation translations can be written as xi x t or xi i t l x where i is the identity matrix and is the zero vector rotation translation also known as rigid body motion or the euclidean trans formation it can be written as xi rx t or xi r t l x where r is a orthonormal rotation matrix with rrt i and r note that sometimes it is more convenient to describe a rigid motion using xi r x c rx rc where c is the center of rotation often the camera center compactly parameterizing a rotation is a non trivial task which we describe in more detail below scaled rotation the similarity transform can be expressed as xi srx t where is an arbitrary scale factor it can also be written as xi sr t l x this transformation preserves angles between lines and planes affine the affine transform is written as xi ax where a is an arbitrary matrix i e a03 xi a13 x a23 parallel lines and planes remain parallel under affine transformations projective this transformation variously known as a perspective transform homogra phy or collineation operates on homogeneous coordinates x i h x where h is an arbitrary homogeneous matrix as in the resulting homogeneous coordinate x i must be normalized in order to obtain an inhomogeneous result x perspective transformations preserve straight lines i e they remain straight after the transformation figure rotation around an axis nˆ by an angle θ rotations the biggest difference between and coordinate transformations is that the parameter ization of the rotation matrix r is not as straightforward but several possibilities exist euler angles a rotation matrix can be formed as the product of three rotations around three cardinal axes e g x y and z or x y and x this is generally a bad idea as the result depends on the order in which the transforms are applied what is worse it is not always possible to move smoothly in the parameter space i e sometimes one or more of the euler angles change dramatically in response to a small change in rotation for these reasons we do not even give the formula for euler angles in this book interested readers can look in other textbooks or technical reports faugeras diebel note that in some applications if the rotations are known to be a set of uni axial transforms they can always be represented using an explicit set of rigid transformations axis angle exponential twist a rotation can be represented by a rotation axis nˆ and an angle θ or equivalently by a vector ω θnˆ figure shows how we can compute the equivalent rotation first we project the vector v onto the axis nˆ to obtain nˆ nˆ v nˆnˆt v which is the component of v that is not affected by the rotation next we compute the perpendicular residual of v from nˆ v v i nˆnˆt v in robotics this is sometimes referred to as gimbal lock we can rotate this vector by using the cross product v nˆ v nˆ v where nˆ is the matrix form of the cross product operator with the vector nˆ nˆx nˆy nˆz nˆz nˆy nˆ nˆz nˆx note that rotating this vector by another is equivalent to taking the cross product again v nˆ v nˆ v v and hence v v v v i nˆ v we can now compute the in plane component of the rotated vector u as u cos θv sin θv sin θ nˆ cos θ nˆ v putting all these terms together we obtain the final rotated vector as u u i sin θ nˆ cos θ nˆ v we can therefore write the rotation matrix corresponding to a rotation by θ around an axis nˆ as r nˆ θ i sin θ nˆ cos θ nˆ which is known as rodriguez formula ayache the product of the axis nˆ and angle θ ω θnˆ ωx ωy ωz is a minimal represen tation for a rotation rotations through common angles such as multiples of can be represented exactly and converted to exact matrices if θ is stored in degrees unfortunately this representation is not unique since we can always add a multiple of radians to θ and get the same rotation matrix as well nˆ θ and nˆ θ represent the same rotation however for small rotations e g corrections to rotations this is an excellent choice in particular for small infinitesimal or instantaneous rotations and θ expressed in radians rodriguez formula simplifies to r ω i sin θ nˆ i θnˆ ωz ωy ωz ωx ωy ωx which gives a nice linearized relationship between the rotation parameters ω and r we can also write r ω v v ω v which is handy when we want to compute the derivative of rv with respect to ω rv z y another way to derive a rotation through a finite angle is called the exponential twist murray li and sastry a rotation by an angle θ is equivalent to k rotations through θ k in the limit as k we obtain r nˆ θ lim i θnˆ k exp ω k k if we expand the matrix exponential as a taylor series using the identity nˆ k nˆ k k and again assuming θ is in radians exp ˆ ˆ ˆ ω i θ n n n i θ nˆ nˆ i sin θ nˆ cos θ nˆ which yields the familiar rodriguez formula unit quaternions the unit quaternion representation is closely related to the angle axis representation a unit quaternion is a unit length vector whose components can be written as q qx qy qz qw or q x y z w for short unit quaternions live on the unit sphere and antipodal opposite sign quaternions q and q represent the same rotation figure other than this ambiguity dual covering the unit quaternion representation of a rotation is unique furthermore the representation is continuous i e as rotation matrices vary continuously one can find a continuous quaternion representation although the path on the quaternion sphere may wrap all the way around before returning to the origin qo for these and other reasons given below quaternions are a very popular representation for pose and for pose interpolation in computer graphics shoemake quaternions can be derived from the axis angle representation through the formula θ θ q v w sin nˆ cos figure unit quaternions live on the unit sphere this figure shows a smooth trajectory through the three quaternions and the antipodal point to namely represents the same rotation as where nˆ and θ are the rotation axis and angle using the trigonometric identities sin θ sin θ cos θ and cos θ θ rodriguez formula can be converted to r nˆ θ i sin θ nˆ cos θ nˆ i v v this suggests a quick way to rotate a vector v by a quaternion using a series of cross products scalings and additions to obtain a formula for r q as a function of x y z w recall that z y xy xz v and y x xy yz xz yz we thus obtain xy zw xz yw r q xy zw yz xw xz yw yz xw the diagonal terms can be made more symmetrical by replacing with etc the nicest aspect of unit quaternions is that there is a simple algebra for composing rota tions expressed as unit quaternions given two quaternions and the quaternion multiply operator is defined as w1v0 with the property that r r r note that quaternion multiplication is not commutative just as rotations and matrix multiplications are not taking the inverse of a quaternion is easy just flip the sign of v or w but not both you can verify this has the desired effect of transposing the r matrix in thus we can also define quaternion division as w1v0 v0 this is useful when the incremental rotation between two rotations is desired in particular if we want to determine a rotation that is partway between two given rota tions we can compute the incremental rotation take a fraction of the angle and compute the new rotation this procedure is called spherical linear interpolation or slerp for short shoe make and is given in algorithm note that shoemake presents two formulas other than the one given here the first exponentiates qr by alpha before multiplying the original quaternion while the second treats the quaternions as vectors on a sphere and uses q sin α θ q sin αθ q sin θ sin θ where θ cos and the dot product is directly between the quaternion vectors all of these formulas give comparable results although care should be taken when and are close together which is why i prefer to use an arctangent to establish the rotation angle which rotation representation is better the choice of representation for rotations depends partly on the application the axis angle representation is minimal and hence does not require any additional con straints on the parameters no need to re normalize after each update if the angle is ex pressed in degrees it is easier to understand the pose say twist around x axis and also easier to express exact rotations when the angle is in radians the derivatives of r with respect to ω can easily be computed quaternions on the other hand are better if you want to keep track of a smoothly moving camera since there are no discontinuities in the representation it is also easier to interpolate between rotations and to chain rigid transformations murray li and sastry bregler and malik my usual preference is to use quaternions but to update their estimates using an incre mental rotation as described in section algorithm spherical linear interpolation slerp the axis and total angle are first com puted from the quaternion ratio this computation can be lifted outside an inner loop that generates a set of interpolated position for animation an incremental quaternion is then computed and multiplied by the starting rotation quaternion to projections now that we know how to represent and geometric primitives and how to transform them spatially we need to specify how primitives are projected onto the image plane we can do this using a linear to projection matrix the simplest model is orthography which requires no division to get the final inhomogeneous result the more commonly used model is perspective since this more accurately models the behavior of real cameras orthography and para perspective an orthographic projection simply drops the z component of the three dimensional coordi nate p to obtain the point x in this section we use p to denote points and x to denote points this can be written as x p if we are using homogeneous projective coordinates we can write x p a view b orthography c scaled orthography d para perspective e perspective f object centered figure commonly used projection models a view of world b orthography c scaled orthography d para perspective e perspective f object centered each diagram shows a top down view of the projection note how parallel lines on the ground plane and box sides remain parallel in the non perspective projections i e we drop the z component but keep the w component orthography is an approximate model for long focal length telephoto lenses and objects whose depth is shallow relative to their distance to the camera sawhney and hanson it is exact only for telecentric lenses baker and nayar in practice world coordinates which may measure dimensions in meters need to be scaled to fit onto an image sensor physically measured in millimeters but ultimately mea sured in pixels for this reason scaled orthography is actually more commonly used x p this model is equivalent to first projecting the world points onto a local fronto parallel image plane and then scaling this image using regular perspective projection the scaling can be the same for all parts of the scene figure or it can be different for objects that are being modeled independently figure more importantly the scaling can vary from frame to frame when estimating structure from motion which can better model the scale change that occurs as an object approaches the camera scaled orthography is a popular model for reconstructing the shape of objects far away from the camera since it greatly simplifies certain computations for example pose camera orientation can be estimated using simple least squares section under orthography structure and motion can simultaneously be estimated using factorization singular value de composition as discussed in section tomasi and kanade a closely related projection model is para perspective aloimonos poelman and kanade in this model object points are again first projected onto a local reference parallel to the image plane however rather than being projected orthogonally to this plane they are projected parallel to the line of sight to the object center figure this is followed by the usual projection onto the final image plane which again amounts to a scaling the combination of these two projections is therefore affine and can be written as a03 x a11 a13 p note how parallel lines in remain parallel after projection in figure d para perspective provides a more accurate projection model than scaled orthography without incurring the added complexity of per pixel perspective division which invalidates traditional factoriza tion methods poelman and kanade perspective the most commonly used projection in computer graphics and computer vision is true perspective figure here points are projected onto the image plane by dividing them by their z component using inhomogeneous coordinates this can be written as x z x pz p y z in homogeneous coordinates the projection has a simple linear form x p i e we drop the w component of p thus after projection it is not possible to recover the distance of the point from the image which makes sense for a imaging sensor a form often seen in computer graphics systems is a two step projection that first projects coordinates into normalized device coordinates in the range x y z and then rescales these coordinates to integer pixel coordinates using a view port transformation watt opengl arb the initial perspective projection is then represented using a matrix x far range near far range p where znear and zfar are the near and far z clipping planes and zrange zfar znear note that the first two rows are actually scaled by the focal length and the aspect ratio so that visible rays are mapped to x y z the reason for keeping the third row rather than dropping it is that visibility operations such as z buffering require a depth for every graphical element that is being rendered if we set znear zfar and switch the sign of the third row the third element of the normalized screen vector becomes the inverse depth i e the disparity okutomi and kanade this can be quite convenient in many cases since for cameras moving around outdoors the inverse depth to the camera is often a more well conditioned parameterization than direct distance while a regular image sensor has no way of measuring distance to a surface point range sensors section and stereo matching algorithms chapter can compute such values it is then convenient to be able to map from a sensor based depth or disparity value d directly back to a location using the inverse of a matrix section we can do this if we represent perspective projection using a full rank matrix as in figure projection of a camera centered point pc onto the sensor planes at location p oc is the camera center nodal point cs is the origin of the sensor plane coordinate system and sx and sy are the pixel spacings camera intrinsics once we have projected a point through an ideal pinhole using a projection matrix we must still transform the resulting coordinates according to the pixel sensor spacing and the relative position of the sensor plane to the origin figure shows an illustration of the geometry involved in this section we first present a mapping from pixel coordinates to rays using a sensor homography m since this is easier to explain in terms of physically measurable quantities we then relate these quantities to the more commonly used camera in trinsic matrix k which is used to map camera centered points pc to pixel coordinates x image sensors return pixel values indexed by integer pixel coordinates xs ys often with the coordinates starting at the upper left corner of the image and moving down and to the right this convention is not obeyed by all imaging libraries but the adjustment for other coordinate systems is straightforward to map pixel centers to coordinates we first scale the xs ys values by the pixel spacings sx sy sometimes expressed in microns for solid state sensors and then describe the orientation of the sensor array relative to the camera projection center oc with an origin cs and a rotation rs figure the combined to projection can then be written as x p rs cs l ys m sx the first two columns of the matrix m are the vectors corresponding to unit steps in the image pixel array along the xs and ys directions while the third column is the image array origin cs the matrix m is parameterized by eight unknowns the three parameters describing the rotation rs the three parameters describing the translation cs and the two scale factors sx sy note that we ignore here the possibility of skew between the two axes on the image plane since solid state manufacturing techniques render this negligible in practice unless we have accurate external knowledge of the sensor spacing or sensor orientation there are only seven degrees of freedom since the distance of the sensor from the origin cannot be teased apart from the sensor spacing based on external image measurement alone however estimating a camera model m with the required seven degrees of freedom i e where the first two columns are orthogonal after an appropriate re scaling is impractical so most practitioners assume a general homogeneous matrix form the relationship between the pixel center p and the camera centered point pc is given by an unknown scaling p spc we can therefore write the complete projection between pc and a homogeneous version of the pixel address x as x αm kpc the matrix k is called the calibration matrix and describes the camera intrinsics as opposed to the camera orientation in space which are called the extrinsics from the above discussion we see that k has seven degrees of freedom in theory and eight degrees of freedom the full dimensionality of a homogeneous matrix in practice why then do most textbooks on computer vision and multi view geometry faugeras hartley and zisserman faugeras and luong treat k as an upper triangular matrix with five degrees of freedom while this is usually not made explicit in these books it is because we cannot recover the full k matrix based on external measurement alone when calibrating a camera chap ter based on external points or other measurements tsai we end up estimating the intrinsic k and extrinsic r t camera parameters simultaneously using a series of measurements x k r t pw p pw where pw are known world coordinates and p k r t is known as the camera matrix inspecting this equation we see that we can post multiply k by and pre multiply r t by rt and still end up with a valid calibration thus it is impossible based on image measurements alone to know the true orientation of the sensor and the true camera intrinsics the choice of an upper triangular form for k seems to be conventional given a full camera matrix p k r t we can compute an upper triangular k matrix using qr w figure simplified camera intrinsics showing the focal length f and the optical center cx cy the image width and height are w and h factorization golub and van loan note the unfortunate clash of terminologies in matrix algebra textbooks r represents an upper triangular right of the diagonal matrix in computer vision r is an orthogonal rotation there are several ways to write the upper triangular form of k one possibility is fx cx k fy cy which uses independent focal lengths fx and fy for the sensor x and y dimensions the entry encodes any possible skew between the sensor axes due to the sensor not being mounted perpendicular to the optical axis and cx cy denotes the optical center expressed in pixel coordinates another possibility is f cx k af cy where the aspect ratio a has been made explicit and a common focal length f is used in practice for many applications an even simpler form can be obtained by setting a and f cx k f cy often setting the origin at roughly the center of the image e g cx cy w h where w and h are the image height and width can result in a perfectly usable camera model with a single unknown i e the focal length f figure central projection showing the relationship between the and coordi nates p and x as well as the relationship between the focal length f image width w and the field of view θ figure shows how these quantities can be visualized as part of a simplified imaging model note that now we have placed the image plane in front of the nodal point projection center of the lens the sense of the y axis has also been flipped to get a coordinate system compatible with the way that most imaging libraries treat the vertical row coordinate cer tain graphics libraries such as use a left handed coordinate system which can lead to some confusion a note on focal lengths the issue of how to express focal lengths is one that often causes confusion in implementing computer vision algorithms and discussing their results this is because the focal length depends on the units used to measure pixels if we number pixel coordinates using integer values say w h the focal length f and camera center cx cy in can be expressed as pixel values how do these quan tities relate to the more familiar focal lengths used by photographers figure illustrates the relationship between the focal length f the sensor width w and the field of view θ which obey the formula θ w w θ l for conventional film cameras w and hence f is also expressed in millimeters since we work with digital images it is more convenient to express w in pixels so that the focal length f can be used directly in the calibration matrix k as in another possibility is to scale the pixel coordinates so that they go from along the longer image dimension and a a along the shorter axis where a is the image aspect ratio as opposed to the sensor cell aspect ratio introduced earlier this can be accomplished using modified normalized device coordinates xis w s and ysi h s where s max w h this has the advantage that the focal length f and optical center cx cy become independent of the image resolution which can be useful when using multi resolution image processing algorithms such as image pyramids section the use of s instead of w also makes the focal length the same for landscape horizontal and portrait vertical pictures as is the case in photography in some computer graphics textbooks and systems normalized device coordinates go from which requires the use of two different focal lengths to describe the camera intrinsics watt opengl arb setting s w in we obtain the simpler unitless relationship f tan θ the conversion between the various focal length representations is straightforward e g to go from a unitless f to one expressed in pixels multiply by w while to convert from an f expressed in pixels to the equivalent focal length multiply by w camera matrix now that we have shown how to parameterize the calibration matrix k we can put the camera intrinsics and extrinsics together to obtain a single camera matrix p k r t l it is sometimes preferable to use an invertible matrix which can be obtained by not dropping the last row in the p matrix p r k l r r t l k e where e is a rigid body euclidean transformation and k is the full rank calibration matrix the camera matrix p can be used to map directly from world coordinates p w xw yw zw to screen coordinates plus disparity xs xs ys d xs p p w where indicates equality up to scale note that after multiplication by p the vector is divided by the third element of the vector to obtain the normalized form xs xs ys d to make the conversion truly accurate after a downsampling step in a pyramid floating point values of w and h would have to be maintained since they can become non integral if they are ever odd at a larger resolution in the pyramid d d d d d d d parallax xw yw zw c xs ys d z z image plane plane d inverse depth d projective depth figure regular disparity inverse depth and projective depth parallax from a reference plane plane plus parallax projective depth in general when using the matrix p we have the freedom to remap the last row to whatever suits our purpose rather than just being the standard interpretation of disparity as inverse depth let us re write the last row of p as where we then have the equation d nˆ p c z w where z p w rz pw c is the distance of pw from the camera center c along the optical axis z figure thus we can interpret d as the projective disparity or projective depth of a scene point pw from the reference plane pw szeliski and coughlan szeliski and golland shade gortler he et al baker szeliski and anandan the projective depth is also sometimes called parallax in reconstruction algorithms that use the term plane plus parallax kumar anandan and hanna sawhney setting and i e putting the reference plane at infinity results in the more standard d z version of disparity okutomi and kanade another way to see this is to invert the p matrix so that we can map pixels plus disparity directly back to points p w p in general we can choose p to have whatever form is convenient i e to sample space us ing an arbitrary projection this can come in particularly handy when setting up multi view stereo reconstruction algorithms since it allows us to sweep a series of planes section through space with a variable projective sampling that best matches the sensed image mo tions collins szeliski and golland saito and kanade y d n p h10 a b figure a point is projected into two images a relationship between the point co ordinate x y z and the projected point x y d b planar homography induced by points all lying on a common plane p mapping from one camera to another what happens when we take two images of a scene from different camera positions or orientations figure using the full rank camera matrix p k e from we can write the projection from world to screen coordinates as x k p assuming that we know the z buffer or disparity value for a pixel in one image we can compute the point location p using p e and then project it into another image yielding x k e p k e e p p m x unfortunately we do not usually have access to the depth coordinates of pixels in a regular photographic image however for a planar scene as discussed above in we can replace the last row of p in with a general plane equation p that maps points on the plane to values figure thus if we set we can ignore the last column of m in and also its last row since we do not care about the final z buffer depth the mapping equation thus reduces to x h where h is a general homography matrix and x and x are now homogeneous coordinates i e vectors szeliski this justifies the use of the parameter homog raphy as a general alignment model for mosaics of planar scenes mann and picard szeliski the other special case where we do not need to know depth to perform inter camera mapping is when the camera is undergoing pure rotation section i e when in this case we can write x 1k k1r10k which again can be represented with a homography if we assume that the calibration matrices have known aspect ratios and centers of projection this homography can be parameterized by the rotation amount and the two unknown focal lengths this particular formulation is commonly used in image stitching applications section object centered projection when working with long focal length lenses it often becomes difficult to reliably estimate the focal length from image measurements alone this is because the focal length and the distance to the object are highly correlated and it becomes difficult to tease these two effects apart for example the change in scale of an object viewed through a zoom telephoto lens can either be due to a zoom change or a motion towards the user this effect was put to dramatic use in some of alfred hitchcock film vertigo where the simultaneous change of zoom and camera motion produces a disquieting effect this ambiguity becomes clearer if we write out the projection equation corresponding to the simple calibration matrix k x f rx p tx c rz p tz y f ry p ty c rz p tz where rx ry and rz are the three rows of r if the distance to the object center tz the size of the object the denominator is approximately tz and the overall scale of the projected object depends on the ratio of f to tz it therefore becomes difficult to disentangle these two quantities to see this more clearly let ηz t z and ηzf we can then re write the above equations as x rx p tx ηzrz p y ry p ty ηzrz p cx cy szeliski and kang pighin hecker lischinski et al the scale of the projection can be reliably estimated if we are looking at a known object i e the coordinates p are known the inverse distance ηz is now mostly decoupled from the estimates of and can be estimated from the amount of foreshortening as the object rotates furthermore as the lens becomes longer i e the projection model becomes orthographic there is no need to replace a perspective imaging model with an orthographic one since the same equation can be used with ηz as opposed to f and tz both going to infinity this allows us to form a natural link between orthographic reconstruction techniques such as factorization and their projective perspective counterparts section lens distortions the above imaging models all assume that cameras obey a linear projection model where straight lines in the world result in straight lines in the image this follows as a natural consequence of linear matrix operations being applied to homogeneous coordinates unfor tunately many wide angle lenses have noticeable radial distortion which manifests itself as a visible curvature in the projection of straight lines see section for a more detailed discussion of lens optics including chromatic aberration unless this distortion is taken into account it becomes impossible to create highly accurate photorealistic reconstructions for example image mosaics constructed without taking radial distortion into account will often exhibit blurring due to the mis registration of corresponding features before pixel blending chapter fortunately compensating for radial distortion is not that difficult in practice for most lenses a simple quartic model of distortion can produce good results let xc yc be the pixel coordinates obtained after perspective division but before scaling by focal length f and shifting by the optical center cx cy i e x rx p tx rz p tz y ry p ty rz p tz the radial distortion model says that coordinates in the observed images are displaced away barrel distortion or towards pincushion distortion the image center by an amount propor tional to their radial distance figure b the simplest radial distortion models use low order polynomials e g xˆc xc c c yˆc yc c c anamorphic lenses which are widely used in feature film production do not follow this radial distortion model instead they can be thought of to a first approximation as inducing different vertical and horizontal scalings i e non square pixels a b c figure radial lens distortions a barrel b pincushion and c fisheye the fisheye image spans almost from side to side where and and are called the radial distortion parameters after the c c c radial distortion step the final pixel coordinates can be computed using xs f xic cx ys f yci cy a variety of techniques can be used to estimate the radial distortion parameters for a given lens as discussed in section sometimes the above simplified model does not model the true distortions produced by complex lenses accurately enough especially at very wide angles a more complete ana lytic model also includes tangential distortions and decentering distortions slama but these distortions are not covered in this book fisheye lenses figure require a model that differs from traditional polynomial models of radial distortion fisheye lenses behave to a first approximation as equi distance projectors of angles away from the optical axis xiong and turkowski which is the same as the polar projection described by equations xiong and turkowski describe how this model can be extended with the addition of an extra quadratic cor rection in φ and how the unknown parameters center of projection scaling factor etc can be estimated from a set of overlapping fisheye images using a direct intensity based non linear minimization algorithm for even larger less regular distortions a parametric distortion model using splines may be necessary goshtasby if the lens does not have a single center of projection it sometimes the relationship between xc and xˆc is expressed the other way around i e xc xˆc this is convenient if we map image pixels into warped rays by dividing through by f we can then undistort the rays and have true rays in space may become necessary to model the line as opposed to direction corresponding to each pixel separately gremban thorpe and kanade champleboux lavalle e sautot et al grossberg and nayar sturm and ramalingam tardif sturm trudeau et al some of these techniques are described in more detail in section which discusses how to calibrate lens distortions there is one subtle issue associated with the simple radial distortion model that is often glossed over we have introduced a non linearity between the perspective projection and final sensor array projection steps therefore we cannot in general post multiply an arbitrary matrix k with a rotation to put it into upper triangular form and absorb this into the global rotation however this situation is not as bad as it may at first appear for many applications keeping the simplified diagonal form of is still an adequate model furthermore if we correct radial and other distortions to an accuracy where straight lines are preserved we have essentially converted the sensor back into a linear imager and the previous decomposition still applies photometric image formation in modeling the image formation process we have described how geometric features in the world are projected into features in an image however images are not composed of features instead they are made up of discrete color or intensity values where do these values come from how do they relate to the lighting in the environment surface properties and geometry camera optics and sensor properties figure in this section we develop a set of models to describe these interactions and formulate a generative process of image formation a more detailed treatment of these topics can be found in other textbooks on computer graphics and image synthesis glassner weyrich lawrence lensch et al foley van dam feiner et al watt cohen and wallace sillion and puech lighting images cannot exist without light to produce an image the scene must be illuminated with one or more light sources certain modalities such as fluorescent microscopy and x ray tomography do not fit this model but we do not deal with them in this book light sources can generally be divided into point and area light sources a point light source originates at a single location in space e g a small light bulb potentially at infinity e g the sun note that for some applications such as modeling soft shadows penumbras the sun may have to be treated as an area light source in addition to its location a point light source has an intensity and a color spectrum i e a distribution over figure a simplified model of photometric image formation light is emitted by one or more light sources and is then reflected from an object surface a portion of this light is directed towards the camera this simplified model ignores multiple reflections which often occur in real world scenes wavelengths l λ the intensity of a light source falls off with the square of the distance between the source and the object being lit because the same light is being spread over a larger spherical area a light source may also have a directional falloff dependence but we ignore this in our simplified model area light sources are more complicated a simple area light source such as a fluorescent ceiling light fixture with a diffuser can be modeled as a finite rectangular area emitting light equally in all directions cohen and wallace sillion and puech glassner when the distribution is strongly directional a four dimensional lightfield can be used instead ashdown a more complex light distribution that approximates say the incident illumination on an object sitting in an outdoor courtyard can often be represented using an environment map greene originally called a reflection map blinn and newell this representa tion maps incident light directions vˆ to color values or wavelengths λ l vˆ λ and is equivalent to assuming that all light sources are at infinity environment maps can be represented as a collection of cubical faces greene as a single longitude latitude map blinn and newell or as the image of a reflecting sphere watt a convenient way to get a rough model of a real world environment map is to take an image of a reflective mirrored sphere and to unwrap this image onto the desired environment map debevec watt gives a nice discussion of environment mapping including the formulas needed to map directions to pixels for the three most commonly used representations a b figure a light scatters when it hits a surface b the bidirectional reflectance distribution function brdf f θi φi θr φr is parameterized by the angles that the inci dent vˆi and reflected vˆr light ray directions make with the local surface coordinate frame dˆx dˆy nˆ reflectance and shading when light hits an object surface it is scattered and reflected figure many different models have been developed to describe this interaction in this section we first describe the most general form the bidirectional reflectance distribution function and then look at some more specialized models including the diffuse specular and phong shading models we also discuss how these models can be used to compute the global illumination corresponding to a scene the bidirectional reflectance distribution function brdf the most general model of light scattering is the bidirectional reflectance distribution func tion brdf relative to some local coordinate frame on the surface the brdf is a four dimensional function that describes how much of each wavelength arriving at an incident direction vˆi is emitted in a reflected direction vˆr figure the function can be written in terms of the angles of the incident and reflected directions relative to the surface frame as fr θi φi θr φr λ the brdf is reciprocal i e because of the physics of light transport you can interchange the roles of vˆi and vˆr and still get the same answer this is sometimes called helmholtz reciprocity actually even more general models of light transport exist including some that model spatial variation along the surface sub surface scattering and atmospheric effects see section dorsey rushmeier and sillion weyrich lawrence lensch et al most surfaces are isotropic i e there are no preferred directions on the surface as far as light transport is concerned the exceptions are anisotropic surfaces such as brushed scratched aluminum where the reflectance depends on the light orientation relative to the direction of the scratches for an isotropic material we can simplify the brdf to fr θi θr φr φi λ or fr vˆi vˆr nˆ λ since the quantities θi θr and φr φi can be computed from the directions vˆi vˆr and nˆ to calculate the amount of light exiting a surface point p in a direction vˆr under a given lighting condition we integrate the product of the incoming light li vˆi λ with the brdf some authors call this step a convolution taking into account the foreshortening factor cos θi we obtain where lr vˆr λ r li vˆi λ fr vˆi vˆr nˆ λ cos θi dvˆi cos θi max cos θi if the light sources are discrete a finite number of point light sources we can replace the integral with a summation lr vˆr λ li λ fr vˆi vˆr nˆ λ cos θi i brdfs for a given surface can be obtained through physical modeling torrance and sparrow cook and torrance glassner heuristic modeling phong or through empirical observation ward westin arvo and torrance dana van gin neken nayar et al dorsey rushmeier and sillion weyrich lawrence lensch et al typical brdfs can often be split into their diffuse and specular components as described below diffuse reflection the diffuse component also known as lambertian or matte reflection scatters light uni formly in all directions and is the phenomenon we most normally associate with shading e g the smooth non shiny variation of intensity with surface normal that is seen when ob serving a statue figure diffuse reflection also often imparts a strong body color to the light since it is caused by selective absorption and re emission of light inside the object material shafer glassner see http cs columbia edu cave software curet for a database of some empirically sampled brdfs figure this close up of a statue shows both diffuse smooth shading and specular shiny highlight reflection as well as darkening in the grooves and creases due to reduced light visibility and interreflections photo courtesy of the caltech vision lab http www vision caltech edu archive html while light is scattered uniformly in all directions i e the brdf is constant fd vˆi vˆr nˆ λ fd λ the amount of light depends on the angle between the incident light direction and the surface normal θi this is because the surface area exposed to a given amount of light becomes larger at oblique angles becoming completely self shadowed as the outgoing surface normal points away from the light figure think about how you orient yourself towards the sun or fireplace to get maximum warmth and how a flashlight projected obliquely against a wall is less bright than one pointing directly at it the shading equation for diffuse reflection can thus be written as ld vˆr λ li λ fd λ cos θi li λ fd λ vˆi nˆ where specular reflection i i vˆi nˆ max vˆi nˆ the second major component of a typical brdf is specular gloss or highlight reflection which depends strongly on the direction of the outgoing light consider light reflecting off a mirrored surface figure incident light rays are reflected in a direction that is rotated by around the surface normal nˆ using the same notation as in equations v i n v i n si v i n v i n a b figure a the diminution of returned light caused by foreshortening depends on vˆi nˆ the cosine of the angle between the incident light direction vˆi and the surface normal nˆ b mirror specular reflection the incident light ray direction vˆi is reflected onto the specular direction sˆi around the surface normal nˆ we can compute the specular reflection direction sˆi as sˆi v1 v i vi the amount of light reflected in a given direction vˆr thus depends on the angle θs cos vˆr sˆi between the view direction vˆr and the specular direction sˆi for example the phong model uses a power of the cosine of the angle fs θs λ ks λ coske θs while the torrance and sparrow micro facet model uses a gaussian fs θs λ ks λ exp larger exponents ke or inverse gaussian widths cs correspond to more specular surfaces with distinct highlights while smaller exponents better model materials with softer gloss phong shading phong combined the diffuse and specular components of reflection with another term which he called the ambient illumination this term accounts for the fact that objects are generally illuminated not only by point light sources but also by a general diffuse illumination corresponding to inter reflection e g the walls in a room or distant sources such as the ambient diffuse exp exp exp 70 a b figure cross section through a phong shading model brdf for a fixed incident illu mination direction a component values as a function of angle away from surface normal b polar plot the value of the phong exponent ke is indicated by the exp labels and the light source is at an angle of away from the normal blue sky in the phong model the ambient term does not depend on surface orientation but depends on the color of both the ambient illumination la λ and the object ka λ fa λ ka λ la λ putting all of these terms together we arrive at the phong shading model lr vˆr λ ka λ la λ kd λ li λ vˆi nˆ ks λ li λ vˆr sˆi ke figure shows a typical set of phong shading model components as a function of the angle away from the surface normal in a plane containing both the lighting direction and the viewer typically the ambient and diffuse reflection color distributions ka λ and kd λ are the same since they are both due to sub surface scattering body reflection inside the surface material shafer the specular reflection distribution ks λ is often uniform white since it is caused by interface reflections that do not change the light color the exception to this are metallic materials such as copper as opposed to the more common dielectric materials such as plastics the ambient illumination la λ often has a different color cast from the direct light sources li λ e g it may be blue for a sunny outdoor scene or yellow for an interior lit with candles or incandescent lights the presence of ambient sky illumination in shadowed areas is what often causes shadows to appear bluer than the corresponding lit portions of a scene note also that the diffuse component of the phong model or of any shading model depends on the angle of the incoming light source vˆi while the specular component depends on the relative angle between the viewer vr and the specular reflection direction sˆi which itself depends on the incoming light direction vˆi and the surface normal nˆ the phong shading model has been superseded in terms of physical accuracy by a number of more recently developed models in computer graphics including the model developed by cook and torrance based on the original micro facet model of torrance and sparrow until recently most computer graphics hardware implemented the phong model but the recent advent of programmable pixel shaders makes the use of more complex models feasible di chromatic reflection model the torrance and sparrow model of reflection also forms the basis of shafer di chromatic reflection model which states that the apparent color of a uniform material lit from a single source depends on the sum of two terms lr vˆr λ li vˆr vˆi nˆ λ lb vˆr vˆi nˆ λ ci λ mi vˆr vˆi nˆ cb λ mb vˆr vˆi nˆ i e the radiance of the light reflected at the interface li and the radiance reflected at the sur face body lb each of these in turn is a simple product between a relative power spectrum c λ which depends only on wavelength and a magnitude m vˆr vˆi nˆ which depends only on geometry this model can easily be derived from a generalized version of phong model by assuming a single light source and no ambient illumination and re arranging terms the di chromatic model has been successfully used in computer vision to segment specular col ored objects with large variations in shading klinker and more recently has inspired local two color models for applications such bayer pattern demosaicing bennett uytten daele zitnick et al global illumination ray tracing and radiosity the simple shading model presented thus far assumes that light rays leave the light sources bounce off surfaces visible to the camera thereby changing in intensity or color and arrive at the camera in reality light sources can be shadowed by occluders and rays can bounce multiple times around a scene while making their trip from a light source to the camera two methods have traditionally been used to model such effects if the scene is mostly specular the classic example being scenes made of glass objects and mirrored or highly pol ished balls the preferred approach is ray tracing or path tracing glassner akenine mo ller and haines shirley which follows individual rays from the camera across multiple bounces towards the light sources or vice versa if the scene is composed mostly of uniform albedo simple geometry illuminators and surfaces radiosity global illumination techniques are preferred cohen and wallace sillion and puech glassner combinations of the two techniques have also been developed wallace cohen and green berg as well as more general light transport techniques for simulating effects such as the caustics cast by rippling water the basic ray tracing algorithm associates a light ray with each pixel in the camera im age and finds its intersection with the nearest surface a primary contribution can then be computed using the simple shading equations presented previously e g equation for all light sources that are visible for that surface element an alternative technique for computing which surfaces are illuminated by a light source is to compute a shadow map or shadow buffer i e a rendering of the scene from the light source perspective and then compare the depth of pixels being rendered with the map williams akenine mo ller and haines additional secondary rays can then be cast along the specular direction towards other objects in the scene keeping track of any attenuation or color change that the specular reflection induces radiosity works by associating lightness values with rectangular surface areas in the scene including area light sources the amount of light interchanged between any two mutually visible areas in the scene can be captured as a form factor which depends on their relative orientation and surface reflectance properties as well as the fall off as light is distributed over a larger effective sphere the further away it is cohen and wallace sillion and puech glassner a large linear system can then be set up to solve for the final lightness of each area patch using the light sources as the forcing function right hand side once the system has been solved the scene can be rendered from any desired point of view under certain circumstances it is possible to recover the global illumination in a scene from photographs using computer vision techniques yu debevec malik et al the basic radiosity algorithm does not take into account certain near field effects such as the darkening inside corners and scratches or the limited ambient illumination caused by partial shadowing from other surfaces such effects have been exploited in a number of computer vision algorithms nayar ikeuchi and kanade langer and zucker while all of these global illumination effects can have a strong effect on the appearance of a scene and hence its interpretation they are not covered in more detail in this book but see section for a discussion of recovering brdfs from real scenes and objects optics once the light from a scene reaches the camera it must still pass through the lens before reaching the sensor analog film or digital silicon for many applications it suffices to treat the lens as an ideal pinhole that simply projects all rays through a common center of projection figures and however if we want to deal with issues such as focus exposure vignetting and aber figure a thin lens of focal length f focuses the light from a plane a distance zo in front of the lens at a distance zi behind the lens where if the focal plane vertical i gray line next to c is moved forward the images are no longer in focus and the circle of confusion c small thick line segments depends on the distance of the image plane motion zi relative to the lens aperture diameter d the field of view f o v depends on the ratio between the sensor width w and the focal length f or more precisely the focusing distance zi which is usually quite close to f ration we need to develop a more sophisticated model which is where the study of optics comes in mo ller hecht ray figure shows a diagram of the most basic lens model i e the thin lens composed of a single piece of glass with very low equal curvature on both sides according to the lens law which can be derived using simple geometric arguments on light ray refraction the relationship between the distance to an object zo and the distance behind the lens at which a focused image is formed zi can be expressed as zo zi f where f is called the focal length of the lens if we let zo i e we adjust the lens move the image plane so that objects at infinity are in focus we get zi f which is why we can think of a lens of focal length f as being equivalent to a first approximation to a pinhole a distance f from the focal plane figure whose field of view is given by if the focal plane is moved away from its proper in focus setting of zi e g by twisting the focus ring on the lens objects at zo are no longer in focus as shown by the gray plane in figure the amount of mis focus is measured by the circle of confusion c shown as short thick blue line segments on the gray plane the equation for the circle of confusion can be derived using similar triangles it depends on the distance of travel in the focal plane zi relative to the original focus distance zi and the diameter of the aperture d see exercise if the aperture is not completely circular e g if it is caused by a hexagonal diaphragm it is sometimes possible to see this effect in the actual blur function levin fergus durand et al joshi szeliski and kriegman or in the glints that are seen when shooting into the sun a b figure regular and zoom lens depth of field indicators the allowable depth variation in the scene that limits the circle of confusion to an accept able number is commonly called the depth of field and is a function of both the focus distance and the aperture as shown diagrammatically by many lens markings figure since this depth of field depends on the aperture diameter d we also have to know how this varies with the commonly displayed f number which is usually denoted as f or n and is defined as f f n d where the focal length f and the aperture diameter d are measured in the same unit say millimeters the usual way to write the f number is to replace the in f with the actual number i e f f f f alternatively we can say n etc an easy way to interpret these numbers is to notice that dividing the focal length by the f number gives us the diameter d so these are just formulas for the aperture diameter notice that the usual progression for f numbers is in full stops which are multiples of since this corresponds to doubling the area of the entrance pupil each time a smaller f number is selected this doubling is also called changing the exposure by one exposure value or ev it has the same effect on the amount of light reaching the sensor as doubling the exposure duration e g from to see exercise now that you know how to convert between f numbers and aperture diameters you can construct your own plots for the depth of field as a function of focal length f circle of confusion c and focus distance zo as explained in exercise and see how well these match what you observe on actual lenses such as those shown in figure of course real lenses are not infinitely thin and therefore suffer from geometric aber rations unless compound elements are used to correct for them the classic five seidel aberrations which arise when using third order optics include spherical aberration coma astigmatism curvature of field and distortion mo ller hecht ray this also explains why with zoom lenses the f number varies with the current zoom focal length setting figure in a lens subject to chromatic aberration light at different wavelengths e g the red and blur arrows is focused with a different focal length f i and hence a different depth zii resulting in both a geometric in plane displacement and a loss of focus chromatic aberration because the index of refraction for glass varies slightly as a function of wavelength sim ple lenses suffer from chromatic aberration which is the tendency for light of different colors to focus at slightly different distances and hence also with slightly different mag nification factors as shown in figure the wavelength dependent magnification fac tor i e the transverse chromatic aberration can be modeled as a per color radial distortion section and hence calibrated using the techniques described in section the wavelength dependent blur caused by longitudinal chromatic aberration can be calibrated using techniques described in section unfortunately the blur induced by longitudinal aberration can be harder to undo as higher frequencies can get strongly attenuated and hence hard to recover in order to reduce chromatic and other kinds of aberrations most photographic lenses today are compound lenses made of different glass elements with different coatings such lenses can no longer be modeled as having a single nodal point p through which all of the rays must pass when approximating the lens with a pinhole model instead these lenses have both a front nodal point through which the rays enter the lens and a rear nodal point through which they leave on their way to the sensor in practice only the location of the front nodal point is of interest when performing careful camera calibration e g when determining the point around which to rotate to capture a parallax free panorama see section not all lenses however can be modeled as having a single nodal point in particular very wide angle lenses such as fisheye lenses section and certain catadioptric imaging systems consisting of lenses and curved mirrors baker and nayar do not have a single point through which all of the acquired light rays pass in such cases it is preferable to explicitly construct a mapping function look up table between pixel coordinates and rays in space gremban thorpe and kanade champleboux lavalle e sautot et al figure the amount of light hitting a pixel of surface area δi depends on the square of the ratio of the aperture diameter d to the focal length f as well as the fourth power of the off axis angle α cosine α grossberg and nayar sturm and ramalingam tardif sturm trudeau et al as mentioned in section vignetting another property of real world lenses is vignetting which is the tendency for the brightness of the image to fall off towards the edge of the image two kinds of phenomena usually contribute to this effect ray the first is called natural vignetting and is due to the foreshortening in the object surface projected pixel and lens aperture as shown in figure consider the light leaving the object surface patch of size δo located at an off axis angle α because this patch is foreshortened with respect to the camera lens the amount of light reaching the lens is reduced by a factor cos α the amount of light reaching the lens is also subject to the usual fall off in this case the distance ro zo cos α the actual area of the aperture through which the light passes is foreshortened by an additional factor cos α i e the aperture as seen from point o is an ellipse of dimensions d d cos α putting all of these factors together we see that the amount of light leaving o and passing through the aperture on its way to the image pixel located at i is proportional to δo cos α π o d cos α δo π α since triangles op q and ip j are similar the projected areas of of the object surface δo and image pixel δi are in the same squared ratio as zo zi δo z δi putting these together we obtain the final relationship between the amount of light reaching pixel i and the aperture diameter d the focusing distance zi f and the off axis angle α π δo z2 π i α δi α which is called the fundamental radiometric relation between the scene radiance l and the light irradiance e reaching the pixel sensor π d horn nalwa hecht ray notice in this equation how the amount of light depends on the pixel surface area which is why the smaller sensors in point and shoot cameras are so much noisier than digital single lens reflex slr cameras the inverse square of the f stop n f d and the fourth power of the α off axis fall off which is the natural vignetting term the other major kind of vignetting called mechanical vignetting is caused by the internal occlusion of rays near the periphery of lens elements in a compound lens and cannot easily be described mathematically without performing a full ray tracing of the actual lens design however unlike natural vignetting mechanical vignetting can be decreased by reducing the camera aperture increasing the f number it can also be calibrated along with natural vi gnetting using special devices such as integrating spheres uniformly illuminated targets or camera rotation as discussed in section the digital camera after starting from one or more light sources reflecting off one or more surfaces in the world and passing through the camera optics lenses light finally reaches the imaging sensor how are the photons arriving at this sensor converted into the digital r g b values that we observe when we look at a digital image in this section we develop a simple model that accounts for the most important effects such as exposure gain and shutter speed non linear mappings sampling and aliasing and noise figure which is based on camera models developed by healey and kondepudy tsin ramesh and kanade liu szeliski kang et al shows a simple version of the processing stages that occur in modern digital cameras chakrabarti scharstein and zickler developed a sophisti cated parameter model that is an even better match to the processing performed in today cameras there are some empirical models that work well in practice kang and weiss zheng lin and kang figure image sensing pipeline showing the various sources of noise as well as typical digital post processing steps light falling on an imaging sensor is usually picked up by an active sensing area inte grated for the duration of the exposure usually expressed as the shutter speed in a fraction of a second e g and then passed to a set of sense amplifiers the two main kinds of sensor used in digital still and video cameras today are charge coupled device ccd and complementary metal oxide on silicon cmos in a ccd photons are accumulated in each active well during the exposure time then in a transfer phase the charges are transferred from well to well in a kind of bucket brigade until they are deposited at the sense amplifiers which amplify the signal and pass it to an analog to digital converter adc older ccd sensors were prone to blooming when charges from one over exposed pixel spilled into adjacent ones but most newer ccds have anti blooming technology troughs into which the excess charge can spill in cmos the photons hitting the sensor directly affect the conductivity or gain of a photodetector which can be selectively gated to control exposure duration and locally am plified before being read out using a multiplexing scheme traditionally ccd sensors outperformed cmos in quality sensitive applications such as digital slrs while cmos was better for low power applications but today cmos is used in most digital cameras the main factors affecting the performance of a digital image sensor are the shutter speed sampling pitch fill factor chip size analog gain sensor noise and the resolution and quality in digital still cameras a complete frame is captured and then read out sequentially at once however if video is being captured a rolling shutter which exposes and transfers each line separately is often used in older video cameras the even fields lines were scanned first followed by the odd fields in a process that is called interlacing of the analog to digital converter many of the actual values for these parameters can be read from the exif tags embedded with digital images while others can be obtained from the camera manufacturers specification sheets or from camera review or calibration web sites shutter speed the shutter speed exposure time directly controls the amount of light reaching the sensor and hence determines if images are under or over exposed for bright scenes where a large aperture or slow shutter speed are desired to get a shallow depth of field or motion blur neutral density filters are sometimes used by photographers for dynamic scenes the shutter speed also determines the amount of motion blur in the resulting picture usually a higher shutter speed less motion blur makes subsequent analysis easier see sec tion for techniques to remove such blur however when video is being captured for display some motion blur may be desirable to avoid stroboscopic effects sampling pitch the sampling pitch is the physical spacing between adjacent sensor cells on the imaging chip a sensor with a smaller sampling pitch has a higher sampling density and hence provides a higher resolution in terms of pixels for a given active chip area however a smaller pitch also means that each sensor has a smaller area and cannot accumulate as many photons this makes it not as light sensitive and more prone to noise fill factor the fill factor is the active sensing area size as a fraction of the theoretically available sensing area the product of the horizontal and vertical sampling pitches higher fill factors are usually preferable as they result in more light capture and less aliasing see section however this must be balanced with the need to place additional electronics between the active sense areas the fill factor of a camera can be determined empirically using a photometric camera calibration process see section chip size video and point and shoot cameras have traditionally used small chip areas inch to inch while digital slr cameras try to come closer to the traditional size of a film frame when overall device size is not important having a larger chip size is preferable since each sensor cell can be more photo sensitive for compact cameras a smaller chip means that all of the optics can be shrunk down proportionately however http www clarkvision com imagedetail digital sensor performance summary these numbers refer to the tube diameter of the old vidicon tubes used in video cameras http www dpreview com learn glossary camera system sensor sizes htm the sensor on the canon cam era actually measures 29mm i e a sixth of the size on side of a full frame dslr sensor when a dslr chip does not fill the full frame it results in a multiplier effect on the lens focal length for example a chip that is only the dimension of a frame will make a lens image the same angular extent as a lens as demonstrated in larger chips are more expensive to produce not only because fewer chips can be packed into each wafer but also because the probability of a chip defect goes up linearly with the chip area analog gain before analog to digital conversion the sensed signal is usually boosted by a sense amplifier in video cameras the gain on these amplifiers was traditionally controlled by automatic gain control agc logic which would adjust these values to obtain a good overall exposure in newer digital still cameras the user now has some additional control over this gain through the iso setting which is typically expressed in iso standard units such as or since the automated exposure control in most cameras also adjusts the aperture and shutter speed setting the iso manually removes one degree of freedom from the camera control just as manually specifying aperture and shutter speed does in theory a higher gain allows the camera to perform better under low light conditions less motion blur due to long exposure times when the aperture is already maxed out in practice however higher iso settings usually amplify the sensor noise sensor noise throughout the whole sensing process noise is added from various sources which may include fixed pattern noise dark current noise shot noise amplifier noise and quantization noise healey and kondepudy tsin ramesh and kanade the final amount of noise present in a sampled image depends on all of these quantities as well as the incoming light controlled by the scene radiance and aperture the exposure time and the sensor gain also for low light conditions where the noise is due to low photon counts a poisson model of noise may be more appropriate than a gaussian model as discussed in more detail in section liu szeliski kang et al use this model along with an empirical database of camera response functions crfs obtained by grossberg and nayar to estimate the noise level function nlf for a given image which predicts the overall noise variance at a given pixel as a function of its brightness a separate nlf is estimated for each color channel an alternative approach when you have access to the camera before taking pictures is to pre calibrate the nlf by taking repeated shots of a scene containing a variety of colors and luminances such as the macbeth color chart shown in figure mccamy marcus and davidson when estimating the variance be sure to throw away or downweight pixels with large gradients as small shifts between exposures will affect the sensed values at such pixels unfortunately the pre calibration process may have to be repeated for different exposure times and gain settings because of the complex interactions occurring within the sensing system in practice most computer vision algorithms such as image denoising edge detection and stereo matching all benefit from at least a rudimentary estimate of the noise level barring the ability to pre calibrate the camera or to take repeated shots of the same scene the simplest approach is to look for regions of near constant value and to estimate the noise variance in such regions liu szeliski kang et al adc resolution the final step in the analog processing chain occurring within an imaging sensor is the analog to digital conversion adc while a variety of techniques can be used to implement this process the two quantities of interest are the resolution of this process how many bits it yields and its noise level how many of these bits are useful in practice for most cameras the number of bits quoted eight bits for compressed jpeg images and a nominal bits for the raw formats provided by some dslrs exceeds the actual number of usable bits the best way to tell is to simply calibrate the noise of a given sensor e g by taking repeated shots of the same scene and plotting the estimated noise as a function of brightness exercise digital post processing once the irradiance values arriving at the sensor have been con verted to digital bits most cameras perform a variety of digital signal processing dsp operations to enhance the image before compressing and storing the pixel values these in clude color filter array cfa demosaicing white point setting and mapping of the luminance values through a gamma function to increase the perceived dynamic range of the signal we cover these topics in section but before we do we return to the topic of aliasing which was mentioned in connection with sensor array fill factors sampling and aliasing what happens when a field of light impinging on the image sensor falls onto the active sense areas in the imaging chip the photons arriving at each active cell are integrated and then digitized however if the fill factor on the chip is small and the signal is not otherwise band limited visually unpleasing aliasing can occur to explore the phenomenon of aliasing let us first look at a one dimensional signal fig ure in which we have two sine waves one at a frequency of f and the other at f if we sample these two signals at a frequency of f we see that they produce the same samples shown in black and so we say that they are aliased why is this a bad effect in essence we can no longer reconstruct the original signal since we do not know which of the two original frequencies was present in fact shannon sampling theorem shows that the minimum sampling oppenheim and schafer oppenheim schafer and buck rate required to reconstruct a signal an alias is an alternate name for someone so the sampled signal corresponds to two different aliases f f figure aliasing of a one dimensional signal the blue sine wave at f and the red sine wave at f have the same digital samples when sampled at f even after convolution with a fill factor box filter the two signals while no longer of the same magnitude are still aliased in the sense that the sampled red signal looks like an inverted lower magnitude version of the blue signal the image on the right is scaled up for better visibility the actual sine magnitudes are and of their original values from its instantaneous samples must be at least twice the highest frequency fs the maximum frequency in a signal is known as the nyquist frequency and the inverse of the minimum sampling frequency rs fs is known as the nyquist rate however you may ask since an imaging chip actually averages the light field over a finite area are the results on point sampling still applicable averaging over the sensor area does tend to attenuate some of the higher frequencies however even if the fill factor is as in the right image of figure frequencies above the nyquist limit half the sampling frequency still produce an aliased signal although with a smaller magnitude than the corresponding band limited signals a more convincing argument as to why aliasing is bad can be seen by downsampling a signal using a poor quality filter such as a box square filter figure shows a high frequency chirp image so called because the frequencies increase over time along with the results of sampling it with a fill factor area sensor a fill factor sensor and a high quality tap filter additional examples of downsampling decimation filters can be found in section and figure the best way to predict the amount of aliasing that an imaging system or even an image processing algorithm will produce is to estimate the point spread function psf which represents the response of a particular pixel sensor to an ideal point light source the psf is a combination convolution of the blur induced by the optical system lens and the finite integration area of a chip sensor the actual theorem states that fs must be at least twice the signal bandwidth but since we are not dealing with modulated signals such as radio waves during image capture the maximum frequency suffices imaging chips usually interpose an optical anti aliasing filter just before the imaging chip to reduce or control the amount of aliasing a b c d figure aliasing of a two dimensional signal a original full resolution image b downsampled with a fill factor box filter c downsampled with a fill factor box filter d downsampled with a high quality tap filter notice how the higher frequencies are aliased into visible frequencies with the lower quality filters while the tap filter completely removes these higher frequencies if we know the blur function of the lens and the fill factor sensor area shape and spacing for the imaging chip plus optionally the response of the anti aliasing filter we can convolve these as described in section to obtain the psf figure shows the one dimensional cross section of a psf for a lens whose blur function is assumed to be a disc of a radius equal to the pixel spacing plus a sensing chip whose horizontal fill factor is taking the fourier transform of this psf section we obtain the modulation transfer function mtf from which we can estimate the amount of aliasing as the area of the fourier magni tude outside the f fs nyquist frequency if we de focus the lens so that the blur function has a radius of figure we see that the amount of aliasing decreases significantly but so does the amount of image detail frequencies closer to f fs under laboratory conditions the psf can be estimated to pixel precision by looking at a point light source such as a pin hole in a black piece of cardboard lit from behind however this psf the actual image of the pin hole is only accurate to a pixel resolution and while it can model larger blur such as blur caused by defocus it cannot model the sub pixel shape of the psf and predict the amount of aliasing an alternative technique described in section is to look at a calibration pattern e g one consisting of slanted step edges reichenbach park and narayanswamy williams and burns joshi szeliski and kriegman whose ideal appearance can be re synthesized to sub pixel precision in addition to occurring during image acquisition aliasing can also be introduced in var ious image processing operations such as resampling upsampling and downsampling sec tions and discuss these issues and show how careful selection of filters can reduce the complex fourier transform of the psf is actually called the optical transfer function otf williams its magnitude is called the modulation transfer function mtf and its phase is called the phase transfer function ptf a b c d figure sample point spread functions psf the diameter of the blur disc blue in a is equal to half the pixel spacing while the diameter in c is twice the pixel spacing the horizontal fill factor of the sensing chip is and is shown in brown the convolution of these two kernels gives the point spread function shown in green the fourier response of the psf the mtf is plotted in b and d the area above the nyquist frequency where aliasing occurs is shown in red the amount of aliasing that operations inject color in section we saw how lighting and surface reflections are functions of wavelength when the incoming light hits the imaging sensor light from different parts of the spectrum is somehow integrated into the discrete red green and blue rgb color values that we see in a digital image how does this process work and how can we analyze and manipulate color values you probably recall from your childhood days the magical process of mixing paint colors to obtain new ones you may recall that blue yellow makes green red blue makes purple and red green makes brown if you revisited this topic at a later age you may have learned that the proper subtractive primaries are actually cyan a light blue green magenta pink and yellow figure although black is also often used in four color printing cmyk if you ever subsequently took any painting classes you learned that colors can have even a b figure primary and secondary colors a additive colors red green and blue can be mixed to produce cyan magenta yellow and white b subtractive colors cyan magenta and yellow can be mixed to produce red green blue and black more fanciful names such as alizarin crimson cerulean blue and chartreuse the subtractive colors are called subtractive because pigments in the paint absorb certain wavelengths in the color spectrum later on you may have learned about the additive primary colors red green and blue and how they can be added with a slide projector or on a computer monitor to produce cyan magenta yellow white and all the other colors we typically see on our tv sets and monitors figure through what process is it possible for two different colors such as red and green to interact to produce a third color like yellow are the wavelengths somehow mixed up to produce a new wavelength you probably know that the correct answer has nothing to do with physically mixing wavelengths instead the existence of three primaries is a result of the tri stimulus or tri chromatic nature of the human visual system since we have three different kinds of cone each of which responds selectively to a different portion of the color spectrum glassner wyszecki and stiles fairchild reinhard ward pattanaik et al livingstone note that for machine vision applications such as remote sensing and terrain clas sification it is preferable to use many more wavelengths similarly surveillance applications can often benefit from sensing in the near infrared nir range cie rgb and xyz to test and quantify the tri chromatic theory of perception we can attempt to reproduce all monochromatic single wavelength colors as a mixture of three suitably chosen primaries see also mark fairchild web page http www cis rit edu fairchild whyiscolor books links html 480 640 a b figure standard cie color matching functions a r λ g λ b λ color spectra obtained from matching pure colors to the r g and b pri maries b x λ y λ z λ color matching functions which are linear combinations of the r λ g λ b λ spectra pure wavelength light can be obtained using either a prism or specially manufactured color filters in the the commission internationale d eclairage cie standardized the rgb representation by performing such color matching experiments using the primary colors of red wavelength green and blue figure shows the results of performing these experiments with a standard observer i e averaging perceptual results over a large number of subjects you will notice that for certain pure spectra in the blue green range a negative amount of red light has to be added i e a certain amount of red has to be added to the color being matched in order to get a color match these results also provided a simple explanation for the existence of metamers which are colors with different spectra that are perceptually indistinguishable note that two fabrics or paint colors that are metamers under one light may no longer be so under different lighting because of the problem associated with mixing negative light the cie also developed a new color space called xyz which contains all of the pure spectral colors within its positive octant it also maps the y axis to the luminance i e perceived relative brightness and maps pure white to a diagonal equal valued vector the transformation from rgb to xyz is given by x r while the official definition of the cie xyz standard has the matrix normalized so that the y value corresponding to pure red is a more commonly used form is to omit the leading figure cie chromaticity diagram showing colors and their corresponding x y val ues pure spectral colors are arranged around the outside of the curve fraction so that the second row adds up to one i e the rgb triplet maps to a y value of linearly blending the r λ g λ b λ curves in figure according to we obtain the resulting x λ y λ z λ curves shown in figure notice how all three spectra color matching functions now have only positive values and how the y λ curve matches that of the luminance perceived by humans if we divide the xyz values by the sum of x y z we obtain the chromaticity coordi nates x x y z y y x y z z z x y z which sum up to the chromaticity coordinates discard the absolute intensity of a given color sample and just represent its pure color if we sweep the monochromatic color λ pa rameter in figure from λ to λ we obtain the familiar chromaticity diagram shown in figure this figure shows the x y value for every color value per ceivable by most humans of course the cmyk reproduction process in this book does not actually span the whole gamut of perceivable colors the outer curved rim represents where all of the pure monochromatic color values map in x y space while the lower straight line which connects the two endpoints is known as the purple line a convenient representation for color values when we want to tease apart luminance and chromaticity is therefore yxy luminance plus the two most distinctive chrominance components l a b color space while the xyz color space has many convenient properties including the ability to separate luminance from chrominance it does not actually predict how well humans perceive differ ences in color or luminance because the response of the human visual system is roughly logarithmic we can perceive relative luminance differences of about the cie defined a non linear re mapping of the xyz space called l a b also sometimes called cielab where differences in luminance or chrominance are more perceptually uniform the l component of lightness is defined as l y yn where yn is the luminance value for nominal white fairchild and t is a finite slope approximation to the cube root with δ the resulting scale roughly measures equal amounts of lightness perceptibility in a similar fashion the a and b components are defined as a f x f y l and b f y f z l where again xn yn zn is the measured white point figure k show the l a b representation for a sample color image color cameras while the preceding discussion tells us how we can uniquely describe the perceived tri stimulus description of any color spectral distribution it does not tell us how rgb still and video cameras actually work do they just measure the amount of light at the nominal wavelengths of red green and blue do color monitors just emit exactly these wavelengths and if so how can they emit negative red light to reproduce colors in the cyan range in fact the design of rgb video cameras has historically been based around the availabil ity of colored phosphors that go into television sets when standard definition color television was invented ntsc a mapping was defined between the rgb values that would drive the three color guns in the cathode ray tube crt and the xyz values that unambiguously de fine perceived color this standard was called itu r bt with the advent of hdtv and newer monitors a new standard called itu r bt was created which specifies the xyz another perceptually motivated color space called l u v was developed and standardized simultaneously fairchild values of each of the color primaries x 180423 y 072169 in practice each color camera integrates light according to the spectral response function of its red green and blue sensors r r l λ sr λ dλ g r l λ sg λ dλ b r l λ sb λ dλ where l λ is the incoming spectrum of light at a given pixel and sr λ sg λ sb λ are the red green and blue spectral sensitivities of the corresponding sensors can we tell what spectral sensitivities the cameras actually have unless the camera manufacturer provides us with this data or we observe the response of the camera to a whole spectrum of monochromatic lights these sensitivities are not specified by a standard such as bt instead all that matters is that the tri stimulus values for a given color produce the specified rgb values the manufacturer is free to use sensors with sensitivities that do not match the standard xyz definitions so long as they can later be converted through a linear transform to the standard colors similarly while tv and computer monitors are supposed to produce rgb values as spec ified by equation there is no reason that they cannot use digital logic to transform the incoming rgb values into different signals to drive each of the color channels properly cal ibrated monitors make this information available to software applications that perform color management so that colors in real life on the screen and on the printer all match as closely as possible color filter arrays while early color tv cameras used three vidicons tubes to perform their sensing and later cameras used three separate rgb sensing chips most of today digital still and video cam eras cameras use a color filter array cfa where alternating sensors are covered by different colored filters a newer chip design by foveon http www foveon com stacks the red green and blue sensors beneath each other but it has not yet gained widespread adoption a b figure bayer rgb pattern a color filter array layout b interpolated pixel values with unknown guessed values shown as lower case the most commonly used pattern in color cameras today is the bayer pattern bayer which places green filters over half of the sensors in a checkerboard pattern and red and blue filters over the remaining ones figure the reason that there are twice as many green filters as red and blue is because the luminance signal is mostly determined by green values and the visual system is much more sensitive to high frequency detail in luminance than in chrominance a fact that is exploited in color image compression see section the process of interpolating the missing color values so that we have valid rgb values for all the pixels is known as demosaicing and is covered in detail in section similarly color lcd monitors typically use alternating stripes of red green and blue filters placed in front of each liquid crystal active area to simulate the experience of a full color display as before because the visual system has higher resolution acuity in luminance than chrominance it is possible to digitally pre filter rgb and monochrome images to enhance the perception of crispness betrisey blinn dresevic et al platt color balance before encoding the sensed rgb values most cameras perform some kind of color balancing operation in an attempt to move the white point of a given image closer to pure white equal rgb values if the color system and the illumination are the same the bt system uses the daylight illuminant as its reference white the change may be minimal however if the illuminant is strongly colored such as incandescent indoor lighting which generally results in a yellow or orange hue the compensation can be quite significant a simple way to perform color correction is to multiply each of the rgb values by a different factor i e to apply a diagonal matrix transform to the rgb color space more complicated transforms which are sometimes the result of mapping to xyz space and back y y visible noise y quantization y noise figure gamma compression a the relationship between the input signal luminance y and the transmitted signal y i is given by y i y γ b at the receiver the signal y i is exponentiated by the factor γ yˆ y iγ noise introduced during transmission is squashed in the dark regions which corresponds to the more noise sensitive region of the visual system actually perform a color twist i e they use a general color transform matrix exer cise has you explore some of these issues gamma in the early days of black and white television the phosphors in the crt used to display the tv signal responded non linearly to their input voltage the relationship between the voltage and the resulting brightness was characterized by a number called gamma γ since the formula was roughly b v γ with a γ of about to compensate for this effect the electronics in the tv camera would pre map the sensed luminance y through an inverse gamma y i y γ with a typical value of the mapping of the signal through this non linearity before transmission had a beneficial side effect noise added during transmission remember these were analog days would be reduced after applying the gamma at the receiver in the darker regions of the signal where it was more visible figure remember that our visual system is roughly sensitive to relative differences in luminance those of you old enough to remember the early days of color television will naturally think of the hue adjustment knob on the television set which could produce truly bizarre results a related technique called companding was the basis of the dolby noise reduction systems used with audio tapes when color television was invented it was decided to separately pass the red green and blue signals through the same gamma non linearity before combining them for encoding today even though we no longer have analog noise in our transmission systems signals are still quantized during compression see section so applying inverse gamma to sensed values is still useful unfortunately for both computer vision and computer graphics the presence of gamma in images is often problematic for example the proper simulation of radiometric phenomena such as shading see section and equation occurs in a linear radiance space once all of the computations have been performed the appropriate gamma should be applied before display unfortunately many computer graphics systems such as shading models operate directly on rgb values and display these values directly fortunately newer color imaging standards such as the bit scrgb use a linear space which makes this less of a problem glassner in computer vision the situation can be even more daunting the accurate determination of surface normals using a technique such as photometric stereo section or even a simpler operation such as accurate image deblurring require that the measurements be in a linear space of intensities therefore it is imperative when performing detailed quantitative computations such as these to first undo the gamma and the per image color re balancing in the sensed color values chakrabarti scharstein and zickler develop a sophisti cated parameter model that is a good match to the processing performed by today digital cameras they also provide a database of color images you can use for your own testing for other vision applications however such as feature detection or the matching of sig nals in stereo and motion estimation this linearization step is often not necessary in fact determining whether it is necessary to undo gamma can take some careful thinking e g in the case of compensating for exposure variations in image stitching see exercise if all of these processing steps sound confusing to model they are exercise has you try to tease apart some of these phenomena using empirical investigation i e taking pictures of color charts and comparing the raw and jpeg compressed color values other color spaces while rgb and xyz are the primary color spaces used to describe the spectral content and hence tri stimulus response of color signals a variety of other representations have been developed both in video and still image coding and in computer graphics the earliest color representation developed for video transmission was the yiq standard developed for ntsc video in north america and the closely related yuv standard developed for pal in europe in both of these cases it was desired to have a luma channel y so called http vision middlebury edu color since it only roughly mimics true luminance that would be comparable to the regular black and white tv signal along with two lower frequency chroma channels in both systems the y signal or more appropriately the y luma signal since it is gamma compressed is obtained from where r g b is the triplet of gamma compressed color components when using the newer color definitions for hdtv in bt the formula is the uv components are derived from scaled versions of bi y i and ri y i namely u bi y i and v ri y i whereas the iq components are the uv components rotated through an angle of in composite ntsc and pal video the chroma signals were then low pass filtered horizon tally before being modulated and superimposed on top of the y luma signal backward compatibility was achieved by having older black and white tv sets effectively ignore the high frequency chroma signal because of slow electronics or at worst superimposing it as a high frequency pattern on top of the main signal while these conversions were important in the early days of computer vision when frame grabbers would directly digitize the composite tv signal today all digital video and still image compression standards are based on the newer ycbcr conversion ycbcr is closely related to yuv the cb and cr signals carry the blue and red color difference signals and have more useful mnemonics than uv but uses different scale factors to fit within the eight bit range available with digital signals for video the y signal is re scaled to fit within the range of values while the cb and cr signals are scaled to fit within gomes and velho fairchild for still images the jpeg standard uses the full eight bit range with no reserved values y i 587 ri cb 331264 cr 081312 gi where the r g b values are the eight bit gamma compressed color components i e the actual rgb values we obtain when we open up or display a jpeg image for most appli cations this formula is not that important since your image reading software will directly provide you with the eight bit gamma compressed r g b values however if you are trying to do careful image deblocking exercise this information may be useful another color space you may come across is hue saturation value hsv which is a pro jection of the rgb color cube onto a non linear chroma angle a radial saturation percentage and a luminance inspired value in more detail value is defined as either the mean or maxi mum color value saturation is defined as scaled distance from the diagonal and hue is defined as the direction around a color wheel the exact formulas are described by hall foley van dam feiner et al such a decomposition is quite natural in graphics applications such as color picking it approximates the munsell chart for color description figure n shows an hsv representation of a sample color image where saturation is encoded using a gray scale saturated darker and hue is depicted as a color if you want your computer vision algorithm to only affect the value luminance of an image and not its saturation or hue a simpler solution is to use either the y xy luminance chromaticity coordinates defined in or the even simpler color ratios r r g b g g r g b b b r g b figure h after manipulating the luma e g through the process of histogram equalization section you can multiply each color ratio by the ratio of the new to old luma to obtain an adjusted rgb triplet while all of these color systems may sound confusing in the end it often may not mat ter that much which one you use poynton in his color faq http www poynton com colorfaq html notes that the perceptually motivated l a b system is qualitatively similar to the gamma compressed r g b system we mostly deal with since both have a fractional power scaling which approximates a logarithmic response between the actual intensity val ues and the numbers being manipulated as in all cases think carefully about what you are trying to accomplish before deciding on a technique to use compression the last stage in a camera processing pipeline is usually some form of image compression unless you are using a lossless compression scheme such as camera raw or png all color video and image compression algorithms start by converting the signal into ycbcr or some closely related variant so that they can compress the luminance signal with higher fidelity than the chrominance signal recall that the human visual system has poorer if you are at a loss for questions at a conference you can always ask why the speaker did not use a perceptual color space such as l a b conversely if they did use l a b you can ask if they have any concrete evidence that this works better than regular colors a rgb b r c g d b e rgb f r g g h b i l j a k b l h m s n v figure color space transformations a d rgb e h rgb i k l a b l n hsv note that the rgb l a b and hsv values are all re scaled to fit the dynamic range of the printed page frequency response to color than to luminance changes in video it is common to subsam ple cb and cr by a factor of two horizontally with still images jpeg the subsampling averaging occurs both horizontally and vertically once the luminance and chrominance images have been appropriately subsampled and separated into individual images they are then passed to a block transform stage the most common technique used here is the discrete cosine transform dct which is a real valued variant of the discrete fourier transform dft see section the dct is a reasonable approximation to the karhunen loe ve or eigenvalue decomposition of natural image patches i e the decomposition that simultaneously packs the most energy into the first coefficients and diagonalizes the joint covariance matrix among the pixels makes transform coefficients figure image compressed with jpeg at three quality settings note how the amount of block artifact and high frequency aliasing mosquito noise increases from left to right statistically independent both mpeg and jpeg use dct transforms wallace le gall although newer variants use smaller blocks or alternative transformations such as wavelets taubman and marcellin and lapped transforms malvar are now used after transform coding the coefficient values are quantized into a set of small integer values that can be coded using a variable bit length scheme such as a huffman code or an arithmetic code wallace the dc lowest frequency coefficients are also adaptively predicted from the previous block dc values the term dc comes from direct current i e the non sinusoidal or non alternating part of a signal the step size in the quantization is the main variable controlled by the quality setting on the jpeg file figure with video it is also usual to perform block based motion compensation i e to encode the difference between each block and a predicted set of pixel values obtained from a shifted block in the previous frame the exception is the motion jpeg scheme used in older dv camcorders which is nothing more than a series of individually jpeg compressed image frames while basic mpeg uses motion compensation blocks with integer motion values le gall newer standards use adaptively sized block sub pixel motions and the ability to reference blocks from older frames in order to recover more gracefully from failures and to allow for random access to the video stream predicted p frames are interleaved among independently coded i frames bi directional b frames are also sometimes used the quality of a compression algorithm is usually reported using its peak signal to noise ratio psnr which is derived from the average mean square error m se i x iˆ x where i x is the original uncompressed image and iˆ x is its compressed counterpart or equivalently the root mean square error rms error which is defined as rm s m se additional reading the psnr is defined as imax p sn r max log10 m se rm s where imax is the maximum signal extent e g for eight bit images while this is just a high level sketch of how image compression works it is useful to understand so that the artifacts introduced by such techniques can be compensated for in various computer vision applications additional reading as we mentioned at the beginning of this chapter it provides but a brief summary of a very rich and deep set of topics traditionally covered in a number of separate fields a more thorough introduction to the geometry of points lines planes and projections can be found in textbooks on multi view geometry hartley and zisserman faugeras and luong and computer graphics foley van dam feiner et al watt opengl arb topics covered in more depth include higher order primitives such as quadrics conics and cubics as well as three view and multi view geometry the image formation synthesis process is traditionally taught as part of a computer graphics curriculum foley van dam feiner et al glassner watt shirley but it is also studied in physics based computer vision wolff shafer and healey the behavior of camera lens systems is studied in optics mo ller hecht ray some good books on color theory have been written by healey and shafer wyszecki and stiles fairchild with livingstone providing a more fun and infor mal introduction to the topic of color perception mark fairchild page of color books and lists many other sources topics relating to sampling and aliasing are covered in textbooks on signal and image processing crane ja hne oppenheim and schafer oppenheim schafer and buck pratt russ burger and burge gonzales and woods exercises a note to students this chapter is relatively light on exercises since it contains mostly background material and not that many usable techniques if you really want to understand http www cis rit edu fairchild whyiscolor books links html multi view geometry in a thorough way i encourage you to read and do the exercises provided by hartley and zisserman similarly if you want some exercises related to the image formation process glassner book is full of challenging problems ex least squares intersection point and line fitting advanced equation shows how the intersection of two lines can be expressed as their cross product assuming the lines are expressed as homogeneous coordinates if you are given more than two lines and want to find a point x that minimizes the sum of squared distances to each line d x li i how can you compute this quantity hint write the dot product as x t li and turn the squared quantity into a quadratic form x t ax to fit a line to a bunch of points you can compute the centroid mean of the points as well as the covariance matrix of the points around this mean show that the line passing through the centroid along the major axis of the covariance ellipsoid largest eigenvector minimizes the sum of squared distances to the points these two approaches are fundamentally different even though projective duality tells us that points and lines are interchangeable why are these two algorithms so appar ently different are they actually minimizing different objectives ex transform editor write a program that lets you interactively create a set of rectangles and then modify their pose transform you should implement the following steps open an empty window canvas shift drag rubber band to create a new rectangle select the deformation mode motion model translation rigid similarity affine or perspective drag any corner of the outline to change its transformation this exercise should be built on a set of pixel coordinate and transformation classes either implemented by yourself or from a software library persistence of the created representation save and load should also be supported for each rectangle save its transformation ex viewer write a simple viewer for points lines and polygons import a set of point and line commands primitives as well as a viewing transform interactively modify the object or camera transform this viewer can be an extension of the one you created in exercise simply replace the viewing transformations with their equivalents optional add a z buffer to do hidden surface removal for polygons optional use a drawing package and just write the viewer control ex focus distance and depth of field figure out how the focus distance and depth of field indicators on a lens are determined compute and plot the focus distance zo as a function of the distance traveled from the focal length zi f zi for a lens of focal length f say does this explain the hyperbolic progression of focus distances you see on a typical lens figure compute the depth of field minimum and maximum focus distances for a given focus setting zo as a function of the circle of confusion diameter c make it a fraction of the sensor width the focal length f and the f stop number n which relates to the aperture diameter d does this explain the usual depth of field markings on a lens that bracket the in focus marker as in figure now consider a zoom lens with a varying focal length f assume that as you zoom the lens stays in focus i e the distance from the rear nodal point to the sensor plane zi adjusts itself automatically for a fixed focus distance zo how do the depth of field indicators vary as a function of focal length can you reproduce a two dimensional plot that mimics the curved depth of field lines seen on the lens in figure ex f numbers and shutter speeds list the common f numbers and shutter speeds that your camera provides on older model slrs they are visible on the lens and shut ter speed dials on newer cameras you have to look at the electronic viewfinder or lcd screen indicator as you manually adjust exposures do these form geometric progressions if so what are the ratios how do these relate to exposure values evs if your camera has shutter speeds of and do you think that these two speeds are exactly a factor of two apart or a factor of apart how accurate do you think these numbers are can you devise some way to measure exactly how the aperture affects how much light reaches the sensor and what the exact exposure times actually are ex noise level calibration estimate the amount of noise in your camera by taking re peated shots of a scene with the camera mounted on a tripod purchasing a remote shutter release is a good investment if you own a dslr alternatively take a scene with constant color regions such as a color checker chart and estimate the variance by fitting a smooth function to each color region and then taking differences from the predicted function plot your estimated variance as a function of level for each of your color channels separately change the iso setting on your camera if you cannot do that reduce the overall light in your scene turn off lights draw the curtains wait until dusk does the amount of noise vary a lot with iso gain compare your camera to another one at a different price point or year of make is there evidence to suggest that you get what you pay for does the quality of digital cameras seem to be improving over time ex gamma correction in image stitching here a relatively simple puzzle assume you are given two images that are part of a panorama that you want to stitch see chapter the two images were taken with different exposures so you want to adjust the rgb values so that they match along the seam line is it necessary to undo the gamma in the color values in order to achieve this ex skin color detection devise a simple skin color detector forsyth and fleck jones and rehg vezhnevets sazonov and andreeva kakumanu makrogiannis and bourbakis based on chromaticity or other color properties take a variety of photographs of people and calculate the xy chromaticity values for each pixel crop the photos or otherwise indicate with a painting tool which pixels are likely to be skin e g face and arms calculate a color chromaticity distribution for these pixels you can use something as simple as a mean and covariance measure or as complicated as a mean shift segmenta tion algorithm see section you can optionally use non skin pixels to model the background distribution use your computed distribution to find the skin regions in an image one easy way to visualize this is to paint all non skin pixels a given color such as white or black how sensitive is your algorithm to color balance scene lighting does a simpler chromaticity measurement such as a color ratio work just as well ex white point balancing tricky a common in camera or post processing tech nique for performing white point adjustment is to take a picture of a white piece of paper and to adjust the rgb values of an image to make this a neutral color describe how you would adjust the rgb values in an image given a sample white color of rw gw bw to make this color neutral without changing the exposure too much does your transformation involve a simple per channel scaling of the rgb values or do you need a full color twist matrix or something else convert your rgb values to xyz does the appropriate correction now only depend on the xy or xy values if so when you convert back to rgb space do you need a full color twist matrix to achieve the same effect if you used pure diagonal scaling in the direct rgb mode but end up with a twist if you work in xyz space how do you explain this apparent dichotomy which approach is correct or is it possible that neither approach is actually correct if you want to find out what your camera actually does continue on to the next exercise ex in camera color processing challenging if your camera supports a raw pixel mode take a pair of raw and jpeg images and see if you can infer what the camera is doing when it converts the raw pixel values to the final color corrected and gamma compressed eight bit jpeg pixel values deduce the pattern in your color filter array from the correspondence between co located raw and color mapped pixel values use a color checker chart at this stage if it makes your life easier you may find it helpful to split the raw image into four separate images subsampling even and odd columns and rows and to treat each of these new images as a virtual sensor evaluate the quality of the demosaicing algorithm by taking pictures of challenging scenes which contain strong color edges such as those shown in in section if you can take the same exact picture after changing the color balance values in your camera compare how these settings affect this processing compare your results against those presented by chakrabarti scharstein and zickler or use the data available in their database of color images http vision middlebury edu color chapter image processing point operators pixel transforms color transforms compositing and matting histogram equalization application tonal adjustment linear filtering separable filtering examples of linear filtering band pass and steerable filters more neighborhood operators non linear filtering morphology distance transforms connected components fourier transforms fourier transform pairs two dimensional fourier transforms wiener filtering application sharpening blur and noise removal pyramids and wavelets interpolation decimation multi resolution representations wavelets application image blending geometric transformations parametric transformations mesh based warping application feature based morphing global optimization regularization markov random fields application image restoration additional reading exercises a b c d e f figure some common image processing operations a original image b increased contrast c change in hue d posterized quantized colors e blurred f rotated now that we have seen how images are formed through the interaction of scene elements lighting and camera optics and sensors let us look at the first stage in most computer vision applications namely the use of image processing to preprocess the image and convert it into a form suitable for further analysis examples of such operations include exposure correction and color balancing the reduction of image noise increasing sharpness or straightening the image by rotating it figure while some may consider image processing to be outside the purview of computer vision most computer vision applications such as computational photography and even recognition require care in designing the image processing stages in order to achieve acceptable results in this chapter we review standard image processing operators that map pixel values from one image to another image processing is often taught in electrical engineering departments as a follow on course to an introductory course in signal processing oppenheim and schafer oppenheim schafer and buck there are several popular textbooks for image processing crane gomes and velho ja hne pratt russ burger and burge gonzales and woods we begin this chapter with the simplest kind of image transforms namely those that manipulate each pixel independently of its neighbors section such transforms are of ten called point operators or point processes next we examine neighborhood area based operators where each new pixel value depends on a small number of neighboring input values sections and a convenient tool to analyze and sometimes accelerate such neighborhood operations is the fourier transform which we cover in section neighbor hood operators can be cascaded to form image pyramids and wavelets which are useful for analyzing images at a variety of resolutions scales and for accelerating certain operations section another important class of global operators are geometric transformations such as rotations shears and perspective deformations section finally we introduce global optimization approaches to image processing which involve the minimization of an energy functional or equivalently optimal estimation using bayesian markov random field models section point operators the simplest kinds of image processing transforms are point operators where each output pixel value depends on only the corresponding input pixel value plus potentially some globally collected information or parameters examples of such operators include brightness and contrast adjustments figure as well as color correction and transformations in the image processing literature such operations are also known as point processes crane we begin this section with a quick review of simple point operators such as brightness a b c d e f figure some local image processing operations a original image along with its three color per channel histograms b brightness increased additive offset b c contrast increased multiplicative gain a d gamma partially linearized γ e full histogram equalization f partial histogram equalization range a b c d figure visualizing image data a original image b cropped portion and scanline plot using an image inspection tool c grid of numbers d surface plot for figures c d the image was first converted to grayscale scaling and image addition next we discuss how colors in images can be manipulated we then present image compositing and matting operations which play an important role in computational photography chapter and computer graphics applications finally we describe the more global process of histogram equalization we close with an example appli cation that manipulates tonal values exposure and contrast to improve image appearance pixel transforms a general image processing operator is a function that takes one or more input images and produces an output image in the continuous domain this can be denoted as g x h f x or g x h x fn x where x is in the d dimensional domain of the functions usually d for images and the functions f and g operate over some range which can either be scalar or vector valued e g for color images or motion for discrete sampled images the domain consists of a finite number of pixel locations x i j and we can write g i j h f i j figure shows how an image can be represented either by its color appearance as a grid of numbers or as a two dimensional function surface plot two commonly used point processes are multiplication and addition with a constant g x af x b the parameters a and b are often called the gain and bias parameters sometimes these parameters are said to control contrast and brightness respectively figures c the an image luminance characteristics can also be summarized by its key average luminanance and range kopf uyttendaele deussen et al bias and gain parameters can also be spatially varying g x a x f x b x e g when simulating the graded density filter used by photographers to selectively darken the sky or when modeling vignetting in an optical system multiplicative gain both global and spatially varying is a linear operation since it obeys the superposition principle h h h we will have more to say about linear shift invariant operators in section operators such as image squaring which is often used to get a local estimate of the energy in a band pass filtered signal see section are not linear another commonly used dyadic two input operator is the linear blend operator g x α x x by varying α from this operator can be used to perform a temporal cross dissolve between two images or videos as seen in slide shows and film production or as a component of image morphing algorithms section one highly used non linear transform that is often applied to images before further pro cessing is gamma correction which is used to remove the non linear mapping between input radiance and quantized pixel values section to invert the gamma mapping applied by the sensor we can use g x f x γ where a gamma value of γ is a reasonable fit for most digital cameras color transforms while color images can be treated as arbitrary vector valued functions or collections of inde pendent bands it usually makes sense to think about them as highly correlated signals with strong connections to the image formation process section sensor design section and human perception section consider for example brightening a picture by adding a constant value to all three channels as shown in figure can you tell if this achieves the desired effect of making the image look brighter can you see any undesirable side effects or artifacts in fact adding the same value to each color channel not only increases the apparent in tensity of each pixel it can also affect the pixel hue and saturation how can we define and manipulate such quantities in order to achieve the desired perceptual effects a b c d figure image matting and compositing chuang curless salesin et al qc ieee a source image b extracted foreground object f c alpha matte α shown in grayscale d new composite c as discussed in section chromaticity coordinates or even simpler color ra tios can first be computed and then used after manipulating e g brightening the luminance y to re compute a valid rgb image with the same hue and saturation figure i shows some color ratio images multiplied by the middle gray value for better visual ization similarly color balancing e g to compensate for incandescent lighting can be per formed either by multiplying each channel with a different scale factor or by the more com plex process of mapping to xyz color space changing the nominal white point and mapping back to rgb which can be written down using a linear color twist transform matrix exercises and have you explore some of these issues another fun project best attempted after you have mastered the rest of the material in this chapter is to take a picture with a rainbow in it and enhance the strength of the rainbow exercise compositing and matting in many photo editing and visual effects applications it is often desirable to cut a foreground object out of one scene and put it on top of a different background figure the process of extracting the object from the original image is often called matting smith and blinn while the process of inserting it into another image without visible artifacts is called compositing porter and duff blinn the intermediate representation used for the foreground object between these two stages is called an alpha matted color image figure c in addition to the three color rgb channels an alpha matted image contains a fourth alpha channel α or a that describes the relative amount of opacity or fractional coverage at each pixel figures and the opacity is the opposite of the transparency pixels within the object are fully opaque α while pixels fully outside the object are transparent α pixels on the boundary of the object vary smoothly between these two extremes which hides the perceptual visible jaggies b α αf c a b c d figure compositing equation c α b αf the images are taken from a close up of the region of the hair in the upper right part of the lion in figure that occur if only binary opacities are used to composite a new or foreground image on top of an old background image the over operator first proposed by porter and duff and then studied extensively by blinn is used c α b αf this operator attenuates the influence of the background image b by a factor α and then adds in the color and opacity values corresponding to the foreground layer f as shown in figure in many situations it is convenient to represent the foreground colors in pre multiplied form i e to store and manipulate the αf values directly as blinn shows the pre multiplied rgba representation is preferred for several reasons including the ability to blur or resample e g rotate alpha matted images without any additional complications just treating each rgba band independently however when matting using local color consistency ruzon and tomasi chuang curless salesin et al the pure un multiplied foreground colors f are used since these remain constant or vary slowly in the vicinity of the object edge the over operation is not the only kind of compositing operation that can be used porter and duff describe a number of additional operations that can be useful in photo editing and visual effects applications in this book we concern ourselves with only one additional commonly occurring case but see exercise when light reflects off clean transparent glass the light passing through the glass and the light reflecting off the glass are simply added together figure this model is use ful in the analysis of transparent motion black and anandan szeliski avidan and anandan which occurs when such scenes are observed from a moving camera see section the actual process of matting i e recovering the foreground background and alpha matte values from one or more images has a rich history which we study in section figure an example of light reflecting off the transparent glass of a picture frame black and anandan qc elsevier you can clearly see the woman portrait inside the picture frame superimposed with the reflection of a man face off the glass smith and blinn have a nice survey of traditional blue screen matting techniques while toyama krumm brumitt et al review difference matting more recently there has been a lot of activity in computational photography relating to natural image matting ruzon and tomasi chuang curless salesin et al wang and cohen which attempts to extract the mattes from a single natural image figure or from ex tended video sequences chuang agarwala curless et al all of these techniques are described in more detail in section histogram equalization while the brightness and gain controls described in section can improve the appearance of an image how can we automatically determine their best values one approach might be to look at the darkest and brightest pixel values in an image and map them to pure black and pure white another approach might be to find the average value in the image push it towards middle gray and expand the range so that it more closely fills the displayable values kopf uyttendaele deussen et al how can we visualize the set of lightness values in an image in order to test some of these heuristics the answer is to plot the histogram of the individual color channels and luminance values as shown in figure from this distribution we can compute relevant statistics such as the minimum maximum and average intensity values notice that the image in figure has both an excess of dark values and light values but that the mid range values are largely under populated would it not be better if we could simultaneously brighten some the histogram is simply the count of the number of pixels at each gray level value for an eight bit image an accumulation table with entries is needed for higher bit depths a table with the appropriate number of entries probably fewer than the full number of gray levels should be used 1000 250000 50000 a b c 100 150 d e f figure histogram analysis and equalization a original image b color channel and in tensity luminance histograms c cumulative distribution functions d equalization trans fer functions e full histogram equalization f partial histogram equalization dark values and darken some light values while still using the full extent of the available dynamic range can you think of a mapping that might do this one popular answer to this question is to perform histogram equalization i e to find an intensity mapping function f i such that the resulting histogram is flat the trick to finding such a mapping is the same one that people use to generate random samples from a probability density function which is to first compute the cumulative distribution function shown in figure think of the original histogram h i as the distribution of grades in a class after some exam how can we map a particular grade to its corresponding percentile so that students at the percentile range scored better than of their classmates the answer is to integrate the distribution h i to obtain the cumulative distribution c i c i h i c i h i where n is the number of pixels in the image or students in the class for any given grade or intensity we can look up its corresponding percentile c i and determine the final value that pixel should take when working with eight bit pixel values the i and c axes are rescaled from a b c figure locally adaptive histogram equalization a original image b block histogram equalization c full locally adaptive equalization figure shows the result of applying f i c i to the original image as we can see the resulting histogram is flat so is the resulting image it is flat in the sense of a lack of contrast and being muddy looking one way to compensate for this is to only partially compensate for the histogram unevenness e g by using a mapping function f i αc i α i which is a linear blend between the cumulative distribution function and the identity transform a straight line as you can see in figure the resulting image maintains more of its original grayscale distribution while having a more appealing balance another potential problem with histogram equalization or in general image brightening is that noise in dark regions can be amplified and become more visible exercise suggests some possible ways to mitigate this as well as alternative techniques to maintain contrast and punch in the original images larson rushmeier and piatko stark locally adaptive histogram equalization while global histogram equalization can be useful for some images it might be preferable to apply different kinds of equalization in different regions consider for example the image in figure which has a wide range of luminance values instead of computing a single curve what if we were to subdivide the image into m m pixel blocks and perform separate histogram equalization in each sub block as you can see in figure the resulting image exhibits a lot of blocking artifacts i e intensity discontinuities at block boundaries one way to eliminate blocking artifacts is to use a moving window i e to recompute the histogram for every m m block centered at each pixel this process can be quite slow m operations per pixel although with clever programming only the histogram entries corresponding to the pixels entering and leaving the block in a raster scan across the image need to be updated m operations per pixel note that this operation is an example of the non linear neighborhood operations we study in more detail in section a more efficient approach is to compute non overlapped block based equalization func tions as before but to then smoothly interpolate the transfer functions as we move between t t a b figure local histogram interpolation using relative t coordinates a block based histograms with block centers shown as circles b corner based spline histograms pixels are located on grid intersections the black square pixel transfer function is interpolated from the four adjacent lookup tables gray arrows using the computed t values block boundaries are shown as dashed lines blocks this technique is known as adaptive histogram equalization ahe and its contrast limited gain limited version is known as clahe pizer amburn austin et al the weighting function for a given pixel i j can be computed as a function of its horizontal and vertical position t within a block as shown in figure to blend the four lookup functions a bilinear blending function fs t i t i t i i i can be used see section for higher order generalizations of such spline functions note that instead of blending the four lookup tables for each output pixel which would be quite slow we can instead blend the results of mapping a given pixel through the four neigh boring lookups a variant on this algorithm is to place the lookup tables at the corners of each m m block see figure and exercise in addition to blending four lookups to compute the final value we can also distribute each input pixel into four adjacent lookup tables during the histogram accumulation phase notice that the gray arrows in figure point both ways i e hk l i i j w i j k l where w i j k l is the bilinear weighting function between pixel i j and lookup table k l this is an example of soft histogramming which is used in a variety of other applica algorithm is implemented in the matlab adapthist function tions including the construction of sift feature descriptors section and vocabulary trees section application tonal adjustment one of the most widely used applications of point wise image processing operators is the manipulation of contrast or tone in photographs to make them look either more attractive or more interpretable you can get a good sense of the range of operations possible by opening up any photo manipulation tool and trying out a variety of contrast brightness and color manipulation options as shown in figures and exercises and have you implement some of these operations in order to become familiar with basic image processing operators more sophisticated techniques for tonal adjustment reinhard ward pattanaik et al bae paris and durand are described in the section on high dynamic range tone mapping section linear filtering locally adaptive histogram equalization is an example of a neighborhood operator or local operator which uses a collection of pixel values in the vicinity of a given pixel to deter mine its final output value figure in addition to performing local tone adjustment neighborhood operators can be used to filter images in order to add soft blur sharpen de tails accentuate edges or remove noise figure d in this section we look at linear filtering operators which involve weighted combinations of pixels in small neighborhoods in section we look at non linear operators such as morphological filters and distance transforms the most commonly used type of neighborhood operator is a linear filter in which an output pixel value is determined as a weighted sum of input pixel values figure g i j f i k j l h k l k l the entries in the weight kernel or mask h k l are often called the filter coefficients the above correlation operator can be more compactly notated as g f h a common variant on this formula is g i j f i k j l h k l f k l h i k j l f x y h x y g x y figure neighborhood filtering convolution the image on the left is convolved with the filter in the middle to yield the image on the right the light blue pixels indicate the source neighborhood for the light green destination pixel where the sign of the offsets in f has been reversed this is called the convolution operator g f h and h is then called the impulse response function the reason for this name is that the kernel function h convolved with an impulse signal δ i j an image that is everywhere except at the origin reproduces itself h δ h whereas correlation produces the reflected signal try this yourself to verify that it is so in fact equation can be interpreted as the superposition addition of shifted im pulse response functions h i k j l multiplied by the input pixel values f k l convolu tion has additional nice properties e g it is both commutative and associative as well the fourier transform of two convolved images is the product of their individual fourier trans forms section both correlation and convolution are linear shift invariant lsi operators which obey both the superposition principle h h h and the shift invariance principle g i j f i k j l h g i j h f i k j l which means that shifting a signal commutes with applying the operator stands for the lsi operator another way to think of shift invariance is that the operator behaves the same everywhere the continuous version of convolution can be written as g x f x u h u du a b c d e f g h figure some neighborhood operations a original image b blurred c sharpened d smoothed with edge preserving filter e binary image f dilated g distance transform h connected components for the dilation and connected components black ink pixels are assumed to be active i e to have a value of in equations figure one dimensional signal convolution as a sparse matrix vector multiply g hf occasionally a shift variant version of correlation or convolution may be used e g g i j f i k j l h k l i j k l where h k l i j is the convolution kernel at pixel i j for example such a spatially varying kernel can be used to model blur in an image due to variable depth dependent defocus correlation and convolution can both be written as a matrix vector multiply if we first convert the two dimensional images f i j and g i j into raster ordered vectors f and g g hf where the sparse h matrix contains the convolution kernels figure shows how a one dimensional convolution can be represented in matrix vector form padding border effects the astute reader will notice that the matrix multiply shown in figure suffers from boundary effects i e the results of filtering the image in this form will lead to a darkening of the corner pixels this is because the original image is effectively being padded with values wherever the convolution kernel extends beyond the original image boundaries to compensate for this a number of alternative padding or extension modes have been developed figure zero set all pixels outside the source image to a good choice for alpha matted cutout images constant border color set all pixels outside the source image to a specified border value clamp replicate or clamp to edge repeat edge pixels indefinitely cyclic wrap repeat or tile loop around the image in a toroidal configuration zero wrap clamp mirror blurred zero normalized zero blurred clamp blurred mirror figure border padding top row and the results of blurring the padded image bottom row the normalized zero image is the result of dividing normalizing the blurred zero padded rgba image by its corresponding soft alpha value mirror reflect pixels across the image edge extend extend the signal by subtracting the mirrored version of the signal from the edge pixel value in the computer graphics literature akenine mo ller and haines p these mech anisms are known as the wrapping mode opengl or texture addressing mode the formulas for each of these modes are left to the reader exercise figure shows the effects of padding an image with each of the above mechanisms and then blurring the resulting padded image as you can see zero padding darkens the edges clamp replication padding propagates border values inward mirror reflection padding pre serves colors near the borders extension padding not shown keeps the border pixels fixed during blur an alternative to padding is to blur the zero padded rgba image and to then divide the resulting image by its alpha value to remove the darkening effect the results can be quite good as seen in the normalized zero image in figure separable filtering the process of performing a convolution requires multiply add operations per pixel where k is the size width or height of the convolution kernel e g the box filter in fig k a box k b bilinear c gaussian d sobel e corner figure separable linear filters for each image a e we show the filter kernel top the corresponding horizontal kernel middle and the filtered image bottom the filtered sobel and corner images are signed scaled up by and respectively and added to a gray offset before display ure in many cases this operation can be significantly sped up by first performing a one dimensional horizontal convolution followed by a one dimensional vertical convolution which requires a total of operations per pixel a convolution kernel for which this is possible is said to be separable it is easy to show that the two dimensional kernel k corresponding to successive con volution with a horizontal kernel h and a vertical kernel v is the outer product of the two kernels k vht see figure for some examples because of the increased efficiency the design of convolution kernels for computer vision applications is often influenced by their separability how can we tell if a given kernel k is indeed separable this can often be done by inspection or by looking at the analytic form of the kernel freeman and adelson a more direct method is to treat the kernel as a matrix k and to take its singular value decomposition svd k σiuivt i see appendix a for the definition of the svd if only the first singular value is non zero the kernel is separable and and provide the vertical and horizontal kernels perona for example the laplacian of gaussian kernel and can be implemented as the sum of two separable filters wiejak buxton and buxton what if your kernel is not separable and yet you still want a faster way to implement it perona who first made the link between kernel separability and svd suggests using more terms in the series i e summing up a number of separable convolutions whether this is worth doing or not depends on the relative sizes of k and the number of sig nificant singular values as well as other considerations such as cache coherency and memory locality examples of linear filtering now that we have described the process for performing linear filtering let us examine a number of frequently used filters the simplest filter to implement is the moving average or box filter which simply averages the pixel values in a k k window this is equivalent to convolving the image with a kernel of all ones and then scaling figure for large kernels a more efficient implementation is to slide a moving window across each scanline in a separable filter while adding the newest pixel and subtracting the oldest pixel from the running sum this is related to the concept of summed area tables which we describe shortly a smoother image can be obtained by separably convolving the image with a piecewise linear tent function also known as a bartlett filter figure shows a version of this filter which is called the bilinear kernel since it is the outer product of two linear first order splines see section convolving the linear tent function with itself yields the cubic approximating spline which is called the gaussian kernel figure in burt and adelson lapla cian pyramid representation section note that approximate gaussian kernels can also be obtained by iterated convolution with box filters wells in applications where the filters really need to be rotationally symmetric carefully tuned versions of sampled gaussians should be used freeman and adelson exercise the kernels we just discussed are all examples of blurring smoothing or low pass ker nels since they pass through the lower frequencies while attenuating higher frequencies how good are they at doing this in section we use frequency space fourier analysis to examine the exact frequency response of these filters we also introduce the sinc sin x x filter which performs ideal low pass filtering in practice smoothing kernels are often used to reduce high frequency noise we have much more to say about using variants on smoothing to remove noise later see sections and surprisingly smoothing kernels can also be used to sharpen images using a process called unsharp masking since blurring the image reduces high frequencies adding some of the difference between the original and the blurred image makes it sharper gsharp f γ f hblur f in fact before the advent of digital photography this was the standard way to sharpen images in the darkroom create a blurred positive negative from the original negative by mis focusing then overlay the two negatives before printing the final image which corresponds to gunsharp f γhblur f this is no longer a linear filter but it still works well linear filtering can also be used as a pre processing stage to edge extraction section and interest point detection section algorithms figure shows a simple edge extractor called the sobel operator which is a separable combination of a horizontal central difference so called because the horizontal derivative is centered on the pixel and a vertical tent filter to smooth the results as you can see in the image below the kernel this filter effectively emphasizes horizontal edges the simple corner detector figure looks for simultaneous horizontal and vertical second derivatives as you can see however it responds not only to the corners of the square but also along diagonal edges better corner detectors or at least interest point detectors that are more rotationally invariant are described in section band pass and steerable filters the sobel and corner operators are simple examples of band pass and oriented filters more sophisticated kernels can be created by first smoothing the image with a unit area gaussian filter g x y σ e and then taking the first or second derivatives marr witkin freeman and adelson such filters are known collectively as band pass filters since they filter out both low and high frequencies the undirected second derivative of a two dimensional image f is known as the laplacian operator blurring an image with a gaussian and then taking its laplacian is equivalent to convolving directly with the laplacian of gaussian log filter y2 a b c figure second order steerable filter freeman qc ieee a original image of einstein b orientation map computed from the second order oriented energy c original image with oriented structures enhanced which has certain nice scale space properties witkin witkin terzopoulos and kass the five point laplacian is just a compact approximation to this more sophisticated filter likewise the sobel operator is a simple approximation to a directional or oriented filter which can obtained by smoothing with a gaussian or some other filter and then taking a directional derivative uˆ which is obtained by taking the dot product between the gradient field and a unit direction uˆ cos θ sin θ uˆ g f uˆ g f uˆg f the smoothed directional derivative filter g g guˆ ugx vgy u x v y where uˆ u v is an example of a steerable filter since the value of an image convolved with guˆ can be computed by first convolving with the pair of filters gx gy and then steering the filter potentially locally by multiplying this gradient field with a unit vector uˆ freeman and adelson the advantage of this approach is that a whole family of filters can be evaluated with very little cost how about steering a directional second derivative filter uˆ uˆguˆ which is the result of taking a smoothed directional derivative and then taking the directional derivative again for example gxx is the second directional derivative in the x direction at first glance it would appear that the steering trick will not work since for every di rection uˆ we need to compute a different first directional derivative somewhat surprisingly freeman and adelson showed that for directional gaussian derivatives it is possible a b c d figure fourth order steerable filter freeman and adelson qc ieee a test image containing bars lines and step edges at different orientations b average oriented energy c dominant orientation d oriented energy as a function of angle polar plot to steer any order of derivative with a relatively small number of basis functions for example only three basis functions are required for the second order directional derivative guˆuˆ furthermore each of the basis filters while not itself necessarily separable can be computed using a linear combination of a small number of separable filters freeman and adelson this remarkable result makes it possible to construct directional derivative filters of in creasingly greater directional selectivity i e filters that only respond to edges that have strong local consistency in orientation figure furthermore higher order steerable filters can respond to potentially more than a single edge orientation at a given location and they can respond to both bar edges thin lines and the classic step edges figure in order to do this however full hilbert transform pairs need to be used for second order and higher filters as described in freeman and adelson steerable filters are often used to construct both feature descriptors section and edge detectors section while the filters developed by freeman and adelson are best suited for detecting linear edge like structures more recent work by koethe shows how a combined boundary tensor can be used to encode both edge and junction corner features exercise has you implement such steerable filters and apply them to finding both edge and corner features summed area table integral image if an image is going to be repeatedly convolved with different box filters and especially filters of different sizes at different locations you can precompute the summed area table crow a s b c s figure summed area tables a original image b summed area table c computation of area sum each value in the summed area table i j red is computed recursively from its three adjacent blue neighbors area sums s green are computed by combining the four values at the rectangle corners purple positive values are shown in bold and negative values in italics which is just the running sum of all the pixel values from the origin i j i j f k l k l this can be efficiently computed using a recursive raster scan algorithm i j i j i j i j f i j the image i j is also often called an integral image see figure and can actually be computed using only two additions per pixel if separate row sums are used viola and jones to find the summed area integral inside a rectangle we simply combine four samples from the summed area table s j1 j1 j1 i j a potential disadvantage of summed area tables is that they require log m log n extra bits in the accumulation image compared to the original image where m and n are the image width and height extensions of summed area tables can also be used to approximate other convolution kernels wolberg section contains a review in computer vision summed area tables have been used in face detection viola and jones to compute simple multi scale low level features such features which consist of adjacent rectangles of positive and negative values are also known as boxlets simard bottou haffner et al in principle summed area tables could also be used to compute the sums in the sum of squared differences ssd stereo and motion algorithms section in practice separable moving average filters are usually preferred kanade yoshida oda et al unless many different window shapes and sizes are being considered veksler recursive filtering the incremental formula for the summed area is an example of a recursive filter i e one whose values depends on previous filter outputs in the signal processing literature such filters are known as infinite impulse response iir since the output of the filter to an impulse single non zero value goes on forever for example for a summed area table an impulse generates an infinite rectangle of below and to the right of the impulse the filters we have previously studied in this chapter which involve the image with a finite extent kernel are known as finite impulse response fir two dimensional iir filters and recursive formulas are sometimes used to compute quan tities that involve large area interactions such as two dimensional distance functions sec tion and connected components section more commonly however iir filters are used inside one dimensional separable filtering stages to compute large extent smoothing kernels such as efficient approximations to gaus sians and edge filters deriche nielsen florack and deriche pyramid based algorithms section can also be used to perform such large area smoothing computations more neighborhood operators as we have just seen linear filters can perform a wide variety of image transformations however non linear filters such as edge preserving median or bilateral filters can sometimes perform even better other examples of neighborhood operators include morphological oper ators that operate on binary images as well as semi global operators that compute distance transforms and find connected components in binary images figure h non linear filtering the filters we have looked at so far have all been linear i e their response to a sum of two signals is the same as the sum of the individual responses this is equivalent to saying that each output pixel is a weighted summation of some number of input pixels linear filters are easier to compose and are amenable to frequency response analysis section in many cases however better performance can be obtained by using a non linear com bination of neighboring pixels consider for example the image in figure where the a b c d e f g h figure median and bilateral filtering a original image with gaussian noise b gaus sian filtered c median filtered d bilaterally filtered e original image with shot noise f gaussian filtered g median filtered h bilaterally filtered note that the bilateral filter fails to remove the shot noise because the noisy pixels are too different from their neighbors a median b α mean c domain filter d range filter figure median and bilateral filtering a median pixel green b selected α trimmed mean pixels c domain filter numbers along edge are pixel distances d range filter noise rather than being gaussian is shot noise i e it occasionally has very large values in this case regular blurring with a gaussian filter fails to remove the noisy pixels and instead turns them into softer but still visible spots figure median filtering a better filter to use in this case is the median filter which selects the median value from each pixel neighborhood figure median values can be computed in expected linear time using a randomized select algorithm cormen and incremental variants have also been developed by tomasi and manduchi and bovik section since the shot noise value usually lies well outside the true values in the neighborhood the median filter is able to filter away such bad pixels figure one downside of the median filter in addition to its moderate computational cost is that since it selects only one input pixel value to replace each output pixel it is not as efficient at averaging away regular gaussian noise huber hampel ronchetti rousseeuw et al stewart a better choice may be the α trimmed mean lee and redner crane p which averages together all of the pixels except for the α fraction that are the smallest and the largest figure another possibility is to compute a weighted median in which each pixel is used a num ber of times depending on its distance from the center this turns out to be equivalent to minimizing the weighted objective function w k l f i k j l g i j p k l where g i j is the desired output value and p for the weighted median the value p is the usual weighted mean which is equivalent to correlation after normalizing by the sum of the weights bovik section haralick and shapiro section the weighted mean also has deep connections to other methods in robust statistics see ap pendix b such as influence functions huber hampel ronchetti rousseeuw et al non linear smoothing has another perhaps even more important property especially since shot noise is rare in today cameras such filtering is more edge preserving i e it has less tendency to soften edges while filtering away high frequency noise consider the noisy image in figure in order to remove most of the noise the gaussian filter is forced to smooth away high frequency detail which is most noticeable near strong edges median filtering does better but as mentioned before does not do as good a job at smoothing away from discontinuities see tomasi and manduchi for some additional references to edge preserving smoothing techniques while we could try to use the α trimmed mean or weighted median these techniques still have a tendency to round sharp corners since the majority of pixels in the smoothing area come from the background distribution bilateral filtering what if we were to combine the idea of a weighted filter kernel with a better version of outlier rejection what if instead of rejecting a fixed percentage α we simply reject in a soft way pixels whose values differ too much from the central pixel value this is the essential idea in bilateral filtering which was first popularized in the computer vision community by tomasi and manduchi chen paris and durand and paris kornprobst tumblin et al cite similar earlier work aurich and weule smith and brady as well as the wealth of subsequent applications in computer vision and computational photography in the bilateral filter the output pixel value depends on a weighted combination of neigh boring pixel values i j k l f k l w i j k l the weighting coefficient w i j k l depends on the product of a domain kernel figure d i j k l exp i k j l and a data dependent range kernel figure r i j k l exp i j f k l when multiplied together these yield the data dependent bilateral weight function w i j k l exp i k j l 2σ2 i j f k l figure shows an example of the bilateral filtering of a noisy step edge note how the do main kernel is the usual gaussian the range kernel measures appearance intensity similarity to the center pixel and the bilateral filter kernel is a product of these two notice that the range filter uses the vector distance between the center and the neighboring pixel this is important in color images since an edge in any one of the color bands signals a change in material and hence the need to downweight a pixel influence tomasi and manduchi show that using the vector distance as opposed to filtering each color band separately reduces color fringing effects they also recommend taking the color difference in the more perceptually uniform cielab color space see section a b c d e f figure bilateral filtering durand and dorsey qc acm a noisy step edge input b domain filter gaussian c range filter similarity to center pixel value d bilateral filter e filtered step edge output f distance between pixels since bilateral filtering is quite slow compared to regular separable filtering a number of acceleration techniques have been developed durand and dorsey paris and durand chen paris and durand paris kornprobst tumblin et al unfortunately these techniques tend to use more memory than regular filtering and are hence not directly applicable to filtering full color images iterated adaptive smoothing and anisotropic diffusion bilateral and other filters can also be applied in an iterative fashion especially if an appear ance more like a cartoon is desired tomasi and manduchi when iterated filtering is applied a much smaller neighborhood can often be used consider for example using only the four nearest neighbors i e restricting k i l j in observe that d i j k l exp i k j l 2σ2 k i l j λ e k i l j we can thus re write as f t i j η f t k l r i j k l t i j η k l f t i j η ηr r i j k l r i j k l f t k l f t i j k l where r k l r i j k l k l are the neighbors of i j and we have made the iterative nature of the filtering explicit as barash notes is the same as the discrete anisotropic diffusion equation first proposed by perona and malik since its original introduction anisotropic dif fusion has been extended and applied to a wide range of problems nielsen florack and de riche black sapiro marimont et al weickert ter haar romeny and viergever weickert it has also been shown to be closely related to other adaptive smooth ing techniques saint marc chen and medioni barash barash and comaniciu as well as bayesian regularization with a non linear smoothness term that can be de rived from image statistics scharr black and haussecker in its general form the range kernel r i j k l r i j f k l which is usually called the gain or edge stopping function or diffusion coefficient can be any monotonically increasing function with ri x as x black sapiro marimont et al show how anisotropic diffusion is equivalent to minimizing a robust penalty function on the image gradients which we discuss in sections and scharr black and haussecker show how the edge stopping function can be derived in a principled manner from local image statistics they also extend the diffusion neighborhood from to which allows them to create a diffusion operator that is both rotationally invariant and incorporates information about the eigenvalues of the local structure tensor note that without a bias term towards the original image anisotropic diffusion and itera tive adaptive smoothing converge to a constant image unless a small number of iterations is used e g for speed it is usually preferable to formulate the smoothing problem as a joint minimization of a smoothness term and a data fidelity term as discussed in sections and and by scharr black and haussecker which introduce such a bias in a principled manner morphology while non linear filters are often used to enhance grayscale and color images they are also used extensively to process binary images such images often occur after a thresholding the ηr factor is not present in anisotropic diffusion but becomes negligible as η a b c d e f figure binary image morphology a original image b dilation c erosion d majority e opening f closing the structuring element for all examples is a square the effects of majority are a subtle rounding of sharp corners opening fails to eliminate the dot since it is not wide enough operation if f t else e g converting a scanned grayscale document into a binary image for further processing such as optical character recognition the most common binary image operations are called morphological operations since they change the shape of the underlying binary objects ritter and wilson chapter to perform such an operation we first convolve the binary image with a binary structuring element and then select a binary output value depending on the thresholded result of the convolution this is not the usual way in which these operations are described but i find it a nice simple way to unify the processes the structuring element can be any shape from a simple box filter to more complicated disc structures it can even correspond to a particular shape that is being sought for in the image figure shows a close up of the convolution of a binary image f with a struc turing element and the resulting images for the operations described below let c f be the integer valued count of the number of inside each structuring element as it is scanned over the image and s be the size of the structuring element number of pixels the standard operations used in binary morphology include dilation dilate f θ c erosion erode f θ c s majority maj f θ c s opening open f dilate erode f closing close f erode dilate f as we can see from figure dilation grows thickens objects consisting of while erosion shrinks thins them the opening and closing operations tend to leave large regions and smooth boundaries unaffected while removing small objects or holes and smoothing boundaries while we will not use mathematical morphology much in the rest of this book it is a handy tool to have around whenever you need to clean up some thresholded images you can find additional details on morphology in other textbooks on computer vision and image processing haralick and shapiro section bovik section ritter and wilson section as well as articles and books specifically on this topic serra serra and vincent yuille vincent and geiger soille distance transforms the distance transform is useful in quickly precomputing the distance to a curve or set of points using a two pass raster algorithm rosenfeld and pfaltz danielsson borge fors paglieroni breu gil kirkpatrick et al felzenszwalb and huttenlocher fabbri costa torelli et al it has many applications including level sets sec tion fast chamfer matching binary image alignment huttenlocher klanderman and rucklidge feathering in image stitching and blending section and nearest point alignment section the distance transform d i j of a binary image b i j is defined as follows let d k l be some distance metric between pixel offsets two commonly used metrics include the city block or manhattan distance and the euclidean distance k l k l k l the distance transform is then defined as d i j min k l b k l d i k j l i e it is the distance to the nearest background pixel whose value is the city block distance transform can be efficiently computed using a forward and backward pass of a simple raster scan algorithm as shown in figure during the forward pass each non zero pixel in b is replaced by the minimum of the distance of its north or west neighbor during the backward pass the same occurs except that the minimum is both over the current value d and the distance of the south and east neighbors figure a b c d figure city block distance transform a original binary image b top to bottom forward raster sweep green values are used to compute the orange value c bottom to top backward raster sweep green values are merged with old orange value d final distance transform efficiently computing the euclidean distance transform is more complicated here just keeping the minimum scalar distance to the boundary during the two passes is not sufficient instead a vector valued distance consisting of both the x and y coordinates of the distance to the boundary must be kept and compared using the squared distance hypotenuse rule as well larger search regions need to be used to obtain reasonable results rather than explaining the algorithm danielsson borgefors in more detail we leave it as an exercise for the motivated reader exercise figure shows a distance transform computed from a binary image notice how the values grow away from the black ink regions and form ridges in the white area of the original image because of this linear growth from the starting boundary pixels the distance transform is also sometimes known as the grassfire transform since it describes the time at which a fire starting inside the black region would consume any given pixel or a chamfer because it resembles similar shapes used in woodworking and industrial design the ridges in the distance transform become the skeleton or medial axis transform mat of the region where the transform is computed and consist of pixels that are of equal distance to two or more boundaries tek and kimia sebastian and kimia a useful extension of the basic distance transform is the signed distance transform which computes distances to boundary pixels for all the pixels lavalle e and szeliski the simplest way to create this is to compute the distance transforms for both the original bi nary image and its complement and to negate one of them before combining because such distance fields tend to be smooth it is possible to store them more compactly with mini mal loss in relative accuracy using a spline defined over a quadtree or octree data structure lavalle e and szeliski szeliski and lavalle e frisken perry rockwood et al such precomputed signed distance transforms can be extremely useful in efficiently aligning and merging curves and surfaces huttenlocher klanderman and rucklidge a b c figure connected component computation a original grayscale image b horizontal runs nodes connected by vertical graph edges dashed blue runs are pseudocolored with unique colors inherited from parent nodes c re coloring after merging adjacent segments szeliski and lavalle e curless and levoy especially if the vectorial version of the distance transform i e a pointer from each pixel or voxel to the nearest boundary or surface element is stored and interpolated signed distance fields are also an essential com ponent of level set evolution section where they are called characteristic functions connected components another useful semi global image operation is finding connected components which are de fined as regions of adjacent pixels that have the same input value or label in the remainder of this section consider pixels to be adjacent if they are immediate neighbors and they have the same input value connected components can be used in a variety of applications such as finding individual letters in a scanned document or finding objects say cells in a thresholded image and computing their area statistics consider the grayscale image in figure there are four connected components in this figure the outermost set of white pixels the large ring of gray pixels the white enclosed region and the single gray pixel these are shown pseudocolored in figure as pink green blue and brown to compute the connected components of an image we first conceptually split the image into horizontal runs of adjacent pixels and then color the runs with unique labels re using the labels of vertically adjacent runs whenever possible in a second phase adjacent runs of different colors are then merged while this description is a little sketchy it should be enough to enable a motivated stu dent to implement this algorithm exercise haralick and shapiro section give a much longer description of various connected component algorithms including ones that avoid the creation of a potentially large re coloring equivalence table well debugged connected component algorithms are also available in most image processing libraries once a binary or multi valued image has been segmented into its connected components it is often useful to compute the area statistics for each individual region r such statistics include the area number of pixels the perimeter number of boundary pixels the centroid average x and y values the second moments m x y r r x x l x x y y l from which the major and minor axis orientation and lengths can be computed using eigenvalue analysis these statistics can then be used for further processing e g for sorting the regions by the area size to consider the largest regions first or for preliminary matching of regions in different images fourier transforms in section we mentioned that fourier analysis could be used to analyze the frequency characteristics of various filters in this section we explain both how fourier analysis lets us determine these characteristics or equivalently the frequency content of an image and how using the fast fourier transform fft lets us perform large kernel convolutions in time that is independent of the kernel size more comprehensive introductions to fourier transforms are provided by bracewell glassner oppenheim and schafer oppen heim schafer and buck how can we analyze what a given filter does to high medium and low frequencies the answer is to simply pass a sinusoid of known frequency through the filter and to observe by how much it is attenuated let x sin x φi sin ωx φi moments can also be computed using green theorem applied to the boundary pixels yang and albregtsen figure the fourier transform as the response of a filter h x to an input sinusoid x ejωx yielding an output sinusoid o x h x x aejωx φ be the input sinusoid whose frequency is f angular frequency is ω and phase is φi note that in this section we use the variables x and y to denote the spatial coordinates of an image rather than i and j as in the previous sections this is both because the letters i and j are used for the imaginary number the usage depends on whether you are reading complex variables or electrical engineering literature and because it is clearer how to distinguish the horizontal x and vertical y components in frequency space in this section we use the letter j for the imaginary number since that is the form more commonly found in the signal processing literature bracewell oppenheim and schafer oppenheim schafer and buck if we convolve the sinusoidal signal x with a filter whose impulse response is h x we get another sinusoid of the same frequency but different magnitude a and phase φo o x h x x a sin ωx φo as shown in figure to see that this is the case remember that a convolution can be expressed as a weighted summation of shifted input signals and that the summation of a bunch of shifted sinusoids of the same frequency is just a single sinusoid at that frequency the new magnitude a is called the gain or magnitude of the filter while the phase difference φ φo φi is called the shift or phase in fact a more compact notation is to use the complex valued sinusoid x ejωx cos ωx j sin ωx in that case we can simply write o x h x x aejωx φ if h is a general non linear transform additional harmonic frequencies are introduced this was traditionally the bane of audiophiles who insisted on equipment with no harmonic distortion now that digital audio has intro duced pure distortion free sound some audiophiles are buying retro tube amplifiers or digital signal processors that simulate such distortions because of their warmer sound the fourier transform is simply a tabulation of the magnitude and phase response at each frequency h ω f h x aejφ i e it is the response to a complex sinusoid of frequency ω passed through the filter h x the fourier transform pair is also often written as h x f h ω unfortunately does not give an actual formula for computing the fourier transform instead it gives a recipe i e convolve the filter with a sinusoid observe the magnitude and phase shift repeat fortunately closed form equations for the fourier transform exist both in the continuous domain and in the discrete domain h ω h x e jωxdx n h k h x e j n x where n is the length of the signal or region of analysis these formulas apply both to filters such as h x and to signals or images such as x or g x the discrete form of the fourier transform is known as the discrete fourier trans form dft note that while can be evaluated for any value of k it only makes sense for values in the range k n n this is because larger values of k alias with lower frequencies and hence provide no additional information as explained in the discussion on aliasing in section at face value the dft takes o n operations multiply adds to evaluate fortunately there exists a faster algorithm called the fast fourier transform fft which requires only o n n operations bracewell oppenheim schafer and buck we do not explain the details of the algorithm here except to say that it involves a series of n stages where each stage performs small transforms matrix multiplications with known coefficients followed by some semi global permutations you will often see the term but terfly applied to these stages because of the pictorial shape of the signal processing graphs involved implementations for the fft can be found in most numerical and signal processing libraries now that we have defined the fourier transform what are some of its properties and how can they be used table lists a number of useful properties which we describe in a little more detail below property signal transform superposition x x ω ω shift f x f ω e reversal f x f ω convolution f x h x f ω h ω correlation f x h x f ω h ω multiplication f x h x f ω h ω differentiation f i x jωf ω domain scaling f ax af ω a real images f x f x f ω f ω parseval theorem x f x ω f ω table some useful properties of fourier transforms the original transform pair is f ω f f x superposition the fourier transform of a sum of signals is the sum of their fourier transforms thus the fourier transform is a linear operator shift the fourier transform of a shifted signal is the transform of the original signal multiplied by a linear phase shift complex sinusoid reversal the fourier transform of a reversed signal is the complex conjugate of the signal transform convolution the fourier transform of a pair of convolved signals is the product of their transforms correlation the fourier transform of a correlation is the product of the first transform times the complex conjugate of the second one multiplication the fourier transform of the product of two signals is the convolution of their transforms differentiation the fourier transform of the derivative of a signal is that signal transform multiplied by the frequency in other words differentiation linearly empha sizes magnifies higher frequencies domain scaling the fourier transform of a stretched signal is the equivalently com pressed and scaled version of the original transform and vice versa real images the fourier transform of a real valued signal is symmetric around the origin this fact can be used to save space and to double the speed of image ffts by packing alternating scanlines into the real and imaginary parts of the signal being transformed parseval theorem the energy sum of squared values of a signal is the same as the energy of its fourier transform all of these properties are relatively straightforward to prove see exercise and they will come in handy later in the book e g when designing optimum wiener filters section or performing fast image correlations section fourier transform pairs now that we have these properties in place let us look at the fourier transform pairs of some commonly occurring filters and signals as listed in table in more detail these pairs are as follows impulse the impulse response has a constant all frequency transform shifted impulse the shifted impulse has unit magnitude and linear phase box filter the box moving average filter box if x else has a sinc fourier transform sinc ω sin ω ω which has an infinite number of side lobes conversely the sinc filter is an ideal low pass filter for a non unit box the width of the box a and the spacing of the zero crossings in the sinc a are inversely proportional tent the piecewise linear tent function tent x max x has a fourier transform gaussian the unit area gaussian of width σ g x σ e 2σ2 has a unit height gaussian of width σ as its fourier transform name signal transform impulse δ x shifted impulse δ x u e jωu box filter box x a asinc aω tent tent x a aω gaussian 0000 g x σ g ω σ 0000 laplacian g x σ ω σ cos ω x g x σ g ω ω σ unsharp mask γ δ x γg x σ γ g ω σ windowed sinc 0000 0000 0000 rcos x aw sinc x a see figure 5000 0000 5000 table some useful continuous fourier transform pairs the dashed line in the fourier transform of the shifted impulse indicates its linear phase all other transforms have zero phase they are real valued note that the figures are not necessarily drawn to scale but are drawn to illustrate the general shape and characteristics of the filter or its response in particular the laplacian of gaussian is drawn inverted because it resembles more a mexican hat as it is sometimes called laplacian of gaussian the second derivative of a gaussian of width σ log x σ g x σ has a band pass response of as its fourier transform σ ω σ gabor the even gabor function which is the product of a cosine of frequency and a gaussian of width σ has as its transform the sum of the two gaussians of width σ centered at ω the odd gabor function which uses a sine is the difference of two such gaussians gabor functions are often used for oriented and band pass filtering since they can be more frequency selective than gaussian derivatives unsharp mask the unsharp mask introduced in has as its transform a unit response with a slight boost at higher frequencies windowed sinc the windowed masked sinc function shown in table has a re sponse function that approximates an ideal low pass filter better and better as additional side lobes are added w is increased figure shows the shapes of these such fil ters along with their fourier transforms for these examples we use a one lobe raised cosine rcos x cos πx box x also known as the hann window as the windowing function wolberg and oppenheim schafer and buck discuss additional windowing functions which include the lanczos window the positive first lobe of a sinc function we can also compute the fourier transforms for the small discrete kernels shown in fig ure see table notice how the moving average filters do not uniformly dampen higher frequencies and hence can lead to ringing artifacts the binomial filter gomes and velho used as the gaussian in burt and adelson laplacian pyramid see section does a decent job of separating the high and low frequencies but still leaves a fair amount of high frequency detail which can lead to aliasing after downsampling the sobel edge detector at first linearly accentuates frequencies but then decays at higher fre quencies and hence has trouble detecting fine scale edges e g adjacent black and white columns we look at additional examples of small kernel fourier transforms in section where we study better kernels for pre filtering before decimation size reduction name kernel transform plot box cos ω box cos ω cos linear cos ω binomial cos ω sobel sin ω corner cos ω table fourier transforms of the separable kernels shown in figure two dimensional fourier transforms the formulas and insights we have developed for one dimensional signals and their trans forms translate directly to two dimensional images here instead of just specifying a hor izontal or vertical frequency ωx or ωy we can create an oriented sinusoid of frequency ωx ωy x y sin ωxx ωyy the corresponding two dimensional fourier transforms are then h ωx ω r r h x y e j ωxx ωy y dx dy and in the discrete domain m n kxx ky y h kx ky m n x h x y e y m n where m and n are the width and height of the image all of the fourier transform properties from table carry over to two dimensions if we replace the scalar variables x ω and a with their vector counterparts x x y ω ωx ωy y0 and a ax ay and use vector inner products instead of multiplications wiener filtering while the fourier transform is a useful tool for analyzing the frequency characteristics of a filter kernel or image it can also be used to analyze the frequency spectrum of a whole class of images a simple model for images is to assume that they are random noise fields whose expected magnitude at each frequency is given by this power spectrum ps ωx ωy i e s ωx ωy ps ωx ωy where the angle brackets denote the expected mean value of a random variable to generate such an image we simply create a random gaussian noise image s ωx ωy where each pixel is a zero mean of variance ps ωx ωy and then take its inverse fft the observation that signal spectra capture a first order description of spatial statistics is widely used in signal and image processing in particular assuming that an image is a the notation e is also commonly used we set the dc i e constant component at s to the mean grey level see algorithm c in appendix c for code to generate gaussian noise sample from a correlated gaussian random noise field combined with a statistical model of the measurement process yields an optimum restoration filter known as the wiener filter to derive the wiener filter we analyze each frequency component of a signal fourier transform independently the noisy image formation process can be written as o x y x y n x y where x y is the unknown image we are trying to recover n x y is the additive noise signal and o x y is the observed noisy image because of the linearity of the fourier trans form we can write o ωx ωy s ωx ωy n ωx ωy where each quantity in the above equation is the fourier transform of the corresponding image at each frequency ωx ωy we know from our image spectrum that the unknown trans form component s ωx ωy has a prior distribution which is a zero mean gaussian with vari ance ps ωx ωy we also have noisy measurement o ωx ωy whose variance is pn ωx ωy i e the power spectrum of the noise which is usually assumed to be constant white pn ωx ωy according to bayes rule appendix b the posterior estimate of s can be written as p s o p o s p s where p o s p o s p s is a normalizing constant used to make the p s o distribution proper integrate to the prior distribution p s is given by s µ p s e where µ is the expected mean at that frequency everywhere except at the origin and the measurement distribution p o s is given by s o p s e 70 get taking the negative logarithm of both sides of and setting µ for simplicity we log p s o log p o s log p s c 2pn s o 1s2 c wiener is pronounced veener since in german the w is pronounced v remember that next time you order wiener schnitzel a b figure one dimensional wiener filter a power spectrum of signal ps f noise level and wiener filter transform w f b wiener filter spatial kernel which is the negative posterior log likelihood the minimum of this quantity is easy to compute pn ps sopt p p o p p o p p o the quantity n n n w ωx ωy p ω ω n x y is the fourier transform of the optimum wiener filter needed to remove the noise from an image whose power spectrum is ps ωx ωy notice that this filter has the right qualitative properties i e for low frequencies where ps it has unit gain whereas for high frequencies it attenuates the noise by a factor ps figure shows the one dimensional transform w f and the corresponding filter kernel w x for the commonly assumed case of p f f field exercise has you compare the wiener filter as a denoising algorithm to hand tuned gaussian smoothing the methodology given above for deriving the wiener filter can easily be extended to the case where the observed image is a noisy blurred version of the original image o x y b x y x y n x y where b x y is the known blur kernel rather than deriving the corresponding wiener fil ter we leave it as an exercise exercise which also encourages you to compare your de blurring results with unsharp masking and na ıve inverse filtering more sophisticated al gorithms for blur removal are discussed in sections and discrete cosine transform the discrete cosine transform dct is a variant of the fourier transform particularly well suited to compressing images in a block wise fashion the one dimensional dct is com puted by taking the dot product of each n wide block of pixels with a set of cosines of 00 figure discrete cosine transform dct basis functions the first dc i e constant basis is the horizontal blue line the second is the brown half cycle waveform etc these bases are widely used in image and video compression standards such as jpeg different frequencies f k n i cos π i n k f i where k is the coefficient frequency index and the pixel offset is used to make the basis coefficients symmetric wallace some of the discrete cosine basis functions are shown in figure as you can see the first basis function the straight blue line encodes the average dc value in the block of pixels while the second encodes a slightly curvy version of the slope in turns out that the dct is a good approximation to the optimal karhunen loe ve decom position of natural image statistics over small patches which can be obtained by performing a principal component analysis pca of images as described in section the kl transform de correlates the signal optimally assuming the signal is described by its spectrum and thus theoretically leads to optimal compression the two dimensional version of the dct is defined similarly f k l n n cos π i n k cos π j n l f i j i j like the fast fourier transform the dct can be implemented separably i e first computing the dct of each line in the block and then computing the dct of each resulting column like the fft each of the dcts can also be computed in o n log n time as we mentioned in section the dct is widely used in today image and video compression algorithms although it is slowly being supplanted by wavelet algorithms si moncelli and adelson as discussed in section and overlapped variants of the dct malvar which are used in the new jpeg xr standard these http www itu int rec t rec t i en newer algorithms suffer less from the blocking artifacts visible edge aligned discontinuities that result from the pixels in each block typically being transformed and quantized independently see exercise for ideas on how to remove blocking artifacts from com pressed jpeg images application sharpening blur and noise removal another common application of image processing is the enhancement of images through the use of sharpening and noise removal operations which require some kind of neighborhood processing traditionally these kinds of operation were performed using linear filtering see sections and section today it is more common to use non linear filters sec tion such as the weighted median or bilateral filter 37 anisotropic diffusion or non local means buades coll and morel variational methods sec tion especially those using non quadratic robust norms such as the norm which is called total variation are also often used figure shows some examples of linear and non linear filters being used to remove noise when measuring the effectiveness of image denoising algorithms it is common to report the results as a peak signal to noise ratio psnr measurement 119 where i x is the original noise free image and iˆ x is the image after denoising this is for the case where the noisy image has been synthetically generated so that the clean image is known a better way to measure the quality is to use a perceptually based similarity metric such as the structural similarity ssim index wang bovik sheikh et al wang bovik and simoncelli exercises and have you implement some of these operations and compare their effectiveness more sophisticated techniques for blur removal and the related task of super resolution are discussed in section pyramids and wavelets so far in this chapter all of the image transformations we have studied produce output images of the same size as the inputs often however we may wish to change the resolution of an image before proceeding further for example we may need to interpolate a small image to make its resolution match that of the output printer or computer screen alternatively we may want to reduce the size of an image to speed up the execution of an algorithm or to save on storage space or transmission time sometimes we do not even know what the appropriate resolution for the image should be consider for example the task of finding a face in an image section since we do not know the scale at which the face will appear we need to generate a whole pyramid of differently sized images and scan each one for possible faces biological visual systems also operate on a hierarchy of scales marr such a pyramid can also be very helpful in accelerating the search for an object by first finding a smaller instance of that object at a coarser level of the pyramid and then looking for the full resolution object only in the vicinity of coarse level detections section finally image pyramids are extremely useful for performing multi scale editing operations such as blending images while maintaining details in this section we first discuss good filters for changing image resolution i e upsampling interpolation section and downsampling decimation section we then present the concept of multi resolution pyramids which can be used to create a complete hierarchy of differently sized images and to enable a variety of applications section a closely related concept is that of wavelets which are a special kind of pyramid with higher frequency selectivity and other useful properties section finally we present a useful application of pyramids namely the blending of different images in a way that hides the seams between the image boundaries section interpolation in order to interpolate or upsample an image to a higher resolution we need to select some interpolation kernel with which to convolve the image g i j f k l h i rk j rl k l this formula is related to the discrete convolution formula except that we replace k and l in h with rk and rl where r is the upsampling rate figure shows how to think of this process as the superposition of sample weighted interpolation kernels one centered at each input sample k an alternative mental model is shown in figure where the kernel is centered at the output pixel value i the two forms are equivalent the latter form is sometimes called the polyphase filter form since the kernel values h i can be stored as r separate kernels each of which is selected for convolution with the input samples depending on the phase of i relative to the upsampled grid what kinds of kernel make good interpolators the answer depends on the application and the computation time involved any of the smoothing kernels shown in tables and can be used after appropriate re scaling the linear interpolator corresponding to the tent kernel produces interpolating piecewise linear curves which result in unappealing creases when applied to images figure the cubic b spline whose discrete pixel sam pling appears as the binomial kernel in table is an approximating kernel the interpolated the smoothing kernels in table have a unit area to turn them into interpolating kernels we simply scale them up by the interpolation rate r f k f k f k f k g i r k rk i r k r k rk i r k a b figure signal interpolation g i k f k h i rk a weighted summation of input values b polyphase filter interpretation image does not pass through the input data points that produces soft images with reduced high frequency detail the equation for the cubic b spline is easiest to derive by convolving the tent function linear b spline with itself while most graphics cards use the bilinear kernel optionally combined with a mip map see section most photo editing packages use bicubic interpolation the cu bic interpolant is a derivative continuous piecewise cubic spline the term spline is synonymous with piecewise polynomial whose equation is h x a x x if x otherwise where a specifies the derivative at x parker kenyon and troxel the value of a is often set to since this best matches the frequency characteristics of a sinc function figure it also introduces a small amount of sharpening which can be visually appeal ing unfortunately this choice does not linearly interpolate straight lines intensity ramps so some visible ringing may occur a better choice for large amounts of interpolation is prob ably a which produces a quadratic reproducing spline it interpolates linear and quadratic functions exactly wolberg section figure shows the a and a cubic interpolating kernel along with their fourier transforms figure and c shows them being applied to two dimensional interpolation splines have long been used for function and data value interpolation because of the abil ity to precisely specify derivatives at control points and efficient incremental algorithms for their evaluation bartels beatty and barsky farin splines are widely used in geometric modeling and computer aided design cad applications although they have the term spline comes from the draughtsman workshop where it was the name of a flexible piece of wood or metal used to draw smooth curves a b c d figure two dimensional image interpolation a bilinear b bicubic a c bicubic a d windowed sinc nine taps windowed sinc tent cubic a cubic a 60 windowed sinc cubic a tent cubic a 100 180 a b figure 29 a some windowed sinc functions and b their log fourier transforms raised cosine windowed sinc in blue cubic interpolators a and a in green and purple and tent function in brown they are often used to perform high accuracy low pass filtering operations started being displaced by subdivision surfaces zorin schro der and sweldens peters and reif in computer vision splines are often used for elastic image deformations section motion estimation section and surface interpolation section in fact it is possible to carry out most image processing operations by representing images as splines and manipulating them in a multi resolution framework unser the highest quality interpolator is generally believed to be the windowed sinc function because it both preserves details in the lower resolution image and avoids aliasing it is also possible to construct a piecewise cubic approximation to the windowed sinc by matching its derivatives at zero crossing szeliski and ito however some people object to the excessive ringing that can be introduced by the windowed sinc and to the repetitive nature of the ringing frequencies see figure for this reason some photographers prefer to repeatedly interpolate images by a small fractional amount this tends to de correlate the original pixel grid with the final image additional possibilities include using the bilat eral filter as an interpolator kopf cohen lischinski et al using global optimization section or hallucinating details section decimation while interpolation can be used to increase the resolution of an image decimation downsam pling is required to reduce the resolution to perform decimation we first conceptually convolve the image with a low pass filter to avoid aliasing and then keep every rth sample in practice we usually only evaluate the convolution at every rth sample g i j f k l h ri k rj l k l as shown in figure note that the smoothing kernel h k l in this case is often a stretched and re scaled version of an interpolation kernel alternatively we can write g i j f k l h i k r j l r k l and keep the same kernel h k l for both interpolation and decimation one commonly used r decimation filter is the binomial filter introduced by burt and adelson as shown in table this kernel does a decent job of separating the high and low frequencies but still leaves a fair amount of high frequency detail which can lead to aliasing after downsampling however for applications such as image blending discussed later in this section this aliasing is of little concern the term decimation has a gruesome etymology relating to the practice of killing every tenth soldier in a roman unit guilty of cowardice it is generally used in signal processing to mean any downsampling or rate reduction operation a b figure 30 signal decimation a the original samples are b convolved with a low pass filter before being downsampled if however the downsampled images will be displayed directly to the user or perhaps blended with other resolutions as in mip mapping section a higher quality filter is desired for high downsampling rates the windowed sinc pre filter is a good choice fig ure 29 however for small downsampling rates e g r more careful filter design is required table shows a number of commonly used r downsampling filters while fig ure shows their corresponding frequency responses these filters include the linear filter gives a relatively poor response the binomial filter cuts off a lot of frequencies but is useful for computer vision analysis pyramids the cubic filters from the a filter has a sharper fall off than the a filter figure table filter coefficients for decimation these filters are of odd length are sym metric and are normalized to have unit dc gain sum up to see figure for their associated frequency responses figure 31 frequency response for some decimation filters the cubic a filter has the sharpest fall off but also a bit of ringing the wavelet analysis filters qmf and jpeg while useful for compression have more aliasing a cosine windowed sinc function table the qmf filter of simoncelli and adelson is used for wavelet denoising and aliases a fair amount note that the original filter coefficients are normalized to gain so they can be self inverting the analysis filter from jpeg taubman and marcellin please see the original papers for the full precision values of some of these coefficients multi resolution representations now that we have described interpolation and decimation algorithms we can build a complete image pyramid figure as we mentioned before pyramids can be used to accelerate coarse to fine search algorithms to look for objects or patterns at different scales and to per form multi resolution blending operations they are also widely used in computer graphics hardware and software to perform fractional level decimation using the mip map which we cover in section the best known and probably most widely used pyramid in computer vision is burt and adelson laplacian pyramid to construct the pyramid we first blur and sub sample the original image by a factor of two and store this in the next level of the pyramid figure because adjacent levels in the pyramid are related by a sampling rate r this kind of pyramid is known as an octave pyramid burt and adelson originally proposed a coarse l medium l fine l figure a traditional image pyramid each level has half the resolution width and height and hence a quarter of the pixels of its parent level five tap kernel of the form with b and c a in practice a which results in the familiar binomial kernel 83 which is particularly easy to implement using shifts and adds this was important in the days when multipliers were expensive the reason they call their resulting pyramid a gaussian pyramid is that repeated convolutions of the binomial kernel converge to a gaussian to compute the laplacian pyramid burt and adelson first interpolate a lower resolu tion image to obtain a reconstructed low pass version of the original image figure they then subtract this low pass version from the original to yield the band pass laplacian image which can be stored away for further processing the resulting pyramid has perfect reconstruction i e the laplacian images plus the base level gaussian in figure are sufficient to exactly reconstruct the original image figure shows the same com putation in one dimension as a signal processing diagram which completely captures the computations being performed during the analysis and re synthesis stages burt and adelson also describe a variant on the laplacian pyramid where the low pass image is taken from the original blurred image rather than the reconstructed pyramid piping the output of the l box directly to the subtraction in figure this variant has less then again this is true for any smoothing kernel wells a b figure the gaussian pyramid shown as a signal processing diagram the a analysis and b re synthesis stages are shown as using similar computations the white circles in dicate zero values inserted by the upsampling operation notice how the reconstruction filter coefficients are twice the analysis coefficients the computation is shown as flowing down the page regardless of whether we are going from coarse to fine or vice versa aliasing since it avoids one downsampling and upsampling round trip but it is not self inverting since the laplacian images are no longer adequate to reproduce the original image as with the gaussian pyramid the term laplacian is a bit of a misnomer since their band pass images are really differences of approximate gaussians or dogs dog i i i i a laplacian of gaussian which we saw in is actually its second derivative log i σ gσ i i where 86 y2 is the laplacian operator of a function figure shows how the differences of gaussian and laplacians of gaussian look in both space and frequency laplacians of gaussian have elegant mathematical properties which have been widely studied in the scale space community witkin witkin terzopoulos and kass lindeberg nielsen florack and deriche and can be used for a variety of appli cations including edge detection marr and hildreth perona and malik stereo matching witkin terzopoulos and kass and image enhancement nielsen florack and deriche a less widely used variant is half octave pyramids shown in figure these were first introduced to the vision community by crowley and stern who call them dif ference of low pass dolp transforms because of the small scale change between adja a b figure the laplacian pyramid a the conceptual flow of images through processing stages images are high pass and low pass filtered and the low pass filtered images are pro cessed in the next stage of the pyramid during reconstruction the interpolated image and the optionally filtered high pass image are added back together the q box indicates quantiza tion or some other pyramid processing e g noise removal by coring setting small wavelet values to b the actual computation of the high pass filter involves first interpolating the downsampled low pass image and then subtracting it this results in perfect reconstruction when q is the identity the high pass or band pass images are typically called laplacian images while the low pass images are called gaussian images space frequency low pass lower pass figure the difference of two low pass filters results in a band pass filter the dashed blue lines show the close fit to a half octave laplacian of gaussian cent levels the authors claim that coarse to fine algorithms perform better in the image processing community half octave pyramids combined with checkerboard sampling grids are known as quincunx sampling feilner van de ville and unser in detecting multi scale features section it is often common to use half octave or even quarter octave pyramids lowe triggs however in this case the subsampling only occurs at every octave level i e the image is repeatedly blurred with wider gaussians until a full octave of resolution change has been achieved figure wavelets while pyramids are used extensively in computer vision applications some people use wavelet decompositions as an alternative wavelets are filters that localize a signal in both space and frequency like the gabor filter in table and are defined over a hierarchy of scales wavelets provide a smooth way to decompose a signal into frequency components without blocking and are closely related to pyramids wavelets were originally developed in the applied math and signal processing communi ties and were introduced to the computer vision community by mallat strang simoncelli and adelson rioul and vetterli chui meyer all provide nice introductions to the subject along with historical reviews while chui pro vides a more comprehensive review and survey of applications sweldens describes the more recent lifting approach to wavelets that we discuss shortly wavelets are widely used in the computer graphics community to perform multi resolution geometric processing stollnitz derose and salesin and have also been used in com puter vision for similar applications szeliski pentland gortler and cohen yaou and chang lai and vemuri szeliski as well as for multi scale ori ented filtering simoncelli freeman adelson et al and denoising portilla strela coarse medium l l l coarse medium l l l fine l fine l a b figure multiresolution pyramids a pyramid with half octave quincunx sampling odd levels are colored gray for clarity b wavelet pyramid each wavelet level stores of the original pixels usually the horizontal vertical and mixed gradients so that the total number of wavelet coefficients and original pixels is the same wainwright et al since both image pyramids and wavelets decompose an image into multi resolution de scriptions that are localized in both space and frequency how do they differ the usual answer is that traditional pyramids are overcomplete i e they use more pixels than the orig inal image to represent the decomposition whereas wavelets provide a tight frame i e they keep the size of the decomposition the same as the image figure however some wavelet families are in fact overcomplete in order to provide better shiftability or steering in orientation simoncelli freeman adelson et al a better distinction therefore might be that wavelets are more orientation selective than regular band pass pyramids how are two dimensional wavelets constructed figure shows a high level dia gram of one stage of the recursive coarse to fine construction analysis pipeline alongside the complementary re construction synthesis stage in this diagram the high pass filter followed by decimation keeps of the original pixels while of the low frequency coef ficients are passed on to the next stage for further analysis in practice the filtering is usually broken down into two separable sub stages as shown in figure the resulting three wavelet images are sometimes called the high high hh high low hl and low high lh images the high low and low high images accentuate the horizontal and vertical edges and gradients while the high high image contains the less frequently occurring mixed derivatives how are the high pass h and low pass l filters shown in figure chosen and how can the corresponding reconstruction filters i and f be computed can filters be designed a b figure 37 two dimensional wavelet decomposition a high level diagram showing the low pass and high pass transforms as single boxes b separable implementation which in volves first performing the wavelet transform horizontally and then vertically the i and f boxes are the interpolation and filtering boxes required to re synthesize the image from its wavelet components that all have finite impulse responses this topic has been the main subject of study in the wavelet community for over two decades the answer depends largely on the intended ap plication e g whether the wavelets are being used for compression image analysis feature finding or denoising simoncelli and adelson show in table some good odd length quadrature mirror filter qmf coefficients that seem to work well in practice since the design of wavelet filters is such a tricky art is there perhaps a better way in deed a simpler procedure is to split the signal into its even and odd components and then perform trivially reversible filtering operations on each sequence to produce what are called lifted wavelets figures and sweldens gives a wonderfully understandable introduction to the lifting scheme for second generation wavelets followed by a comprehen sive review sweldens as figure demonstrates rather than first filtering the whole input sequence image a b figure one dimensional wavelet transform a usual high pass low pass filters fol lowed by odd and even downsampling b lifted version which first selects the odd and even subsequences and then applies a low pass prediction stage l and a high pass correction stage c in an easily reversible manner with high pass and low pass filters and then keeping the odd and even sub sequences the lifting scheme first splits the sequence into its even and odd sub components filtering the even sequence with a low pass filter l and subtracting the result from the even sequence is trivially reversible simply perform the same filtering and then add the result back in furthermore this operation can be performed in place resulting in significant space savings the same applies to filtering the even sequence with the correction filter c which is used to ensure that the even sequence is low pass a series of such lifting steps can be used to create more complex filter responses with low computational cost and guaranteed reversibility this process can perhaps be more easily understood by considering the signal processing diagram in figure during analysis the average of the even values is subtracted from the odd value to obtain a high pass wavelet coefficient however the even samples still contain an aliased sample of the low frequency signal to compensate for this a small amount of the high pass wavelet is added back to the even sequence so that it is properly low pass filtered it is easy to show that the effective low pass filter is which is in h0 a b figure lifted transform shown as a signal processing diagram a the analysis stage first predicts the odd value from its even neighbors stores the difference wavelet and then compensates the coarser even value by adding in a fraction of the wavelet b the synthesis stage simply reverses the flow of computation and the signs of some of the filters and op erations the light blue lines show what happens if we use four taps for the prediction and correction instead of just two deed a low pass filter during synthesis the same operations are reversed with a judicious change in sign of course we need not restrict ourselves to two tap filters figure shows as light blue arrows additional filter coefficients that could optionally be added to the lifting scheme without affecting its reversibility in fact the low pass and high pass filtering operations can be interchanged e g we could use a five tap cubic low pass filter on the odd sequence plus center value first followed by a four tap cubic low pass predictor to estimate the wavelet although i have not seen this scheme written down lifted wavelets are called second generation wavelets because they can easily adapt to non regular sampling topologies e g those that arise in computer graphics applications such as multi resolution surface manipulation schro der and sweldens it also turns out that lifted weighted wavelets i e wavelets whose coefficients adapt to the underlying problem being solved fattal can be extremely effective for low level image manipulation tasks and also for preconditioning the kinds of sparse linear systems that arise in the optimization based approaches to vision algorithms that we discuss in section szeliski an alternative to the widely used separable approach to wavelet construction which de composes each level into horizontal vertical and cross sub bands is to use a representation that is more rotationally symmetric and orientationally selective and also avoids the aliasing inherent in sampling signals below their nyquist frequency simoncelli freeman adelson et al introduce such a representation which they call a pyramidal radial frequency such aliasing can often be seen as the signal content moving between bands as the original signal is slowly shifted c a b d figure 40 steerable shiftable multiscale transforms simoncelli freeman adelson et al qc ieee a radial multi scale frequency domain decomposition b original image c a set of four steerable filters d the radial multi scale wavelet decomposition implementation of shiftable multi scale transforms or more succinctly steerable pyramids their representation is not only overcomplete which eliminates the aliasing problem but is also orientationally selective and has identical analysis and synthesis basis functions i e it is self inverting just like regular wavelets as a result this makes steerable pyramids a much more useful basis for the structural analysis and matching tasks commonly used in computer vision figure shows how such a decomposition looks in frequency space instead of re cursively dividing the frequency domain into squares which results in checkerboard high frequencies radial arcs are used instead figure illustrates the resulting pyramid sub bands even through the representation is overcomplete i e there are more wavelet co efficients than input pixels the additional frequency and orientation selectivity makes this representation preferable for tasks such as texture analysis and synthesis portilla and simon celli and image denoising portilla strela wainwright et al lyu and simoncelli a b c d figure laplacian pyramid blending burt and adelson qc acm a orig inal image of apple b original image of orange c regular splice d pyramid blend application image blending one of the most engaging and fun applications of the laplacian pyramid presented in sec tion is the creation of blended composite images as shown in figure burt and adelson while splicing the apple and orange images together along the midline produces a noticeable cut splining them together as burt and adelson called their procedure creates a beautiful illusion of a truly hybrid fruit the key to their approach is that the low frequency color variations between the red apple and the orange are smoothly blended while the higher frequency textures on each fruit are blended more quickly to avoid ghosting effects when two textures are overlaid to create the blended image each source image is first decomposed into its own lapla cian pyramid figure left and middle columns each band is then multiplied by a smooth weighting function whose extent is proportional to the pyramid level the simplest and most general way to create these weights is to take a binary mask image figure and to construct a gaussian pyramid from this mask each laplacian pyramid image is then a b c d e f g h i j k l figure laplacian pyramid blending details burt and adelson qc acm the first three rows show the high medium and low frequency parts of the laplacian pyramid taken from levels and the left and middle columns show the original apple and orange images weighted by the smooth interpolation functions while the right column shows the averaged contributions a b c d figure laplacian pyramid blend of two images of arbitrary shape burt and adelson qc acm a first input image b second input image c region mask d blended image multiplied by its corresponding gaussian mask and the sum of these two weighted pyramids is then used to construct the final image figure right column figure shows that this process can be applied to arbitrary mask images with sur prising results it is also straightforward to extend the pyramid blend to an arbitrary number of images whose pixel provenance is indicated by an integer valued label image see exer cise this is particularly useful in image stitching and compositing applications where the exposures may vary between different images as described in section geometric transformations in the previous sections we saw how interpolation and decimation could be used to change the resolution of an image in this section we look at how to perform more general transfor mations such as image rotations or general warps in contrast to the point processes we saw in section where the function applied to an image transforms the range of the image g x h f x f f f f x x x x figure 44 image warping involves modifying the domain of an image function rather than its range figure basic set of geometric image transformations here we look at functions that transform the domain g x f h x see figure 44 we begin by studying the global parametric transformation first introduced in sec tion such a transformation is called parametric because it is controlled by a small number of parameters we then turn our attention to more local general deformations such as those defined on meshes section finally we show how image warps can be combined with cross dissolves to create interesting morphs in between animations in section for readers interested in more details on these topics there is an excellent survey by heck bert as well as very accessible textbooks by wolberg gomes darsa costa et al and akenine mo ller and haines note that heckbert survey is on tex ture mapping which is how the computer graphics community refers to the topic of warping images onto surfaces parametric transformations parametric transformations apply a global deformation to an image where the behavior of the transformation is controlled by a small number of parameters figure shows a few ex transformation matrix dof preserves icon translation i t orientation rigid euclidean r t l lengths similarity sr t l angles affine a l parallelism projective h straight lines table hierarchy of coordinate transformations each transformation also preserves the properties listed in the rows below it i e similarity preserves not only angles but also parallelism and straight lines the matrices are extended with a third row to form a full matrix for homogeneous coordinate transformations amples of such transformations which are based on the geometric transformations shown in figure the formulas for these transformations were originally given in table and are reproduced here in table for ease of reference in general given a transformation specified by a formula xi h x and a source image f x how do we compute the values of the pixels in the new image g x as given in think about this for a minute before proceeding and see if you can figure it out if you are like most people you will come up with an algorithm that looks something like algorithm this process is called forward warping or forward mapping and is shown in figure can you think of any problems with this approach algorithm forward warping algorithm for transforming an image f x into an image g xi through the parametric transform xi h x x f x x g x a b figure forward warping algorithm a a pixel f x is copied to its corresponding location xi h x in image g xi b detail of the source and destination pixel locations in fact this approach suffers from several limitations the process of copying a pixel f x to a location xi in g is not well defined when xi has a non integer value what do we do in such a case what would you do you can round the value of xi to the nearest integer coordinate and copy the pixel there but the resulting image has severe aliasing and pixels that jump around a lot when animating the transformation you can also distribute the value among its four nearest neighbors in a weighted bilinear fashion keeping track of the per pixel weights and normalizing at the end this technique is called splatting and is sometimes used for volume rendering in the graphics community levoy and whitted levoy westover rusinkiewicz and levoy unfortunately it suffers from both moderate amounts of aliasing and a fair amount of blur loss of high resolution detail the second major problem with forward warping is the appearance of cracks and holes especially when magnifying an image filling such holes with their nearby neighbors can lead to further aliasing and blurring what can we do instead a preferable solution is to use inverse warping algorithm where each pixel in the destination image g xi is sampled from the original image f x figure how does this differ from the forward warping algorithm for one thing since hˆ xi is presumably defined for all pixels in g xi we no longer have holes more importantly resampling an image at non integer locations is a well studied problem general image inter polation see section and high quality filters that control aliasing can be used where does the function hˆ xi come from quite often it can simply be computed as the inverse of h x in fact all of the parametric transforms listed in table 5 have closed form solutions for the inverse transform simply take the inverse of the matrix specifying the transform in other cases it is preferable to formulate the problem of image warping as that of re sampling a source image f x given a mapping x hˆ xi from destination pixels xi to source pixels x for example in optical flow section we estimate the flow field as the algorithm inverse warping algorithm for creating an image g xi from an image f x using the parametric transform xi h x x f x x g x a b figure inverse warping algorithm a a pixel g xi is sampled from its corresponding location x hˆ xi in image f x b detail of the source and destination pixel locations location of the source pixel which produced the current pixel whose flow is being estimated as opposed to computing the destination pixel to which it is going similarly when correcting for radial distortion section we calibrate the lens by computing for each pixel in the final undistorted image the corresponding pixel location in the original distorted image what kinds of interpolation filter are suitable for the resampling process any of the fil ters we studied in section 5 can be used including nearest neighbor bilinear bicubic and windowed sinc functions while bilinear is often used for speed e g inside the inner loop of a patch tracking algorithm see section bicubic and windowed sinc are preferable where visual quality is important to compute the value of f x at a non integer location x we simply apply our usual fir resampling filter g x y f k l h x k y l k l where x y are the sub pixel coordinate values and h x y is some interpolating or smooth ing kernel recall from section 5 that when decimation is being performed the smoothing kernel is stretched and re scaled according to the downsampling rate r unfortunately for a general non zoom image transformation the resampling rate r is not well defined consider a transformation that stretches the x dimensions while squashing a b c figure anisotropic texture filtering a jacobian of transform a and the induced horizontal and vertical resampling rates axi x axi y ayi x ayi y b elliptical footprint of an ewa smoothing kernel c anisotropic filtering using multiple samples along the major axis image pixels lie at line intersections the y dimensions the resampling kernel should be performing regular interpolation along the x dimension and smoothing to anti alias the blurred image in the y direction this gets even more complicated for the case of general affine or perspective transforms what can we do fortunately fourier analysis can help the two dimensional general ization of the one dimensional domain scaling law given in table is g ax a a t f for all of the transforms in table 5 except perspective the matrix a is already defined for perspective transformations the matrix a is the linearized derivative of the perspective transformation figure i e the local affine approximation to the stretching induced by the projection heckbert wolberg gomes darsa costa et al akenine mo ller and haines to prevent aliasing we need to pre filter the image f x with a filter whose frequency response is the projection of the final desired spectrum through the a t transform szeliski winder and uyttendaele in general for non zoom transforms this filter is non separable and hence is very slow to compute therefore a number of approximations to this filter are used in practice include mip mapping elliptically weighted gaussian averaging and anisotropic filtering akenine mo ller and haines mip mapping mip mapping was first proposed by williams as a means to rapidly pre filter images being used for texture mapping in computer graphics a mip is a standard image the term mip stands for multi in parvo meaning many in one pyramid figure 32 where each level is pre filtered with a high quality filter rather than a poorer quality approximation such as burt and adelson five tap binomial to resample an image from a mip map a scalar estimate of the resampling rate r is first com puted for example r can be the maximum of the absolute values in a which suppresses aliasing or it can be the minimum which reduces blurring akenine mo ller and haines discuss these issues in more detail once a resampling rate has been specified a fractional pyramid level is computed using the base logarithm l r one simple solution is to resample the texture from the next higher or lower pyramid level depending on whether it is preferable to reduce aliasing or blur a better solution is to re sample both images and blend them linearly using the fractional component of l since most mip map implementations use bilinear resampling within each level this approach is usu ally called trilinear mip mapping computer graphics rendering apis such as opengl and have parameters that can be used to select which variant of mip mapping and of the sampling rate r computation should be used depending on the desired tradeoff between speed and quality exercise has you examine some of these tradeoffs in more detail elliptical weighted average the elliptical weighted average ewa filter invented by greene and heckbert is based on the observation that the affine mapping x axi defines a skewed two dimensional coordinate system in the vicinity of each source pixel x figure for every destina tion pixel xi the ellipsoidal projection of a small pixel grid in xi onto x is computed fig ure this is then used to filter the source image g x with a gaussian whose inverse covariance matrix is this ellipsoid despite its reputation as a high quality filter akenine mo ller and haines we have found in our work szeliski winder and uyttendaele that because a gaussian kernel is used the technique suffers simultaneously from both blurring and aliasing compared to higher quality filters the ewa is also quite slow although faster variants based on mip mapping have been proposed szeliski winder and uyttendaele provide some addi tional references anisotropic filtering an alternative approach to filtering oriented textures which is sometimes implemented in graphics hardware gpus is to use anisotropic filtering barkans akenine mo ller and haines in this approach several samples at different resolutions fractional levels in the mip map are combined along the major axis of the ewa gaussian figure f a i interpolate x b x warp ax t c x filter x d x sample δ x e i u f u g u h u i u j figure one dimensional signal resampling szeliski winder and uyttendaele a original sampled signal f i b interpolated signal x c warped signal x d filtered signal x e sampled signal f i i the corresponding spectra are shown below the signals with the aliased portions shown in red multi pass transforms the optimal approach to warping images without excessive blurring or aliasing is to adap tively pre filter the source image at each pixel using an ideal low pass filter i e an oriented skewed sinc or low order e g cubic approximation figure figure shows how this works in one dimension the signal is first theoretically interpolated to a continuous waveform ideally low pass filtered to below the new nyquist rate and then re sampled to the final desired resolution in practice the interpolation and decimation steps are concate nated into a single polyphase digital filtering operation szeliski winder and uyttendaele for parametric transforms the oriented two dimensional filtering and resampling opera tions can be approximated using a series of one dimensional resampling and shearing trans forms catmull and smith heckbert wolberg gomes darsa costa et al szeliski winder and uyttendaele the advantage of using a series of one dimensional transforms is that they are much more efficient in terms of basic arithmetic operations than large non separable two dimensional filter kernels in order to prevent aliasing however it may be necessary to upsample in the opposite di rection before applying a shearing transformation szeliski winder and uyttendaele figure shows this process for a rotation where a vertical upsampling stage is added be fore the horizontal shearing and upsampling stage the upper image shows the appearance of the letter being rotated while the lower image shows its corresponding fourier transform vertical upsample horizontal shear upsample vertical shear downsample horizontal downsample a b c d e figure 50 four pass rotation szeliski winder and uyttendaele a original pixel grid image and its fourier transform b vertical upsampling c horizontal shear and up sampling d vertical shear and downsampling e horizontal downsampling the general affine case looks similar except that the first two stages perform general resampling mesh based warping while parametric transforms specified by a small number of global parameters have many uses local deformations with more degrees of freedom are often required consider for example changing the appearance of a face from a frown to a smile fig ure what is needed in this case is to curve the corners of the mouth upwards while leaving the rest of the face intact to perform such a transformation different amounts of motion are required in different parts of the image figure shows some of the commonly used approaches the first approach shown in figure b is to specify a sparse set of corresponding points the displacement of these points can then be interpolated to a dense displacement field chapter using a variety of techniques nielson one possibility is to triangulate the set of points in one image de berg cheong van kreveld et al litwinowicz and williams buck finkelstein jacobs et al and to use an affine motion model table 5 specified by the three triangle vertices inside each triangle if the destination rowland and perrett pighin hecker lischinski et al blanz and vetter leyvand cohen or dror et al show more sophisticated examples of changing facial expression and appearance a b c d figure image warping alternatives gomes darsa costa et al qc morgan kaufmann a sparse control points deformation grid b denser set of control point correspondences c oriented line correspondences d uniform quadrilateral grid image is triangulated according to the new vertex locations an inverse warping algorithm figure can be used if the source image is triangulated and used as a texture map computer graphics rendering algorithms can be used to draw the new image but care must be taken along triangle edges to avoid potential aliasing alternative methods for interpolating a sparse set of displacements include moving nearby quadrilateral mesh vertices as shown in figure using variational energy minimizing interpolants such as regularization litwinowicz and williams see section or using locally weighted radial basis function combinations of displacements nielson see section for additional scattered data interpolation techniques if quadrilateral meshes are used it may be desirable to interpolate displacements down to individual pixel values using a smooth interpolant such as a quadratic b spline farin lee wolberg chwa et al in some cases e g if a dense depth map has been estimated for an image shade gortler he et al we only know the forward displacement for each pixel as mentioned before drawing source pixels at their destination location i e forward warping figure suffers from several potential problems including aliasing and the appearance of small cracks an alternative technique in this case is to forward warp the displacement field or depth map to note that the block based motion models used by many video compression standards le gall can be thought of as a order piecewise constant displacement field a b c figure line based image warping beier and neely qc acm a distance computation and position transfer b rendering algorithm c two intermediate warps used for morphing its new location fill small holes in the resulting map and then use inverse warping to perform the resampling shade gortler he et al the reason that this generally works better than forward warping is that displacement fields tend to be much smoother than images so the aliasing introduced during the forward warping of the displacement field is much less noticeable a second approach to specifying displacements for local deformations is to use corre sponding oriented line segments beier and neely as shown in figures and 52 pixels along each line segment are transferred from source to destination exactly as specified and other pixels are warped using a smooth interpolation of these displacements each line segment correspondence specifies a translation rotation and scaling i e a similarity trans form table 5 for pixels in its vicinity as shown in figure line segments influence the overall displacement of the image using a weighting function that depends on the mini mum distance to the line segment v in figure if u else the shorter of the two distances to p and q for each pixel x the target location xi for each line correspondence is computed along with a weight that depends on the distance and the line segment length figure the weighted average of all target locations xii then becomes the final destination location note that while beier and neely describe this algorithm as a forward warp an equivalent algorithm can be written by sequencing through the destination pixels the resulting warps are not identical because line lengths or distances to lines may be different exercise has you implement the beier neely line based warp and compare it to a number of other local deformation methods yet another way of specifying correspondences in order to create image warps is to use snakes section 5 combined with b splines lee wolberg chwa et al this tech nique is used in apple shake software and is popular in the medical imaging community figure image morphing gomes darsa costa et al qc morgan kaufmann top row if the two images are just blended visible ghosting results bottom row both images are first warped to the same intermediate location e g halfway towards the other image and the resulting warped images are then blended resulting in a seamless morph one final possibility for specifying displacement fields is to use a mesh specifically adapted to the underlying image content as shown in figure specifying such meshes by hand can involve a fair amount of work gomes darsa costa et al describe an interactive system for doing this once the two meshes have been specified intermediate warps can be generated using linear interpolation and the displacements at mesh nodes can be interpolated using splines application feature based morphing while warps can be used to change the appearance of or to animate a single image even more powerful effects can be obtained by warping and blending two or more images using a process now commonly known as morphing beier and neely lee wolberg chwa et al gomes darsa costa et al figure shows the essence of image morphing instead of simply cross dissolving between two images which leads to ghosting as shown in the top row each image is warped toward the other image before blending as shown in the bottom row if the correspondences have been set up well using any of the techniques shown in figure corresponding features are aligned and no ghosting results the above process is repeated for each intermediate frame being generated during a morph using different blends and amounts of deformation at each interval let t be the time parameter that describes the sequence of interpolated frames the weighting func tions for the two warped images in the blend go as t and t conversely the amount of motion that image undergoes at time t is t of the total amount of motion that is specified by the correspondences however some care must be taken in defining what it means to par tially warp an image towards a destination especially if the desired motion is far from linear sederberg gao wang et al exercise has you implement a morphing algorithm and test it out under such challenging conditions global optimization so far in this chapter we have covered a large number of image processing operators that take as input one or more images and produce some filtered or transformed version of these images in many applications it is more useful to first formulate the goals of the desired transformation using some optimization criterion and then find or infer the solution that best meets this criterion in this final section we present two different but closely related variants on this idea the first which is often called regularization or variational methods section con structs a continuous global energy function that describes the desired characteristics of the solution and then finds a minimum energy solution using sparse linear systems or related iterative techniques the second formulates the problem using bayesian statistics model ing both the noisy measurement process that produced the input images as well as prior assumptions about the solution space which are often encoded using a markov random field section examples of such problems include surface interpolation from scattered data figure image denoising and the restoration of missing regions figure and the segmentation of images into foreground and background regions figure regularization the theory of regularization was first developed by statisticians trying to fit models to data that severely underconstrained the solution space tikhonov and arsenin engl hanke and neubauer consider for example finding a smooth surface that passes through or near a set of measured data points figure such a problem is described as ill posed because many possible surfaces can fit this data since small changes in the input can sometimes lead to large changes in the fit e g if we use polynomial interpolation such problems are also often ill conditioned since we are trying to recover the unknown function f x y from which the data point d xi yi were sampled such problems are also often called a b figure a simple surface interpolation problem a nine data points of various height scattered on a grid b second order controlled continuity thin plate spline interpolator with a tear along its left edge and a crease along its right szeliski qc springer inverse problems many computer vision tasks can be viewed as inverse problems since we are trying to recover a full description of the world from a limited set of images in order to quantify what it means to find a smooth solution we can define a norm on the solution space for one dimensional functions f x we can integrate the squared first derivative of the function r f x dx or perhaps integrate the squared second derivative r f x dx here we use subscripts to denote differentiation such energy measures are examples of functionals which are operators that map functions to scalar values they are also often called variational methods because they measure the variation non smoothness in a function in two dimensions e g for images flow fields or surfaces the corresponding smooth ness functionals are r f x y f x y dx dy r f x y dx dy x y and r f x y x y f x y dx dy where the mixed term is needed to make the measure rotationally invariant grimson the first derivative norm is often called the membrane since interpolating a set of data points using this measure results in a tent like structure in fact this formula is a small deflection approximation to the surface area which is what soap bubbles minimize the second order norm is called the thin plate spline since it approximates the behavior of thin plates e g flexible steel under small deformations a blend of the two is called the thin plate spline under tension versions of these formulas where each derivative term is mul tiplied by a local weighting function are called controlled continuity splines terzopoulos figure shows a simple example of a controlled continuity interpolator fit to nine scattered data points in practice it is more common to find first order smoothness terms used with images and flow fields section and second order smoothness associated with surfaces section in addition to the smoothness term regularization also requires a data term or data penalty for scattered data interpolation nielson the data term measures the dis tance between the function f x y and a set of data points di d xi yi ed f xi yi di i for a problem like noise removal a continuous version of this measure can be used ed r f x y d x y dx dy 97 to obtain a global energy that can be minimized the two energy terms are usually added together e ed λes 98 where es is the smoothness penalty or some weighted blend and λ is the regulariza tion parameter which controls how smooth the solution should be in order to find the minimum of this continuous problem the function f x y is usually first discretized on a regular grid the most principled way to perform this discretization is to use finite element analysis i e to approximate the function with a piecewise continuous spline and then perform the analytic integration bathe fortunately for both the first order and second order smoothness functionals the judi cious selection of appropriate finite elements results in particularly simple discrete forms terzopoulos the corresponding discrete smoothness energy functions become sx i j f i j f i j gx i j i j sy i j f i j f i j gy i j and h cx i j f i j i j f i j 100 i j 21 the alternative of using kernel basis functions centered on the data points boult and kender nielson is discussed in more detail in section 2cm i j f i j f i j f i j f i j cy i j f i j 2f i j f i j where h is the size of the finite element grid the h factor is only important if the energy is being discretized at a variety of resolutions as in coarse to fine or multigrid techniques the optional smoothness weights sx i j and sy i j control the location of horizon tal and vertical tears or weaknesses in the surface for other problems such as coloriza tion levin lischinski and weiss and interactive tone mapping lischinski farbman uyttendaele et al they control the smoothness in the interpolated chroma or expo sure field and are often set inversely proportional to the local luminance gradient strength for second order problems the crease variables cx i j cm i j and cy i j control the locations of creases in the surface terzopoulos szeliski the data values gx i j and gy i j are gradient data terms constraints used by al gorithms such as photometric stereo section hdr tone mapping section fattal lischinski and werman poisson blending section pe rez gangnet and blake and gradient domain blending section levin zomet peleg et al they are set to zero when just discretizing the conventional first order smoothness functional 94 the two dimensional discrete data energy is written as ed w i j f i j d i j i j where the local weights w i j control how strongly the data constraint is enforced these values are set to zero where there is no data and can be set to the inverse variance of the data measurements when there is data as discussed by szeliski and in section the total energy of the discretized problem can now be written as a quadratic form e ed λes xt ax b c 102 where x f f m n is called the state vector the sparse symmetric positive definite matrix a is called the hessian since it encodes the second derivative of the energy function for the one dimensional first order problem a is tridiagonal for the two dimensional first order problem it is multi banded with five non zero entries per row we call b the weighted data vector minimizing the above quadratic form is equivalent to solving the sparse linear system ax b 103 we use x instead of f because this is the more common form in the numerical analysis literature golub and van loan in numerical analysis a is called the coefficient matrix saad in finite element analysis bathe it is called the stiffness matrix d i j w i j f i j sy i j f i j f i j sx i j f i j figure 55 graphical model interpretation of first order regularization the white circles are the unknowns f i j while the dark circles are the input data d i j in the resistive grid interpretation the d and f values encode input and output voltages and the black squares denote resistors whose conductance is set to sx i j sy i j and w i j in the spring mass system analogy the circles denote elevations and the black squares denote springs the same graphical model can be used to depict a first order markov random field figure 56 which can be done using a variety of sparse matrix techniques such as multigrid briggs henson and mccormick and hierarchical preconditioners szeliski as de scribed in appendix a 5 while regularization was first introduced to the vision community by poggio torre and koch and terzopoulos for problems such as surface interpolation it was quickly adopted by other vision researchers for such varied problems as edge detection sec tion optical flow section and shape from shading section poggio torre and koch horn and brooks terzopoulos bertero poggio and torre brox bruhn papenberg et al poggio torre and koch also showed how the discrete energy defined by equations 100 101 could be implemented in a resistive grid as shown in figure 55 in computational photography chapter regularization and its variants are commonly used to solve problems such as high dynamic range tone mapping fattal lischinski and werman lischinski farbman uyttendaele et al pois son and gradient domain blending pe rez gangnet and blake levin zomet peleg et al agarwala dontcheva agrawala et al colorization levin lischinski and weiss and natural image matting levin lischinski and weiss robust regularization while regularization is most commonly formulated using quadratic norms compare with the squared derivatives in 92 95 and squared differences in 100 101 it can also be formulated using non quadratic robust penalty functions appendix b for exam ple 100 can be generalized to sx i j ρ f i j f i j 104 i j sy i j ρ f i j f i j where ρ x is some monotonically increasing penalty function for example the family of norms ρ x x p is called p norms when p the resulting smoothness terms become more piecewise continuous than totally smooth which can better model the discontinuous nature of images flow fields and surfaces an early example of robust regularization is the graduated non convexity gnc algo rithm introduced by blake and zisserman here the norms on the data and derivatives are clamped to a maximum value ρ x min x2 v 105 because the resulting problem is highly non convex it has many local minima a continua tion method is proposed where a quadratic norm which is convex is gradually replaced by the non convex robust norm allgower and georg around the same time terzopou los was also using continuation to infer the tear and crease variables in his surface interpolation problems today it is more common to use the p norm which is often called total variation chan osher and shen tschumperle and deriche tschumperle kaftory schechner and zeevi other norms for which the influence derivative more quickly decays to zero are presented by black and rangarajan black sapiro marimont et al and discussed in appendix b even more recently hyper laplacian norms with p have gained popularity based on the observation that the log likelihood distribution of image derivatives follows a p 5 slope and is therefore a hyper laplacian distribution simoncelli levin and weiss weiss and freeman krishnan and fergus such norms have an even stronger tendency to prefer large discontinuities over small ones see the related discussion in section 7 while least squares regularized problems using norms can be solved using linear sys tems other p norms require different iterative techniques such as iteratively reweighted least squares irls levenberg marquardt or alternation between local non linear subproblems and global quadratic regularization krishnan and fergus such techniques are dis cussed in section and appendices a and b 7 markov random fields as we have just seen regularization which involves the minimization of energy functionals defined over piecewise continuous functions can be used to formulate and solve a variety of low level computer vision problems an alternative technique is to formulate a bayesian model which separately models the noisy image formation measurement process as well as assuming a statistical prior model over the solution space in this section we look at priors based on markov random fields whose log likelihood can be described using local neighborhood interaction or penalty terms kindermann and snell geman and geman marroquin mitter and poggio li szeliski zabih scharstein et al the use of bayesian modeling has several potential advantages over regularization see also appendix b the ability to model measurement processes statistically enables us to extract the maximum information possible from each measurement rather than just guessing what weighting to give the data similarly the parameters of the prior distribution can often be learned by observing samples from the class we are modeling roth and black tappen li and huttenlocher furthermore because our model is probabilistic it is possible to estimate in principle complete probability distributions over the unknowns being recovered and in particular to model the uncertainty in the solution which can be useful in latter processing stages finally markov random field models can be defined over discrete variables such as image labels where the variables have no proper ordering for which regularization does not apply recall from 68 in section or see appendix b that according to bayes rule the posterior distribution for a given set of measurements y p y x combined with a prior p x over the unknowns x is given by p x y p y x p x 106 where p y x p y x p x is a normalizing constant used to make the p x y distribution proper integrate to taking the negative logarithm of both sides of 106 we get log p x y log p y x log p x c 107 which is the negative posterior log likelihood to find the most likely maximum a posteriori or map solution x given some measure ments y we simply minimize this negative log likelihood which can also be thought of as an energy e x y ed x y ep x 108 we drop the constant c because its value does not matter during energy minimization the first term ed x y is the data energy or data penalty it measures the negative log likelihood that the data were observed given the unknown state x the second term ep x is the prior energy it plays a role analogous to the smoothness energy in regularization note that the map estimate may not always be desirable since it selects the peak in the posterior dis tribution rather than some more stable statistic see the discussion in appendix b and by levin weiss durand et al for image processing applications the unknowns x are the set of output pixels x f f m n and the data are in the simplest case the input pixels y d d m n as shown in figure 56 for a markov random field the probability p x is a gibbs or boltzmann distribution whose negative log likelihood according to the hammersley clifford theorem can be writ ten as a sum of pairwise interaction potentials ep x i j k l n vi j k l f i j f k l 109 where n i j denotes the neighbors of pixel i j in fact the general version of the theorem says that the energy may have to be evaluated over a larger set of cliques which depend on the order of the markov random field kindermann and snell geman and geman bishop kohli ladicky and torr kohli kumar and torr the most commonly used neighborhood in markov random field modeling is the neighborhood where each pixel in the field f i j interacts only with its immediate neigh bors the model in figure 56 which we previously used in figure 55 to illustrate the discrete version of first order regularization shows an mrf the sx i j and sy i j black boxes denote arbitrary interaction potentials between adjacent nodes in the random field and the w i j denote the data penalty functions these square nodes can also be inter preted as factors in a factor graph version of the undirected graphical model bishop which is another name for interaction potentials strictly speaking the factors are improper probability functions whose product is the un normalized posterior distribution as we will see in 112 there is a close relationship between these interaction potentials and the discretized versions of regularized image restoration problems thus to a first approximation we can view energy minimization being performed when solving a regularized problem and the maximum a posteriori inference being performed in an mrf as equivalent while neighborhoods are most commonly used in some applications or even higher order neighborhoods perform better at tasks such as image segmentation because d i j w i j f i j sy i j f i j f i j sx i j f i j figure 56 graphical model for an neighborhood markov random field the blue edges are added for an neighborhood the white circles are the unknowns f i j while the dark circles are the input data d i j the sx i j and sy i j black boxes denote arbi trary interaction potentials between adjacent nodes in the random field and the w i j denote the data penalty functions the same graphical model can be used to depict a discrete version of a first order regularization problem figure 55 they can better model discontinuities at different orientations boykov and kolmogorov rother kohli feng et al kohli ladicky and torr kohli kumar and torr binary mrfs the simplest possible example of a markov random field is a binary field examples of such fields include bit black and white scanned document images as well as images segmented into foreground and background regions to denoise a scanned image we set the data penalty to reflect the agreement between the scanned and final images ed i j wδ f i j d i j 110 and the smoothness penalty to reflect the agreement between neighboring pixels ep i j ex i j ey i j sδ f i j f i j sδ f i j f i j once we have formulated the energy how do we minimize it the simplest approach is to perform gradient descent flipping one state at a time if it produces a lower energy this ap proach is known as contextual classification kittler and fo glein iterated conditional modes icm besag or highest confidence first hcf chou and brown if the pixel with the largest energy decrease is selected first unfortunately these downhill methods tend to get easily stuck in local minima an al ternative approach is to add some randomness to the process which is known as stochastic gradient descent metropolis rosenbluth rosenbluth et al geman and geman when the amount of noise is decreased over time this technique is known as simulated an nealing kirkpatrick gelatt and vecchi carnevali coletti and patarnello wol berg and pavlidis swendsen and wang and was first popularized in computer vision by geman and geman and later applied to stereo matching by barnard among others even this technique however does not perform that well boykov veksler and zabih for binary images a much better technique introduced to the computer vision com munity by boykov veksler and zabih is to re formulate the energy minimization as a max flow min cut graph optimization problem greig porteous and seheult this technique has informally come to be known as graph cuts in the computer vision community boykov and kolmogorov for simple energy functions e g those where the penalty for non identical neighboring pixels is a constant this algorithm is guaranteed to produce the global minimum kolmogorov and zabih formally characterize the class of binary energy potentials regularity conditions for which these results hold while newer work by komodakis tziritas and paragios and rother kolmogorov lempitsky et al provide good algorithms for the cases when they do not in addition to the above mentioned techniques a number of other optimization approaches have been developed for mrf energy minimization such as loopy belief propagation and dynamic programming for one dimensional problems these are discussed in more detail in appendix b 5 as well as the comparative survey paper by szeliski zabih scharstein et al ordinal valued mrfs in addition to binary images markov random fields can be applied to ordinal valued labels such as grayscale images or depth maps the term ordinal indicates that the labels have an implied ordering e g that higher values are lighter pixels in the next section we look at unordered labels such as source image labels for image compositing in many cases it is common to extend the binary data and smoothness prior terms as ed i j w i j ρd f i j d i j 112 and ep i j sx i j ρp f i j f i j sy i j ρp f i j f i j which are robust generalizations of the quadratic penalty terms 101 and 100 first introduced in 105 as before the w i j sx i j and sy i j weights can be used to locally control the data weighting and the horizontal and vertical smoothness instead of a b c d figure 57 grayscale image denoising and inpainting a original image b image corrupted by noise and with missing data black bar c image restored using loopy be lief propagation d image restored using expansion move graph cuts images are from http vision middlebury edu mrf results szeliski zabih scharstein et al using a quadratic penalty however a general monotonically increasing penalty function ρ is used different functions can be used for the data and smoothness terms for example ρp can be a hyper laplacian penalty ρp d d p p which better encodes the distribution of gradients mainly edges in an image than either a quadratic or linear total variation penalty levin and weiss use such a penalty to separate a transmitted and reflected image figure by encouraging gradients to lie in one or the other image but not both more recently levin fergus durand et al use the hyper laplacian as a prior for image deconvolution deblurring and krishnan and fergus develop a faster algorithm for solving such problems for the data penalty ρd can be quadratic to model gaussian noise or the log of a contaminated gaussian appendix b when ρp is a quadratic function the resulting markov random field is called a gaussian markov random field gmrf and its minimum can be found by sparse linear system solving 103 when the weighting functions are uniform the gmrf becomes a special case of wiener filtering section allowing the weighting functions to depend on the input image a special kind of conditional random field which we describe below enables quite sophisticated image processing algorithms to be performed including colorization levin lischinski and weiss interactive tone mapping lischinski farbman uyttendaele et al natural image matting levin lischinski and weiss and image restoration tappen liu freeman et al note that unlike a quadratic penalty the sum of the horizontal and vertical derivative p norms is not rotationally invariant a better approach may be to locally estimate the gradient direction and to impose different norms on the perpendicular and parallel components which roth and black call a steerable random field a initial labeling b standard move c α β swap d α expansion figure 58 multi level graph optimization from boykov veksler and zabih qc ieee a initial problem configuration b the standard move only changes one pixel c the α β swap optimally exchanges all α and β labeled pixels d the α expansion move optimally selects among current pixel values and the α label when ρd or ρp are non quadratic functions gradient descent techniques such as non linear least squares or iteratively re weighted least squares can sometimes be used ap pendix a however if the search space has lots of local minima as is the case for stereo matching barnard boykov veksler and zabih more sophisticated techniques are required the extension of graph cut techniques to multi valued problems was first proposed by boykov veksler and zabih in their paper they develop two different algorithms called the swap move and the expansion move which iterate among a series of binary labeling sub problems to find a good solution figure 58 note that a global solution is generally not achievable as the problem is provably np hard for general energy functions because both these algorithms use a binary mrf optimization inside their inner loop they are subject to the kind of constraints on the energy functions that occur in the binary labeling case kolmogorov and zabih appendix b 5 discusses these algorithms in more detail along with some more recently developed approaches to this problem another mrf inference technique is belief propagation bp while belief propagation was originally developed for inference over trees where it is exact pearl it has more recently been applied to graphs with loops such as markov random fields freeman pasz tor and carmichael yedidia freeman and weiss in fact some of the better performing stereo matching algorithms use loopy belief propagation lbp to perform their inference sun zheng and shum lbp is discussed in more detail in appendix b 5 as well as the comparative survey paper on mrf optimization szeliski zabih scharstein et al figure 57 shows an example of image denoising and inpainting hole filling using a non quadratic energy function non gaussian mrf the original image has been corrupted by noise and a portion of the data has been removed the black bar in this case the loopy d i j d i j w i j f i j sy i j f i j f i j sx i j f i j figure 59 graphical model for a markov random field with a more complex measurement model the additional colored edges show how combinations of unknown values say in a sharp image produce the measured values a noisy blurred image the resulting graphical model is still a classic mrf and is just as easy to sample from but some inference algorithms e g those based on graph cuts may not be applicable because of the increased network complexity since state changes during the inference become more entangled and the posterior mrf has much larger cliques belief propagation algorithm computes a slightly lower energy and also a smoother image than the alpha expansion graph cut algorithm of course the above formula 113 for the smoothness term ep i j just shows the simplest case in more recent work roth and black propose a field of experts foe model which sums up a large number of exponentiated local filter outputs to arrive at the smoothness penalty weiss and freeman analyze this approach and compare it to the simpler hyper laplacian model of natural image statistics lyu and simoncelli use gaussian scale mixtures gsms to construct an inhomogeneous multi scale mrf with one positive exponential gmrf modulating the variance amplitude of another gaussian mrf it is also possible to extend the measurement model to make the sampled noise corrupted input pixels correspond to blends of unknown latent image pixels as in figure 59 this is the commonly occurring case when trying to de blur an image while this kind of a model is still a traditional generative markov random field finding an optimal solution can be difficult because the clique sizes get larger in such situations gradient descent techniques such as iteratively reweighted least squares can be used joshi zitnick szeliski et al exercise 31 has you explore some of these issues figure 60 an unordered label mrf agarwala dontcheva agrawala et al qc acm strokes in each of the source images on the left are used as constraints on an mrf optimization which is solved using graph cuts the resulting multi valued label field is shown as a color overlay in the middle image and the final composite is shown on the right unordered labels another case with multi valued labels where markov random fields are often applied are unordered labels i e labels where there is no semantic meaning to the numerical difference between the values of two labels for example if we are classifying terrain from aerial imagery it makes no sense to take the numeric difference between the labels assigned to forest field water and pavement in fact the adjacencies of these various kinds of terrain each have different likelihoods so it makes more sense to use a prior of the form ep i j sx i j v l i j l i j sy i j v l i j l i j where v is a general compatibility or potential function note that we have also replaced f i j with l i j to make it clearer that these are labels rather than discrete function samples an alternative way to write this prior energy boykov veksler and zabih szeliski zabih scharstein et al is ep p q n vp q lp lq 116 where the p q are neighboring pixels and a spatially varying potential function vp q is eval uated for each neighboring pair an important application of unordered mrf labeling is seam finding in image composit ing davis agarwala dontcheva agrawala et al see figure 60 which is explained in more detail in section here the compatibility vp q lp lq measures the quality of the visual appearance that would result from placing a pixel p from image lp next to a pixel q from image lq as with most mrfs we assume that vp q l l i e it is per fectly fine to choose contiguous pixels from the same image for different labels however figure 61 image segmentation boykov and funka lea qc springer the user draws a few red strokes in the foreground object and a few blue ones in the background the system computes color distributions for the foreground and background and solves a binary mrf the smoothness weights are modulated by the intensity gradients edges which makes this a conditional random field crf the compatibility vp q lp lq may depend on the values of the underlying pixels ilp p and ilq q consider for example where one image is all sky blue i e p q b while the other image has a transition from sky blue p b to forest green q g in this case vp q the colors agree while vp q the colors disagree conditional random fields in a classic bayesian model 106 108 p x y p y x p x 117 the prior distribution p x is independent of the observations y sometimes however it is useful to modify our prior assumptions say about the smoothness of the field we are trying to estimate in response to the sensed data whether this makes sense from a probability viewpoint is something we discuss once we have explained the new model consider the interactive image segmentation problem shown in figure 61 boykov and funka lea in this application the user draws foreground red and background blue strokes and the system then solves a binary mrf labeling problem to estimate the extent of the foreground object in addition to minimizing a data term which measures the pointwise similarity between pixel colors and the inferred region distributions section 5 5 the mrf d i j w i j f i j sy i j f i j f i j sx i j f i j figure 62 graphical model for a conditional random field crf the additional green edges show how combinations of sensed data influence the smoothness in the underlying mrf prior model i e sx i j and sy i j in 113 depend on adjacent d i j values these additional links factors enable the smoothness to depend on the input data however they make sampling from this mrf more complex is modified so that the smoothness terms sx x y and sy x y in figure 56 and 113 depend on the magnitude of the gradient between adjacent pixels since the smoothness term now depends on the data bayes rule 117 no longer ap plies instead we use a direct model for the posterior distribution p x y whose negative log likelihood can be written as e x y ed x y es x y vp xp y vp q xp xq y 118 using the notation introduced in 116 the resulting probability distribution is called a conditional random field crf and was first introduced to the computer vision field by ku mar and hebert based on earlier work in text modeling by lafferty mccallum and pereira figure 62 shows a graphical model where the smoothness terms depend on the data values in this particular model each smoothness term depends only on its adjacent pair of data values i e terms are of the form vp q xp xq yp yq in 118 the idea of modifying smoothness terms in response to input data is not new for ex ample boykov and jolly used this idea for interactive segmentation as shown in figure 61 and it is now widely used in image segmentation section 5 5 blake rother 25 an alternative formulation that also uses detected edges to modulate the smoothness of a depth or motion field and hence to integrate multiple lower level vision modules is presented by poggio gamble and little d i j d i j w i j f i j sy i j f i j f i j sx i j f i j figure 63 graphical model for a discriminative random field drf the additional green edges show how combinations of sensed data e g d i j influence the data term for f i j the generative model is therefore more complex i e we cannot just apply a simple function to the unknown variables and add noise brown et al rother kolmogorov and blake denoising tappen liu freeman et al and object recognition section winn and shotton shotton winn rother et al in stereo matching the idea of encouraging disparity discontinuities to coincide with intensity edges goes back even further to the early days of optimization and mrf based algorithms poggio gamble and little fua bobick and intille boykov veksler and zabih and is discussed in more detail in section 5 in addition to using smoothness terms that adapt to the input data kumar and hebert also compute a neighborhood function over the input data for each vp xp y term as illustrated in figure 63 instead of using the classic unary mrf data term vp xp yp shown in figure 56 because such neighborhood functions can be thought of as dis criminant functions a term widely used in machine learning bishop they call the resulting graphical model a discriminative random field drf in their paper kumar and hebert show that drfs outperform similar crfs on a number of applications such as structure detection figure 64 and binary image denoising here again one could argue that previous stereo correspondence algorithms also look at a neighborhood of input data either explicitly because they compute correlation measures criminisi cross blake et al as data terms or implicitly because even pixel wise disparity costs look at several pixels in either the left or right image barnard boykov veksler and zabih kumar and hebert call the unary potentials vp xp y association potentials and the pairwise potentials vp q xp yq y interaction potentials figure 64 structure detection results using an mrf left and a drf right kumar and hebert qc springer what then are the advantages and disadvantages of using conditional or discriminative random fields instead of mrfs classic bayesian inference mrf assumes that the prior distribution of the data is in dependent of the measurements this makes a lot of sense if you see a pair of sixes when you first throw a pair of dice it would be unwise to assume that they will always show up thereafter however if after playing for a long time you detect a statistically significant bias you may want to adjust your prior what crfs do in essence is to select or modify the prior model based on observed data this can be viewed as making a partial inference over addi tional hidden variables or correlations between the unknowns say a label depth or clean image and the knowns observed images in some cases the crf approach makes a lot of sense and is in fact the only plausi ble way to proceed for example in grayscale image colorization section levin lischinski and weiss the best way to transfer the continuity information from the input grayscale image to the unknown color image is to modify local smoothness constraints similarly for simultaneous segmentation and recognition winn and shotton shotton winn rother et al it makes a lot of sense to permit strong color edges to influence the semantic image label continuities in other cases such as image denoising the situation is more subtle using a non quadratic robust smoothness term as in 113 plays a qualitatively similar role to setting the smoothness based on local gradient information in a gaussian mrf gmrf tappen liu freeman et al in more recent work tanaka and okutomi use a larger neighborhood and full covariance matrix on a related gaussian mrf the advantage of gaus sian mrfs when the smoothness can be correctly inferred is that the resulting quadratic energy can be minimized in a single step however for situations where the discontinuities are not self evident in the input data such as for piecewise smooth sparse data interpolation blake and zisserman terzopoulos classic robust smoothness energy minimiza tion may be preferable thus as with most computer vision algorithms a careful analysis of the problem at hand and desired robustness and computation constraints may be required to choose the best technique perhaps the biggest advantage of crfs and drfs as argued by kumar and hebert tappen liu freeman et al and blake rother brown et al is that learning the model parameters is sometimes easier while learning parameters in mrfs and their variants is not a topic that we cover in this book interested readers can find more details in recently published articles kumar and hebert roth and black tappen liu freeman et al tappen li and huttenlocher 7 application image restoration in section we saw how two dimensional linear and non linear filters can be used to remove noise or enhance sharpness in images sometimes however images are degraded by larger problems such as scratches and blotches kokaram in this case bayesian meth ods such as mrfs which can model spatially varying per pixel measurement noise can be used instead an alternative is to use hole filling or inpainting techniques bertalmio sapiro caselles et al bertalmio vese sapiro et al criminisi pe rez and toyama as discussed in sections 5 and 5 figure 57 shows an example of image denoising and inpainting hole filling using a markov random field the original image has been corrupted by noise and a portion of the data has been removed in this case the loopy belief propagation algorithm computes a slightly lower energy and also a smoother image than the alpha expansion graph cut algo rithm additional reading if you are interested in exploring the topic of image processing in more depth some popular textbooks have been written by lim crane gomes and velho ja hne pratt russ burger and burge gonzales and woods the pre eminent conference and journal in this field are the ieee conference on image pro cesssing and the ieee transactions on image processing for image compositing operators the seminal reference is by porter and duff while blinn b provides a more detailed tutorial for image compositing smith and blinn were the first to bring this topic to the attention of the graphics community while wang and cohen provide a recent in depth survey in the realm of linear filtering freeman and adelson provide a great introduc tion to separable and steerable oriented band pass filters while perona shows how to additional reading approximate any filter as a sum of separable components the literature on non linear filtering is quite wide and varied it includes such topics as bilateral filtering tomasi and manduchi durand and dorsey paris and durand chen paris and durand paris kornprobst tumblin et al related itera tive algorithms saint marc chen and medioni nielsen florack and deriche black sapiro marimont et al weickert ter haar romeny and viergever weick ert barash scharr black and haussecker barash and comaniciu and variational approaches chan osher and shen tschumperle and deriche tschumperle kaftory schechner and zeevi good references to image morphology include haralick and shapiro section 5 bovik section ritter and wilson section 7 serra serra and vincent yuille vincent and geiger soille the classic papers for image pyramids and pyramid blending are by burt and adelson b wavelets were first introduced to the computer vision community by mallat and good tutorial and review papers and books are available strang simoncelli and adelson rioul and vetterli chui meyer sweldens wavelets are widely used in the computer graphics community to perform multi resolution geomet ric processing stollnitz derose and salesin and have been used in computer vision for similar applications szeliski pentland gortler and cohen yaou and chang lai and vemuri szeliski as well as for multi scale oriented filter ing simoncelli freeman adelson et al and denoising portilla strela wainwright et al while image pyramids section 5 are usually constructed using linear filtering op erators some recent work has started investigating non linear filters since these can better preserve details and other salient features some representative papers in the computer vision literature are by gluckman b lyu and simoncelli and in computational pho tography by bae paris and durand farbman fattal lischinski et al fattal high quality algorithms for image warping and resampling are covered both in the im age processing literature wolberg dodgson gomes darsa costa et al szeliski winder and uyttendaele and in computer graphics williams heckbert barkans akenine mo ller and haines where they go under the name of texture mapping combination of image warping and image blending techniques are used to enable morphing between images which is covered in a series of seminal papers and books beier and neely gomes darsa costa et al the regularization approach to computer vision problems was first introduced to the vi sion community by poggio torre and koch and terzopoulos b and continues to be a popular framework for formulating and solving low level vision problems ju black and jepson nielsen florack and deriche nordstro m brox bruhn papenberg et al levin lischinski and weiss more detailed mathe matical treatment and additional applications can be found in the applied mathematics and statistics literature tikhonov and arsenin engl hanke and neubauer the literature on markov random fields is truly immense with publications in related fields such as optimization and control theory of which few vision practitioners are even aware a good guide to the latest techniques is the book edited by blake kohli and rother other recent articles that contain nice literature reviews or experimental compar isons include boykov and funka lea szeliski zabih scharstein et al kumar veksler and torr the seminal paper on markov random fields is the work of geman and geman who introduced this formalism to computer vision researchers and also introduced the no tion of line processes additional binary variables that control whether smoothness penalties are enforced or not black and rangarajan showed how independent line processes could be replaced with robust pairwise potentials boykov veksler and zabih devel oped iterative binary graph cut algorithms for optimizing multi label mrfs kolmogorov and zabih characterized the class of binary energy potentials required for these tech niques to work and freeman pasztor and carmichael popularized the use of loopy belief propagation for mrf inference many more additional references can be found in sections 7 and 5 5 and appendix b 5 exercises ex color balance write a simple application to change the color balance of an image by multiplying each color value by a different user specified constant if you want to get fancy you can make this application interactive with sliders do you get different results if you take out the gamma transformation before or after doing the multiplication why or why not take the same picture with your digital camera using different color balance settings most cameras control the color balance from one of the menus can you recover what the color balance ratios are between the different settings you may need to put your camera on a tripod and align the images manually or automatically to make this work alternatively use a color checker chart figure 3b as discussed in sections and if you have access to the raw image for the camera perform the demosaicing yourself section or downsample the image resolution to get a true rgb image does your camera perform a simple linear mapping between raw values and the color balanced values in a jpeg some high end cameras have a raw jpeg mode which makes this comparison much easier can you think of any reason why you might want to perform a color twist sec tion on the images see also exercise for some related ideas ex compositing and reflections section describes the process of compositing an alpha matted image on top of another answer the following questions and optionally validate them experimentally most captured images have gamma correction applied to them does this invalidate the basic compositing equation if so how should it be fixed the additive pure reflection model may have limitations what happens if the glass is tinted especially to a non gray hue how about if the glass is dirty or smudged how could you model wavy glass or other kinds of refractive objects ex blue screen matting set up a blue or green background e g by buying a large piece of colored posterboard take a picture of the empty background and then of the back ground with a new object in front of it pull the matte using the difference between each colored pixel and its assumed corresponding background pixel using one of the techniques described in section or by smith and blinn ex difference keying implement a difference keying algorithm see section toyama krumm brumitt et al consisting of the following steps compute the mean and variance or median and robust variance at each pixel in an empty video sequence for each new frame classify each pixel as foreground or background set the back ground pixels to rgba optional compute the alpha channel and composite over a new background optional clean up the image using morphology section label the connected components section compute their centroids and track them from frame to frame use this to build a people counter ex 5 photo effects write a variety of photo enhancement or effects filters contrast so larization quantization etc which ones are useful perform sensible corrections and which ones are more creative create unusual images ex histogram equalization compute the gray level luminance histogram for an im age and equalize it so that the tones look better and the image is less sensitive to exposure settings you may want to use the following steps convert the color image to luminance section compute the histogram the cumulative distribution and the compensation transfer function section optional try to increase the punch in the image by ensuring that a certain fraction of pixels say 5 are mapped to pure black and white optional limit the local gain f i i in the transfer function one way to do this is to limit f i γi or f i i γ while performing the accumulation keeping any unaccumulated values in reserve i ll let you figure out the exact details 5 compensate the luminance channel through the lookup table and re generate the color image using color ratios 116 optional color values that are clipped in the original image i e have one or more saturated color channels may appear unnatural when remapped to a non clipped value extend your algorithm to handle this case in some useful way ex 7 local histogram equalization compute the gray level luminance histograms for each patch but add to vertices based on distance a spline build on exercise luminance computation distribute values counts to adjacent vertices bilinear convert to cdf look up functions optional use low pass filtering of cdfs 5 interpolate adjacent cdfs for final lookup ex padding for neighborhood operations write down the formulas for computing the padded pixel values f i j as a function of the original pixel values f k l and the image width and height m n for each of the padding modes shown in figure for example for replication clamping f i j f k l k max min m i l max min n j hint you may want to use the min max mod and absolute value operators in addition to the regular arithmetic operators describe in more detail the advantages and disadvantages of these various modes optional check what your graphics card does by drawing a texture mapped rectangle where the texture coordinates lie beyond the range and using different texture clamping modes ex separable filters implement convolution with a separable kernel the input should be a grayscale or color image along with the horizontal and vertical kernels make sure you support the padding mechanisms developed in the previous exercise you will need this functionality for some of the later exercises if you already have access to separable filtering in an image processing package you are using such as ipl skip this exercise optional use pietro perona technique to approximate convolution as a sum of a number of separable kernels let the user specify the number of kernels and report back some sensible metric of the approximation fidelity ex discrete gaussian filters discuss the following issues with implementing a dis crete gaussian filter if you just sample the equation of a continuous gaussian filter at discrete locations will you get the desired properties e g will the coefficients sum up to similarly if you sample a derivative of a gaussian do the samples sum up to or have vanishing higher order moments would it be preferable to take the original signal interpolate it with a sinc blur with a continuous gaussian then pre filter with a sinc before re sampling is there a simpler way to do this in the frequency domain would it make more sense to produce a gaussian frequency response in the fourier domain and to then take an inverse fft to obtain a discrete filter how does truncation of the filter change its frequency response does it introduce any additional artifacts are the resulting two dimensional filters as rotationally invariant as their continuous analogs is there some way to improve this in fact can any discrete separable or non separable filter be truly rotationally invariant ex sharpening blur and noise removal implement some softening sharpening and non linear diffusion selective sharpening or noise removal filters such as gaussian median and bilateral section as discussed in section take blurry or noisy images shooting in low light is a good way to get both and try to improve their appearance and legibility ex 12 steerable filters implement freeman and adelson steerable filter algo rithm the input should be a grayscale or color image and the output should be a multi banded image consisting of and the coefficients for the filters can be found in the paper by freeman and adelson test the various order filters on a number of images of your choice and see if you can reliably find corner and intersection features these filters will be quite useful later to detect elongated structures such as lines section ex 13 distance transform implement some raster scan algorithms for city block and euclidean distance transforms can you do it without peeking at the literature danielsson borgefors if so what problems did you come across and resolve later on you can use the distance functions you compute to perform feathering during image stitching section 9 ex connected components implement one of the connected component algorithms from section or section from haralick and shapiro s book and discuss its computational complexity threshold or quantize an image to obtain a variety of input labels and then compute the area statistics for the regions that you find use the connected components that you have found to track or match regions in differ ent images or video frames ex fourier transform prove the properties of the fourier transform listed in ta ble and derive the formulas for the fourier transforms listed in tables and these exercises are very useful if you want to become comfortable working with fourier transforms which is a very useful skill when analyzing and designing the behavior and efficiency of many computer vision algorithms ex wiener filtering estimate the frequency spectrum of your personal photo collec tion and use it to perform wiener filtering on a few images with varying degrees of noise collect a few hundred of your images by re scaling them to fit within a window and cropping them take their fourier transforms throw away the phase information and average together all of the spectra pick two of your favorite images and add varying amounts of gaussian noise σn 5 gray levels for each combination of image and noise determine by eye which width of a gaussian blurring filter σs gives the best denoised result you will have to make a subjective decision between sharpness and noise 5 compute the wiener filtered version of all the noised images and compare them against your hand tuned gaussian smoothed images optional do your image spectra have a lot of energy concentrated along the horizontal and vertical axes fx and fy can you think of an explanation for this does rotating your image samples by 45 move this energy to the diagonals if not could it be due to edge effects in the fourier transform can you suggest some techniques for reducing such effects ex 17 deblurring using wiener filtering use wiener filtering to deblur some images modify the wiener filter derivation 66 74 to incorporate blur 75 2 discuss the resulting wiener filter in terms of its noise suppression and frequency boosting characteristics assuming that the blur kernel is gaussian and the image spectrum follows an inverse frequency law compute the frequency response of the wiener filter and compare it to the unsharp mask synthetically blur two of your sample images with gaussian blur kernels of different radii add noise and then perform wiener filtering 5 repeat the above experiment with a pillbox disc blurring kernel which is charac teristic of a finite aperture lens section 2 2 compare these results to gaussian blur kernels be sure to inspect your frequency plots 6 it has been suggested that regular apertures are anathema to de blurring because they introduce zeros in the sensed frequency spectrum veeraraghavan raskar agrawal et al show that this is indeed an issue if no prior model is assumed for the signal i e ps if a reasonable power spectrum is assumed is this still a problem do we still get banding or ringing artifacts ex 18 high quality image resampling implement several of the low pass filters pre sented in section 5 2 and also the discussion of the windowed sinc shown in table 2 and figure 29 feel free to implement other filters wolberg unser apply your filters to continuously resize an image both magnifying interpolating and minifying decimating it compare the resulting animations for several filters use both a a b c figure 65 sample images for testing the quality of resampling algorithms a a synthetic chirp b and c some high frequency images from the image compression community synthetic chirp image figure 65a and natural images with lots of high frequency detail figure 65b c you may find it helpful to write a simple visualization program that continuously plays the animations for two or more filters at once and that let you blink between different results discuss the merits and deficiencies of each filter as well as its tradeoff between speed and quality ex 19 pyramids construct an image pyramid the inputs should be a grayscale or color image a separable filter kernel and the number of desired levels implement at least the following kernels 2 2 block filtering burt and adelson s binomial kernel 6 burt and adelson a high quality seven or nine tap filter compare the visual quality of the various decimation filters also shift your input image by to pixels and compare the resulting decimated quarter size image sequence ex 20 pyramid blending write a program that takes as input two color images and a binary mask image and produces the laplacian pyramid blend of the two images construct the laplacian pyramid for each image 2 construct the gaussian pyramid for the two mask images the input image and its complement these particular images are available on the book s web site multiply each laplacian image by its corresponding mask and sum the images see figure 43 reconstruct the final image from the blended laplacian pyramid generalize your algorithm to input n images and a label image with values n the value can be reserved for no input discuss whether the weighted summation stage step needs to keep track of the total weight for renormalization or whether the math just works out use your algorithm either to blend two differently exposed image to avoid under and over exposed regions or to make a creative blend of two different scenes ex 21 wavelet construction and applications implement one of the wavelet families described in section 5 or by simoncelli and adelson as well as the basic lapla cian pyramid exercise 19 apply the resulting representations to one of the following two tasks compression compute the entropy in each band for the different wavelet implemen tations assuming a given quantization level say gray level to keep the rounding error acceptable quantize the wavelet coefficients and reconstruct the original im ages which technique performs better see simoncelli and adelson or any of the multitude of wavelet compression papers for some typical results denoising after computing the wavelets suppress small values using coring i e set small values to zero using a piecewise linear or other function compare the results of your denoising using different wavelet and pyramid representations ex 22 parametric image warping write the code to do affine and perspective image warps optionally bilinear as well try a variety of interpolants and report on their visual quality in particular discuss the following in a mip map selecting only the coarser level adjacent to the computed fractional level will produce a blurrier image while selecting the finer level will lead to aliasing explain why this is so and discuss whether blending an aliased and a blurred image tri linear mip mapping is a good idea when the ratio of the horizontal and vertical resampling rates becomes very different anisotropic the mip map performs even worse suggest some approaches to reduce such problems ex local image warping open an image and deform its appearance in one of the following ways click on a number of pixels and move drag them to new locations interpolate the resulting sparse displacement field to obtain a dense motion field sections 6 2 and 5 2 draw a number of lines in the image move the endpoints of the lines to specify their new positions and use the beier neely interpolation algorithm beier and neely discussed in section 6 2 to get a dense motion field overlay a spline control grid and move one grid point at a time optionally select the level of the deformation have a dense per pixel flow field and use a soft paintbrush to design a horizontal and vertical velocity field 5 optional prove whether the beier neely warp does or does not reduce to a sparse point based deformation as the line segments become shorter reduce to points ex forward warping given a displacement field from the previous exercise write a forward warping algorithm write a forward warper using splatting either nearest neighbor or soft accumulation section 6 2 write a two pass algorithm which forward warps the displacement field fills in small holes and then uses inverse warping shade gortler he et al compare the quality of these two algorithms ex 25 feature based morphing extend the warping code you wrote in exercise 23 to import two different images and specify correspondences point line or mesh based be tween the two images create a morph by partially warping the images towards each other and cross dissolving section 6 2 try using your morphing algorithm to perform an image rotation and discuss whether it behaves the way you want it to ex image editor extend the program you wrote in exercise 2 2 to import images and let you create a collage of pictures you should implement the following steps open up a new image in a separate window figure 3 66 there is a faint image of a rainbow visible in the right hand side of this picture can you think of a way to enhance it exercise 3 29 2 shift drag rubber band to crop a subregion or select whole image 3 paste into the current canvas select the deformation mode motion model translation rigid similarity affine or perspective 5 drag any corner of the outline to change its transformation 6 optional change the relative ordering of the images and which image is currently being manipulated the user should see the composition of the various images pieces on top of each other this exercise should be built on the image transformation classes supported in the soft ware library persistence of the created representation save and load should also be sup ported for each image save its transformation ex 3 27 texture mapped viewer extend the viewer you created in exercise 2 3 to in clude texture mapped polygon rendering augment each polygon with u v w coordinates into an image ex 3 28 image denoising implement at least two of the various image denoising tech niques described in this chapter and compare them on both synthetically noised image se quences and real world low light sequences does the performance of the algorithm de pend on the correct choice of noise level estimate can you draw any conclusions as to which techniques work better ex 3 29 rainbow enhancer challenging take a picture containing a rainbow such as figure 3 66 and enhance the strength saturation of the rainbow draw an arc in the image delineating the extent of the rainbow 2 fit an additive rainbow function explain why it is additive to this arc it is best to work with linearized pixel values using the spectrum as the cross section and estimating the width of the arc and the amount of color being added this is the trickiest part of the problem as you need to tease apart the low frequency rainbow pattern and the natural image hiding behind it 3 amplify the rainbow signal and add it back into the image re applying the gamma function if necessary to produce the final image ex 3 30 image deblocking challenging now that you have some good techniques to distinguish signal from noise develop a technique to remove the blocking artifacts that occur with jpeg at high compression settings section 2 3 3 your technique can be as simple as looking for unexpected edges along block boundaries to looking at the quantization step as a projection of a convex region of the transform coefficient space onto the corresponding quantized values does the knowledge of the compression factor which is available in the jpeg header information help you perform better deblocking 2 because the quantization occurs in the dct transformed ycbcr space 2 115 it may be preferable to perform the analysis in this space on the other hand image priors make more sense in an rgb space or do they decide how you will approach this dichotomy and discuss your choice 3 while you are at it since the ycbcr conversion is followed by a chrominance subsam pling stage before the dct see if you can restore some of the lost high frequency chrominance signal using one of the better restoration techniques discussed in this chapter if your camera has a raw jpeg mode how close can you come to the noise free true pixel values this suggestion may not be that useful since cameras generally use reasonably high quality settings for their raw jpeg models ex 3 31 inference in de blurring challenging write down the graphical model corre sponding to figure 3 59 for a non blind image deblurring problem i e one where the blur kernel is known ahead of time what kind of efficient inference optimization algorithms can you think of for solving such problems chapter feature detection and matching points and patches feature detectors 2 feature descriptors 3 feature matching feature tracking 5 application performance driven animation 2 edges 2 edge detection 2 2 edge linking 2 3 application edge editing and enhancement 3 lines 3 1 successive approximation 3 2 hough transforms 3 3 vanishing points 3 application rectangle detection additional reading 5 exercises a b c d figure 1 a variety of feature detectors and descriptors can be used to analyze describe and match images a point like interest operators brown szeliski and winder qc ieee b region like interest operators matas chum urban et al qc elsevier c edges elder and goldberg qc ieee d straight lines sinha steedly szeliski et al qc acm feature detection and matching are an essential component of many computer vision appli cations consider the two pairs of images shown in figure 2 for the first pair we may wish to align the two images so that they can be seamlessly stitched into a composite mosaic chapter 9 for the second pair we may wish to establish a dense set of correspondences so that a model can be constructed or an in between view can be generated chapter in either case what kinds of features should you detect and then match in order to establish such an alignment or set of correspondences think about this for a few moments before reading on the first kind of feature that you may notice are specific locations in the images such as mountain peaks building corners doorways or interestingly shaped patches of snow these kinds of localized feature are often called keypoint features or interest points or even corners and are often described by the appearance of patches of pixels surrounding the point location section 1 another class of important features are edges e g the profile of mountains against the sky section 2 these kinds of features can be matched based on their orien tation and local appearance edge profiles and can also be good indicators of object bound aries and occlusion events in image sequences edges can be grouped into longer curves and straight line segments which can be directly matched or analyzed to find vanishing points and hence internal and external camera parameters section 3 in this chapter we describe some practical approaches to detecting such features and also discuss how feature correspondences can be established across different images point features are now used in such a wide variety of applications that it is good practice to read and implement some of the algorithms from section 1 edges and lines provide information that is complementary to both keypoint and region based descriptors and are well suited to describing object boundaries and man made objects these alternative descriptors while extremely useful can be skipped in a short introductory course 1 points and patches point features can be used to find a sparse set of corresponding locations in different im ages often as a pre cursor to computing camera pose chapter 7 which is a prerequisite for computing a denser set of correspondences using stereo matching chapter such corre spondences can also be used to align different images e g when stitching image mosaics or performing video stabilization chapter 9 they are also used extensively to perform object instance and category recognition sections 3 and a key advantage of keypoints is that they permit matching even in the presence of clutter occlusion and large scale and orientation changes feature based correspondence techniques have been used since the early days of stereo figure 2 two pairs of images to be matched what kinds of feature might one use to establish a set of correspondences between these images matching hannah moravec hannah and have more recently gained pop ularity for image stitching applications zoghlami faugeras and deriche brown and lowe as well as fully automated modeling beardsley torr and zisserman schaffalitzky and zisserman brown and lowe snavely seitz and szeliski there are two main approaches to finding feature points and their correspondences the first is to find features in one image that can be accurately tracked using a local search tech nique such as correlation or least squares section 1 the second is to independently detect features in all the images under consideration and then match features based on their local appearance section 1 3 the former approach is more suitable when images are taken from nearby viewpoints or in rapid succession e g video sequences while the lat ter is more suitable when a large amount of motion or appearance change is expected e g in stitching together panoramas brown and lowe establishing correspondences in wide baseline stereo schaffalitzky and zisserman or performing object recognition fergus perona and zisserman in this section we split the keypoint detection and matching pipeline into four separate stages during the feature detection extraction stage section 1 1 each image is searched for locations that are likely to match well in other images at the feature description stage section 1 2 each region around detected keypoint locations is converted into a more com pact and stable invariant descriptor that can be matched against other descriptors the figure 3 image pairs with extracted patches below notice how some patches can be localized or matched with higher accuracy than others feature matching stage section 1 3 efficiently searches for likely matching candidates in other images the feature tracking stage section 1 is an alternative to the third stage that only searches a small neighborhood around each detected feature and is therefore more suitable for video processing a wonderful example of all of these stages can be found in david lowe s paper which describes the development and refinement of his scale invariant feature transform sift comprehensive descriptions of alternative techniques can be found in a series of survey and evaluation papers covering both feature detection schmid mohr and bauck hage mikolajczyk tuytelaars schmid et al tuytelaars and mikolajczyk and feature descriptors mikolajczyk and schmid shi and tomasi and triggs also provide nice reviews of feature detection techniques 1 1 feature detectors how can we find image locations where we can reliably find correspondences with other images i e what are good features to track shi and tomasi triggs look again at the image pair shown in figure 3 and at the three sample patches to see how well they might be matched or tracked as you may notice textureless patches are nearly impossible to localize patches with large contrast changes gradients are easier to localize although straight line segments at a single orientation suffer from the aperture problem horn and schunck lucas and kanade anandan i e it is only possible to align the patches along the direction normal to the edge direction figure 4b patches with a b c figure aperture problems for different image patches a stable corner like flow b classic aperture problem barber pole illusion c textureless region the two images yellow and red are overlaid the red vector u indicates the displacement between the patch centers and the w xi weighting function patch window is shown as a dark circle gradients in at least two significantly different orientations are the easiest to localize as shown schematically in figure 4a these intuitions can be formalized by looking at the simplest possible matching criterion for comparing two image patches i e their weighted summed square difference ewssd u w xi xi u xi 2 1 i where and are the two images being compared u u v is the displacement vector w x is a spatially varying weighting or window function and the summation i is over all the pixels in the patch note that this is the same formulation we later use to estimate motion between complete images section 1 when performing feature detection we do not know which other image locations the feature will end up being matched against therefore we can only compute how stable this metric is with respect to small variations in position u by comparing an image patch against itself which is known as an auto correlation function or surface eac u w xi xi u xi 2 2 i figure 5 1 note how the auto correlation surface for the textured flower bed figure 5b and the red cross in the lower right quadrant of figure 5a exhibits a strong minimum indicating that it can be well localized the correlation surface corresponding to the roof edge figure 5c has a strong ambiguity along one direction while the correlation surface corresponding to the cloud region figure 5d has no stable minimum 1 strictly speaking a correlation is the product of two patches 3 12 i m using the term here in a more qualitative sense the weighted sum of squared differences is often called an ssd surface section 1 a b c d figure 5 three auto correlation surfaces eac u shown as both grayscale images and surface plots a the original image is marked with three red crosses to denote where the auto correlation surfaces were computed b this patch is from the flower bed good unique minimum c this patch is from the roof edge one dimensional aperture problem and d this patch is from the cloud no good peak each grid point in figures b d is one value of u using a taylor series expansion of the image function xi u xi xi u lucas and kanade shi and tomasi we can approximate the auto correlation surface as where eac u w xi xi u xi 2 3 i w xi xi xi u xi 2 i w xi xi u 2 5 i ut a u 6 xi x y xi 7 is the image gradient at xi this gradient can be computed using a variety of techniques schmid mohr and bauckhage the classic harris detector harris and stephens uses a 2 1 1 2 filter but more modern variants schmid mohr and bauckhage triggs convolve the image with horizontal and vertical derivatives of a gaussian typically with σ 1 the auto correlation matrix a can be written as a w r ixiy l ixiy where we have replaced the weighted summations with discrete convolutions with the weight ing kernel w this matrix can be interpreted as a tensor multiband image where the outer products of the gradients i are convolved with a weighting function w to provide a per pixel estimate of the local quadratic shape of the auto correlation function as first shown by anandan and further discussed in section 1 3 and 44 the inverse of the matrix a provides a lower bound on the uncertainty in the location of a matching patch it is therefore a useful indicator of which patches can be reliably matched the easiest way to visualize and reason about this uncertainty is to perform an eigenvalue analysis of the auto correlation matrix a which produces two eigenvalues and two eigenvector directions figure 6 since the larger uncertainty depends on the smaller eigen value i e λ 1 2 it makes sense to find maxima in the smaller eigenvalue to locate good features to track shi and tomasi fo rstner harris while anandan and lucas and kanade were the first to analyze the uncertainty structure of the auto correlation matrix they did so in the context of asso ciating certainties with optic flow measurements fo rstner and harris and stephens figure 6 uncertainty ellipse corresponding to an eigenvalue analysis of the auto correlation matrix a were the first to propose using local maxima in rotationally invariant scalar measures derived from the auto correlation matrix to locate keypoints for the purpose of sparse feature matching schmid mohr and bauckhage triggs give more detailed histori cal reviews of feature detection algorithms both of these techniques also proposed using a gaussian weighting window instead of the previously used square patches which makes the detector response insensitive to in plane image rotations the minimum eigenvalue shi and tomasi is not the only quantity that can be used to find keypoints a simpler quantity proposed by harris and stephens is det a α trace a 2 α 2 9 with α 06 unlike eigenvalue analysis this quantity does not require the use of square roots and yet is still rotationally invariant and also downweights edge like features where triggs suggests using the quantity say with α 05 which also reduces the response at edges where aliasing errors sometimes inflate the smaller eigenvalue he also shows how the basic 2 2 hessian can be extended to parametric motions to detect points that are also accurately localizable in scale and rotation brown szeliski and winder on the other hand use the harmonic mean det a tr a which is a smoother function in the region where figure 7 shows isocontours of the various interest point operators from which we can see how the two eigenvalues are blended to determine the final interest value figure 7 isocontours of popular keypoint detection functions brown szeliski and winder each detector looks for points where the eigenvalues λ1 of a w i it are both large algorithm 1 outline of a basic feature detection algorithm a b c figure interest operator responses a sample image b harris response and c dog response the circle sizes and colors indicate the scale at which each interest point was detected notice how the two detectors tend to respond at complementary locations the steps in the basic auto correlation based keypoint detector are summarized in algo rithm 1 figure shows the resulting interest operator responses for the classic harris detector as well as the difference of gaussian dog detector discussed below adaptive non maximal suppression anms while most feature detectors simply look for local maxima in the interest function this can lead to an uneven distribution of feature points across the image e g points will be denser in regions of higher contrast to mitigate this problem brown szeliski and winder only detect features that are both local maxima and whose response value is significantly greater than that of all of its neigh bors within a radius r figure 9c d they devise an efficient way to associate suppression radii with all local maxima by first sorting them by their response strength and then creating a second list sorted by decreasing suppression radius brown szeliski and winder figure 9 shows a qualitative comparison of selecting the top n features and using anms measuring repeatability given the large number of feature detectors that have been de veloped in computer vision how can we decide which ones to use schmid mohr and bauckhage were the first to propose measuring the repeatability of feature detectors which they define as the frequency with which keypoints detected in one image are found within e say e 1 5 pixels of the corresponding location in a transformed image in their paper they transform their planar images by applying rotations scale changes illumination changes viewpoint changes and adding noise they also measure the information content available at each detected feature point which they define as the entropy of a set of rotation ally invariant local grayscale descriptors among the techniques they survey they find that the improved gaussian derivative version of the harris operator with σd 1 scale of the derivative gaussian and σi 2 scale of the integration gaussian works best a strongest b strongest c anms r 24 d anms r figure 9 adaptive non maximal suppression anms brown szeliski and winder qc ieee the upper two images show the strongest and interest points while the lower two images show the interest points selected with adaptive non maximal sup pression along with the corresponding suppression radius r note how the latter features have a much more uniform spatial distribution across the image scale invariance in many situations detecting features at the finest stable scale possible may not be appro priate for example when matching images with little high frequency detail e g clouds fine scale features may not exist one solution to the problem is to extract features at a variety of scales e g by performing the same operations at multiple resolutions in a pyramid and then matching features at the same level this kind of approach is suitable when the images being matched do not undergo large scale changes e g when matching successive aerial images taken from an airplane or stitching panoramas taken with a fixed focal length camera figure 10 shows the output of one such approach the multi scale oriented patch detector of brown szeliski and winder for which responses at five different scales are shown however for most object recognition applications the scale of the object in the image figure 10 multi scale oriented patches mops extracted at five pyramid levels brown szeliski and winder qc ieee the boxes show the feature orientation and the region from which the descriptor vectors are sampled is unknown instead of extracting features at many different scales and then matching all of them it is more efficient to extract features that are stable in both location and scale lowe mikolajczyk and schmid early investigations into scale selection were performed by lindeberg who first proposed using extrema in the laplacian of gaussian log function as interest point locations based on this work lowe proposed computing a set of sub octave difference of gaussian filters figure 11a looking for space scale maxima in the re sulting structure figure 11b and then computing a sub pixel space scale location using a quadratic fit brown and lowe the number of sub octave levels was determined after careful empirical investigation to be three which corresponds to a quarter octave pyramid which is the same as used by triggs as with the harris operator pixels where there is strong asymmetry in the local curvature of the indicator function in this case the dog are rejected this is implemented by first computing the local hessian of the difference image d h dxx dxy dxy dyy l 12 and then rejecting keypoints for which tr h 2 det h 10 13 scale next octave scale first octave gaussian difference of gaussian dog a b figure scale space feature detection using a sub octave difference of gaussian pyra mid lowe qc springer a adjacent levels of a sub octave gaussian pyramid are subtracted to produce difference of gaussian images b extrema maxima and minima in the resulting volume are detected by comparing a pixel to its 26 neighbors while lowe s scale invariant feature transform sift performs well in practice it is not based on the same theoretical foundation of maximum spatial stability as the auto correlation based detectors in fact its detection locations are often complementary to those produced by such techniques and can therefore be used in conjunction with these other approaches in order to add a scale selection mechanism to the harris corner detector mikolajczyk and schmid evaluate the laplacian of gaussian function at each detected harris point in a multi scale pyramid and keep only those points for which the laplacian is extremal larger or smaller than both its coarser and finer level values an optional iterative refinement for both scale and position is also proposed and evaluated additional examples of scale invariant region detectors are discussed by mikolajczyk tuytelaars schmid et al tuytelaars and mikolajczyk rotational invariance and orientation estimation in addition to dealing with scale changes most image matching and object recognition algo rithms need to deal with at least in plane image rotation one way to deal with this problem is to design descriptors that are rotationally invariant schmid and mohr but such descriptors have poor discriminability i e they map different looking patches to the same descriptor figure 12 a dominant orientation estimate can be computed by creating a histogram of all the gradient orientations weighted by their magnitudes or after thresholding out small gradients and then finding the significant peaks in this distribution lowe qc springer a better method is to estimate a dominant orientation at each detected keypoint once the local orientation and scale of a keypoint have been estimated a scaled and oriented patch around the detected point can be extracted and used to form a feature descriptor figures 10 and 17 the simplest possible orientation estimate is the average gradient within a region around the keypoint if a gaussian weighting function is used brown szeliski and winder this average gradient is equivalent to a first order steerable filter section 3 2 3 i e it can be computed using an image convolution with the horizontal and vertical derivatives of gaus sian filter freeman and adelson in order to make this estimate more reliable it is usually preferable to use a larger aggregation window gaussian kernel size than detection window brown szeliski and winder the orientations of the square boxes shown in figure 10 were computed using this technique sometimes however the averaged signed gradient in a region can be small and therefore an unreliable indicator of orientation a more reliable technique is to look at the histogram of orientations computed around the keypoint lowe computes a bin histogram of edge orientations weighted by both gradient magnitude and gaussian distance to the cen ter finds all peaks within 80 of the global maximum and then computes a more accurate orientation estimate using a three bin parabolic fit figure 12 affine invariance while scale and rotation invariance are highly desirable for many applications such as wide baseline stereo matching pritchett and zisserman schaffalitzky and zisserman or location recognition chum philbin sivic et al full affine invariance is preferred figure 13 affine region detectors used to match two images taken from dramatically different viewpoints mikolajczyk and schmid qc springer xi0 a 1 2xi a 1 2xi rxi 1 1 1 x1 figure affine normalization using the second moment matrices as described by miko lajczyk tuytelaars schmid et al qc springer after image coordinates are trans formed using the matrices a 1 2 and a 1 2 they are related by a pure rotation r which 1 can be estimated using a dominant orientation technique affine invariant detectors not only respond at consistent locations after scale and orientation changes they also respond consistently across affine deformations such as local perspective foreshortening figure 13 in fact for a small enough patch any continuous image warping can be well approximated by an affine deformation to introduce affine invariance several authors have proposed fitting an ellipse to the auto correlation or hessian matrix using eigenvalue analysis and then using the principal axes and ratios of this fit as the affine coordinate frame lindeberg and garding baumberg mikolajczyk and schmid mikolajczyk tuytelaars schmid et al tuyte laars and mikolajczyk figure shows how the square root of the moment matrix can be used to transform local patches into a frame which is similar up to rotation another important affine invariant region detector is the maximally stable extremal region mser detector developed by matas chum urban et al to detect msers binary regions are computed by thresholding the image at all possible gray levels the technique therefore only works for grayscale images this operation can be performed efficiently by first sorting all pixels by gray value and then incrementally adding pixels to each connected component as the threshold is changed niste r and stewe nius as the threshold is changed the area of each component region is monitored regions whose rate of change of area with respect to the threshold is minimal are defined as maximally stable such regions figure 15 maximally stable extremal regions msers extracted and matched from a number of images matas chum urban et al qc elsevier figure 16 feature matching how can we extract local descriptors that are invariant to inter image variations and yet still discriminative enough to establish correct correspon dences are therefore invariant to both affine geometric and photometric linear bias gain or smooth monotonic transformations figure 15 if desired an affine coordinate frame can be fit to each detected region using its moment matrix the area of feature point detectors continues to be very active with papers appearing ev ery year at major computer vision conferences xiao and shah koethe carneiro and jepson kenney zuliani and manjunath bay tuytelaars and van gool platel balmachnova florack et al rosten and drummond mikolajczyk tuyte laars schmid et al survey a number of popular affine region detectors and provide experimental comparisons of their invariance to common image transformations such as scal ing rotations noise and blur these experimental results code and pointers to the surveyed papers can be found on their web site at http www robots ox ac uk vgg research affine of course keypoints are not the only features that can be used for registering images zoghlami faugeras and deriche use line segments as well as point like features to estimate homographies between pairs of images whereas bartoli coquerelle and sturm use line segments with local correspondences along the edges to extract structure and motion tuytelaars and van gool use affine invariant regions to detect corre spondences for wide baseline stereo matching whereas kadir zisserman and brady detect salient regions where patch entropy and its rate of change with scale are locally max imal corso and hager use a related technique to fit oriented gaussian kernels to homogeneous regions more details on techniques for finding and matching curves lines and regions can be found later in this chapter figure 17 mops descriptors are formed using an sampling of bias and gain nor malized intensity values with a sample spacing of five pixels relative to the detection scale brown szeliski and winder qc ieee this low frequency sampling gives the features some robustness to interest point location error and is achieved by sampling at a higher pyramid level than the detection scale 1 2 feature descriptors after detecting features keypoints we must match them i e we must determine which features come from corresponding locations in different images in some situations e g for video sequences shi and tomasi or for stereo pairs that have been rectified zhang deriche faugeras et al loop and zhang scharstein and szeliski the lo cal motion around each feature point may be mostly translational in this case simple error metrics such as the sum of squared differences or normalized cross correlation described in section 1 can be used to directly compare the intensities in small patches around each feature point the comparative study by mikolajczyk and schmid discussed below uses cross correlation because feature points may not be exactly located a more accurate matching score can be computed by performing incremental motion refinement as described in section 8 1 3 but this can be time consuming and can sometimes even decrease perfor mance brown szeliski and winder in most cases however the local appearance of features will change in orientation and scale and sometimes even undergo affine deformations extracting a local scale orientation or affine frame estimate and then using this to resample the patch before forming the feature descriptor is thus usually preferable figure 17 even after compensating for these changes the local appearance of image patches will usually still vary from image to image how can we make image descriptors more invariant to such changes while still preserving discriminability between different non corresponding patches figure 16 mikolajczyk and schmid review some recently developed view invariant local image descriptors and experimentally compare their performance be low we describe a few of these descriptors in more detail bias and gain normalization mops for tasks that do not exhibit large amounts of fore shortening such as image stitching simple normalized intensity patches perform reasonably well and are simple to implement brown szeliski and winder figure 17 in or der to compensate for slight inaccuracies in the feature point detector location orientation and scale these multi scale oriented patches mops are sampled at a spacing of five pixels relative to the detection scale using a coarser level of the image pyramid to avoid aliasing to compensate for affine photometric variations linear exposure changes or bias and gain 3 3 patch intensities are re scaled so that their mean is zero and their variance is one scale invariant feature transform sift sift features are formed by computing the gradient at each pixel in a 16 16 window around the detected keypoint using the appropriate level of the gaussian pyramid at which the keypoint was detected the gradient magnitudes are downweighted by a gaussian fall off function shown as a blue circle in figure 18a in order to reduce the influence of gradients far from the center as these are more affected by small misregistrations in each quadrant a gradient orientation histogram is formed by conceptually adding the weighted gradient value to one of eight orientation histogram bins to reduce the effects of location and dominant orientation misestimation each of the original weighted gradient magnitudes is softly added to 2 2 2 histogram bins using trilinear interpolation softly distributing values to adjacent histogram bins is generally a good idea in any appli cation where histograms are being computed e g for hough transforms section 3 2 or local histogram equalization section 3 1 the resulting non negative values form a raw version of the sift descriptor vector to reduce the effects of contrast or gain additive variations are already removed by the gra dient the d vector is normalized to unit length to further make the descriptor robust to other photometric variations values are clipped to 2 and the resulting vector is once again renormalized to unit length pca sift ke and sukthankar propose a simpler way to compute descriptors in spired by sift it computes the x and y gradient derivatives over a 39 patch and then reduces the resulting dimensional vector to using principal component analysis pca section 2 1 and appendix a 1 2 another popular variant of sift is surf bay tuytelaars and van gool which uses box filters to approximate the derivatives and integrals used in sift gradient location orientation histogram gloh this descriptor developed by miko lajczyk and schmid is a variant on sift that uses a log polar binning structure instead of the four quadrants used by lowe figure 19 the spatial bins are of radius 6 a image gradients b keypoint descriptor figure 18 a schematic representation of lowe s scale invariant feature transform sift a gradient orientations and magnitudes are computed at each pixel and weighted by a gaussian fall off function blue circle b a weighted gradient orientation histogram is then computed in each subregion using trilinear interpolation while this figure shows an 8 8 pixel patch and a 2 2 descriptor array lowe s actual implementation uses 16 16 patches and a array of eight bin histograms 11 and 15 with eight angular bins except for the central region for a total of 17 spa tial bins and 16 orientation bins the dimensional histogram is then projected onto a dimensional descriptor using pca trained on a large database in their evaluation mikolajczyk and schmid found that gloh which has the best performance overall outperforms sift by a small margin steerable filters steerable filters section 3 2 3 are combinations of derivative of gaus sian filters that permit the rapid computation of even and odd symmetric and anti symmetric edge like and corner like features at all possible orientations freeman and adelson because they use reasonably broad gaussians they too are somewhat insensitive to localiza tion and orientation errors performance of local descriptors among the local descriptors that mikolajczyk and schmid compared they found that gloh performed best followed closely by sift see fig ure 25 they also present results for many other descriptors not covered in this book the field of feature descriptors continues to evolve rapidly with some of the newer tech niques looking at local color information van de weijer and schmid abdel hakim and farag winder and brown develop a multi stage framework for feature descriptor computation that subsumes both sift and gloh figure 20a and also allows them to learn optimal parameters for newer descriptors that outperform previous hand tuned a image gradients b keypoint descriptor figure 19 the gradient location orientation histogram gloh descriptor uses log polar bins instead of square bins to compute orientation histograms mikolajczyk and schmid descriptors hua brown and winder extend this work by learning lower dimensional projections of higher dimensional descriptors that have the best discriminative power both of these papers use a database of real world image patches figure 20b obtained by sam pling images at locations that were reliably matched using a robust structure from motion algorithm applied to internet photo collections snavely seitz and szeliski goesele snavely curless et al in concurrent work tola lepetit and fua developed a similar daisy descriptor for dense stereo matching and optimized its parameters based on ground truth stereo data while these techniques construct feature detectors that optimize for repeatability across all object classes it is also possible to develop class or instance specific feature detectors that maximize discriminability from other classes ferencz learned miller and malik 1 3 feature matching once we have extracted features and their descriptors from two or more images the next step is to establish some preliminary feature matches between these images in this section we divide this problem into two separate components the first is to select a matching strategy which determines which correspondences are passed on to the next stage for further process ing the second is to devise efficient data structures and algorithms to perform this matching as quickly as possible see the discussion of related techniques in section 3 2 a b figure 20 spatial summation blocks for sift gloh and some newly developed feature descriptors winder and brown qc ieee a the parameters for the new features e g their gaussian weights are learned from a training database of b matched real world image patches obtained from robust structure from motion applied to internet photo collec tions hua brown and winder matching strategy and error rates determining which feature matches are reasonable to process further depends on the context in which the matching is being performed say we are given two images that overlap to a fair amount e g for image stitching as in figure 16 or for tracking objects in a video we know that most features in one image are likely to match the other image although some may not match because they are occluded or their appearance has changed too much on the other hand if we are trying to recognize how many known objects appear in a clut tered scene figure 21 most of the features may not match furthermore a large number of potentially matching objects must be searched which requires more efficient strategies as described below to begin with we assume that the feature descriptors have been designed so that eu clidean vector magnitude distances in feature space can be directly used for ranking poten tial matches if it turns out that certain parameters axes in a descriptor are more reliable than others it is usually preferable to re scale these axes ahead of time e g by determin ing how much they vary when compared against other known good matches hua brown and winder a more general process which involves transforming feature vectors into a new scaled basis is called whitening and is discussed in more detail in the context of eigenface based face recognition section 2 1 given a euclidean distance metric the simplest matching strategy is to set a threshold maximum distance and to return all matches from other images within this threshold set ting the threshold too high results in too many false positives i e incorrect matches being returned setting the threshold too low results in too many false negatives i e too many correct matches being missed figure 22 we can quantify the performance of a matching algorithm at a particular threshold by figure 21 recognizing objects in a cluttered scene lowe qc springer two of the training images in the database are shown on the left these are matched to the cluttered scene in the middle using sift features shown as small squares in the right image the affine warp of each recognized database image onto the scene is shown as a larger parallelogram in the right image figure 22 false positives and negatives the black digits 1 and 2 are features being matched against a database of features in other images at the current threshold setting the solid circles the green 1 is a true positive good match the blue 1 is a false negative failure to match and the red 3 is a false positive incorrect match if we set the threshold higher the dashed circles the blue 1 becomes a true positive but the brown becomes an additional false positive true matches true non matches predicted matches predicted non matches table 1 the number of matches correctly and incorrectly estimated by a feature matching algorithm showing the number of true positives tp false positives fp false negatives fn and true negatives tn the columns sum up to the actual number of positives p and negatives n while the rows sum up to the predicted number of positives p and negatives n the formulas for the true positive rate tpr the false positive rate fpr the positive predictive value ppv and the accuracy acc are given in the text first counting the number of true and false matches and match failures using the following definitions fawcett tp true positives i e number of correct matches fn false negatives matches that were not correctly detected fp false positives proposed matches that are incorrect tn true negatives non matches that were correctly rejected table 1 shows a sample confusion matrix contingency table containing such numbers we can convert these numbers into unit rates by defining the following quantities fawcett true positive rate tpr false positive rate fpr tpr tp tp fn fpr fp fp tn tp 14 p fp 15 n positive predictive value ppv ppv tp tp 16 accuracy acc tp fp p tp tn acc p n 17 1 8 1 tp tn 1 fp fn false positive rate θ d a b figure 23 roc curve and its related rates a the roc curve plots the true positive rate against the false positive rate for a particular combination of feature extraction and match ing algorithms ideally the true positive rate should be close to 1 while the false positive rate is close to the area under the roc curve auc is often used as a single scalar measure of algorithm performance alternatively the equal error rate is sometimes used b the distribution of positives matches and negatives non matches as a function of inter feature distance d as the threshold θ is increased the number of true positives tp and false positives fp increases in the information retrieval or document retrieval literature baeza yates and ribeiro neto manning raghavan and schu tze the term precision how many returned documents are relevant is used instead of ppv and recall what fraction of relevant docu ments was found is used instead of tpr any particular matching strategy at a particular threshold or parameter setting can be rated by the tpr and fpr numbers ideally the true positive rate will be close to 1 and the false positive rate close to as we vary the matching threshold we obtain a family of such points which are collectively known as the receiver operating characteristic roc curve fawcett figure 23a the closer this curve lies to the upper left corner i e the larger the area under the curve auc the better its performance figure 23b shows how we can plot the number of matches and non matches as a function of inter feature distance d these curves can then be used to plot an roc curve exercise 3 the roc curve can also be used to calculate the mean average precision which is the average precision ppv as you vary the threshold to select the best results then the two top results etc the problem with using a fixed threshold is that it is difficult to set the useful range figure 24 fixed threshold nearest neighbor and nearest neighbor distance ratio matching at a fixed distance threshold dashed circles descriptor da fails to match db and dd incorrectly matches dc and de if we pick the nearest neighbor da correctly matches db but dd incorrectly matches dc using nearest neighbor distance ratio nndr matching the small nndr correctly matches da with db and the large nndr correctly rejects matches for dd of thresholds can vary a lot as we move to different parts of the feature space lowe mikolajczyk and schmid a better strategy in such cases is to simply match the nearest neighbor in feature space since some features may have no matches e g they may be part of background clutter in object recognition or they may be occluded in the other image a threshold is still used to reduce the number of false positives ideally this threshold itself will adapt to different regions of the feature space if sufficient training data is available hua brown and winder it is sometimes possible to learn different thresholds for different features often however we are simply given a collection of images to match e g when stitching images or constructing models from unordered photo collections brown and lowe snavely seitz and szeliski in this case a useful heuristic can be to compare the nearest neighbor distance to that of the second nearest neighbor preferably taken from an image that is known not to match the target e g a different object in the database brown and lowe lowe we can define this nearest neighbor distance ratio mikolajczyk and schmid as nndr db 18 dc where and are the nearest and second nearest neighbor distances da is the target descriptor and db and dc are its closest two neighbors figure 24 the effects of using these three different matching strategies for the feature descriptors evaluated by mikolajczyk and schmid are shown in figure 25 as you can see the nearest neighbor and nndr strategies produce improved roc curves 1 gloh cross correlation 9 sift gradient moments 8 pca sift complex filters shape context differential invariants 7 6 spin hes lap gloh steerable filters 5 4 3 2 1 1 2 3 4 5 6 7 8 9 1 1 precision a 1 gloh cross correlation 1 gloh cross correlation 9 sift gradient moments 9 sift gradient moments pca sift 8 shape context complex filters differential invariants 8 pca sift shape context complex filters differential invariants 7 6 spin hes lap gloh steerable filters 7 6 spin hes lap gloh steerable filters 5 5 0 4 0 4 0 3 0 3 0 2 0 2 0 1 0 1 0 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 1 precision 0 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 1 precision b c figure 4 25 performance of the feature descriptors evaluated by mikolajczyk and schmid qc ieee shown for three matching strategies a fixed threshold b nearest neighbor c nearest neighbor distance ratio nndr note how the ordering of the algo rithms does not change that much but the overall performance varies significantly between the different matching strategies figure 4 26 the three haar wavelet coefficients used for hashing the mops descriptor de vised by brown szeliski and winder are computed by summing each 8 8 normalized patch over the light and dark gray regions and taking their difference efficient matching once we have decided on a matching strategy we still need to search efficiently for poten tial candidates the simplest way to find all corresponding feature points is to compare all features against all other features in each pair of potentially matching images unfortunately this is quadratic in the number of extracted features which makes it impractical for most applications a better approach is to devise an indexing structure such as a multi dimensional search tree or a hash table to rapidly search for features near a given feature such indexing struc tures can either be built for each image independently which is useful if we want to only consider certain potential matches e g searching for a particular object or globally for all the images in a given database which can potentially be faster since it removes the need to it erate over each image for extremely large databases millions of images or more even more efficient structures based on ideas from document retrieval e g vocabulary trees niste r and stewe nius can be used section 14 3 2 one of the simpler techniques to implement is multi dimensional hashing which maps descriptors into fixed size buckets based on some function applied to each descriptor vector at matching time each new feature is hashed into a bucket and a search of nearby buckets is used to return potential candidates which can then be sorted or graded to determine which are valid matches a simple example of hashing is the haar wavelets used by brown szeliski and winder in their mops paper during the matching structure construction each 8 8 scaled oriented and normalized mops patch is converted into a three element index by perform ing sums over different quadrants of the patch figure 4 26 the resulting three values are normalized by their expected standard deviations and then mapped to the two of b 10 nearest bins the three dimensional indices formed by concatenating the three quantized values are used to index the 23 8 bins where the feature is stored added at query time only the primary closest indices are used so only a single three dimensional bin needs to a b figure 4 27 k d tree and best bin first bbf search beis and lowe qc ieee a the spatial arrangement of the axis aligned cutting planes is shown using dashed lines individual data points are shown as small diamonds b the same subdivision can be repre sented as a tree where each interior node represents an axis aligned cutting plane e g the top node cuts along dimension at value and each leaf node is a data point during a bbf search a query point denoted by first looks in its containing bin d and then in its nearest adjacent bin b rather than its closest neighbor in the tree c be examined the coefficients in the bin can then be used to select k approximate nearest neighbors for further processing such as computing the nndr a more complex but more widely applicable version of hashing is called locality sen sitive hashing which uses unions of independently computed hashing functions to index the features gionis indyk and motwani shakhnarovich darrell and indyk shakhnarovich viola and darrell extend this technique to be more sensitive to the distribution of points in parameter space which they call parameter sensitive hashing even more recent work converts high dimensional descriptor vectors into binary codes that can be compared using hamming distances torralba weiss and fergus weiss torralba and fergus or that can accommodate arbitrary kernel functions kulis and grauman raginsky and lazebnik another widely used class of indexing structures are multi dimensional search trees the best known of these are k d trees also often written as kd trees which divide the multi dimensional feature space along alternating axis aligned hyperplanes choosing the threshold along each axis so as to maximize some criterion such as the search tree balance samet figure 4 27 shows an example of a two dimensional k d tree here eight different data points a h are shown as small diamonds arranged on a two dimensional plane the k d tree recursively splits this plane along axis aligned horizontal or vertical cutting planes each split can be denoted using the dimension number and split value figure 4 27b the splits are arranged so as to try to balance the tree i e to keep its maximum depth as small as possible at query time a classic k d tree search first locates the query point in its appropriate bin d and then searches nearby leaves in the tree c b until it can guarantee that the nearest neighbor has been found the best bin first bbf search beis and lowe searches bins in order of their spatial proximity to the query point and is therefore usually more efficient many additional data structures have been developed over the years for solving nearest neighbor problems arya mount netanyahu et al liang liu xu et al hjalta son and samet for example nene and nayar developed a technique they call slicing that uses a series of binary searches on the point list sorted along different dimen sions to efficiently cull down a list of candidate points that lie within a hypercube of the query point grauman and darrell reweight the matches at different levels of an indexing tree which allows their technique to be less sensitive to discretization errors in the tree con struction niste r and stewe nius use a metric tree which compares feature descriptors to a small number of prototypes at each level in a hierarchy the resulting quantized visual words can then be used with classical information retrieval document relevance techniques to quickly winnow down a set of potential candidates from a database of millions of images section 14 3 2 muja and lowe compare a number of these approaches introduce a new one of their own priority search on hierarchical k means trees and conclude that mul tiple randomized k d trees often provide the best performance despite all of this promising work the rapid computation of image feature correspondences remains a challenging open research problem feature match verification and densification once we have some hypothetical putative matches we can often use geometric alignment section 6 1 to verify which matches are inliers and which ones are outliers for example if we expect the whole image to be translated or rotated in the matching view we can fit a global geometric transform and keep only those feature matches that are sufficiently close to this estimated transformation the process of selecting a small set of seed matches and then verifying a larger set is often called random sampling or ransac section 6 1 4 once an initial set of correspondences has been established some systems look for additional matches e g by looking for additional correspondences along epipolar lines section 11 1 or in the vicinity of estimated locations based on the global transform these topics are discussed further in sections 6 1 11 2 and 14 3 1 4 1 4 feature tracking an alternative to independently finding features in all candidate images and then matching them is to find a set of likely feature locations in a first image and to then search for their corresponding locations in subsequent images this kind of detect then track approach is more widely used for video tracking applications where the expected amount of motion and appearance deformation between adjacent frames is expected to be small the process of selecting good features to track is closely related to selecting good features for more general recognition applications in practice regions containing high gradients in both directions i e which have high eigenvalues in the auto correlation matrix 4 8 provide stable locations at which to find correspondences shi and tomasi in subsequent frames searching for locations where the corresponding patch has low squared difference 4 1 often works well enough however if the images are undergo ing brightness change explicitly compensating for such variations 8 9 or using normalized cross correlation 8 11 may be preferable if the search range is large it is also often more efficient to use a hierarchical search strategy which uses matches in lower resolution images to provide better initial guesses and hence speed up the search section 8 1 1 alternatives to this strategy involve learning what the appearance of the patch being tracked should be and then searching for it in the vicinity of its predicted position avidan jurie and dhome williams blake and cipolla these topics are all covered in more detail in section 8 1 3 if features are being tracked over longer image sequences their appearance can undergo larger changes you then have to decide whether to continue matching against the originally detected patch feature or to re sample each subsequent frame at the matching location the former strategy is prone to failure as the original patch can undergo appearance changes such as foreshortening the latter runs the risk of the feature drifting from its original location to some other location in the image shi and tomasi mathematically small mis registration errors compound to create a markov random walk which leads to larger drift over time a preferable solution is to compare the original patch to later image locations using an affine motion model section 8 2 shi and tomasi first compare patches in neigh boring frames using a translational model and then use the location estimates produced by this step to initialize an affine registration between the patch in the current frame and the base frame where a feature was first detected figure 4 28 in their system features are only detected infrequently i e only in regions where tracking has failed in the usual case an area around the current predicted location of the feature is searched with an incremental reg istration algorithm section 8 1 3 the resulting tracker is often called the kanade lucas tomasi klt tracker figure 4 28 feature tracking using an affine motion model shi and tomasi qc ieee top row image patch around the tracked feature location bottom row image patch after warping back toward the first frame using an affine deformation even though the speed sign gets larger from frame to frame the affine transformation maintains a good resemblance between the original and subsequent tracked frames since their original work on feature tracking shi and tomasi s approach has generated a string of interesting follow on papers and applications beardsley torr and zisserman use extended feature tracking combined with structure from motion chapter 7 to incremen tally build up sparse models from video sequences kang szeliski and shum tie together the corners of adjacent regularly gridded patches to provide some additional stability to the tracking at the cost of poorer handling of occlusions tommasini fusiello trucco et al provide a better spurious match rejection criterion for the basic shi and tomasi algorithm collins and liu provide improved mechanisms for feature selec tion and dealing with larger appearance changes over time and shafique and shah develop algorithms for feature matching data association for videos with large numbers of moving objects or points yilmaz javed and shah and lepetit and fua survey the larger field of object tracking which includes not only feature based techniques but also alternative techniques based on contour and region section 5 1 one of the newest developments in feature tracking is the use of learning algorithms to build special purpose recognizers to rapidly search for matching features anywhere in an image lepetit pilet and fua hinterstoisser benhimane navab et al rogez rihan ramalingam et al o zuysal calonder lepetit et al 2 by taking the time to train classifiers on sample patches and their affine deformations extremely fast and reliable feature detectors can be constructed which enables much faster motions to be supported figure 4 29 coupling such features to deformable models pilet lepetit and fua or structure from motion algorithms klein and murray can result in even higher stability 2 see also my previous comment on earlier work in learning based tracking avidan jurie and dhome williams blake and cipolla figure 4 29 real time head tracking using the fast trained classifiers of lepetit pilet and fua qc ieee 4 1 5 application performance driven animation one of the most compelling applications of fast feature tracking is performance driven an imation i e the interactive deformation of a graphics model based on tracking a user s motions williams litwinowicz and williams lepetit pilet and fua buck finkelstein jacobs et al present a system that tracks a user s facial expres sions and head motions and then uses them to morph among a series of hand drawn sketches an animator first extracts the eye and mouth regions of each sketch and draws control lines over each image figure 4 30a at run time a face tracking system toyama deter mines the current location of these features figure 4 30b the animation system decides which input images to morph based on nearest neighbor feature appearance matching and triangular barycentric interpolation it also computes the global location and orientation of the head from the tracked features the resulting morphed eye and mouth regions are then composited back into the overall head model to yield a frame of hand drawn animation fig ure 4 30d in more recent work barnes jacobs sanders et al watch users animate paper cutouts on a desk and then turn the resulting motions and drawings into seamless anima tions where we have computed the integral over φ using the same method as in equa tion unfortunately we still cannot compute the remaining integral in closed form so we instead take the approach of maximizing over hidden variables to give an approximate expression for the marginal likelihood p r w x max rnormw xt h dtt gamhd ν ν as long as the true distribution over the hidden variables is concentrated tightly around the mode this will be a reasonable approximation when hd takes a large value the prior has a small variance hd and the associated coefficient φd will be forced to be close to zero in effect this means that the dth dimension of x does not contribute to the solution and can be dropped from the equations the general approach to fitting the model is now clear there are two unknown quantities the variance and the hidden variables h and we alternately update each to maximize the log marginal likelihood to update the hidden variables we take the derivative of the log of this expression with respect to h equate the result to zero and re arrange to get the iteration hnew hdσdd ν d ν where µd is the dth element of the mean µ of the posterior distribution over the weights φ and σdd is the dth element of the diagonal of the covariance σ of the posterior distribution over the weights equation so that µ a σ a and a is defined as a xxt h to update the variance we take the derivative of the log of this expression with respect to equate the result to zero and simplify to get new d h σ w xµ w xµ details about how these non obvious update equations were generated can be found in section of bishop and tipping regression models figure sparse linear regression a bayesian linear regression from two dimensional data the background color represents the mean µw x of the gaussian prediction p r w x for w the variance of p r w x is not shown the color of the data points indicates the training value w so for a perfect regression fit this should match exactly the surrounding color here the elements of φ take arbitrary values and so the gradient of the function points in an arbitrary direction b sparse linear regression here the elements of φ are encouraged to be zero where they are not necessary to explain the data the algorithm has found a good fit where the second element of φ is zero and so there is no dependence on the vertical axis in between each of these updates the posterior mean µ and variance σ should be recalculated in practice we choose a very small value for the degrees of freedom ν to encourage sparseness we may also restrict the maximum possible values of the hidden variables hi to ensure numerical stability at the end of the training all dimensions of φ where the hidden variable hd is large say are discarded figure shows an example fit to some two dimensional data the sparse solution depends only on one of the two possible directions and so is twice as efficient in principle a nonlinear version of this algorithm can be generated by trans forming the input data x to create the vector z f x however if the transformed data z is very high dimensional we will need correspondingly more hidden vari ables hd to cope with these dimensions obviously this idea will not transfer to kernel functions where the dimensionality of the transformed data could be infinite to resolve this problem we will develop the relevance vector machine this model also imposes sparsity but it does so in a way that makes the final prediction depend only on a sparse subset of the training data rather than a sparse subset of the observed dimensions before we can investigate this model we must develop a version of linear regression where there is one parameter per data example rather than one per observed dimension this model is known as dual linear regression dual linear regression dual linear regression in the standard linear regression model the parameter vector φ contains d entries corresponding to each of the d dimensions of the possibly transformed input data in the dual formulation we re parameterize the model in terms of a vector ψ which has i entries where i is the number of training examples this is more efficient in situations where we are training a model where the input data are high dimensional but the number of examples is small i d and leads to other interesting models such as relevance vector regression dual model in the dual model we retain the original linear dependence of the prediction w on the input data x so that problem p r wi xi normx φt xi however we now represent the slope parameters φ as a weighted sum of the ob served data points so that φ xψ where ψ is a i vector representing the weights figure we term this the dual parameterization notice that if there are fewer data examples than data dimensions then there will be fewer unknowns here than in the standard linear regression model and hence learning and inference will be more efficient note that the term dual is heavily overloaded in computer science and the reader should be careful not to confuse this use with its other meanings if the data dimensionality d is less than the number of examples i then we can find parameters ψ to represent any gradient vector φ however if d i often true in vision where measurements can be high dimensional then the vector xψ can only span a subspace of the possible gradient vectors however this is not a problem if there was no variation in the data x in a given direction in space then the gradient along that axis should be zero anyway since we have no information about how the world state w varies in this direction making the substitution from equation the regression model becomes p r wi xi θ normxi ψt xt xi or writing all of the data likelihoods in one term p r w x θ normw xt xψ where the parameters of the model are θ ψ we now consider how to learn this model using both the maximum likelihood and bayesian approaches figure dual variables two di mensional training data xi i and associated world state wi i indi cated by marker color the linear re gression parameter φ determines the direction in this space in which w changes most quickly we can alter nately represent the gradient direc tion as a weighted sum of data ex amples here we show the case φ in practical problems the data dimensionality d is greater than the number of examples i so we take a weighted sum φ xψ of all of the data points this is the dual parameterization maximum likelihood solution we apply the maximum likelihood method to estimate the parameters ψ in the dual formulation to this end we maximize the logarithm of the likelihood equation algorithm with respect to ψ and σ so that ψˆ argmax ψ i log i log σ w xt xψ t w xt xψ problem to maximize this expression we take derivatives with respect to ψ and equate the resulting expressions to zero and solve to find ψˆ xt x w xt xψ t w xt xψ i this solution is actually the same as for the original linear regression model equa tions for example if we substitute in the definition φ xψ φˆ xψˆ x xt x xxt x xt x xxt which was the original maximum likelihood solution for φ bayesian solution we now explore the bayesian approach to the dual regression model as before we treat the dual parameters ψ as uncertain assuming that the noise is known once again we will estimate this separately using maximum likelihood the goal of the bayesian approach is to compute the posterior distribution p r ψ x w over possible values of the parameters ψ given the training data pairs xi wi i we start by defining a prior p r ψ over the parameters since we have no particular prior knowledge we choose a normal distribution with a large spherical covariance p r ψ normψ we use bayes rule to compute the posterior distribution over the parameters p r ψ x w p r x w ψ p r ψ p r x w which can be shown to yield the closed form expression where p r ψ x w normψ a xw a a xt xxt x i p to compute the predictive distribution p r w x we take an infinite weighted sum over the predictions of the model associated with each possible value of the parameters ψ p r w x x w p r w x ψ p r ψ x w dψ normw x xa x xw x xa x x σ to generalize the model to the nonlinear case we replace the training data x xi with the transformed data z zi and the test data x with the transformed test data z since the resulting expression depends only on inner products of the form zt z and zt z it is directly amenable to kernelization as for the original regression model the variance parameter can be estimated by maximizing the log of the marginal likelihood which is given by p r w x normw xxt x relevance vector regression having developed the dual approach to linear regression we are now in a position to develop a model that depends only sparsely on the training data to this end algorithm algorithm figure relevance vector regres sion a prior applying sparseness is applied to the dual parameters this means that the final classifier only de pends on a subset of the data points indicated by the six larger points the resulting regression function is considerably faster to evaluate and tends to be simpler this means it is less likely to overfit to random statis tical fluctuations in the training data and generalizes better to new data we impose a penalty for every non zero weighted training example we achieve this by replacing the normal prior over the dual parameters ψ with a product of one dimensional t distributions so that p r ψ i studψi ν i this model is known as relevance vector regression this situation is exactly analogous to the sparse linear regression model sec tion except that now we are working with dual variables as for the sparse model it is not possible to marginalize over the variables ψ with the t distributed prior our approach will again be to approximate the t distributions by maxi mizing with respect to their hidden variables rather than marginalizing over them equation by analogy with section the marginal likelihood becomes p r w x max rnormw xt xh x dtt gamhd ν ν where the matrix h contains the hidden variables hi i associated with the t distribution on its diagonal and zeros elsewhere notice that this expression is similar to equation except that instead of every data point having the same prior variance they now have individual variances that are determined by the hidden variables that form the elements of the diagonal matrix h in relevance vector regression we alternately i optimize the marginal likeli hood with respect to the hidden variables and ii optimize the marginal likelihood with respect to the variance parameter using hnew hiσii ν and i ν new i i hiσii w xt xµ t w xt xµ in between each step we update the mean µ and variance σ of the posterior dis tribution where a is defined as µ a xw σ a a xt xxt x h at the end of the training all data examples where the hidden variable hi is large say are discarded as here the coefficients ψi will be very small and contribute almost nothing to the solution since this algorithm depends only on inner products a nonlinear version of this algorithm can be generated by replacing the inner products with a kernel function k xi xj if the kernel itself contains parameters these may be also be manipulated to improve the log marginal variance during the fitting procedure figure shows an example fit using the rbf kernel the final solution now only depends on six data points but nonetheless still captures the important aspects of the data regression to multivariate data throughout this chapter we have discussed predicting a scalar value wi from mul tivariate data xi in real world situations such as the pose regression problem the world states wi are multivariate it is trivial to extend the models in this chapter we simply construct a separate regressor for each dimension the exception to this rule is the relevance vector machine here we might want to ensure that the sparse structure is common for each of these models so the efficiency gains are retained to this end we modify the model so that a single set of hidden variables is shared across the model for each world state dimension applications regression methods are used less frequently in vision than classification but nonethe less there are many useful applications the majority of these involve estimating the position or pose of objects since the unknowns in such problems are naturally treated as continuous figure body pose estimation re sults a silhouettes of walking avatar b estimated body pose based on silhouette using a relevance vector machine the rvm used ra dial basis functions and constructed its final solution from just of of the training examples it produced a mean test error of averaged over the three joint an gles for the main body parts and the overall compass direction of the model adapted from agarwal triggs human body pose estimation agarwal triggs developed a system based on the relevance vector ma chine to predict body pose w from silhouette data x to encode the silhouette they computed a dimensional shape context feature section at each of points on the boundary of the object to reduce the data dimension ality they computed the similarity of each shape context feature to each of different prototypes finally they formed a dimensional histogram containing the aggregated dimensional similarities for all of the boundary points this histogram was used as the data vector x the body pose was encoded by the joint angles of each of the major body joints and the overall azimuth compass heading of the body the resulting dimensional vector was used as the world state w a relevance vector machine was trained using data vectors xi extracted from silhouettes that were rendered using the commercial program poser from known motion capture data wi using a radial basis function kernel the relevance vector machine based its solution on just of these training examples the body pose angles of test data could be predicted to within an average of error figure they also demonstrated that the system worked reasonably well on silhouettes from real images figure silhouette information is by its nature ambiguous it is very hard to tell which leg is in front of the other based on a single silhouette agarwal triggs partially circumvented this system by tracking the body pose wi through a video sequence essentially the ambiguity at a given frame is resolved by encouraging the estimated pose in adjacent frames in the sequence to be similar information from frames where the pose vector is well defined is propagated through the sequence to resolve ambiguities in other parts see chapter however the ambiguity of silhouette data is an argument for not using this type of classifier the regression models in this chapter are designed to give a unimodal normal output to effectively classify single frames of data we should use a regression method that produces a multi modal prediction that can effectively describe the ambiguity applications figure tracking using displacement experts the goal of the system is to predict a displacement vector indicating the motion of the object based on the pixel data at its last known position a the system is trained by perturbing the bounding box around the object to simulate the motion of the object b the system successfully tracks a face even in the presence c of partial occlusions d if the system is trained using gradient vectors rather than raw pixel values it is also quite robust to changes in illumination adapted from williams et al qc ieee displacement experts regression models are also used to form displacement experts in tracking applica tions the goal is to take a region of the image x and return a set of numbers w that indicate the change in position of an object relative to the window the world state w might simply contain the horizontal and vertical translation vectors or might contain parameters of a more complex transformation chapter for simplicity we will describe the former situation training data are extracted as follows a bounding box around of the object of interest car face etc is identified in a number of frames for each of these frames the bounding box is perturbed by a pre determined set of translation vectors to simulate the object moving in the opposite direction figure in this way we associate a translation vector wi with each perturbation the data from the perturbed bounding box are extracted resized to a standard shape and histogram equalized section to induce a degree of invariance to illumination changes the resulting values are then concatenated to form the data vector xi williams et al describe a system of this kind in which the elements of w were learned by a set of independent relevance vector machines they initialize the position of the object using a standard object detector see chapter in the subsequent frame they compute a prediction for the displacement vector w using the relevance vector machines on the data x from the original position this prediction is combined in a kalman filter like system chapter that imposes prior knowledge about the continuity of the motion to create a robust method for tracking known objects in scenes figure d show a series of tracking results from this system regression models discussion the goal of this chapter was to introduce discriminative approaches to regression these have niche applications in vision related to predicting the pose and position of objects however the main reason for studying these models is that the concepts involved sparsity dual variables kernelization are all important for discriminative classification methods these are very widely used but are rather more complex and are discussed in the following chapter notes notes regression methods rasmussen williams is a comprehensive resource on gaussian processes the relevance vector machine was first introduced by tipping several innovations within the vision community have extended these models williams et al presented a semi supervised method for gaussian process regression in which the world state w is only known for a subset of examples ranganathan yang presented an efficient algorithm for online learning of gaussian processes when the kernel matrix is sparse thayananthan et al developed a multivariate version of the relevance vector machine applications applications of regression in vision include head pose estimation williams et al ranganathan yang rae ritter body tracking williams et al agarwal triggs thayananthan et al eye tracking williams et al and tracking of other objects williams et al ranganathan yang multimodal posterior one of the drawbacks of using the methods in this chapter is that they always produce a unimodal normally distributed posterior for some problems e g body pose estimation the posterior probability over the world state may be gen uinely multimodal there is more than one interpretation of the data one approach to this is to build many regressors that relate small parts of the world state to the data thayananthan et al alternatively it is possible to use generative regression meth ods in which either the joint density is modeled directly navaratnam et al or the likelihood and prior are modeled separately urtasun et al in these methods the posterior distribution over the world is multi modal however the cost of this is that it is intractable to compute exactly and so we must rely on optimization techniques to find its modes problems problem consider a regression problem where the world state w is known to be positive to cope with this we could construct a regression model in which the world state is modeled as a gamma distribution we could constrain both parameters α β of the gamma distribution to be the same so that α β and make them a function of the data x describe a maximum likelihood approach to fitting this model problem consider a robust regression model based on the t distribution rather than the normal distribution define this model precisely in mathematical terms and sketch out a maximum likelihood approach to fitting the parameters problem prove that the maximum likelihood solution for the gradient in the linear regression model is φˆ xxt problem for the bayesian linear regression model section show that the posterior distribution over the parameters φ is given by p r φ x w normφ a a where regression models a xxt i p problem for the bayesian linear regression model section show that the pre dictive distribution for a new data example x is given by p r w x x w normw x t a x t a problem use the matrix inversion lemma appendix c to show that a xxt id xt x ii xt p p p p problem compute the derivative of the marginal likelihood p r w x normw x with respect to the variance parameter problem compute a closed form expression for the approximated t distribution used to impose sparseness q h max normφ h gamh ν ν h plot this function for ν plot the function q q for ν problem describe maximum likelihood learning and inference algorithms for a non linear regression model based on polynomials where p r w x normw φ3x3 problem i wish to learn a linear regression model in which i predict the world w from i examples of d data x using the maximum likelihood method if i d is it more efficient to use the dual parameterization or the original linear regression model problem show that the maximum likelihood estimate for the parameters ψ in the dual linear regression model section is given by ψˆ xt x chapter classification models this chapter concerns discriminative models for classification the goal is to di rectly model the posterior probability distribution p r w x over a discrete world state w k given the continuous observed data vector x models for clas sification are very closely related to those for regression and the reader should be familiar with the contents of chapter before proceeding to motivate the models in this chapter we will consider gender classification here we observe a rgb image containing a face figure and concatenate the rgb values to form the vector x our goal is to take the vector x and return the probability distribution p r w x over a label w indicating whether the face is male w or female w gender classification is a binary classification task as there are only two possible values of the world state throughout most of this chapter we will restrict our discussion to binary classification we discuss how to extend these models to cope with an arbitrary number of classes in section logistic regression we will start by considering logistic regression which despite its name is a model that can be applied to classification logistic regression figure is a discrimi native model we select a probability distribution over the world state w and make its parameters contingent on the observed data x since the world state is binary we will describe it with a bernoulli distribution and we will make the single bernoulli parameter λ indicating the probability that the world state takes the value w a function of the measurements x in contrast to the regression model we cannot simply make the parameter λ a linear function φt x of the measurements a linear function can return any value but the parameter λ must lie between and consequently we first compute the linear function and then pass this through the logistic sigmoid function sig that maps the range to the final model is hence classification models figure gender classification con sider a pixel image of a face we concatenate the rgb values to make a data vector x the goal of gender classification is to use the data x to infer a label w in dicating whether the window contains a a male or b a female face this is challenging because the differences are subtle and there is image varia b tion due to changes in pose lighting and expression note that real sys tems would preprocess the image be fore classification by registering the faces more closely and compensating in some way for lighting variation see chapter p r w φ x bernw sig a where a is termed the activation and is given by the linear function a φt x the logistic sigmoid function sig is given by sig a exp a problem as the activation a tends to this function tends to one as a tends to it tends to zero when a is zero the logistic sigmoid function returns a value of one half for data x the overall effect of this transformation is to describe a sigmoid curve relating x to λ figures and the horizontal position of the sigmoid is determined by the place that the linear function a crosses zero i e the x intercept and the steepness of the sigmoid depends on the gradient in more than one dimension the relationship between x and λ is more complex figure the predicted parameter λ has a sigmoid profile in the direction of the gradient vector φ but is constant in all perpendicular directions this induces a linear decision boundary this is the set of positions in data space x p r w x where the posterior probability is the decision boundary separates the region where the world state w is more likely to be from the region where it is more likely to be for logistic regression the decision boundary takes the form of a hyperplane with the normal vector in the direction of φ as for regression we can simplify the notation by attaching the y intercept to the start of the parameter vector φ so that φ φt t and attaching to the start of the data vector x so that x xt t after these changes the activation is now a φt x and the final model becomes p r w φ x bernw exp φt x logistic regression a b c figure logistic regression a we represent the world state w as a bernoulli distribution and make the bernoulli parameter λ a function of the observations x b we compute the activation a as a linear sum a of the observations c the bernoulli parameter λ is formed by passing the activation through a logistic sigmoid function sig to constrain the value to lie between and giving the characteristic sigmoid shape in learning we fit parameters θ using training pairs xi wi in inference we take a new datum x and evaluate the posterior p r w x over the state notice that this is very similar to the linear regression model section except for the introduction of the nonlinear logistic sigmoid function sig explaining the unfortunate name logistic regression however this small change has serious implications maximum likelihood learning of the parameters φ is considerably harder than for linear regression and to adopt the bayesian approach we will be forced to make approximations learning maximum likelihood in maximum likelihood learning we consider fitting the parameters φ using i paired examples of training data x w i figure assuming independence of the algorithm training pairs we have p r w x φ i i i i λwi λ wi i tt wi exp φt xi wi i exp φt xi exp φt xi where x xi is a matrix containing the measurements and w wi t is a vector containing all of the binary world states the maxi mum likelihood method finds parameters φ which maximize this expression as usual however it is simpler to maximize the logarithm l of this expression since the logarithm is a monotonic transformation it does not change the position classification models a figure logistic regression model fitted to two different datasets a one dimensional data green points denote set of examples where w pink points denote where w note that in this and all future figures in this chapter we have only plotted the probability p r w x compare to figure the probability p r w x can be computed as p r w x b two dimensional data here the model has a sigmoid profile in the direction of the gradient φ and p r w x is constant in the orthogonal directions the decision boundary cyan line is linear of the maximum with respect to φ applying the logarithm replaces the product with a sum so that l i wi log exp φt xi i wi log exp φt xi exp φ xi problem the derivative of the log likelihood l with respect to the parameters φ is l φ i exp φt x i xi i sig ai wi xi unfortunately when we equate this expression to zero there is no way to re arrange to get a closed form solution for the parameters φ instead we must rely on a nonlinear optimization technique to find the maximum of this objective function optimization techniques are discussed in detail in appendix b in brief we start with an initial estimate of the solution φ and iteratively improve it until no more progress can be made here we will apply the newton method in which we base the update of the parameters on the first and second derivatives of the function at the current position so that logistic regression c figure parameter estimation for logistic regression with data a in maximum likelihood learning we seek the maximum of p r w x φ with respect to φ b in practice we instead maximize log likelihood notice that the peak is in the same place crosses show results of two iterations of optimization using newton method c the logistic sigmoid functions asso ciated with the parameters at each optimization step as the log likelihood increases the model fits the data more closely the green points represent data where w and the purple points represent data where w so we expect the best fitting model to increase from left to right just like curve φ t φ t α l φ where φ t denotes the estimate of the parameters φ at iteration t and α determines how much we change this estimate and is usually chosen by an explicit search at each iteration for the logistic regression model the d vector of first derivatives and the problem d d matrix of second derivatives are given by l sig a w x φ i i i i i l sig a sig a x xt these are known as the gradient vector and the hessian matrix the expression for the gradient vector has an intuitive explanation the con tribution of each data point depends on the difference between the actual class wi and the predicted probability λ sig ai of being in class points that are classi fied incorrectly contribute more to this expression and hence have more influence that these are the gradient and hessian of the log likelihood that we aim to maximize if this is implemented using a nonlinear minimization algorithm we should multiply the objective function gradient and hessian by on the parameter values figure shows maximum likelihood learning of the parameters φ for data using a series of newton steps for general functions the newton method only finds local maxima at the end of the procedure we cannot be certain that there is not a taller peak in the likelihood function elsewhere however the log likelihood for logistic regression has a special property it is a concave function of the parameters φ for concave functions there are never multiple maxima and gradient based approaches are guaranteed to find the global maximum it is possible to establish whether a function is concave or not by examining the hessian matrix if this is negative definite for all φ then the function is concave this is the case for logistic regression as the hessian equation consists of a negative weighted sum of outer problems with the logistic regression model problem algorithm the logistic regression model works well for simple data sets but for more complex visual data it will not generally suffice it is limited in the following ways it is overconfident as it was learned using maximum likelihood it can only describe linear decision boundaries it is inefficient and prone to over fitting in high dimensions in the remaining part of this chapter we will extend this model to cope with these problems figure bayesian logistic regression in the bayesian approach we learn a distribution p r φ x w over the possible parameter values φ that are compatible with the training data in inference we observe a new data example x and use this distribution to weight the predictions for the world state w given by each possible estimate of φ in linear regression section there were closed form expressions for both of these steps however the nonlinear function sig in logistic regression means that this is no longer the case to get around this we will approximate both steps so that we retain neat closed form expressions and the algorithm is tractable learning we start by defining a prior over the parameters φ unfortunately there is no we are concerned with minimizing functions we equivalently consider whether the function is convex and so has only a single minimum if the hessian matrix is positive definite everywhere then the function is convex problem overconfident bayesian formulation logistic regression problem linear project data through non linearity non linear logistic regression relevance vector classification problem computational cost incremental learning boosting classification trees figure family of classification models a in the remaining part of the chapter we will address several of the limitations of logistic regression for binary classification b the logistic regression model with maximum like lihood learning is overconfident and hence we develop a bayesian version section c it is unrealistic to always assume a linear relationship be tween the data and the world and to this end we introduce a nonlinear version section d combining the bayesian and nonlinear versions of regression leads to gaussian process classification e the logistic regression model also has many parameters and may require considerable resources to learn when the data dimension is high and so we develop relevance vector classification which encourages sparsity f we can also build a sparse model by incrementally adding parameters in a boosting model g finally we consider a very fast classification model based on a tree structure conjugate prior for the likelihood in the logistic regression model equation this is why there won t be closed form expressions for the likelihood and predictive distribution with nothing else to guide us a reasonable choice for the prior over the continuous parameters φ is a multivariate normal distribution with zero mean and a large spherical covariance so that p r φ normφ to compute the posterior probability distribution p r φ x w over the param eters φ given the training data pairs xi wi we apply bayes rule p r φ x w p r w x φ p r φ p r w x where the likelihood and prior are given by equations and respectively figure laplace approximation a probability density blue curve is ap proximated by a normal distribution red curve the mean of the normal and hence the peak is chosen to coin cide with the peak of the original pdf the variance of the normal is chosen so that its second derivatives at the mean match the second derivatives of the original pdf at the peak problem since we are not using a conjugate prior there is no simple closed form expression for this posterior and so we are forced to make an approximation of some kind one possibility is to use the laplace approximation figure which is a gen problem eral method for approximating complex probability distributions the goal is to approximate the posterior distribution by a multivariate normal we select the parameters of this normal so that i the mean is at the peak of the posterior distribution i e at the map estimate and ii the covariance is such that the second derivatives at the peak match the second derivatives of the true posterior distribution at its peak hence to make the laplace approximation we first find the map estimate of the parameters φˆ and to this end we use a nonlinear optimization technique such as newton method to maximize the criterion i l log p r wi xi φ log p r φ i newton method needs the derivatives of the log posterior which are l sig a w x φ φ i i i i i p l sig a sig a x xt we then approximate the posterior by a multivariate normal so that problem p r φ x w q φ normφ µ σ where the mean µ is set to the map estimate φˆ and the covariance σ is chosen so that the second derivatives of the normal match those of the posterior distribution at the map estimate figure so that bayesian logistic regression figure laplace approximation for logistic regression a the prior p r φ over the parameters is a normal distribution with mean zero and a large spherical covariance b the posterior distribution p r φ x w represents the refined state of our knowledge after observing the data unfortunately this posterior cannot be expressed in closed form c we approximate the true posterior with a normal distribution q φ normφ µ σ whose mean is at the peak of the posterior and whose covariance is chosen so that the second derivatives at the peak of the true posterior match the second derivatives at the peak of the normal this is termed the laplace approximation µ φˆ inference in inference we aim to compute a posterior distribution p r w x x w over the world state w given new observed data x to this end we compute an infi nite weighted sum i e an integral of the predictions p r w x φ given by each possible value of the parameters φ p r w x x w p r w x φ p r φ x w dφ p r w x φ q φ dφ where the weights q φ are given by the approximated posterior distribution over the parameters from the learning stage unfortunately this integral cannot be computed in closed form either and so we must make a further approximation classification models figure approximation of activation integral equation a actual result of integral as a function of µa and b the non obvious approx imation from equation c the absolute difference between the actual result and the approximation is very small over a range of reasonable values we first note that the prediction p r w x φ depends only on a linear pro jection a φt x of the parameters see equation hence we could re express the prediction as p r w x x w p r w a p r a da the probability distribution p r a can be computed using the transformation prop erty of the normal distribution section and is given by p r a p r φt x norma µt x x t σx norma µa where we have denoted the mean and variance of the activation by µa and re spectively the one dimensional integration in equation can now be computed using numerical integration over a or we can approximate the result with a similar function such as p r w a norma µa σa da exp µ it is not obvious by inspection that this function should approximate the integral well however figure demonstrates that the approximation is quite accurate figure compares the classification predictions p r w x for the maximum likelihood and bayesian approaches for logistic regression the bayesian approach makes more moderate predictions for the final class this is particularly the case in regions of data space that are far from the mean non linear logistic regression a figure bayesian logistic regression predictions a the bayesian predic tion for the class w is more moderate than the maximum likelihood predic tion b in the decision boundary in the bayesian case blue line is still linear but iso probability contours at levels other than are curved com pare to maximum likelihood case in figure here too the bayesian solution makes more moderate predictions than the maximum likelihood model non linear logistic regression the logistic regression model described previously can only create linear decision boundaries between classes to create nonlinear decision boundaries we adopt the same approach as we did for regression section we compute a nonlinear transformation z f x of the observed data and then build the logistic regression model substituting the original data x for the transformed data z so that p r w x φ bernw sig φt z bernw sig φt f x the logic of this approach is that arbitrary nonlinear activations can be built as a linear sum of nonlinear basis functions typical nonlinear transformations include heaviside step functions of projections zk heaviside αt x arc tangent functions of projections zk arctan αt x and radial basis functions zk exp x αk x αk where zk denotes the kth element of the transformed vector z and the function heaviside returns zero if its argument is less than zero and one otherwise in the first two cases we have attached a to the start of the observed data x where we use projections αt x to avoid having a separate offset parameter figures and show examples of nonlinear classification using arc tangent functions for one and two dimensional data respectively classification models a b c d figure non linear classification in using arc tangent transformation we consider a complex data set bottom of all panels where the posterior p r w x cannot easily be described by a single sigmoid green circles represent data xi where wi pink circles represent data xi where wi a the seven dimensional transformed data vectors zi are computed by evaluating each data example against seven predefined arc tangent functions zik fk xi arctan b when we learn the parameters φ we are learning weights for these nonlinear arc tangent functions the functions are shown after applying the maximum likelihood weights φˆ c the final activation a φt z is a weighted sum of the nonlinear functions d the probability p r w x is computed by passing the activation a through the logistic sigmoid function note that the basis functions also have parameters for example in the arc tangent example there are the projection directions αk k each of which con tains an offset and a set of gradients these can also be optimized during the fitting procedure together with the weights φ we form a new vector of unknowns θ φt αt αt αt t and optimize the model with respect to all of these k unknowns together the gradient vector and the hessian matrix depend on the chosen transformation f but can be computed using the expressions figure non linear classification in using arc tangent transform a these data have been successfully classified with nonlinear logistic regres sion note the nonlinear decision boundary cyan line to compute the posterior p r w x we transform the data to a new two dimensional space z f x where the elements of z are computed by evaluating x against the arc tangent functions in b and c the arc tangent activations are weighted the first by a negative number and summed and the result is put through the logistic sigmoid to compute p r w x l w ai sig a θ i i i i θ t l sig a sig a ai ai w ai sig a where ai φt f xi these relations were established using the chain rule for derivatives unfortunately this joint optimization problem is generally not convex and will be prone to terminating in local maxima in the bayesian case it would be typical to marginalize over the parameters φ but maximize over the function parameters dual logistic regression there is a potential problem with the logistic regression models as described earlier in the original linear model there is one element of the gradient vector φ corre sponding to each dimension of the observed data x and in the nonlinear extension there is one element corresponding to each transformed data dimension z if the relevant data x or z is very high dimensional then the model will have a large number of parameters this will render the newton update slow or even intractable algorithm to solve this problem we switch to the dual representation for simplicity we will develop this model using the original data x but all of the ideas transfer directly to the nonlinear case where we use transformed data z in the dual parameterization we express the gradient parameters φ as a weighted sum of the observed data see figure so that φ xψ where ψ is an i variable where each element weights one of the data examples if the number of data points i is less than the dimensionality d of the data x then the number of parameters has been reduced the price that we pay for this reduction is that we can now only choose gradient vectors φ that are in the space spanned by the data examples however the gradient vector represents the direction in which the final probability p r w x changes fastest and this should not point in a direction in which there was no variation in the training data anyway so this is not a limitation substituting equation into the original logistic regression model leads to the dual logistic regression model p r w x ψ itt bernwi sig ai itt bernwi ψt xt xi l the resulting learning and inference algorithms are very similar to those for the original logistic regression model so we cover them only in brief in the maximum likelihood method we learn the parameters ψ by nonlinear optimization of the log likelihood l log p r w x ψ using the newton method this optimization requires the derivatives of the log likelihood which are l sig a w xt x ψ i i i i i l sig a sig a xt x xt x in the bayesian approach we use a normal prior over the parameters ψ p r ψ normψ the posterior distribution p r ψ x w over the new parameters is found using bayes rule and once more this cannot be written in closed form so we apply the laplace approximation we find the map solution ψˆ using nonlinear optimization which requires the derivatives of the log posterior l log p r ψ x w l sig a w xt x ψ ψ i i i i i l sig a sig a xt x xt x the posterior is now approximated by a multivariate normal so that where p r ψ x w q ψ normψ µ σ µ ψˆ in inference we compute the distribution over the activation p r a p r ψt xt x norma µa norma µt xt x x t xσxt x and then approximate the predictive distribution using equation dual logistic regression gives identical results to the original logistic regression al gorithm for the maximum likelihood case and very similar results in the bayesian situation where the difference results from the slightly different priors how ever the dual classification model is much faster to fit in high dimensions as the parameters are fewer kernel logistic regression we motivated the dual model by the reduction in the number of parameters ψ in the model when the data lies in a high dimensional space however now that we have developed the model a further advantage is easy to identify both learning and inference in the dual model rely only on inner products xt xj of that data equivalently the nonlinear version of this algorithm depends only on inner products zt zj of the transformed data vectors this means that the algorithm is suitable for kernelization see section the idea of kernelization is to define a kernel function k which computes the quantity algorithm k xi xj zt zj where zi f xi and zj f xj are the nonlinear transformations of the two data vectors replacing the inner products with the kernel function means that we do not have to explicitly calculate the transformed vectors z and hence they may be of very high or even infinite dimensions see section for a more detailed description of kernel functions the kernel logistic regression model compare to equation is hence p r w x ψ itt bernwi sig ai itt bernwi ψt k x xi l where the notation k x x i represents a column vector of dot products where element k is given by k xk xi for maximum likelihood learning we simply optimize the log posterior proba bility l with respect to the parameters which requires the derivatives l sig a w k x x ψ i i i i i l sig a sig a k x x k x x problem the bayesian formulation of kernel logistic regression which is sometimes known as gaussian process classification proceeds along similar lines we follow the dual formulation replacing each of the dot products between data examples with the kernel function a very common example of a kernel function is the radial basis kernel in which the nonlinear transformation and inner product operations are replaced by k xi xj exp xi xj t xi xj l this is equivalent to computing transformed vectors zi and zj of infinite length where each entry evaluates the data x against a radial basis function at a different position and then computing the inner product zt zj examples of the kernel logistic regression with a radial basis kernel are shown in figures and relevance vector classification algorithm the bayesian version of the kernel logistic regression model is powerful but com putationally expensive as it requires us to compute dot products between the new data example and the all of the training examples in the kernel function in equa tion it would be more efficient if the model depended only sparsely on the figure kernel logistic regression using rbf kernel and maximum likeli hood learning a with a small length scale λ the model does not interpolate much from the data examples b with a reasonable length scale the clas sifier does a good job of modeling the posterior p r w x c with a large length scale the estimated posterior is very smooth and the model interpolates confident decisions into regions such as the top left where there is no data figure kernel logistic regression with rbf kernel in a bayesian set ting we now take account of our un certainty in the dual parameters ψ by approximating their posterior dis tribution using laplace method and marginalizing them out of the model this produces a very similar result to the maximum likelihood case with the same length scale figure how ever as is typical with bayesian imple mentations the confidence is appro priately somewhat lower training data to achieve this we impose a penalty for every non zero weighted training example as in the relevance regression model section we replace the normal prior over the dual parameters ψ equation with a product of one dimensional t distributions so that p r ψ i studψi ν i applying the bayesian approach to this model with respect to the parameters ψ is known as relevance vector classification following the argument of section we re write each student t distribution as a marginalization of a joint distribution p r ψi hi p r ψ itt normψi hi tt ν ν gamhi dhi where the matrix h contains the hidden variables hi i on its diagonal and zeros elsewhere now we can write the model likelihood as p r w x p r w x ψ p r ψ dψ tti sig ψt k x xi lnormψ h dtt gamhd ν ν dhdψ now we make two approximations first we use the laplace approximation to describe the first two terms in this integral as a normal distribution with mean µ and covariance σ centered at the map parameters and use the following result for the integral over ψ q ψ dψ q µ exp ψ µ t σ ψ µ l dψ q µ d σ this yields the expression p r w x tti ν ν l where the matrix h contains the hidden variables hi i on the diagonal and we have used the general result for the laplace approximation in the second approximation we maximize over the hidden variables rather than integrate over them this yields the expression p r w x rtti ν ν to learn the model we now alternate between updating the mean and variance µ and σ of the posterior distribution and updating the hidden variables hi to update the mean and variance parameters we find the solution ψ that maximizes figure relevance vector regres sion with rbf kernel we place a prior over the dual parameters ψ that encourages sparsity after learning the posterior distribution over most of the parameters is tightly centered around zero and they can be dropped from the model large points indi cate data examples associated with non zero dual parameters the solu tion here can be computed from just of the data points but nonetheless classifies the data almost as well as the full kernel approach figure l i log sig ψt k x xi ll log normψ h using the derivatives l sig a w k x x hψ ψ i i i i i l sig a sig a k x x k x x h and then set i i i i i µ ψˆ to update the hidden variables hi we use the same expression as for relevance vector regression hnew hiσii ν i ν as this optimization proceeds some of the hidden variables hi will become very large this means that the prior over the relevant parameter becomes very concentrated around zero and that the associated data points contribute nothing to the final solution these can be removed leaving a kernelized classifier that depends only sparsely on the data and can hence be evaluated very efficiently in inference we aim to compute the distribution over the world state w given a new data example x we take the familiar strategy of approximating the posterior distribution over the activation as p r a p r ψt k xt x norma µa r norma µt k x x k x x σk x x and then approximate the predictive distribution using equation an example of relevance vector classification is shown in figure which shows that the data set can be discriminated based on of the original data points this results in a considerable computational saving and the simpler solution guards against over fitting of the training set incremental fitting and boosting algorithm in the previous section we developed the relevance vector classification model in which we applied a prior that encourages sparsity in the dual logistic regression parameters ψ and hence encouraged the model to depend on only a subset of the training data it is similarly possible to develop a sparse logistic regression method by placing a prior that encourages sparsity in the original parameters φ and hence encourages the classifier to depend only on a subset of the data dimensions this is left as an exercise to the reader in this section we will investigate a different approach to inducing sparsity we will add one parameter at a time to the model in a greedy fashion in other words we add the parameter that improves the objective function most at each stage and then consider this fixed as the most discriminative parts of the model are added first it is possible to truncate this process after only a small fraction of the parameters are added and still achieve good results the remaining unused parameters can be considered as having a value of zero and so this model also provides a sparse solution we term this approach incremental fitting we will work with the original formulation so that the sparsity is over the data dimensions although these ideas can equally be adapted to the dual case to describe the incremental fitting procedure let us work with the nonlinear formulation of logistic regression section where the probability of the class given the data was described as p r wi xi bernwi sig ai where sig is the logistic sigmoid function and the activation ai is given by ai φt zi φt f xi and f is a nonlinear transformation that returns the transformed vector zi to simplify the subsequent description we will now write the activation term in a slightly different way so that the dot product is described explicitly as a weighted sum of individual nonlinear functions of the data k ai φkf xi ξk k here f is a fixed nonlinear function that takes the data vector xi and some parameters ξk and returns a scalar value in other words the kth entry of the transformed vector z arises by passing the data x through the function with the kth parameters ξk example functions f might include radial basis functions ξ α f x ξ exp arc tan functions ξ α x α t x α and f x ξ arctan αt x in incremental learning we construct the activation term in equation piece wise at each stage we add a new term leaving all of the previous terms unchanged except the additive constant so at the first stage we use the activation ai xi and learn the parameters and using the maximum likelihood approach at the second stage we fit the function ai xi xi and learn the parameters and while keeping the remaining parameters and constant at the kth stage we fit a model with activation k ai φkf xi ξk k and learn the parameters φk and ξk while keeping the remaining parameters φk and ξk constant at each stage the learning is carried out using the maximum likelihood ap proach we use a nonlinear optimization procedure to maximize the log posterior probability l with respect to the relevant parameters the derivatives required by the optimization procedure depend on the choice of nonlinear function but can be computed using the chain rule relations equation this procedure is obviously sub optimal as we do not learn the parameters together or even revisit early parameters once they have been set however it has three nice properties it creates sparse models the weights φk tend to decrease as we move through the sequence and each subsequent basis function tends to have less influence on the model consequently the series can be truncated to the desired length and the associated performance is likely to remain good figure incremental approach to fitting nonlinear logistic regression model with rbf functions a before fitting the activation and hence the posterior probability is uniform b posterior probability after fitting one function mean and scale of rbf shown in blue c e after fitting two three and four rbfs f after fitting ten rbfs the data are now all classified correctly as can be seen from the decision boundary cyan line the previous logistic regression models have been suited to cases where either the dimensionality d of the data is small original formulation or the num ber of training examples i is small dual formulation however it is quite possible that neither of these things is true a strong advantage of incre mental fitting is that it is still practical when the data are high dimensional and there are a large number of training examples during training we do not need to hold all of the transformed vectors z in memory at once at the kth stage we need only the kth dimension of the transformed parameters zk f x ξk and the aggregate of the previous contributions to activation term k φkf xi ξ learning is relatively inexpensive because we only optimize a few parameters at each stage figure illustrates the incremental approach to learning a data set using radial basis functions notice that even after only a few functions have been added to the sequence the classification is substantially correct nonetheless it is worth figure boosting a we start with a uniform prediction p r w x and b incrementally add a step function to the activation green line indi cates position of step in this case the parameters of the step function were chosen greedily from a pre determined set containing angles each with offsets c e as subsequent functions are added the overall classification improves f however the final decision surface cyan line is complex and does not interpolate smoothly between regions of high confidence continuing to train this model even after the training data are classified correctly usually the model continues to improve and the classification performance on test data will continue to increase for some time boosting there is a special case of the incremental approach to fitting nonlinear logistic re gression that is commonly used in vision applications consider a logistic regression model based on a sum of step functions ai φkheaviside αt xi algorithm k where the function heaviside returns if its argument is less than and oth erwise as usual we have attached a to the start of the data x so that the parameters αk contain both a direction αkd in the d dimensional space which determines the direction of the step function and an offset that determines where the step occurs one way to think about the step functions is as weak classifiers they return or depending on the value of xi so each classifies the data the model combines these weak classifiers to compute a final strong classifier schemes for combining weak classifiers in this way are generically known as boosting and this particular model is called logitboost unfortunately we cannot simply fit this model using a gradient based optimiza tion approach because the derivative of the heaviside step function with respect to the parameters αk is not smooth consequently it is usual to predefine a large set of j weak classifiers and assume that each parameter vector αk is taken from this set so that αk α α j as before we learn the logitboost model incrementally by adding one term at a time to the activation equation however now we exhaustively search over the weak classifiers α α j and for each we use nonlinear optimization to estimate the weights and φk we choose the combination αk φk that improves the log likelihood the most this procedure may be made even more efficient but more approximate by choosing the weak classifier based on the log likelihood after just a single newton or gradient descent step in the nonlinear optimization stage when we have selected the best weak classifier αk we can return and perform the full optimization over the offset and weight φk note that after each classifier is added the relative importance of each data point is effectively changed the data points contribute to the derivative according to how well they are currently predicted equation consequently the later weak classifiers become more specialized to the more difficult parts of the data set that are not well classified by the early ones usually these are close to the final decision boundary figure shows several iterations of the boosting procedure because the model is composed from step functions the final classification boundary is irregular and does not interpolate smoothly between the data examples this is a potential disadvantage of this approach in general a classifier based on arc tangent functions which are roughly smooth step functions will have superior generalization and can also be fit using continuous optimization classification trees in the nonlinear logistic regression model we created complex decision boundaries using an activation function that is a linear combination φt z of nonlinear functions z f x of the data x we now investigate an alternative method to induce complex decision boundaries we partition data space into distinct regions and apply a different classifier in each region the branching logistic regression model has activation ai g xi ω φt xi g xi ω φt xi figure branching logistic regression a this data set needs a nonlinear decision surface cyan line to classify the data reasonably b this linear activation is an expert that is specialized to describing the right hand side of the data c this linear activation is an expert that describes the left hand side of the data d a gating function takes the data vector x and returns a number between and which we will use to decide which expert contributes at each decision e the final activation consists of a weighted sum of the activation indicated by the two experts where the weight comes from the gating function f the final classifier predictions p r w x are generated by passing this activation through the logistic sigmoid function the term g is a gating function that returns a number between and if this gating function returns then the activation will be whereas if it returns the activation will be if the gating returns an intermediate value then the activation will be a weighted sum of these two components the gating function itself depends on the data xi and takes parameters ω this model induces a complex nonlinear decision boundary figure where the two linear functions and are specialized to different regions of the data space in this context they are sometimes referred to as experts the gating function could take many forms but an obvious possibility is to use a second logistic regression model in other words we compute a linear function ωt xi of the data that is passed through a logistic sigmoid so that g xi ω sig ωt xi figure logistic classification tree data flows from the root to the leaves each node is a gating function that weights the contributions of terms in the sub branches in the final activation the gray region indi cates variables that would be learned together in an incremental training approach to learn this model we maximize the log probability l i log p r wi xi problem ω as usual this can be accomplished using a nonlinear optimization procedure the parameters can be estimated simultaneously or using a coordinate ascent approach in which we alternately update the three sets of parameters we can extend this idea to create a hierarchical tree structure by nesting gating functions figure for example consider the activation ai g xi ω xi g xi φt xi g xi φt xil g xi ω xi g xi φt xi g xi φt xil this is an example of a classification tree to learn the parameters θ ω we could take an incremental approach at the first stage we fit the top part of the tree equation setting parameters ω then we fit the left branch set ting parameters φ01 and subsequently the right branch setting parameters φ10 and so on the classification tree has the potential advantage of speed if each gating function produces a binary output like the heaviside step function then each data point passes down just one of the outgoing edges from each node and ends up at a single leaf when each branch in the tree is a linear operation as in this example these operations can be aggregated to a single linear operation at each leaf since each data point receives specialized processing the tree need not usually be deep and new data can be classified very efficiently multi class logistic regression multi class logistic regression throughout this chapter we have discussed binary classification we now discuss how to extend these models to handle n world states one possibility is to build n one against all binary classifiers each of which computes the probability that the nth class is present as opposed to any of the other classes the final label is assigned according to the one against all classifier with the highest probability the one against all approach works in practice but is not very elegant a more principled way to cope with multi class classification problems is to describe the the posterior p r w x as a categorical distribution where the parameters λ λn are functions of the data x p r w x catw λ x where the parameters are in the range λn and sum to one n λn in constructing the function λ x we must ensure that we obey these constraints as for the two class logistic regression case we will base the model on lin ear functions of the data x and pass these through a function that enforces the constraints to this end we define n activations one for each class an φt x where φn are parameter vectors we assume that as usual we have pre pended a to each of the data vectors xi so that the first entry of each parameter vectors φn represents an offset the nth entry of the final categorical distribution is now defined by algorithm λ softmax a a a exp an n n n n m exp am the function softmax takes the n activations an which can take any problem real number and maps them to the n parameters λn of the categorical distribution which are constrained to be positive and sum to one figure to learn the parameters θ φn given training pairs wi xi we opti mize the log likelihood of the training data i l log p r wi xi i as for the two class case there is no closed form expression for the maximum likelihood parameters however this is a convex function and the maximum can be found using a nonlinear optimization technique such as the newton method these techniques require the first and second derivatives of the log likelihood with respect to the parameters which are given by problem classification models a b figure multi class logistic regression a we form one activation for each class based on linear functions of the data b we pass these activations through the softmax function to create the distribution p r w x which is shown here as a function of x the softmax function takes the three real valued activations and returns three positive values that sum to one ensuring that the distribution p r w x is a valid probability distribution for all x l φn i yin i i δ wi n xi l y δ m n y x xt where we define the term yin p r wi n xi softmaxn ain it is possible to extend multi class logistic regression in all of the ways that we extended the two class model we can construct bayesian nonlinear dual and kernelized versions it is possible to train incrementally and combine weak classifiers in a boosting framework here we will consider tree structured models as these are very common in modern vision applications random trees forests and ferns algorithm in section we introduced the idea of tree structured classifiers in which the processing for each data example is different and becomes steadily more specialized this idea has recently become extremely popular for multi class problems in the form of random classification trees as for the two class case the key idea is to construct a binary tree where at each node the data are evaluated to determine whether it will pass to the left or the right branch unlike in section we will assume that each data point passes into just one branch in a random classification tree the data are evaluated against a function q x that was randomly chosen from a predefined family of possible functions for example this might be the response of a randomly chosen filter the data proceeds one way in the tree if the response of this function exceeds a threshold τ and the other way if not while the functions are chosen randomly the threshold is carefully selected we select the threshold that maximizes the log likelihood l of the data l i heaviside q xi τ log l ll heaviside q xi τ log r ll here the first term represents the contribution of the data that passes down the left branch and the second term represents the contribution of the data that passes down the right branch in each case the data are evaluated against a categorical distribution with parameters λ l and λ r respectively these parameters are set using maximum likelihood λ l i i δ wi k heaviside q xi τ k i i heaviside q xi τ λ r i i δ wi k heaviside q xi τ k i i heaviside q xi τ the log likelihood is not a smooth function of the threshold τ and so in practice we maximize the log likelihood by empirically trying a number of different threshold values and choosing the one that gives the best result we then perform this same procedure recursively the data that pass to the left branch has a new randomly chosen classifier applied to them and a new threshold is chosen that splits it again this can be done without recourse to the data in the right branch when we classify a new data example x we pass it down the tree until it reaches one of the leaves the posterior distribution p r w x over the world state w is set to catw λ where the parameters λ are the categorical parameters associated with this leaf during the training process the random classification tree is attractive because it is very fast to train after all most of its parameters are chosen randomly it can also be trained with very large amounts of data as its complexity is linear in the number of data examples there are two important variations on this model a fern is a tree where the randomly chosen functions at each level of the tree are constrained to be the same in other words the data that pass through the left and right branches at any node are subsequently acted on by the same function although the threshold level may optionally be different in each branch in practice this means that every data point is acted on by the same sequence of functions this can make implementation extremely efficient when we are evaluating the classifier repeatedly a random forest is a collection of random trees each of which uses a different randomly chosen set of functions by averaging together the probabilities p r w x predicted by these trees a more robust classifier is produced one way to think of this is as approximating the bayesian approach we are constructing the final answer by taking a weighted sum of the predictions suggested by different sets of parameters relation to non probabilistic models in this chapter we have described a family of probabilistic algorithms for classi fication each is based on maximizing either the log bernoulli probability of the training class labels given the data two class case or the log categorical probability of the training class labels given the data multi class case however it is more common in the computer vision literature to use non probabilistic classification algorithms such as the multilayer perceptron adaboost or support vector classification at their core these algorithms optimize different objective functions and so are neither directly equivalent to each other nor to the models in this chapter we chose to describe the less common probabilistic algorithms because they have no serious disadvantages relative to non probabilistic techniques they naturally produce estimates of certainty they are easily extensible to the multi class case whereas non probabilistic algorithms usually rely on one against all formulations and they are more easily related to one another and to the rest of the book in short it can reasonably be argued that the dominance of non probabilistic approaches to classification is largely for historical reasons we will now briefly describe the relationship between our models and common non probabilistic ap proaches the multi layer perceptron or neural network is very similar to our nonlin ear logistic regression model in the special case where the nonlinear transform consists of a set of sigmoid functions applied to linear projections of data e g zk arctan αt x in the mlp learning is known as back propagation and the transformed variable z is known as the hidden layer adaboost is very closely related to the the logitboost model described in this chapter but adaboost is not probabilistic performance of the two algorithms is similar the support vector machine svm is similar to relevance vector classification it is a kernelized classifier that depends sparsely on the data it has the advantage that its objective function is convex whereas the objective function in relevance vector classification is non convex and only guarantees to converge to a local minimum however the svm has several disadvantages it does not assign certainty to its class predictions it is not so easily extended to the multi class case it produces solutions that are less sparse than relevance vector classification and it places more restrictions on the form of the kernel function in practice classification performance of the two models is again similar applications we now present a number of examples of the use of classification in computer vision from the research literature in many of the examples the method used was non probabilistic e g adaboost but is very closely related to the algorithms in this chapter and one would not expect the performance to differ significantly if these were substituted gender classification the algorithms in this chapter were motivated by the problem of gender detection in unconstrained facial images the goal is to assign a label w indicating whether a small patch of an image x contains a male or a female face prince aghajanian developed a system of this type first a bounding box around the face was identified using a face detector see next section the data within this bounding box was resized to 60 converted to grayscale and histogram equalized the resulting image was convolved with a bank of gabor functions and the filtered images sampled at regular intervals that were proportionate to the wavelength to create a final feature vector of length each dimension was whitened to have mean zero and unit standard deviation chapter contains information about these and other preprocessing methods a training database of examples was used to learn a nonlinear logistic regression model of the form p r w x bern l i i wi exp φ k φ f x ξ where the nonlinear functions f were arc tangents of linear projections of the data so that f xi ξk arctan ξt xi as usual the data were augmented by prepending a so the projection vectors ξk were of length d this model was learned using an incremental approach so that at each stage the parameters φk and ξk were modified the system achieved performance with k arc tangent functions on a challenging real world database that contained large variations in scale pose lighting and expression similar to the faces in figure human observers managed only performance on the same database using the resized face region alone classification models a b d c figure fast face detection using a boosting method viola jones a each weak classifier consists of the response of the image to a haar like filter which is then passed through a step function b the first two weak classifiers learned in this implementation have clear interpretations the first responds to the dark horizontal region belonging to the eyes and the second responds to the relative brightness of the bridge of the nose c the data passes through a cascade most regions can be quickly rejected after evaluating only a few weak classifiers as they look nothing like faces more ambiguous regions undergo further preprocessing d example results adapted from viola jones face and pedestrian detection before we can determine the gender of a face we must first find it in face detection figure we assign a label w to a small region of the image x indicating whether a face is present w or not w to ensure that the face is found this process is repeated at every position and scale in the image and consequently the classifier must be very fast viola jones presented a face detection system based on adaboost fig ure this is a non probabilistic analogue of the boosting methods described in section the final classification is based on the sign of a sum of nonlinear functions of the data k a φkf x ξk k where the nonlinear functions f are heaviside step functions of projections of the data weak classifiers giving a response of zero or one for each possible data vector x so that f x ξk heaviside ξt x applications as usual the data vector x was prepended with a to account for an offset the system was trained on faces and non face regions each of which was represented as a image patch since the model is not smooth due to the step function gradient based optimization is unsuitable and so viola jones exhaustively searched through a very large number of pre defined projections ξk there were two aspects of the design that ensured that the system ran quickly the structure of the classifier was exploited training in boosting is incremen tal the weak classifiers nonlinear functions of the data are incrementally added to create an increasingly sophisticated strong classifier viola jones exploited this structure when they ran the classifier they reject re gions that are very unlikely to be faces based on responses of the first few weak classifiers and only subject more ambiguous regions to further process ing this is known as a cascade structure during training the later stages of the cascade are trained with new negative examples that were not rejected by the earlier stages the projections ξk were carefully chosen so that they were very fast to eval uate they consisted of haar like filters section which require only a few operations to compute the final system consisted of weak classifiers divided into a stage cas cade it found of frontal faces across images with a false positive rate of less than per frame and processed images in fractions of a second viola et al developed a similar system for detecting pedestrians in video sequences figure the main modification was to extend the set of weak classifiers to encompass features that span more than one frame and hence select for the particular temporal patterns associated with human motion to this end their system used not only the image data itself but also the difference image between adjacent frames and similar difference images when taken after offsetting the frames in each of four directions the final system achieved an detection rate with a false alarm rate of which corresponds to one false positive for every two frames semantic segmentation the goal of semantic segmentation is to assign a label w m to each pixel indicating which of m objects is present based on the local image data x shotton et al developed a system known as textonboost that was based on a non probabilistic boosting algorithm called jointboost torralba et al the decision was based on a one against all strategy in which m binary classifiers are computed based on the weighted sums k am φkmf x ξk k classification models a frame frame difference up down left right b c figure boosting methods based on the thresholded responses of haar functions have also been used for pedestrian detection in video footage a to improve detection rates two subsequent frames are used the absolute difference between the frames is computed as is the difference when one of the frames is offset in each of four directions the set of potential weak classifiers consists of haar functions applied to all six of these representations b c example results adapted from viola et al qc springer where the nonlinear functions f were once more based on heaviside step functions note that the weighted sums associated with each object class share the same nonlinear functions but weight them differently after computing these series the decision is assigned based on the activation am that is the greatest shotton et al based the nonlinear functions on a texton representation of the image each pixel in the image is replaced by a discrete index indicating the type of texture present at that position see section each nonlinear function considers one of these texton types and computes the number of times that it is found within a rectangular area this area has a fixed spatial displacement from the pixel under consideration figure f if this displacement is zero then the function provides evidence about the pixel directly e g it looks like grass if the spatial displacement is larger then the function provides evidence of the local context e g there is grass nearby so this pixel may belong to a cow for each nonlinear function an offset is added to the texton count and the result is passed through a step function the system was learned incrementally by assessing each of a set of randomly chosen classifiers defined by the choice of texton rectangular region and offset and choosing the best at the current stage the full system also included a post processing step in which the result was refined using a conditional random field model see chapter it achieved applications figure semantic image labeling using textonboost a original im age b image converted to textons a discrete value at each pixel indicating the type of texture that is present c the system was based on weak clas sifiers that count the number of textons of a certain type within a rectangle that is offset from the current position yellow cross d this provides both information about the object itself contains sheep like textons and nearby objects near to grass like textons e f another example of a weak clas sifier g test image h per pixel classification is not very precise at the edges of objects and so i a conditional random field is used to improve the result j examples of results and ground truth adapted from shotton et al qc springer performance on the challenging mrsc database that includes diverse object classes including wiry objects such as bicycles and objects with a large degree of variation such as dogs recovering surface layout to recover the surface layout of a scene we assign a label w to each pixel in the image indicating whether the pixel contains a support object e g floor a vertical object e g building or the sky this decision is based on local image data x hoiem et al constructed a system of this type using a one against all principle each of the three binary classifiers was based on logitboosted classification trees different classification trees are treated as weak classifiers and the results are weighted together to compute the final probability hoiem et al worked with the intermediate representation of superpixels an over segmentation of the scene into small homogeneous regions which are assumed to belong to the same object each superpixel was assigned a label w using the classifier based on a data vector x which contained location appearance texture and perspective information associated with the superpixel classification models input location colour texture perspective all cues figure recovering surface layout the goal is to take an image and return a label indicating whether the pixel is part of a support surface green pixels vertical surface red pixels or the sky blue pixels vertical surfaces were sub classified into planar objects at different orientations left arrows upward arrows and right arrows denote left facing fronto parrallel and right facing surfaces and non planar objects which can be porous marked as o or non porous marked as x the final classification was based on i location cues which include position in the image and position relative to the horizon ii color cues iii texture cues and iv perspective cues which were based on the statistics of line segments in the region the figure shows example classifications for each of these cues alone and when combined adapted from hoiem et al qc springer to mitigate against the possibility that the original superpixel segmentation was wrong multiple segmentations were computed and the results merged to provide a final per pixel classification figure in the full system regions that were clas sified as vertical were sub classified into left facing planar surfaces fronto parallel planar surfaces or right facing planar surfaces or non planar surfaces which may be porous e g trees or solid the system was trained and tested on a data set consisting of images collected from the web including diverse environments forests cities roads etc and conditions snowy sunny cloudy etc the data set was pruned to remove photos where the horizon was not within the image the system correctly labeled of pixels correctly with respect to the main three classes and correctly with respect to the subclasses of the vertical surface this algorithm was the basis of a remarkable system for creating a model from a single photograph hoiem et al d e figure identifying human parts a the goal of the system is to take a depth image x and assign a discrete label w to each pixel indicating which of possible body parts is present these depth labels are used to form proposals about the position of joints b the classification is based on decision trees at each point in the tree the data are divided according to the relative depth at two points red circles offset relative to the current pixel yellow crosses in this example this difference is large in both cases whereas in c this difference is small hence these differences provide infor mation about the pose d e two more examples of depth image labeling and hypothesized pose adapted from shotton et al qc ieee identifying human parts shotton et al describe a system that assigns a discrete label w indicating which of body parts is present at each pixel based on a depth image x the resulting distribution of labels is an intermediate representation in a system that proposes a possible configuration of the joint positions in the microsoft kinect gaming system figure the classification was based on a forest of decision trees the final probability p r w x is an average i e a mixture of the predictions from a number of different classification trees the goal is to mitigate against biases introduced by the greedy method with which a single tree is trained within each tree the decision about which branch a data point travels down is based on the difference in measured depths at two points each of which is spatially offset from the current pixel the offsets are inversely scaled by the distance to the pixel itself which ensures that they address the same relative positions on the body when the person moves closer or further away to the depth camera the system was trained from a very large data set of depth images which were synthesized based on motion capture data and consisted of three trees of depth remarkably the system is capable of assigning the correct label of the time and this provides a very solid basis for the subsequent joint proposals discussion in this chapter we have considered classification problems we note that all of the ideas that were applied to regression models in chapter are also applicable to classification problems however for classification the model includes a nonlinear mapping between the data x and the parameters of the distribution p r w x over the world w this means that we cannot find the maximum likelihood solution in closed form although the problem is still convex and we cannot compute a full bayesian solution without making approximations classification techniques have many uses in machine vision notice though that these models have no domain specific information about the problem other than that provided by the preprocessing of the data this is both an advantage they find many applications and a disadvantage they cannot take advantage of a priori information about the problem in the remaining part of the book we will explore models that introduce increasing amounts of domain specific information to the problem notes notes classification in vision classification techniques such as those discussed in this chapter have been applied to many problems in vision including face detection viola jones surface layout estimation hoiem et al boundary detection doll ar et al keypoint matching lepetit et al body part classification hoiem et al semantic segmentation he et al object recognition csurka et al and gender classification kumar et al probabilistic classification more information about logistic regression can be found in bishop and many other statistics textbooks kernel logistic regression or gaussian process regression was presented in williams barber and more information can be found in rasmussen williams a sparse version of kernel logistic regression relevance vector classification was presented by tipping and a sparse multi class variant was developed by brishnapuram et al probabilistic interpretations of boosting were introduced by friedman et al random forests of multinomial regressors were introduced in prinzie van den poel other classification schemes in this chapter we have presented a family of proba bilistic classification models based on logistic regression there are other non probabilistic techniques for classification and these include single and multi layer perceptrons rosen blatt rumelhart et al support vector machines vapnik cristianini shawe taylor and adaboost freund schapire a critical difference be tween these techniques is the underlying objective function logistic regression models optimize the log bernoulli probability but the other models optimize different criteria such as the hinge loss support vector machines or exponential error adaboost it is difficult to make general statements about the relative merits of these approaches but it is probably fair to say that i there is no major disadvantage to using the probabilistic techniques in this chapter and ii the choice of classification method is usually less impor tant in vision problems than the preprocessing of the data methods based on boosting and classification trees are particularly popular in vision because of their speed boosting adaboost was introduced by freund schapire since then there have been a large number of variations most of which have been used in computer vision these include discrete adaboost freund schapire real adaboost schapire singer gentleboost friedman et al logitboost friedman et al floatboost li et al klboost liu shum asymmetric boost viola jones and statboost pham chan boosting has also been applied to the multi class case schapire singer torralba et al and for regression friedman a review of boosting approaches can be found in meir m atsch classification trees classification trees have a long history in computer vision dating back to at least shepherd modern interest was stimulated by amit geman and breiman who investigated the use of random forests since this time classification trees and forests have been applied to keypoint matching lepetit et al segmentation yin et al human pose detection rogez et al shotton et al object detection bosch et al image classification moosmann et al moosmann et al deciding image suitability mac aodha et al detect ing occlusions humayun et al and semantic image segmentation shotton et al gender classification automatic determination of gender from a facial image has variously been tackled with neural networks golomb et al support vector machines moghaddam yang linear discriminant analysis bekios calfa et al and classification models both adaboost baluja rowley and logitboost prince aghajanian a review is provided by m akinen raisamo and quantative comparisons are presented in m akinen raisamo representative examples of the state of the art can be found in kumar et al and shan in press face detection the application of boosting to face detection viola jones usurped earlier techniques e g osuna et al schneiderman kanade since then many boosting variants have been applied to the problem including floatboost li et al li zhang gentleboost lienhart et al realboost huang et al wu et al asymboost pham chan viola jones and statboost pham chan a recent review of this area can be found in zhang zhang semantic segmentation the authors of the system described in the text shotton et al subsequently presented a much faster system based on classification trees shotton et al a recent comparison of quantitative performance can be found in ranganathan other work has investigated the imposition of prior knowledge such as the co presence of object classes he et al and likely spatial configurations of objects he et al problems problem the logistic sigmoid function is defined as sig a exp a show that i sig ii sig iii sig problem show that the derivative of the log posterior probability for the logistic regression model l i wi log exp φt xi i wi log exp φt xi exp φ xi with respect to the parameters φ is given by l sig a w x problem show that the second derivative of the log likelihood of the logistic regression model is given by i l sig a sig a x xt problem consider fitting a logistic regression model to data x where the two classes are perfectly separable for example perhaps all the data x where the world state w takes values less than and all the data x where the world state is w takes notes a b c figure mixture of two experts model for data pink circles indicate positive examples green circles indicate negative examples a two expert is specialized to model the left and right sides of the data respectively b the mixing weights change as a function of the data c the final output of the model is mixture of the two constituent experts and fits the data well values greater than hence it is possible to classify the training data perfectly what will happen to the parameters of the model during learning how could you rectify this problem problem compute the laplace approximation to a beta distribution with parameters α β problem show that the laplace approximation to a univariate normal distribution with mean µ and variance is the normal distribution itself problem show that the second derivative of the logarithm l log normx µ σ of a normal distribution evaluated at the mean µ is given by l lµ σ problem devise a method to choose the scale parameter in the radial basis function in kernel logistic regression equation problem a mixture of experts jordan jacobs divides space into different regions each of which receives specialized attention figure for example we could describe the data as a mixture of logistic classifiers so that p r wi xi k λk xi bernwi sig φt xi each logistic classifier is considered as an expert and the mixing weights decide the com bination of experts that are applied to the data the mixing weights which are positive and sum to one depend on the data x for a two component model they could be based on a second logistic regression model with activation ωt x this model can be expressed as the marginalization of a joint distribution between wi and a hidden variable hi so that k k p r wi xi p r wi hi k xi p r wi hi k xi p r hi k xi classification models where p r wi hi k xi bernwi p r hi k xi bernh sig φt xi sig ωt xi how does this model differ from branching logistic regression section devise a learning algorithm for this model problem the softmax function is defined to return a multivariate quan tity where the k element is given by softmax a a a exp ak k k k k j exp aj show that sk and that k sk problem show that the first derivative of the log probability of the multi class logistic regression model is given by equation 61 problem the classifiers in this chapter have all been based on continuous data x devise a model that can distinguish between m world states w m based on a discrete observation x k and discuss potential learning algorithms part iii connecting local models part iii connecting local models the models in chapters describe the relationship between a set of measurements and the world state they work well when the measurements and the world state are both low dimensional however there are many situations where this is not the case and these models are unsuitable for example consider the semantic image labeling problem in which we wish to assign a label to each pixel in the image where the label denotes the object class for example in a road scene we might wish to label pixels as road sky car tree building or other for an image with n pixels this means we need to build a model relating the measured rgb triples to possible world states none of the models discussed so far can cope with this challenge the number of parameters involved and hence the amount of training data and the computational requirements of the learning and inference algorithms is far beyond what current machines can handle one possible solution to this problem would be to build a set of independent local models for example we could build models that relate each pixel label sep arately to the nearby rgb data however this is not ideal as the image may be locally ambiguous for example a small blue image patch might result from a va riety of semantically different classes sky water a car door or a person clothing in general it is insufficient to build independent local models the solution to this problem is to build local models that are connected to one another consider again the semantic labeling example given the whole image we can see that when the image patch is blue and is found above trees and mountains and alongside similar patches across the top of the image then the correct class is probably sky hence to solve this problem we still model the relationship between the label and its local image region but we also connect these models so that nearby elements can help to disambiguate one another in chapter we introduce the idea of conditional independence which is a way of characterizing redundancies in the model i e the lack of direct depen dence between variables we show how conditional independence relations can be visualized with graphical models we distinguish between directed and undirected graphical models in chapter we discuss models in which the local units are combined together to form chains or trees in chapter we extend this to the case where they have more general connections chapter graphical models the previous chapters discussed models that relate the observed measurements to some aspect of the world that we wish to estimate in each case this relation ship depended on a set of parameters and for each model we presented a learning algorithm that estimated these parameters unfortunately the utility of these models is limited because every element of the model depends on every other for example in generative models we model the joint probability of the observations and the world state in many problems both of these quantities may be high dimensional consequently the number of parameters required to characterize their joint density accurately is very large discriminative models suffer from the same pathology if every element of the world state depends on every element of the data a large number of parameters will be required to characterize this relationship in practice the required amount of training data and the computational burden of learning and inference reach impractical levels the solution to this problem is to reduce the dependencies between variables in the model by identifying or asserting some degree of redundancy to this end we introduce the idea of conditional independence which is a way of characterizing these redundancies we then introduce graphical models which are graph based representations of the conditional independence relations we discuss two differ ent types of graphical models directed and undirected and we consider the implications for learning inference and drawing samples this chapter does not develop specific models or discuss vision applications the goal is to provide the theoretical background for the models in subsequent chapters we will illustrate the ideas with probability distributions where the constituent variables are discrete however almost all of the ideas transfer directly to the continuous case conditional independence when we first discussed probability distributions we introduced the notion of independence section two variables and are independent if their joint graphical models d figure conditional independence a joint pdf of three discrete vari ables which take four three and two possible values respectively all probability values sum to one b marginalizing we see that variables and are dependent the conditional distribution of is different for different values of the elements in each row are not in the same propor tions and vice versa c variables and are also dependent d variables and are also dependent e g however and are conditionally independent given for fixed tells us nothing more about and vice versa probability distribution factorizes as p r p r p r in layman terms one variable provides no information about the other if they are independent with more than two random variables independence relations become more complex the variable is said to be conditionally independent of variable given variable when and are independent for fixed figure in mathematical terms we have p r p r p r p r note that conditional independence relations are always symmetric if is condi tionally independent of given then it is also true that is independent of given confusingly the conditional independence of and given does not mean that and are themselves independent it merely implies that if we know variable then provides no further information about and vice versa one way that this can occur is in a chain of events if event causes event and causes then the dependence of on might be entirely mediated by now consider decomposing the joint probability distribution p r into the product of conditional probabilities when is independent of given we find that p r p r p r p r p r p r p r the conditional independence relation means that the probability distribution fac torizes in a certain way and is hence redundant this redundancy implies that we can describe the distribution with fewer parameters and so working with models with large numbers of variables becomes more tractable throughout this chapter we will explore the relationship between factorization of the distribution and conditional independence relations to this end we will introduce graphical models these are graph based representations that make both the factorization and the conditional independence relations easy to establish in this book we will consider two different types of graphical model directed and undirected graphical models each of which corresponds to a different type of factorization directed graphical models a directed graphical model or bayesian network represents the factorization of the joint probability distribution into a product of conditional distributions that take the form of a directed acyclic graph dag so that where xn n n p r n p r xn xpa n n represent the constituent variables of the joint distribution and the function pa n returns the indices of variables that are parents of variable xn we can visualize the factorization as a directed graphical model figure by adding one node per random variable and drawing an arrow to each variable xn from each of its parents xpa n this directed graphical model should never contain cycles if it does then the original factorization was not a valid probability distribution to retrieve the factorization from the graphical model we introduce one factor ization term per variable in the graph if variable xn is independent of all others has no parents then we write p r xn otherwise we write p r xn xpa n where the parents xpa n consist of the set of variables with arrows that point to xn example the graphical model in figure represents the factorization problem problem figure example a directed graphical model has one node per term in the factorization of the joint probability distribution a node xn with no incoming connections repre sents the term p r xn a node xn with incoming connections xpa n represents the term p r xn xpa n variable xn is conditionally inde pendent of all of the others given its markov blanket this comprises its parents its children and other par ents of its children for example the markov blanket for variable is indicated by the shaded region p r p r p r p r p r p r p r p r p r p r p r p r p r p r p r p r x12 the graphical model or factorization implies a set of independence and condi tional independence relations between the variables some statements about these relations can be made based on a superficial look at the graph first if there is no directed path between two variables following the arrow directions and they have no common ancestors then they are independent so variable in figure is independent of all of the other variables and variables and are independent of each other variables and are not independent as they share an ancestor second any variable is conditionally independent of all the other variables given its parents children and the other parents of its children its markov blanket so for example variable in figure is conditionally independent of the remaining variables given those in the shaded area for vision applications these rules are usually sufficient to gain an understand ing of the main properties of a graphical model however occasionally we may wish to test whether one arbitrary set of nodes is independent of another given a third this is not easily established by looking at the graph but can be tested using the following criterion the variables in set a are conditionally independent of those in set b given set c if all routes from a to b are blocked a route is blocked at a node if i this node is in c and the arrows meet head to tail or tail to tail or ii neither this node nor any of its descendants are in c and the arrows meet head to head see koller friedman for more details of why this is the case figure example directed graphical model relating variables from figure this model implies that the joint probability can be broken down as p r p r p r p r example figure tells us that p r p r p r p r in other words this is the graphical model corresponding to the distribution in figure if we condition on then the only route from to is blocked at the arrows meet head to tail here and so must be conditionally independent of given we could have reached the same conclusion by noticing that the markov blanket for variable is just variable in this case it is easy to prove this conditional independence relation alge braically writing out the conditional probability of given and p r x x x p r p r p r p r p r p r p r p r p r p r p r p r we see that the final expression does not depend on and so we deduce that is conditionally independent of given as required notice that the factorized distribution is more efficient to represent than the full version the original distribution p r figure contains entries however the terms p r p r and p r contain and entries respectively giving a total of entries in this case this is not a dramatic reduction but in more practical situations it would be for example if each variable took possible values the full joint distribution would have values but the factorized distribution would have only values for even larger systems this can make a huge saving one way to think about conditional independence relations is to consider them as redundancies in the full joint probability distribution example finally in figure we present graphical models for the mixture of gaussians t distribution and factor analysis models from chapter these depictions imme graphical models a b c figure example graphical models for a mixture of gaussians b t distribution and c factor analysis a node black circle represents a random variable in a graphical model a bullet represents a variable whose value is considered to be fixed each variable may be repeated many times and this is indicated by a plate blue rectangle where the number of copies is indicated in the lower right corner for example in a there are i copies of the training data xi i and i copies of the variable hi i similarly there are k sets of parameters µ σk k but just one weight vector λ diately demonstrate that these models have very similar structures they also add several new features to the graphical representation first they include multidimensional variables second they include variables that are con sidered as fixed and these are marked by a bullet we condition on the fixed variables but do not define a probability distribution over them figure depicts the factorization p r hi xi p r hi p r xi hi µ φ σ finally we have also used plate notation a plate is depicted as a rectangle with a number in the corner it indicates that the quantities inside the rectangle should be repeated the given number of times for example in figure there are i copies xi hi i of the variables x and h but only one set of parameters µ φ and σ summary to summarize we can think about the structure of the joint probability distribution in three ways first we can consider the way that the probability distribution factorizes second we can examine the directed graphical model third we can think about the conditional independence relations there is a one to one mapping between directed graphical models acyclic di rected graphs of conditional probability relations and factorizations however the relationship between the graphical model or factorization and the conditional independence relations is more complicated a directed graphical model or its equivalent factorization determines a set of conditional independence relations however as we shall see later in this chapter there are some sets of conditional independence relations that cannot be represented by directed graphical models undirected graphical models undirected graphical models in this section we introduce a second family of graphical models undirected graph ical models represent probability distributions over variables xn form of a product of potential functions φ n so that n n that take the problem problem problem p r x tt φ x where the potential function φc n always returns a positive number since the probability increases when φc n increases each of these functions modulates the tendency for the variables n to take certain values the probability is greatest where all of the functions c return high values however it should be emphasized that potential functions are not the same as conditional probabilities and there is not usually a clear way to map from one to the other the term z is known as the partition function and normalizes the product of these positive functions so that the total probability is one in the discrete case it would be computed as z tt φc n for realistically sized systems this sum will be intractable we will not be able to compute z and hence will only be able to compute the overall probability up to an unknown scale factor we can equivalently write equation as p r n z exp c ψc n where ψc n log φc n when written in this form the probability is referred to as a gibbs distribution the terms ψc n are functions that may return any real number and can be thought of as representing a cost for every combination of labels n as the cost increases the probability decreases the total cost c ψc n is sometimes known as the energy and the process of fitting the model increasing the probability is hence sometimes termed energy minimization when each potential function φ or alternatively each cost function ψ addresses all of the variables n the undirected graphical model is known as a product of experts however in computer vision it is more common for each potential function to operate on a subset of the variables s xn n these subsets are called cliques and it is the choice of these cliques that determines the conditional independence relations denoting the cth clique by c we can rewrite equation as p r x tt φ s figure example undirected graphical model relating variables and this model im plies that the joint probability can be factorized as p r in other words the probability distribution is factorized into a product of terms each of which only depends on a subset of variables in this situation the model is sometimes referred to as a markov random field to visualize the undirected graphical model we draw one node per random variable then for every clique sc we draw a connection from every member variable xi c to every other member variable moving in the opposite direction we can take a graphical model and establish the underlying factorization using the following method we add one term to the factorization per maximal clique see figure a maximal clique is a fully connected subset of nodes i e a subset where every node is connected to every other where it is not possible to add another node and remain fully connected it is much easier to establish the conditional independence relations from an undirected graphical model than for directed graphical models they can be found using the following property one set of nodes is conditionally independent of another given a third if the third set separates them prevents a path from the first node to the second it follows that a node is conditionally independent of all other nodes given its set of immediate neighbors and so these neighbors form the markov blanket example consider the graphical model in figure this represents the factorization p r z we can immediately see that variable is conditionally independent of variable given because separates the other two variables it blocks the path from to in this case the conditional independence relation is easy to prove p r x x x p r p r φ2 φ x x φ x x dx φ x x dx the final expression does not depend on and so we conclude that is condi tionally independent of given example figure example undirected graphical model representing variables xi i the associated probability distribution factorizes into a product of one potential function per maximal clique the clique is a maximal clique as there is no other node that we can add that connects to every node in the clique the clique is not a maximal clique as it is possible to add node and all three nodes in the new clique are con nected to each other consider the graphical model in figure there are four maximal cliques in this graph and so it represents the factorization p r z φ2 we can deduce various conditional independence relations from the graphical representation for example variable is conditionally independent of variables and given and and variable is independent of variables and given and and so on note also that the factorization p r z φ2 φ3 creates the same graphical model there is a many to one mapping from factoriza tions to undirected graphical models as opposed to the one to one mapping for directed graphical models when we compute a factorization from the graphical model based on the maximal cliques we do so in a conservative way it is possible that there are further redundancies which were not made explicit by the undirected graphical model comparing directed and undirected graphical models in sections and we have discussed directed and undirected graphical mod els respectively each graphical model represents a factorization of the probability distribution we have presented methods to extract the conditional independence relations from each type of graphical model the purpose of this section is to argue that these representations are not equivalent there are patterns of condi tional independence that can be represented by directed graphical models but not undirected graphical models and vice versa graphical models a b c figure directed vs undirected graphical models a directed graph ical model with three nodes there is only one conditional independence relation implied by this model the node is the markov blanket of node shaded area and so where the notation can be read as is independent of b this undirected graphical model implies the same conditional independence relation c second directed graphical model the relation is no longer true but and are independent if we don t condition on so we can write there is no undirected graphical model with three variables that has this pattern of independence and conditional independence problem problem problem figures b show an undirected and directed graphical model that do rep resent the same conditional independence relations however figure shows a directed graphical model for which there is no equivalent undirected graphical model there is simply no way to induce the same pattern of independence and conditional independence with an undirected graphical model conversely figure shows an undirected graphical model that induces a pattern of conditional independence relations that cannot be replicated by any directed graphical model figure shows a directed graphical model that is close but still not equivalent the markov blanket of is different in each model and so are its conditional independence relations we conclude from this brief argument that directed and undirected graphical models do not represent the same subset of independence and conditional indepen dence relations and so we cannot eliminate one or the other from our consideration in fact there are other patterns of conditional independence that cannot be rep resented by either type of model however these will not be considered in this book for further information concerning the families of distributions that can be represented by different types of graphical model consult barber or koller friedman graphical models in computer vision a b figure directed vs undirected models a this undirected graphical model induces two conditional independence relations however there is no equivalent directed graphical model that produces the same pattern b this directed graphical model also induces two conditional independence relations but they are not the same in both cases the shaded region represents the markov blanket of variable graphical models in computer vision we will now introduce a number of common vision models and look at their associated graphical models we will discuss each of these in detail in subsequent chapters however it is instructive to see them together figure shows the graphical model for a hidden markov model or hmm problem problem we observe a sequence of measurements xn n each of which tells us something about the corresponding discrete world state wn n adjacent world states are connected together so that the previous world state influences the current one and potentially resolves situations where the measurements are ambiguous a prototypical application would be tracking sequences of sign language gestures figure there is information at each frame about which gesture is present but it may be ambiguous however we can impose prior knowledge that certain signs are more likely to follow others using the hmm and get an improved result figure represents a markov tree again we observe a number of mea surements each of which provides information about the associated discrete world state however the world states are now connected in a tree structure a prototyp ical application would be human body fitting figure where each unknown world state represents a body part the parts of the body naturally have a tree structure and so it makes sense to build a model that exploits this figure illustrates the use of a markov random field or mrf as a prior the mrf here describes the world state as a grid of undirected connections each node might correspond to a pixel there is also a measurement variable associated with each world state variable these pairs are connected with directed links so overall this is a mixed model partly directed and partly undirected a prototypical application of an mrf in vision would be for semantic labeling figure the a b c d e f g h figure commonly used graphical models in computer vision a hidden markov model hmm b one possible application of the hmm is inter preting sign language sequences the choice of sign at time n depends on the sign at time n c markov tree d an example application is fitting a tree structured body model e markov random field mrf prior with in dependent observations f the mrf is often used as a prior distribution in semantic labeling tasks here the goal is to infer a binary label at each pixel determining whether it belongs to the cow or the grass g kalman filter an example application is tracking an object through a sequence it has the same graphical model as the hmm but the unknown quantities are continuous as opposed to discrete measurements constitute the rgb values at each position the world state at each pixel is a discrete variable that determines the class of object present i e cow vs grass the markov random field prior ties together all of the individual classifiers to help yield a solution that makes global sense finally figure shows the kalman filter this has the same graphical model as the hidden markov model but in this case the world state is continuous rather than discrete a prototypical application of the kalman filter is for tracking objects through a time sequence figure at each time we might want to know the position size and orientation of the hand however in a given frame the measurements might be poor the frame may be blurred or the object may be temporarily occluded by building a model that connects the estimates from adjacent frames we can increase the robustness to these factors earlier frames can resolve the uncertainty in the current ambiguous frame notice that all of these graphical models have directed links from the world w to the data x that indicate a relationship of the form p r x w hence they all construct a probability distribution over the data and are generative models we will also consider discriminative models but historically speaking generative models of this kind have been more important each model is quite sparsely con nected each data variable x connects only to one world state variable w and each world state variable connects to only a few others the result of this is that there are many conditional independence relations in the model we will exploit these redundancies to develop efficient algorithms for learning and inference we will return to all of these models later in the book we investigate the hidden markov model and the markov tree in chapter we discuss the markov random field in chapter and we will present the kalman filter in chapter the remaining part of this chapter answers two questions i how can we perform inference when there are a large number of unknown world variables ii what are the implications of using a directed graphical model vs an undirected one inference in models with many unknowns we will now consider inference in these models ideally we would compute the full posterior distribution p r n n using bayes rule however the unknown world states in the preceding models are generally much larger than previously considered in this book and this makes inference challenging for example consider the space of world states in the hmm example if we are given frames of video and there are common signs in the sign language then there are possible states it is clearly not practical to compute and store the posterior probability associated with each even when the world states are continuous computing and storing the parameters of a high dimensional prob ability model is still problematic fortunately there are alternative approaches to inference which we now consider in turn figure map solution vs max marginals solution the main figure shows the joint posterior distribution p r the map solu tion is at the peak of this distribu tion at highlighted in green the figure also shows the two marginal distributions p r and p r the maximum marginals solution is computed by in dividually finding the maximum of each marginal distributions which gives the solution highlighted in red for this distri bution this is very unrepresentative although these labels are individually likely they rarely co occur and the joint posterior for this combination has low probability finding the map solution one obvious possibility is to find the maximum a posteriori map solution n argmax p r n n n argmax p r n n p r n n this is still far from trivial the number of world states is extremely large so we cannot possibly explore every one and take the maximum we must employ intelligent and efficient algorithms that exploit the redundancies in the distribution to find the correct solution where possible however as we shall see for some models there is no known polynomial algorithm to find the map solution finding the marginal posterior distribution an alternative strategy is to find the marginal posterior distributions p r wn n p r n n n dwn n since each of these distributions is over a single label it is not implausible to compute and store each one separately obviously it is not possible to do this by directly computing the extremely large joint distribution and marginalizing it directly we must use algorithms that exploit the conditional independence relations in the distribution to efficiently compute the marginals drawing samples maximum marginals if we want a single estimate of the world state we could return the maximum values of the marginal distributions giving the criterion wˆn argmax p r wn n wn this produces estimates of each world state that are individually most probable but which may not reflect the joint statistics for example world state wn might be the most probable value for the nth world state and wm might be the most probable value for the mth world state but it could be that the posterior probability for this configuration is zero although the states are individually probable they never co occur figure sampling the posterior for some models it is intractable to compute either the map solution or the marginal distributions one possibility in this circumstance is to draw samples from the posterior distribution methods based on sampling the posterior would fall under the more general category of approximate inference as they do not normally return the true answer having drawn a number of samples from the posterior we can approximate the posterior probability distribution as a mixture of delta functions where there is one delta function at each of the sample positions alternatively we could make estimates of marginal statistics such as the mean and variance based on the sampled values or select the sample with the highest posterior probability as an estimate of the map state this latter approach has the advantage of being consistent with the full posterior distribution as opposed to maximum marginals which is not even if we cannot be sure that we have the correct answer an alternative way to compute a point estimate from a set of samples from the posterior is to compute the empirical max marginals we estimate the marginal probability distributions by looking at the marginal statistics of the samples in other words we consider one variable wn at a time and look at the distribution of different values observed for a discrete distribution this information is captured in a histogram for a continuous distribution we could fit a univariate model such as a normal distribution to these values to summarize them drawing samples we have seen that some of the approaches to inference require us to draw samples from the posterior distribution we will now discuss how to do this for both directed and undirected models and we will see that this is generally more straightforward in directed models graphical models figure ancestral sampling we work our way through the graph in an order red number that guarantees that the parents of every node are vis ited before the node itself at each step we draw a sample conditioned on the values of the samples at the parents this is guaranteed to pro duce a valid sample from the full joint distribution sampling from directed graphical models directed graphical models take the form of directed acyclic graphs of conditional probability relations that have the following algebraic form i p r n p r xn xpa n n it is relatively easy to sample from a directed graphical model using a technique known as ancestral sampling the idea is to sample each variable in the network in turn where the order is such that all parents of a node are sampled before the node itself at each node we condition on the observed sample values of the parents the simplest way to understand this is with an example consider the directed graphical model in figure whose probability distribution factorizes as p r p r p r p r p r p r to sample from this model we first identify as a node with no parents and draw a sample from the distribution p r let us say the observed sample at took value we now turn to the remaining nodes node is the only node in the network where all of the parents have been processed and so we turn our attention here next we draw a sample from the distribution p r to yield a sample we now see that we are not yet ready to sample from as not all of its parents have been sampled but we can sample from the distribution p r to yield the value continuing this process we draw from p r and finally from p r the resulting vector w α4 is guaranteed to be a valid sample from the full joint distribution p r an equivalent way to think about this algorithm is to consider it as working through the terms in the factorized joint distribution right hand side of equation sampling from each in turn conditioned on the previous values learning sampling from undirected graphical models unfortunately it is much harder to draw samples from undirected models except in certain special cases e g where the variables are continuous and gaussian or where the graph structure takes the form of a tree in general graphs we cannot use ancestral sampling because i there is no sense in which any variable is a parent to any other so we don t know which order to sample in and ii the terms φ in the factorization are not probability distributions anyway one way to generate samples from any complex high dimensional probability distribution is to use a markov chain monte carlo mcmc method the prin ciple is to generate a series chain of samples from the distribution so that each sample depends directly on the previous one hence markov however the generation of the sample is not completely deterministic hence monte carlo one of the simplest mcmc methods is gibbs sampling which proceeds as follows first we randomly choose the initial state x using any method we generate the next sample in the chain x by updating the state at each dimension algorithm xn n in turn in any order to update the nth dimension xn we fix the other n dimensions and draw from the conditional distribution p r xn n n where the set n n denotes all of the n variables xn except xn having modified every dimension in this way we obtain the second sample in the chain this idea is illustrated in figure for the multivariate normal distribution when this procedure is repeated a very large number of times so that the initial conditions are forgotten a sample from this sequence can be considered as a draw from the distribution p r n although this is not immediately obvious and a proof is beyond the scope of this book this procedure does clearly have some sensible properties since we are sampling from the conditional probability distri bution at each pixel we are more likely to change the current value to one which has an overall higher probability however the stochastic update rule provides the possibility of infrequently visiting less probable regions of the space for undirected graphical models the conditional distribution p r xn n n can be quite efficient to evaluate because of the conditional independence prop erties variable xn is conditionally independent of the rest of the nodes given its immediate neighbors and so computing this term only involves the immediate neighbors however overall this method is extremely inefficient it requires a large amount of computational effort to generate even a single sample sampling from directed graphical models is far easier learning in the section we argued that sampling from directed graphical models is con siderably easier than sampling from undirected graphical models in this section we consider learning in each type of model and come to a similar conclusion note that we are not discussing the learning of the graph structure here we are talking about learning the parameters of the model itself for directed graphical mod els these parameters would determine the conditional distributions p r xn xpa n graphical models a b figure gibbs sampling we generate a chain of samples by cycling through each dimension in turn and drawing a sample from the conditional distribution of that dimension given that the others are fixed a for this multivariate normal distribution we start at a random position x we alternately draw samples from the conditional distribution of the first dimen sion keeping the second fixed horizontal changes and the second dimension keeping the first fixed vertical changes for the multivariate normal these conditional distributions are themselves normal section each time we cycle through both of the dimensions we create a new sample x t b many samples generated using this method and for undirected graphical models they would determine the potential functions φc n learning in directed graphical models any directed graphical model can be written in the factorized form n p r xn p r xn xpa n θ n where the conditional probability relations form a directed acyclic graph and θ denotes the parameters of the model for example in the discrete distributions that we have focused on in this chapter an individual conditional model might be p r k λk where the parameters here are λk k in general the parameters can be learned using the maximum likelihood method by finding θˆ argmax rtti tt p r xi n xi pa n θ θ i n argmax r i log p r xi n xi pa n θ θ i n where xi n represents the nth dimension of the ith training example this criterion leads to simple learning algorithms and often the maximum likelihood parameters can be computed in closed form learning in undirected graphical models an undirected graphical model is written as p r x tt φ x θ where x xn and we have assumed that the training samples are independent however in this form we must constrain the parameters so that they ensure that each φc always returns a positive number a more practical approach is to re parameterize the undirected graphical model in the form of the gibbs distribution p r x exp z c ψc n θ so that we do not have to worry about constraints on the parameters given i training examples xi i we aim to fit parameters θ assuming that the training examples are independent the maximum likelihood solution is θˆ argmax exp z θ i ψc xi θ θ i c argmax r i log z θ ψc xi θ θ i c where as usual we have taken the log to simplify the expression to maximize this expression we calculate the derivative of the log likelihood l with respect to the parameters θ i c l i log z θ ψc xi θ θ i θ log xi i c c c θ θ ψc xi θ ll c ψc xi θ θ i c the second term is readily computable but the first term involves an intractable sum over all possible values of the variable x we cannot compute the deriva tive with respect to the parameters for reasonable sized models and so learning is difficult moreover we cannot evaluate the original probability expression equa tion as this too contains an intractable sum consequently we can t compute the derivative using finite differences either in short we can neither find an algebraic solution nor use a straightforward optimization technique as we cannot compute the gradient the best that we can do is to approximate the gradient algorithm contrastive divergence one possible solution to this problem is the contrastive divergence algorithm this is a method for approximating the gradient of the log likelihood with respect to parameters θ for functions with the general form p r x z θ f x θ where z θ x f x θ is the normalizing constant and the derivative of the log likelihood is log p r x θ log z θ θ log f x θ θ the main idea behind contrastive divergence follows from some algebraic manipu lation of the first term log z θ θ z θ z θ θ z θ x f x θ θ f x θ f x θ log f x θ p r x log f x θ θ x where we have used the relation log f x x f x x f x between the third and fourth lines the final term in equation is the expectation of the derivative of log f x θ we cannot compute this exactly but we can approximate it by drawing j inde pendent samples x from the distribution to yield j log f x θ log z θ p r x log f x θ j figure the contrastive diver gence algorithm changes the parame ters so that the un normalized distri bution increases at the observed data points blue crosses but decreases at sampled data points from the model these two components counterbal ance one another and ensure that the likelihood increases when the model fits the data these two forces will can cel out and the parameters will remain constant with i training examples xi i the gradient of the log likelihood l is hence j log f x θ i l i j log f xi θ a visual explanation of this expression is presented in figure the gradient points in a direction that i increases the logarithm of the un normalized function at the data points xi but ii decreases the same quantity in places where the model believes the density is high i e the samples x j when the model fits the data these two forces cancel out and the parameters will stop changing this algorithm requires us to draw samples x from the model at each iteration of the optimization procedure in order to compute the gradient unfortunately the only way to draw samples from general undirected graphical models is to use costly markov chain monte carlo methods such as gibbs sampling section and this is impractically time consuming in practice it has been found that even approximate samples will do one method is to re start j i samples at the data points at each iteration and do just a few mcmc steps surprisingly this works well even with a single step a second approach is to start with the samples from the previous iteration and perform a few mcmc steps so that the samples are free to wander without restarting this technique is known as persistent contrastive divergence discussion in this chapter we introduced directed and undirected graphical models each represents a different type of factorization of the joint distribution a graphical model implies a set of independence and conditional independence relations there are some sets that can only be represented by directed graphical models others that can only be represented by undirected graphical models some that can be represented by both and some that cannot be represented by either we presented a number of common vision models and examined their graphical models each had sparse connections and hence many conditional independence relations in subsequent chapters we will exploit these redundancies to develop efficient learning and inference algorithms the world state is usually very high dimensional in these models and so we discussed alternative forms of inference including maximum marginals and sampling finally we looked at the implications of choosing directed or undirected graph ical models for sampling and for learning we concluded that it is generally more straightforward to draw samples from directed graphical models moreover it is also easier to learn directed graphical models the best known learning algorithm for general undirected graphical models requires us to draw samples which is itself challenging notes notes graphical models for a readable introduction to graphical models consult jordan or bishop for a more comprehensive overview i would recommend barber for an even more encyclopaedic resource consult koller friedman contrastive divergence the contrastive divergence algorithm was introduced by hin ton further information about this technique can be found in carreira perpin an hinton and bengio delalleau problems problem the joint probability model between variables xn factorizes as p r p r p r p r x7 p r p r x7 p r x4 p r x5 x4 draw a directed graphical model relating these variables which variables form the markov blanket of variable problem write out the factorization corresponding to the directed graphical model in figure 14a a b figure a graphical model for problem b graphical model for problem problem an undirected graphical model has the form p r x x φ x x x φ x x x φ x x φ x x z draw the undirected graphical model that corresponds to this factorization graphical models figure factor graphs contain one node square per factor in the joint pdf as well as one node circle per variable each factor node is connected to all of the variables that belong to that factor this type of graphical model can distinguish between the undirected graphical models a p r and b p r x3 x3 z z problem write out the factorization corresponding to the undirected graphical model in figure 14b problem consider the undirected graphical model defined over binary values xi defined by p r x x x x φ x x φ x x φ x x φ x x z where the function φ is defined by φ φ φ φ compute the probability of each of the possible states of this system problem what is the markov blanket for each of the variables in figures and problem show that the stated patterns of independence and conditional indepen dence in figure and figure are true problem a factor graph is a third type of graphical model that depicts the fac torization of a joint probability as usual it contains a single node per variable but it also contains one node per factor usually indicated by a solid square each factor variable is connected to all of the variables that are contained in the associated term in the factorization by undirected links for example the factor node corresponding to the term p r x3 in a directed model would connect to all three variables and x3 similarly the factor node corresponding to the term in an undirected model would connect variables and figure shows two examples of factor graphs draw the factor graphs corresponding to the graphical models in figures and you must first establish the factorized joint distribution associated with each graph problem what is the markov blanket of variable in figure problem what is the markov blanket of variable in figure 9e chapter models for chains and trees in this chapter we model the relationship between a multidimensional set of measurements xn n and an associated multidimensional world state wn n when n is large it is not practical to describe the full set of dependencies between all of these variables as the number of model parameters will be too great instead we construct models where we only directly describe the probabilistic dependence between variables in small neighborhoods in particular we will consider models in which the world variables wn n are structured as chains or trees we define a chain model to be one in which the world state variables wn n are connected to only the previous variable and the subsequent variable in the associated graphical model as in figure we define a tree model to be one in which the world variables have more complex connections but so that there are no loops in the resulting graphical model importantly we disregard the directionality of the connections when we assess whether a directed model is a tree hence our definition of a tree differs from the standard computer science usage we will also make the following assumptions the world states wn are discrete there is an observed data variable xn associated with each world state wn the nth data variable xn is conditionally independent of all other data vari ables and world states given the associated world state wn these assumptions are not critical for the development of the ideas in this chapter but are typical for the type of computer vision applications that we consider we will show that both maximum a posteriori and maximum marginals inference are tractable for this sub class of models and we will discuss why this is not the case when the states are not organized as a chain or a tree to motivate these models consider the problem of gesture tracking here the goal is to automatically interpret sign language from a video sequence figure we observe n frames xn n of a video sequence and wish to infer the n discrete variables wn n that encode which sign is present in each of the n frames the data at time n tells us something about the sign at time n but may be insufficient to specify it accurately consequently we also model dependencies between adjacent figure interpreting sign language we observe a sequence of images of a person using sign language in each frame we extract a vector xn describing the shape and position of the hands the goal is to infer the sign wn that is present unfortunately the visual data in a single frame may be ambiguous we improve matters by describing probabilistic connections between adjacent states wn and wn we impose knowledge about the likely sequence of signs and this helps disambiguate any individual frame images from purdue rvl slll asl database wilbur kak world states we know that the signs are more likely to appear in some orders than others and we exploit this knowledge to help disambiguate the sequence since we model probabilistic connections only between adjacent states in the time series this has the form of a chain model models for chains in this section we will describe both a directed and an undirected model for de scribing chain structure and show that these two models are equivalent directed model for chains the directed model describes the joint probability of a set of continuous measure ments xn n and a set of discrete world states wn n with the graphical model shown in figure the tendency to observe the measurements xn given that state wn takes value k is encoded in the likelihood p r xn wn k the prior probability of the first state is explicitly encoded in the discrete distribution p r but for simplicity we assume that this is uniform and omit it from most of the ensuing discussion the remaining states are each dependent on the previous one and this information is captured in the distribution p r wn wn this is sometimes termed the markov assumption hence the overall joint probability is p r n n n n p r xn wn ttn p r wn wn this is known as a hidden markov model hmm the world states wn n in the directed model have the form of a chain and the overall model has the form of a tree as we shall see these properties are critical to our ability to perform inference undirected model for chains the undirected model see section describes the joint probability of the mea surements xn n and the world states wn n with the graphical model shown in figure the tendency for the measurements and the data to take certain values is encoded in the potential function φ xn wn this function always returns positive values and returns larger values when the measurements and the world state are more compatible the tendency for adjacent states to take certain values is encoded in a second potential function ζ wn wn which returns larger values when the adjacent states are more compatible hence the overall probability is n p r n n z n φ xn wn ttn ζ wn wn once more the states form a chain and the overall model has the form of a tree there are no loops equivalence of models when we take a directed model and make the edges undirected we usually create a different model however comparing equations and reveals that these two models represent the same factorization of the joint probability density in this special case the two models are equivalent this equivalence becomes even more apparent if we make the substitutions p r xn wn φ xn wn n p r wn wn zi ζ wn wn where zn and zni are normalizing factors which form the partition function n z n ttn zni since the directed and undirected versions of the chain model are equivalent we will continue our discussion in terms of the directed model alone models for chains and trees a b figure models for chains a directed model there is one observation variable xn for each state variable wn and these are related by the condi tional probability p r xn wn downward arrows each state wn is related to the previous one by the conditional probability p r wn wn horizontal arrows b undirected model here each observed variable xn is related to its associated state variable wn via the potential function φ xn wn and the neighboring states are connected via the potential function ζ wn wn hidden markov model for sign language application we will now briefly describe how this directed model relates to the sign language application we preprocess the video frame to create a vector xn that represents the shape of the hands for example we might just extract a window of pixels around each hand and concatenate their rgb pixel values we now model the likelihood p r xn wn k of observing this measurement vector given that the sign wn in this image takes value k a very simple model might assume that the measurements have a normal distribution with parameters that are contingent on which sign is present so that p r xn wn k normxn µk σk we model the sign wn as a being categorically distributed where the parameters depend on the previous sign wn so that p r wn wn k catwn λk this hidden markov model has parameters µk σk λk k for most of this chapter we will assume that these parameters are known but we return briefly to the issue of learning in section we now turn our focus to inference in this type of model map inference for chains consider a chain with n unknown variables wn n each of which can take k possible values here there are kn possible states of the world for real world problems this means that there are far too many states to evaluate exhaustively figure dynamic programming formulation each solution is equated with a particular path from left to right through an acyclic directed graph the n columns of the graph represent variables n and the k rows represent possible states k the nodes and edges of the graph have costs associated with the unary and pairwise terms respectively any path from left to right through the graph has a cost that is the sum of the costs at all of the nodes and edges that it passes through optimizing the function is now equivalent to finding the path with the least cost we can neither compute the full posterior distribution nor search directly through all of the states to find the maximum a posteriori map estimate fortunately the factorization of the joint probability distribution the condi tional independence structure can be exploited to find more efficient algorithms for map inference than brute force search the map solution is given by n argmax p r n n n argmax p r n n n argmin log p r n n n where line follows from bayes rule we have reformulated this as a minimization problem in line substituting in the expression for the log probability equation we get n argmin n r n log p r xn wn n log p r wn wn which has the general form n argmin n n n un wn n pn wn wn where un is a unary term and depends only on a single variable wn and pn is a pairwise term depending on two variables wn and wn in this instance the unary and pairwise terms can be defined as un wn log p r xn wn pn wn wn log p r wn wn any problem that has the form of equation can be solved in polynomial time using the viterbi algorithm which is an example of dynamic programming dynamic programming viterbi algorithm to optimize the cost function in equation we first visualize the problem with algorithm a graph with vertices v n k n k n k the vertex v n k represents choosing the kth world state at the nth variable figure vertex vn k is connected by a directed edge to each of the vertices vn k k at the next pixel position hence the organization of the graph is such that each valid horizontal path from left to right represents a possible solution to the problem it corresponds to assigning one value k k to each variable wn we now attach the costs un wn k to the vertices vn k we also attach the costs pn wn k wn l to the edges joining vertices vn l to vn k we define the total cost of a path from left to right as the sum of the costs of the edges and vertices that make up the path now every horizontal path represents a solution and the cost of that path is the cost for that solution we have reformulated the problem as finding the minimum cost path from left to right across the graph finding the minimum cost the approach to finding the minimum cost path is simple we work through the graph from left to right computing at each vertex the minimum possible cumulative cost sn k to arrive at this point by any route when we reach the right hand side we compare the k values sn and choose the minimum this is the lowest possible cost for traversing the graph we now retrace the route we took to reach this point using information that was cached during the forward pass the easiest way to understand this method is with a concrete example figures and the reader is encouraged to scrutinize these figures before continuing a more formal description is as follows our goal is to assign the minimum possible cumulative cost sn k for reaching vertex vn k starting at the left hand side we set the first column of vertices to the unary costs for the first variable k k the cumulative total k for the kth vertex in the second column should represent the minimum possible cumulative cost to reach this point to calculate this we consider the k possible predecessors and compute the cost for reaching this vertex by each possible route we set k to the minimum of these values and store the route by which we reached this vertex so that map inference for chains a b c figure dynamic programming a the unary cost un wn k is given by the number above and to the right of each node the pairwise costs pn wn wn are zero if wn wn horizontal two if wn wn and otherwise this favors a solution that is mostly constant but can also vary smoothly for clarity we have removed the edges with infinite cost as they cannot become part of the solution we now work from left to right computing the minimum cost sn k for arriving at vertex vn k by any route b for vertices k k the minimum cost is just the unary cost associated with that vertex we have stored the values inside the circle representing the respective vertex c to compute the minimum cost at vertex n k we must consider two possible routes the path could have traveled horizontally from vertex giving a total cost of or it may have come diagonally upward from vertex with cost since the former route is cheaper we use this cost storing at the vertex and also remembering the path used to get here now we repeat this procedure at vertex where there are three possible routes from vertices and here it turns out that the best route is from and has total cumulative cost of example continued in figure a b c figure dynamic programming worked example continued from fig ure a having updated the vertices at pixel n we carry out the same procedure at pixel n accumulating at each vertex the minimum total cost to reach this point b we continue updating the minimum cu mulative costs sn k to arrive at pixel n in state k until we reach the right hand side c we identify the minimum cost from among the right most vertices in this case it is vertex which has cost this is the minimum possible cost for traversing the graph by tracing back the route that we used to arrive here red arrows we find the world state at each pixel that was responsible for this cost figure tree based models as before there is one observation xn for each world state wn and these are related by the conditional probabil ity p r xn wn however disregard ing the directionality of the edges the world states are now connected as a tree vertex has two incoming connections which means that there is a three wise term p r in the factorization the tree struc ture means it is possible to perform map and max marginals inference efficiently k k min l k l l more generally to calculate the cumulative totals sn k we use the recursion sn k un wn k min sn l pn wn k wn l and we also cache the route by which this minimum was achieved at each stage when we reach the right hand side we find the value of the final variable wn that minimizes the total cost wˆn argmin sn k k and set the remaining labels n according to the route that we followed to get to this value this method exploits the factorization structure of the joint probability between the observations and the states to make vast computational savings the cost of this procedure is n as opposed to kn for a brute force search through every possible solution map inference for trees to show how map inference works in tree structured models consider the model in figure for this graph the prior probability over the states factorizes as p r p r p r p r p r p r p r and the world states have the structure of a tree disregarding the directionality of the edges once more we can exploit this factorization to compute the map solution effi ciently our goal is to find problem problem algorithm problem problem models for chains and trees a b d c e figure dynamic programming example for tree model in figure a table of three wise costs at vertex this is a k k k table consisting of the costs associated with p r pairwise costs are as for the example in figure b tree structured model with unary and pairwise costs attached c we work from the leaves finding the minimal possible cost sn k to reach vertex n in state k as in the original dynamic programming formulation d when we reach the vertex above a branch here vertex we find the minimal possible cost considering every combination of the incoming states e we continue until we reach the root there we find the minimum overall cost and trace back making sure to split at the junction according to which pair of states was chosen argmax n log p r xn wn log p r by a similar process to that in section we can rewrite this as a minimization problem with the following cost function argmin n un wn as before we reformulate this cost function in terms of finding a route through a graph see figure the unary costs un are associated with each vertex the pairwise costs pm are associated with edges between pairs of adjacent vertices the three wise cost is associated with the combination of states at the point where the tree branches our goal now is to find the least cost path from all of the leaves simultaneously to the root we work from the leaves to the root of the tree at each stage computing sn k the cumulative cost for arriving at this vertex see worked example in figure for the first four vertices we proceed as in standard dynamic programming k k k k min l k l l k k k k min k l l when we come to the branch in the tree we try to find the best combination of routes to reach the nodes for variable we must now minimize over both variables to compute the next term in other words k k min l m k l m l m finally we compute the last terms as normal so that k k min l k l l now we find the world state associated with the minimum of this final sum and trace back the route that we came by as before splitting the route appropriately at junctions in the tree dynamic programming in this tree has a greater computational complexity than dynamic programming in a chain with the same number of variables as we must minimize over two variables at the junction in the tree the overall complexity is proportional to kw where w is the maximum number of variables over which we must minimize for directed models w is equal to the largest number of incoming connections at any vertex for undirected models w will be the size of the largest clique it should be noted that for undirected models the critical property that allows dynamic programming solutions is that the cliques themselves form a tree see figure marginal posterior inference for chains in section we demonstrated that it is possible to perform map inference in chain models efficiently using dynamic programming in this section we will con sider a different form of inference we will aim to calculate the marginal distribution p r wn n over each state variable wn separately consider computing the marginal distribution over the variable wn by bayes rule we have p r wn n p r wn n pr w p r n n n the right hand side of this equation is computed by marginalizing over all of the other state variables except wn so we have p r wn n p r n n ttn p r xn wn p r ttn p r wn wn wn n n unfortunately in its most basic form this marginalization involves summing over n dimensions of the n dimensional probability distribution since this discrete probability distribution contains kn entries computing this summation directly is not practical for realistic sized problems to make progress we must again exploit the structured factorization of this distribution computing one marginal distribution we will first discuss how to compute the marginal distribution p r wn n for the last variable in the chain wn in the following section we will exploit these ideas to compute all of the marginal distributions p r wn n simultaneously we observe that not every term in the product in equation is relevant to every summation we can re arrange the summation terms so that only the variables over which they sum are to the right p r wn n p r xn wn p r p r p r p r p r then we proceed from right to left computing each summation in turn this technique is known as variable elimination let us denote the rightmost two terms as p r p r then we sum over to compute the function p r p r at the nth stage we compute fn wn p r xn wn p r wn wn fn wn wn and we repeat this process until we have computed the full expression we then normalize the result to find the marginal posterior p r wn n equation problem this solution consists of n summations over k values it is much more efficient to compute than explicitly computing all kn solutions and marginalizing over n dimensions forward backward algorithm in the previous section we showed an algorithm that could compute the marginal posterior distribution p r wn n for the last world state wn it is easy to adapt this method to compute the marginal posterior p r wn n over any other problem variable wn however we usually want all of the marginal distributions and it is inefficient to compute each separately as much of the effort is replicated the goal of this section is to develop a single procedure that computes the marginal posteriors for all of the variables simultaneously and efficiently using a technique known as the forward backward algorithm the principle is to decompose the marginal posterior into two terms p r wn n p r wn n p r wn n p r xn n wn n p r wn n p r xn n wn where the relation between the second and third line is true because n and xn n are conditionally independent given wn as can be gleaned from fig ure we will now focus on finding efficient ways to calculate each of these two terms problem algorithm forward recursion let us consider the first term p r wn n we can exploit the recursion p r wn n p r wn wn n wn p r wn xn wn n p r wn n wn p r xn wn wn n p r wn wn n p r wn n wn p r xn wn p r wn wn p r wn n wn where we have again applied the conditional independence relations implied by the graphical model between the last two lines the term p r wn n is exactly the intermediate function fn wn that we cal culated in the solution for the single marginal distribution in the previous section we have reproduced the recursion fn wn p r xn wn p r wn wn fn wn wn but this time we based the argument on conditional independence rather than the factorization of the probability distribution using this recursion we can efficiently compute the first term of equation for all n in fact we were already doing this in our solution for the single marginal distribution p r wn n backward recursion now consider the second term p r xn n wn from equation our goal is to develop a recursive relation for this quantity so that we can compute it efficiently for all n this time the recursion works backwards from the end of the chain to the front so our goal is to establish an expression for p r xn n wn in terms of p r xn n wn p r xn n wn p r xn n wn wn wn p r xn n wn wn p r wn wn wn p r xn n xn wn wn p r xn wn wn p r wn wn wn p r xn n wn p r xn wn p r wn wn wn here we have again applied the conditional independence relations implied by the graphical model between the last two lines denoting the probability p r xn n wn as bn wn we see that we have the recursive relation bn wn p r xn wn p r wn wn bn wn wn we can use this to compute the second term in equation efficiently for all n forward backward algorithm we can now summarize the forward backward algorithm to compute the marginal posterior probability distribution for all n first we observe equation that the marginal distribution can be computed as p r wn n p r wn n p r xn n wn fn wn bn wn we recursively compute the forward terms using the relation fn wn p r xn wn p r wn wn fn wn 33 wn where we set p r p r we recursively compute the backward terms using the relation bn wn p r xn wn p r wn wn bn wn wn where we set bn wn to the constant value k finally to compute the nth marginal posterior distribution we take the product of the associated forward and backward terms and normalize belief propagation the forward backward algorithm can be considered a special case of a more general technique called belief propagation here the intermediate functions f and b are considered as messages that convey information about the variables in this section we describe a version of belief propagation known as the sum product algorithm this does not compute the marginal posteriors any faster than the forward backward algorithm but it is much easier to see how to extend it to models based on trees the sum product algorithm operates on a factor graph a factor graph is a new type of graphical model that makes the factorization of the joint probability more explicit it is very simple to convert directed and undirected graphical models to factor graphs as usual we introduce one node per variable for example variables and all have a variable node associated with them we also intro duce one function node per term in the factorized joint probability distribution in a directed model this would represent a conditional probability term such as problem figure factor graph for chain model there is one node per variable circles and one function node per term in the factorization squares each function node connects to all of the variables associated with this term p r and in an undirected model it would represent a potential function such as φ we then connect each function node to all of the variable nodes relevant to that term with undirected links so in a directed model a term like p r would result in a function node that connects to and in an undirected model a term like would result in a function node that connects to and figure shows the factor graph for the chain model algorithm sum product algorithm the sum product algorithm proceeds in two phases a forward pass and a backward pass the forward pass distributes evidence through the graph and the backward pass collates this evidence both the distribution and collation of evidence are accomplished by passing messages from node to node in the factor graph every edge in the graph is connected to exactly one variable node and each message is defined over the domain of this variable there are three types of messages a message mzp gq from an unobserved variable zp to a function node gq is given by mzp gq r ne p q mgr zp where ne p returns the set of the neighbors of zp in the graph and so the expression ne p q denotes all of the neighbours except q in other words the message from a variable to a function node is the pointwise product of all other incoming messages to the variable it is the combination of other beliefs a message mzp gq from an observed variable zp z p to a function node gq is given by mzp gq δ z p in other words the message from an observed node to a function conveys the certain belief that this node took the observed value a message mgp zq from a function node gp to a recipient variable zq is defined as mgp zq ne p q gp ne p r ne p q mzr gp this takes beliefs from all variables connected to the function except the recipient variable and uses the function gp to convert these to a belief about the recipient variable in the forward phase the message passing can proceed in any order as long as the outgoing message from any variable or function is not sent until all the other incoming messages have arrived in the backward pass the messages are sent in the opposite order to the forward pass finally the marginal distribution at node zp can be computed from a product of all of the incoming messages from both the forward and reverse passes so that p r zp r ne p mgr zp a proof that this algorithm is correct is beyond the scope of this book however to make this at least partially convincing and more concrete we will work through these rules for the case of the chain model figure and we will show that exactly the same computation occurs as for the forward backward algorithm sum product algorithm for chain model the factor graph for the chain solution annotated with messages is shown in figure we will now describe the sum product algorithm for the chain model forward pass we start by passing a message from node to the function node using rule this message is a delta function at the observed value x so that δ x now we pass a message from function to node using rule we have p r δ x p r x by rule the message from node to function is simply the product of the incoming nodes and since there is only one incoming node this is just figure sum product algorithm for chain model forward pass the sum product algorithm has two phases in the forward phase messages are passed through the graph in an order such that a message cannot be sent until all incoming messages are received at the source node so the message cannot be sent until the messages and have been received p r x by rule the message from function to node is computed as p r p r continuing this process the messages from x2 to and to are δ x mg2 p r x2 x and the message from to is p r x2 x2 p r p r x a clear pattern is emerging the message from node wn to function gn n is equal to the forward term from the forward backward algorithm mwn gn n fn wn p r wn n in other words the sum product algorithm is performing exactly the same compu tations as the forward pass of the forward backward algorithm figure factor graph corre sponding to tree model in figure there is one function node connecting each world state variable to its asso ciated measurement and these corre spond to the terms p r xn wn there is one function node for each of the three pairwise terms p r p r and p r and this is connected to both contributing variables the function node cor responding to the three wise term p r has three neighbors and backward pass when we reach the end of the forward pass of the belief propagation we initiate the backward pass there is no need to pass messages toward the observed variables xn since we already know their values for certain hence we concentrate on the horizontal connections between the unobserved variables i e along the spine of the model the message from node wn to function gn n is given by mwn gn n p r xn x n wn and the message from gn n to wn is given by mgn n wn p r wn wn p r xn x n wn wn in general we have mgn n wn p r wn wn p r xn wn mgn n wn wn bn wn which is exactly the backward recursion from the forward backward algorithm collating evidence finally to compute the marginal probabilities we use the relation p r wn n m ne n mgm wn and for the general case this consists of three terms p r wn n mgn n wn mgn wn mgn n wn mwn gn n mgn n wn fn wn bn wn models for chains and trees a b figure converting an undirected model to a factor graph a undi rected model b corresponding factor graph there is one function node for each maximal clique each clique which is not a subset of another clique although there was clearly a loop in the original graph there is no loop in the factor graph and so the sum product algorithm is still applicable where in the second line we have used the fact that the outgoing message from a variable node is the product of the incoming messages we conclude that the sum product algorithm computes the posterior marginals in exactly the same way as the forward backward algorithm marginal posterior inference for trees problem to compute the marginals in tree structured models we simply apply the sum product algorithm to the new graph structure the factor graph for the tree in figure is shown in figure the only slight complication is that we must ensure that the first two incoming messages to the function relating variables and have arrived before sending the outgoing message this is very similar to the order of operations in the dynamic programming algorithm for undirected graphs the key property is that the cliques not the nodes form a tree for example there is clearly a loop in the undirected model in figure but when we convert this to a factor graph the structure is a tree figure for models with only pairwise cliques the cliques always form a tree if there are no loops in the original graphical model learning in chains and trees so far we have only discussed inference for these models here we briefly discuss learning which can be done in a supervised or unsupervised context in the super vised case we are given a training set of i matched sets of states win i n and beyond chains and trees data xin i n in the unsupervised case we only observe the data xin i n supervised learning for directed models is relatively simple we first isolate the part of the model that we want to learn for example we might learn the parameters θ of p r xn wn θ from paired examples of xn and wn we can then learn these parameters in isolation using the ml map or bayesian methods unsupervised learning is more challenging the states wn are treated as hidden variables and the em algorithm is applied in the e step we compute the posterior marginals over the states using the forward backward algorithm in the m step we use these marginals to update the model parameters for the hidden markov model the chain model this is known as the baum welch algorithm as we saw in the previous chapter learning in undirected models can be chal lenging we cannot generally compute the normalization constant z and this in turn prevents us from computing the derivative with respect to the parameters however for the special case of tree and chain models it is possible to compute z efficiently and learning is tractable to see why this is the case consider the undirected model from figure which we will treat here as representing the conditional distribution n p r n n z n φ xn wn ttn ζ wn wn since the data nodes xn n are fixed this model is known as a conditional random field the unknown constant z now has the form z ttn φ xn wn ttn ζ wn wn wn n n we have already seen that it is possible to compute this type of sum efficiently using a recursion equation hence z can be evaluated and maximum likelihood learning can be performed in this model without the need for contrastive divergence beyond chains and trees unfortunately there are many models in computer vision that do not take the form of a chain or a tree of particular importance are models that are structured to have one unknown wn for each rgb pixel xn in the image these models are naturally structured as grids and the world states are each connected to their four pixel neighbors in the graphical model figure stereo vision segmentation de noising super resolution and many other vision problems can all be framed in this way we devote the whole of the next chapter to grid based problems but we will briefly take the time to examine why the methods developed in this chapter are not suitable consider a simple model based on a grid figure illustrates why models for chains and trees a b figure grid based models for many vision problems the natural description is a grid based model we observe a grid of pixel values xn n and wish to infer an unknown world state wn n associated with each site each world state is connected to its neighbors these connections are usually applied to ensure a smooth or piecewise smooth solution a directed grid model b undirected grid model conditional random field figure dynamic programming fails when there are undirected loops here we show a image where we have performed a na ıive forward pass through the variables on re tracing the route we see that the two branches disagree over which state the first variable took for a coherent so lution the cumulative minimum costs at node we should have forced the two paths to have common ancestors with a large number of ancestors this is too computationally expensive to be practical we cannot blindly use dynamic programming to compute the map solution to compute the minimum cumulative cost sn at each node we might na ıvely proceed as normal k k k k min l w2 k l l k k min l k l l now consider the fourth term unfortunately figure pruning graphs with loops one approach to dealing with models with loops is simply to prune the connections until the loops are re moved this graphical model is the model from figure after such a pruning process most of the con nections are retained but now the re maining structure is a tree the usual approach to pruning is to associate a strength with each edge so that weaker edges are more desirable then we compute the minimum spanning tree based in these strengths and discard any connections that do not form part of the tree k wk min w2 l m t k w2 l m l m the reason for this is that the partial cumulative sums and at the two previous vertices both rely on minimizing over the same variable however they did not necessarily choose the same value at if we were to trace back the paths we took the two routes back to vertex one might predict a different answer to properly calculate the minimum cumulative cost at node k we would have to take account of all three ancestors the recursion is no longer valid and the problem becomes intractable once more similarly we cannot perform belief propagation on this graph the algorithm requires us to send a message from a node only when all other incoming messages have been received however the nodes w2 and all simultaneously require messages from one another and so this is not possible inference in graphs with loops although the methods of this chapter are not suitable for models based on graphs with loops there are a number of ways to proceed prune the graph an obvious idea is to prune the graph by removing edges until what is left has a tree structure figure the choice of which edges to prune will depend on the real world problem combine variables a second approach is to combine variables together until what remains has the structure of a chain or tree for example in figure we combine the variables w2 and to make a new variable and the variables and to form continuing in this way we form a model that has a chain structure if each of the original variables had k states then the compound variables will have states problem figure combining variables a a this graphical model contains loops b we form three com pound variables each of which con sists of all of the variables in one of the original columns these are now connected by a chain structure however the price we pay is that if there were k states for each orig inal variable the compound vari ables now have k states and so inference will be more expensive in general the merging of variables can be automated using the junction tree algorithm unfortunately this example illustrates why this approach will not work for large grid models we must merge together so many variables that the resulting compound variable has too many states to work with loopy belief propagation another idea is to simply apply belief propaga tion regardless of the loops all messages are initialized to uniform and then the messages are repeatedly passed in some order according to the normal rules this algorithm is not guaranteed to converge to the correct solution for the marginals or indeed to converge at all but in practice it produces useful results in many situations sampling approaches for directed graphical models it is usually easy to draw samples from the posterior these can then be aggregated to compute an empirical estimate of the marginal distributions other approaches there are several other approaches for exact or ap proximate inference in graphs including tree reweighted message passing and graph cuts the latter is a particularly important class of algorithm and we devote most chapter to describing it applications the models in this chapter are attractive because they permit exact map inference they have been applied to a number of problems in which there are assumed spatial or temporal connections between parts of a model figure gesture tracking from starner et al a camera was mounted on a baseball cap inset looking down at the users hands the camera image main figure was used to track the hands in a hmm based system that could accurately classify a word lexicon and worked in real time each word was associated with four states in the hmm the sys tem was based on a compact descrip tion of the hand position and orienta tion within each frame adapted from starner et al qc springer gesture tracking the goal of gesture tracking is to classify the position wn n of the hands within each of the n captured frames from a video sequence into a discrete set of possible gestures wn k based on extracted data xn n from those frames starner et al presented a wearable system for automatically interpreting sign language gestures a camera mounted in the user hat captured a top down view of their hands figure the positions of the hands were identified by using a per pixel skin segmentation technique see section the state of each hand was characterized in terms of the position and shape of a bounding ellipse around the associated skin region the final eight dimensional data vector x concatenated these measurements from each hand to describe the time sequences of these measurements starner et al developed a hidden markov model based system in which the states wn each rep resented a part of a sign language word each of these words was represented by a progression through four values of the state variable w representing the various stages in the associated gesture for that word the progression through these states might last any number of time steps each state can be followed by itself so it can cycle indefinitely but must come in the required order they trained the system using training sentences they used a dynamic programming method to es timate the most likely states w and achieved recognition accuracy of using a word lexicon with a test set of sentences they found that performance was further improved if they imposed knowledge about the fixed grammar of each phrase pronoun verb noun adjective pronoun remarkably the system worked at a rate of frames a second stereo vision in dense stereo vision we are given two images of the same scene taken from slightly different positions for our purposes we will assume that they have been preprocessed so that for each pixel in image the corresponding pixel is on the same scanline in image a process known as rectification see chapter the figure dense stereo vision a b two images taken from slightly different positions the corresponding point for every pixel in the first image is somewhere on the same scanline in the second image the horizontal offset is known as the disparity and is inversely related to depth c ground truth disparity map for this image d close up of part of first image with two pixels highlighted e close up of second image with potential corresponding pixels highlighted f rgb values for red pixel dashed lines in first image and as a function of the position in second image solid lines at the correct disparity there is very little difference between the rgb values in the two images and so g the likelihood that this disparity is correct is large h i for the green pixel which is in a smoothly changing region of the image there are many positions where the rgb values in the second image are similar and hence many disparities have high likelihoods the solution is ambiguous horizontal offset or disparity between corresponding points depends on the depth our goal is to find the discrete disparity field w given the observed images x and x from which the depth of each pixel can be recovered figure we assume that the pixel in image should closely resemble the pixel at the appropriate offset disparity in image and any remaining small differences are treated as noise so that p r x wm n k norm 55 applications figure dense stereo results recovered disparity maps for a indepen dent pixels model b independent scanlines model and c tree based model of veksler where w m n is the disparity at pixel m n of image x is the rgb vector from pixel m n of image and x is the rgb vector from pixel m n of image unfortunately if we compute the maximum likelihood disparities wm n at each pixel separately the result is extremely noisy figure as figure illustrates the choice of disparity is ambiguous in regions of the image where there are few visual changes in layman terms if the nearby pixels are all similar it is difficult to establish with certainty which corresponds to a given position in the other image to resolve this ambiguity we introduce a prior p r w that encourages piecewise smoothness in the disparity map we are exploiting the fact that we know that the scene mainly consists of smooth surfaces with occasional jumps in disparity at the edge of objects one possible approach attributed originally to ohta kanade to recov ering the disparity is to use an independent prior for each scanline so that m p r w p r wm m where each scanline was organized into a chain model figure so that n p r wm p r wm p r wm n wm n n the distributions p r wm n wm n are chosen so that they allot a high probability when adjacent disparities are the same an intermediate probability when adjacent disparities change by a single value and a low probability if they take values that are more widely separated hence we encourage piecewise smoothness map inference can be performed within each scanline separately using the dynamic programming approach and the results combined to form the full disparity field w although this definitely improves the fidelity of the solution it results in a characteristic streaky result figure these artifacts are due to the erroneous assumption that the scanlines are independent to get an improved problem figure pictorial structure this face model consists of seven parts red dots which are connected together in a tree like structure red lines the possible positions of each part are indicated by the yellow boxes al though each part can take several hun dred pixel positions the map po sitions can be inferred efficiently by exploiting the tree structure of the graph using a dynamic programming approach localizing facial features is a common element of many face recog nition pipelines solution we should smooth in the vertical direction as well but the resulting grid based model will contain loops making map inference problematic veksler addressed this problem by pruning the full grid based model until it formed a tree each edge was characterized by a cost that increased if the associated pixels were close to large changes in the image at these positions either there is texture in the image and so the disparity is relatively well defined or there is an edge between two objects in the scene in either case there is no need to apply a smoothing prior here hence the minimum spanning tree tends to retain edges in regions where they are most needed the minimum spanning tree can be computed using a standard method such as prim algorithm see cormen et al the results of map inference using this model are shown in figure the solution is piecewise smooth in both directions and is clearly superior to either the independent pixels model or the independent scanline approach however even this model is an unnecessary approximation we would ideally like the variables to be fully connected in a grid structure but this would obviously contain loops in chapter we consider models of this sort and re visit stereo vision pictorial structures pictorial structures are models for object classes that consist of a number of in dividual parts that are connected together by spring like connections a typical example would be a face model figure which might consist of a nose eyes and mouth the spring like connections encourage the relative positions of these features to take sensible values for example the mouth is strongly encouraged to be below the nose pictorial structures have a long history in computer vision but were revived in a modern form by felzenszwalb huttenlocher who identified that if the connections between parts take the form of an acyclic graph a tree then they can be fit to images in polynomial time the goal of matching a pictorial structure to an image is to identify the positions wn n of the n parts based on data xn associated with each for example a simple system might assign a likelihood p r x wn k that is a normal distribution figure pictorial structure for human body a original image b after background subtraction c f four samples from the posterior distribution over part positions each part position is represented by a rectangle of fixed aspect ratio and characterized by its position size and angle adapted from felzenszwalb huttenlocher qc springer over a patch of the image at position k the relative positions of the parts are encoded using distributions of the form p r wn wpa n map inference in this system can be achieved using a dynamic programming technique figure shows a pictorial structure for a face this model is something of a compromise in that it would be preferable if the features had more dense connections for example the left eye provides information about the position of the right eye as well as the nose nonetheless this type of model can reliably find features on frontal faces a second application is for fitting articulated models such as human bodies figure these naturally have the form of a tree and so the structure is determined by the problem itself felzenszwalb huttenlocher developed a system of this sort in which each state wn represented a joint in the model and could take k possible values each of which represented a different position and shape of an associated rectangle the image was pre classified into foreground and background using a back ground subtraction technique the likelihood p r xn w k for a particular part position was evaluated using this binary image in particular the likelihood was chosen so that it increased if the area within the rectangle was considered fore ground and the area surrounding it was considered background unfortunately map inference in this model is somewhat unreliable a common failure mode is for more than one part of the body to become associated with the same part of the binary image this is technically possible as the limbs may occlude each other but it can also happen erroneously if one limb dominates and models for chains and trees figure segmentation using snakes a two points are fixed but the remaining points can take any position within their respective boxes the posterior distribution favors positions that are on image contours due to the likelihood term and positions that are close to other points due to the pairwise connections b results of inference c two other points are considered fixed d result of inference in this way a closed contour in the image is identified adapted from felzenszwalb zabih c ieee supports the rectangle model significantly more than the others felzenszwalb huttenlocher dealt with this problem by drawing samples from the posterior distribution p r n n over the positions of the parts of the model and using a more complex criterion to choose the most promising sample segmentation problem in section we considered segmentation as the problem of labeling pixels ac cording to the object to which they belong a different approach to segmentation is to infer the position of a closed contour that delineates two objects in inference the goal is usually to infer the positions of a set of points wn on the boundary of this contour based on the image data x as we update these points during an attempt to find the map solution the contour moves across the image and for this reason this type of model is referred to as an active contour or snake model figure shows an example of fitting this type of model at each iteration the positions wn of all of the points except two are updated and can take any position within small region surrounding their previous position the likelihood of taking a particular value wn k is high at positions in the image where the intensity changes rapidly i e the edges and low in constant regions in addition neighboring points are connected and have an attractive force they are more likely to be close to one another as usual inference can be carried out using the dynamic programming method during inference the points tend to become closer together due to their mutual attraction but get stuck on the edge of an object in the full system this process is repeated but with a different pair of adjacent applications points chosen to be fixed at each step hence the dynamic programming is a component step of a larger inference problem as the inference procedure continues the contour moves across the image and eventually fixes onto the boundary of an object for this reason these models are known as snakes or active contour models they are considered in more detail in chapter discussion in this chapter we have considered models based on acyclic graphs chains and trees in the chapter we will consider grid based models which contain many loops we will see that map inference is only tractable in a few special cases in contrast to this chapter we will also see a large difference between directed and undirected models models for chains and trees notes dynamic programming dynamic programming is used in many vision algorithms including those where there is not necessarily a clear probabilistic interpretation it is an attractive approach when it is applicable because of its speed and some efforts have been made to improve this further raphael interesting examples include image retargeting avidan shamir contour completion sha ashua ullman fitting of deformable templates amit kong coughlan et al shape matching basri et al the computation of superpixels moore et al and semantic labeling of scenes with tiered structure felzenszwalb veksler as well as the applications described in this chapter felzenszwalb zabih provide a recent review of dynamic programming and other graph algorithms in computer vision stereo vision dynamic programming was variously applied to stereo vision by baker binford ohta kanade who use a model based on edges and geiger et al who used a model based on intensities birchfield tomasi improved the speed by removing unlikely search nodes from the dynamic programming solution and introduced a mechanism that made depth discontinuities more likely where there was intensity variation torr criminisi developed a system that integrated dynamic programming with known constraints such as matched keypoints gong yang developed a dynamic programming algorithm that ran on a graphics processing unit gpu kim et al introduced a method for identifying disparity candidates at each pixel using spatial filters and a two pass method that performed optimization both along and across the scanlines veksler used dynamic programming in a tree to solve for the whole image at once and this idea has subsequently been used in a method based on line segments deng lin a recent quantitative comparison of dynamic programming algorithms in computer vision can be found in salmen et al alternative approaches to stereo vision which are not based on dynamic programming are considered in chapter pictorial structures pictorial structures were originally introduced by fischler er schlager but recent interest was stimulated by the work of felzenszwalb hut tenlocher who introduced efficient methods of inference based on dynamic pro gramming there have been a number of attempts to improve the appearance likeli hood term of the model kumar et al eichner ferrari andriluka et al felzenszwalb et al models that do not conform to a tree structure have also been introduced kumar et al sigal black ren et al jiang martin and here alternative methods such as loopy propagation must be used for inference these more general structures are particularly important for addressing problems associated with occlusions in human body models other authors have based their model on a mixture of trees everingham et al felzenszwalb et al in terms of applications ramanan et al have developed a notable system for tracking humans in video sequences based on pictorial structures everingham et al have developed a widely used system for locating facial features and felzenszwalb et al have presented a system that is used for detecting more general objects hidden markov models hidden markov models are essentially chain based models that are applied to quantities evolving in time good tutorials on the subject including details of how to learn them in the unsupervised case can be found in rabiner and ghahramani their most common application in vision is for gesture recognition starner et al rigoll et al and see moni ali for a recent review but they have also been used in other contexts such as modeling interactions of pedestrians oliver et al some recent work e g bor wang et al uses a related dis criminative model for tracking objects in time known as a conditional random field see chapter snakes the idea of a contour evolving over the surface of an image is due to kass et al both amini et al and geiger et al describe dynamic programming approaches to this problem these models are considered further in chapter belief propagation the sum product algorithm kschischang et al is a devel opment of earlier work on belief propagation by pearl the factor graph repre sentation is due to frey et al the use of belief propagation for finding marginal posteriors and map solutions in graphs with loops has been investigated by murphy et al and weiss freeman respectively notable applications of loopy belief propagation in vision include stereo vision sun et al and super resolving images freeman et al more information about belief propagation can be found in ma chine learning textbooks such as bishop barber and koller friedman problems problem compute by hand the lowest possible cost for traversing the graph in figure using the dynamic programming method figure dynamic programming example for problem problem map inference in chain models can also be performed by running djikstra algorithm on the graph in figure starting from the node on the left hand side and terminating when we first reach the node on the right hand side if there are n variables each of which takes k values what is the best and worst case complexity of the algorithm describe a situation where djikstra algorithm outperforms dynamic programming problem consider the graphical model in figure 24a write out the cost function for map estimation in the form of equation discuss the difference between your answer and equation problem compute the solution minimum cost path to the dynamic programming problem on the tree in figure which corresponds to the graphical model from figure figure graph construction for problem this is the same as the dynamic programming graph figure except that i there are two extra nodes at the start and the end of the graph ii there are no vertex costs iii the costs associated with the left most edges are k and the costs associated with the right most edges are the general edge cost for passing from label a and node n to label b at node n is given by pn n a b un b a b figure a graphical model for problem b graphical model for problem the unknown variables in this model receive connections from the two preceding variables and so the graph contains loops problem map inference for the chain model can be expressed as wˆn argmax wn max max w2 max wn n n log p r xn wn n log p r wn wn show that it is possible to compute this expression piecewise by moving the maximiza tion terms through the summation sequence in a manner similar to that described in section problem develop an algorithm that can compute the marginal distribution for an arbitrary variable wn in a chain model notes a b figure dynamic programming example for problem problem develop an algorithm that computes the joint marginal distribution of any two variables wm and wn in a chain model problem consider the following two distributions over three variables x2 and x3 p r x x x φ x x φ x x φ x x 23 31 p r x x x φ x x x 123 draw i an undirected model and ii a factor graph for each distribution what do you conclude problem convert each of the graphical models in figure into the form of a factor graph which of the resulting factor graphs take the form of a chain a b c d figure graphical models for problem problem figure 24b shows a chain model in which each unknown variable w depends on its two predecessors describe a dynamic programming approach to finding models for chains and trees the map solution hint you need to combine variables if there are n variables in the chain and each takes k values what is the overall complexity of your algorithm problem in the stereo vision problem the solution was very poor when the pixels are treated independently figure 18a suggest some improvements to this method while keeping the pixels independent problem consider a variant on the segmentation application figure in which we update all of the contour positions at once the graphical model for this problem is a loop i e a chain where there is also a edge between wn and devise an approach to finding the exact map solution in this model if there are n variables each of which can take k values what is the complexity of your algorithm chapter models for grids in chapter we discussed models that were structured as chains or trees in this chapter we consider models that associate a label with each pixel of an image since the unknown quantities are defined on the pixel lattice models defined on a grid structure are appropriate in particular we will consider graphical models in which each label has a direct probabilistic connection to each of its four neighbours critically this means that there are loops in the underlying graphical model and so the dynamic programming and belief propagation approaches of the previous chapter are no longer applicable these grid models are predicated on the idea that the pixel provides only very ambiguous information about the associated label however certain spatial config urations of labels are known to be more common than others and we aim to exploit this knowledge to resolve the ambiguity in this chapter we describe the relative preference for different configurations of labels with a pairwise markov random field or mrf as we shall see maximum a posteriori inference for pairwise mrfs is tractable in some circumstances using a family of approaches known collectively as graph cuts to motivate the grid models we introduce a representative application in image denoising we observe a corrupted image in which the intensities at a certain proportion of pixels have been randomly changed to another value according to a uniform distribution figure our goal is to recover the original clean image we note two important aspects of the problem most of the pixels are uncorrupted so the data usually tell us which intensity value to pick the uncorrupted image is mainly smooth with few changes between intensity levels consequently our strategy will be to construct a generative model where the map solution is an image that is mostly the same as the noisy version but is smoother as part of this solution we need to define a probability distribution over images that favors smoothness in this chapter we will use a discrete formulation of a markov random field to fulfil this role models for grids a b c d figure image denoising a original binary image b observed image created by randomly flipping the polarity of a fixed proportion of pixels our goal is to recover the original image from the corrupted one c original grayscale image d observed corrupted image is created by setting a certain proportion of the pixels to values drawn from a uniform distribution once more we aim to recover the original image markov random fields a markov random field is formally determined by a set of sites s n these will correspond to the n pixel locations a set of random variables wn n associated with each of the sites a set of neighbors nn n at each of the n sites to be a markov random field the model must obey the markov property p r wn ws n p r wn wnn in other words the model should be conditionally independent of all of the other variables given its neighbors this property should sound familiar this is exactly how conditional independence works in an undirected graphical model consequently we can consider a markov random field mrf as an undirected model section that describes the joint probability of the variables as a product of potential functions so that p r w tt φ w where φj is the jth potential function and always returns a non negative value this value depends on the state of the subset of variables j n in this context this subset is known as a clique the term z is called the partition function and is a normalizing constant that ensures that the result is a valid probability distribution alternatively we can rewrite the model as a gibbs distribution figure graphical model for worked mrf example the variables form a grid this is an undirected model where each link represents a potential function defined over the two variables that it connects each potential returns a positive number that indicates the tendency of the two variables to take these particular values where ψ log φ is known as a cost function and returns either positive or negative values grid example in a markov random field each potential function φ or cost function ψ addresses only a small subset of the variables in this chapter we will mainly be concerned with pairwise markov random fields in which the cliques subsets consist of only neighboring pairs in a regular grid structure to see how the pairwise mrf can be used to encourage smoothness in an image consider the graphical model for a image figure here we have defined the probability p r over the associated discrete states as a normalized product of pairwise terms p r w z w2 w2 w3 w3 where φmn wm wn is a potential function that takes the two states wm and wn and returns a positive number let us consider the situation where the world state wn at each pixel is binary and so takes a value of or the function φmn will now return four possible values depending on which of the four configurations of wm and wn is present for simplicity we will assume that the functions φ23 and are identical and that for each φmn φmn φmn φmn since there are only four binary states we can calculate the constant z explicitly by computing the un normalized probabilities for each of the possible combinations and taking the sum the resulting probabilities for each of the possible states are figure samples from markov random field prior four samples from the mrf prior which were generated using a gibbs sampling procedure see section each sample is a bi nary image that is smooth almost ev erywhere there are only very occa sional changes from black to white and vice versa this prior encourages smooth solutions like the original im age in the denoising problems and discourages isolated changes in label as are present in the noise p r p r p r p r 1000 0101 1110 00471 00471 00471 00471 1111 problem the potential functions in equation encourage smoothness the functions φmn return higher values when the neighbors take the same state and lower values when they differ and this is reflected in the resulting probabilities we can visualize this by scaling this model up to a larger image sized grid where there is one node per pixel and drawing samples from the resulting probability distribution figure the resulting binary images are mostly smooth with only occasional changes between the two values it should be noted that for this more realistically sized model we cannot com pute the normalizing constant z by brute force as for the case for example with 000 pixels each taking a binary value the normalizing constant is the sum of 000 terms in general we will have to cope with only knowing the probabilities up to an unknown scaling factor image denoising with discrete pairwise mrfs now we will apply the pairwise markov random field model to the denoising task our goal is to recover the original image pixel values from the observed noisy image more precisely the observed image x x2 xn is assumed to con sist of discrete variables where the different possible values labels represent dif ferent intensities our goal is to recover the original uncorrupted image w w2 wn which also consists of discrete variables representing the inten sity we will initially restrict our discussion to generative models and compute the posterior probability over the unknown world state w using bayes rule p r n n n n p r xn wn p r n p r n figure denoising model the ob served data xn at pixel n is condi tionally dependent on the associated world state wn red directed edges each world state wn has undirected edges to its four connected neighbors blue undirected edges this is hence a mixed model it contains both di rected and undirected elements to gether the world states are connected in a markov random field with cliques that consist of neighboring pairs of variables for example variable contributes to cliques c85 where we have assumed that the conditional probability p r n n factorizes into a product of individual terms associated with each pixel we will first consider denoising binary images in which the noise process flips the pixel polarity with probability ρ so that p r xn wn bernxn ρ p r xn wn bernxn ρ we subsequently consider gray level denoising where the observed pixel is replaced with probability ρ by a draw from a uniform distribution we now define a prior that encourages the labels wn to be smooth we want them to mostly agree with the observed image but to discourage configurations with isolated changes in label to this end we model the prior as a pairwise mrf each pair of four connected neighboring pixels contributes one clique so that p r w n exp z m n c ψ wm wn θ where we have assumed that the clique costs ψ are the same for every wm wn the parameters θ define the costs ψ for each combination of neighboring pairwise values ψ wm j wn k θ θjk so when the first variable wm in the clique takes label j and the second variable wn takes label k we pay a price of θjk as before we will choose these values so that there is a small cost when neighboring labels are the same so and are small and a larger one when the neighboring labels differ so and are large this has the effect of encouraging solutions that are mostly smooth the associated graphical model is illustrated in figure it is a mixed model containing both directed and undirected links the likelihood terms equation contribute the red directed links between the observed data and the denoised image at each pixel and the mrf prior equation contributes the blue grid that connects the pixels together map inference for binary pairwise mrfs to denoise the image we estimate the variables wn n using map inference we aim to find the set of world states wn n that maximizes the posterior probability p r n n so that n argmax p r n n n argmax n n n r n p r xn wn p r n where we have applied bayes rule and transformed to the log domain because the prior is an mrf with pairwise connections we can express this as n n argmax log p r xn wn ψ wm wn θ n n n m n c n argmin un wn pmn wm wn where un wn denotes the unary term at pixel n this is a cost for observing the data at pixel n given state wn and is the negative log likelihood term similarly pmn wm wn denotes the pairwise term this is a cost for placing labels wm and wn at neighboring locations m and n and is due to the clique costs ψ wm wn θ from the mrf prior note that we have omitted the term log z from the mrf definition as it is constant with respect to the states wn n and hence does not affect the optimal solution the cost function in equation can be optimized using a set of techniques known collectively as graph cuts we will consider three cases binary mrfs i e wi where the costs for different combinations of adjacent labels are submodular we will explain what this means later in the chapter exact map inference is tractable here figure max flow problem we are given a network of vertices connected by directed edges each of which has a non negative capacity cmn there are two special vertices and t termed the source and sink respectively in the max flow problem we seek to push as much flow from source to sink while respecting the capacities of the edges multi label mrfs i e wi k where the costs are submodular once more exact map inference is possible multi label mrfs where the costs are more general exact map inference is intractable but good approximate solutions can be found in some cases to solve these map inference tasks we will translate them into the form of maximum flow or max flow problems max flow problems are well studied and exact polynomial time algorithms exist in the following section we describe the max flow problem and its solution in subsequent parts of the chapter we describe how to translate map inference in markov random fields into a max flow problem max flow min cut consider a graph g with vertices and directed edges connecting them figure each edge has a non negative capacity so that the edge between vertices m and n has capacity cmn two of the vertices are treated as special and are termed the source and the sink consider transferring some quantity flow through the network from the source to the sink the goal of the max flow algorithm is to compute the maximum amount of flow that can be transferred across the network without exceeding any of the edge capacities when the maximum possible flow is being transferred the so called max flow solution every path from source to sink must include a saturated edge one where the capacity is reached if not then we could push more flow down this path and so by definition this is not the maximum flow solution it follows that an alternate way to think about the problem is to consider the edges that saturate we define a cut on the graph to be a minimal set of edges that separate the source from the sink in other words when these edges are removed there is no path from the source to the sink more precisely a cut partitions the vertices into two groups vertices that can be reached by some path from the source but cannot reach the sink and vertices that cannot be reached from the source but can reach the sink via some path for short we will refer to a cut as separating the source from the sink every cut is given an associated cost which is the sum of the capacities of the excised edges since the saturated edges in the max flow solution separate the source from the sink they form a cut in fact this particular choice of cut has the minimum possible cost and is referred to as the min cut solution hence the maximum flow and minimum cut problems can be considered interchangeably augmenting paths algorithm for maximum flow there are many algorithms to compute the maximum flow and to describe them properly is beyond the scope of this volume however for completeness we present a sketch of the augmenting paths algorithm figure consider choosing any path from the source to the sink and pushing the max imum possible amount of flow along it this flow will be limited by the edge on that path that has the smallest capacity which will duly saturate we remove this amount of flow from the capacities of all of the edges along the path causing the saturated edge to have a new capacity of zero we repeat this procedure finding a second path from source to sink pushing as much flow as possible along it and updating the capacities we continue this process until there is no path from source to sink without at least one saturated edge the total flow that we have transferred is the maximum flow and the saturated edges form the minimum cut in the full algorithm there are some extra complications for example if there is already some flow along edge i j it may be that there is a remaining path from source to sink that includes the edge j i in this situation we reduce the flow in i j before adding flow to j i the reader should consult a specialized text on graph based algorithms for more details if we choose the path with the greatest remaining capacity at each step the algorithm is guaranteed to converge and has complexity o where is the number of edges and the number of vertices in the graph from now on we will assume that the max flow min cut problem can be solved and concentrate on how to convert map estimation problems with mrfs into this form map inference binary variables algorithm recall that to find the map solution we must find n argmin un wn pmn wm wn n where un wn denotes the unary term and pmn wm wn denotes the pairwise term for pedagogical reasons we will first consider cases where the unary terms are positive and the pairwise terms have the following zero diagonal form pmn pmn pmn pmn where we discuss the more general case later in this section t t source sink source sink t 9 9 5 9 t source sink source sink 5 5 5 5 f 5 9 9 5 9 t 5 t source sink source sink 5 figure augmenting paths algorithm for max flow the numbers at tached to the edges correspond to current flow capacity a we choose any path from source to sink with spare capacity and push as much flow as possible along this path the edge with the smallest capacity here edge t saturates b we then choose another path where there is still spare ca pacity and push as much flow as possible now edge 5 saturates c e we repeat this until there is no path from source to sink that does not contain a saturated edge the total flow pushed is the maximum flow f in the min cut problem we seek a set of edges that separate the source from the sink and have minimal total capacity the min cut dashed line consists of the saturated edges in the max flow problem in this example the paths were chosen arbitrarily but to ensure that this algorithm converges in the general case we should choose the remaining path with the greatest capacity at each step figure graph structure for find ing map solution for a mrf with bi nary labels and pairwise connections in a image there is one vertex per pixel and neighbors in the pixel grid are connected by reciprocal pairs of directed edges each pixel vertex receives a connection from the source and sends a connection to the sink to separate source from sink the cut must include one of these two edges for each vertex the choice of which edge is cut will determine which of two labels is assigned to the pixel 7 5 source 9 sink t figure graph construction for binary mrf with diagonal pairwise terms using simple example after the cut vertices attached to the source are given label and vertices attached to the sink are given label we hence attach the appropriate unary costs to the links between the sink source and the pixel vertices the pairwise costs are attached to the horizontal links be tween pixels as shown this arrange ment ensures that the correct cost is paid for each of the eight possible so lutions see figure 9 the key idea will be to set up a directed graph and attach weights to the edges so that the minimum cut on this graph corresponds to the maximum a posteriori solution in particular we construct a graph with one vertex per pixel and a pair of directed edges between adjacent vertices in the pixel grid in addition there is a directed edge from the source to every vertex and a directed edge from every vertex to the sink figure 7 now consider a cut on the graph in any cut we must either remove the edge that connects the source to a pixel vertex or the edge that connects the pixel vertex to the sink or both if we do not do this then there will still be a path from source to sink and it is not a valid cut for the minimum cut we will never cut both assuming the general case where the two edges have different capacities this is unnecessary and will inevitably incur a greater cost than cutting one or the other we will label pixels where the edge to the source was cut as wn and pixels where the edge to the sink was cut as having label wn so each plausible minimum cut is associated with a pixel labeling our goal is now to assign capacities to the edges so the cost of each cut matches the cost of the associated labeling as prescribed in equation for simplicity we illustrate this with a image with three pixels figure but we stress that all the ideas are also valid for images and higher dimensional constructions map inference for binary pairwise mrfs a b c d solution cost e f solution cost g solution cost solution cost h solution cost solution cost solution cost solution cost figure 9 eight possible solutions for three pixel example when we set the costs as in figure each solution has the appropriate cost a for example the solution a b c requires us to cut edges a b c and pay the cost ua ub uc b for the solution a b c we must cut edges a b c t and c b to prevent flow through the path c b t this incurs a total cost of ua ub uc pbc c similarly in this example with a b c we pay the appropriate cost ua ub uc pab pbc d h the other five possible configurations we attach the unary costs un and un to the edges from the pixel to the source and sink respectively if we cut the edge from the source to a given pixel and hence assign wn we pay the cost un conversely if we cut the edge from the pixel to the sink and hence assign wn we pay the cost un we attach the pairwise costs pmn and pmn to the pairs of edges be tween adjacent pixels now if one pixel is attached to the source and the other to the sink we pay either pmn or pmn as appropriate to sepa problem figure graph structure for gen eral i e non diagonal pairwise costs consider the solution a b we must break the edges a and b giving a total cost of ua ub pab for the solution a b we must break the edges a t a b and b giving a total cost of ua ub pab simi larly the cuts corresponding to the so lutions a b and a b on this graph have pairwise costs pab and pab respectively rate source from sink the cuts corresponding to all eight possible configurations of the three pixel model and their costs are illustrated in figure 9 any cut on the graph in which each pixel is either separated from the source or the sink now has the appropriate cost from equation it follows that the minimum cut on this graph will have the minimum cost and the associated labeling n will correspond to the maximum a posteriori solution general pairwise costs now let us consider how to use the more general pairwise costs pmn pmn pmn pmn problem algorithm to illustrate this we use an even simpler graph with only two pixels figure notice that we have added the pairwise cost pab to the edge b we will have to pay this cost appropriately in the configuration where wa and wb unfortunately we would also pay it in the case where wa and wb hence we subtract the same cost from the edge a b which must also be cut in this solution by a similar logic we add pab to the edge a t and subtract it from edge a b in this way we associate the correct costs with each labeling reparameterization the preceding discussion assumed that the edge costs are all non negative and can be valid capacities in the max flow problem if they are not then it is not possible to compute the map solution unfortunately it is often the case that they are negative even if the original unary and pairwise terms were positive the edge a b in figure with cost pab pab pab could be negative the solution to this problem is reparameterization the goal of reparameterization is to modify the costs associated with the edges in the graph in such a way that the map solution is not changed in particular we will adjust the edge capacities so that every possible solution has a constant cost added to it this does not change which solution has the minimum cost and so the map labeling will be unchanged we consider two reparameterizations figure first consider adding a constant cost α to the edge from a given pixel to the source and the edge from the same pixel to the sink since any solution cuts exactly one of these edges the overall cost of every solution increases by α we can use this to ensure that none of the edges connecting the pixels to the source and sink have negative costs we simply add a sufficiently large positive value α to make them all non negative a more subtle type of reparameterization is illustrated in figure by changing the costs in this way we increase the total cost of each possible solution by β for example in the assignment wa wb we must cut the links a b a and b t giving a total cost of ua ub pab β applying the reparameterization in figure to the general construction in figure we must ensure that the capacities on edges between pixel nodes are non negative so that β β adding these equations together we can eliminate β to get a single inequality if this condition holds the problem is termed submodular and the graph can be reparameterized to have only non negative costs it can then be solved in polynomial time using the max flow algorithm if the condition does not hold then this approach cannot be used and in general the problem is np hard fortunately the former case is common for vision problems we generally favor smooth solutions where neighboring labels are the same and hence the costs for labels differing are naturally greater than the costs θ11 for the labels agreeing figure shows the map solutions to the binary denoising problem with an mrf prior as we increase the strength of the cost for having adjacent labels that differ here we have assumed that the costs for adjacent labels being different are the same θ10 and that there is no cost when neighboring labels are the same θ11 we are in the zero diagonal regimen when the mrf costs are small the solution is dominated by the unary terms and the map solution looks like the noisy image as the costs increase the solution ceases to tolerate isolated regions and most of the noise is removed when the costs become larger details such as the center of the in are lost and eventually nearby regions are connected together with very high pairwise costs the map solution is a uniform field of labels the overall cost is dominated by the pairwise terms from the mrf and the unary terms merely determine the polarity problem a source b source c source a b a b a b sink t sink t sink t figure reparameterization a original graph construction b repa rameterization adding a constant cost α to the connections from a pixel vertex to both the source and sink results in a problem with the same map solution since we must cut either but not both of these edges every so lution increases in cost by α and the minimum cost solution remains the same c reparameterization manipulating the edge capacities in this way results in a constant β being added to every solution and so the choice of minimum cost solution is unaffected a b c d e f g h figure denoising results a observed noisy image b h maximum a posteriori solution as we increase zero diagonal pairwise costs when the pairwise costs are low the unary terms dominate and the map solution is the same as the observed image as the pairwise costs increase the image gets more and more smooth until eventually it becomes uniform map inference for multi label pairwise mrfs b figure a graph setup for multi label case for two pixels a b and four labels there is a chain of five vertices associated with each pixel the four vertical edges between these vertices are assigned the unary costs for the four labels the minimum cut must break this chain to separate source from sink and the label is assigned according to where the chain is broken vertical constraint edges of infinite capacity run between the four vertices in the opposite direction there are also diagonal edges between the ith vertex of pixel a and the jth vertex of pixel b with assigned costs cab i j see text b the vertical constraint edges prevent solutions like this example with three pixels here the chain of vertices associated with the central pixel is cut in more than one place and so the labeling has no clear interpretation however for this to happen a constraint link must be cut and hence this solution has an infinite cost map inference for multi label pairwise mrfs we now investigate map inference using mrf priors with pairwise connections when the world state wn at each pixel can take multiple labels k to solve the multi label problem we change the graph construction figure with k labels and n pixels we introduce k n vertices into the graph for each pixel the k associated vertices are stacked the top and bottom of the stack are connected to the source and sink by edges with infinite capacity between the k vertices in the stack are k edges forming a path from source to sink these edges are associated with the k unary costs un un k to separate the source from the sink we must cut at least one of the k edges in this chain we will interpret a cut at the kth edge in this chain as indicating that the pixel takes label k and this incurs the appropriate cost of un k algorithm models for grids cost cost figure example cuts for multi label case to separate the source and sink we must cut all of the links that pass from above the chosen label for pixel a to below the chosen label for pixel b a pixel a is set to label and pixel b is set to label meaning we must cut the links from vertex to nodes and b pixel a takes label and pixel b takes label problem 5 problem problem to ensure that only a single edge from the chain is part of the minimum cut and hence that each cut corresponds to one valid labeling we add constraint edges these are edges of infinite capacity that are strategically placed to prevent certain cuts occurring in this case the constraint edges connect the vertices backwards along each chain any cut that crosses the chain more than once must cut one of these edges and will never be the minimum cut solution figure in figure there are also diagonal inter pixel edges from the vertices associated with pixel a to those associated with pixel b these are assigned costs cab i j where i indexes the vertex associated with pixel a and j indexes the vertex associated with pixel b we choose the edge costs to be cab i j pab i j pab i j pab i j pab i j map inference for multi label pairwise mrfs figure reparameterization for multi label graph cuts the original construction a is equivalent to construction b the label at pixel b deter mines which edges that leave node are cut hence we can remove these edges and add the extra costs to the vertical links associated with pixel b similarly the costs of the edges passing into node can be added to the vertical edges associated with pixel a if any of the resulting vertical edges associated with a pixel are negative we can add a constant α to each since exactly one is broken the total cost increases by α but the map solution remains the same where we define any superfluous pairwise costs associated with the non existent labels or k to be zero so that pab i pab i k i k pab j pab k j j k when label i is assigned to pixel a and label j to pixel b we must cut all of the links from vertices ai to the vertices bj bk to separate the source from the sink figure so the total cost due to the inter pixel edges for assigning label i to pixel a and label j to pixel b is problem 7 k k cab i j pab i j pab i j pab i j pab i j i j j i j j pab i j pab j pab i k pab k pab i j models for grids figure submodularity con straint for multi label case color at position m n indicates pairwise costs pab m n for all edges in the graph to be positive we require that the pairwise terms obey pab β γ pab α δ pab β δ pab α γ for all α β γ δ such that β α and δ γ in other words for any four positions arranged in a square configuration as in the figure the sum of the two costs on the diagonal from top left to bottom right must be less than the sum on the off diagonal if this condition holds the problem can be solved in polynomial time label 5 7 9 5 7 9 adding the unary terms the total cost is ua i ub j pab i j as required once more we have implicitly made the assumption that the costs associated with edges are non negative if the vertical intra pixel edge terms have negative costs it is possible to reparameterize the graph by adding a constant α to all of the unary terms since the final cost includes exactly one unary term per pixel every possible solution increases by α and the map solution is unaffected the diagonal inter pixel edges are more problematic it is possible to remove the edges that leave node and the edges that arrive at bk by adding terms to the intra pixel edges associated with the unary terms figure these intra pixel edges can then be reparameterized as described above if necessary unfortunately we can neither remove nor reparameterize the remaining inter pixel edges so we require that cab i j pab i j pab i j pab i j pab i j by mathematical induction we get the more general result figure pab β γ pab α δ pab β δ pab α γ where α β γ δ are any four values of the state y such that β α and δ γ this is the multi label generalization of the submodularity condition equation an important class of pairwise costs that are submodular are those that are convex in the absolute difference wi wj between the labels at adjacent pixels figure here smoothness is encouraged the penalty becomes increasingly stringent as the jumps between labels increase 4 multi label mrfs with non convex potentials unfortunately convex potentials are not always appropriate for example in the denoising task we might expect the image to be piecewise smooth there are smooth 4 multi label mrfs with non convex potentials a b c figure 17 convex vs non convex potentials the method for map in ference for multi valued variables depends on whether the costs are a convex or non convex function of the difference in labels a quadratic function convex pmn wm wn κ wm wn for convex functions it is possible to draw a chord between any two points on the function without intersecting the function elsewhere e g dotted blue line b truncated quadratic func tion non convex pmn wm wn min wm wn c potts model non convex pmn wm wn κ δ wm wn figure denoising results with convex quadratic pairwise costs a noisy observed image b denoised image has artiwfacts where there are large intensity changes in the original image convex costs imply that there is a lower cost for a number of small changes rather than a single large one regions corresponding to objects followed by abrupt jumps corresponding to the boundaries between objects a convex potential function cannot describe this situation because it penalizes large jumps much more than smaller ones the result is that the map solution smooths over the sharp edges changing the label by several smaller amounts rather than one large jump figure to solve this problem we need to work with interactions that are non convex in the absolute label difference such as the truncated quadratic function or the problem figure the alpha expansion al gorithm breaks the problem down into a series of binary sub problems at each step we choose a label α and we expand for each pixel we either leave the label as it is or replace it with α this sub problem is solved in such a way that it is guaranteed to decrease the multilabel cost function a initial labeling b orange label is expanded each label stays the same or becomes orange c yellow label is expanded d red label is expanded a b c d potts model figures c these favor small changes in the label and penalize large changes equally or nearly equally this reflects the fact that the exact size of an abrupt jump in label is relatively unimportant unfortunately these pairwise costs do not satisfy the submodularity constraint equation here the map solution cannot in general be found exactly with the method described previously and the problem is np hard fortunately there are good approximate methods for optimizing such problems one of which is the alpha expansion algorithm 4 inference alpha expansion algorithm 4 the alpha expansion algorithm works by breaking the solution down into a series of binary problems each of which can be solved exactly at each iteration we choose one label value α and for each pixel we consider either retaining the current label or switching it to α the name alpha expansion derives from the fact that the space occupied by label α in the solution expands at each iteration figure the process is iterated until no choice of α causes any change each expansion move is guaranteed to lower the overall objective function although the final result is not guaranteed to be the global minimum for the alpha expansion algorithm to work we require that the edge costs form a metric in other words we require that p α β α β p α β p β α p α β p α γ p γ β these assumptions are reasonable for many applications in vision and allow us to model non convex priors in the alpha expansion graph construction figure there is one vertex associated with each pixel each of these vertices is connected to the source rep resenting keeping the original label or α and the sink representing the label α to separate source from sink we must cut one of these two edges at each pixel the choice of edge will determine whether we keep the original label or set it to figure alpha expansion graph setup each pixel node a b c d e is connected to the source and the sink by edges with costs u α and u α respectively in the minimum cut exactly one of these links will be cut the nodes and vertices describing the relationship between neighboring pixels depend on their current labels which may be α α as for pixels a and b α β as for pixels b and c β β as for pixels c and d or β γ as for pixels d and e for the last case an auxiliary node k must be added to the graph α accordingly we associate the unary costs for each edge being set to α or its original label with the two links from each pixel if the pixel already has label α then we set the cost of being set to α to the remaining structure of the graph is dynamic it changes at each iteration depending on the choice of α and the current labels there are four possible relationships between adjacent pixels pixel i has label α and the pixel j has label α here the final configuration is inevitably α α and so the pairwise cost is zero and there is no need to add further edges connecting nodes i and j in the graph pixels a and b in figure have this relationship the first pixel has label α but the second pixel has a different label β here the final solution may be α α with zero pairwise cost or α β with pairwise cost pij α β here we add a single edge connecting pixel j to pixel i with pairwise cost pij α β pixels b and c in figure have this relationship both pixels i and j have the same label β here the final solution may be α α with zero pairwise cost β β with zero pairwise cost α β with pairwise cost pij α β or β α with pairwise cost pij β α we add two edges between the pixels representing the two non zero pairwise costs pixels c and d in figure have this relationship pixel i has label β and pixel j has a second label γ here the final solution may be α α with zero pairwise cost β γ with pairwise cost pij β γ β α problem 9 with pairwise cost pij β α or α γ with pairwise cost pij α γ we add a new vertex k between vertices i and j and add the three non zero pairwise costs to edges k α i k and j k respectively pixels d and e in figure have this relationship three example cuts are shown in figure note that this construction critically relies on the triangle inequality equa tion for example consider pixels d and e in figure if the triangle inequality does not hold so that pde β γ pde β α pde α γ then the wrong costs will be assigned rather than the link k α the two links d k and e k will both be cut and the wrong cost will be assigned in practice it is sometimes possible to ignore this constraint by truncating the offending cost pij β γ and running the algorithm as normal after the cut is done the true objective function sum of the unary and pairwise costs can be computed for the new label map and the answer accepted if the cost has decreased it should be emphasized that although each step optimally updates the objective function with respect to expanding α this algorithm is not guaranteed to converge to the overall global minimum however it can be proven that the result is within a factor of two of the minimum and often it behaves much better figure 22 shows an example of multi label denoising using the alpha expansion algorithm on each iteration one of the labels is chosen and expands and the ap propriate region is denoised sometimes the label is not supported at all by the unary costs and nothing happens the algorithm terminates when no choice of α causes any further change 5 conditional random fields in the models presented in this chapter the markov random fields have described the prior p r w in a generative model of the image data we could alternatively describe the joint probability distribution p r w x with the undirected model p r w x exp ψ z c c w ζ w x 23 where the functions ψ encourage certain configurations of the label field and the functions ζ encourage agreement between the data and the label field if we now condition on the data i e assume that it is fixed then we can use the relation p r w x p r w x to write p r w x exp r ψ w ζ w x where z p r x this discriminative model is known as a conditional random field or crf we can choose the functions ζ so that they each determine the compatibility of one label wn to its associated measurement xn if the functions ψ are used to a source a b cost sink b source a b cost sink c source a b c c c d d d states before k e states after states before k e states after states before k e cost sink states after figure alpha expansion algorithm a c example cuts on this graph illustrate that the appropriate unary and pairwise costs are always paid figure 22 alpha expansion algorithm for denoising task a observed noisy image b label black is expanded removing noise from the hair c f subsequent iterations in which the labels corresponding to the boots trousers skin and background are expanded respectively encourage smoothness between neighboring labels then the negative log posterior probability will again be the sum of unary and pairwise terms the map labels wˆ can hence be found by minimizing a cost function of the form n wˆ argmin un wn pmn wm wn and the graphical model will be as in figure 23 this cost function can be minimized using the graph cuts techniques described throughout this chapter figure 23 graphical model for con ditional random field compare to fig ure 4 the posterior probability of the labels w is a markov random field for fixed data x in this model the two sets of cliques relate i neigh bouring labels and ii each label to its associated measurement since this model only includes unary and pair wise interactions between the labels the unknown labels wn n can be higher order models optimized using graph cut techniques figure directed graphical model for grid although this model appears similar to the pairwise markov random field model it repre sents a different factorization of the joint probability in particular the factorization contains terms involving three variables such as p r w2 w4 this means that the resulting cost function for map inference is no longer amenable to exact solution using graph cut methods in this case an attractive alternative is to use sampling based methods as it is easy to generate samples from this directed model the models that we have discussed so far have only connected immediate neighbors however these only allow us to model relatively simple statistical properties of the label field one way to improve this situation is to consider each variable wn k as representing the index of a square patch of labels from a predefined library the pairwise mrf now encodes the affinity of neighboring patches for each other unfortunately the resulting costs are less likely to be submodular or even obey the triangle inequality and the number k of patches in the library is usually very large making graph cut algorithms inefficient a second approach to modeling more complex statistical properties of the label field is to increase the number of the connections for the undirected models crf mrf this would mean introducing larger cliques for example to model local texture we might connect all of the variables in every 5 5 region of the image unfortunately inference is hard in these models optimizing the resulting complex cost functions is still an open research topic models for grids 7 directed models for grids the markov random field and conditional random field models are attractive be cause we can use graph cuts approaches to search for the map solution however they have the drawback that it is very hard to learn the parameters of the model because they are based on undirected models an obvious alternative is to use a similar directed model figure 24 here learning is relatively easy but it turns out that map inference using graph cuts is not generally possible to see this consider the cost function for map inference in this model n argmax log p r n n log p r n n argmax n n n log p r xn wn n log p r wn wpa n argmin n n n log p r xn wn n log p r wn wpa n where we have multiplied the objective function by minus one and now seek the minimum this minimization problem now has the general form n argmin n n n un wn n tn wn n n where un wn is called a unary term reflecting the fact that it only depends on a single element wn of the label field and tn wn n n is called a three wise term reflecting the fact in general the label at a pixel is conditioned on the two parents n and n above and to the left of the current position notice that this cost function is fundamentally different from the cost function for map inference in a pairwise mrf equation it includes three wise terms and there is no known polynomial algorithm to optimize this criterion however since this model is a directed graphical model it is easy to generate samples from this model and this can be exploited for approximate inference methods such as computing the empirical max marginals 8 applications the models and algorithms in this chapter are used in a large number of computer vision applications including stereo vision motion estimation background sub traction interactive segmentation semantic segmentation image editing image denoising image super resolution and building models here we review a few key examples we consider background subtraction which is a simple application with binary labels and interactive segmentation which uses binary labels in a sys tem that simultaneously estimates the parameters in the likelihood terms then we 8 applications consider stereo motion estimation and image editing all of which are multi label graph cut problems we consider super resolution which is a multi label problem where the units are patches rather than pixels and which there are so many labels that the alpha expansion algorithm is not suitable finally we consider drawing samples from directed grid models to generate novel images 8 background subtraction first let us revisit the background subtraction algorithm that we first encountered in section 6 in background subtraction the goal is to associate a binary label wn n with each of the n pixels in the image indicating whether this pixel belongs to the foreground or background based on the observed rgb data xn n at each pixel when the pixel is background wn the data are assumed to be generated from a normal distribution with known mean and covariance when the pixel is foreground wn a uniform distribution over the data is assumed so that p r xn w normxn µn σn p r xn w κ where κ is a constant in the original description we assumed that the models at each pixel were independent and when we inferred the labels the results were noisy figure we now place a markov random field prior over the binary labels where the pairwise cliques are organized as a grid as in most of the models in this chapter and where the potential functions encourage smoothness figure 25 illustrates the results of performing inference in this model using the graph cuts algorithm there are now far fewer isolated foreground regions and fewer holes in the foreground object the model has still erroneously discovered the shadow a more sophisticated model would be required to deal with this problem 8 interactive segmentation grabcut the goal of interactive segmentation is to cut out the foreground object in a photo based on some input from the user figure more precisely we aim to as sociate a binary label wn n to each of the n pixels in the image indicating whether this pixel belongs to the foreground or background based on the observed rgb data xn n at each pixel however unlike background subtraction we do not have any prior knowledge of either the foreground or the background in the grabcut system of rother et al the likelihoods of observing the background w and foreground w are each modeled as a mixture of k gaussians so that models for grids figure 25 background subtraction revisited a original image b map solution of background subtraction model with independent pixels the solution contains noise c map solution of background subtraction model with markov random field prior this smoothed solution has eliminated most of the noise figure grab cut a the user draws a bounding box around the object of interest b the algorithm segments the foreground from the background by alternating between building color models and segmenting the image c d a second example e f failure mode this algorithm does not segment wiry objects well as the pairwise costs for tracing around all the boundaries are prohibitive adapted from rother et al qc acm p r xn w j k λjknormxn µjk σjk k and the prior over the labels is modeled as a pairwise connected markov random field with the potentials chosen to encourage smoothness in this application the image may have a wide variety of content and so there is no suitable training data from which to learn the parameters λjk µjk k j k of the foreground and background color models however we note that i if we knew the color models we could perform the segmentation via map inference with the graph cuts algorithm and ii if we knew the segmentation then we could compute the foreground and background color models based on the pixels assigned to each category this observation leads to an alternating approach to inference in this model in which the segmentation and parameters are computed in turn until the system converges in the grabcut algorithm the user draws a bounding box around the desired object to be segmented this effectively defines a rough segmentation pixels within the box are foreground and pixels outside are background from which the system is initialized if the segmentation is not correct after the alternating optimization algorithm converges the user may paint regions of the image with a foreground or background brush indicating that these must belong to the appropriate class in the final solution in practice this means that the unary costs are set to ensure that these take the appropriate values and the alternating solution is run again from this point until convergence example results are shown in figure to improve the performance of this algorithm it is possible to modify the mrf so that the pairwise cost for changing from foreground to background label is less where there is an edge in the image this is referred to as using geodesic distance from a pure probabilistic viewpoint this is somewhat dubious as the mrf prior should embody what we know about the task before seeing the data and hence cannot depend on the image however this is largely a philosophical objection and the method works well in practice for a wide variety of objects a notable failure mode is in segmenting wiry objects such as trees here the model is not prepared to pay the extensive pairwise costs to cut exactly around the many edges of the object and so the segmentation is poor 8 stereo vision in stereo vision the goal is to infer a discrete multivalued label wn n represent ing the disparity horizontal shift at each pixel in the image given the observed image data xn n more details about the likelihood terms in this problem can be found in section 8 where we described tree based priors for the unknown disparities a more suitable approach is to use an mrf prior as for the denoising example it is undesirable to use an mrf prior where the costs are a convex function of the difference in neighboring labels this results in a map solution where the edges of objects are smoothed hence it is usual to use a non convex prior such as the potts function which embodies the idea that the scene consists of smooth surfaces with sudden jumps in depth between them where the size of the jump is unimportant boykov et al used the alpha expansion algorithm to perform approx imate inference in a model of this sort figure 27 the performance of this algorithm is good but errors are found where there is no true match in the other image i e where the corresponding point is occluded by another object kol figure 27 stereo vision a one image of the original stereo pair b disparity estimated using the method of boykov et al c ground truth disparity blue pixels indicate regions which are occluded in the second image and so do not have a valid match or disparity the algorithm does not take account of this fact and produces noisy estimates in these regions adapted from boykov et al mogorov zabih subsequently developed a bespoke graph for dealing with occlusions in stereo vision and an alpha expansion algorithm for optimizing the associated cost function these methods can also be applied to optical flow in which we attempt to identify pixel correspondences between adjacent frames in a video sequence unlike in stereo vision there is no guarantee that the correspond ing matches will be on the same scanline but other than this the problem is very similar 8 4 rearranging images markov random field models can also be used for rearranging images we are given an original image i and wish to create a new image i by rearranging the pixels from i in some way depending on the application we may wish to change the dimensions of the original image termed image re targeting remove an object or move an object from one place to another pritch et al constructed a model with variables w w1 wn at each of the n pixels of i each possible value of wn k represents a relative offset to image i that tells us which pixel from image i will appear at the nth pixel of the new image the label map w is hence termed a shift map as it represents shifts to the original image each possible shift map defines a different output image i figure 28 pritch et al model the shift map w as an mrf with pairwise costs that encourage smoothness the result of this is that only shift maps that are piecewise constant have high probability in other words new images which consist of large chunks of the original image that have been copied verbatim are favored they modify the pairwise costs so that they are lower when adjacent labels encode offsets with similar surrounding regions this means that where the label does change it does so in such a way that there is no visible seam in the output image the remainder of the model depends on the application figure 29 8 applications a c d 21 100 figure 28 shift maps for image re targeting to reduce width a new image i is created from b the original image i by copying piecewise regions five regions shown c these regions are carefully chosen to produce a seamless result d the underlying representation is a shiftmap a label at each pixel of the new image that specifies the offset to the position in the original image that will be copied from an mrf encourages the labels to be piecewise constant and hence the result tends to consist of large chunks copied verbatim figure shows method of pritch et al to move an object we specify unary costs in the new region that ensure that we copy the desired object here the remainder of the shifts are left free to vary but favor small offsets so that parts of the scene that are far from the change tend to be unperturbed to replace an area of the image we specify unary costs so that the remainder of the image must have a shift of zero verbatim copying and the shift in the missing region must be such that it copies from outside the region to retarget an image to larger width we set the unary costs so that the left and right edges of the new image are forced to have shifts that correspond to the left and right of the original image we also use the unary costs to specify that vertical shifts must be small to retarget an image to a smaller width figure 28 we additionally specify that the horizontal offset can only increase as we move from left to right this ensures that the new image does not contain replicated objects and that their horizontal order remains constant in each case the best solution can be found using the alpha expansion algorithm since the pairwise terms do not form a metric here it is necessary to truncate the relevant costs see section 4 in practice there are many labels and so pritch et al introduce a coarse to fine scheme in which a low resolution version of the image is initially synthesized and the result of this is used to guide further refinements at higher resolutions models for grids figure 29 applications of shift maps shift maps can be used to a take an object from the original image b move it to a new position and c then fill in the remaining pixels to produce a new picture d they can also be used to remove an undesirable object e specified by a mask from an image by f filling in the missing area g h finally they can be used to retarget an original image to a smaller size or i j to re target an original image to a larger size results from method of pritch et al 8 5 super resolution image super resolution can also be framed as inference within a markov random field model here the basic unit of currency is an image patch rather than a pixel for example consider dividing the original image into a regular grid of n low resolution patches xn n the goal is to infer a set of corresponding labels wn n at each position in the grid each label can take one of k values each of which corresponds to a different possible high resolution 7 7 patch these patches are extracted from training images the pairwise cost for placing high resolution patches together is determined by the agreement at the abutting edge the unary cost for choosing a patch at a given position depends on the agreement between the proposed high resolution patch and the observed low resolution patch this can be computed by downsampling the high resolution patch to pixels and then using a normal noise model a b c figure 30 super resolution a the observed image which is broken down into a regular grid of low resolution patches b we infer a regular grid of labels each of which corresponds to a high resolution patch and quilt these together to form the super resolved image c ground truth adapted from freeman et al qc springer in principle we could perform inference in this model with a graph cut for mulation but there are two problems first the resulting cost function is not submodular second the number of possible high resolution patches must be very large and so the alpha expansion algorithm which chooses these in turn would be extremely inefficient freeman et al used loopy belief propagation to perform approximate inference in a model similar to this to make this relatively fast they used only a subset of j k possible patches at each position where these were chosen so that they were the j patches which agreed best with the observed data and so had the lowest unary costs although the results figure 30 are quite convincing they are sadly far from the imagined feats of tv crime drama 8 6 texture synthesis the applications so far have all been based on performing inference in the undi rected markov random field model we now consider the directed model inference is difficult in this model due to the presence of three wise terms in the associated cost function see section 7 however generation from this model is relatively easy since this is a directed model we can use an ancestral sampling technique to generate examples one possible application of this technique is for texture synthesis the goal of texture synthesis is to learn a generative model from a small patch of texture such that when we draw samples from the model they look like extended examples of the same texture figure 31 the particular technique that we describe here is known as image quilting and was originally described by efros freeman we will first describe the algorithm as it was initially conceived and then relate it to the directed model for grids figure 31 texture synthesis a b original texture samples c d synthe sized textures using image quilting adapted from efros freeman original sample a b patch library figure image quilting a original texture sample b library of all overlapping patches from the original texture sample c the first patch is chosen randomly from the library d the second patch is chosen randomly from the k library patches that are most similar in the overlapping region e in subsequent rows patches are chosen so that the overlapping region agrees with the previously placed patches to the left and above f this continues until we reach the bottom right of the image g the patches are then blended together to give the final results figure 33 image quilting as ancestral sampling from a graphical model when we synthesize images we are effectively ancestral sampling from a directed grid model where each hidden node represents a patch index and each observed variable represents the patch data the first step see figure 32 is to extract all possible patches of a given size from the input texture to form a patch library the synthesized image will consist of a regular grid of these library patches such that each overlaps its neighbors by a few pixels a new texture is synthesized starting in the top left of this grid and proceeding to the bottom right at each position a library patch is chosen such that it is visually consistent with the patches that have previously been placed above and to the left for the top left position we randomly choose a patch from the library we then consider placing a second patch to the right of the first patch such that they overlap by roughly 6 of their width we search through the library for the j patches where the squared rgb intensity difference in the overlapping region is smallest we choose one of these j patches randomly and place it into the image at the second position we continue in this way synthesizing the top row of patches in the image when we reach the second row we must consider the overlap with the patches to the left and above in deciding whether a candidate library patch is suitable we choose the j patches where the total rgb difference between the overlapping portions of the candidate patch and the previously chosen patches is minimal this process continues until we reach the bottom right of the image in this way we synthesize a new example of the texture figure 32a f by forcing the overlapping regions to be similar we enforce visual consistency between adjacent patches by choosing randomly from the j best patches we ensure that the result is stochastic if we always chose the most visually consistent patch we would replicate the original texture verbatim at the end of this process it is common to blend the resulting patches together to remove remaining artifacts in figure 34 synthesizing novel faces a a sample is drawn from a subspace model see chapter 7 that has been trained on facial images b texture synthesis now proceeds but with two differences from before first the choice of patch must now agree with the sample from the subspace model as well as the previously placed patches second the library patches are now different at each position in this way we ensure that a nose patch is always chosen in the center and so on c after completing the synthesis and blending together the patches d f three more examples of synthesized faces adapted from mohammed et al qc acm the overlapping region figure 32g image quilting can be thought of as ancestral sampling from the directed model for images figure 33 the observed data xn n are the output patches and the hidden labels wn n represent the patch index the labels are conditioned on their parents with a probability distribution that allots a constant probability if the overlapping region is one of the j closest and zero otherwise the only real change is that the relationship between label and observed data is now deterministic a given label always produces exactly the same output patch 8 7 synthesizing novel faces mohammed et al presented a related technique to synthesize more complex objects such as frontal faces figure 35 based on a large database of weakly aligned training examples faces have a distinct spatial structure and we must figure 35 graphical model for syn thesizing novel faces when we gener ate a new image we are ancestral sam pling from a directed image model where each label w is conditioned on the hidden variable h of the subspace model adapted from mohammed et al qc acm ensure that our model enforces these constraints to this end we build a separate library of patches for each position in the image this ensures that the features have roughly the correct spatial relations the nose always appears in the center and the chin at the bottom in principle we could now apply a standard image quilting approach by syn thesizing patches starting in the top left and moving to the bottom right unfor tunately the resulting faces can drift in appearance e g from male to female as we move through the image to prevent this from happening we condition the patch synthesis on a draw from a factor analysis model section 7 6 which has been trained with frontal faces a sample from this model looks like a blurry but globally coherent face now when we choose potential patches they must agree with both the previously placed patches to the left and above but also be similar to the appropriate part of the blurry sample from the subspace model the generated images from this model look like highly realistic human faces in terms of probability the labels wn n in this model are conditioned not only on their ancestors wpa but also on the hidden variable in the subspace model h this connects to every patch label wn n and gives the resulting image a greater visual coherence than the markov connections of the patches alone discussion models for grids are ubiquitous in vision they occur in almost all applications that attempt to associate a label with each position in the image depending on the application this label may indicate the depth object type segmentation mask or motion at that pixel unfortunately most problems of this type are np hard and so we must resort to efficient approximate inference techniques such as the alpha expansion algorithm models for grids notes mrfs and crfs markov random fields were first investigated in computer vision by geman geman although much of the early work dealt with continuous variables rather than the discrete case as discussed in this chapter a good review can be found in li conditional random fields were first used in computer vision by kumar hebert an overview can be found in sutton mccallum applications grid based models and graph cuts are used extensively in vision and graphics a partial list of applications includes stereo vision kolmogorov zabih woodford et al optical flow kolmogorov zabih texture synthesis kwatra et al photo montage agarwala et al summarizing photo collections with collages rother et al rother et al bi layer segmentation kolmogorov et al interactive segmentation rother et al boykov et al super resolution freeman et al image re targeting pritch et al denoising greig et al over segmentation moore et al veksler et al image colorization levin et al segmantic segmentation shotton et al multi view reconstruction kolmogorov zabih vogiatzis et al and matching image points isack boykov graph cuts the first application of graph cuts to inference in an mrf is due to greig et al who investigated binary denoising however it was not until the work of boykov et al that this result was rediscovered and graph cuts became widely used ishikawa presented the exact solution for multi label graph cuts with convex potentials and this was generalized by schlesinger flach the presentation in this chapter is a hybrid of these two methods boykov et al introduced the idea of optimizing non convex multi label energies via a series of binary problems they proposed two algorithms of this kind the alpha beta swap in which pairs of labels are exchanged for one another and the alpha expansion algorithm they also proved that the alpha expansion solution is guaranteed to be within a factor of two of the true solution in the same spirit lempitsky et al and kumar et al have proposed more complex moves tarlow et al elucidates the conection between graph cut methods and max product belief propagation for more detailed overviews of graph cut methods consult boykov veksler felzenszwalb zabih and blake et al max flow graph cut methods rely on algorithms for computing maximum flow the most common of these are the augmenting paths method of ford fulkerson and the push relabel method of goldberg tarjan details of these and other approaches to the same problem can be found in any standard textbook on algorithms such as cormen et al the most common technique in computer vision is a modified version of the augmented paths algorithm due to boykov kolmogorov that has been demonstrated to have very good performance for vision problems kohli torr juan boykov and alahari et al have all investigated methods for improving the efficiency of graph cuts by reusing solutions to similar graph cut problems e g based on the solution to the previous frames in a time sequence cost functions and optimization kolmogorov zabih provide a summary of the cost functions that can be optimized using the basic graph cuts max flow formulation with binary variables kolmogorov rother summarize graph cut approaches to non submodular energies rother et al and komodakis et al present algorithms that can approximately optimize more general cost functions constraint edges recent work has investigated bespoke graph constructions that make heavy use of constraint edges edges of infinite strength to ensure that the solution conforms to a certain structure for example delong boykov devised a method that forced certain labels to surround others and moore et al describe a method that forces the label field to conform to a lattice see also felzenszwalb veksler for a related scheme based on dynamic programming higher order cliques all of the methods discussed in this chapter assume pairwise connections the cliques include only two discrete variables however to model more complex statistics of the label field it is necessary to include more than two variables in the cliques and these are known as higher order models roth black demonstrated good denoising and inpainting results with a continuous mrf model of this kind and domke et al demonstrated the efficacy of a directed model in which each variable was conditioned on a number of variables above and to the right in the image there has recently been considerable interest in developing algorithms for map estimation in models with discrete variables and higher order cliques ishikawa kohli et al kohli et al rother et al other approaches to map estimation there are many other contemporary ap proaches to map estimation in mrfs and crfs these include loopy belief propagation weiss freeman quadratic pseudo boolean optimization which is used in non submodular cost functions kolmogorov rother random walks grady and linear programming lp relaxations weiss et al and various approaches to maximize the lp lower bound such as tree reweighted message passing wainright et al kolmogorov an experimental comparison between different energy mini mization methods for mrfs can be found in szeliski et al texture synthesis texture synthesis was originally investigated as a continuous prob lem and the focus was on modeling the joint statistics of the rgb values in a small patch heeger bergen portilla simoncelli although texture synthesis as a con tinuous problem is still an active research area e g heess et al these early methods were displaced by methods that represented the texture in terms of discrete variables ei ther by quantizing the rgb values indexing patches or using a shift map representation the resulting algorithms e g efros leung wei levoy efros freeman kwatra et al were originally described as heuristic approaches to generating textures but can also be interpreted as exact or approximate ways to draw samples from directed or undirected grid models interactive segmentation the use of graph cuts for interactive segmentation al gorithms was pioneered by boykov jolly in early works boykov jolly boykov funka lea li et al the user interacted with the image by placing marks indicating foreground and background regions grab cut rother et al allowed the user to draw a box around the object in question more recent systems liu et al are fast enough to allow the user to interactively paint the selection onto the images current interest in graph cut based segmentation is mainly focused on developing novel priors over the shape that improve performance e g malcolm et al veksler chittajallu et al freiman et al to this end kumar et al introduced a method for imposing high level knowledge about the articula tion of the object vicente et al developed an algorithm that is suited for cutting out elongated objects and lempitsky et al used a prior based on a bounding box around the object stereo vision most state of the art stereo vision algorithms rely on mrfs or crfs and are solved using either graph cuts e g kolmogorov zabih or belief propagation e g sun et al comparisons of these approaches can be found in tappen freeman and szeliski et al an active area of research in dense stereo vision is the formulation of the compatibility of the two images given a certain disparity offset e g bleyer chambon hirschmu ller scharstein which is rarely based on single pixels in practice see yoon kweon tombari et al for more information about stereo vision see the reviews by scharstein szeliski and brown et al or consult szeliski which contains a good modern sum mary chapter of this book summarizes dynamic programming approaches notable stereo implementations include the region growing approach of lhuillier quan the systems of zitnick kanade and hirschmu ller both of which are avail able online and the extremely efficient gpu based system of sizintsev wildes for an up to date quantitative comparison of the latest stereo vision algorithms consult the middlebury stereo vision website http vision middlebury edu stereo problems problem consider a markov random field with the structure p r x x x x φ x x φ x x φ x x φ x x 4 z 4 4 but where the variables x2 x3 and x4 are continuous and the potentials are defined as φ a b exp a b this is known as a gaussian markov random field show that the joint probability is a normal distribution and find the information matrix inverse covariance matrix problem compute the map solution to the three pixel graph cut problem in fig ure by i computing the cost of all eight possible solutions explicitly and finding the one with the minimum cost ii running the augmenting paths algorithm on this graph by hand and interpreting the minimum cut figure graph for problem problem explicitly compute the costs associated with the four possible minimum cuts of the graph in figure 10 problem 4 compute the cost for each the four possible cuts of the graph in fig ure 11c problem 5 consider the graph construction in figure 37a which contains a number of constraint edges of infinite cost capacity there are 25 possible minimum cuts on this graph each of which corresponds to one possible labeling of the two pixels write out the cost for each labeling which solutions have finite cost for this graph construction source b source a3 b3 a3 b3 c3 sink t sink t figure 37 alternative multi label graph constructions each of these these graphs has extra constraint links with infinite weight these have the effect of giving an infinite cost to a subset of the possible solutions problem 6 which of the possible minimum cuts of the graph in figure 37b have a finite cost problem 7 confirm that the costs of the cuts in figure are as claimed by explicitly performing the summation over the relevant terms cij problem 8 show that the potts model figure 17c is not submodular by providing a counter example to the required criterion pab β γ pab α δ pab β δ pab α γ problem 9 an alternative to the alpha expansion algorithm is the alpha beta swap here a multi label mrf with non convex potentials is optimized by repeatedly choosing pairs of labels α β and performing a binary graph cut that allows them to swap in such a way that the overall cost function decreases devise a graph structure that can be used to perform this operation hint consider separate cases for neighboring labels α α β β β γ α γ and γ γ where γ is a label that is neither α nor β part iv preprocessing part iv preprocessing the main focus of this book is on statistical models for computer vision the pre vious chapters concern models that relate visual measurements x to the world w however there has been little discussion of how the measurement vector x was created and it has often been implied that it contains concatenated rgb pixel values in state of the art vision systems the image pixel data are almost always preprocessed to form the measurement vector we define preprocessing to be any transformation of the pixel data prior to building the model that relates the data to the world such transformations are often ad hoc heuristics their parameters are not learned from training data but they are chosen based on experience of what works well the philosophy behind image preprocessing is easy to understand the image data may be contingent on many aspects of the real world that do not pertain to the task at hand for example in an object detection task the rgb values will change depending on the camera gain illumination object pose and particular instance of the object the goal of image preprocessing is to remove as much of this unwanted variation as possible while retaining the aspects of the image that are critical to the final decision in a sense the need for preprocessing represents a failure we are admitting that we cannot directly model the relationship between the rgb values and the world state inevitably we must pay a price for this although the variation due to extraneous factors is jettisoned it is very probable that some of the task related in formation is also discarded fortunately in these nascent years of computer vision this rarely seems to be the limiting factor that governs the overall performance we devote the single chapter in this section to discussing a variety of prepro cessing techniques although the treatment here is not extensive it should be emphasized that preprocessing is very important in practice the choice of prepro cessing technique can influence the performance of vision systems at least as much as the choice of model chapter image preprocessing and feature extraction this chapter provides a brief overview of modern preprocessing methods for com puter vision in section we introduce methods in which we replace each pixel in the image with a new value section considers the problem of finding and characterizing edges corners and interest points in images in section we discuss visual descriptors these are low dimensional vectors that attempt to char acterize the interesting aspects of an image region in a compact way finally in section 4 we discuss methods for dimensionality reduction per pixel transformations we start our discussion of preprocessing with per pixel operations these methods return a single value corresponding to each pixel of the input image we denote the original array of pixel data as p where pij is the element at the ith of i rows and the jth of j columns the element pij is a scalar representing the grayscale intensity per pixel operations return a new array x of the same size as p containing elements xij whitening the goal of whitening figure is to provide invariance to fluctuations in the mean intensity level and contrast of the image such variation may arise because of a change in ambient lighting intensity the object reflectance or the camera gain to compensate for these factors the image is transformed so that the resulting pixel values have zero mean and unit variance to this end we compute the mean µ and variance of the original grayscale image p image preprocessing and feature extraction a b c figure whitening and histogram equalization a a number of faces which have been captured with widely varying contrasts and mean levels b after whitening the images have the same mean and variance c after histogram equalization the distribution of gray values is approximately uni form both of these transformations reduce the amount of variation due to contrast and intensity changes µ i i i i j j ij j j pij pij µ ij these statistics are used to transform each pixel value separately so that xij pij µ σ for color images this operation may be carried out by computing the statistics µ and from all three channels or by separately transforming each of the rgb channels based on their own statistics note that even this simple transform has the potential to hamper subsequent inference about the scene depending on the task the absolute intensities may or may not contain critical information even the simplest preprocessing methods must be applied with care histogram equalization the goal of histogram equalization figure is to modify the statistics of the intensity values so that all of their moments take predefined values to this end a nonlinear transformation is applied that forces the distribution of pixel intensities to be flat we first compute the histogram of the original intensities h where the kth of k entries is given by 00 figure 2 histogram equalization the abscissa indicates the pixel inten sity the ordinate indicates the pro portion of intensities that were less than or equal to this value this plot can be used as a look up table for histogram equalizing the intensities for a given intensity value on the ab scissa we choose the new intensity to be the maximum output intensity k times the value on the ordinate after this transformation the intensities are equally distributed in the example image many of the pixels are bright histogram equalization spreads these bright values out over a larger inten sity range and so has the effect of in creasing the contrast in the brighter regions i j hk δ pij k i j where the operation δ returns one if the argument is zero and zero otherwise we then cumulatively sum this histogram and normalize by the total number of pixels to compute the cumulative proportion c of pixels that are less than or equal to each intensity level ck k l ij hl 4 finally we use the cumulative histogram as a look up table to compute the trans formed value so that xij kcpij 5 for example in figure 2 the value will be mapped to k 29 where k is the maximum intensity usually the result is a continuous number rather than a discretized pixel intensity but is in the same range as the original data the result can be rounded to the nearest integer if subsequent processing demands linear filtering after filtering an image the new pixel value xij consists of a weighted sum of the intensities of pixels in the surrounding area of the original image p the weights are stored in a filter kernel f which has entries fm n where m m m and n n n more formally when we apply a filter we convolve the p with the filter f where two dimensional convolution is defined as problem figure image blurring a original image b result of convolving with a gaussian filter filter shown in bottom right of image each pixel in this image is a weighted sum of the surrounding pixels in the original image where the weights are given by the filter the result is that the image is slightly blurred c e convolving with a filter of increasing standard deviation causes the resulting image to be increasingly blurred xij pi m j nfm n 6 problem 2 m m n n notice that by convention the filter is flipped in both directions so the top left of the filter f m n weights the pixel pi m j n to the right and below the current point in p many filters used in vision are symmetric in such a way that this flipping makes no practical difference without further modification this formulation will run into problems near the borders of the image it needs to access points that are outside the image one way to deal with this is to use zero padding in which it is assumed that the value of p is outside the defined image region we now consider a number of common types of filter gaussian blurring filter to blur an image we convolve it with a gaussian f m n exp 7 problem 4 each pixel in the resulting image is a weighted sum of the surrounding pixels where the weights depend on the gaussian profile nearer pixels contribute relatively more to the final output this process blurs the image where the degree of blurring is dependent on the standard deviation σ of the gaussian filter figure this is a simple method to reduce noise in images taken at very low light levels first derivative filters and edge filters a second use for image filtering is to locate places in the image where the intensity changes abruptly consider taking the first derivative of the image along the rows we could approximate this operation by simply computing the difference between two offset pixels along the row this operation can be accomplished by filtering with the operator f this filter gives zero response when the image is flat in the horizontal direction it is hence invariant to constant additive luminance changes it gives a negative response when the image pixel values are increasing as we move in the horizontal direction and a positive response when they are decreasing recall that convolution flips the filter by as such it is selective for edges in the image the response to the filter f is noisy because of its limited spatial extent consequently slightly more sophisticated filters are used to find edges in practice examples include the prewitt operators figures b problem 5 problem 6 and the sobel operators 2 where in each case the filter fx is a filter selective for edges in the horizontal direction and fy is a filter selective for edges in the vertical direction laplacian filters the laplacian filter is the discrete two dimensional approximation to the laplacian operator 2 and is given by f 4 10 applying the discretized filter f to an image results in a response of high mag nitude where the image is changing regardless of the direction of that change figure the response is zero in regions that are flat and significant where edges occur in the image it is hence invariant to constant additive changes in luminance and useful for identifying interesting regions of the image laplacian of gaussian filters in practice the laplacian operator produces noisy results a superior approach is to first smooth the image with a gaussian filter and then apply the laplacian due to the associative property of convolution we can equivalently convolve the laplacian filter by a gaussian and apply the resulting laplacian of gaussian filter to the image figure this laplacian of gaussian has the advantage that it figure 4 image filtering with first and second derivative operators the original image is shown in figure a convolving with the vertical prewitt filter produces a response that is proportional to the size and polarity of edges in the vertical direction b the horizontal prewitt filter produces a response to edges in the horizontal direction c the laplacian filter gives a significant response where the image changes rapidly regardless of direction d the laplacian of gaussian filter produces similar results but the output is smoothed and hence less noisy e the difference of gaussians filter is a common approximation to the laplacian of gaussian can be tuned to be selective for changes at different scales depending on the scale of the gaussian component difference of gaussians the laplacian of gaussian filter is very well approximated by the difference of gaussians filter compare figures and as the name implies this filter is created by taking the difference of two gaussians at nearby scales the same result can be achieved by filtering the image with the two gaussians separately and taking the difference between the results again this filter responds strongly in regions of the image that are changing at a predetermined scale gabor filters gabor filters are selective for both scale and orientation the gabor function is the product of a gaussian with a sinusoid it is parameterized by the covariance of the gaussian and the phase φ orientation ω and wavelength λ of the sine wave if the gaussian component is spherical it is defined by fmn exp sin cos ω m sin ω n φ λ where σ controls the scale of the spherical gaussian it is typical to make the wavelength proportional to the scale σ of the gaussian so a constant number of cycles is visible the gabor filter is selective for elements within the image at a certain frequency and orientation band and with a certain phase figure 5 it is invariant to con stant additive changes in luminance when the sinusoidal component is asymmetric this is also nearly true for symmetric gabor functions as long as several cycles of the sinusoid are visible a response that is independent of phase can easily be generated by squaring and summing the responses of two gabor features with the same frequency orientation and scale but with phases that are π 2 radians apart the resulting quantity is termed the gabor energy and is somewhat invariant to small displacements of the image filtering with gabor functions is motivated by mammalian visual perception this is one of the first processing operations applied to visual data in the brain moreover it is known from psychological studies that certain tasks e g face de tection are predominantly dependent on information at intermediate frequencies this may be because high frequency filters see only a small image region and are hence noisy and relatively uninformative and low frequency filters act over a large region and respond disproportionately to slow changes due to lighting haar like filters haar like filters consist of adjacent rectangular regions that are balanced so that the average filter value is zero and they are invariant to constant luminance changes depending on the configuration of these regions they may be similar to derivative or gabor filters figure 6 image preprocessing and feature extraction figure 5 filtering with gabor functions a original image b after filtering with horizontal asymmetric gabor function at a large scale filter shown bottom right c result of filtering with horizontal symmetric gabor function at a large scale d filtering with vertical gabor filter responds to vertical changes e response to diagonal gabor function however haar like filters are noisier than the filters they approximate they have sharp edges between positive and negative regions and so moving by a single pixel near an edge may change the response significantly this drawback is com pensated for by the relative speed with which haar functions can be computed to compute haar functions rapidly we first form the integral image figure this is an intermediate representation in which each pixel contains the sum of all of the intensity values above and to the left of the current position so the value in the top left corner is the original pixel value at that position and the value in the bottom right corner is the sum of all of the pixel values in the image the values in the other parts of the integral image are between these extremes given the integral image i it is possible to compute the sum of the intensities in any rectangular region with just four operations regardless of how large this region is consider the region is defined by the range down the columns and along the rows the sum s of the internal pixel intensities is s problem the logic behind this calculation is illustrated in figure i since haar like filters are composed of rectangular regions they can be com puted using a similar trick for a filter with two adjacent rectangular regions six operations are required with three adjacent rectangular regions eight opera tions are required when the filter dimensions m and n are large this approach compares very favorably to a na ıve implementation of conventional filtering which requires o m n operations to compute the filter response due to a m n ker nel haar like filters are often used in real time applications such as face detection because of the speed with which they can be computed per pixel transformations figure 6 haar like filters a d haar like filters consist of rectangular regions convolution with haar like filters can be done in constant time e to see why consider the problem of filtering with this single rectangular region f we denote the sum of the pixel values in these four regions as a b c and d our goal is to compute d g the integral image has a value that is the sum of the intensities of the pixels above and to the left of the current position the integral image at position hence has value a b c d h the integral image at has value a c i the integral image at has value a b j the integral image at j0 has value a the sum of the pixels in region d can now be computed as j1 j0 j0 j1 a b c d a a c a b d this requires just four operations regardless of the size of the original square region 4 local binary patterns the local binary patterns lbp operator returns a discrete value at each pixel that characterizes the local texture in a way that is partially invariant to luminance changes for this reason features based on local binary patterns are commonly used as a substrate for face recognition algorithms the basic lbp operator compares the eight neighboring pixel intensities to the center pixel intensity assigning a or a to each neighbor depending on whether they are less than or greater than the center value these binary values are then concatenated in a predetermined order and converted to a single decimal number that represents the type of local image structure figure 7 with further processing the lbp operator can be made orientation invari ant the binary representation is repeatedly subjected to bit wise shifts to create eight new binary values and the minimum of these values is chosen this reduces the number of possible lbp values to in practice it has been found that the distribution over these 36 lbp values is dominated by those that are relatively uni form in other words binary strings where transitions are absent e g or infrequent e g occur most frequently the number of texture classes can be further reduced by aggregating all of the non uniform lbps into a single class now the local image structure is categorized into problem 7 image preprocessing and feature extraction a b figure 7 local binary patterns a the local binary pattern lbp is computed by comparing the central pixel to each of its eight neighbors the binary value associated with each position is set to one if that neighbor is greater than or equal to the central pixel the eight binary values can be read out and combined to make a single 8 bit number b local binary patterns can be computed over larger areas by comparing the current pixels to the interpolated image at positions on a circle this type of lbp is characterized by the number of samples p and the radius of the circle r nine lbp types eight rotationally invariant uniform patterns and one non uniform class the lbp operator can be extended to use neighborhoods of different sizes the central pixel is compared to positions in a circular pattern figure in general these positions do not exactly coincide with the pixel grid and the intensity at these positions must be estimated using bilinear interpolation this extended lbp operator can capture texture at different scales in the image 5 texton maps the term texton stems from the study of human perception and refers to a prim itive perceptual element of texture in other words it roughly occupies the role that a phoneme takes in speech recognition in a machine vision context a texton is a discrete variable that designates which one of a finite number of possible texture classes is present in a region surrounding the current pixel a texton map is an image in which the texton is computed at every pixel figure 8 texton assignment depends on training data a bank of n filters is convolved with a set of training images the responses are concatenated to form one n vector for each pixel position in each training image these vectors are then clustered into k classes using the k means algorithm section 4 4 textons are computed for a new image by convolving it with the same filter bank for each pixel the texton is assigned by noting which cluster mean is closest to the n filter output vector associated with the current position the choice of filter bank seems to be relatively unimportant one approach has been to use gaussians at scales σ and to filter all three color channels and derivatives of gaussians at scales and and laplacians of gaussians at scales σ and to filter the luminance figure in this way both color and figure 8 texton maps in a tex ton map each pixel is replaced by the texton index this index characterizes the texture in the surrounding region a original image b associated tex ton map note how similar regions are assigned the same texton index in dicated by color c original image d associated texton map using dif ferent filter bank from b texton maps are often used in semantic image segmentation adapted from shotton et al qc springer a c d b figure 9 textons the image is convolved with a filter bank to yield an n vector of filter responses at each position possible choices for the filter bank include a a combination of gaussians derivatives of gaussians and laplacians of gaussians b rotationally invariant filters and c the maximum response database d in training the n filter response vectors are clustered using k means for new data the texton index is assigned based on the nearest of these clusters thus the filter space is effectively partitioned into voronoi regions texture information is captured it may be desirable to compute textons that are invariant to orientation one way of achieving this is to choose rotationally invariant filters to form the filter bank figure however these have the undesirable property of not responding at all to oriented structures in the image the maximum response filter bank is designed to provide a rotationally invariant measure of local texture which does not discard this information the filter bank figure 9c consists of a gaussian and a laplacian of gaussian filter an edge filter at three scales and a bar filter a symmetric oriented filter at the same three scales the edge and bar filter are replicated at 6 orientations at each scale giving a total of filters to induce rotational invariance only the maximum filter response over orientation is used hence the final vector of filter responses consists of eight numbers corresponding to the gaussian and laplacian filters already invariant and the maximum responses over orientation of the edge and bar filters at each of the three scales figure 10 reconstruction from edges a original image b edge map each edge pixel has associated scale and orientation information as well as a record of the luminance levels at either side c the image can be recon structed almost perfectly from the edges and their associated information adapted from elder qc springer 2 edges corners and interest points in this section we consider methods that aim to identify informative parts of the image in edge detection the goal is to return a binary image where a non zero value denotes the presence of an edge in the image edge detectors optionally also return other information such as the orientation and scale associated with the edge edge maps are a highly compact representation of an image and it has been shown that it is possible to reconstruct an image very accurately with just information about the edges in the scene figure 10 corners are positions in the image that contain rich visual information and can be found reproducibly in different images of the same object figure there are many schemes to find corners but they all aim to identify points that are locally unique corner detection algorithms were originally developed for geometric computer vision problems such as wide baseline image matching here we see the same scene from two different angles and wish to identify which points correspond to which in recent years corners have also been used in object recognition algorithms where they are usually referred to as interest points the idea here is that the regions surrounding interest points contain information about which object class is present 2 canny edge detector to compute edges with the canny edge detector figure the image p is first blurred and then convolved with a pair of orthogonal derivative filters such as pre witt filters to create images h and v containing derivatives in the horizontal and vertical directions respectively for pixel i j the orientation θij and magnitude aij of the gradient is computed using figure canny edge detection a original image b result of vertical prewitt filter c results of horizontal prewitt filter d quantized orien tation map e gradient amplitude map f amplitudes after non maximal suppression g thresholding at two levels the white pixels are above the higher threshold the red pixels are above the lower threshold but below the higher one h final edge map after hysteresis thresholding contains all of the white pixels from g and those red pixels that connect to them θij arctan vij hij aij a simple approach would be to assign an edge to position i j if the amplitude there exceeds a critical value this is termed thresholding unfortunately it pro duces poor results the amplitude map takes high values on the edge but also at adjacent positions the canny edge detector eliminates these unwanted responses using a method known as non maximum suppression in non maximum suppression the gradient orientation is quantized into one of four angles 135o where angles apart are treated as equivalent the pixels associated with each angle are now treated separately for each pixel the amplitude is set to zero if either of the neighboring two pixels perpendicular to the gradient have higher values for example for a pixel where the gradient orientation is vertical the image is changing in the horizontal direction the pixels to the left and right are examined and the amplitude is set to zero if either of these figure harris corner detector a image with detected corners the corner detection algorithm is based on the image structure tensor which captures information about the distribution of gradients around the point b in flat regions both singular values of the image structure tensor are small c on edges one is small and the other large d at corners both are large indicating that the image is changing quickly in both directions are greater than the current value in this way the gradients at the maximum of the edge amplitude profile are retained and those away from this maximum are suppressed a binary edge map can now be computed by comparing the remaining non zero amplitudes to a fixed threshold however for any given threshold there will be misses places where there are real edges but their amplitude falls below the threshold and false positives pixels labeled as edges where none exist in the orig inal image to decrease these undesirable phenomena knowledge about the con tinuity of real world edges is exploited two thresholds are defined all of the pixels whose amplitude is above the higher threshold are labeled as edges and this threshold is chosen so that there are few false positives to try to decrease the number of misses pixels that are above the lower amplitude threshold and are connected to an existing edge pixel are also labeled as edges by iterating this last step it is possible to trace along weaker parts of strong contours this technique is known as hysteresis thresholding 2 edges corners and interest points 2 2 harris corner detector the harris corner detector figure considers the local gradients in the hori zontal and vertical directions around each point the goal is to find points in the image where the image intensity is varying in both directions a corner rather than in one direction an edge or neither a flat region the harris corner detector bases this decision on the image structure tensor sij i d j d wmn 2 mn mn vmnl m i d n j d hmnvmn where sij is the image structure tensor at position i j which is computed over a square region of size around the current position the term hmn denotes the response of a horizontal derivative filter such as the sobel at position m n and the term vmn denotes the response of a vertical derivative filter the term wmn is a weight that diminishes the contribution of positions that are far from the central pixel i j to identify whether a corner is present the harris corner detector considers the singular values of the image structure tensor if both singular values are small then the region around the point is smooth and this position is not chosen if one singular value is large but the other small then the image is changing in one direction but not the other and point lies on or near an edge however if both singular values are large then this image is changing rapidly in both directions in this region and the position is deemed to be a corner in fact the harris detector does not directly compute the singular values but evaluates a criterion which accomplishes the same thing more efficiently cij κ λ2 det sij κ trace sij where κ is a constant values between 0 04 and 0 are sensible if the value of cij is greater than a predetermined threshold then a corner may be assigned there is usually an additional non maximal suppression stage similar to that in the canny edge detector to ensure that only peaks in the function cij are retained 2 sift detector the scale invariant feature transform sift detector is a second method for iden tifying interest points unlike the harris corner detector it associates a scale and orientation to each of the resulting interest points to find the interest points a number of operations are performed in turn the intensity image is filtered with a difference of gaussian kernel at a series of k increasingly coarse scales figure then the filtered images are stacked to make a volume of size i j k where i and j are the vertical and horizontal size of the image extrema are identified within this volume these are positions where the voxel neighbors from a block are either all greater than or all less than the current value image preprocessing and feature extraction figure the sift detector a original image b h the image is filtered with difference of gaussian kernels at a range of increasing scales i the resulting images are stacked to create a volume points that are local extrema in the filtered image volume i e are either greater than or less than all 26 neighbors are considered to be candidates for interest points figure refinement of sift detector candidates a positions of ex trema in the filtered image volume figure note that the scale is not shown these are considered candidates to be interest points b re maining candidates after eliminating those in smooth regions c remaining candidate points after removing those on edges using the image structure tensor these extrema are localized to sub voxel accuracy by applying a local quadratic approximation and returning the position of the peak or trough the quadratic approximation is made by taking a taylor expansion about the current point this provides a position estimate that has sub pixel resolution and an estimate of the scale that is more accurate than the resolution of the scale sampling finally the image structure tensor sij equation is computed at the location and scale of each point candidate points in smooth regions and on edges are removed by considering the singular values of sij as in the harris corner detector figure this procedure returns a set of interest points that are localized to sub pixel descriptors figure results of sift detec tor each final interest point is in dicated using an arrow the length of the arrow indicates the scale with which the interest point is identified and the angle of the arrow indicates the associated orientation notice that there are some positions in the image where the orientation was not unique and here two interest points are used one associated with each ori entation an example of this is on the right shirt collar subsequent descrip tors that characterize the structure of the image around the interest points are computed relative to this scale and orientation and hence inherit some in variance to these factors accuracy and associated accurately with a particular scale finally a unique ori entation is also assigned to each interest point to this end the amplitude and orientation of the local gradients are computed equations in a region sur rounding the interest point whose size is proportional to the identified scale an orientation histogram is then computed over this region with 36 bins covering all of orientation the contribution to the histogram depends on the gradient amplitude and is weighted by a gaussian profile centered at the location of the interest point so that nearby regions contribute more the orientation of the in terest point is assigned to be the peak of this histogram if there is a second peak within of the maximum we may choose to compute descriptors at two orienta tions at this point the final detected points are hence associated with a particular orientation and scale figure 15 descriptors in this section we consider descriptors these are compact representations that summarize the contents of an image region histograms the simplest approach to aggregating information over a large image region is to compute a histogram of the responses in this area for example we might collate rgb pixel intensities filter responses local binary patterns or textons into a histogram depending on the application the histogram entries can be treated as discrete and modeled with a categorical distribution or treated as a continuous vector quantity image preprocessing and feature extraction a b figure sift descriptor a gradients are computed for every pixel within a region around the interest point b this region is subdivided into cells information is pooled within these cells to form an histogram these histograms are concatenated to provide a final descriptor that pools locally to provide invariance to small deformations but also retains some spatial information about the image gradients in this figure information from an 8 8 pixel patch has been divided to make a 2 2 grid of cells in the original implementation of the sift detector a patch was divided into at 4 4 grid of cells for continuous quantities such as filter responses the level of quantization is critical quantizing the responses into many bins potentially allows fine discrimi nation between responses however if data are scarce then many of these bins will be empty and it is harder to reliably determine the statistics of the descriptor one approach is to use an adaptive clustering method such as k means section 4 4 to automatically determine the bin sizes and shapes histogramming is a useful approach for tasks where spatial resolution is not paramount for example to classify a large region of texture it makes sense to pool information however this approach is largely unsuitable for characterizing structured objects the spatial layout of the object is important for identification we now introduce two representations for image regions that retain some spatial information but also pool information locally and thus provide invariance to small displacements and warps of the image both the sift descriptor section 2 and the hog descriptor section concatenate several histograms that were computed over spatially distinct blocks 2 sift descriptors the scale invariant feature transform sift descriptor figure characterizes the image region around a given point it is usually used in conjunction with interest points that were found using the sift detector these interest points are associated with a particular scale and rotation and the sift descriptor would typically be computed over a square region that is transformed by these values descriptors figure 17 hog descriptor a original image b gradient orientation quantized into nine bins from 0 to c gradient magnitude d cell descriptors are orientation histograms that are computed within 6 6 pixel regions e block descriptors are computed by concatenating blocks of cell descriptors the block descriptors are normalized the final hog descriptor consists of the concatenated block descriptors the goal is to characterize the image region in a way that is partially invariant to intensity and contrast changes and small geometric deformations to compute the sift descriptor we first compute gradient orientation and amplitude maps equation as for the canny edge detector over a pixel region around the interest point the resulting orientation is quantized into eight bins spread over the range 0 360o then the detector region is divided into a regular grid of non overlapping 4 4 cells within each of these cells an eight dimensional histogram of the image orientations is computed each contribution to the histogram is weighted by the associated gradient amplitude and by distance so that positions further from the interest point contribute less the 4 4 histograms are concatenated to make a single vector which is then normalized the descriptor is invariant to constant intensity changes as it is based on gra dients the final normalization provides some invariance to contrast small defor mations do not affect the descriptor too much as it pools information within each cell however by keeping the information from each cell separate some spatial information is retained 3 histogram of oriented gradients the histogram of oriented gradients hog descriptor attempts to construct a more detailed characterization of the spatial structure with a small image win dow it is a useful preprocessing step for algorithms that detect objects with quasi regular structure such as pedestrians like the sift descriptor the hog descriptor consists of a collection of normalized histograms computed over spatially offset patches the result is a descriptor that captures coarse spatial structure but is invariant to small local deformations the process of computing a hog descriptor suitable for pedestrian detection consists of the following stages first the orientation and amplitude of the image gradients are computed at every pixel in a window using equation the orientation is quantized into nine bins spread over the range 0 180o the detector region is divided into a regular grid of overlapping 6 6 cells a orientation histogram is computed within each cell where the contribution to the histogram is weighted by the gradient amplitude and the distance from the center of the cell so that more central pixels contribute more for each 3 3 block of cells the descriptors are concatenated and normalized to form a block descriptor all of the block descriptors are concatenated to form the final hog descriptor the final descriptor contains spatially pooled information about local gradients within each cell but maintains some spatial resolution as there are many cells it creates invariance to contrast polarity by only using the gradient magnitudes it creates invariance to local contrast strength by normalizing relative to each block the hog descriptor is similar in spirit to the sift descriptor but is distinguished by being invariant to contrast polarity having a higher spatial resolution of com puted histograms and performing normalization more locally 3 4 bag of words descriptor the descriptors discussed thus far have been intended to characterize small regions of images often these regions have been connected to interest points the bag of words representation attempts to characterize a larger region or an entire image by summarizing the statistics of the descriptors e g sift associated with all of the interest points in a region each observed descriptor is considered to be one of a finite vocabulary of pos sible descriptors termed visual words collectively this vocabulary is known as a dictionary the bag of words descriptor is simply a histogram describing the frequency of observing these words giving no regard to their position to compute the dictionary interest points are found in a large number of images and the as sociated descriptor is computed these descriptors are clustered using k means section 4 4 to compute the bag of words representation each descriptor is assigned to the nearest word in this dictionary the bag of words representation is a remarkably good substrate for object recog nition this is somewhat surprising given that it surrenders any knowledge about the spatial configuration about the object of course the drawback of approaches based on the bag of words is that it is very hard to localize the object after we have identified its presence or to decide how many instances are present using the same model a d e figure shape context descrip tor a object silhouette b con tour of silhouette c points are placed at equally spaced intervals around the silhouette d a log polar sampling array is centered at each point e the shape of the object relative to this point is captured by the histogram over the bins of the log polar array the final descriptor would consist of a concatenation of the values from his tograms from multiple points around the edge of the object 3 5 shape context descriptor for certain vision tasks the silhouette of the object contains much more informa tion than the rgb values themselves consider for example the problem of body pose estimation given an image of a human being the goal is to estimate the joint angles of the body unfortunately the rgb values of the image depend on the person s clothing and are relatively uninformative in such situations it is wiser to attempt to characterize the shape of the object the shape context descriptor is a fixed length vector that characterizes the object contour essentially it encodes the relative position of points on the contour in common with the sift and hog descriptors it pools information locally over space to provide a representation that can capture the overall structure of the object but is not affected too much by small spatial variations to compute the shape context descriptor figure a discrete set of points is sampled along the contour of the object a fixed length vector is associated with each point that characterizes the relative position of the other points to this end a log polar sampling array is centered on the current point a histogram is then computed where each bin contains the number of the other points on the silhouette that fell into each bin of the log polar array the choice of the log polar scheme means that the descriptor is very sensitive to local changes in the shape but only captures the approximate configuration of distant parts the collection of histograms for all of the points on this image captures the shape however to directly match to another shape the point correspondence must be established it is possible to make this descriptor invariant to orientation by evaluating the orientation of the contour at each point and rotating the log polar sampling scheme so that it is aligned with this orientation 4 dimensionality reduction it is often desirable to reduce the dimensionality of either the original or pre processed image data if we can do this without losing too much information then the resulting models will require fewer parameters and be faster to learn and to image preprocessing and feature extraction b figure reduction to a single dimension a original data and direction φ of maximum variance b the data are projected onto φ to produce a one dimensional representation c to reconstruct the data we re multiply by φ most of the original variation is retained pca extends this model to project high dimensional data onto the k orthogonal dimensions with the most variance to produce a k dimensional representation use for inference dimensionality reduction is possible because a given type of image data e g rgb values from face images usually lie in a tiny subset of the possible data space not all sets of rgb values look like real images and not all real images look like faces we refer to the subset of the space occupied by a given dataset as a manifold dimensionality reduction can hence be thought of as a change of variables we move from the original coordinate system to the reduced coordinate system within the manifold our goal is hence to find a low dimensional or hidden representation h which can approximately explain the data x so that x f h θ 16 where f is a function that takes the hidden variable and a set of parameters θ we would like the lower dimensional representation to capture all of the rele vant variation in the original data hence one possible criterion for choosing the parameters is to minimize the least squares reconstruction error so that θˆ i argmin θ i i i xi f hi θ t xi f hi θ 17 where xi is the ith of i training examples in other words we aim to find a set of low dimensional variables hi i and a mapping from h to x so that it reconstructs the original data as closely as possible in a least squares sense 4 dimensionality reduction 4 approximation with a single number let us first consider a very simple model in which we attempt to represent each observed datum with a single number figure so that xi φhi µ 18 where the parameter µ is the mean of the data set and the parameter φ is a basis vector mapping the low dimensional representation h back to the original data space x for simplicity we will assume from now on that the mean of the dataset is zero and xi φhi this can be achieved by computing the empirical mean µ and subtracting it from every example xi the learning algorithm optimizes the criterion φˆ i argmin e argmin r i xi φhi t xi φhi φ i φ i i careful consideration of the cost function equation 19 reveals an immediate problem the solution is ambiguous as we can multiply the basis function φ by any constant k and divide each of the hidden variables hi i by the same number to yield exactly the same cost to resolve this problem we force the vector φ to have unit length this is accomplished by adding in a lagrange multiplier λ so that the cost function becomes i e xi φhi t xi φhi λ φt φ i xt xi xi λ φt φ to minimize the function we first take the derivative with respect to hi and then equate the resulting expression to zero to yield ˆ ˆt hi φ xi 21 in other words to find the reduced dimension representation hi we simply project the observed data onto the vector φ we now take the derivative of equation 20 with respect to φ substitute in the solution for hi equate the result to zero and re arrange to get i t ˆ ˆ i or in matrix form i xxt φˆ λφˆ 23 image preprocessing and feature extraction where the matrix x x2 xi contains the data examples in its columns this is an eigenvalue problem to find the optimal vector we compute the svd ulvt xxt and choose the first column of u the scatter matrix xxt is a constant multiple of the covariance matrix and so this has a simple geometric interpretation the optimal vector φ to project onto corresponds to the principal direction of the covariance ellipse this makes intuitive sense we retain information from the direction in space where the data vary most 4 2 principal component analysis principal component analysis pca generalizes the above model instead of find ing a scalar variable hi that represents the ith data example xi we now seek a k dimensional vector hi the relation between the hidden and observed spaces is xi φhi 24 where the matrix φ φ2 φk contains k basis functions or principal components the observed data are modeled as a weighted sum ofthe principal components where the kth dimension of hi weights the kth component the solution for the unknowns φ and i can now be written as φ i argmin e argmin r i xi φhi t xi φhi 25 φ i φ i i once more the solution to this is non unique as we can post multiply φ by any matrix a and pre multiply each hidden variable hi by the inverse a and still get the same cost to partially resolve this problem we add the extra constraint that φt φ i in other words we force the principal components to be orthogonal and length one this gives a modified cost function of i e xi φhi t xi φhi λ φt φ i 26 i where λ is a lagrange multiplier we now minimize this expression with respect to φ i and λ the expression for the hidden variables becomes hi φt xi 27 the k principal components φ φ2 φk are now found by computing the singular value decomposition ulvt xxt and taking the first k columns of u in other words to reduce the dimensionality we project the data xi onto a hyperplane defined by the k largest axes of the covariance ellipsoid this algorithm is very closely related to probabilistic principal component anal ysis section 17 5 probabilistic pca additionally models the noise that accounts for the inexact approximation in equation 24 factor analysis section 7 6 is also very similar but constructs a more sophisticated model of this noise 4 dimensionality reduction 4 3 dual principal component analysis the preceding method requires us to compute the svd of the scatter matrix xxt unfortunately if the data had dimension d then this is a d d matrix which may be very large we can sidestep this problem by using dual variables we define φ as a weighted sum of the original data points so that φ xψ 28 where ψ ψk is a i k matrix representing these weights the associated cost function now becomes algorithm i e xi xψhi t xi xψhi λ ψt xt xψ i 29 i the solution for the hidden variables becomes hi ψt xt xi φt xi 30 and the k dual principal components ψ ψk are extracted from the matrix u in the svd ulvt xt x this is a smaller problem of size i i and is more efficient when the number of data examples i is less than the dimensionality of the observed space d notice that this algorithm does not require the original data points it only requires the inner products between them and so it is amenable to kernelization this resulting method is known as kernel pca 4 4 the k means algorithm a second common approach to dimensionality reduction is to abandon a continuous representation altogether and represent each data point using one of a limited set of prototype vectors in this model the data are approximated as xi µhi 31 where hi 2 k is an index that identifies which of the k prototype vectors algorithm 2 µk k approximates the ith example to find the assignment indices and the prototype vectors figure 20 we optimize k i argmin µ h i i xi µhi t xi µhi 32 in the k means algorithm this cost function is minimized using an alternating strategy in which we first assign each datapoint to the nearest prototype hˆi argmin xi hi µhi t x µhi l 33 figure 20 k means algorithm for k 3 clusters a we initialize the three prototype vectors crosses to random positions we alternately b assign the data to the nearest prototype vector and c update the prototype vectors to be equal to the mean of the points assigned to them d i we repeat these steps until there is no further change and then update the prototypes µˆk argmin µk i i xi µhi t xi µhi i i i xiδ hi k δ hi k 34 i where δ is a function that returns one when its argument is zero and zero other wise in other words the new prototype µˆk is simply the average of the data points that are assigned to this cluster this algorithm is not guaranteed to converge to the global minimum and so it requires sensible starting conditions the k means algorithm is very closely related to the mixtures of gaussians model section 7 4 the main differences are that the mixtures of gaussians model is probabilistic and defines a density over the data space it also assigns weights to the clusters and describes their covariance conclusion careful reading of the information in this chapter should convince you that there are certain recurring ideas in image preprocessing to make a descriptor invariant to intensity changes we filter the image and normalize the filter responses over the region a unique descriptor orientation and scale can be computed by maximizing over responses at different orientations and scales to create invariance to small spatial changes local responses are pooled despite the simplicity of these ideas it is remarkable how much impact they have on the performance of real systems problem 8 problem 9 problem 10 image preprocessing and feature extraction notes image processing there are numerous texts on image processing which contain far more information than i could include in this chapter i would particularly recommend the books by o gorman et al gonzalez woods pratt and nixon aguado a comprehensive recent summary of local image features can be found in li allinson edge and corner detection the canny edge detector was first described in canny elder investigated whether it was possible to reconstruct an image based on edge information alone nowadays it is common to use machine learning methods to identify object boundaries in images e g doll ar et al early work in corner detection interest point detection includes that of moravec forstner and the harris corner detector harris stephens which we de scribed in this chapter other more recent efforts to identify stable points and regions include the susan corner detector smith brady a saliency based descriptor kadir brady maximally stable extremal regions matas et al the sift detector lowe and the fast detector rosten drummond there has been considerable recent interest in affine invariant interest point detection which aims to find features that are stable under affine transforms of the image e g schaffalitzky zisserman mikolajczyk schmid mikolajczyk schmid mikolajczyk et al present a quantitative comparison of different affine region detectors a recent review of this area can be found in tuytelaars mikolajczyk image descriptors for robust object recognition and image matching it is crucial to characterize the region around the detected interest point in a way that is compact and sta ble to changes in the image to this end lowe developed the sift descriptor dalal triggs developed the hog descriptor and forss en lowe developed a descriptor for use with maximally stable extremal regions bay et al developed a very efficient version of sift features known as surf mikolajczyk schmid present a quantitative comparison of region descriptors recent work on image descriptors has applied machine learning techniques to optimize their performance brown et al philbin et al more information about local binary patterns can be found in ojala et al more information about the shape context descriptor can be found in belongie et al dimensionality reduction principal components analysis is a linear dimensionality reduction method however there are also many nonlinear approaches that describe a manifold of images in high dimensions with fewer parameters notable methods include kernel pca sch olkopf et al isomap tenenbaum et al local linear embed ding roweis saul charting brand the gaussian process latent variable model lawrence and laplacian eigenmaps belkin niyogi recent reviews of dimensionality reduction can be found in burgess and de la torre problems problem consider an 8 bit image in which the pixel values are evenly distributed in the range with no pixels taking a value of 128 or larger draw the cumulative histogram for this image see figure 2 what will the histogram of pixel intensities figure 21 clustering with the k means algorithm in the presence of outliers problem 9 this data set contains two clusters and a single out lier the point on the right hand side the outlier causes problems for the k means algorithm when k 2 clusters are used due to the implicit assump tion that the clusters can be modeled as normal distributions with spherical covariance look like after applying histogram equalization problem 2 consider a continuous image p i j and a continuous filter f n m in the continuous domain the operation f p of convolving an image with the filter is defined as f p p i m j n f m n dmdn now consider two filters f and g prove that convolving the image first with f and then with g has the same effect as convolving f with g and then convolving the image with the result in other words g f p g f p does this result extend to discrete images problem 3 describe the series of operations that would be required to compute the haar like filters in figures 6a d from an integral image how many points from the integral image are needed to compute each problem 4 consider a blurring filter where each pixel in an image is replaced by a weighted average of local intensity values but the the weights decrease if these intensity values differ markedly from the central pixel what effect would this bilateral filter have when applied to an image problem 5 define a 3 3 filter that is specialized to detecting luminance changes at a angle and gives a positive response where the image intensity increases from the bottom left to the bottom right of the image problem 6 define a 3 3 filter that responds to the second derivative in the horizontal direction but is invariant to the gradient and absolute intensity in the horizontal direction and invariant to all changes in the vertical direction problem 7 why are most local binary patterns in a natural image typically uniform or near uniform problem 8 give one example of a data set where the mixtures of gaussians model will succeed in clustering the data but the k means algorithm will fail problem 9 consider the data in figure 21 what do you expect to happen if we run the k means algorithm with two clusters on this data set suggest a way to resolve this problem problem 10 an alternative approach to clustering the data would be to find modes peaks in the density of the points this potentially has the advantage of also automat ically selecting the number of clusters propose an algorithm to find these modes part v models for geometry part v models for geometry in part v we finally acknowledge the process by which real world images are formed light is emitted from one or more sources and travels through the scene interacting with the materials via physical processes such as reflection refraction and scattering some of this light enters the camera and is measured we have a very good understanding of this forward model given known geometry light sources and material properties computer graphics techniques can simulate what will be seen by the camera very accurately the ultimate goal for a vision algorithm would be a complete reconstruction in which we aim to invert this forward model and estimate the light sources materials and geometry from the image here we aim to capture a structural description of the world we seek an understanding of where things are and to measure their optical properties rather than a semantic understanding such a structural description can be exploited to navigate around the environment or build models for computer graphics unfortunately full visual reconstruction is very challenging for one thing the solution is non unique for example if the light source intensity increases but the object reflectance decreases commensurately the image will remain unchanged of course we could make the problem unique by imposing prior knowledge but even then reconstruction remains difficult it is hard to effectively parameterize the scene and the problem is highly non convex in this part of the book we consider a family of models that approximate both the scene and the observed image with sparse sets of visual primitives points the forward model that maps the proxy representation of the world points to the proxy representation of the image points is much simpler than the full light transport model and is called the projective pinhole camera we investigate the properties of this model in chapter in chapter 15 we consider the situation where the pinhole camera views a plane in the world there is now a one to one mapping between points on the plane and points in the image and we characterize this mapping with a family of transformations in chapter 16 we will further exploit the pinhole camera model to recover a sparse geometric model of the scene chapter the pinhole camera this chapter introduces the pinhole or projective camera this is a purely geometric model that describes the process whereby points in the world are projected into the image clearly the position in the image depends on the position in the world and the pinhole camera model captures this relationship to motivate this model we will consider the problem of sparse stereo reconstruc tion figure we are given two images of a rigid object taken from different positions let us assume that we can identify corresponding features between the two images points that are projected versions of the same position in the world now the goal is to establish this position using the observed feature points the resulting information could be used by a robot to help it navigate through the scene or to facilitate object recognition the pinhole camera in real life a pinhole camera consists of a closed with a small hole the pinhole in the front figure 2 rays from an object in the world pass through this hole to form an inverted image on the back face of the box or image plane our goal is to build a mathematical model of this process it is slightly inconvenient that the image from the pinhole camera is upside down hence we instead consider the virtual image that would result from placing the image plane in front of the pinhole of course it is not physically possible to build a camera this way but it is mathematically equivalent to the true pinhole model except that the image is the right way up and it is easier to think about from now on we will always draw the image plane in front of the pinhole figure 3 illustrates the pinhole camera model and defines some terminology the pinhole itself the point at which the rays converge is called the optical center we will assume for now that the optical center is at the origin of the world coordinate system in which points are represented as w u v w t the virtual is not an accidental choice of world the term camera is derived from the latin word for chamber figure sparse stereo reconstruction a b we are given two images of the same scene taken from different positions and a set of i pairs of points in these images that are known to correspond to the same points in the world e g the points connected by the red line are a corresponding pair c our goal is to establish the position of each of the world points here the depth is encoded by color so that closer points are red and more distant points are blue figure 2 the pinhole camera model rays from an object in the world pass through the pinhole in the front of the camera and form an image on the back plane the image plane this image is upside down so we can alternatively consider the virtual image that would have been created if the image plane was in front of the pinhole this is not physically possible but it is more convenient to work with image is created on the image plane which is displaced from the optical center along the w axis or optical axis the point where the optical axis strikes the image plane is known as the principal point the distance between the principal point and the optical center i e the distance between the image plane and the pinhole is known as the focal length the pinhole camera model is a generative model that describes the likelihood p r x w of observing a feature at position x x y t in the image given that it is the projection of a point w u v w t in the world although light transport figure 3 pin hole camera model terminology the optical center pinhole is placed at the origin of the world coordinate system u v w and the image plane where the virtual image is formed is displaced along the w axis which is also known as the optical axis the position where the optical axis strikes the image plane is called the principal point the distance between the image plane and the optical center is called the focal length is essentially deterministic we will nonetheless build a probability model there is noise in the sensor and unmodeled factors in the feature detection process can also affect the measured image position however for pedagogical reasons we will defer a discussion of this uncertainty until later and temporarily treat the imaging process as if it were deterministic our task then is to establish the position x x y t where the point w u v w t is imaged considering figure 3 it is clear how to do this we connect a ray between w and the optical center the image position x can be found by observing where this ray strikes the image plane this process is called perspective projection in the next few sections we will build a more precise mathematical model of this process we will start with a very simple camera model the normalized camera and build up to a full camera parameterization the normalized camera in the normalized camera the focal length is one and it is assumed that the origin of the coordinate system x y on the image plane is centered at the principal point figure 4 shows a slice of the geometry of this system the u and x axes now point upward out of the page and cannot be seen by similar triangles it can easily be seen that the y position in the image of the world point figure 4 normalized camera the focal length is one and the im age coordinate system x y is cen tered on the principal point only y axis shown by similar triangles the y position in the image of a point at u v w is given by v w this cor responds to our intuition as an ob ject gets more distant its projection becomes closer to the center of the image at w u v w t is given by v w more generally in the normalized camera a point w u v w t is projected into the image at x x y t using the relations u x w v y 1 w where x y u v and w are measured in the same real world units e g mm 1 2 focal length parameters the normalized camera is unrealistic for one thing in a real camera there is no particular reason why the focal length should be one moreover the final position in the image is measured in pixels not physical distance so the model must take into account the photoreceptor spacing both of these factors have the effect of changing the mapping between points w u v w t in the world and their positions x x y t in the image plane by a constant scaling factor φ figure 5 so that φu x w φv y 2 w to add a further complication the spacing of the photoreceptors may differ in the x and y directions and so the scaling may be different in each direction giving the relations x φxu w y φy v 3 w where φx and φy are separate scaling factors for the x and y directions these parameters are known as the focal length parameters in the x and y directions 1 the pinhole camera figure 5 focal length and photoreceptor spacing a b changing the distance between the optical center and the image plane the focal length changes the relationship between the world point w u v w t and the image point x x y t in particular if we take the original focal length a and halve it b the image coordinate is also halved the field of view of the camera is the total angular range that is imaged usually different in the x and y directions when the focal length decreases the field of view increases c d the position in the image x x y t is usually measured in pixels hence the position x depends on the density of the receptors on the image plane if we take the original photoreceptor density c and halve it d then the image coordinate is also halved hence the photoreceptor spacing and focal length both change the mapping from rays to pixels in the same way but this name is somewhat misleading they account for not just the distance between the optical center and the principal point the true focal length but also the photoreceptor spacing 1 3 offset and skew parameters the model so far is still incomplete in that pixel position x 0 0 t is at the principal point where the w axis intersects the image plane in most imaging systems the pixel position x 0 0 t is at the top left of the image rather than the center to cope with this we add offset parameters δx and δy so that x φxu δ w x y φy v δ 4 w y where δx and δy are the offsets in pixels from the top left corner of the image to the position where the w axis strikes the image plane another way to think about this is that the vector δx δy t is the position of the principal point in pixels if the image plane is exactly centered on the w axis these offset parameters should be half the image size for a vga image δx and δy would be and respectively however in practice it is difficult and superfluous to manufacture cameras with the imaging sensor perfectly centered and so we treat the offset parameters as variable quantities we also introduce a skew term γ which moderates the projected position x as a function of the height v in the world this parameter has no clear physical interpretation but can help explain the projection of points into the image in practice the resulting camera model is x φxu γv δ w x y φy v δ 5 w y 1 4 position and orientation of camera finally we must account for the fact that the camera is not always conveniently centered at the origin of the world coordinate system with the optical axis exactly aligned with the w axis in general we may want to define an arbitrary world coordinate system that may be common to more than one camera to this end we express the world points w in the coordinate system of the camera before they are passed through the projection model using the coordinate transformation ui u τx wi w τz or wi ωw τ 7 where wi is the transformed point ω is a 3 3 rotation matrix and τ is a 3 1 translation vector 1 5 full pinhole camera model problem 1 problem 2 we are now in a position to describe the full camera model by combining equations 5 and 6 a point w u v w t is projected to a point x x y t by the relations x φx τx γ τy δ τz x y φy τy δ 8 ω32v τz y there are two sets of parameters in this model the intrinsic or camera parameters φx φy γ δx δy describe the camera itself and the extrinsic parameters ω τ describe the position and orientation of the camera in the world for reasons that will become clear in section 3 1 we will store the intrinsic parameters in the intrinsic matrix λ where φx γ δx λ 0 φy δy 0 0 1 9 we can now abbreviate the full projection model equations 8 by just writing x pinhole w λ ω τ 10 finally we must account for the fact that the estimated position of a feature in the image may differ from our predictions there are a number of reasons for this including noise in the sensor sampling issues and the fact that the detected position in the image may change at different viewpoints we model these factors with additive noise that is normally distributed with a spherical covariance to give the final relation p r x w λ ω τ normx pinhole w λ ω τ 11 where is the variance of the noise note that the pinhole camera is a generative model we are describing the likelihood p r x w λ ω τ of observing a image point x given a world point w and the parameters λ ω τ 1 6 radial distortion in the previous section we introduced the pinhole camera model however it has probably not escaped your attention that real world cameras are rarely based on the pinhole they have a lens or possibly a system of several lenses that collects light from a larger area and re focuses it on the image plane in practice this leads to a number of deviations from the pinhole model for example some parts of the image may be out of focus which essentially means that the assumption that a point in the world w maps to a single point in the image x is no longer valid there are more complex mathematical models for cameras that deal effectively with this situation but they are not discussed here figure 6 radial distortion the pinhole model is only an approximation of the true imaging process one important deviation from this model is a warping in which points deviate from their expected positions by moving along radial lines from the center of the image by an amount that depends on the distance from the center this is known as radial distortion a an image that suffers from radial distortion is easily spotted because lines that were straight in the world are mapped to curves in the image e g red dotted line b after applying the inverse radial distortion model straight lines in the world now correctly map to straight lines in the image the distortion caused the magenta point to move along the red radial line to the position of the yellow point however there is one deviation from the pinhole model that must be addressed radial distortion is a nonlinear warping of the image that depends on the distance from the center of the image in practice this occurs when the field of view of the lens system is large it can easily be detected in an image because straight lines in the world no longer project to straight lines in the image figure 6 radial distortion is commonly modeled as a polynomial function of the distance r from the center of the image in the normalized camera the final image positions xi yi are expressed as functions of the original positions x y by xi x 1 β1r2 yi y 1 β1r2 where the parameters and control the degree of distortion these relations describe a family of possible distortions that approximate the true distortion closely for most common lenses this distortion is implemented after perspective projection division by w but before the effect of the intrinsic parameters focal length offset etc so the warp ing is relative to the optical axis and not the origin of the pixel coordinate system we will not discuss radial distortion further in this volume however it is impor tant to realize that for accurate results all of the algorithms in this and chapters 15 and 16 should account for radial distortion when the field of view is large it is particularly critical to incorporate this into the pinhole camera model 2 three geometric problems figure 7 problem 1 learning extrinsic parameters exterior orientation problem given points wi i on a known object blue lines their po sitions xi i in the image circles on image plane and known intrinsic parameters λ find the rotation ω and translation τ relating the camera and the object a when the rotation or translation are wrong the image points predicted by the model where the rays strike the image plane do not agree well with the observed points xi b when the rotation and translation are correct they agree well and the likelihood p r xi w λ ω τ will be high 2 three geometric problems now that we have described the pinhole camera model we will consider three important geometric problems each is an instance of learning or inference within this model we will first describe the problems themselves and then tackle them one by one later in the chapter 2 1 problem 1 learning extrinsic parameters we aim to recover the position and orientation of the camera relative to a known scene this is sometimes known as the perspective n point pnp problem or the exterior orientation problem one common application is augmented reality where we need to know this relationship to render virtual objects that appear to be stable parts of the real scene the problem can be stated more formally as follows we are given a known object with i distinct points wi i their corresponding projections in the image xi i and known intrinsic parameters λ our goal is to estimate the rotation ω and translation τ that map points in the coordinate system of the object to points in the coordinate system of the camera so that ωˆ τˆ argmax ω τ i i 1 log p r xi wi λ ω τ 1 13 this is a maximum likelihood learning problem in which we aim to find pa rameters ω τ that make the predictions pinhole wi λ ω τ of the model agree with the observed points xi figure 7 the pinhole camera figure 8 problem 2 learning intrinsic parameters given a set of points wi i 1 on a known object in the world blue lines and the positions x i 1 of these points in an image find the intrinsic parameters λ to do this we must also simultaneously estimate the extrinsic parameters ω τ a when the intrinsic or extrinsic parameters are wrong the prediction of the pinhole camera where rays strike the image plane will deviate significantly from the observed points b when the intrinsic and extrinsic parameters are correct the prediction of the model will agree with the observed image 2 2 problem 2 learning intrinsic parameters we aim to estimate the intrinsic parameters λ that relate the direction of rays through the optical center to coordinates on the image plane this estimation process is known as calibration knowledge of the intrinsic parameters is critical if we want to use the camera to build models of the world the calibration problem can be stated more formally as follows given a known object with i distinct points wi i and their corresponding projections in the image xi i estimate the intrinsic parameters λˆ argmax λ max ω τ i i 1 log p r xi wi λ ω τ 11 once more this is a maximum likelihood learning problem in which we aim to find parameters λ ω τ that make the predictions of the model pinhole wi λ ω τ agree with the observed points xi figure 8 we do not particularly care about the extrinsic parameters ω τ finding these is just a means to the end of estimating the intrinsic parameters λ the calibration process requires a known object on which distinct points can be identified and their projections in the image found a common approach is to construct a bespoke calibration that achieves these goals figure 9 should be noted that in practice calibration is more usually based on a number of views of a known planar object see section 15 4 2 figure 9 camera calibration tar get one way to calibrate the cam era estimate its intrinsic parameters is to view a object a camera cal ibration target for which the geome try is known the marks on the sur face are at known positions in the frame of reference of the object and are easy to locate in the image us ing basic image processing techniques it is now possible to find the intrin sic and extrinsic parameters that opti mally map the known positions to their projections in the image im age from hartley zisserman figure 10 problem 3 inferring world points given two cameras with known position and orientation and the projections x1 and x2 of the same point in each image the goal of calibrated stereo reconstruction is to infer the position w of the world point a when the estimate of the world point red circle is wrong the predictions of the pinhole camera models where rays strike the image plane will deviate from the observed data brown circles on image plane b when the estimate of w is correct the predictions of the model agree with the observed data 2 3 problem 3 inferring world points we aim to estimate the position of a point w in the scene given its projections xj j in j 2 calibrated cameras when j 2 this is known as calibrated stereo reconstruction with j 2 calibrated cameras it is known to as multi view reconstruction if we repeat this process for many points the result is a sparse point cloud this could be used to help an autonomous vehicle navigate through the environment or to generate an image of the scene from a new viewpoint more formally the multi view reconstruction problem can be stated as fol lows given j calibrated cameras in known positions i e cameras with known λ ω τ viewing the same point w and knowing the corresponding projec tions xj j in the j images establish the position w of the point in the figure 11 geometric interpreta tion of homogeneous coordinates the different scalar multiples λ of the ho mogeneous 3 vector x define a ray through the origin of a coordinate space the corresponding image point x can be found by considering the point that this ray strikes on the plane at z 1 an interest ing side effect of this representation is that it is possible to represent points at infinity known as ideal points for example the homogeneous coordinate 0 1 0 t defines a ray that is parallel to z 1 and so never intersects the plane it represents the point at infin ity in direction 0 1 t world j wˆ argmax log p r xj w λj ωj τ j 15 the form of this inference problem is similar to that of the preceding learning problems we perform an optimization in which we manipulate the variable of in terest w until the predictions pinhole w λj ωj τ j of the pinhole camera models agree with the data xj figure 10 for obvious reasons the principle behind reconstruction is known as triangulation 2 4 solving the problems we have introduced three geometric problems each of which took the form of a learning or inference problem using the pinhole camera model we formulated each in terms of maximum likelihood estimation and in each case this results in an optimization problem unfortunately none of the resulting objective functions can be optimized in closed form each solution requires the use of nonlinear optimization in each case it is critical to have a good initial estimate of the unknown quantities to ensure that the optimization process converges to the global maximum in the remaining part of this chapter we develop algorithms that provide these initial estimates the general approach is to choose new objective functions that can be optimized in closed form and where the solution is close to the solution of the true problem 3 homogeneous coordinates 3 homogeneous coordinates to get good initial estimates of the geometric quantities in the preceding opti mization problems we play a simple trick we change the representation of both the image points and world points so that the projection equations become linear after this change it is possible to find solutions for the unknown quanti ties in closed form however it should be emphasized that these solutions do not directly address the original optimization criteria they minimize more abstract objective functions based on algebraic error whose solutions are not guaranteed to be the same as those for the original problem however they are generally close enough to provide a good starting point for a nonlinear optimization of the true cost function we convert the original cartesian representation of the image points x to a homogeneous coordinate x so that x where λ is an arbitrary scaling factor this is a redundant representation in that any scalar multiple λ represents the same point for example the homogeneous problem 4 problem 14 5 problem 14 6 vectors x 2 4 2 t and x 3 6 3 t both represent the cartesian point x 1 2 t where scaling factors λ 2 and λ 3 have been used respectively converting between homogeneous and cartesian coordinates is easy to move to homogeneous coordinates we choose λ 1 and simply append a 1 to the original cartesian coordinate to recover the cartesian coordinates we divide the first two entries of the homogeneous 3 vector by the third so that if we observe the homogeneous vector x x y z t then we can recover the cartesian coordinate x x y t as part i image formation and image models there are many types of imaging devices from animal eyes to video cameras and radio tele scopes they may or may not be equipped with lenses for example the first models of the camera obscura literally dark chamber invented in the century did not have lenses but instead used a pinhole to focus light rays onto a wall or translucent plate and demonstrate the laws of perspective discovered a century earlier by brunelleschi pinholes were replaced by more and more sophisticated lenses as early as and the modern photographic or digital camera is essentially a camera obscura capable of recording the amount of light striking every small area of its backplane figure figure image formation on the backplate of a photographic camera figure from us navy manual of basic optics and optical instruments prepared by the bureau of naval personnel reprinted by dover publications inc cameras chap the imaging surface of a camera is generally a rectangle but the shape of the human retina is much closer to a spherical surface and panoramic cameras may be equipped with cylindrical retinas imaging sensors have other characteristics they may record a spatially discrete picture like our eyes with their rods and cones mm cameras with their grain and digital cameras with their rectangular picture elements or pixels or a continuous one in the case of old fashioned tv tubes for example the signal that an imaging sensor records at a point on its retina may be discrete or continuous and it may consist of a single number black and white camera a few values e g the r g b intensities for a color camera or the responses of the three types of cones for the human eye many numbers e g the responses of hyperspectral sensors or even a continuous function of wavelength which is essentially the case for spectrometers examining these characteristics is the subject of this chapter pinhole cameras perspective projection imagine taking a box pricking a small hole in one of its sides with a pin and then replacing the opposite side with a translucent plate if you hold that box in front of you in a dimly lit room with the pinhole facing some light source say a candle you see an inverted image of the candle appearing on the translucent plate figure this image is formed by light rays issued from the scene facing the box if the pinhole were really reduced to a point which is of course physically impossible exactly one light ray would pass through each point in the plane of the plate or image plane the pinhole and some scene point in reality the pinhole has a finite albeit small size and each point in the image plane collects light from a cone of rays subtending a finite solid angle so this idealized and extremely simple model of the imaging geometry does not strictly apply in addition real cameras are normally equipped with lenses which further complicates things still the pinhole perspective also called central perspective projection model first proposed by brunelleschi at the beginning of the century is mathematically convenient despite its simplicity it often provides an acceptable approximation of the imaging process perspective projection creates inverted images and it is sometimes convenient to consider instead a virtual image associated with a plane lying in front of the pinhole at the same distance from it as the actual image plane figure this virtual image is not inverted but is otherwise strictly equivalent to the actual one depending on the context it may be more convenient to think about one or the other figure a illustrates an obvious effect of perspective projection the apparent size of objects depends on their distance for example the images and c of the posts b and c have the same height but a and c image plane figure the pinhole imaging model sec pinhole cameras a b figure perspective effects a far objects appear smaller than close ones the distance d from the pinhole o to the plane containing c is half the distance from o to the plane containing a and b b the images of parallel lines intersect at the horizon after hilbert and cohn vossen figure note that the image plane is behind the pinhole in a physical retina and in front of it in b virtual image plane most of the diagrams in this chapter and the rest of this book feature the physical image plane but a virtual one is also used when appropriate as in b are really half the size of b figure b illustrates another well known effect the projections of two parallel lines lying in some plane it appear to converge on a horizon line h formed by the intersection of the image plane with the plane parallel to it and passing through the pinhole note that the line l in it that is parallel to the image plane has no image at all these properties are easy to prove in a purely geometric fashion however it is often convenient if not quite as elegant to reason in terms of reference frames coordinates and equations consider for example a coordinate system o i j k attached to a pinhole camera whose origin o coincides with the pinhole and vectors i and j form a basis for a vector plane parallel to the image plane which is located at a positive distance f from the pinhole along the vector k figure the line perpendicular to and passing through the pinhole is called the optical axis and the point c where it pierces is called the image center this point can be used as the origin of an image plane coordinate frame and it plays an important role in camera calibration procedures let p denote a scene point with coordinates x y z and denote its image with coordi nates x since lies in the image plane we have f since the three points p o cameras chap figure the perspective projection equations are derived in this section from the collinearity of the point p its image and the pinhole o and are collinear we have o p λ o p for some number λ so and therefore x λx f λz x λ x x f y z affine projection x f z f y z as noted in the previous section pinhole perspective is only an approximation of the geometry of the imaging process this section discusses a class of coarser approximations called affine projection models that are also useful on occasion we focus on two specific affine models namely weak perspective and orthographic projections a third one the paraperspective model is introduced in chapter where the name affine projection is also justified consider the fronto parallel plane defined by z figure for any point p in we can rewrite the perspective projection eq as x mx f where m physical constraints impose that be negative the plane must be in front of the pinhole so the magnification m associated with the plane is positive this name is justified by the following remark consider two points p and q in and their images and figure obviously the vectors p q and p are parallel and we have p m p q this is the dependence of image size on object distance noted earlier when the scene depth is small relative to the average distance from the camera the mag nification can be taken to be constant this projection model is called weak perspective or scaled sec cameras with lenses figure weak perspective projection all line segments in the plane are projected with the same magnification figure orthographic projection unlike other geometric models of the image formation process orthographic projection does not involve a reversal of image features accordingly the magnification is taken to be negative which is a bit unnatural but simplifies the projection equations orthography when it is a priori known that the camera always remains at a roughly constant distance from the scene we can go further and normalize the image coordinates so that m this is orthographic projection defined by x x y with all light rays parallel to the k axis and orthogonal to the image plane figure although weak perspective projection is an acceptable model for many imaging conditions as suming pure orthographic projection is usually unrealistic cameras with lenses most cameras are equipped with lenses there are two main reasons for this the first one is to gather light since a single ray of light would otherwise reach each point in the image plane under ideal pinhole projection real pinholes have a finite size of course so each point in the image cameras chap figure reflection and refraction at the interface between two homoge neous media with indexes of refraction and plane is illuminated by a cone of light rays subtending a finite solid angle the larger the hole the wider the cone and the brighter the image but a large pinhole gives blurry pictures shrinking the pinhole produces sharper images but reduces the amount of light reaching the image plane and may introduce diffraction effects the second main reason for using a lens is to keep the picture in sharp focus while gathering light from a large area ignoring diffraction interferences and other physical optics phenomena the behavior of lenses is dictated by the laws of geometric optics figure light travels in straight lines light rays in homogeneous media when a ray is reflected from a surface this ray its reflec tion and the surface normal are coplanar and the angles between the normal and the two rays are complementary and when a ray passes from one medium to another it is refracted i e its direction changes according to snell law if is the ray incident to the interface between two transparent materials with indexes of refraction and and is the refracted ray then and the normal to the interface are coplanar and the angles and between the normal and the two rays are related by sin sin in this chapter we only consider the effects of refraction and ignore those of reflection in other words we concentrate on lenses as opposed to catadioptric optical systems e g tele scopes that may include both reflective mirrors and refractive elements tracing light rays as they travel through a lens is simpler when the angles between these rays and the refracting surfaces of the lens are assumed to be small the next section discusses this case paraxial geometric optics in this section we consider paraxial or first order geometric optics where the angles between all light rays going through a lens and the normal to the refractive surfaces of the lens are small in addition we assume that the lens is rotationally symmetric about a straight line called its optical axis and that all refractive surfaces are spherical the symmetry of this setup allows us to determine the projection geometry by considering lenses with circular boundaries lying in a plane that contains the optical axis let us consider an incident light ray passing through a point on the optical axis and refracted at the point p of the circular interface of radius r separating two transparent media with indexes of refraction and figure let us also denote by the point where the refracted ray intersects the optical axis a second time the roles of and are completely symmetric and by c the center of the circular interface sec cameras with lenses figure paraxial refraction a light ray passing through the point is re fracted at the point p where it intersects a circular interface the refracted ray intersects the optical axis in the center of the interface is at the point c of the optical axis and its radius is r the angles and are all assumed to be small let and respectively denote the angles between the two rays and the chord joining c to p if resp is the angle between the optical axis and the line joining resp to p the angle between the optical axis and the line joining c to p is as shown by figure γ now let h denote the distance between p and the optical axis and r the radius of the circular interface if we assume all angles are small and thus to first order equal to their sines and tangents we have α γ β h i and α γ β h i writing snell law for small angles yields the paraxial refraction equation n α n α r note that the relationship between and depends on r and but not on or this is the main simplification introduced by the paraxial assumption it is easy to see that eq remains valid when some or all of the values of and r become negative corresponding to the points or c switching sides of course real lenses are bounded by at least two refractive surfaces the corresponding ray paths can be constructed iteratively using the paraxial refraction equation the next section illustrates this idea in the case of thin lenses thin lenses let us now consider a lens with two spherical surfaces of radius r and index of refraction n we assume that this lens is surrounded by vacuum or to an excellent approximation by air with an index of refraction equal to and that it is thin i e that a ray entering the lens and refracted at its right boundary is immediately refracted again at the left boundary consider a point p located at negative depth z off the optical axis and denote by po the ray passing through this point and the center o of the lens figure as shown in the exercises it follows from snell law and eq that the ray po is not refracted and that all other rays passing through p are focused by the thin lens on the point with depth along cameras chap figure a thin lens rays passing through the point o are not refracted rays parallel to the optical axis are focused on the focal point f po such that where f r n z is the focal length of the lens f pinhole perspective projection if we take f since p and lie on a ray passing through the center of the lens but that points located at a distance z from o are only in sharp focus when the image plane is located at a distance from o on the other side of the lens that satisfies eq i e the thin lens equation letting z shows that f is the distance between the center of the lens and the plane where objects such as stars which are effectively located at z focus the two points f and f located at distance f from the lens center on the optical axis are called the focal points of the lens in practice objects within some range of distances called depth of field or depth of focus are in acceptable focus as shown in the exercises the depth of field increases with the f number of the lens i e the ratio between the focal length of the lens and its diameter the field of view of a camera is the portion of scene space that actually projects onto the retina of the camera it is not defined by the focal length alone but also depends on the effective area of the retina e g the area of film that can be exposed in a photographic camera or the area of the ccd sensor in a digital camera figure figure the field of view of a camera is where φ def arctan d d is the diameter of the sensor film or ccd chip and f is the focal length of the camera when f is much shorter than d we have a wide angle lens with rays that can be off the optical axis by more than telephoto lenses have a small field of view and produce pictures closer to affine ones sec cameras with lenses y p figure a simple thick lens with two spherical surfaces real lenses a more realistic model of simple optical systems is the thick lens the equations describing its behavior are easily derived from the paraxial refraction equation and they are the same as the pinhole perspective and thin lens projection equations except for an offset figure if h and h denote the principal points of the lens then eq holds when z resp is the distance between p resp and the plane perpendicular to the optical axis and passing through h resp h in this case the only undeflected ray is along the optical axis simple lenses suffer from a number of aberrations to understand why let us remember first that the paraxial refraction eq is only an approximation valid when the angle α between each ray along the optical path and the optical axis of the length is small and sin α α for larger angles a third order taylor expansion of the sine function yields the following refinement of the paraxial equation i i r r r here h denotes as in figure the distance between the optical axis and the point where the incident ray intersects the interface in particular rays striking the interface farther from the optical axis are focused closer to the interface the same phenomenon occurs for a lens and it is the source of two types of spherical aberrations figure a consider a point p on the optical axis and its paraxial image the distance between and the intersection of the optical axis with a ray issued from p and refracted by the lens is called the longitudinal spherical aberration of that ray note that if an image plane were erected in p the ray would intersect this plane at some distance from the axis called the transverse spherical aberration of that ray together all rays passing through p and refracted by the lens form a circle of confusion centered in p as they intersect the size of that circle changes when we move along the optical axis the circle with minimum diameter is called the circle of least confusion and it is not in general located in besides spherical aberration there are four other types of primary aberrations caused by the differences between first and third order optics namely coma astigmatism field curvature and distortion a precise definition of these aberrations is beyond the scope of this book suffice to say that like spherical aberration they degrade the image by blurring the picture of every object point distortion plays a different role and changes the shape of the image as a whole figure b this effect is due to the fact that different areas of a lens have slightly different focal lengths the aberrations mentioned so far are monochromatic i e they are independent of the response of the lens to various wavelengths however the index of refraction of a transparent cameras chap a b c figure aberrations a spherical aberration the grey region is the parax ial zone where the rays issued from p intersect at its paraxial image if an image plane is erected in the image of in that plane forms a circle of confusion of diameter the focus plane yielding the circle of least confusion is indicated by a dashed line b distortion from left to right the nominal im age of a fronto parallel square pincushion distortion and barrel distortion c chromatic aberration the index of refraction of a transparent medium depends on the wavelength or color of the incident light rays here a prism decomposes white light into a palette of colors figure from us navy manual of basic optics and optical instruments prepared by the bureau of naval per sonnel reprinted by dover publications inc medium depends on wavelength figure c and it follows from the thin lens eq that the focal length depends on wavelength as well this causes the phenomenon of chromatic aberration refracted rays corresponding to different wavelengths intersect the optical axis at different points longitudinal chromatic aberration and form different circles of confusion in the same image plane transverse chromatic aberration aberrations can be minimized by aligning several simple lenses with well chosen shapes and refraction indexes separated by appropriate stops these compound lenses can still be mod eled by the thick lens equations they suffer from one more defect relevant to machine vision light beams emanating from object points located off axis are partially blocked by the various apertures including the individual lens components positioned inside the lens to limit aberra tions figure this phenomenon called vignetting causes the brightness to drop in the image periphery vignetting may pose problems to automated image analysis programs but it is not as important in photography thanks to the human eye remarkable insensitivity to smooth sec the human eye figure vignetting effect in a two lens system the shaded part of the beam never reaches the second lens additional apertures and stops in a lens further contribute to vignetting brightness gradients speaking of which it is time to look at this extraordinary organ in a bit more detail the human eye here we give a brief overview of the anatomical structure of the eye it is largely based on the presentation in wandell and the interested reader is invited to read this excellent book for more details figure left is a sketch of the section of an eyeball through its vertical plane of symmetry showing the main elements of the eye the iris and the pupil which control the amount of light penetrating the eyeball the cornea and the crystalline lens which together refract the light to create the retinal image and finally the retina where the image is formed despite its globular shape the human eyeball is functionally similar to a camera with a field of view covering a width height area like any other optical system it suffers nerve sclera retina ciliary body and sheath macula vitreous cavity optic axis cornea f f lutea visual axis iris fovea lens chorioid figure left the main components of the human eye reproduced with permission the american society for photogrammetry and remote sensing a l nowicki stereoscopy manual of photogrammetry edited by m m thompson r c eller w a radlinski and j l speert third edition pp bethesda american society of photogrammetry right helmoltz schematic eye as modified by laurance after driscoll and vaughan the distance between the pole of the cornea and the anterior principal plane is mm and the radii of the cornea anterior and posterior surfaces of the lens are respectively mm mm and mm cameras chap from various types of geometric and chromatic aberrations several models of the eye obeying the laws of first order geometric optics have been proposed and figure right shows one of them helmoltz schematic eye there are only three refractive surfaces with an infinitely thin cornea and a homogeneous lens the constants given in figure are for the eye focusing at infinity unaccommodated eye this model is of course only an approximation of the real optical characteristics of the eye let us have a second look at the components of the eye one layer at a time the cornea is a transparent highly curved refractive window through which light enters the eye before being partially blocked by the colored and opaque surface of the iris the pupil is an opening at the center of the iris whose diameter varies from about to mm in response to illumination changes dilating in low light to increase the amount of energy that reaches the retina and contracting in normal lighting conditions to limit the amount of image blurring due to spherical aberration in the eye the refracting power reciprocal of the focal length of the eye is in large part an effect of refraction at the the air cornea interface and it is fine tuned by deformations of the crystalline lens that accommodates to bring objects into sharp focus in healthy adults it varies between unaccommodated case and diopters diopter m corresponding to a range of focal lengths between and mm the retina itself is a thin layered membrane populated by two types of photoreceptors rods and cones that respond to light in the to nm wavelength range violet to red as mentioned in chapter there are three types of cones with different spectral sensitivities and these play a key role in the perception of color there are about million rods and million cones in a human eye their spatial distribution varies across the retina the macula lutea is a region in the center of the retina where the concentration of cones is particularly high and images are sharply focused whenever the eye fixes its attention on an object figure the highest concentration of cones occurs in the fovea a depression in the middle of the macula lutea where it peaks at with the centers of two neighboring cones separated by only half a minute of visual angle figure conversely there are no rods in the center of the fovea but the rod density increases toward the periphery of the visual field there is also a blind spot on the retina where the ganglion cell axons exit the retina and form the optic nerve the rods are extremely sensitive photoreceptors they are capable of responding to a single photon but they yield relatively poor spatial detail despite their high number because many rods converge to the same neuron within the retina in contrast cones become active at higher light angle relative to fovea deg figure the distribution of rods and cones across the retina reprinted from foundations of vision by b wandell sinauer associates inc qc sinauer associates inc sec sensing levels but the signal output by each cone in the fovea is encoded by several neurons yielding a high resolution in that area more generally the area of the retina influencing a neuron response is traditionally called its receptive field although this term now also characterizes the actual electrical response of neurons to light patterns of course much more could and should be said about the human eye for example how our two eyes verge and fixate on targets cooperate in stereo vision and so on besides vision only starts with this camera of our mind which leads to the fascinating and still largely unsolved problem of deciphering the role of the various portions of our brain in human vision we come back to various aspects of this endeavor later in this book sensing what differentiates a camera in the modern sense of the world from the portable camera obscura of the century is its ability to record the pictures that form on its backplane although it had been known since at least the middle ages that certain silver salts rapidly darken under the action of sunlight it was only in that niepce obtained the first true photographs by exposing paper treated with silver chloride to the light rays striking the image plane of a camera obscura then fixing the picture with nitric acid these first images were negatives and niepce soon switched to other photosensitive chemicals to obtain positive pictures the earliest photographs have been lost and the first one to have been preserved is la table servie the set table reproduced in figure niepce invented photography but daguerre would be the one to popularize it after the two became associates in daguerre went on to develop his own photographic process using mercury fumes to amplify and reveal the latent image formed on an iodized plating of silver on copper daguerre otypes were an instant success when arago presented daguerre process at the french academy of sciences in three years after niepce death other milestones in the long history of photography include the introduction of the wet plate negative positive process by legray and archer in which required the pictures to be developed on the spot but produced excellent negatives the invention of the gelatin process by maddox in which eliminated the need for immediate development the introduction in of the photographic film that has replaced glass plates in most modern applications by eastman and the invention by the lumie re brothers of cinema in and color photography in figure the first photograph on record la table servie obtained by nice phore niepce in collection harlinge viollet cameras chap figure a ccd device the invention of television in the by people like baird farnsworth and zworykin was of course a major impetus for the development of electronic sensors the vidicon is a com mon type of tv vacuum tube it is a glass envelope with an electron gun at one end and a faceplate at the other the back of the faceplate is coated with a thin layer of photoconductor material laid over a transparent film of positively charged metal this double coating forms the target the tube is surrounded by focusing and deflecting coils that are used to repeatedly scan the target with the electron beam generated by the gun this beam deposits a layer of electrons on the target to balance its positive charge when a small area of the faceplate is struck by light electrons flow through locally depleting the charge of the target as the electron beam scans this area it replaces the lost electrons creating a current proportional to the incident light intensity the current variations are then transformed into a video signal by the vidicon circuitry ccd cameras let us now turn to charge coupled device ccd cameras that were proposed in and have replaced vidicon cameras in most modern applications from consumer camcorders to special purpose cameras geared toward microscopy or astronomy applications a ccd sensor uses a rectangular grid of electron collection sites laid over a thin silicon wafer to record a measure of the amount of light energy reaching each of them figure each site is formed by growing a layer of silicon dioxide on the wafer and then depositing a conductive gate structure over the dioxide when photons strike the silicon electron hole pairs are generated photo conversion and the electron are captured by the potential well formed by applying a positive electrical po tential to the corresponding gate the electrons generated at each site are collected over a fixed period of time t at this point the charges stored at the individual sites are moved using charge coupling charge packets are transfered from site to site by manipulating the gate potentials preserving the separation of the packets the image is read out of the ccd one row at a time each row being transfered in parallel to a serial output register with one element in each column between two row reads the register transfers its charges one at a time to an output amplifier that generates a signal proportional to the charge it receives this process continues until the entire image has sec sensing been read out it can be repeated times per second tv rate for video applications or at a much slower pace leaving ample time seconds minutes even hours for electron collection in low light level applications such as astronomy it should be noted that the digital output of most ccd cameras is transformed internally into an analog video signal before being passed to a frame grabber that constructs the final digital image consumer grade color ccd cameras essentially use the same chips as black and white cameras except that successive rows or columns of sensors are made sensitive to red green or blue light often using a filter coating that blocks the complementary light other filter patterns are possible including mosaics of blocks formed by two green one red and one blue receptors bayer patterns the spatial resolution of single ccd cameras is of course limited and higher quality cameras use a beam splitter to ship the image to three different ccds via color filters the individual color channels are then either digitized separately rgb output or combined into a composite color video signal ntsc output in the united states secam or pal in europe and japan or into a component video format separating color and brightness information sensor models for simplicity we restrict our attention in this section to black and white ccd cameras color cameras can be treated in a similar fashion by considering each color channel separately and taking the effect of the associated filter response explicitly into account the number i of electrons recorded at the cell located at row r and column c of a ccd array can be modeled as i r c t e p λ r p q λ dp dλ λ p s r c where t is the electron collection time and the integral is calculated over the spatial domain s r c of the cell and the range of wavelengths to which the ccd has a nonzero response in this integral e is is the power per unit area and unit wavelength i e the irradiance see chapter for a formal definition arriving at the point p r is the spatial response of the site and q is the quantum efficiency of the device i e the number of electrons generated per unit of incident light energy in general both e and q depend on the light wavelength λ and e and r depend on the point location p within s r c the output amplifier of the ccd transforms the charge collected at each site into a mea surable voltage in most cameras this voltage is then transformed into a low pass video signal by the camera electronics with a magnitude proportional to i the analog image can be once again transformed into a digital one using a frame grabber that spatially samples the video signal and quantizes the brightness value at each image point or pixel from picture element there are several physical phenomena that alter the ideal camera model presented earlier blooming occurs when the light source illuminating a collection site is so bright that the charge stored at that site overflows into adjacent ones it can be avoided by controlling the illumination but other factors such as fabrication defects thermal and quantum effects and quantization noise are inherent to the imaging process as shown next these factors are appropriately captured by simple statistical models quantum physics effects introduce an inherent uncertainty in the photoconversion process at each site shot noise more precisely the number of electrons generated by this process can be modeled by a random integer variable ni r c obeying a poisson distribution with mean β r c i r c where β r c is a number between and that reflects the variation of the spatial response and quantum efficiency across the image and also accounts for bad pixels electrons is roughly speaking spatially or temporally averaged more on this later cameras chap freed from the silicon by thermal energy add to the charge of each collection site their con tribution is called dark current and it can be modeled by a random integer variable ndc r c whose mean µdc r c increases with temperature the effect of dark current can be controlled by cooling down the camera additional electrons are introduced by the ccd electronics bias and their number can also be modeled by a poisson distributed random variable nb r c with mean µb r c the output amplifier adds read out noise that can be modeled by a real valued random variable r obeying a gaussian distribution with mean µr and standard deviation σr there are other sources of uncertainty e g charge transfer efficiency but they can often be neglected finally the discretization of the analog voltage by the frame grabber introduces both geometric effects line jitter which can be corrected via calibration and a quantization noise which can be modeled as a zero mean random variable q r c with a uniform distribution in the δ interval and a variance of where δ is the quantization step this yields the following composite model for the digital signal d r c d r c γ ni r c ndc r c nb r c r r c q r c in this equation γ is the combined gain of the amplifier and camera circuitry the statistical properties of this model can be estimated via radiometric camera calibration for example dark current can be estimated by taking a number of sample pictures in a dark environment i notes the classical textbook by hecht is an excellent introduction to geometric optics it in cludes a detailed discussion of paraxial optics as well as the various aberrations briefly men tioned in this chapter see also driscoll and vaughan vignetting is discussed in horn and russ wandell gives an excellent treatment of image formation in table reference card camera models problems the human visual system the helmoltz schematic model of the eye is detailed in driscoll and vaughan ccd devices were introduced in boyle and smith and amelio et al sci entific applications of ccd cameras to microscopy and astronomy are discussed in aiken et al janesick et al snyder et al and tyson the statistical sensor model presented in this chapter is based on snyder et al with an additional term for the quantization noise taken from healey and kondepudy these two articles contain inter esting applications of sensor modeling to image restoration in astronomy and radiometric camera calibration in machine vision given the fundamental importance of the notions introduced in this chapter the main equa tions derived in its course have been collected in table for reference problems derive the perspective equation projections for a virtual image located at a distance f in front of the pinhole prove geometrically that the projections of two parallel lines lying in some plane it appear to converge on a horizon line h formed by the intersection of the image plane with the plane parallel to it and passing through the pinhole prove the same result algebraically using the perspective projection eq you can assume for simplicity that the plane it is orthogonal to the image plane use snell law to show that rays passing through the optical center of a thin lens are not refracted and derive the thin lens equation hint consider a ray passing through the point p and construct the rays and obtained respectively by the refraction of by the right boundary of the lens and the refraction of by its left boundary consider a camera equipped with a thin lens with its image plane at position and the plane of scene points in focus at position z now suppose that the image plane is moved to show that the diameter of the corresponding blur circle is d where d is the lens diameter use this result to show that the depth of field i e the distance between the near and far planes that will keep the diameter of the blur circles below some threshold ε is given by d d z z f f and conclude that for a fixed focal length the depth of field increases as the lens diameter decreases and thus the f number increases hint solve for the depth zˆ of a point whose image is focused on the image plane at position considering both the case where is larger than and the case where it is smaller give a geometric construction of the image of a point p given the two focal points f and f of a thin lens derive the thick lens equations in the case where both spherical boundaries of the lens have the same radius surfaces are bright or dark for two main reasons their albedo and the amount of light they are re ceiving a model of how the brightness of a surface is obtained is usually called a shading model shading models are important because with an appropriate shading model we can interpret pixel values if the right shading model applies it is possible to reconstruct objects and their albedos using just a few images furthermore we can interpret shadows and explain their puzzling and seldom noticed absence in most indoor scenes qualitative radiometry we should like to know how bright surfaces are going to be under various lighting conditions and how this brightness depends on local surface properties on surface shape and on illumi nation as we saw in chapter foreshortening means that different sources can have the same effect on a surface the most powerful tool for analyzing this problem is to think about what a source looks like from the surface this qualitative radiometry is one of these tricks that looks unsophisticated no hard math but is extremely powerful in some cases this technique gives qualitative descriptions of brightness without even knowing what the term means recall from section and figure that a surface patch sees the world through a hemisphere of directions at that patch the radiation arriving at the surface along a particular direction passes through a point on the hemisphere if two surface patches have equivalent in coming hemispheres they must have the same incoming radiation whatever the outside world looks like this means that any difference in brightness between patches with the same incom ing hemisphere is a result of different surface properties in particular if two surface patches with the same brdf see the same incoming hemisphere then the radiation they output must be the same sec sources and their effects a b c figure a geometry in which a qualitative radiometric solution can be ob tained by thinking about what the world looks like from the point of view of a patch we wish to know what the brightness looks like at the base of two dif ferent infinitely high walls in this geometry an infinitely high matte black wall cuts off the view of the overcast sky which is a hemisphere of infinite radius and uniform brightness on the right we show a representation of the direc tions that see or do not see the source at the corresponding points obtained by flattening the hemisphere to a circle of directions or equivalently by viewing it from above since each point has the same input hemisphere the brightness must be uniform lambert determined the distribution of brightness on a uniform plane at the base of an infinitely high black wall illuminated by an overcast sky see figure in this case every point on the plane must see the same hemisphere half of its viewing sphere is cut off by the wall and the other half contains the sky which is uniform and the plane is uniform so every point must have the same brightness a second example is somewhat trickier we now have an infinitely thin black wall that is infinitely long in only one direction and on an infinite plane figure a qualitative description would be to find what the curves of equal brightness look like it is fairly easy to see that all points on any line passing through the point p in figure see the same input hemisphere and so must have the same brightness furthermore the distribution of brightness on the plane must have a symmetry about the line of the wall we expect the brightest points to be along the extension of the line of the wall and the darkest to be at the base of the wall sources and their effects radiometric properties of light sources we define a light source to be anything that emits light that is internally generated i e not just reflected to describe a source we need a description of the radiance it emits in each direction typically internally generated radiance is dealt with separately from reflected radiance this is because although a source may reflect light the light it reflects depends on the environment whereas the light it generates internally usually does not sources shadows and shading chap darker figure we now have a matte black infinitely thin half infinite wall on an infinite white plane shown on the left this geometry also sees an overcast sky of infinite radius and uniform brightness in the text we show how to deter mine the curves of similar brightness on the plane these curves are shown on the right depicted on an overhead view of the plane the thick line represents the wall superimposed on these curves is a representation of the input hemisphere for some of these isophotes along these curves the hemisphere is fixed by a geometrical argument but they change as one moves from curve to curve we seldom need a complete description of the radiance a source emits in each direction it is more usual to model sources as emitting a constant radiance in each direction possibly with a family of directions zeroed like a spotlight the proper quantity in this case is the exitance defined as the internally generated energy radiated per unit time and per unit area on the radiating surface exitance is similar to radiosity and can be computed as e p le p θo φo cos θo dω together with a description of the exitance we need a description of the geometry of the source which has profound effects on the spatial variation of light around the source and on the shadows cast by objects near the source sources are usually modeled with quite simple geometries for two reasons first many synthetic sources can be modeled as point sources line sources or area sources fairly effectively second sources with simple geometries can still yield surprisingly complex effects point sources a common approximation is to assume that the light source is an extremely small sphere in fact a point such a source is known as a point source it is a natural model to use because many sources are physically small compared with the environment in which they stand we can obtain sec sources and their effects approx ti e d radius e d e constant radiance patch due to source figure a surface patch sees a distant sphere of small radius the sphere produces a small illuminated patch on the input hemisphere of the sphere in the text by reasoning about the scaling behavior of this patch as the distant sphere moves farther away or gets bigger we obtain an expression for the behavior of the point source a model for the effects of a point source by modeling the source as a very small sphere that emits light at each point on the sphere with an exitance constant over the sphere assume that a surface patch is viewing a sphere of radius e at a distance r away and that e r figure the assumption that the sphere is far away from the patch relative to its radius almost always applies for real sources now the solid angle that the source subtends is this behaves approximately proportional to r the pattern of illumination that the source creates on the hemisphere roughly scales too as the sphere moves away the rays leaving the surface patch and striking the sphere move closer to gether roughly evenly and the collection changes only slightly a small set of new rays is added at the rim the contribution from these rays must be small because they come from directions tangent to the sphere in the limit as e tends to zero no new rays are added the radiosity due to the source is obtained by integrating the pattern generated by the source times cos θi over the patch of solid angle as e tends to zero the patch shrinks and the cos θi is close to constant if ρ is the surface albedo all this means the expression for radiosity due to the point source is sources shadows and shading chap e ρ r e cos θ where e is a term in the exitance of the source integrated over the small patch we do not need a more detailed expression for e to determine one we would need to actually do the integral we have shirked a nearby point source the angle term can be written in terms of n p the unit normal to the surface and s p a vector from p to the source whose length is to yield the standard model of a nearby point source ρ p n p s p d r p this is an extremely convenient model because it gives an explicit relationship between radiosity and shape the normal term in this model s is usually called the source vector it is common and incorrect to omit the dependency on distance to the source from this model a point source at infinity the sun is far away as a result the terms r p and s p are essentially constant in this case the point source is referred to as being a point source at infinity if all the surface patches we are interested in are close together with respect to the distance to the source r p f r p where f r p furthermore s p f s p where f s p we now have n s p n f s p n r p f r p now both and are constants and there is no particular point in keeping them explicitly in the expression instead we can write s r and our model for the radiosity due to a point source at infinity becomes b p ρd p n s the term s is again known as the source vector typically this model is used by inferring a source vector that is appropriate rather than by computing its value from the exitance of the source and the geometry choosing a point source model a point source at infinity is a good model for the sun for example because the solid angle that the sun subtends is small and essentially constant wherever it appears in the field of view this test means that our approximation step is valid if we use linear sensors with an unknown gain for example we can roll the source intensity and the unknown gain into the source vector term this is quite often done without comment as you should expect from the derivation a point source at infinity is a poor model when the distance between objects is similar in magnitude to the distance to the source in this case we cannot use the series approximation to pretend that the radiosity due to the source does not go down with distance to the source the heart of the problem is easy to see if we consider what the source looks like from different surface patches it must look bigger to nearer surface patches however small its radius this means that the radiosity due to the source must go up if the source is sufficiently distant for example the sun we can ignore this effect because the source does not change in apparent size for any plausible motion however for configurations like a light bulb in the center of a room the solid angle sub tended by the source goes up as the inverse square of the distance meaning that the radiosity sec sources and their effects due to the source will do so too the correct model to use in this case is the point source of section the difficulty with this model is that radiosity changes sharply over space in a way that is inconsistent with experience for example if a point source is placed at the center of a cube then the the model predicts that radiosity in the corners is roughly one third that at the center of each face but the corners of real rooms are nowhere near as dark as that it is quite common in practice to suppress the distance term in the nearby point source model to account for this an activity that is radiometrically incorrect but that tends to yield a better model the explanation of this apparent contradiction must wait until we have discussed shading models line sources a line source has the geometry of a line a good example is a single fluorescent light bulb line sources are not terribly common in natural scenes or in synthetic environments and we discuss them only briefly their main interest is as an example for radiometric problems in particular the radiosity of patches reasonably close to a line source changes as the reciprocal of distance to the source rather than the square of the distance the reasoning is more interesting than the effect we model a line source as a thin cylinder with diameter e assume for the moment that the line source is infinitely long and that we are considering a patch that views the source frontally as in figure figure sketches the appearance of the source from the point of view of patch now move the patch closer and consider patch the width of the region on the hemisphere corre sponding to the source changes but not the length because the source is infinitely long in turn because the width is approximately e r the radiosity due to the source must go down with the reciprocal of distance it is easy to see that with a source that is not infinitely long this applies as long as the patch is reasonably close area sources an area source is an area that radiates light area sources are important for two reasons first they occur quite commonly in natural scenes an overcast sky is a good example and in syn figure the radiosity due to a line source goes down as the reciprocal of dis tance for points that are reasonably close to the source on the left two patches viewing an infinitely long narrow cylinder with constant exitance along its sur face and diameter e on the right the view of the source from each patch drawn as the underside of the input hemisphere seen from below notice that the length of the source on this hemisphere does not change but the width does as e r this yields the result sources shadows and shading chap thetic environments for example the fluorescent light boxes found in many industrial ceilings second a study of area sources allows us to explain various shadowing and interreflection effects area sources are normally modeled as surface patches whose emitted radiance is independent of position and of direction they can be described by their exitance an argument similar to that used for line sources shows that for points not too distant from the source the radiosity due to an area source does not change with distance to the source this is because if the area is large enough with respect to the distance to the source the area subtended by the source on some input hemisphere is about the same as we move toward and away from the source this explains the widespread use of area sources in illumination engineering they gen erally yield fairly uniform illumination for our applications we need a more exact description of the radiosity due to an area source so we need to write out the integral the exact radiosity due to an area source assume we have a diffuse surface patch that is illuminated by an area source with exitance e q at the source point q instead of writing angles in coordinates we write q p for the direction from q to p more notation figure a diffuse source illuminates a diffuse surface the source has exi tance e q and we wish to compute the radiosity on the patch due to the source we do this by transforming the integral of incoming radiance at the surface into an integral over the source area this transformation is convenient because it avoids us having to use different angular domains for different surfaces how ever it still leads to an integral that is usually impossible in closed form sec local shading models is illustrated in figure the radiosity on the surface is obtained by summing the incoming radiance over all incoming directions this integral can be transformed into an integral over the source as follows b p ρd p ρd p ρd p li p q p cos θi dω le q q p cos θi dω π e q cos θi dω daq ρd p source π e q cos θi cos θs cos θ cos θ r ρd p source e q i πr daq the transformation works because radiance is constant along straight lines and because e q π le q it is useful because it means we do not have to worry about consistent angular coordinate systems however we transform them integrals describing the effect of area sources are generally difficult or impossible to do in closed form local shading models we have studied the physics of light because we want to know how bright things will be and why in the hope of extracting object information from these models currently we know the radiosity at a patch due to a source but this is not a shading model radiance could arrive at surface patches in other ways e g it could be reflected from other surface patches we need to know which components to account for the easiest model to manipulate is a local shading model which models the radiosity at a surface patch as the sum of the radiosity due only to light internally generated at sources this means that we assume that light is not reflected from surface to surface but instead leaves a source arrives at some surface and proceeds directly to the camera this model is palpably unphysical but is easy to analyze the model supports a variety of algorithms and theories see section unfortunately this model often produces wildly inaccurate predictions even worse there is little reliable information about when it is safe to use this model an alternate model is to account for all radiation section this takes into account radiance arriving from sources and that arriving from radiating surfaces this model is physically accurate but usually hard to manipulate local shading models for point sources the local shading model for a set of point sources is obtained by writing out the radiosity due to light internally generated at sources this gives b p sources visible from p bs p where bs p is the radiosity due to source this expression is fairly innocuous but notice that if all the sources are point sources at infinity the expression becomes sources shadows and shading chap b p sources visible from p ρd p n p ss so that if we confine our attention to a region where all points can see the same sources we could add all the source vectors to obtain a single virtual source that had the same effects the relationship between shape and shading is pretty direct here the radiosity is a measurement of one component of the surface normal for point sources that are not at infinity the model becomes b p ρ p n p s p d sources visible from p rs p where rs p is the distance from the source to p the presence of this term means that the relationship between shape and shading is somewhat more obscure the appearance of shadows in a local shading model shadows occur when the patch can not see one or more sources in this model point sources produce a series of shad ows with crisp boundaries shadow regions where no source can be seen are particularly dark shadows cast with a single source can be crisp and black depending on the size of the source and the albedo of other nearby surfaces which could reflect light into the shadow and soften its boundary it was a popular century pastime to cast such shadows onto paper and then draw them yielding the silhouettes still occasionally found in antiques shops the geometry of the shadow cast by a point source on a plane is analogous to the geometry of viewing in a perspective camera figure any patch on the plane is in shadow if a ray from the patch to the source passes through an object this means that there are two kinds of shadow boundary at self shadow boundaries the surface is turning away from the light and a ray from the patch to the source is tangent to the surface at cast shadow boundaries from the perspective of the patch the source suddenly disappears behind an occluding object shadows cast onto curved surfaces can have extremely complex geometries however figure shadows cast by point sources on planes are relatively simple self shadow boundaries occur when the surface turns away from the light and cast shadow boundaries occur when a distant surface occludes the view of the source sec local shading models if there are many sources the shadows are less dark except at points where no source is visible and there can be many qualitatively distinct shadow regions each source casts its own shadow some points may not see more than one source one example of this effect occurs in televised soccer matches because the stadium has multiple bright distant point like illuminants spaced evenly around the perimeter of the stadium there is a set of shadows radiating evenly around each player feet these shadows typically become brighter or darker as the player moves around usually because the illumination due to other sources and to interreflections in the region of the shadow increases or decreases area sources and their shadows the local shading model for a set of area sources is significantly more complex because it is possible for patches to see only a portion of a given source the model becomes b p all sources r visible component of source radiosity due to sourcel e q cos θq cos θs daq using the terminology of figure usually we assume that e is constant over the source area sources do not produce dark shadows with crisp boundaries this is because from the perspective of a viewing patch the source appears slowly from behind the occluding object think of an eclipse of the moon it is an exact analogy it is common to distinguish between points in the umbra a latin word meaning shadow which cannot see the source at all and points in the penumbra a compound of latin words meaning almost shadow which see part of the source the vast majority of indoor sources are area sources of one form or another so the effects are quite easy to see hold an arm quite close to the wall and look at the shadow it casts there is a dark core which gets larger as the arm gets closer to the wall this is the umbra surrounded by a lighter region with a fuzzier boundary the penumbra figure illustrates the geometry ambient illumination one problem with local shading models should be apparent immediately they predict that some shadow regions are arbitrarily dark because they cannot see the source this prediction is inac curate in almost every case because shadows are illuminated by light from other diffuse surfaces this effect can be significant in rooms with light walls and area sources it is possible to see shadows only by holding objects close to the wall or close to the source this is because a patch on the wall sees all the other walls in the room until an object is close to the wall it blocks out only a small fraction of the visual hemisphere of each patch for some environments the total irradiance a patch obtains from other patches is roughly constant and roughly uniformly distributed across the input hemisphere this must be true for the interior of a sphere with a constant distribution of radiosity by symmetry and by accepting a model of a cube as a sphere is roughly true for the interior of a room with white walls in such an environment it is sometimes possible to model the effect of other patches by adding an ambient illumination term to each patch radiosity there are two strategies for determining this term first if each patch sees the same proportion of the world e g the interior of a sphere we can add the same constant term to the radiosity of each patch the magnitude of this term is usually guessed sources shadows and shading chap area source occluder figure area sources generate complex shadows with smooth boundaries because from the point of view of a surface patch the source disappears slowly behind the occluder regions where the source cannot be seen at all are known as the umbra regions where some portion of the source is visible are known as the penumbra a good model is to imagine lying with your back to the surface looking at the world above at point you can see all of the source at point you can see some of it and at point you can see none of it second if some patches see more or less of the world than others this happens if regions of the world occlude a patch view e g a patch at the bottom of a groove this can be taken into account to do so we need a model of the world from the perspective of the patch under consideration a natural strategy is to model the world as a large distant polygon of constant radiosity where the view of this polygon is occluded at some patches see figure the result is that the ambient term is smaller for patches that see less of the world this model is often more accurate than adding a constant ambient term unfortunately it is much more difficult to extract information from this model possibly as difficult as for a global shading model application photometric stereo we reconstruct a patch of surface from a series of pictures of the surface taken under differ ent illuminants for simplicity we use an orthographic camera and choose a coordinate system such that the point x y z in space projects onto the point x y in the image the method we describe works for the other camera models described in chapter in this case to measure the shape of the surface we need to obtain the depth to the surface this suggests representing the surface as x y f x y a representation known as a monge patch after a french military engineer who first used it figure this representation is attrac tive because we can determine a unique point on the surface by giving the image coordinates notice that to obtain a measurement of a solid object we would need to reconstruct more than one patch because we need to observe the back of the object photometric stereo is a method for recovering a representation of the monge patch from image data the method involves reasoning about the image intensity values for several sec application photometric stereo view from view from figure ambient illumination is a term added to the radiosity predictions of local shading models to model the effects of radiosity from distant reflecting surfaces in a world like the interior of a sphere or of a cube the case on the left where a patch sees roughly the same thing from each point a constant ambient illumination term is often acceptable in more complex worlds some surface patches see much less of the surrounding world than others for example the patch at the base of the groove on the right sees relatively little of the outside world which we model as an infinite polygon of constant exitance its input hemisphere is shown below direction of projection height y x figure a monge patch is a representation of a piece of surface as a height function for the photometric stereo example we assume that an orthographic camera one that maps x y z in space to x y in the camera is viewing a monge patch this means that the shape of the surface can be represented as a function of position in the image sources shadows and shading chap different images of a surface in a fixed view illuminated by different sources this method re covers the height of the surface at points corresponding to each pixel in computer vision circles the resulting representation is often known as a height map depth map or dense depth map fix the camera and the surface in position and illuminate the surface using a point source that is far away compared with the size of the surface we adopt a local shading model and assume that there is no ambient illumination more about this later so that the radiosity at a point p on the surface is b p ρ p n p where n is the unit surface normal and is the source vector with our camera model there is only one point p on the surface for each point x y in the image and we can write b x y for b p now we assume that the response of the camera is linear in the surface radiosity so the value of a pixel at x y is i x y kb x y kρ x y n x y g x y where k is the constant connecting the camera response to the input radiance g x y ρ x y n x y and in these equations g x y describes the surface and is a property of the illumination and of the camera we have a dot product between a vector field g x y and a vector which could be measured with enough of these dot products we could reconstruct g and so the surface normal and albedo from many views now if we have n sources for each of which vi is known we stack each of these vi into a known matrix v where t v for each image point we stack the measurements into a vector i x y x y x y in x y t notice that we have one vector per image point each vector contains all the image brightnesses observed at that point for different sources now we have i x y vg x y and g is obtained by solving this linear system or rather one linear system per point in the image typically n so that a least squares solution is appropriate this has the advantage that the residual error in the solution provides a check on our measurements the difficulty with this approach is that substantial regions of the surface may be in shadow for one or the other light see figure there is a simple trick that deals with shadows if there really is no ambient illumination then we can form a matrix from the image vector and multiply both sides by this matrix this zeroes out any equations from points that are in shadow sec application photometric stereo figure five synthetic images of a sphere all obtained in an orthographic view from the same viewing position these images are shaded using a local shading model and a distant point source this is a convex object so the only view where there is no visible shadow occurs when the source direction is paral lel to the viewing direction the variations in brightness occuring under different sources code the shape of the surface we form x y i x y and in x y ii ivg x y and has the effect of zeroing the contributions from shadowed regions because the relevant elements of the matrix are zero at points that are in shadow again there is one linear system per point in the image at each point we solve this linear system to recover the g vector at that point measuring albedo we can extract the albedo from a measurement of g because n is the unit normal this means that g x y ρ x y this provides a check on our measurements as well because the albedo is in the range zero to one any pixels where g is greater than one are suspect either the pixel is not working or is incorrect figure shows albedo recovered using this method for the images of figure recovering normals we can extract the surface normal from g because the normal is a unit vector n x y g x y g x y figure shows normal values recovered for the images of figure sources shadows and shading chap figure the magnitude of the vector field g x y recovered from the in put data of figure represented as an image this is the reflectance of the surface shape from normals the surface is x y f x y so the normal as a function of x y is f f t n x y i f f x y to recover the depth map we need to determine f x y from measured values of the unit normal assume that the measured value of the unit normal at some point x y is a x y b x y c x y then f a x y f b x y x c x y and y c x y figure the normal field recovered from the input data of figure sec application photometric stereo we have another check on our data set because f f so we expect that x y y x a x y c x y y b x y c x y x should be small at each point in principle it should be zero but we would have to estimate these partial derivatives numerically and so should be willing to accept small values this test is known as a test of integrability which in vision applications always boils down to checking that mixed second partials are equal algorithm photometric stereo obtain many images in a fixed view under different illuminants determine the matrix from source and camera information create arrays for albedo normal components p measured value of f and q measured value of f for each point in the image array stack image values into a vector i construct the diagonal matrix i solve ivg ii to obtain g for this point albedo at this point is g normal at this point is p at this point is q at this point is g end check is p q small everywhere top left corner of height map is zero for each pixel in the left column of height map height value previous height value corresponding q value end for each row for each element of the row except for leftmost height value previous height value corresponding p value end end shape by integration assuming that the partial derivatives pass this sanity test we can reconstruct the surface up to some constant depth error the partial derivative gives the change in surface height with a small step in either the x or the y direction this means we can sources shadows and shading chap figure the height field obtained by integrating the normal field of figure using the method described in the text get the surface by summing these changes in height along some path in particular we have f f f x y x y dl c where c is a curve starting at some fixed point and ending at x y and c is a constant of integration which represents the unknown height of the surface at the start point the recovered surface does not depend on the choice of curve exercises for example we can reconstruct the surface at u v by starting at summing the y derivative along the line x to the point v and then summing the x derivative along the line y v to the point u v f u v v f y dy u f x x v dx c this is the integration path given in algorithm any other set of paths would work as well although it is probably best to use many different paths and average so as to spread around the error in the derivative estimates figure shows the reconstruction obtained for the data of figure another approach to recovering shape is to choose the function f x y whose partial derivatives most look like the measured partial derivatives we explore this approach for a similar problem in section interreflections global shading models local shading models can be quite misleading in the real world each surface patch is illuminated not only by sources but also by light reflected off other surface patches a phenomenon known as interreflection a model that incorporates interreflection effects is known as a global shading model interreflections lead to a variety of complex shading effects which are still quite poorly sec interreflections global shading models figure the column on the left shows data from a room with matte black walls and containing a collection of matte black polyhedral objects that on the right shows data from a white room containing white objects the images are qualitatively different with darker shadows and crisper boundaries in the black room and bright reflexes in the concave corners in the white room the graphs show sections of the image intensity along the corresponding lines in the images figure from mutual illumination by d a forsyth and a p zisserman proc cvpr qc ieee understood unfortunately these effects occur widely and it is still not yet known how to simplify global shading models without losing essential qualitative properties for example figure shows views of the interior of two rooms one room has black walls and contains black objects the other has white walls and contains white objects each is illuminated approximately by a distant point source given that the intensity of the source is adjusted appropriately the local shading model predicts that these pictures would be indistin guishable in fact the black room has much darker shadows and crisper boundaries at the creases of the polyhedra than the white room this is because surfaces in the black room reflect less light onto other surfaces they are darker whereas in the white room other surfaces are significant sources of radiation the sections of the camera response to the radiosity these are proportional to radiosity for diffuse surfaces shown in the figure are hugely different qualitatively in the black room the radiosity is constant in patches as a local shading model would predict whereas in the white room slow image gradients are quite common these occur in concave corners where object faces reflect light onto one another this effect also explains why a room illuminated by a point light source does not show the sharp illumination gradients that a local shading model predicts recall section the walls and floor of the room reflect illumination back and this tends to light up the corners which would otherwise be dark an interreflection model it is well understood how to predict the radiosity on a set of diffuse surface patches the total radiosity leaving a patch is its exitance which is zero for all but sources plus all the radiosity sources shadows and shading chap that is reflected from the patch b p e p brefl p from the point of view of our patch there is no distinction between energy leaving another patch due to exitance and that due to reflection this means we can take the expression for an area source and use it to obtain an expression for brefl q in particular from the perspective of our patch the patch at r in the world is equivalent to an area source with exitance b r this means brefl p ρd p world visible p q b q cos θp cos θq pq daq ρd p world visible p q k p q b q daq where the terminology is that of figure and visible p q if p can see q if p cannot see q the term visible p q k p q is usually referred to as the interreflection kernel substi tuting the expression for brefl p gives b p e p ρd p visible p q k p q b q daq world in particular the solution appears inside the integral equations of this form are known as fredholm integral equations of the second kind this particular equation is a fairly nasty sample of the type because the interreflection kernel is generally not continuous and may have singularities solutions of this equation can yield quite good models of the appearance of diffuse surfaces and the topic supports a substantial industry in the computer graphics community good daq figure terminology for expression derived in the text for the interreflec tion kernel sec interreflections global shading models illumination from an infinitely distant point source in this direction predicted corner position p p p position position in pixels figure the model described in the text produces quite accurate qualita tive predictions for interreflections the top figure shows a concave right angled groove illuminated by a point source at infinity where the source direction is par allel to the one face on the left of the bottom row is a series of predictions of the radiosity for this configuration these predictions have been scaled to lie on top of one another the case ρ corresponds to the local shading model on the right an observed image intensity for an image of this form for a corner made of white paper showing the roof like gradient in radiosity associated with the edge a local shading model predicts a step figure from mutual illumination by d a forsyth and a p zisserman proc cvpr qc ieee places to start for this topic are cohen and wallace or sillion the model produces good predictions of observed effects figure solving for radiosity we sketch one approach to solving the global shading model to illustrate the methods subdivide the world into small flat patches and approximate the radiosity as being constant over each patch this approximation is reasonable because we could obtain an accurate representation by working with small patches now we construct a vector b which contains the value of the radiosity for each patch in particular the i th component of b is the radiosity of the i th patch we write the incoming radiosity at the i th patch due to radiosity on the j th patch as bj i p ρd p patch j visible p q k p q daq bj where p is a coordinate on the i th patch and r is a coordinate on the j th patch now this expression is not a constant and so we must average it over the i th patch to get sources shadows and shading chap b j i a patch i ρd p patch j visible p q k p q dap daq bj where ai is the area of the i th patch if we insist that the exitance on each patch is constant too we obtain the model where bi ei b j i all j ei kij bj all j kij a patch i ρd p patch j visible p q k p q dap daq the elements of this matrix are sometimes known as form factors this is a system of linear equations in bi although an awfully big one kij could be a million by a million matrix and as such can in principle be solved the tricks that are necessary to solve the system efficiently quickly and accurately are well beyond our scope sillion is an excellent account as is the book of cohen and wallace the qualitative effects of interreflections we should like to extract shape information from radiosity this is relatively easy to do with a local model see section for some details but the model describes the world poorly and little is known about how severely this affects the resulting shape information extracting shape information from a global shading model is difficult for two reasons first the relationship be tween shape and radiosity is complicated because it is governed by the interreflection kernel second there are almost always surfaces that are not visible but radiate to the objects in view these so called distant surfaces mean it is hard to account for all radiation in the scene using an interreflection model because some radiators are invisible and we may know little or nothing about them all this suggests that understanding qualitative local effects of interreflection is important armed with this understanding we can either discount the effects of interreflection or exploit them this topic remains largely an open research topic but there are some things we can say smoothing and regional properties first interreflections have a characteristic smoothing effect this is most obviously seen if one tries to interpret a stained glass window by looking at the pattern it casts on the floor this pattern is almost always a set of indistinct colored blobs the effect is seen most easily with the crude model of figure the geometry consists of a patch with a frontal view of an infinite plane which is a unit distance away and carries a radiosity sin ωx there is no reason to vary the distance of the patch from the plane because interreflection problems have scale invariant solutions this means that the solution for a patch two units away can be obtained by reading our graph at the patch is small enough that its contribution to the plane radiosity can be ignored if the patch is slanted by σ with respect to the plane it carries radiosity that is nearly periodic with spatial frequency ω cos σ we refer to the amplitude of the component at this frequency as the gain of the patch and plot the gain in figure the important property of this graph is that high spatial frequencies have a difficult time jumping the gap from the plane to the patch this means that shading effects with high sec notes source spatial frequency radians per unit length figure a small patch views a plane with sinusoidal radiosity of unit am plitude this patch has a roughly sinusoidal radiosity due to the effects of the plane we refer to the amplitude of this component as the gain of the patch the graph shows numerical estimates of the gain for patches at equal steps in slant angle from to π as a function of spatial frequency on the plane the gain falls extremely fast meaning that large terms at high spatial frequencies must be regional effects rather than the result of distant radiators this is why it is hard to determine the pattern in a stained glass window by looking at the floor at foot of the window reprinted from shading primitives finding folds and shallow grooves by j haddon and d a forsyth proc int conf computer vision qc ieee spatial frequency and high amplitude generally cannot come from distant surfaces unless they are abnormally bright the extremely fast fall off in amplitude with spatial frequency of terms due to distant surfaces means that if one observes a high amplitude term at a high spatial frequency it is very unlikely to have resulted from the effects of distant passive radiators because these effects die away quickly there is a convention which we see in section that classifies effects in shading as due to reflectance if they are fast edges and the dynamic range is relatively low and due to illumination otherwise we can expand this convention there is a mid range of spatial frequencies that are largely unaffected by mutual illumination from distant surfaces because the gain is small spatial frequencies in this range cannot be transmitted by distant passive radiators unless these radiators have improbably high radiosity as a result spatial frequencies in this range can be thought of as regional properties which can result only from interreflection effects within a region the most notable regional properties are probably reflexes small bright patches that ap pear mainly in concave regions illustrated in figure and figure a second important effect is color bleeding where a colored surface reflects light onto another colored surface this is a common effect that people tend not to notice unless they are consciously looking for it it is quite often reproduced by painters notes shading models are handled in a quite unsystematic way in the vision literature the point source approximation is widely abused you should use it with care and inspect others use of it with suspicion we believe we are the first to draw the distinction between a the physical effects of sources and b the shading model sources shadows and shading chap illumination from an infinitely distant point source in this direction position position in pixels figure reflexes at concave edges are a common qualitative result of in terreflections the figure on the top shows the situation here a concave right angled groove illuminated by a point light source at infinity whose source vector is along the angle bisector the graph on the left shows the intensity predictions of an interreflection model for this configuration the case ρ is a local shad ing model the graphs have been lined up for easy comparison as the surface albedo goes up a roof like structure appears the graph on the right shows an observation of this effect in an image of a real scene figure from mutual il lumination by d a forsyth and a p zisserman proc cvpr c ieee local shading models the great virtue of local shading models is that the analysis is simple the primary characteristic of a local shading model is that on a surface of constant albedo the radiosity of a surface patch is a function of the normal alone this means that one can avoid the abstraction of reflectance and sources and instead simply code the properties of surface and source as a reflectance map the reflectance map is a function that takes a representation of the normal and returns the radiosity to be expected at a point with that normal horn started the systematic study of shading in computer vision with important papers on recovering shape from a local shading model using a point source horn with a more recent account in horn the methods discussed have largely fallen into disuse at least partially because they appear unable to cope with the difficulties created by a global shading model so we do not survey the vast literature here a comprehensive summary is in horn and brooks shape and albedo are ambiguous with appropriate changes in albedo surfaces of different shapes can generate the same image belhumeur kriegman and yuille kriegman and belhumeur because the surface normal is the key in local shading models such models typically yield elegant links between surface shading and curvature koenderink and van doorn sec notes self shadow a b c predicted observed position 500 position in pixels figure reflexes occur quite widely they are usually caused by a favor able view of a large reflecting surface in the geometry shown on the top the shadowed region of the cylindrical bump sees the plane background at a fairly favorable angle if the background is large enough near half the hemisphere of the patch at the base of the bump is a view of the plane this means there will be a reflex with a large value attached to the edge of the bump and inside the cast shadow region which a local model predicts as black there is another reflex on the other side too as the series of solutions again normalized for easy comparison on the left show on the right an observation of this effect in a real scene figure from mutual illumination by d a forsyth and a p zisserman proc cvpr qc ieee interreflections the effects of global shading are often ignored in the shading literature which causes a reflex response of hostility in one of the authors the reason to ignore interreflections is that they are extremely hard to analyze particularly from the perspective of inferring object properties given the output of a global shading model if interreflection effects do not change the output of a method much then it is probably all right to ignore them unfortunately this line of reasoning is seldom pursued because it is quite difficult to show that a method is stable under interreflections the discussion of spatial frequency issues follows haddon and forsyth after an idea of koenderink and van doorn apart from this there is not much knowledge about the overall properties of interreflected shading which is an important gap in our knowledge an alternative strategy is to iteratively reestimate shape using a rendering model nayar ikeuchi and kanade horn is also the first author to indicate the significance of global shading effects horn koenderink and van doorn noted that the radiosity under a global model is ob tained by taking the radiosity under a local model and applying a linear operator one then studies that operator in some cases its eigenfunctions often called geometrical modes are informative forsyth and zisserman then demonstrated a variety of the qual itative effects due to interreflections sources shadows and shading chap photometric stereo in its original form photometric stereo is due to woodham there are a number of variants of this useful idea horn woodham and silver woodham there are a variety of variations on photometric stereo one interesting idea is to illuminate the surface with three lights of different colors and in different positions and use a color image for an appropriate choice of colors this is equivalent to obtaining three images so the measurement process is simplified generally photometric stereo is used under circumstances where the illumination is quite easily controlled so that it is possible to ensure that no ambient illumination is in the image it is relatively simple to insert ambient illumination into the formulation given we extend the matrix by attaching a column of ones in this case g x y becomes a four dimensional vector and the fourth component is the ambient term however this approach does not guarantee that the ambient term is constant over space instead we would have to check that this term was constant and adjust the model if it were not photometric stereo depends only on adopting a local shading model this model need not be a lambertian surface illuminated by a distant point source if the radiosity of the surface is a known function of the surface normal satisfying a small number of constraints photometric stereo is still possible this is because the intensity of a pixel in a single view determines the normal up to a one parameter family this means that two views determine the normal the simplest example of this case occurs for a surface of known albedo illuminated by a distant point source in fact if the radiosity of the surface is a k parameter function of the surface normal photometric stereo is still possible the intensity of the pixel in a single view determines the normal up to a k parameter family and k views give the normal for this approach to work the radiosity needs to be given by a function for which our arithmetic works e g if the radiosity of the surface is a constant function of the surface normal it is not possible to infer any constraint on the normal from the radiosity one can then recover shape and reflectance maps simultaneously garcia bermejo diaz pernas and coronado mukawa nayar ikeuchi and kanade and tagare and de figueiredo alternative shading representations instead of trying to extract shape information from the shading signal one might try to match it to a collection of different possible examples this suggests studying what kinds of shaded view a surface can generate the collection of available shadings is notably limited belhumeur and kriegman a knowledge of this collection structure is valuable because it makes it possible to understand how to compare shaded images without being confused by illumination changes illumination changes are a particular problem in face finding and recognition applica tions adini moses and ullman phillips and vardi knowing the possible varia tions in illumination seems to help georghiades kriegman and belhumeur jacobs belhumeur and basri another possibility is to extend the notion of qualitative analysis of interreflections to ob tain a shading primitive a shading pattern that is characteristic and stably linked to a shape pattern for example narrow grooves and deep holes in surfaces are dark and cylinders have a characteristic extended pattern of shading few such primitives are known but some appear to be useful haddon and forsyth problems problems what shapes can the shadow of a sphere take if it is cast on a plane and the source is a point source we have a square area source and a square occluder both parallel to a plane the source is the same size as the occluder and they are vertically above one another with their centers aligned what is the shape of the umbra what is the shape of the outside boundary of the penumbra we have a square area source and a square occluder both parallel to a plane the edge length of the source is now twice that of the occluder and they are vertically above one another with their centers aligned what is the shape of the umbra what is the shape of the outside boundary of the penumbra we have a square area source and a square occluder both parallel to a plane the edge length of the source is now half that of the occluder and they are vertically above one another with their centers aligned what is the shape of the umbra what is the shape of the outside boundary of the penumbra a small sphere casts a shadow on a larger sphere describe the possible shadow boundaries that occur explain why it is difficult to use shadow boundaries to infer shape particularly if the shadow is cast onto a curved surface an infinitesimal patch views a circular area source of constant exitance frontally along the axis of symmetry of the source compute the radiosity of the patch due to the source exitance e u as a function of the area of the source and the distance between the center of the source and the patch you may have to look the integral up in tables if you don t you re entitled to feel pleased with yourself but this is one of few cases that can be done in closed form it is easier to look up if you transform it to get rid of the cosine terms as in figure a small patch views an infinite plane at unit distance the patch is sufficiently small that it reflects a trivial quantity of light onto the plane the plane has radiosity b x y sin ax the patch and the plane are parallel to one another we move the patch around parallel to the plane and consider its radiosity at various points show that if one translates the patch its radiosity varies periodically with its position in x fix the patch center at determine a closed form expression for the radiosity of the patch at this point as a function of a you ll need a table of integrals for this if you don t you re entitled to feel very pleased with yourself if one looks across a large bay in the daytime it is often hard to distinguish the mountains on the opposite side near sunset they are clearly visible this phenomenon has to do with scattering of light by air a large volume of air is actually a source explain what is happening we have modeled air as a vacuum and asserted that no energy is lost along a straight line in a vacuum use your explanation to give an estimate of the kind of scales over which that model is acceptable read the book colour and light in nature by lynch and livingstone published by cambridge uni versity press programming assignments an area source can be approximated as a grid of point sources the weakness of this approximation is that the penumbra contains quantization errors which can be quite offensive to the eye explain render this effect for a square source and a single occluder casting a shadow onto an infinite plane for a fixed geometry you should find that as the number of point sources goes up the quantization error goes down sources shadows and shading chap this approximation has the unpleasant property that it is possible to produce arbitrarily large quantization errors with any finite grid by changing the geometry this is because there are con figurations of source and occluder that produce large penumbrae use a square source and a single occluder casting a shadow onto an infinite plane to explain this effect make a world of black objects and another of white objects paper glue and spraypaint are useful here and observe the effects of interreflections can you come up with a criterion that reliably tells from an image which is which if you can publish it the problem looks easy but isn t this exercise requires some knowledge of numerical analysis do the numerical integrals required to reproduce figure these integrals aren t particularly easy if one uses coordinates on the infinite plane the size of the domain is a nuisance if one converts to coordinates on the view hemisphere of the patch the frequency of the radiance becomes infinite at the boundary of the hemisphere the best way to estimate these integrals is using a monte carlo method on the hemisphere you should use importance sampling because the boundary contributes rather less to the integral than the top set up and solve the linear equations for an interreflection solution for the interior of a cube with a small square source in the center of the ceiling implement a photometric stereo system how accurate are its measurements i e how well do they compare with known shape informa tion do interreflections affect the accuracy how repeatable are its measurements i e if you obtain another set of images perhaps under different illuminants and recover shape from those how does the new shape compare with the old compare the minimization approach to reconstruction with the integration approach which is more accurate or more repeatable and why does this difference appear in experiment one possible way to improve the integration approach is to obtain depths by integrating over many different paths and then average these depths you need to be a little careful about constants here does this improve the accuracy or repeatability of the method the entertainment industry touches hundreds of millions of people every day and synthetic pic tures of real scenes often mixed with actual film footage are now common place in computer games sports broadcasting tv advertising and feature films creating these images is what image based rendering defined here as the synthesis of new views of a scene from prerecorded pictures is all about and it does require the recovery of quantitative although not necessarily three dimensional shape information from images this chapter presents a number of represen tative approaches to image based rendering dividing them rather arbitrarily into a techniques that first recover a three dimensional scene model from a sequence of pictures then render it with classical computer graphics tools naturally these approaches are often related to stereo and motion analysis b methods that do not attempt to recover the camera or scene parameters but construct instead an explicit representation of the set of all possible pictures of the observed scene then use the image position of a small number of tie points to specify a new view of the scene and transfer all the other points into the new image in the photogrammetric sense already mentioned in chapter and c approaches that model images by a two dimensional set of light rays or more precisely by the value of the radiance along these rays and the set of all pictures of a scene by a four dimensional set of rays the light field figure constructing models from image sequences this section addresses the problem of building and rendering a three dimensional object model from a sequence of pictures it is of course possible to construct such a model by fusing regis tered depth maps acquired by range scanners as described in chapter but we focus here on the case where the input images are digitized photographs or film clips of a rigid or dynamic scene sec constructing models from image sequences pre recorded images model control new image figure approaches to image based rendering from top to bottom three dimensional model construction from image sequences transfer based image synthesis the light field from left to right the image based rendering pipeline a scene model that may not be three dimensional is constructed from sample images and used to render new images of the scene the rendering engine may be controlled by a joystick or equivalently by the specification of camera param eters or in the case of transfer based techniques by setting the image position of a small number of tie points scene modeling from registered images volumetric reconstruction let us assume that an object has been delineated per haps interactively in a collection of photographs registered in the same global coordinate system it is impossible to uniquely recover the object shape from the image contours since as observed in chapter the concave portions of its surface never show up on the image contour still we should be able to construct a reasonable approximation of the surface from a large enough set of pictures there are two main global constraints imposed on a solid shape by its image contours a it lies in the volume defined by the intersection of the viewing cones attached to each image and b the cones are tangent to its surface there are other local constraints e g as shown in chapter convex resp concave parts of the contour are the projections of convex resp saddle shaped parts of the surface baumgart exploited the first of these constraints in his phd thesis to construct polyhedral models of various objects by intersecting the polyhedral cones associated with polygonal approximations of their silhouettes his ideas have inspired a number of approaches to object modeling from silhouettes including the technique presented in the rest of this section sullivan and ponce that also incorporates the tangency constraint associated with the viewing cones as in baumgart system a polyhedral approximation of the observed object is first constructed by intersecting the visual cones associated with a few pho tographs figure the vertices of this polyhedron are then used as the control points of a smooth spline surface which is deformed until it is tangent to the visual rays we focus here on the construction and deformation of this surface spline construction a spline curve is a piecewise polynomial parametric curve that satisfies certain smoothness conditions for example it may be ck i e differentiable with continuous derivatives of order up to k with k usually taken to be or or gk i e not necessarily differentiable everywhere but with continuous tangents in the case and continuous curvatures in the case spline curves are usually constructed by stitching together be zier arcs a be zier curve of degree n is a polynomial parametric curve p defined as the barycentric application image based rendering chap b c figure constructing object models by intersecting polyhedral viewing cones a six photographs of a teapot b the raw intersection of the correspond ing viewing cones c the triangulation obtained by splitting each face into tri angles and simplifying the resulting mesh reprinted from automatic model construction pose estimation and object recognition from photographs using triangular splines by s sullivan and j ponce ieee transactions on pattern analysis and machine intelligence oc ieee combination n n p t bi t pi i of n control points pn where the weights b n t d ef n ti t n i are called the bernstein polynomials of degree n i i but not the other ones figure as shown in the exercises the tangents at its endpoints are along the first and last line segments of the control polygon formed by the control points the definition of be zier arcs and spline curves naturally extends to surfaces a triangu lar be zier patch of degree n is a parametric surface p defined as the barycentric combination p u v i j k n b n u v u v pijk of a triangular array of control points pijk where the homogeneous polynomials b n u v w d ef i n uiv jwk are the trivariate bernstein polynomials of degree n in the rest of this section we j k quartic be zier patches n each defined by control points figure their boundaries are the quartic be zier curves p u p v and p u u by definition a triangular spline is a network of triangular be zier patches that share the same tangent plane along their common boundaries a necessary but not sufficient condition for continuity is that all control points surrounding a common vertex be coplanar we first construct these points then place the remaining control points to ensure that the resulting spline is indeed continuous as discussed in loop a set of coplanar points qp can be created as a barycentric is indeed a barycentric combination as defined in chapter since the bernstein polynomials are easily shown to always add to in particular be zier curves are affine constructs a desirable property since it allows the definition of these curves purely in terms of their control points and independently of the choice of any external coordinate system sec constructing models from image sequences a b figure be zier curves and patches a a cubic be zier curve and its con trol polygon b a quartic triangular be zier patch and its control mesh tensor product be zier patches can also be defined using a rectangular array of control points farin triangular patches are however more appropriate for mod eling free form closed surfaces t v cubic control points ai interior quartic control points degree raised quartic control points figure construction of a triangular spline over a triangular polyhedral mesh top from left to right the cubic boundary control points the boundary curves surrounding a mesh vertex and the construction of internal control points from tangent specification bottom splitting a patch three ways to enforce continuity the white points are the control points obtained by raising the de gree of the control curves and the gray points are the remaining control points computed to ensure continuity after sullivan and ponce application image based rendering chap combination of p other points cp in general position in our case the centroids of the p triangles tj adjacent to a vertex v of the input triangulation figure top left as q cos π cos i j i π c this construction places the points qi in a plane passing through the centroid o of the points ci translating this plane so that o coincides with v yields a new set of points ai lying in a plane passing through v figure top center since cubic be zier curves are defined by four points we can interpret two adjacent vertices v and v i and the points ai and aii associated with the corresponding edge as the control points of a cubic curve this yields a set of cubic arcs that interpolate the vertices of the control mesh and form the boundaries of triangular patches once these curves have been constructed the control points on both sides of a boundary can be chosen to satisfy interpatch continuity in this con struction the cross boundary tangent field linearly interpolates the tangents at the two endpoints of the boundary curve at the endpoint v the tangent t across the curve that contains the point ai is taken to be parallel to the line joining ai to ai the tangent ti is obtained by a similar con struction the interior control points f f i g and gi figure top right are constructed by solving the set of linear equations associated with this geometric condition chiyokura however there are not enough degrees of freedom in a quartic patch to allow the simultane ous setting of the interior points for all three boundaries thus each patch must be split three ways using for example the method of shirman and sequin to ensure continuity among the new patches performing degree elevation on the boundary curves replaces them by quartic be zier curves with the same shape see exercises three quartic triangular patches can then be constructed from the boundaries as shown in figure bottom the result is a set of three quartic patches for each mesh face which are continuous across all boundaries spline deformation we have given a method for constructing a continuous triangular spline approximation of a surface from a triangulation such as the one shown in figure b let us now show how to deform this spline to ensure that it is tangent to the viewing cones associated with the input photographs the shape of the spline surface s is determined by the position of its control vertices vp we denote by vjk k the coordinates of the point vj j p in some reference euclidean coordinate system and use these p coefficients as shape parameters given a set of rays rq we minimize the energy function q r ri s λ puu puv pvv du dv with respect to the parameters vjk of s here d r s denotes the distance between the ray r and the surface s the integral is a thin plate spline energy term used to enforce smoothness in ar eas of sparse data and λ is a constant weight introduced to balance the distance and smoothness terms the variables u and v in this integral are the patch parameters and the summation is done over the r patches that form the spline surface the signed distance between a ray and a surface patch can be computed using newton method for rays that do not intersect the surface we de fine d r s min q p q r p s and compute the distance by minimizing q p for those rays that intersect the surface we follow brunie lavalle e and szeliski and mea sure the distance to the farthest point from the ray that lies on the surface in the direction of the surface normal at the corresponding occluding contour point in both cases newton iterations are initialized from a sampling of the surface s during surface fitting the spline is deformed sec constructing models from image sequences figure shaded and texture mapped models of a teapot gargoyle and di nosaur the teapot was constructed from six registered photographs the gar goyle and dinosaur models were each built from nine images reprinted from automatic model construction pose estimation and object recognition from photographs using triangular splines by s sullivan and j ponce ieee transactions on pattern analysis and machine intelligence oc ieee to minimize the mean squared ray surface distance using a simple gradient descent technique although each distance is computed numerically its derivatives with respect to the surface pa rameters vjk are easily computed by differentiating the constraints satisfied by the surface and ray points where the distance is reached the three object models shown in figure have been constructed using the method described in this section this technique does not require establishing any correspondence across the input pictures but its scope is currently limited to static scenes in contrast the approach presented next is based on multicamera stereopsis and as such requires correspondences but it handles dynamic scenes as well as static ones virtualized reality kanade and his colleagues have proposed the concept of virtualized reality as a new visual medium for manipulating and rendering prerecorded and synthetic images of real scenes captured in a controlled environment the first physical imple mentation of this concept at carnegie mellon university consisted of a geodesic dome equipped with synchronized video cameras hooked to consumer grade vcrs as of this writing the latest implementation is a room where a volume of cubic feet is observed by color cameras connected to a pc cluster and registered in the same world coordinate sys tem with the capability of digitizing in real time the synchronized video streams of all cameras three dimensional scene models are acquired by fusing dense depth maps acquired via multiple camera stereo see okutami and kanade chapter one such map is acquired by each camera and a small number of its neighbors between three and six every range image is then application image based rendering chap figure multicamera stereo from left to right the range map associated with a cluster of cameras a texture mapped image of the corresponding mesh observed from a different viewpoint note the dark areas associated with depth discontinuities in the map a texture mapped image constructed from two adja cent camera clusters note that the gaps have been filled reprinted from vir tualized reality constructing virtual worlds from real scenes by t kanade rander and j p narayanan ieee multimedia c ieee b figure virtualized reality a a sequence of synthetic images note that occlusion in the two elliptical regions of the first view is handled correctly the corresponding mesh model reprinted from appearance based virtual view generation of temporally varying events from multi camera images in the room by h saito s baba m kimura s vedula and t kanade tech rep cmu cs school of computer science carnegie mellon univer sity sec constructing models from image sequences converted to a surface mesh that can be rendered using classical computer graphics techniques such as texture mapping as shown by figure images of a scene constructed from a single depth map may exhibit gaps these gaps can be filled by rendering in the same image the meshes corresponding to adjacent cameras it is also possible to directly merge the surface meshes associated with different cameras into a single surface model this task is challenging since a multiple conflicting measure ments of the same surface patches are available in areas where the fields of view of several cameras overlap and b certain scene patches are not observed by any camera both problems can be solved using the volumetric technique for range image fusion proposed by curless and levoy and introduced in chapter once a global surface model has been constructed it can of course be texture mapped as before synthetic animations can also be obtained by inter polating two arbitrary views in the input sequence first the surface model is used to establish correspondences between these two views the optical ray passing through any point in the first image is intersected with the mesh and the intersection point is reprojected in the second image yielding the desired match once the correspondences are known new views are constructed by linearly interpolating both the positions and colors of matching points as discussed in saito et al this simple algorithm only provides an approximation of true perspective imaging and additional logic has to be added in practice to handle points that are visible in the first image but not in the second one nevertheless it can be used to generate realistic animations of dynamic scenes with changing occlusion patterns as demonstrated by figure scene modeling from unregistered images this section addresses again the problem of acquiring and rendering three dimensional object models from a set of images but this time the positions of the cameras observing the scene are not known a priori and must be recovered from image information using methods related to those presented in chapters and the techniques presented in this section are however explicitly geared toward computer graphics applications the fac ade system the fac ade system for modeling and rendering architectural scenes from digitized photographs was developed at uc berkeley by debevec taylor and ma lik this system takes advantage of the relatively simple overall geometry of many build ings to simplify the estimation of scene structure and camera motion and it uses the simple but powerful idea of model based stereopsis to be described in a minute to add detail to rough building outlines figure shows an example fac ade models are constrained hierarchies of parametric primitives such as boxes prisms and solids of revolution these primitives are defined by a small number of coefficients e g the height width and breadth of a box and related to each other by rigid transformations any of the parameters defining a model is either a constant or variable and constraints can be specified between the various unknowns e g two blocks may be constrained to have the same height model hierarchies are defined interactively with a graphical user interface and the main com putational task of the fac ade system is to use image information to assign definite values to the unknown model parameters the overall system is divided into three main components the first one or photogrammetric module recasts structure and motion estimation as the nonlinear optimization problem of minimizing the discrepency between line segments selected by hand in the photographs and the projections of the corresponding parts of the parametric model see exercises for details as shown in debevec et al this process involves relatively few narrow baseline methods like correlation would be ineffective in this context since the two views may be far from each other a similar method is used in the fac ade system described later in this chapter to establish correspondences between widely separated images when the rough shape of the observed surface is known application image based rendering chap figure fac ade model of the berkeley campanile from left to right a photograph of the campanile with selected edges overlaid the model recovered by photogrammetric modeling reprojection of the model into the photograph a texture mapped view of the model reprinted from modeling and rendering architecture from photographs a hybrid geometry and image based approach by p debevec c j taylor and j malik proc siggraph oc acm inc included here by permission variables namely the positions and orientations of the cameras used to photograph a building and the parameters of the building model and when the orientation of some of the model edges is fixed relative to the world coordinate system an initial estimate for these parameters is easily found using linear least squares the second main component of fac ade is the view dependent texture mapping module that renders an architectural scene by mapping different photographs onto its geometric model according to the user viewpoint conceptually the cameras are replaced by slide projectors that project the original images onto the model of course each camera only sees a portion of a building and several photographs must be used to render a complete model in general parts of a building are observed by several cameras so the renderer must not only pick but also appropriately merge the pictures relevant to the synthesis of a virtual view the solution adopted in fac ade is to assign to each pixel in a new image a weighted average of the values predicted from the overlapping input pictures with weights inversely proportional to the angle between the corresponding light rays in the input and virtual views the last component of fac ade is the model based stereopsis module which uses stereo pairs to add fine geometric detail to the relatively rough scene description constructed by the photogrammetric modeling module the main difficulty in using stereo vision in this setting is the wide separation of the cameras which prevents the straightforward use of correlation based matching techniques the solution adopted in fac ade is to exploit a priori shape information to map the stereo images into the same reference frame figure top specifically given key and offset pictures the offset image can be projected onto the scene model before being rendered from the key camera viewpoint yielding a warped offset picture similar to the key image fig ure bottom in turn this allows the use of correlation to establish correspondences between these two images and thus between the key and offset images as well once the matches between these two pictures have been established stereo reconstruction reduces to the usual triangulation process sec transfer based approaches to image based rendering figure model based stereopsis top synthesis of a warped offset image the point pi in the offset image is mapped onto the point q of the surface model then reprojected onto the point q of the warped offset image the actual surface point p observed by both cameras projects onto the point p of the key image note that the point q must lie on the epipolar line ep which facilitates the search for matches as in the conventional stereo case note also that the disparity be tween p and q along the epipolar line measures the discrepancy between the modeled and actual surfaces after debevec et al figure bottom from left to right a key image an offset image and the corresponding warped offset image reprinted from modeling and rendering architecture from pho tographs a hybrid geometry and image based approach by p debevec c j taylor and j malik proc siggraph c acm inc included here by permission transfer based approaches to image based rendering this section explores a completely different approach to image based rendering in this frame work an explicit three dimensional scene reconstruction is never performed instead new images are created directly from a possibly small set of views among which point correspondences have been established by feature tracking or conventional stereo matching this approach is related to the classical transfer problem from photogrammetry already mentioned in chapter given the image positions of a number of tie points in a set of reference images and in a new image and given the image positions of a ground point in the reference images predict the position of that point in the new image transfer based techniques for image based rendering were introduced in the projective setting by laveau and faugeras who proposed to first estimate the pairwise epipolar ge ometry between reference views then reproject the scene points into a virtual image specified by the projections of the new optical center in two reference pictures i e the epipoles and the position of four tie points in the new view by definition the epipolar geometry constrains the application image based rendering chap figure augmented reality experiment the affine world coordinate system is defined by corners of the black polygons reprinted from calibration free augmented reality by k kutulakos and j vallino ieee transactions on visualization and computer graphics oc ieee possible reprojections of points in the reference images in the new view the projection of the scene point is at the intersection of the two epipolar lines associated with the point and two ref erence pictures once the feature points have been reprojected realistic pictures are synthesized using ray tracing and texture mapping as noted by laveau and faugeras however since the eu clidean constraints associated with calibrated cameras are not enforced the rendered images are separated from correct pictures by arbitrary planar projective transformations unless additional scene constraints are taken into account the rest of this section explores two affine variants of the transfer based approach that circumvent this difficulty both techniques construct a param eterization of the set of all images of a rigid scene in the first case section the affine structure of the space of affine images is used to render synthetic objects in an augmented re ality system because the tie points in this case are always geometrically valid image features e g the corners of calibration polygons see figure the synthesized images are automat ically euclidean ones in the second instance section the metric constraints associated with calibrated cameras are explicitly taken into account in the image space parameterization guaranteeing once again the synthesis of correct euclidean images let us note again a particularity of transfer based approaches to image based rendering already mentioned in the introduction because no three dimensional model is ever constructed a joystick cannot be used to control the synthesis of an animation instead the position of tie points must be specified interactively by a user this is not a problem in an augmented reality context but whether this is a viable user interface for virtual reality applications remains to be shown affine view synthesis here we address the problem of synthesizing new affine images of a scene from old ones with out setting an explicit three dimensional euclidean coordinate system recall from chapter that if we denote the coordinate vector of a scene point p in some world coordinate system by p x y z t and denote by p u v t the coordinate vector of the projection p of p onto the image plane the affine camera model of eq can be written as at where b is the position of the projection into the image of the object coordinate system origin and and are vectors in sec transfer based approaches to image based rendering let us consider four noncoplanar scene points say and we can choose without loss of generality these points as an affine reference frame so their coordinate vectors are the points p i are not in general at a unit distance from p nor are the vectors pi and p pj orthogonal to each other when i j this is irrelevant since we work in an affine setting since the matrix with columns and is the identity eq can be rewritten as p ap b y z b finally since we have chosen as the origin of the world coordinate system we have b and we obtain p x y z x y z this result is related to the affine structure of affine images as discussed in chapter in the context of image based rendering it follows from eq that x y and z can be computed from m images of the points and p through linear least squares once these values are known new images can be synthesized by specifying the image positions of the points and using eq to compute all the other point positions kutulakos and vallino in addition since the affine representation of the scene is truly three dimensional the relative depth of scene points can be computed and used to eliminate hidden surfaces in the z buffer part of the graphics pipeline it should be noted that specifying arbitrary positions for the points generally produces affinely deformed pictures this is not a problem in augmented reality applications where graphical and physical objects co exist in the image in this case the anchor points can be chosen among true image points guaranteed to be in the correct euclidean position figure shows an example where synthetic objects have been overlaid on real images when longer image sequences are available a variant of this approach that takes into ac count all scene points in a uniform manner can be obtained as follows suppose we observe a fixed set of points pn with coordinate vectors pi i n and let pi denote the coordinate vectors of the corresponding image points writing eq for all the scene points yields pt pn n b t t n in other words the set of all affine images of n fixed points is an eight dimensional vector space v embedded in and parameterized by the vectors and b given m views does not contradict the result established in chapter which states that the set of m fixed views of an arbitrary collection of points is a three dimensional affine subspace of application image based rendering chap of n points a basis for this vector space can be identified by performing the singular value decomposition of the m matrix m p n p m where p j denotes the position of the image point number i in frame number j once a basis for v has been constructed new images can be constructed by assigning arbitrary values to and b for interactive image synthesis purposes a more intuitive control of the imaging geometry can be obtained by specifying as before the position of four image points solving for the corresponding values of and b and computing the remaining image positions euclidean view synthesis as discussed earlier a drawback of the method presented in the previous section is that specifying arbitrary positions for the points generally yields affinely deformed pictures this can be avoided by taking into account from the start the euclidean constraints associated with calibrated cameras we saw in chapter that a weak perspective camera is an affine camera satisfying the two quadratic constraints and the previous section showed that the affine images of a fixed scene form an eight dimensional vector space v now if we restrict our attention to weak perspective cameras the set of im ages becomes the six dimensional subspace defined by these two polynomial constraints sim ilar constraints apply to paraperspective and true perspective projection and they also define a six dimensional variety i e a subspace defined by polynomial equations in each case let us suppose that we observe three points whose images are not collinear we can choose without loss of generality a euclidean coordinate system such that the coordinate vectors of the four points in this system are p where p and q are nonzero but a priori unknown let us denote as before by pi the projection of the point pi i since is the origin of the world coordinate system we have b we are also free to pick as the origin of the image coordinate system this amounts to submitting all image points to a known translation so eq simplifies into at p p ap now applying eq to and p yields def def u u u and v v v at least eight images may seem like overkill since the affine structure of a scene can be recovered from two pictures as shown in chapter indeed as shown in the exercises a basis for v can in fact be constructed from two images of at least four points sec transfer based approaches to image based rendering where t def p p q x y z in turn this implies that where pt qu and qv q d ef p λ µ and λ p q µ q α z β z z α x λy β µy using eq and letting def t the weak perspective constraints of eq can be rewritten as r ut ru v rv with u rv α z α α β µ z β equation defines a pair of linear constraints on the coefficients ξi i α and β these can be rewritten as t t where d def def and def u u u u β uv when the four points and p are rigidly attached to each other the five structure coefficients α and β are fixed for a rigid scene formed by n points choosing three of the points as a reference triangle and writing eq for the remaining ones yields a set of quadratic equations in unknowns which define a parameterization of the set of all weak perspective images of the scenes this is the parameterized image variety piv of genc and ponce note again that the weak perspective constraints of eq are linear in the five structure coefficients thus given a collection of images and point correspondences these coefficients can be estimated through linear least squares once the vector ξ has been estimated arbitrary image application image based rendering chap figure z buffering reprinted from parameterized image varieties a novel approach to the analysis and synthesis of image sequences by y genc and j ponce proc international conference on computer vision oc ieee positions can be assigned to the three reference points equation yields for each feature point two quadratic constraints on the two unknowns u and v although this system should a priori admit four solutions it admits as shown in the exercises exactly two real solutions in fact given n point correspondences and the image positions of the three tie points it can also be shown genc and ponce that the pictures of the remaining n points can be determined in closed form up to a two fold ambiguity once the positions of all feature points have been determined the scene can be rendered by triangulating these points and texture mapping the triangles interestingly hidden surface removal can also be performed via traditional z buffer techniques although no explicit three dimensional reconstruction is performed the idea is to assign relative depth values to the vertices of the triangulation and it is closely related to the method used in the affine structure from motion theorem from chapter let δ denote the image plane of one of our input images and δi the image plane of our synthetic image to render correctly two points p and q that project onto the same point r i in the synthetic image we must compare their depths figure let r denote the intersection of the viewing ray joining p to q with the plane spanned by the reference points and and let p q r denote the projections of p q and r into the reference image suppose for the time being that p and q are two of the points tracked in the input image it follows that the positions of p and q are known the position of r is easily computed by remarking that its coordinates in the affine basis of δ formed by the pro jections of the reference points are the same as the coordinates of r in the affine basis formed by the points in their own plane and thus are also the same as the coordinates of r i in the affine basis of δi formed by the projections of the reference points the ratio of the depths of p and q relative to the plane δ is simply the ratio pr qr not that deciding which point is actually visible requires orienting the line supporting the points p q r which is simply the epipolar line associated with the point r i a coherent orientation should be chosen for all epipolar lines this is easy since they are all parallel to each other note that this does not require explicitly computing the epipolar geometry given a first point pi one can orient the line pr and then use the same orientation for all other point correspon dences the orientations chosen should also be consistent over successive frames but this is not a problem since the direction of the epipolar lines changes slowly from one frame to the next sec the light field figure two images of a face synthesized using parameterized image varieties courtesy of yakup genc and one can simply choose the new orientation so that it makes an acute angle with the pre vious one examples of synthetic pictures constructed using this method are shown in figure the light field this section discusses a different approach to image based rendering whose only similarity with the techniques discussed in the previous section is that like them it does not require the construc tion of any implicit or explicit model of a scene let us consider for example a panoramic camera that optically records the radiance along rays passing through a single point and covering a full hemisphere see e g peri and nayar figure left it is possible to create panoramic cameras cylindrical mosaics figure constructing synthetic views of a scene from a fixed viewpoint application image based rendering chap figure the plenoptic function and the light field left the plenoptic function can be parameterized by the position p of the observer and the viewing direction v right the light field can be parameterized by the four parameters u v t defining a light slab in practice several light slabs are necessary to model a whole object and obtain full spherical coverage any image observed by a virtual camera whose pinhole is located at this point by mapping the original image rays onto virtual ones this allows a user to arbitrarily pan and tilt the virtual camera and interactively explore his or her visual environment similar effects can be obtained by stitching together close by images taken by a hand held camcorder into a mosaic see e g shum and szeliski figure middle or by combining the pictures taken by a camera panning and possibly tilting about its optical center into a cylindrical mosaic see e g chen figure right these techniques have the drawback of limiting the viewer motions to pure rotations about the optical center of the camera a more powerful approach can be devised by considering the plenoptic function adelson and bergen that associates with each point in space the wavelength dependent radiant energy along a ray passing through this point at a given time figure left the light field levoy and hanrahan is a snapshot of the plenoptic function for light traveling in vacuum in the absence of obstacles this relaxes the dependence of the radiance on time and on the position of the point of interest along the corresponding ray since radiance is constant along straight lines in a nonabsorbing medium and yields a representation of the plenoptic function by the radiance along the four dimensional set of light rays these rays can be parameterized in many different ways e g using the plu cker coordinates introduced in chapter but a convenient parameterization in the context of image based rendering is the light slab where each ray is specified by the coordinates of its intersections with two arbitrary planes figure right the light slab is the basis for a two stage approach to image based rendering during the learning stage many views of a scene are used to create a discrete version of the slab that can be thought of as a four dimensional lookup table at synthesis time a virtual camera is defined and the corresponding view is interpolated from the lookup table the quality of the synthe sized images depends on the number of reference images the closer the virtual view is to the reference images the better the quality of the synthesized image note that constructing the light slab model of the light field does not require establishing correspondences between images it should be noted that unlike most methods for image based rendering that rely on texture mapping and thus assume implicitly that the observed surfaces are lambertian light field techniques can be used to render under a fixed illumination pictures of objects with arbitrary brdfs sec the light field figure the acquisition of a light slab from images and the synthesis of new images from a light slab can be modeled via projective transformations between the x y image plane and the u v and t planes defining the slab in practice a sample of the light field is acquired by taking a large number of images and mapping pixel coordinates onto slab coordinates figure illustrates the general case the mapping between any pixel in the x y image plane and the corresponding areas of the u v and t plane defining a light slab is a planar projective transformation hardware or software based texture mapping can thus be used to populate the light field on a four dimensional rectangular grid in the experiments described in levoy and hanrahan light slabs are acquired in the simple setting of a camera mounted on a planar gantry and equipped with a pan tilt head so it can rotate about its optical center and always point toward the center of the object of interest in this context all calculations can be simplified by taking the u v plane to be the plane in which the camera optical center is constrained to remain at rendering time the projective mapping between the virtual image plane and the two planes defining the light slab can once again be used to efficiently synthesize new images fig ure shows sample pictures generated using the light field approach the top three im age pairs were generated using synthetic pictures of various objects to populate the light field the last pair of images was constructed by using the planar gantry mentioned earlier to acquire images of a toy lion grouped into four slabs each consisting of im ages an important issue is the size of the light slab representation the raw input images of the lion take of disk space there is of course much redundancy in these pictures as in the case of successive frames in a motion sequence a simple but effective two level approach to image de compression is proposed in levoy and hanrahan the light slab is first decomposed into four dimensional tiles of color values these tiles are encoded using vector quantization gersho and gray a lossy compression technique where the dimensional vectors representing the rgb values at the corners of the original tiles are replaced by a rel atively small set of reproduction vectors called codewords that best approximate in the mean squared error sense the input vectors the light slab is thus represented by a set of indexes in the codebook formed by all codewords in the case of the lion the codebook is relatively small and the size of the set of indexes is the second compression stage consists of applying the gzip implementation of entropy coding ziv and lempel to the codebook and the indexes the final size of the representation is only corresponding to a compres sion rate of at rendering time entropy decoding is performed as the file is loaded in main memory dequantization is performed on demand during display and it allows interactive refresh rates application image based rendering chap figure images synthesized with the light field approach reprinted from light field rendering by m levoy and p hanrahan proc siggraph oc acm inc included here by permission problems notes image based rendering is a quickly expanding field to close this chapter let us just mention a few alternatives to the approaches already mentioned in the previous sections the intersection of all of the cones that graze the surface of a solid forms its visual hull laurentini a solid is always contained in its visual hull which in turn is contained in the solid convex hull the volumetric approach to object modeling from registered silhouettes presented in section is aimed at constructing an approximation of the visual hull from a finite set of photographs variants use polyhedra or octrees martin and aggarwal connolly and stenstrom srivastava and ahuja to represent the cones and their intersection and include a commer cial system niem and buschmann for automatically constructing polyhedral models from images see also kutulakos and seitz for a related approach called space carving where empty voxels are iteratively removed using brightness or color coherence con straints the tangency constraint has been used in various approaches for reconstructing a surface from a continuous sequence of outlines under known or unknown camera motions arbogast and mohr cipolla and blake vaillant and faugeras cipolla et al boyer and berger cross et al joshi et al variants of the view interpolation method discussed in section include williams and chen and seitz and dyer transfer based approaches to image based rendering include besides those discussed in sec tion havaldar et al and avidan and shashua as briefly mentioned in sec tion a number of techniques have been developed for interactively exploring a user visual environment from a fixed viewpoint these include a commercial system quicktime vr de veloped at apple by chen and algorithms that reconstruct pinhole perspective images from panoramic pictures acquired by special purpose cameras see e g peri and nayar similar effects can be obtained in a less controlled setting by stitching together close by images taken by a hand held camcorder into a mosaic see e g irani et al shum and szeliski for images of distant terrains or cameras rotating about their optical center the mosaic can be constructed by registering successive pictures via planar homographies in this context estimating the optical flow i e the vector field of apparent image velocities at every image point a notion that has admittedly largely been ignored in this book may also prove important for fine registration and deghosting shum and szeliski variants of the light field approach discussed in section include mcmillan and bishop and gortler et al an excellent introduction to be zier arcs and patches and spline curves and surfaces can be found in farin problems given n point pn we recursively define the parametric curve pk t by t pi i i and pk t t pk t tpk t for k n and i n k we show in this exercise that pn t is the be zier curve of degree n associated with the n points pn this construction of a be zier curve is called the de casteljeau algorithm show that bernstein polynomials satisfy the recursion b n t t b n t tb n t with b t and by convention b n t when j or j n application image based rendering chap use induction to show that k k k p t b t pi j for k n and i n k consider a be zier curve of degree n defined by n control points pn we address here the problem of constructing the n control points qn of a be zier curve of degree n with the same shape this process is called degree elevation show that and j q j n pj j n pj for j n hint write that the same point is defined by the barycentric combinations associated with the two curves and equate the polynomial coefficients on both sides of the equation show that the tangent to the be zier curve p t defined by the n control points pn is n n pi t n b t pj pj j conclude that the tangents at the endpoints of a be zier arc are along the first and last line segments of its control polygon show that the construction of the points qi in section places these points in a plane that passes through the centroid o of the points ci fac ade photogrammetric module we saw in the exercises of chapter that the mapping between a line δ with plu cker coordinate vector and its image δ with homogeneous coordinates can be represented by ρδ here is a function of the model parameters and corresponding camera position and orientation depends on the assuming that the line δ has been matched with an image edge e of length l a convenient measure of the discrepancy between predicted and observed data is obtained by multiplying by l the mean squared distance separating the points of e from δ defining d t as the signed distance between the edge point p t t and the line δ show that e t dt d d d d where and denote the signed distances between the endpoints of e and δ if and denote the homogeneous coordinate vectors of these points show that d pt m and d pt m m m where a denotes the vector formed by the first two coordinates of the vector a in formulate the recovery of the camera and model parameters as a non linear least squares prob lem show that a basis for the eight dimensional vector space v formed by all affine images of a fixed set of points pn can be constructed from at least two images of these points when n hint use the matrix m m u n v n u m n m n where u j v j are the coordinates of the projection of the point pi into image number j i i show that the set of all projective images of a fixed scenes is an eleven dimensional variety problems show that the set of all perspective images of a fixed scene for a camera with constant intrinsic parameters is a six dimensional variety in this exercise we show that eq only admits two solutions show that eq can be rewritten as r x y where e x u y v and e and are coefficients depending on and the structure parameters show that the solutions of eq are given by x i cos arctan e r y i j e2 sin j and x ii y ii x i y i hint use a change of variables to rewrite eq as a system of trigonometric equa tions prof adriana kovashka university of pittsburgh august about the instructor born in sofia bulgaria got ba in at pomona college ca computer science media studies got phd in at university of texas at austin computer vision course website instructor adriana kovashka kovashka cs pitt edu please use at the beginning of your subject office sennott square office hours mw ta changsheng liu to be confirmed ta office hours to be determined by richard szeliski by kristen grauman and bastian leibe more resources available on course webpage your notes from class are your best study material slides are not complete with notes to learn about the basic computer vision tasks and approaches to get experience with some computer vision techniques to learn absolute basics of machine learning to think critically about vision approaches and to see connections between works and potential for improvement blitz introductions what is computer vision why do we care what are the challenges what is the current research like course structure and policies overview of topics if time blitz introductions blitz introductions sec what is your name tell us one fun thing about yourself i ll ask you more questions in computer vision done we see with our brains not with our eyes oliver sacks and others kristen grauman adapted automatic understanding of images and video computing properties of the world from visual data measurement algorithms and representations to allow a machine to recognize objects people scenes and activities perception and interpretation algorithms to mine search and interact with visual data search and organization kristen grauman real time stereo structure from motion pollefeys et al kristen grauman multi view stereo for community photo collections goesele et al slide credit l lazebnik vision for perception interpretation objects activities scenes locations text writing faces gestures motions emotions visual search organization query image or video archives relevant content graphics image processing artificial intelligence computer vision algorithms machine learning cognitive science images vision model graphics inverse problems analysis and synthesis as image sources multiply so do applications relieve humans of boring easy tasks human computer interaction perception for robotics autonomous agents organize and give access to visual content description of image content for the visually impaired fun applications e g transfer art styles to my photos why vision images and video are everywhere hours uploaded to youtube daily mil photos uploaded to flickr daily bil images indexed by google personal photo albums surveillance and security movies news sports medical and scientific images faces and digital cameras camera waits for everyone to smile to take a photo canon setting camera focus via face detection face recognition linking to info with a mobile device situated search yeh et al mit msr lincoln kooaba exploring photo collections snavely et al the matrix what dreams may come mocap for pirates of the carribean industrial light and magic source s seitz human joystick newsbreaker live assistive technology systems camera mouse boston college image guided surgery mit ai vision group fmri data golland et al navigation driver safety monitoring pool poseidon pedestrian detection merl viola et al surveillance farmbot io by myers et al iccv pirsiavash et al assessing the quality of actions eccv reed et al icml radford et al iclr obstacles kristen grauman read more about the history szeliski sec why is this problematic ill posed problem real world much more complex than what we can measure in images impossible to literally invert image formation process with limited information need information outside of this particular image to generalize what image portrays e g to resolve occlusion illumination object pose clutter occlusions intra class appearance viewpoint think again about the pixels challenges complexity thousands to millions of pixels in an image human recognizable object categories degrees of freedom in the pose of articulated objects humans billions of images indexed by google image search billion prints produced from digital camera images in million camera phones sold in about half of the cerebral cortex in primates is devoted to processing visual information felleman and van essen challenges limited supervision less more challenges vision requires reasoning antol et al vqa visual question answering iccv ok clearly the vision problem is deep and challenging time to give up active research area with exciting progress kristen grauman datasets today imagenet categories images microsoft coco categories images pascal categories images sun categories images some visual recognition problems recognition what is this recognition what objects do you see building balcony street truck carriage horse table person person car detection where are the cars activity what is this person doing scene is this an indoor scene instance which city which building visual question answering what are all these people participating in the latest at cvpr cvpr ieee conference on computer vision and pattern recognition unified real time object detection decoding physical sensation from a first person video question answering gatys et al cvpr convolutional neural networks images styles results seeing behind the camera identifying the authorship of a photograph thomas and kovashka cvpr is computer vision solved given an image we can guess with accuracy what object categories are shown resnet but we only answer why questions about images with accuracy why does it seem that it solved deep learning makes excellent use of massive data labeled for the task of interest but it hard to understand how it does so it doesn t work well when massive data is not available and your task is different than tasks for which data is available sometimes the manner in which deep methods work is not intellectually appealing but our smarter more complex methods perform worse course structure and policies course website schedule int roduct ion to comp x e cs pitt edu l r j introduction to vision oduction to computer vision fall location sennott square tim e fonday and wednesday ins tru ct o r adriana kovashka email kovashka at cs dotpitt dot edu use at the beginning of the subject line office sennott square office hours monda y and wednesday ta tbd email tbd ta offi ce hours tbd this page is under construction overview course description in this class students will learn the basics of modern computer vision the first major part of the course will co er fundamental concepts such as image formation image filtering edge detection ture description feature extraction and matching and grouping and fitting a brief intro to machine learning will follm in prep aration for the second course chapter on readings readings for class n will be posted by on the day of class n they provide background and more detailed explanations of topics we ll cover generally they provide more info than what you need for the exams sometimes i will post research papers as reading written hw assignments x each programming hw assignments x each midterm exam final exam participation short answer or multiple choice answers will help ensure that you understand the most recent topics covered so you can do the programming homework due two days before the programming assignments free late days do not apply i grade these assignments implement a technique or practice concepts we discussed one assignment roughly every week please comment your code free late days apply see next slide ta grades these on programming assignments only you get free late days i e you can submit homework a total of days late for example you can submit one problem set hours late and another hours late once you ve used up your free late days you will incur a penalty of from the total project credit possible for each late day a late day is anything from minute to hours navigate to the courseweb page for click on assignments and the corresponding hw id your written answers should be a single pdf doc docx file your code should be a single zip file with m files and images results if requested name the file extension homework is due at on the due grades will appear on courseweb one mid term and one final exam midterm counts for less because based on only meat lectures excluding intro matlab reviews the final exam will focus on the latter half of the course exams will be preceded by review sessions if our schedule allows it there will be no make up exams unless you or a close relative is seriously ill excludes cold flu of grade will be based on attendance and participation answer questions asked by instructor and others ask meaningful questions ask or answer questions on piazza bring in relevant articles about recent developments in computer vision feedback is welcome you will work individually the work you turn in must be your own work you can discuss the assignments with your classmates but do not look at their code or answers you cannot use posted solutions search for code on the internet or use matlab implementations of something you are asked to write when in doubt ask the instructor or ta plagiarism will cause you to fail the class and receive disciplinary penalty if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services drs william pitt union for asl users as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course if you have a medical condition which will prevent you from doing a certain assignment you must inform the instructor of this before the deadline you must then submit documentation of your condition within a week of the assignment deadline there will be no make up exams to ensure the free and open discussion of ideas students may not record classroom lectures discussion and or activities without the advance written permission of the instructor and any such recording properly approved in advance can be used solely for the student own private use warnings this class is a lot of work this time i ve opted for shorter more manageable hw assignments but there is more of them i expect you d be spending hours on homework each week but you get to understand algorithms and concepts in detail some parts will be hard and require that you pay close attention i will use the written hw to gauge how you re doing i will also pick on students randomly to answer questions use instructor and ta office hours you will learn a lot programming assignments will be in matlab since that very common in computer vision and is optimized for work with matrices matlab also has great documentation is just matlab practice some people won t like matlab i like it you will learn a new programming language evidence that you should take my warnings seriously is due on monday labor day is due on wednesday sept if this doesn t sound like your cup of coffee drop deadline is september next friday note to waitlisted students keep coming to class if it sounds interesting questions overview of topics transforming and describing images textures colors edges kristen grauman detecting distinctive repeatable features describing images with local statistics matching features and regions across images how does light in world project to form images hartley and zisserman multi view geometry matching invariant features stereo vision clustering segmentation fitting what parts belong together fig from shi et al recognizing objects and categories learning techniques detecting novel instances of objects classifying regions as one of several categories describing the high level properties of objects allows recognition of unseen objects state of the art on many recognition tasks image prediction recurrent neural networks sequence processing e g question answering motion and tracking tracking objects video analysis kristen grauman tomas izo pose and actions automatically annotating human pose joints recognizing actions in first person video matlab tutorial out will read in class due on monday read szeliski sec enroll for piazza cs intro to computer vision matlab tutorial plan for today intro course basics refresher overview of topics what are images linear algebra lightning quick over re view matlab tutorial outro overview of homework w and p if time course info course website instructor adriana kovashka kovashka cs pitt edu please use at the beginning of your subject office sennott square office hours mw tas ta grader yuhuan jiang yuhuan cs pitt edu additional tas office hours only chris thomas and nils murrugarra tas office hours tbd your homework fill out the doodle at ignore dates look at days of the week min increments matlab you can get it for free through my pitt edu my resources software downloads get the latest version mostly because that what i use make sure to check the image processing toolbox computer vision system toolbox and statistics and machine learning toolbox boxes during installation easiest to install all course components written hw assignments x each programming hw assignments x each midterm exam final exam participation the rest of the course policies read the course website carefully warnings this class is a lot of work this time i ve opted for shorter more manageable hw assignments but there is more of them i expect you d be spending hours on homework each week but you get to understand algorithms and concepts in detail some parts will be hard and require that you pay close attention i will use the written hw to gauge how you re doing i will also pick on students randomly to answer questions use instructor and tas office hours you will learn a lot programming assignments will be in matlab since that very common in computer vision and is optimized for work with matrices matlab also has great documentation is just matlab practice some people won t like matlab i like it you will learn a new programming language what is the role of datasets whatever our computer vision algorithms learn they will learn from some set of datasets of images we will also use the datasets to test our algorithms why did i say it hard to understand deep learning a k a deep neural networks because most deep learning methods just take an image as input and output predictions and they learn how to represent and examine the image on their own so they appear to be black boxes questions overview of topics the next slides will be very quick then we ll slow down ready transforming and describing images textures colors edges kristen grauman detecting distinctive repeatable features describing images with local statistics matching features and regions across images how does light in world project to form images hartley and zisserman multi view geometry matching invariant features stereo vision clustering segmentation fitting what parts belong together fig from shi et al recognizing objects and categories learning techniques detecting novel instances of objects classifying regions as one of several categories describing the high level properties of objects allows recognition of unseen objects state of the art on many recognition tasks image prediction recurrent neural networks sequence processing e g question answering motion and tracking tracking objects video analysis kristen grauman tomas izo pose and actions automatically annotating human pose joints recognizing actions in first person video what are images what are images in matlab matlab treats images as matrices of numbers to proceed let talk very briefly about how images are formed image formation film slide credit derek hoiem a digital camera replaces film with a sensor array each cell in the array is light sensitive diode that converts photons to electrons slide by steve seitz sample the space on a regular grid quantize each sample round to nearest integer sample the space on a regular grid quantize each sample round to nearest integer what does quantizing signal look like image thus represented as a matrix of integer values color images rgb color space split image into three channels r g b adapted from kristen grauman images in matlab color images represented as a matrix with multiple channels if grayscale suppose we have a nxm rgb image called im im top left pixel value in r channel im y x b y pixels down x pixels to right in the bth channel im n m bottom right pixel in b channel imread filename returns a image values to convert to double format values to with double row column 96 96 r 42 adapted from derek hoiem linear algebra very brief all you need to know for most of course exception last three lectures before midterm some more review then raise your hand if you ve had a linear algebra course matlab vectors and matrices are just collections of ordered numbers that represent something movements in space scaling factors word counts movie ratings pixel brightnesses etc we ll define some common uses and standard operations on them a column vector where a row vector where denotes the transpose operation you ll want to keep track of the orientation of your vectors when programming in matlab you can transpose a vector v in matlab by writing v vectors can represent an offset in or space points are just vectors from the origin data can also be treated as a vector such vectors don t have a geometric interpretation but calculations like distance still have value a matrix is an array of numbers with size 𝑚 by 𝑛 i e m rows and n columns if we say that is square addition can only add matrices with matching dimensions or a scalar to a matrix scaling inner product dot product of vectors multiply corresponding entries of two vectors and add up the result we won t worry about the geometric interpretation for now multiplication the product ab is each entry in the result is that row of a dot product with that column of b multiplication example each entry of the matrix product is made by taking the dot product of the corresponding row in the left matrix with the corresponding column in the right one transpose flip matrix so row becomes column a useful identity special matrices identity matrix i square matrix along diagonal elsewhere i another matrix that matrix diagonal matrix square matrix with numbers along diagonal elsewhere a diagonal another matrix scales the rows of that matrix minute break matlab tutorial we ll cover parts do parts at home finish the matlab tutorial on your own post on piazza if questions no class monday labor day but due wednesday image filtering due reading for wednesday szeliski sec reminder fill out doodle cs intro to computer vision image filtering slides from kristen grauman plan for today matlab tutorial part image filtering overview note and are out questions from graded with comments and answers see courseweb practical skills see next what are programming assignments like see what is my research like see objectives be more comfortable with matlab know one way to remove noise from images filtering know how to resize images in a content aware way you will be able to remove noise from images detect basic patterns such as lines compute a meaningful representation of the image beyond pixels that allows you to do higher level tasks e g finding cars stitch panoramas from multiple images of the same object automatically recognize different types of scenes e g beaches vs forests vs cities detect faces in images track moving objects use and understand support vector machines use neural networks automatically group data etc let x be an axb matrix y be an bxc matrix then z x y is an axc matrix second dimension of first matrix and first dimension of first matrix have to be the same for matrix multiplication to be possible matrix multiplication is not commutative x y y x but is distributive over addition and associative practice let x be an matrix let factorize it into matrices inner vs outer vs matrix vs element wise product x y column vectors x y matrices mxn x y scalars xt y x y inner product x scalar x y x yt outer product x matrix x y matrix product x y element wise product images in matlab color images represented as a matrix with multiple channels if grayscale suppose we have a nxm rgb image called im im top left pixel value in r channel im y x b y pixels down rows x pixels to right cols in bth channel im n m bottom right pixel in b channel imread filename returns a image values to row column 42 92 96 88 91 95 37 88 58 58 84 58 39 92 91 96 85 48 37 88 66 42 90 90 61 79 93 91 93 r 92 95 91 91 92 95 79 85 49 74 82 93 49 66 42 90 79 90 61 79 73 93 97 49 89 93 91 49 56 66 42 73 90 99 79 73 90 61 69 79 73 93 97 91 94 89 49 41 77 89 99 93 adapted from derek hoiem matlab tutorial we ll cover parts do parts at home sliding window detector enter noise we talked about how the same object will look very different across images even multiple images of the same static scene will not be identical how could we reduce the noise i e give an estimate of the true intensities what if there only one image common types of noise salt and pepper noise random occurrences of black and white pixels impulse noise random occurrences of white pixels gaussian noise variations in intensity drawn from a gaussian normal distribution source s seitz gaussian noise noise randn size im sigma output im noise what is impact of the sigma fig m hebert let replace each pixel with an average of all the values in its neighborhood assumptions expect pixels to be like their neighbors expect noise processes to be independent from pixel to pixel let replace each pixel with an average of all the values in its neighborhood moving average in can add weights to our moving average weights non uniform weights 16 board compute a function of the local neighborhood at each pixel in the image function specified by a filter or mask saying how to combine values from neighbors element wise multiplication uses of filtering enhance an image denoise resize etc extract information texture edges etc detect patterns template matching adapted from derek hoiem say the averaging window size is x attribute uniform weight to each pixel loop over all pixels in neighborhood around image pixel f i j now generalize to allow different weights depending on neighboring pixel relative position non uniform weights this is called cross correlation denoted filtering an image replace each pixel with a linear combination of its neighbors the filter kernel or mask h u v is the prescription for the weights in the linear combination what values belong in the kernel h for the moving average example box filter depicts box filter white high value black low value original filtered what if the filter size was x instead of x what if we want nearest neighboring pixels to have the most influence on the output removes high frequency components from the image low pass filter source s seitz what parameters matter here size of kernel or mask note gaussian function has infinite support but discrete filters use finite kernels σ with x kernel σ with x kernel what parameters matter here variance of gaussian determines extent of smoothing σ with x kernel σ with x kernel how big should the filter be values at edges should be near zero important rule of thumb for gaussian set filter half width to about σ source derek hoiem mesh h imagesc h outim imfilter im h correlation imshow outim outim parameter σ is the scale width spread of the gaussian kernel and controls the amount of smoothing for sigma h fspecial gaussian fsize sigma out imfilter im h imshow out pause end smoothing values positive sum to overall intensity same as input amount of smoothing proportional to mask size remove high frequency components low pass filter predict the outputs using correlation filtering original original filtered no change original original shifted left by pixel with correlation original original blur with a box filter original original sharpening filter accentuates differences with local average final example slide credit derek hoiem homework seam carving content aware image resizing resize effect content aware resizing traditional resizing intuition content aware resizing to reduce or increase size in one dimension remove irregularly shaped non straight seams optimal solution via dynamic programming energy f want to remove seams where they won t be very noticeable measure energy as gradient magnitude horizontal vertical change choose seam based on minimum total energy path across image subject to connectedness energy f let a vertical seam consist of h positions that form an connected path let the cost of a seam be cost energy f i si optimal seam minimizes this cost min cost compute it efficiently with dynamic programming compute the cumulative minimum energy for all possible connected seams at each entry i j m i j energy i min m i j m i j m i j energy matrix gradient magnitude m matrix cumulative min energy for vertical seams then min value in last row of m indicates end of the minimal connected vertical seam backtrack up from there selecting min of above in m computing horizontal seams is analogous m i j energy i j min m i j m i j m i j cumulative energy from m i j energy i min m i j m i j m i j now backtrack computing cumulative energy and finding the optimal seam with back tracking is implemented for you but you still have to understand how to use it original image energy map blue low energy red high energy john phillips linear filters useful for enhancing images smoothing removing noise box filter gaussian filter impact of scale width of smoothing filter detecting patterns e g gradients content aware image resizing using image energy and gradients more filters properties of filters applications texture representation and synthesis pattern matching reminders and due monday cs intro to computer vision filtering and texture prof adriana kovashka university of pittsburgh september reminder due tonight types of filters linear smoothing other non linear median texture representation with filters compute a function of the local neighborhood at each pixel in the image function specified by a filter or mask saying how to combine values from neighbors element wise multiplication uses of filtering enhance an image denoise resize etc extract information texture edges etc detect patterns template matching adapted from derek hoiem say the averaging window size is x attribute uniform weight loop over all pixels in neighborhood around to each pixel image pixel f i j now generalize to allow different weights depending on neighboring pixel relative position non uniform weights this is called cross correlation denoted filtering an image replace each pixel with a linear combination of its neighbors the filter kernel or mask h u v is the prescription for the weights in the linear combination what values belong in the kernel h for the moving average example box filter depicts box filter white high value black low value original filtered what if the filter size was x instead of x f image h filter boundary issues what is the size of the output matlab output size options shape full output size is larger than the size of f shape same output size is same as f shape valid output size is difference of sizes of f and h discontinued full same valid adapted from s lazebnik what about near the edge the filter window might fall off the edge of the image in same or full need to extrapolate methods clip filter black wrap around copy edge reflect across edge what about near the edge the filter window might fall off the edge of the image in same or full need to extrapolate methods matlab clip filter black imfilter f g wrap around imfilter f g circular copy edge imfilter f g replicate reflect across edge imfilter f g symmetric convolution flip the filter in both dimensions bottom to top right to left then apply cross correlation notation for convolution operator kristen grauman convolution cross correlation for a gaussian or box filter how will the outputs differ f convolution u v h i j f convolution u v v h i j f convolution u v v v h i j f convolution u v v v u v h i j cross correlation u v f h i j cross correlation u v v f h i j cross correlation u v v v f h i j cross correlation f i j u v v v u v h properties of convolution commutative f g g f associative f g h f g h distributes over addition f g h f g f h scalars factor out kf g f kg k f g identity unit impulse e f e f separability in some cases filter is separable and we can factor into two steps convolve all rows convolve all columns separability example filtering center location only the filter factors into an outer product of filters perform filtering along rows followed by filtering along the remaining column plan for today filters math and properties types of filters linear other non linear median texture representation with filters gaussian filter what if we want nearest neighboring pixels to have the most influence on the output source s seitz smoothing with a gaussian what parameters matter here size of kernel or mask note gaussian function has infinite support but discrete filters use finite kernels σ with x kernel σ with x kernel what parameters matter here variance of gaussian determines extent of smoothing σ with x kernel σ with x kernel how big should the filter be values at edges should be near zero important rule of thumb for gaussian set filter half width to about σ source derek hoiem mesh h imagesc h outim imfilter im h correlation imshow outim outim parameter σ is the scale width spread of the gaussian kernel and controls the amount of smoothing for sigma h fspecial gaussian fsize sigma out imfilter im h imshow out pause end smoothing values positive sum to overall intensity same as input amount of smoothing proportional to mask size remove high frequency components low pass filter filters math and properties types of filters linear smoothing non linear median texture representation with filters predict the outputs using correlation filtering kristen grauman original sharpening filter accentuates differences with local average source d lowe kristen grauman kristen grauman aude oliva antonio torralba philippe g schyns siggraph gaussian filter oliva a torralba p g schyns siggraph laplacian filter sharpening unit impulse gaussian laplacian of gaussian kristen grauman filters for computing gradients slide credit derek hoiem filters math and properties types of filters linear smoothing other texture representation with filters no new pixel values introduced removes spikes good for impulse salt pepper noise non linear filter median filter is edge preserving salt and pepper noise plots of a row of the image matlab output im im h w median filtered source m hebert minute break plan for today filters math and properties types of filters linear smoothing other non linear median texture what defines a texture kristen grauman kristen grauman important for how we perceive objects often indicative of a material properties can be important appearance cue especially if shape is similar across objects to represent objects we want a feature one step above building blocks of filters edges textures are made up of repeated local patterns so find the patterns use filters that look like patterns spots bars raw patches consider magnitude of response describe their statistics within each local window mean standard deviation histogram original image original image original image dimension mean d dx value statistics to summarize patterns in small windows with primarily horizontal edges both windows with small gradient in both directions windows with primarily vertical edges statistics to summarize patterns in small original image kristen grauman derivative filter responses squared far dissimilar text close similar text dimension mean d dx value kristen grauman statistics to summarize patterns in small windows d a b dimension a b dimension distance reveals how dissimilar texture b from window a is from texture in window b our previous example used two filters and resulted in a dimensional feature vector to describe texture in a window x and y derivatives revealed something about local structure we can generalize to apply a collection of multiple d filters a filter bank then our feature vectors will be d dimensional still can think of nearness farness in feature space computing distances with d dimensional features d a b euclidean distance filter banks scales what filters to put in the bank typically we want a combination of scales and orientations different types of patterns matlab code available for these examples filter bank d d d d d ej i l l l j lj j 20 multivariate gaussian image from vectors of texture responses we can form a feature vector from the list of responses at each pixel gives us a representation of the pixel image you try can you match the texture to the response filters a mean abs responses b c derek hoiem representing texture by mean abs response filters mean abs responses derek hoiem classifying materials stuff figure by varma zisserman kristen grauman summary filters useful for enhancing images smoothing removing noise e g box filter linear gaussian filter linear median filter detecting patterns e g gradients texture is a useful property that is often indicative of materials appearance cues texture representations attempt to summarize repeating patterns of local structure filter banks useful to measure redundant variety of structures in local neighborhood cs intro to computer vision texture representation image pyramids prof adriana kovashka university of pittsburgh september reminders announcements due tonight out shuffle plan for today filtering application to subsampling image pyramids detecting interesting content start convolution vs correlation convolution cross correlation texture what defines a texture kristen grauman kristen grauman important for how we perceive objects often indicative of a material properties can be important appearance cue especially if shape is similar across objects to represent objects we want a feature one step above building blocks of filters edges textures are made up of repeated local patterns so find the patterns use filters that look like patterns spots bars raw patches consider magnitude of response describe their statistics within each local window mean standard deviation histogram original image original image original image dimension mean d dx value statistics to summarize patterns in small windows with primarily horizontal edges both windows with small gradient in both directions windows with primarily vertical edges statistics to summarize patterns in small original image kristen grauman derivative filter responses squared far dissimilar text close similar text dimension mean d dx value kristen grauman statistics to summarize patterns in small windows d a b dimension a b dimension distance reveals how dissimilar texture b from window a is from texture in window b our previous example used two filters and resulted in a dimensional feature vector to describe texture in a window x and y derivatives revealed something about local structure we can generalize to apply a collection of multiple d filters a filter bank then our feature vectors will be d dimensional still can think of nearness farness in feature space scales what filters to put in the bank typically we want a combination of scales and orientations different types of patterns matlab code available for these examples filter bank d d d d d ej i l l 20 20 20 20 20 201hil 20 l j lj j 20 20 20 20 20 20 20 20 20 20 20 20 20 multivariate gaussian image from 50 50 50 50 150 250 50 50 150 250 50 150 250 50 50 100 150 250 50 100 150 200 250 50 50 100 150 200 250 vectors of texture responses we can form a feature vector from the list of responses at each pixel gives us a representation of the pixel image you try can you match the texture to the response filters a b c representing texture by mean abs response filters vectors of texture responses the frequency of image patch responses in an image tells us something about the image if my patches tend to respond strongly to blobs and another image patches respond strongly to vertical edges then that image and i are probably of different materials how to capture one idea concatenate from the previous slide for all image pixels problems means or histograms of feature responses instead of concatenating compute the mean across all image pixels to each of the filters instead of concatenating count assume all responses between and how many times within the image did i see an response between and between and how many times did i see response between and concatenate these counts into a histogram tradeoffs classifying materials stuff figure by varma zisserman kristen grauman minute break plan for today filtering representing texture detecting interesting content start sampling why does a lower resolution image still make sense to us what do we lose derek hoiem image subsampling by a factor of throw away every other row and column to create a size image derek hoiem example sinewave example sinewave sub sampling may be dangerous characteristic errors may appear wagon wheels rolling the wrong way in movies striped shirts look funny on color television sampling and aliasing nyquist shannon sampling theorem when sampling a signal at discrete intervals the sampling frequency must be fmax fmax max frequency of the input signal this will allows to reconstruct the original perfectly from the sampled version good bad anti aliasing solutions sample more often get rid of all frequencies that are greater than half the new sampling frequency will lose information but it better than aliasing apply a smoothing filter algorithm for downsampling by factor of start with image h w apply low pass filter imfilter image fspecial gaussian sample every other pixel end end gaussian filter sample anti aliasing forsyth and ponce subsampling without pre filtering zoom zoom subsampling with gaussian pre filtering gaussian g g subsampling away why would we want to do this can we reconstruct the original from the laplacian pyramid derek hoiem laplacian filter unit impulse gaussian laplacian of gaussian plan for today filtering representing texture application to subsampling image pyramids an image is a set of pixels not invariant to small changes translation illumination etc some parts of an image are more important than others what do we want to represent yarbus eye tracking choosing distinctive interest ing points if you wanted to meet a friend would you say let meet on campus let meet on green street let meet at green and wright corner detection or if you were in a secluded area let meet in the plains of akbar let meet on the side of mt doom let meet on top of mt doom blob valley peak detection where would you tell your friend to meet you where would you tell your friend to meet you suppose you have to click on some point go away and come back after i deform the image and click on the same points again which points would you choose original prof adriana kovashka university of pittsburgh september graded graded with some comments on submission itself please read my feedback i didn t respond to you on the last questions my silence does not mean you re correct try to be concise without failing to give me the answer if you have or less you should be concerned the book is often a useful resource laplacian vs sobel m lots of questions about is it ok if i do this you don t have to worry so much about doing exactly what i said as long as you do the core task of the assignment after it doesn t matter how you print or whether you put multiple figures in one etc how long did it take what took the most time what was most fun what was most annoying enter socrative separability example filtering center location only the filter factors into an outer product of filters perform filtering along rows followed by filtering along the remaining column kristen grauman plan for today feature detection keypoint extraction corner detection blob detection start feature description of detected features problems with pixel representation not invariant to small changes translation illumination etc some parts of an image are more important than others what do we want to represent local features local means that they only cover a small part of the image there will be many local features detected in an image later we ll talk about how to use those to compute a representation of the whole image local features usually exploit image gradients rarely color we ll revisit this statement with cnns local features desired properties locality a feature occupies a relatively small area of the image robust to clutter and occlusion repeatability and flexibility the same feature can be found in several images despite geometric photometric transformations robustness to expected variations maximize correct matches distinctiveness each feature has a distinctive description minimize wrong matches compactness and efficiency many fewer features than image pixels interest ing points note interest points keypoints also sometimes called features many applications image search which points would allow us to match images between query and database recognition which patches are likely to tell us something about object category reconstruction how to find correspondences across different views tracking which points are good to track interest points suppose you have to click on some point go away and come back after i deform the image and click on the same points again which points would you choose original where would you tell your friend to meet you corner detection where would you tell your friend to meet you blob detection find a set of distinctive key points define a region around each keypoint window compute a local descriptor from the region d f a fb t match descriptors adapted from k grauman b leibe we want to detect points that are repeatable and distinctive repeatable so that if images are slightly different we can still retrieve them distinctive so we don t retrieve irrelevant content adapted from d hoiem step extract features step match features step extract features step match features step align images no chance to find true matches yet we have to be able to run the detection procedure independently per image we want to detect at least some of the same points in both images want repeatability of the interest operator we want to be able to reliably determine which point goes with which want operator distinctiveness must provide some invariance to geometric and photometric differences between the two views without finding many false matches corners as distinctive interest points we should easily recognize the keypoint by looking through a small window shifting a window in any direction should give a large change in intensity corners as distinctive interest points we should easily recognize the keypoint by looking through a small window shifting a window in any direction should give a large change in intensity corners as distinctive interest points we should easily recognize the keypoint by looking through a small window shifting a window in any direction should give a large change in intensity corners as distinctive interest points we should easily recognize the keypoint by looking through a small window shifting a window in any direction should give a large change in intensity corners as distinctive interest points we should easily recognize the keypoint by looking through a small window shifting a window in any direction should give a large change in intensity what points would you choose grauman window averaged squared change of intensity induced by shifting the image data by u v window function w x y or in window outside gaussian window averaged squared change of intensity induced by shifting the image data by u v e u v expanding i x y in a taylor series expansion we have for small shifts u v a quadratic approximation to the error surface between a patch and itself shifted by u v where m is a matrix computed from image derivatives m w x y ix ix ix i y x y i y i y notation i i x x i i y y i x i y i i x y since m is symmetric we have m x x t mxi i xi the eigenvalues of m reveal the amount of intensity change in the two principal orthogonal gradient directions in the window edge corner and are large flat region and are small measure of corner response k empirical constant k compute image gradients ix and iy for all pixels for each pixel compute by looping over neighbors x y compute k empirical constant k find points with large corner response function r r threshold take the points of locally maximum r as the detected feature points i e pixels where r is bigger than for all the or neighbors corner response at every pixel effect a very precise corner detector cs intro to computer vision local feature detection cont d description prof adriana kovashka university of pittsburgh september announcements graded due tonight please read the entire assignment before you start let make sure we re on the same page for part i problems concerns out plan for today feature detection keypoint extraction corner detection recap properties blob detection feature description of detected features local features desired properties locality a feature occupies a relatively small area of the image robust to clutter and occlusion repeatability and flexibility the same feature can be found in several images despite geometric photometric transformations robustness to expected variations maximize correct matches distinctiveness each feature has a distinctive description minimize wrong matches compactness and efficiency many fewer features than image pixels adapted from k grauman and d hoiem corners as distinctive interest points we should easily recognize the keypoint by looking through a small window shifting a window in any direction should give a large change in intensity flat region no change in all directions edge no change along the edge direction corner significant change in all directions efros d frolova d simakov harris detector algorithm compute image gradients ix and iy for all pixels for each pixel compute by looping over neighbors x y compute k empirical constant k find points with large corner response function r r threshold non max suppression take the points of locally maximum r as the detected feature points i e pixels where r is bigger than for all the or neighbors frolova d simakov corner response at every pixel effect a very precise corner detector a function is invariant under a certain family of transformations if its value does not change when a transformation from this family is applied to its argument a function is covariant when it commutes with the transformation i e applying the transformation to the argument of the function has the same effect as applying the transformation to the output of the function for example the area of a surface is invariant under rotations since rotating a surface does not make it any smaller or bigger but the orientation of the major axis of inertia of the surface is covariant under the same family of transformations since rotating a surface will affect the orientation of its major axis in exactly the same way local invariant feature detectors a survey by tinne tuytelaars and krystian mikolajczyk in foundations and trends in computer graphics and vision vol no chapter i a i b only derivatives are used invariance to intensity shift i i b intensity scaling i a i r r threshold x image coordinate x image coordinate derivatives and window function are shift invariant second moment ellipse rotates but its shape i e eigenvalues remains the same invariant to image scale image zoomed image corner all points will be classified as edges lazebnik problem how do we choose corresponding circles independently in each image do objects in the image have a characteristic scale that we can identify frolova d simakov solution design a function on the region which is scale invariant has the same shape even if the image is resized take a local maximum of this function f f scale adapted from a torralba region size region size a good function for scale detection has one stable sharp peak f f f region size region size region size for usual images a good function would be a one which responds to contrast sharp local intensity change torralba f im x f im x how to find corresponding patch sizes function responses for increasing scale scale signature function responses for increasing scale scale signature function responses for increasing scale scale signature function responses for increasing scale scale signature function responses for increasing scale scale signature function responses for increasing scale scale signature f im x f im x laplacian of gaussian blob detector laplacian of gaussian circularly symmetric operator for blob detection in g g g grauman we can approximate the laplacian with a difference of gaussians more efficient to implement laplacian dog g x y k g x y difference of gaussians allows detection of increasingly coarse detail difference of gaussian efficient computation computation in gaussian scale pyramid sampling with step original image k grauman b leibe find local maxima in position scale space of difference of gaussian position scale space adapted from k grauman b leibe find places where x greater than all of its neighbors in green list of x y results difference of gaussian k grauman b leibe additional references survey paper on local features local invariant feature detectors a survey by tinne tuytelaars and krystian mikolajczyk in foundations and trends in computer graphics and vision vol no mostly chapters making harris detection scale invariant and more on scale invariance indexing based on scale invariant interest points by krystian mikolajczyk and cordelia schmid in iccv raw patches as local descriptors the simplest way to describe the neighborhood around an interest point is to write down the list of intensities to form a feature vector but this is very sensitive to even small shifts rotations e g scale translation rotation tuytelaars scale invariant feature transform sift descriptor lowe iccv histogram of oriented gradients captures important texture information robust to small translations affine deformations k grauman b leibe computing gradients l the image intensity tan α 𝑜𝑝𝑝𝑜𝑠𝑖𝑡𝑒 𝑠𝑖𝑑𝑒 𝑎𝑑𝑗𝑎𝑐𝑒𝑛𝑡 𝑠𝑖𝑑𝑒 m x y sqrt θ x y atan m x y sqrt θ x y atan m x y sqrt 41 θ x y atan basic idea take square window around detected feature compute gradient orientation for each pixel create histogram over edge orientations weighted by magnitude that your feature descriptor full version divide the window into a grid of cells case shown below quantize the gradient orientations i e snap each gradient to one of angles each gradient contributes not just but magnitude gradient to the histogram i e stronger gradients contribute more cells orientations dimensional descriptor for each detected feature full version divide the window into a grid of cells case shown below quantize the gradient orientations i e snap each gradient to one of angles each gradient contributes not just but magnitude gradient to the histogram i e stronger gradients contribute more cells orientations dimensional descriptor for each detected feature normalize clip threshold normalize to normalize the descriptor after normalizing we have such that making descriptor rotation invariant rotate patch according to its dominant gradient orientation this puts the patches into a canonical orientation grauman image from matthew brown sift is robust can handle changes in viewpoint up to about degree out of plane rotation can handle significant changes in illumination sometimes even day vs night below fast and efficient can run in real time can be made to work without feature detection resulting in dense sift more points means robustness to occlusion one commonly used implementation adapted from s seitz images from s seitz object recognition indexing and retrieval robot navigation reconstruction wide baseline stereo motion tracking image alignment panoramas and mosaics adapted from k grauman and l lazebnik keypoint detection repeatable and distinctive corners blobs stable regions laplacian of gaussian automatic scale selection descriptors robust and selective histograms for robustness to small shifts and translations sift descriptor adapted from d hoiem k grauman prof adriana kovashka university of pittsburgh september post mortem matlab of you reviewed of it please review the entire tutorial asap how long did take answer on socrative what did you learn from it what took the most time plan for today feature detection wrap up matching features indexing features visual words application to image retrieval matching local features image image to generate candidate matches find patches that have the most similar appearance e g lowest feature euclidean distance simplest approach compare them all take the closest or closest k or within a thresholded distance robust matching image image at what euclidean distance value do we have a good match to add robustness to matching can consider ratio distance to best match distance to second best match if low first match looks good if high could be ambiguous match matching sift descriptors nearest neighbor euclidean distance threshold ratio of nearest to nearest descriptor lowe ijcv efficient matching so far we discussed matching across just two images what if you wanted to match a query feature from one image to all frames in a video or to a giant database with potentially thousands of features per image and hundreds to millions of images to search how to efficiently find those that are relevant to a new image each patch region has a descriptor which is a point in some high dimensional feature space e g sift when we see close points in feature space we have similar descriptors which indicates similar local content grauman database images indexing local features inverted file index for text documents an efficient way to find all pages on which a word occurs is to use an index we want to find all images in which a feature occurs to use this idea we ll need to map our features to visual words k grauman extract some local features from a number of images e g sift descriptor space each point is dimensional each point is a local descriptor e g sift feature vector quantize the space by grouping clustering the features note for now we ll treat clustering as a black box visual words patches on the right regions used to compute sift if i group these each group of patches will belong to the same visual word figure from sivic zisserman iccv visual words for indexing map high dimensional descriptors to tokens words by quantizing the feature space each cluster has a center determine which word to assign to each new image region by finding the closest cluster center database images are loaded into the index by mapping words to image numbers when will this indexing process give us a gain in efficiency for a new query image we can figure out which database images share a word with it and retrieve those images as matches we can call this retrieval process instance recognition adapted from k grauman of all the sensory impressions proceeding to the brain the visual experiences are the dominant ones our perception of the world around us is based essentially on the messages that reach the brain from our eyes for a long time it was thought that the retinal image was trasnsemnittseod proyi ntbbryapionin t to visual centers in thveisbruaian l thpe ecerrceberapl tcioortnex was a movie screen so to speak upon which the image inrtehteienyae lw acseprroejebctreadl tchororutgehxth e discoveries oef hyueb elcaendllw oiepsetliwceanlow know that behind the origin of the visual perception in the brain there is a considerably more complicatheducbouersle owf eiveesntes lby following the visual impulses along their path to the various cell layers of the optical cortex hubel and wiesel have been able to demonstrate that the message about the image falling on the retina undergoes a step wise analysis in a system of nerve cells stored in columns in this system each cell has its specific function and is responsible for a specific detail in the pattern of the retinal image china is forecasting a trade surplus of to this year a threefold increase on the commerce ministry said the surplus would be created by a predicted jump in exports to compared with a rise in imports to the figcurhesinarae likterlay dtoefu rther annoy thesuusr pwhluichsh acsolomngmargeurecdeth at china exports are unfairly helped by a deliberateelyxupnoderrtvsal ueimd ypuaonr tbse ijuings agrees tyheusaunrp lubs ias ntoko hidgho mbutessaytsicth e yuan is only one factor bank of china governor zhou xiaochuan said the country also needed to dtormaodreet ovbaoolustedomestic demand so more goods stayed within the country china increased the value of the yuan against the dollar by in july and permitted it to trade within a narrow band but the us wants the yuan to be allowed to trade freely however beijing has made it clear that it will take its time and tread carefully before allowing the yuan to rise further in value iccv short course l fei fei summarize entire image based on its distribution histogram of word occurrences analogous to bag of words representation commonly used for documents feature patches visual words rank images by normalized scalar product between their occurrence counts nearest neighbor search for similar images 𝑠𝑖𝑚 𝑉 𝑑𝑗 𝑖 𝑞 𝑖 d q for vocabulary of v words sim q dot q norm norm q flexible to geometry deformations viewpoint compact summary of image content very good results in practice basic model ignores geometry must verify afterwards or encode via features background and foreground mixed when bag covers whole image optimal vocabulary formation remains unclear adapted from k grauman inverted file index and bags of words similarity offline extract features in database images cluster them to find words make index extract words in query extract features and map each to closest cluster center use inverted file index to find frames relevant to query for each relevant frame rank them by comparing word counts of query and frame adapted from k grauman one more trick tf idf weighting term frequency inverse document frequency describe image frame by frequency of each word within it but downweight words that appear often in the database standard weighting for text retrieval number of occurrences of word i in document d number of words in document d normalized bag of words total number of documents in database number of documents in which word i occurs adapted from k grauman bags of words for content based image retrieval slide from andrew zisserman example retrieved shots r i ilvk t t j stru t fnun e 90 key frame end frame st ut fnune f key frame f ti c t r i r shut frame fra me ji k ey craine end frame o i t r i i i i j f slide from andrew zisserman frame l r i frame ke fr e l o r i p key frame etl l f encl fnune video google system collect all words within query region inverted file index to find relevant frames skip for compare word counts bow spatial verification skip sivic zisserman iccv demo online at esearch vgoogle index html grauman db image with high bow similarity db image with high bow similarity both image pairs have many visual words in common db image with high bow similarity db image with high bow similarity only some of the matches are mutually consistent example applications mobile tourist guide object building recognition self localization photo video augmentation b leibe quack leibe van gool civr scoring retrieval quality query database size images relevant total images e g images of golden gate results ordered precision relevant returned recall relevant total relevant recall indexing and retrieval summary bag of words representation quantize feature space to make discrete set of visual words summarize image by distribution of words index individual words inverted index pre compute index to enable faster search at query time recognition of instances match local features optionally perform spatial verification cs intro to computer vision affine and projective transformations prof adriana kovashka university of pittsburgh october alignment problem we previously discussed how to match features across images of the same or different objects now let focus on the case of two images of the same object e g xi and xi and examine it in more detail what transformation relates xi and xi in alignment we will fit the parameters of some transformation according to a set of matching feature pairs correspondences xi xi two questions alignment given two images what is the transformation between them warping given a source image and a transformation what does the transformed output look like motivation for feature based alignment image mosaics what are the correspondences compare content in local patches find best matches simplest approach scan xi with template formed from a point in xi and compute euclidean distance a k a ssd or normalized cross correlation between list of pixel intensities in the patch what are the transformations examples of transformations translation rotation aspect affine perspective parametric global warping p x y p x y transformation t is a coordinate changing machine p t p what does it mean that t is global it is the same for any point p it can be described by just a few numbers parameters let represent t as a matrix p mp x y x y alyosha efros scaling scaling a coordinate means multiplying each of its components by a scalar uniform scaling means this scalar is the same for all components scaling non uniform scaling different scalars per component x y scaling scaling operation or in matrix form x ax y by x mx ny ax y px qy by scaling matrix s linear transformations x y a c b x d y only linear transformations can be represented with a matrix linear transformations are combinations of scale rotation shear and mirror what transforms can we write with a matrix scaling x x x x sx x y y y sy y y rotate around x cos x sin y x cos sin x y sin x cos y y sin cos y shear x x shx y x shx x y shy x y y sh y y what transforms can we write with a matrix mirror about y axis x x x x y y y y mirror over x x x x y y y y translation x y x t x y t y can t do homogeneous coordinates to convert to homogeneous coordinates homogeneous image coordinates converting from homogeneous coordinates translation homogeneous coordinates xx tx xx xx ttxx yy y y t ty y y tyy tx ty affine transformations x a b c x y d e f y w w affine transformations are combinations of linear transformations and translations maps lines to lines parallel lines remain parallel fitting an affine transformation assuming we know the correspondences how do we get the transformation m xi xi y m m y t m i i alyosha efros fitting an affine transformation m xi yi xi x y m y i i i how many matches correspondence pairs do we need to solve for the transformation parameters once we have solved for the parameters how do we compute x new y new given xnew ynew where do the matches come from projective transformations x a b c x y w d e g h f y i w projective transformations affine transformations and projective warps parallel lines do not necessarily remain parallel projection world coord image coord p x y optical center f y z x camera center z x x p y x y modified from derek hoiem and kristen grauman scene point image coordinates image mosaics goals obtain a wider angle view by combining multiple images image mosaics camera setup two images with camera rotation zoom but no translation mosaic many views of same object steve seitz mosaic plane the mosaic has a natural interpretation in the images are reprojected onto a common plane the mosaic is formed on this plane mosaic is a synthetic wide angle camera image reprojection basic question how to relate two images from the same camera center how to map a pixel from to answer cast a ray through each pixel in draw the pixel where that ray intersects observation rather than thinking of this as a reprojection think of it as a image warp from one image to another projective transforms a projective transform is a mapping between any two pps with the same center of projection rectangle should map to arbitrary quadrilateral parallel lines aren t but preserves straight lines also called homography wx wy x y w p h p adapted from alyosha efros how to stitch together a panorama a k a mosaic basic procedure take a sequence of images from the same position rotate the camera about its optical center compute the homography transformation between second image and first transform the second image to overlap with the first blend the two together to create a mosaic if there are more images repeat computing the homography xn yn xn yn to compute the homography given pairs of corresponding points in the images we need to set up an equation where the parameters of h are the unknowns kristen grauman computing the homography p hp wx a b c x wy d e f y w g h i can set scale factor i so there are unknowns set up a system of linear equations ah b where vector of unknowns h a b c d e f g h t need at least eqs but the more the better solve for h if overconstrained solve using least squares min ah b computing the homography assume we have four matched points how do we compute homography h w x h p hp p w y h h h h w h a h h x y xx yx x h x y xy yy y h h apply svd udvt a u s v svd a h vsmallest column of v corr to smallest singular value derek hoiem how to stitch together a panorama a k a mosaic basic procedure take a sequence of images from the same position rotate the camera about its optical center compute the homography transformation between second image and first transform the second image to overlap with the first blend the two together to create a mosaic if there are more images repeat transforming the second image x y image image canvas wx w wy w x y to apply a given homography h compute p hp regular matrix multiply wx wy x y convert p from homogeneous to image coordinates w modified from kristen grauman p h p transforming the second image image image canvas y forward warping send each pixel f x y to its corresponding location x y h x y in the right image transforming the second image y x f x y x g x y forward warping send each pixel f x y to its corresponding location x y h x y in the right image q what if pixel lands between two pixels a distribute color among neighboring pixels x y transforming the second image image image canvas y inverse warping get each pixel g x y from its corresponding location x y h x y in the left image transforming the second image y x f x y x g x y inverse warping get each pixel g x y from its corresponding location x y h x y in the left image q what if pixel comes from between two pixels a interpolate color value from neighbors help homography example image rectification to unwarp rectify an image solve for homography h given p and p p hp summary write transformations as matrix vector multiplication including translation when we use homogeneous coordinates projection equations express how world points mapped to image fitting transformations solve for unknown parameters given corresponding points from two views linear affine projective homography mosaics uses homography and image warping to merge views taken from same center of projection perform image warping forward inverse adapted from kristen grauman the next hidden slides give some more detail about how image formation works i e how objects are mapped to images please review on your own time if you re interested cs intro to computer vision epipolar geometry and stereo vision prof adriana kovashka university of pittsburgh october announcement please send me three topics you want me to review next class for the midterm the more requests i get for each topic the more time i will devote to it if no one asks me to review a topic i won t review it exam format multiple choice and true false short answers demonstrating an algorithm on a particular example last class vs this class last class same camera center but camera rotates this class camera center is not the same we have multiple cameras epipolar geometry relates cameras from two positions cameras stereo depth estimation recover depth from disparities between two images adapted from derek hoiem why multiple views structure and depth are inherently ambiguous from single views multiple views help us to perceive shape and depth stereo photography and stereo viewers take two pictures of the same subject from two slightly different viewpoints and display so that each eye sees only one of the images invented by sir charles wheatstone image from fisher price com stereo photography and stereo viewers depth from stereo two cameras simultaneous views single moving camera and static scene depth from stereo goal recover depth by finding image coordinate x that corresponds to x x x z c baseline c b depth from stereo goal recover depth by finding image coordinate x that corresponds to x sub problems calibration how do we recover the relation of the cameras if not already known correspondence how do we search for the matching point x x geometry for a simple stereo system assume parallel optical axes known camera parameters i e calibrated cameras what is expression for z similar triangles pl p pr and ol p or t xl xr t z f z depth is inversely proportional to disparity depth disparity z f t xr xl depth from disparity we have two images taken from cameras with different intrinsic and extrinsic parameters how do we match a point in the first image to a point in the second image i x y disparity map d x y image i x y so if we could find the corresponding points in two images we could estimate relative depth summary introduction of terms epipolar geometry epipoles are intersection of baseline with image planes matching point in second image is on a line passing through its epipole epipolar constraint limits where points from one view will be imaged in the other which makes search for correspondences quicker essential e and fundamental f matrices map from a point in one image to a line its epipolar line in the other can solve for e f given corresponding points e g interest points stereo depth estimation find corresponding points along epipolar scanline estimate disparity depth is inverse to disparity adapted from kristen grauman and derek hoiem stereo correspondence constraints given p in left image where can corresponding point p be stereo correspondence constraints epipolar constraint geometry of two views constrains where the corresponding pixel for some image point in the first view must occur in the second view it must be on the line carved out by a plane connecting the world point and optical centers potential matches for p have to lie on the corresponding line l potential matches for p have to lie on the corresponding line l epipolar geometry notation derek hoiem baseline line connecting the two camera centers epipoles intersections of baseline with image planes projections of the other camera center epipolar plane plane containing baseline epipolar lines intersections of epipolar plane with image planes always come in corresponding pairs note all epipolar lines intersect at the epipole epipolar constraint the epipolar constraint is useful because it reduces the correspondence problem to a search along an epipolar line kristen grauman image from andrew zisserman summary introduction of terms epipolar geometry epipoles are intersection of baseline with image planes matching point in second image is on a line passing through its epipole epipolar constraint limits where points from one view will be imaged in the other which makes search for correspondences quicker essential e and fundamental f matrices map from a point in one image to a line its epipolar line in the other can solve for e f given corresponding points e g interest points stereo depth estimation find corresponding points along epipolar scanline estimate disparity depth is inverse to disparity adapted from kristen grauman and derek hoiem stereo geometry with calibrated cameras if the stereo rig is calibrated we know how to rotate and translate camera reference frame to get to camera reference frame rotation matrix r translation vector t stereo geometry with calibrated cameras if the stereo rig is calibrated we know how to rotate and translate camera reference frame to get to camera reference frame x c rxc t an aside cross product vector cross product takes two vectors and returns a third vector that perpendicular to both inputs so here c is perpendicular to both a and b which means the dot product from geometry to algebra t x normal to the plane t rx cross product of vector with itself is aside matrix form of cross product a a b a a b c can be expressed as a matrix multiplication a a a x essential matrix x tx rx let e t x r x ex x t ex e is called the essential matrix and it relates corresponding image points between both cameras given the rotation and translation before we said if we observe a point in one image its position in other image is constrained to lie on line defined by above turns out ex is the epipolar line through x in the first image corresp to x note these points are in camera coordinate systems essential matrix example parallel cameras r p x y f t e t x r p x y f p ep for the parallel cameras image of any point must lie on same horizontal line in each image plane image i x y disparity map d x y x y x d x y y image i x y we can also find correspondences and depth if cameras optical axes are not parallel not discussed here what if we don t know camera parameters r t want to estimate world geometry without requiring calibrated cameras archival videos photos from multiple unrelated users weak calibration estimate epipolar geometry from a redundant set of point correspondences between two uncalibrated cameras computing f from correspondences each point correspondence generates one constraint on f collect n of these constraints im right im left solve for f vector of parameters fundamental matrix relates pixel coordinates in the two views more general form than essential matrix we remove need to know intrinsic parameters summary introduction of terms epipolar geometry epipoles are intersection of baseline with image planes matching point in second image is on a line passing through its epipole epipolar constraint limits where points from one view will be imaged in the other which makes search for correspondences quicker essential e and fundamental f matrices map from a point in one image to a line its epipolar line in the other can solve for e f given corresponding points e g interest points stereo depth estimation using epipolar geometry for stereo fuse a calibrated binocular stereo pair to produce a depth image image image dense depth map basic stereo matching algorithm for each pixel in the first image find corresponding epipolar scanline in the right image left right scanline matching cost disparity slide a window along the right scanline and compare contents of that window with the reference window in the left image matching cost e g euclidean distance assume parallel optical axes known camera parameters i e calibrated cameras what is expression for z similar triangles pl p pr and ol p or t xl xr t z f z depth disparity z f t xr xl results with window search data window based matching ground truth how can we improve uniqueness for any point in one image there should be at most one matching point in the other image ordering corresponding points should be in the same order in both views smoothness we expect disparity values to change slowly for the most part many of these constraints can be encoded in an energy function and solved using graph cuts before graph cuts ground truth for the latest and greatest projective structure from motion given m images of n fixed points xij pi xj i m j n problem estimate m projection matrices pi and n points xj from the mn corresponding points xij xj photo synth from multiple images building rome in a day agarwal et al recap epipoles point x in left image corresponds to epipolar line l in right image epipolar line passes through the epipole the intersection of the cameras baseline with the image plane recap fundamental matrix fundamental matrix maps from a point in one image to a line in the other if x and x correspond to the same point x essential matrix is like fundamental matrix but more constrained recap stereo with calibrated cameras given image pair r t detect some features compute essential matrix e match features using the epipolar and other constraints triangulate for structure and get depth prof adriana kovashka university of pittsburgh october reminders the midterm exam is in class on this coming wednesday there will be no make up exams unless you or a close relative is seriously ill review requests i received textures and texture representations image responses to size and orientation of gaussian filter banks comparisons corner detection alg harris invariance vs covariance affine intensity change and applications to know scale invariant detection blob detection harris automatic scale selection sift and feature description keypoint matching alg feature matching examples of how to compute and apply homography epipolar geometry why it makes sense to use the ratio distance to best match distance to second best match when matching features across images summary of equations students need to know pyramids convolution practical use filters for transforming the image transformations homographies epipolar geometry x y a c b x d y only linear transformations can be represented with a matrix linear transformations are combinations of scale rotation shear and mirror x a b c x y d e f y w w affine transformations are combinations of linear transformations and translations maps lines to lines parallel lines remain parallel x a b c x y w d e g h f y i w projective transformations affine transformations and projective warps parallel lines do not necessarily remain parallel how to stitch together a panorama a k a mosaic basic procedure take a sequence of images from the same position rotate the camera about its optical center compute the homography transformation between second image and first transform the second image to overlap with the first blend the two together to create a mosaic if there are more images repeat xn yn xn yn to compute the homography given pairs of corresponding points in the images we need to set up an equation where the parameters of h are the unknowns p hp wx a b c x wy d e f y w g h i can set scale factor i so there are unknowns set up a system of linear equations ah b where vector of unknowns h a b c d e f g h t need at least eqs but the more the better solve for h if overconstrained solve using least squares min ah b assume we have four matched points how do we compute homography h w x h p hp p w y h h h h w h a h h x y xx yx x h x y xy yy y h h apply svd udvt a u s v svd a h vsmallest column of v corr to smallest singular value derek hoiem image image canvas test point x y wx w wy w x y to apply a given homography h compute p hp regular matrix multiply wx wy x y convert p from homogeneous to image coordinates w modified from kristen grauman p h p image image canvas y forward warping send each pixel f x y to its corresponding location x y h x y in the right image depth from disparity we have two images taken from cameras with different intrinsic and extrinsic parameters how do we match a point in the first image to a point in the second image i x y disparity map d x y image i x y so if we could find the corresponding points in two images we could estimate relative depth epipolar geometry notation derek hoiem baseline line connecting the two camera centers epipoles intersections of baseline with image planes projections of the other camera center epipolar plane plane containing baseline epipolar lines intersections of epipolar plane with image planes always come in corresponding pairs note all epipolar lines intersect at the epipole epipolar constraint the epipolar constraint is useful because it reduces the correspondence problem to a search along an epipolar line essential matrix x tx rx let e t x r x ex x t ex e is called the essential matrix and it relates corresponding image points between both cameras given the rotation and translation before we said if we observe a point in one image its position in other image is constrained to lie on line defined by above turns out ex is the epipolar line through x in the first image corresp to x note these points are in camera coordinate systems basic stereo matching algorithm for each pixel in the first image find corresponding epipolar scanline in the right image left right scanline matching cost disparity slide a window along the right scanline and compare contents of that window with the reference window in the left image matching cost e g euclidean distance assume parallel optical axes known camera parameters i e calibrated cameras what is expression for z similar triangles pl p pr and ol p or t xl xr t z f z depth disparity z f t xr xl results with window search data window based matching ground truth how can we improve uniqueness for any point in one image there should be at most one matching point in the other image ordering corresponding points should be in the same order in both views smoothness we expect disparity values to change slowly for the most part many of these constraints can be encoded in an energy function and solved using graph cuts before graph cuts ground truth for the latest and greatest projective structure from motion given m images of n fixed points xij pi xj i m j n problem estimate m projection matrices pi and n points xj from the mn corresponding points xij xj photo synth building rome in a day agarwal et al point x in left image corresponds to epipolar line l in right image epipolar line passes through the epipole the intersection of the cameras baseline with the image plane fundamental matrix maps from a point in one image to a line in the other if x and x correspond to the same point x essential matrix is like fundamental matrix but more constrained given image pair r t detect some features compute essential matrix e match features using the epipolar and other constraints triangulate for structure and get depth texture representations correlation filtering say the averaging window size is x attribute uniform weight to each pixel loop over all pixels in neighborhood around image pixel f i j now generalize to allow different weights depending on neighboring pixel relative position non uniform weights convolution vs correlation f convolution u v h i j slide credit derek hoiem original image kristen grauman derivative filter responses squared statistics to summarize patterns in small windows filter banks scales what filters to put in the bank typically we want a combination of scales and orientations different types of patterns matlab code available for these examples kristen grauman matching with filters goal find in image method filter the image with eye patch g m n h k l k l f m k n l f image g filter input filtered image what went wrong matching with filters goal find in image likes bright pixels where filters are above average dark pixels where filters are below average method filter the image with zero mean eye g m n h k l mean h k l f m k n l input filtered image scaled thresholded image 50 50 50 100 150 200 250 50 100 150 200 250 50 50 100 150 200 250 representing texture by mean abs response filters mean abs responses derek hoiem computing distances using texture d a b dimension kristen grauman feature detection harris corners as distinctive interest points we should easily recognize the keypoint by looking through a small window shifting a window in any direction should give a large change in intensity flat region no change in all directions edge no change along the edge direction corner significant change in all directions efros d frolova d simakov window averaged squared change of intensity induced by shifting the image data by u v window function w x y or in window outside gaussian expanding i x y in a taylor series expansion we have for small shifts u v a quadratic approximation to the error surface between a patch and itself shifted by u v where m is a matrix computed from image derivatives m w x y ix ix ix i y x y i y i y notation i i x x i i y y i x i y i i x y since m is symmetric we have m x x t mxi i xi the eigenvalues of m reveal the amount of intensity change in the two principal orthogonal gradient directions in the window edge corner and are large flat region and are small compute image gradients ix and iy for all pixels for each pixel compute by looping over neighbors x y compute k empirical constant k find points with large corner response function r r threshold take the points of locally maximum r as the detected feature points i e pixels where r is bigger than for all the or neighbors d frolova d simakov grauman feature detection scale invariance a function is invariant under a certain family of transformations if its value does not change when a transformation from this family is applied to its argument a function is covariant when it commutes with the transformation i e applying the transformation to the argument of the function has the same effect as applying the transformation to the output of the function for example the area of a surface is invariant under rotations since rotating a surface does not make it any smaller or bigger but the orientation of the major axis of inertia of the surface is covariant under the same family of transformations since rotating a surface will affect the orientation of its major axis in exactly the same way local invariant feature detectors a survey by tinne tuytelaars and krystian mikolajczyk in foundations and trends in computer graphics and vision vol no 280 chapter i a i b only derivatives are used invariance to intensity shift i i b intensity scaling i a i r r threshold x image coordinate x image coordinate derivatives and window function are shift invariant second moment ellipse rotates but its shape i e eigenvalues remains the same corner all points will be classified as edges problem how do we choose corresponding circles independently in each image do objects in the image have a characteristic scale that we can identify frolova d simakov solution design a function on the region which is scale invariant has the same shape even if the image is resized take a local maximum of this function f f scale adapted from a torralba region size region size function responses for increasing scale scale signature function responses for increasing scale scale signature function responses for increasing scale scale signature f im x f im x laplacian of gaussian blob detector we can approximate the laplacian with a difference of gaussians more efficient to implement laplacian dog g x y k g x y difference of gaussians difference of gaussian efficient computation computation in gaussian scale pyramid sampling with step original image find local maxima in position scale space of difference of gaussian position scale space adapted from k grauman b leibe find places where x greater than all of its neighbors in green list of x y laplacian pyramid example allows detection of increasingly coarse detail results difference of gaussian k grauman b leibe feature description gradients m x y sqrt θ x y atan full version divide the window into a grid of cells case shown below quantize the gradient orientations i e snap each gradient to one of angles each gradient contributes not just but magnitude gradient to the histogram i e stronger gradients contribute more cells orientations dimensional descriptor for each detected feature full version divide the window into a grid of cells case shown below quantize the gradient orientations i e snap each gradient to one of angles each gradient contributes not just but magnitude gradient to the histogram i e stronger gradients contribute more cells orientations dimensional descriptor for each detected feature normalize clip threshold normalize to normalize the descriptor after normalizing we have such that making descriptor rotation invariant rotate patch according to its dominant gradient orientation this puts the patches into a canonical orientation grauman image from matthew brown keypoint matching matching local features image image to generate candidate matches find patches that have the most similar appearance e g lowest feature euclidean distance simplest approach compare them all take the closest or closest k or within a thresholded distance robust matching image image at what euclidean distance value do we have a good match to add robustness to matching can consider ratio distance to best match distance to second best match if low first match looks good if high could be ambiguous match ratio example let q be the query from the first image be the closest match in the second image and be the second closest match let dist q and dist q be the distances let r dist q dist q what is the largest that r can be what is the lowest that r can be if r is what do we know about the two distances what about when r is indexing local features setup when we see close points in feature space we have similar descriptors which indicates similar local content grauman database images image matching describing images w visual words summarize entire image based on its distribution histogram of word occurrences analogous to bag of words representation commonly used for documents feature patches grauman visual words bag of visual words two uses represent the image using that representation look for similar images can also use bow to compute an inverted index to simplify application extract some local features from a number of images e g sift descriptor space each point is dimensional quantize the space by grouping clustering the features note for now we ll treat clustering as a black box inverted file index and bags of words similarity offline extract features in database images cluster them to find words make index extract words in query extract features and map each to closest cluster center use inverted file index to find frames relevant to query for each relevant frame rank them by comparing word counts bow of query and frame adapted from k grauman scoring retrieval quality query database size images relevant total images e g images of golden gate results ordered precision returned relevant returned recall returned relevant total relevant recall ondrej chum prof adriana kovashka university of pittsburgh october midterm exam statistics edges more low level don t need to be closed segments ideally one segment for each semantic group object should include closed contours edge detection goal map image from array of pixels to a set of curves or line segments or contours why figure from j shotton et al pami main idea look for strong gradients post process what causes an edge reflectance change appearance information texture depth discontinuity object boundary change in surface orientation shape cast shadows criteria for a good edge detector good detection find all real edges ignoring noise or other artifacts good localization detect edges as close as possible to the true edges return one point only for each true edge point cues of edge detection differences in color intensity or texture across the boundary continuity and closure high level knowledge an edge is a place of rapid change in the image intensity function image intensity function along horizontal scanline first derivative edges correspond to extrema of derivative intensity profile gradient consider a single row or column of the image plotting intensity as a function of position gives a signal where is the edge difference filters respond strongly to noise image noise results in pixels that look very different from their neighbors generally the larger the noise the stronger the response what can we do about it solution smooth first f g f g d f dx g to find edges look for peaks in d f dx g derivative theorem of convolution differentiation is convolution and convolution is associative d f dx g f d g dx this saves us one operation f d g dx f d g dx image with edge derivative of gaussian edge max of derivative gradients edges primary edge detection steps smoothing suppress noise edge enhancement filter for contrast edge localization determine which local maxima from filter output are actually edges vs noise threshold thin thresholding choose a threshold value t set any pixels less than t to off set any pixels greater than or equal to t to on original image source k grauman gradient magnitude image source k grauman source k grauman source k grauman canny edge detector filter image with derivative of gaussian find magnitude and orientation of gradient non maximum suppression thin wide ridges down to single pixel width linking and thresholding hysteresis define two thresholds low and high use the high threshold to start edge curves and the low threshold to continue them matlab edge image canny help edge source d lowe l fei fei input image lena derivative of gaussian y derivative of gaussian gradient magnitude norm of the gradient magnitude thresholding how to turn these thick regions of the gradient into curves check if pixel is local maximum along gradient direction select single max across width of the edge requires checking interpolated pixels p and r bilinear interpolation the canny edge detector problem pixels along this edge didn t survive the thresholding thinning non maximum suppression use a high threshold to start edge curves and a low threshold to continue them original image high threshold strong edges low threshold weak edges hysteresis threshold high threshold strong edges low threshold weak edges hysteresis threshold effect of gaussian kernel spread size original canny with canny with the choice of depends on desired behavior large detects large scale edges small detects fine features source s seitz low level edges vs perceived contours image human segmentation gradient magnitude berkeley segmentation database source l lazebnik how can we do better so far we have only considered change in intensity as a cue for the existence of an edge brightness color texture combined human for more image human segmentation group together similar looking pixels for efficiency of further processing superpixels x ren and j malik iccv oversegmentation undersegmentation multiple segmentations bottom up group tokens with similar features top down group tokens that likely belong to the same object source d hoiem levin and weiss figure by j shi determine image regions jpg group video frames into shots figure by wang suter figure ground figure by grauman darrell object level grouping goals gather features that belong together obtain an intermediate representation that compactly describes key image video parts hard to measure success what is interesting depends on the application gestalt psychology whole is greater than sum of its parts relationships among parts can yield new properties features psychologists identified series of factors that predispose set of elements to be grouped by human visual system good intuition and basic principles for grouping some e g symmetry are difficult to implement in practice source k grauman the muller lyer illusion continuity explanation by occlusion principles of perceptual organization source d hoiem from steve lehar the constructive aspect of visual perception similarity common fate image credit arthus bertrand via f durand source k grauman proximity segmentation and grouping inspiration from human perception gestalt properties bottom up segmentation via clustering features color texture algorithms mode finding and mean shift k means mean shift graph based normalized cuts image segmentation toy example black pixels gray pixels white pixels input image intensity these intensities define the three groups we could label every pixel in the image according to which of these primary intensities it is i e segment the image based on the intensity feature what if the image isn t quite so simple input image input image intensity intensity input image intensity now how to determine the three main intensities that define our groups we need to cluster intensity goal choose three centers as the representative intensities and label every pixel according to which of these centers it is nearest to best cluster centers are those that minimize ssd between all points and their nearest cluster center ci clustering with this objective it is a chicken and egg problem if we knew the cluster centers we could allocate points to groups by assigning each to its closest center if we knew the group memberships we could get the centers by computing the mean per group k means clustering basic idea randomly initialize the k cluster centers and iterate between the two steps we just saw randomly initialize the cluster centers ck given cluster centers determine points in each cluster for each point p find the closest ci put p into cluster i given points in each cluster solve for ci set ci to be the mean of points in cluster i if ci have changed repeat step properties will always converge to some solution can be a local minimum does not always find the global minimum of objective function source steve seitz k means ask user how many cllusters they d nke e g k k means l ask user how clusters they d lliike e g k guess k clluster center locations k means ask user how many clusters they d like e g k s guess k cluster ce nt e r locat ions each dlatapoint finds out wh ich center it closest to thus each cente r a set of datapo ints l k means ask user how many clusters they d like e g k guess k cluster center locations each datapoint finds out which cent er it closest to each center finds the centroid of the points it owns o l k means converges to a local minimum k means clustering matlab demo java demos k means pros and cons pros simple fast to compute converges to local minimum of within cluster squared error cons issues setting k sensitive to initial centers sensitive to outliers detects spherical clusters assuming means can be computed an aside smoothing out cluster assignments assigning a cluster label per pixel may yield outliers original labeled by cluster center intensity how to ensure they are segmentation as clustering depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on intensity similarity feature space intensity value d k k quantization of the feature space segmentation label map depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on color similarity r g b r g b r g r b r g b feature space color value d source k grauman depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on intensity similarity clusters based on intensity similarity don t have to be spatially coherent source k grauman depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on intensity position similarity source k grauman x both regions are black but if we also include position x y then we could group the two into distinct segments way to encode both similarity proximity color brightness position alone are not enough to distinguish all regions source l lazebnik depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on texture similarity filter bank of filters feature space filter bank responses e g d source k grauman find textons by clustering vectors of filter bank outputs describe texture in a window based on texton histogram image texton map texton index malik belongie leung and shi ijcv texton index texton index source l lazebnik image segmentation example k means pros and cons pros simple fast to compute converges to local minimum of within cluster squared error cons issues setting k sensitive to initial centers sensitive to outliers detects spherical clusters assuming means can be computed the mean shift algorithm seeks modes or local maxima of density in the feature space image feature space l u v color values estimated density search window center of mass mean shift vector source d hoiem cluster all data points in the attraction basin of a mode attraction basin the region for which all trajectories lead to the same mode slide by y ukrainitz b sarel compute features for each point color texture etc initialize windows at individual feature points perform mean shift for each window until convergence merge windows that end up near the same peak or mode source d hoiem pros mean shift does not assume shape on clusters one parameter choice window size generic technique find multiple modes robust to outliers cons selection of window size mean shift reading nicely written mean shift explanation with math includes m code for mean shift clustering mean shift paper by comaniciu and meer adaptive mean shift in higher dimensions fully connected graph node vertex for every pixel link between every pair of pixels p q affinity weight wpq for each link edge wpq measures similarity similarity is inversely proportional to difference in color and position a b c break graph into segments want to delete links that cross between segments easiest to break links that have low similarity low weight similar pixels should be in the same segments dissimilar pixels should be in different segments b link cut set of links whose removal makes a graph disconnected cost of a cut find minimum cut cut a b w p a q b p q gives you a segmentation fast algorithms exist for doing this k means iteratively re assign points to the nearest cluster center mean shift clustering estimate modes graph cuts split the nodes in a graph based on assigned links with similarity weights agglomerative clustering start with each point as its own cluster and iteratively merge the closest clusters summarizing data look at large amounts of data represent a large continuous vector with the cluster number counting histograms of texture color sift vectors segmentation separate the image into different mid level regions find object boundaries prediction images in the same cluster may have the same labels slide credit j hays d hoiem cs intro to computer vision fitting models hough transform ransac prof adriana kovashka university of pittsburgh october plan for today last lecture detecting edges this lecture detecting lines and other shapes find the parameters of a line that best fits our data least squares hough transform ransac characterizing edges an edge is a place of rapid change in the image intensity function intensity function image along horizontal scanline gradient first derivative edges correspond to extrema of gradient gradients edges primary edge detection steps smoothing suppress noise edge enhancement filter for contrast compute gradients edge localization determine which local maxima from filter output are actually edges vs noise basic step thresholding choose a threshold value t set any pixels less than t off set any pixels greater than or equal to t on more advanced steps thinning and hysteresis original image source k grauman thresholding gradient with a higher threshold source k grauman related line detection fitting why fit lines many objects characterized by presence of straight lines why aren t we done just by running edge detection difficulty of line fitting noise in measured edge points orientations e g edges not collinear where they should be how to detect true underlying parameters extra edge points clutter which points go with which line if any only some parts of each line detected and some parts are missing how to find a line that bridges missing evidence fitting other objects want to associate a model with observed features the model could be a line a circle or an arbitrary shape fig from marszalek schmid least squares line fitting data xn yn line equation yi m xi b find m b to minimize xi yi y mx b where line you found tells you point is along y axis where point really is along y axis you want to find a single line that explains all of the points in your data but data may be noisy m m e n x y ap y i i b i b xn y n outliers can hurt the quality of our parameter estimates e g an edge point that is noise or doesn t belong to the line we are fitting two common ways to deal with outliers both boil down to voting hough transform ransac voting is a general technique where we let the features vote for all models that are compatible with it cycle through features cast votes for model parameters look for model parameters that receive a lot of votes noise clutter features they will cast votes too but typically their votes should be inconsistent with the majority of good features fitting lines hough transform given points that belong to a line what is the line how many lines are there which points belong to which lines hough transform is a voting technique that can be used to answer all of these questions main idea record vote for each possible line on which some edge point lies look for lines that get many votes y b x m image space hough parameter space connection between image x y and hough m b spaces a line in the image corresponds to a point in hough space y b x m image space hough parameter space connection between image x y and hough m b spaces a line in the image corresponds to a point in hough space what does a point in the image space map to answer the solutions of b this is a line in hough space to go from image space to hough space given a pair of points x y find all m b such that y mx b y b x m image space hough parameter space what are the line parameters for the line that contains both and it is the intersection of the lines b and b y b x m image space hough parameter space how can we use this to find the most likely parameters m b for the most prominent line in the image space let each edge point in image space vote for a set of possible parameters in hough space accumulate votes in discrete set of bins parameters with the most votes indicate line in image space y m x b y m x b problems with the m b space unbounded parameter domains vertical lines require infinite m problems with the m b space unbounded parameter domains vertical lines require infinite m alternative polar representation y sin each point x y will add a sinusoid in the parameter space p v c hough machine analysis of bubble chamber pictures proc int conf high energy accelerators and instrumentation use a polar representation for the parameter space each line is a sinusoid in hough parameter space y x hough space x cos ysin initialize accumulator h to all zeros for each feature point x y in the image for θ to ρ x cos θ y sin θ h θ ρ h θ ρ end end find the value of θ ρ where h θ ρ is a local maximum the detected line in the image is given by ρ x cos θ y sin θ derek hoiem x image space edge coordinates votes y d x image space edge coordinates votes what difficulty does this present for an implementation features votes need to adjust grid size or smooth image space edge coordinates votes here everything appears to be noise or random edge points but we still see peaks in the vote space initialize accumulator h to all zeros find the value of θ ρ where h θ ρ is a local maximum the detected line in the image is given by ρ x cos θ y sin θ recall when we detect an edge point we also know its gradient direction but this means that the line is uniquely determined modified hough transform a circle with radius r and center a b can be described as x a r cos θ y b r sin θ x y a b circle center a b and radius r xi a yi b for a fixed radius r unknown gradient direction b hough space a hough transform for circles circle center a b and radius r xi a yi b for a fixed radius r unknown gradient direction intersection most votes for center occur here image space hough space hough transform for circles circle center a b and radius r xi a yi b for an unknown radius r unknown gradient direction b hough transform for circles circle center a b and radius r xi a yi b for an unknown radius r unknown gradient direction b image space hough space circle center a b and radius r xi a yi b for an unknown radius r known gradient direction image space hough space for every edge pixel x y for each possible radius value r x a r cos θ y b r sin θ for each possible gradient direction θ or use estimated gradient at x y a x r cos θ column b y r sin θ row h a b r end end end original edges votes penny note a different hough transform with separate accumulators was used for each circle radius quarters vs penny comboinreigdindaeltections edges votes quarter note a different hough transform with separate accumulators was used for each circle radius quarters vs penny example iris detection gradient threshold hough space fixed radius max detections hemerson pistori and eduardo rocha costa voting practical tips minimize irrelevant tokens first reduce noise choose a good grid discretization too fine too coarse too coarse large votes obtained when too many different lines correspond to a single bucket too fine miss lines because points that are not exactly collinear cast votes for different buckets vote for neighbors also smoothing in accumulator array use direction of edge to reduce parameters by to read back which points voted for winning peaks keep tags on the votes we want to find a template defined by its reference point center and several distinct types of landmark points in stable spatial configuration template triangle circle diamond some type of visual token e g feature or edge point what if we want to detect arbitrary shapes specifically where is the reference point intuition model image novel image x vote space now suppose those colors encode gradient directions define a model shape by its boundary points and a reference point offline procedure at each boundary point compute displacement vector r a pi store these vectors in a table indexed by gradient orientation θ kristen grauman dana h ballard generalizing the hough transform to detect arbitrary shapes detection procedure for each edge point use its gradient orientation θ to index into stored table use retrieved r vectors to vote for reference point novel image assuming translation is the only transformation here i e orientation and scale are fixed template representation for each type of landmark point store all possible displacement vectors towards the center template model detecting the template for each feature in a new image look up that feature type in the model and vote for the possible center locations associated with that type in the model test image model svetlana lazebnik index displacements by visual codeword training image visual codeword with displacement vectors learning in computer vision svetlana lazebnik hough transform pros and cons pros all points are processed independently so can cope with occlusion gaps some robustness to noise noise points unlikely to contribute consistently to any single bin can detect multiple instances of a model in a single pass cons complexity of search time for maxima increases exponentially with the number of model parameters if parameters and choices for each search is o non target shapes can produce spurious peaks in parameter space quantization can be tricky to pick a good grid size adapted from kristen grauman ransac random sample consensus approach we want to avoid the impact of outliers so let look for inliers and use those only intuition if an outlier is chosen to compute the current fit then the resulting line won t have much support from rest of the points ransac general form ransac loop randomly select a seed group of points on which to base model estimate fit model to these points find inliers to this model i e points whose distance from the line is less than t if there are d or more inliers re compute estimate of model on all of the inliers repeat n times keep the model with the largest number of inliers algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence ransac random sample consensus fischler bolles in line fitting example algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence ransac random sample consensus fischler bolles in line fitting example n i algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence ransac random sample consensus fischler bolles in line fitting example algorithm n i sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model review keypoint matching for search find a set of distinctive key points define a region around each keypoint window compute a local descriptor from the region d f a fb t match descriptors adapted from k grauman b leibe given matched points in a and b estimate the translation of the object x b x a tx i i i i ty derek hoiem tx ty least squares solution x b x a tx write down objective function in form ax b i i solve using pseudo inverse or eigenvalue decomposition i i ty x b x a y b y a tx y x b x a n n derek hoiem y b y a n n tx ty problem outliers ransac solution x b x a tx sample a set of matching points pair i i solve for transformation parameters score parameters with number of inliers repeat steps n times i i ty tx ty problem outliers multiple objects and or many to one matches hough transform solution x b x a tx initialize a grid of parameter values i i each matched pair casts a vote for consistent values find the parameters with the most votes solve using least squares with inliers i i ty fitting and matching summary fitting problems require finding any supporting evidence for a model even within clutter and missing features voting and inlier approaches such as the hough transform and ransac make it possible to find likely model parameters without searching all combinations of features can use these approaches to compute robust feature alignment matching and to match object templates adapted from kristen grauman and derek hoiem the remaining slides show another more detailed example of ransac and pros cons review on your own time least squares fit randomly select minimal subset of points randomly select minimal subset of points hypothesize a model randomly select minimal subset of points hypothesize a model compute error function randomly select minimal subset of points hypothesize a model compute error function select points consistent with model randomly select minimal subset of points hypothesize a model compute error function select points consistent with model repeat hypothesize and verify loop randomly select minimal subset of points hypothesize a model compute error function select points consistent with model repeat hypothesize and verify loop uncontaminated sample randomly select minimal subset of points hypothesize a model compute error function select points consistent with model repeat hypothesize and verify loop randomly select minimal subset of points hypothesize a model compute error function select points consistent with model repeat hypothesize and verify loop how to choose parameters number of samples n choose n so that with probability p at least one random sample is free from outliers e g p 99 outlier ratio e number of sampled points minimum number needed to fit the model distance threshold choose so that a good point with noise is likely e g prob 95 within threshold explanation in szeliski marc pollefeys and derek hoiem ransac pros and cons pros simple and general applicable to many different problems often works well in practice cons lots of parameters to tune see previous slide doesn t work well for low inlier ratios too many iterations or can fail completely can t always get a good initialization of the model based on the minimum number of samples common applications image stitching relating two views spatial verification svetlana lazebnik prof adriana kovashka university of pittsburgh october plan for today examples of visual recognition problems what should we recognize recognition pipeline features data overview of some methods for classification k nearest neighbors linear classifiers some translations feature vector descriptor representation recognition often involves classification classes categories hence classification categorization training learning a model e g classifier happens at training time from training data classification prediction happens at test time machine learning problems supervised learning unsupervised learni ng q cj cl q c c given a feature representation for images how do we learn a model for distinguishing features from different classes decision boundary zebra non zebra assign input vector to one of two or more classes any decision rule divides the input space into decision regions separated by decision boundaries example spam filter examples of categorization in vision part or object detection e g for each window face or non face scene categorization indoor vs outdoor urban forest kitchen etc action recognition picking up vs sitting down vs standing emotion recognition happy vs scared vs surprised region classification label pixels into different object surface categories boundary classification boundary vs non boundary etc etc adapted from d hoiem what do you see in this image trees bear camera man rabbit grass forest slide credit d hoiem describe predict or interact with the object based on visual cues is it dangerous how fast does it run is it alive is it soft does it have a tail can i poke with it slide credit d hoiem two class binary cat vs dog multi class often object recognition caltech average object images fine grained recognition place recognition places database dating historical photos image style recognition slide credit d hoiem layout prediction assign regions to orientation geometric context assign regions to depth material recognition a farhadi i endres d hoiem and d forsyth describing objects by their attributes cvpr kovashka s vijayanarasimhan and k grauman actively selecting annotations among objects and attributes iccv kovashka d parikh and k grauman whittlesearch image search with relative attribute feedback cvpr generic categorization problem instance level recognition problem john car which one do you think is harder generic or instance level recognition visual object categories what stuff should we bother to recognize basic level categories in human categorization rosch lakoff the highest level at which category members have similar perceived shape the highest level at which a single mental image reflects the entire category the level at which human subjects are usually fastest at identifying category members the first level named and understood by children the highest level at which a person uses similar motor actions for interaction with category members visual object categories basic level categories in humans seem to be defined predominantly visually there is evidence that humans usually start with basic level categorization before doing identification basic level categorization is easier and faster for humans than object identification abstract levels animal quadruped basic level dog cat cow german shepherd doberman individual level fido object categorization task description given a small number of training images of a category recognize a priori unknown instances of that category and assign the correct category label which categories are feasible visually fido german shepherd dog animal living being how many object categories are there source fei fei li rob fergus antonio torralba biederman other types of categories functional categories e g chairs something you can sit on other types of categories ad hoc categories e g something you can find in an office environment why recognition recognition a fundamental part of perception e g robots autonomous agents organize and give access to visual content connect to information detect trends and themes slide credit k grauman why is recognition hard recognition a machine learning approach apply a prediction function to a feature representation of the image to get the desired output f apple f tomato f cow y f x output prediction function image feature training given a training set of labeled examples xn yn estimate the prediction function f by minimizing the prediction error on the training set testing apply f to a never before seen test example x and output the predicted value y f x training steps testing test image slide credit d hoiem and l lazebnik recognizing a beach recognizing cloth fabric recognizing a mug fine grained recognition what breed is this dog what are the right features depend on what you want to know object shape local shape info shading shadows texture material properties albedo feel hardness color texture scene geometric layout linear perspective gradients line segments action motion optical flow tracked points color l a b color space hsv color space texture filter banks or hog over regions histograms of descriptors sift lowe ijcv bag of visual words bag of visual words image patches cluster patches bow histogram training steps testing test image slide credit d hoiem and l lazebnik recognition training data images in the training set must be annotated with the correct answer that the model is expected to produce motorbike slide credit l lazebnik datasets today imagenet categories images microsoft coco categories images pascal categories images sun categories images the pascal visual object classes challenge challenge classes person person animal bird cat cow dog horse sheep vehicle aeroplane bicycle boat bus car motorbike train indoor bottle chair dining table potted plant sofa tv monitor dataset size by training validation images bounding boxes segmentations pascal competitions classification for each of the twenty classes predicting presence absence of an example of that class in the test image detection predicting the bounding box if any and label of each object from the twenty target classes in the test image illumination object pose clutter occlusions intra class appearance viewpoint realistic scenes are crowded cluttered have overlapping objects challenges importance of context painter identification how would you learn to identify the author of a painting goya kirchner klimt marc monet van gogh one way to think about it training labels dictate that two examples are the same or different in some sense features and distances define visual similarity goal of training is to learn feature weights so that visual similarity predicts label similarity linear classifier confidence in positive label is a weighted sum of features what are the weights we want the simplest function that is confidently correct adapted from d hoiem nearest neighbor classifier training examples from class test example training examples from class f x label of the training example nearest to x all we need is a distance function for our inputs no training required k nearest neighbors classification for a new point find the k closest points from training data labels of the k points vote to classify black negative red positive k if query lands here the nn consist of negatives and positives so we classify it as negative what are the tradeoffs of having a too large k too small k a nearest neighbor recognition example estimating geographic information from a single image james hays and alexei efros cvpr where in the world how much can an image tell about its geographic location nearest neighbors according to gist bag of sift color histogram a few others million geotagged photos by photographers slides james hays eographic information from a single image c slides james hays hays and efros estimating geographic information from a single image cvpr slides james hays the importance of data hays and efros estimating geographic information from a single image cvpr slides james hays discriminative classifiers learn a simple function of the input features that correctly predicts the true labels on the training set 𝑦 𝑓 training goals accurate classification of training data correct classifications are confident classification function is simple slide credit d hoiem linear classifier what about this line find a linear function to separate the classes f x sgn wdxd sgn w x nn vs linear classifiers nn pros simple to implement decision boundaries not necessarily linear works for any number of classes nonparametric method nn cons need good distance function slow at test time large search problem to find neighbors storage of data linear pros low dimensional parametric representation very fast at test time linear cons works for two classes how to train the linear function what if data is not linearly separable evaluating classifiers accuracy correctly classified all test examples precision recall precision retrieved positives retrieved recall retrieved positives positives f measure p r prof adriana kovashka university of pittsburgh october beyond bags of features spatial pyramid matching for recognizing natural scene categories cvpr svetlana lazebnik slazebni uiuc edu beckman institute university of illinois at urbana champaign cordelia schmid cordelia schmid inrialpes fr inria rhône alpes france jean ponce ponce di ens fr ecole normale supérieure france scene category dataset fei fei perona oliva torralba bags of words bag of words steps extract local features learn visual vocabulary using clustering quantize local features using visual vocabulary represent images by frequencies of visual words slide credit l lazebnik feature extraction on which bow is based weak features strong features edge points at scales and orientations vocabulary size sift descriptors of patches sampled on a regular grid quantized to form visual vocabulary size slide credit l lazebnik learning the visual vocabulary learning the visual vocabulary image categorization with bag of words training compute bag of words representation for training images train classifier on labeled examples using histogram values as features labels are the scene types e g mountain vs field testing extract keypoints descriptors for test images quantize into visual words using the clusters computed at training time compute visual word histogram for test images compute labels on test images using classifier obtained at training time measure accuracy of test predictions by comparing them to ground truth test labels obtained from humans what about spatial layout all of these images have the same color histogram spatial pyramid compute histogram in each spatial bin spatial pyramid pyramid matching indyk thaper grauman darrell matching using pyramid and histogram intersection for some particular visual word original images adapted from l lazebnik scene category dataset fei fei perona oliva torralba multi class classification results training images per class fei fei perona scene category confusions difficult indoor images kitchen living room bedroom dataset fei fei et al multi class classification results training images per class prof adriana kovashka university of pittsburgh october plan for today support vector machines separable case non separable case linear non linear kernels the importance of generalization the bias variance trade off applies to all classifiers let a w c x x y ax cy b let a w c x x y ax cy b w x b lines in let a w c x x y ax cy b w x b lines in let a w c x x y ax cy b w x b d b distance from point to line lines in let a w c x y ax cy b w x b b w x b distance from d w point to line find linear function to separate positive and negative examples xi positive xi negative w b xi w b which line is best discriminative classifier based on optimal separating line for case maximize the margin between the positive and negative training examples xi positive yi xi w b xi negative yi xi w b support vectors margin for support vectors xi w b xi positive yi xi w b xi negative yi xi w b for support vectors xi w b distance between point and line for support vectors xi w b w support vectors margin wτ x b m want line that maximizes the margin xi positive yi xi w b xi negative yi xi w b for support vectors xi w b distance between point and line xi w b w support vectors margin therefore the margin is w maximize margin w correctly classify all training data points xi positive yi xi w b xi negative yi xi w b quadratic optimization problem one constraint for each training point note sign trick solution w i i yi xi solution w i i yi xi b yi w xi for any support vector classification function f x sign w x b sign i i yi xi x b if f x classify as negative otherwise classify as positive notice that it relies on an inner product between the test point x and the support vectors xi solving the optimization problem also involves computing the inner products xi xj between all pairs of training points datasets that are linearly separable work out great x but what if the dataset is just too hard x we can map it to a higher dimensional space general idea the original input space can always be mapped to some higher dimensional feature space where the training set is separable nonlinear kernel example consider the mapping x x x y x y xy k x y xy the kernel trick the linear classifier relies on dot product between vectors k xi xj xi xj if every data point is mapped into high dimensional space via some transformation φ xi φ xi the dot product becomes k xi xj φ xi φ xj a kernel function is similarity function that corresponds to an inner product in some expanded feature space the kernel trick instead of explicitly computing the lifting transformation φ x define a kernel function k such that k xi xj φ xi φ xj examples of kernel functions linear k xi x j i j polynomials of degree up to d 𝐾 𝑥𝑖 𝑥𝑗 𝑥𝑖𝑇𝑥𝑗 𝑑 gaussian rbf k xi x j exp histogram intersection k xi x j min k xi k x j k the w that minimizes maximize margin misclassification cost data samples slack variable the w that minimizes maximize margin minimize misclassification unfortunately there is no definitive multi class svm formulation in practice we have to obtain a multi class svm by combining multiple two class svms one vs others training learn an svm for each class vs the others testing apply each svm to the test example and assign it to the class of the svm that returns the highest decision value one vs one training learn an svm for each pair of classes testing each learned svm votes for a class to assign to the test example svetlana lazebnik one vs all a k a one vs others train k classifiers in each pos data from class i neg data from classes other than i the class with the most confident prediction wins example you have classes train classifiers vs others score vs others score vs others score vs other score final prediction class one vs one a k a all vs all train k k binary classifiers all pairs of classes they all vote for the label example you have classes then train classifiers vs vs vs vs vs vs votes final prediction is class svms for recognition define your representation for each example select a kernel function compute pairwise kernel values between labeled examples use this kernel matrix to solve for svm support vectors weights to classify a new example compute kernel values between new input and support vectors apply weights check sign of output example learning gender with svms moghaddam and yang learning gender with support faces tpami moghaddam and yang face gesture learning gender with svms training examples males females experiment with various kernels select gaussian rbf k xi xj exp support faces moghaddam and yang learning gender with support faces tpami moghaddam and yang learning gender with support faces tpami gender perception experiment how well can humans do subjects people male female ages mid to mid test data face images males females low res and high res versions task classify as male or female forced choice no time limit gender perception experiment how well can humans do error error human vs machine svms performed better than any single human test subject at either resolution kristen grauman svms pros and cons pros many publicly available svm packages or use built in matlab version but slower kernel based framework is very powerful flexible often a sparse set of support vectors compact at test time work very well in practice even with very small training sample sizes cons no direct multi class svm must combine two class svms can be tricky to select best kernel function for a problem computation memory during training time must compute matrix of kernel values for every pair of examples learning can take a very long time for large scale problems adapted from lana lazebnik precision recall f measure true positives images that contain people true negatives images that do not contain people predicted positives images predicted to contain people predicted negatives images predicted not to contain people precision recall f measure accuracy training set labels known test set labels unknown how well does a learned model generalize from the data it was trained on to a new test set components of generalization error bias how much the average model over all training sets differs from the true model error due to inaccurate assumptions simplifications made by the model variance how much models estimated from different training sets differ from each other underfitting model is too simple to represent all the relevant class characteristics high bias and low variance high training error and high test error overfitting model is too complex and fits irrelevant characteristics noise in the data low bias and high variance models with too few parameters are inaccurate because of a large bias not enough flexibility models with too many parameters are inaccurate because of a large variance too much sensitivity to the sample underfitting overfitting high bias low variance low bias high variance high bias low variance low bias high variance need validation set validation set is separate from the test set high bias low variance low bias high variance number of training examples choose a simpler classifier use fewer features get more training data regularize the parameters slide credit d hoiem no regularization huge regularization figures from bishop characteristics of vision learning problems lots of continuous features spatial pyramid may have features imbalanced classes often limited positive examples practically infinite negative examples difficult prediction tasks recently massive training sets became available if we have a massive training set we want classifiers with low bias high variance is ok and reasonably efficient training remember no free lunch machine learning algorithms are tools three kinds of error inherent unavoidable bias due to over simplifications variance due to inability to perfectly estimate parameters from limited data try simple classifiers first better to have smart features and simple classifiers than simple features and smart classifiers use increasingly powerful classifiers with more training data bias variance tradeoff prof adriana kovashka university of pittsburgh november today window based generic object detection basic pipeline boosting classifiers face detection as case study basic framework build train object model choose a representation learn or fit parameters of model classifier generate candidates in new image score the candidates representation choice part based consider edges contours and oriented intensity gradients summarize local distribution of gradients with histogram locally orderless offers invariance to small shifts and rotations given the representation train a binary classifier noy enso tcaarc ar window based object detection recap training obtain training data define features define classifier given new image slide window score by classifier feature extraction face detection and recognition sally challenges of face detection sliding window detector must evaluate tens of thousands of location scale combinations faces are rare per image a megapixel image has pixels and a comparable number of candidate face locations for computational efficiency we should try to spend as little time as possible on the non face windows to avoid having a false positive in every image our false positive rate has to be less than viola jones face detector discriminative classifier construction slide adapted from antonio torralba weak classifier weights increased weak classifier weights increased weak classifier final classifier is a combination of weak classifiers boosting training initially weight each training example equally in each boosting round find the weak learner that achieves the lowest weighted training error raise weights of training examples misclassified by current weak learner compute final classifier as linear combination of all weak learners weight of each learner is directly proportional to its accuracy exact formulas for re weighting and combining weak learners depend on the particular boosting scheme e g adaboost viola jones face detector main idea represent local texture with efficiently computable rectangular features within window of interest select discriminative features to be weak classifiers use boosted combination of them as final classifier form a cascade of such classifiers rejecting clear negatives quickly viola jones detector features rectangular filters feature output is difference between adjacent regions value pixels in white area pixels in black area efficiently computable with integral image any sum can be computed in constant time value at x y is sum of pixels above and to the left of x y integral image fast computation with integral images the integral image computes a value at each pixel x y that is the sum of the pixel values above and to the left of x y inclusive this can quickly be computed in one pass through the image computing sum within a rectangle let a b c d be the values of the integral image at the corners of a then the sum of original image values within the rectangle can be computed as sum a b c d only additions are required for any size of rectangle example source result viola jones detector features considering all possible filter parameters position scale and type possible features associated with each x window which subset of these features should we use to determine if a window has a face use adaboost both to select the informative features and to form the classifier viola jones detector adaboost want to select the single rectangle feature and threshold that best separates positive faces and negative non faces training examples in terms of weighted error resulting weak classifier outputs of a possible rectangle feature on faces and non faces for next round reweight the examples according to errors choose another filter threshold combo start with uniform weights on training examples for m rounds evaluate weighted error for each weak learner pick best learner figure from c bishop notes from k grauman normalize the weights so they sum to re weight the examples incorrectly classified get more weight correctly classified get less weight final classifier is combination of weak ones weighted according to error they had boosting for face detection first two features selected by boosting this feature combination can yield detection rate and false positive rate boosting pros and cons advantages of boosting integrates classification with feature selection complexity of training is linear in the number of training examples flexibility in the choice of weak learners boosting scheme testing is fast easy to implement disadvantages needs many training examples often found not to work as well as an alternative discriminative classifier support vector machine svm are we done even if the filters are fast to compute each new image has a lot of possible windows to search how to make the detection more efficient cascading classifiers for detection form a cascade with low false negative rates early on apply less accurate but faster classifiers first to immediately discard windows that clearly appear to be negative train with positives negatives real time detector using layer cascade features in all layers a seminal approach to real time object detection training is slow but detection is very fast key ideas integral images for fast feature evaluation boosting for feature selection attentional cascade of classifiers for fast rejection of non face windows p viola and m jones cvpr p viola and m jones ijcv matlab demo example using viola jones detector frontal faces detected and then tracked character names inferred with alignment of script and subtitles everingham m sivic j and zisserman a hello my name is buffy automatic naming of characters in tv video bmvc face detection and recognition sally can be trained to recognize pets slide credit lana lazebnik prof adriana kovashka university of pittsburgh november today object category detection window based approaches last time viola jones detector dalal triggs pedestrian detector part based approaches implicit shape model deformable parts model dpm improvements speeding up dpm analyzing the failures of dpm dalal triggs pedestrian detector extract fixed sized pixel window at each position and scale compute hog histogram of gradient features within each window score the window with a linear svm classifier perform non maxima suppression to remove overlapping detections with lower scores histogram of gradient orientations orientation bins for unsigned angles histograms in pixel cells votes weighted by magnitude cells pos w neg w pedestrian non max suppression adapted from derek hoiem single rigid template usually not enough to represent a category many objects e g humans are articulated or have parts that can vary in configuration many object categories look very different from different viewpoints or from instance to instance slide by n snavely images from caltech images from d ramanan dataset define object by collection of parts modeled by appearance spatial configuration one extreme fixed template object model sum of scores of features at fixed positions non object object another extreme bag of words star shaped model star shaped model articulated parts model object is configuration of parts each part is detectable and can move around adapted from derek hoiem images from felzenszwalb visual vocabulary is used to index votes for object position a visual word part training image annotated with object localization info visual codeword with displacement vectors learning in computer vision lana lazebnik build vocabulary of patches around extracted interest points using clustering build vocabulary of patches around extracted interest points using clustering map the patch around each interest point to closest word build vocabulary of patches around extracted interest points using clustering map the patch around each interest point to closest word for each word store all positions it was found relative to object center template representation for each type of landmark point store all possible displacement vectors towards the center template svetlana lazebnik model given new test image extract patches match to vocabulary words cast votes for possible positions of object center search for maxima in voting space lana lazebnik detection results qualitative performance recognizes different kinds of objects robust to clutter occlusion noise low contrast k grauman b leibe root filter part filters deformation weights multiple components the score of a hypothesis is the sum of appearance scores minus the sum of deformation costs part n features n displacements score p p w x p d dx dy dy i i i e how much the part pi moved from its expected location in the x y directions adapted from lana lazebnik appearance weights deformation weights how much we ll penalize the part pi for moving from its expected location training data consists of images with labeled bounding boxes need to learn the weights and deformation parameters f x w h x w are model parameters z are latent hypotheses latent svm training initialize w and iterate fix w and find the best z for each training example fix z and solve for w standard svm training component component today object category detection window based approaches last time viola jones detector dalal triggs pedestrian detector part based approaches implicit shape model deformable parts model dpm improvements speeding up dpm analyzing the failures of dpm speeding up detection restrict set of windows we pass through svm to those w high objectness objectness cue where people look objectness cue color contrast at boundary objectness cue no segments straddling the object box boxes found to have high objectness cyan ground truth bounding boxes yellow correct and red incorrect predictions for objectness only run the sheep horse chair etc classifier on the yellow red boxes today object category detection window based approaches last time viola jones detector dalal triggs pedestrian detector part based approaches implicit shape model deformable parts model dpm improvements speeding up dpm analyzing the failures of dpm most errors that detectors make are reasonable localization error and confusion with similar objects misdetection of occluded or small objects detectors have different sensitivity to different factors e g less sensitive to truncation than to size differences failure analysis code and annotations available online other objects similar objects bird boat car background other objects additional annotations for seven categories occlusion level parts visible sides visible occlusion poor robustness to occlusion but little impact on overall performance easier none harder heavy size strong preference for average to above average sized airplanes large medium x large small x small aspect ratio better at detecting wide side views than tall views x wide wide medium x tall tall easier wide harder tall sides parts best performance direct side view with all parts visible easier side harder non side detection in object detection system overview our system takes an input image extracts around bottom up region proposals computes features for each proposal using a large convolutional neural network cnn and then classifies each region using class specific linear svms r cnn achieves a mean average precision map of on pascal voc for comparison uijlings et al report map using the same region proposals but with a spatial pyramid and bag of visual words approach the popular deformable part models perform at lana lazebnik summary window based approaches assume object appears in roughly the same configuration in different images look for alignment with a global template part based methods allow parts to move somewhat from their usual locations look for good fits in appearance for both the global template and the individual part templates speed up by only scoring boxes that look like any object models prefer that objects appear in certain views prof adriana kovashka university of pittsburgh november announcements please watch the videos i sent you if you haven t yet that your reading we won t be ready to release until tomorrow night so we re pushing deadline until then thursday will be due after thanksgiving why convolutional neural networks neural network basics architecture biological inspiration loss functions optimization training with backprop cnns special operations common architectures understanding cnns visualization synthesis style transfer breaking cnns practical matters tips and tricks for training transfer learning software packages obtained state of the art performance on many problems most papers in cvpr use deep learning razavian et al cvpr workshops deng et al cvpr million labeled images classes images gathered from internet human labels via amazon turk challenge million training images classes alexnet similar framework to lecun but bigger model hidden layers units params more data vs images gpu implementation speedup over cpu trained on two gpus for a week better regularization for training dropout krizhevsky et al error top next best non convnet error object detection system overview our system takes an input image extracts around bottom up region proposals computes features for each proposal using a large convolutional neural network cnn and then classifies each region using class specific linear svms r cnn achieves a mean average precision map of on pascal voc for comparison uijlings et al report map using the same region proposals but with a spatial pyramid and bag of visual words approach the popular deformable part models perform at faster faster better using vgg cnn on pascal voc dataset detection segmentation regression pose estimation synthesis and many more convolutional neural networks are a type of neural network the neural network includes layers that perform special operations used in vision but to a lesser extent also in nlp biomedical etc often they are deep image video pixels object class features are key to recent progress in recognition but research shows they re flawed where next better classifiers or keep building more features what about learning the features learn a feature hierarchy all the way from pixels to classifier each layer extracts features from the output of previous layer train all layers jointly image video pixels simple classifier shallow vs deep architectures traditional recognition shallow architecture image video pixels object class image video pixels deep learning deep architecture object class activations nonlinear activation function h e g sigmoid tanh layer final outputs binary finally multiclass binary sigmoid tanh tanh x maxout elu leaky relu max x relu max x nonlinear classifier can approximate any continuous function to arbitrary accuracy given sufficiently many hidden units neurons accept information from multiple inputs transmit information to other neurons multiply inputs by weights along edges apply some function to the set of inputs at each node if output of function over threshold neuron fires input weights d linear neuron logistic neuron perceptron θ denotes the same as w cascade neurons together output from one layer is the input to the next each layer has its own sets of weights predictions are fed forward through the network to classify predictions are fed forward through the network to classify predictions are fed forward through the network to classify predictions are fed forward through the network to classify predictions are fed forward through the network to classify predictions are fed forward through the network to classify lots of hidden layers depth power usually it involves computing gradients backpropagation backprop propagates error from output layer to intermediate layers goal is to iteratively find such a set of weights that allow the activations to match the desired output trained with stochastic gradient descent we want to minimize a loss function example dataset cifar labels training images each image is test images f x w numbers indicating class scores array of numbers numbers total svm decision sign wtx sign x1 what should the weights be we want to train a spam or not classifier features are how many times each word occurs let say you only have words in your vocabulary cash bank homework beer book say i want a high score for spam and low score for not spam what would the weights be on each array of numbers numbers indicating class scores parameters or weights example with an image with pixels and classes cat dog ship going forward loss function optimization todo define a loss function that quantifies our unhappiness with the scores across the training data come up with a way of efficiently finding the parameters that minimize the loss function optimization suppose training examples classes with some w the scores are cat car frog suppose training examples classes with some w the scores are cat car frog suppose training examples classes with some w the scores are suppose training examples classes with some w the scores are suppose training examples classes with some w the scores are suppose training examples classes with some w the scores are multiclass svm loss given an example where where is the image and is the integer label and using the shorthand for the scores vector cat car frog the svm loss has the form and the full training loss is the mean over all examples in the training data l losses weight regularization λ regularization strength hyperparameter in common use regularization regularization dropout will see later scores unnormalized log probabilities of the classes where cat car frog want to maximize the log likelihood or for a loss function to minimize the negative log likelihood of the correct class cat car frog unnormalized probabilities exp normalize log 89 unnormalized log probabilities probabilities in dimension the derivative of a function in multiple dimensions the gradient is the vector of partial derivatives current w loss gradient dw current w loss w h first dim loss gradient dw current w loss w h first dim loss gradient dw current w loss w h second dim loss gradient dw current w loss w h second dim loss gradient dw current w loss w h third dim 78 33 loss gradient dw want current w 78 81 33 loss dw some function data and w gradient dw negative gradient direction original w we ll update weights move in direction opposite to gradient time l learning rate use gradient descent iteratively subtract the gradient with respect to the model parameters w i e we re moving in a direction opposite to the gradient of the loss i e we re moving towards smaller loss andrej karpathy the effects of step size or learning rate in classic gradient descent we compute the gradient from the loss for all training examples could also only use some of the data for each gradient update then cycle through all training samples allows faster training e g on gpus parallelization we ll update weights move in direction opposite to gradient how to update the weights at all layers answer backpropagation of error from higher layers to lower layers using sigmoid activations two layer net initialize all weights to small random values until convergence error stops decreasing repeat for each x t class x in training set calculate network outputs yk compute errors gradients wrt activations for each unit δk yk yk tk yk for output units δj yk yk k wkj δk for hidden units update weights wkj wkj η δk zj for output units wji wji η δj xi for hidden units adapted from rebecca hwa and ray mooney backpropagation activations local gradient f gradients andrej karpathy entire lecture on backpropagation karpathy lecture second video i sent you not guaranteed to converge to zero training error may converge to local optima or oscillate indefinitely however in practice does converge to low error for many large networks on real data thousands of epochs epoch network sees all training data once may be required hours or days to train to avoid local minima problems run several trials starting with different random weights random restarts and take results of trial with lowest training set error may be hard to set learning rate and to select number of hidden units and layers neural networks had fallen out of fashion in early back with a new name and significantly improved performance deep networks trained with dropout and lots of data running too many epochs can result in over fitting on test data 0 training epochs on training data keep a hold out validation set and test accuracy on it after every epoch stop training when additional epochs actually increase validation error too few hidden units prevents the network from adequately fitting the data too many hidden units can result in over fitting on test data 0 hidden units on training data use internal cross validation to empirically determine an optimal number of hidden units more neurons more capacity do not use size of neural network as a regularizer use stronger regularization instead you can play with this demo over at convnetjs trained hidden units can be seen as newly constructed features that make the target concept linearly separable in the transformed space on many real domains hidden units can be interpreted as representing meaningful features such as vowel detectors or edge detectors etc however the hidden layer can also become a distributed representation of the input in which each individual unit is not easily interpretable as a meaningful feature prof adriana kovashka university of pittsburgh november why convolutional neural networks neural network basics architecture biological inspiration loss functions optimization training with backprop cnns special operations common architectures understanding cnns visualization breaking cnns synthesis style transfer practical matters tips and tricks for training transfer learning software packages a biological neuron an artificial neuron hubel and weisel architecture multi layer neural network neural network with specialized connectivity structure stack multiple stages of feature extractors higher stages compute more global more invariant more abstract features classification layer at the end adapted from rob fergus feed forward feature extraction convolve input with learned filters apply non linearity spatial pooling downsample supervised training of convolutional filters by back propagating classification error adapted from lana lazebnik apply learned filter weights one feature map per filter stride can be greater than faster less memory non linearity per element independent options tanh sigmoid exp x rectified linear unit relu avoids saturation issues sum or max over non overlapping overlapping regions role of pooling invariance to small transformations larger receptive fields neurons see more of input max sum sum or max over non overlapping overlapping regions role of pooling invariance to small transformations larger receptive fields neurons see more of input image height depth width image filter convolve the filter with the image i e slide over the image spatially computing dot products convolution layer image filter number the result of taking a dot product between the filter and a small chunk of the image i e dimensional dot product bias convolution layer image filter activation map convolve slide over all spatial locations convolution layer consider a second green filter image filter activation maps convolve slide over all spatial locations for example if we had filters we ll get separate activation maps activation maps convolution layer we stack these up to get a new image of size preview convnet is a sequence of convolution layers interspersed with activation functions conv relu e g filters preview convnet is a sequence of convolutional layers interspersed with activation functions conv relu e g filters conv relu e g filters conv relu from recent yann lecun slides one filter one activation map example filters total we call the layer convolutional because it is related to convolution of two signals elementwise multiplication and sum of a filter and the signal image a closer look at spatial dimensions image filter activation map convolve slide over all spatial locations a closer look at spatial dimensions input spatially assume filter a closer look at spatial dimensions input spatially assume filter a closer look at spatial dimensions input spatially assume filter a closer look at spatial dimensions input spatially assume filter a closer look at spatial dimensions input spatially assume filter output a closer look at spatial dimensions input spatially assume filter applied with stride a closer look at spatial dimensions input spatially assume filter applied with stride a closer look at spatial dimensions input spatially assume filter applied with stride output a closer look at spatial dimensions input spatially assume filter applied with stride a closer look at spatial dimensions input spatially assume filter applied with stride doesn t fit cannot apply filter on input with stride convolutions more detail n output size n f stride n e g n f stride stride stride in practice common to zero pad the border e g input filter applied with stride pad with pixel border what is the output recall n f stride in practice common to zero pad the border e g input filter applied with stride pad with pixel border what is the output output in practice common to zero pad the border e g input filter applied with stride pad with pixel border what is the output output in general common to see conv layers with stride filters of size fxf and zero padding with f will preserve size spatially e g f zero pad with f zero pad with f zero pad with examples time input volume filters with stride pad output volume size examples time input volume filters with stride pad output volume size spatially so examples time input volume filters with stride pad number of parameters in this layer convolutions more detail examples time input volume filters with stride pad number of parameters in this layer each filter has params for bias convolutions more detail preview zeiler and fergus alexnet but change from stride to stride instead of filters use imagenet top error simonyan and zisserman only conv stride pad and max pool stride best model top error in ilsvrc top error szegedy et al inception module ilsvrc winner top error he et al ilsvrc winner top error slide from kaiming he recent presentation slide from kaiming he recent presentation he et al ilsvrc winner top error weeks of training on gpu machine at runtime faster than a vggnet even though it has more layers slide from kaiming he recent presentation visualization what are these cnns learning hubel and weisel architecture multi layer neural network adapted from jia bin huang patches from validation images that give maximal activation of a given feature map layer layer visualizing and understanding convolutional networks layer visualizing and understanding convolutional networks zeiler fergus as a function of the position of the square of zeros in the original image zeiler fergus as a function of the position of the square of zeros in the original image repeat forward an image set activations in layer of interest to all zero except for a 0 for a neuron of interest backprop to image do an image update understanding neural networks through deep visualization yosinski et al intriguing properties of neural networks jia bin huang deep neural networks are easily fooled high confidence predictions for unrecognizable images fooling a linear classifier to fool a linear classifier add a small multiple of the weight vector to the training example x x αw jia bin huang question given a cnn code is it possible to reconstruct the original image find an image such that its code is similar to a given code it looks natural image prior regularization understanding deep image representations by inverting them mahendran and vedaldi original image reconstructions from the log probabilities for imagenet ilsvrc classes reconstructions from the representation after last last pooling layer immediately before the first fully connected layer deepdream more info stanford lecture deepdream modifies the image in a way that boosts all activations at any layer this creates a feedback loop e g any slightly detected dog face will be made more and more dog like over time deep dream grocery trip deep dreaming fear loathing in las vegas the great san francisco acid wave synthesis style transfer a neural algorithm of artistic style by leon a gatys alexander s ecker and matthias bethge good implementation by justin johnson in torch h make your own easily on deepart io step extract content targets convnet activations of all layers for the given content image content activations e g at layer we would have a array of target activations step extract style targets gram matrices of convnet activations of all layers for the given style image style gram matrices e g at layer with activations would give a gram matrix of all pairwise activation covariances summed across spatial locations more info stanford lecture step optimize over image to have the content of the content image activations match content the style of the style image gram matrices of activations match style total variation regularization maybe match content match style practical matters use mini batch use regularization use gradient checks use cross validation for your parameters use relu or leaky relu or elu don t use sigmoid center subtract mean from your data to initialize use xavier initialization learning rate too high too low randomly turn off some neurons allows individual neurons to independently be responsible for performance dropout a simple way to prevent neural networks from overfitting adapted from jia bin huang create virtual training samples horizontal flip random crop color casting geometric distortion jia bin huang deep image transfer learning you need a lot of a data if you want to train use cnns train on imagenet small dataset feature extractor freeze these train this medium dataset finetuning more data retrain more of the network or all of it freeze these train this generic specific simplest way to use cnns take model trained on e g imagenet training set easiest take outputs of e g or fully connected layer and plug features from each layer into linear svm features are neuron activations at that level can train linear svm for different tasks not just one used to learn the deep net better fine tune features and or classifier on new dataset classify test set of new dataset transfer learning with cnns is pervasive extract patch run through a cnn classify center pixel cow repeat for every pixel who took this photograph deep net features achieve accuracy chance is less than human performance is method learns what proto objects scenes authors shoot thomas and kovashka cvpr dea d p lno t o hers work x htt ps t hest ad co worl d n ew p hotograph from dea c i i i i university of pittsburgh emtler at 10am pitt compute r scientists have resurrected fa med photographe rs world new ph otographs from dead photogr aphers with convoluti onal al networks martin anderson thu nov re search ers out of the university of pittsburgh have used con v o l urti on al neural netw orks cnns to id entify th e st ran ge obsessions and unique styles of known photographers and to ge ne rate new photographs whiicih accord with their urniq ue p e rsp e ctiv es on th e world around us and with keras lasagne overview neuroscience perceptron multi layer neural networks convolutional neural network cnn convolution nonlinearity max pooling understanding and visualizing cnn find images that maximize some class scores visualize individual neuron activation and input patterns breaking cnns training cnn dropout data augmentation transfer learning using cnns for your own task basic first step try the pre trained caffenet layers as features adapted from jia bin huang prof adriana kovashka university of pittsburgh december announcements next time review for the final exam by tuesday at noon send me three topics you want me to review for participation credit please do omets thanks grades before final see courseweb overall column i won t need to curve plan for today motivation history vision and language image captioning tools recurrent neural networks recent problem visual question answering some approaches vision and language humans don t use only their visual processing abilities or speaking listening abilities in isolation they use them together while computer vision and natural language processing are separate fields there has been increased interest in combining them a popular task is image captioning given an image automatically generate a caption for this image that agrees well with human generated captions it was an arresting face pointed of chin square of jaw her eyes were pale green without a touch of hazel starred with bristly black lashes and slightly tilted at the ends above them her thick black brows slanted upward cutting a startling oblique line in her magnolia white skin that skin so prized by southern women and so carefully guarded with bonnets veils and mittens against hot georgia suns scarlett o hara described in gone with the wind more nuance than traditional recognition person car shoe car pink car attributes of objects car on road relationships between objects little pink smart car parked on the side of a road in a london shopping district complex structured recognition outputs telling the story of an image this is a picture of one sky one road and one sheep the gray sky is over the gray road the gray sheep is by the gray road here we see one road one sky and one bicycle the road is near the blue sky and near the colorful bicycle the colorful bicycle is within the blue sky this is a picture of two dogs the first dog is near the second furry dog kulkarni et al missed detections some bad results false detections incorrect attributes here we see one potted plant this is a picture of one dog there are one road and one cat the furry road is in the furry cat this is a picture of one tree one road and one person the rusty tree is under the red road the colorful person is near the rusty tree and under the red road this is a photograph of two sheeps and one grass the first black sheep is by the green grass and by the second black sheep the second black sheep is by the green grass this is a photograph of two horses and one grass the first feathered horse is within the green grass and by the second feathered horse the second feathered horse is within the green grass kulkarni et al results with recurrent neural networks vanilla neural networks e g image captioning image sequence of words e g sentiment classification sequence of words sentiment e g machine translation seq of words seq of words e g video classification on frame level usually want to output a prediction at some time steps we can process a sequence of vectors x by applying a recurrence formula at every time step new state old state input vector at some time step some function with parameters w we can process a sequence of vectors x by applying a recurrence formula at every time step notice the same function and the same set of parameters are used at every time step the state consists of a single hidden vector h character level language model example vocabulary h e l o example training sequence hello character level language model example vocabulary h e l o example training sequence hello character level language model example vocabulary h e l o example training sequence hello character level language model example vocabulary h e l o example training sequence hello explain images with multimodal recurrent neural networks mao et al deep visual semantic alignments for generating image descriptions karpathy and fei fei show and tell a neural image caption generator vinyals et al long term recurrent convolutional networks for visual recognition and description donahue et al learning a recurrent visual representation for image caption generation chen and zitnick recurrent neural network convolutional neural network test image andrej karpathy andrej karpathy whh h whh h wih v caption generated straw hat sample end token finish sta rt straw hat start microsoft coco tsung yi lin et al currently images sentences each task given an image and a natural language open ended question generate a natural language answer aishwarya agrawal an aid to visually impaired is it safe to cross the street now surveillance what kind of car did the man in red shirt leave in interacting with robot is my laptop in my bedroom upstairs image embedding dim neural network softmax over top k answers convolution layer non linearity pooling layer convolution layer non linearity pooling layer fully connected mlp question embedding how many horses are in this image dim visual question answering demo cs intro to computer vision motion tracking pose and actions prof adriana kovashka university of pittsburgh december in this slide deck estimating human pose recognizing human actions even impoverished motion data can evoke a strong percept johansson visual perception of biological motion and a model for its analysis perception and psychophysics tracking some applications body pose tracking activity recognition censusing a bat population video based interfaces medical apps surveillance example a camera mouse video interface use feature tracking as mouse replacement specialized software for communication games james gips and margrit betke things that make visual tracking difficult erratic movements moving very quickly occlusions leaving and coming back surrounding similar looking objects tracking by repeated detection works well if object is easily detectable e g face or colored glove and there is only one need some way to link up detections best you can do if you can t predict motion key idea based on a model of expected motion predict where objects will occur in next frame before even seeing the image restrict search for the object measurement noise is reduced by trajectory smoothness robustness to missing or weak observations assumptions camera is not moving instantly to new viewpoint objects do not disappear and reappear in different places in the scene t t t t kristen grauman detection we detect the object independently in each frame and can record its position over time e g based on detection window coordinates adapted from kristen grauman tracking with dynamics we use image measurements to estimate position of object but also incorporate position predicted by dynamics i e our expectation of object motion pattern time t time t belief measurement corrected prediction time t time t state x the actual state of the moving object that we want to estimate but cannot observe e g position observations y our actual measurement or observation of state x which can be very noisy at each time t the state changes to xt and we get a new observation yt our goal is to recover the most likely state xt given all observations so far i e yt knowledge about dynamics of state transitions p x yt t p x yt t correction compute an updated estimate of the state from prediction and measurements p x yt yt yt yt we have models for details in hidden slides that follow likelihood of next state given current state dynamics model p x x likelihood of observation given the state observation or measurement model p y x we want to recover for each t p x y prediction details in hidden slides that follow p xt yt p xt xt p xt yt dxt dynamics model corrected estimate from previous step correction observation model predicted estimate p x y y p yt xt p xt yt t t p yt xt p xt yt dxt prediction know corrected state from previous time step and all measurements up to excluding the current one predict distribution over next state advances t p x receive measurement correction know prediction of state and next measurement update distribution over current state p x y kalman filter processing kalman filter processing kalman filter processing kalman filter processing ground truth observation correction amin sadeghi in this slide deck tracking how an object moves action a transition from one state to another what is the name of the action who is the actor how is the state of the actor changing what if anything is being acted on how is that thing changing what is the purpose of the action if any motion pose held objects nearby objects jamie shotton andrew fitzgibbon mat cook toby sharp mark finocchio richard moore alex kipman andrew blake best paper award at cvpr adapted from jamie shotton recognize large variety of human poses all shapes sizes limited compute budget super real time on xbox to allow games to run concurrently no temporal information frame by frame local pose estimate of parts each pixel each body joint treated independently very fast simple depth image features decision forest classifier capture depth image remove bg infer body parts per pixel cluster pixels to hypothesize body joint positions fit model track skeleton compute p ci wi body parts considered body part ci image window wi discriminative approach learn classifier p ci wi from training data depth comparisons very fast to compute 𝑑𝐼 x δ toy example distinguish left l and right r sides of the body to classify pixel x start here fθ i x no yes fθ i x no yes p c l r p c l r p c l r amit geman breiman geurts et al tree 𝐼 x 𝐼 x tree t pt c c c c trained on different random subset of images bagging helps avoid over fitting 𝑇 average tree posteriors 𝑃 𝑇 𝑃𝑡 𝑐 𝐼 x 𝑡 65 45 40 6 number of trees 15 depth of trees input depth inferred body parts front view side view top view jamie shotton inferred joint positions modes found using mean shift no tracking or smoothing via the pose of persons in the video how it changes via tracked points via spatio temporal interest points corners in space time talk on phone get out of car space time interest point detectors descriptors hog hof pyramid histograms svms with chi squared kernel spatio temporal binning interest points detecting activities of daily living in first person camera views hamed pirsiavash deva ramanan cvpr wearable adl detection it is easy to collect natural data low level features high level features space time interest points laptev ijcv human pose difficulties of pose detectors are not accurate enough not useful in first person camera views high level features appearance feature bag of objects fridge stove tv video clip fridge bag of detected objects stove tv accuracy on action categories our model 40 6 stip baseline integrated reasoning human pose estimation integrated reasoning human pose estimation object detection tennis racket integrated reasoning human pose estimation object detection action categorization tennis racket head torso activity tennis forehand object detection human pose estimation is challenging yao fei fei felzenszwalb huttenlocher ren et al ramanan ferrari et al yang mori andriluka et al eichner ferrari human pose estimation object detection facilitate given the object is detected human pose estimation object detection is challenging facilitate given the pose is estimated prof adriana kovashka university of pittsburgh december final info format multiple choice true false fill in the blank short answers apply an algorithm non cumulative i expect you to know how to do a convolution unlike last time i ll have one handout with the exam questions and a separate one where you re supposed to write the answers algorithms you should be able to apply k means apply a few iterations to a small example mean shift to see where a single point ends up hough transform write pseudocode only hough transform how can we use it to find the parameters matrix of a transformation when we have noisy examples compute a spatial pyramid at level grid formulate the svm objective and constraints in math and explain it work through an example for zero shot prediction boosting show how to increase weights pedestrian detection write high level pseudocode algorithms able to apply cont d compute neural network activations compute svm and softmax loss show how to use weights to compute loss show how to numerically compute gradient show one iteration of gradient descent with gradient computed for you apply convolution relu max pooling compute output dimensions from convolution monday 30 30pm anyone for whom this does not work convolutional neural networks requests hough transform support vector machines deformable part models zero shot learning face detection recurrent neural networks k means mean shift spatial pyramids backpropagation meaning of weights and how computed requests math for neural networks computing activations gradients gradient descent convolution non linearity pooling convolution output size architectures losses and finding weights that minimize them minibatch are the training examples cycled over more than once effect of number of neurons and regularization neural networks nonlinear activation function h e g sigmoid tanh outputs binary multiclass how can i write as a function of xd sigmoid tanh tanh x relu max x when do i need to compute activations how many times do i need to do that how many times do i need to train a network to extract features from it activations forward propagation start from inputs compute activations from inputs to outputs training backward propagation compute a loss at the outputs backpropagate error towards the inputs first calculate error of output units and use this to change the top layer of weights update weights into j output k hidden j input i next calculate error for hidden units based on errors on the output units it feeds into output k hidden j input i finally update bottom layer of weights based on errors calculated for hidden units output k update weights into i hidden j input i denoted as diff notations i e how does the loss change as a function of the weights we want to find those weights change the weights in such a way that makes the loss decrease as fast as possible we ll update weights move in direction opposite to gradient time learning rate in dimension the derivative of a function in multiple dimensions the gradient is the vector of partial derivatives current w 78 55 81 loss gradient dw current w 78 12 55 81 33 loss w h first dim 78 12 55 81 33 loss gradient dw current w 11 78 12 55 81 33 loss w h first dim 0001 11 78 12 55 81 5 33 loss gradient dw 5 losses depend on the prediction functions scores e g fw x for class cat one set of weights for each class the prediction functions scores depend on the inputs x and the model parameters w hence losses depend on w e g for a linear classifier scores are for a neural network 10x1 array of numbers numbers indicating class scores parameters or weights in the second layer of weights one set of weights to compute the probability of each class suppose training examples classes with some w the scores are cat car frog suppose training examples classes with some w the scores are suppose training examples classes with some w the scores are suppose training examples classes with some w the scores are suppose training examples classes with some w the scores are multiclass svm loss given an example where where is the image and is the integer label and using the shorthand for the scores vector cat car frog 5 5 the svm loss has the form and the full training loss is the mean over all examples in the training data l losses 4 weight regularization λ regularization strength hyperparameter in common use regularization regularization dropout will see later in the case of a neural network regularization turns some neurons off they don t matter for computing an activation do not use size of neural network as a regularizer use stronger regularization instead you can play with this demo over at convnetjs more neurons more capacity cat car frog unnormalized probabilities exp normalize log unnormalized log probabilities probabilities in classic gradient descent we compute the gradient from the loss for all training examples could also only use some of the data for each gradient update then cycle through all training samples yes we cycle through the training examples multiple times each time we ve cycled through all of them once is called an epoch allows faster training e g on gpus parallelization the more weights you need to learn the more data you need that why with a deeper network you need more data for training than for a shallower network that why if you have sparse data you only train the last few layers of a deep net set these to the already learned weights from another network learn these on your own task convolutional neural networks feed forward feature extraction convolve input with learned filters apply non linearity spatial pooling downsample adapted from lana lazebnik apply learned filter weights one feature map per filter stride can be greater than faster less memory non linearity per element independent options tanh sigmoid exp x rectified linear unit relu avoids saturation issues spatial pooling sum or max over non overlapping overlapping regions role of pooling invariance to small transformations larger receptive fields neurons see more of input convolution layer image filter number the result of taking a dot product between the filter and a small chunk of the image i e 5 5 dimensional dot product bias convolution layer image filter activation map convolve slide over all spatial locations for example if we had filters we ll get separate activation maps activation maps convolution layer we stack these up to get a new image of size preview convnet is a sequence of convolutional layers interspersed with activation functions conv relu e g filters 6 conv relu 5x5x6 filters 10 conv relu from recent yann lecun slides n output size n f stride n e g n f stride 5 stride stride 33 in practice common to zero pad the border e g input filter applied with stride pad with pixel border what is the output output in general common to see conv layers with stride filters of size fxf and zero padding with f will preserve size spatially e g f zero pad with f 5 zero pad with f 7 zero pad with n padding f stride hough transform ransac won t be on the exam least squares line fitting data xn yn line equation yi m xi b find m b to minimize xi yi y mx b where line you found tells you point is along y axis where point really is along y axis you want to find a single line that explains all of the points in your data but data may be noisy voting is a general technique where we let the features vote for all models that are compatible with it cycle through features cast votes for model parameters look for model parameters that receive a lot of votes noise clutter features they will cast votes too but typically their votes should be inconsistent with the majority of good features y b x m image space hough parameter space connection between image x y and hough m b spaces a line in the image corresponds to a point in hough space y b x m image space hough parameter space connection between image x y and hough m b spaces a line in the image corresponds to a point in hough space what does a point in the image space map to answer the solutions of b this is a line in hough space to go from image space to hough space given a pair of points x y find all m b such that y mx b y b x m image space hough parameter space what are the line parameters for the line that contains both x0 and it is the intersection of the lines b y0 and b y b x m image space hough parameter space how can we use this to find the most likely parameters m b for the most prominent line in the image space let each edge point in image space vote for a set of possible parameters in hough space accumulate votes in discrete set of bins parameters with the most votes indicate line in image space hough machine analysis of bubble chamber pictures proc int conf high energy accelerators and instrumentation use a polar representation for the parameter space each line is a sinusoid in hough parameter space y x hough space x cos ysin initialize accumulator h to all zeros for each feature point x y ρ in the image θ gradient orientation at x y ρ x cos θ y sin θ h θ ρ h θ ρ θ end find the value of θ ρ where h θ ρ is a local maximum the detected line in the image is given by ρ x cos θ y sin θ hough transform for circles xi a yi b r2 x a r cos θ y b r sin θ for every edge pixel x y θ gradient orientation at x y for each possible radius value r a x r cos θ b y r sin θ h a b r end end given matched points in a and b estimate the translation of the object x b x a tx i i i i t y tx ty problem outliers multiple objects and or many to one matches hough transform solution x b x a tx initialize a grid of parameter values i i each matched pair casts a vote for consistent values find the parameters with the most votes i i t y support vector machines find linear function to separate positive and negative examples xi positive xi w b xi negative xi w b which line is best discriminative classifier based on optimal separating line for case maximize the margin between the positive and negative training examples want line that maximizes the margin xi positive yi xi negative yi xi w b xi w b for support vectors xi w b distance between point xi w b and line for support vectors w support vectors margin wτ x b m 2 maximize margin 2 w correctly classify all training data points xi positive yi xi w b xi negative yi xi w b quadratic optimization problem one constraint for each training point note sign trick solution w i i yi xi solution w i i yi xi b yi w xi for any support vector classification function f x sign w x b sign i i yi xi x b if f x classify as negative otherwise classify as positive notice that it relies on an inner product between the test point x and the support vectors xi the kernel trick the linear classifier relies on dot product between vectors k xi xj xi xj if every data point is mapped into high dimensional space via some transformation φ xi φ xi the dot product becomes k xi xj φ xi φ xj the kernel trick instead of explicitly computing the lifting transformation φ x define a kernel function k such that k xi xj φ xi φ xj nonlinear svms datasets that are linearly separable work out great x but what if the dataset is just too hard x we can map it to a higher dimensional space nonlinear kernel example consider the mapping x x x y x y xy k x y xy y2 examples of kernel functions linear k xi x j i j polynomials of degree up to d 𝐾 𝑥𝑖 𝑥𝑗 𝑥𝑖𝑇𝑥𝑗 𝑑 gaussian rbf k xi x j exp 2 2 histogram intersection k xi x j min xi k k x j k objective the w that minimizes constraints maximize margin objective misclassification cost data samples slack variable the w that minimizes constraints maximize margin minimize misclassification deformable part models zero shot learning image classification textual descriptions which image shows an aye aye description aye aye is nocturnal lives in trees has large eyes has long middle fingers we can classify based on textual descriptions vocabulary of attributes and class descriptions aye ayes have properties x and y but not z train classifiers for each attibute x y z from visual examples of related classes make image attributes predictions combine into decision this image is not an aye aye p x img 8 p y img 3 p z img 6 riptions but not z z from visual examples of related classes make image attributes predictions combine into decision this image is not an aye aye define attribute probability p az x p am x if az 1 m 1 p am x otherwise assign a given image to class z example cat attributes 1 1 bear attributes 1 image x probability of the attributes p attributei 1 x 7 4 0 8 0 5 0 1 probability that class x cat probability that class x bear unsupervised learning example clustering group together similar examples di xi unsupervised learning density estimation a probability density of a point in the two dimensional space model used here mixture of gaussians cs introduction to machine learning lecture topics support vector machines cont roc analysis nonparametric methods milos hauskrecht sennott square last lecture outline outline algorithms for linear decision boundary support vector machines maximum margin hyperplane support vectors support vector machines learning extensions to the linearly non separable case kernel functions problem there are multiple hyperplanes that separate the data points which one to choose problem there are multiple hyperplanes that separate the data points which one to choose the decision boundary that maximizes the distance of the and points from it for the maximum margin hyperplane only examples on the margin matter only these affect the distances these are called support vectors decision boundary defined by a set of support vectors sv and their alpha values support vectors a subset of datapoints in the training data that define the margin wˆ t x w ˆi i sv yi xi t x w classification decision lagrange multipliers yˆ sign ˆ y x t x w i i i i sv note that we do not have to explicitly compute wˆ this will be important for the nonlinear kernel case decision on a new x depends on the inner product between two examples the decision boundary wˆ t x w ˆi i sv yi xi t x w classification decision yˆ sign ˆ y x t x w i i i i sv similarly the optimization depends on xi x j j n i i n i j yi y j i j idea allow some flexibility on crossing the separating hyperplane the solution of the linearly non separable case has the same properties as the linearly separable case the decision boundary is defined only by a set of support vectors points that are on the margin or that cross the margin the decision boundary and the optimization can be expressed in terms of the inner product in between pairs of examples wˆ t x w ˆi i sv yi xi t x w yˆ sign wˆ t x w sign ˆ y w n n i i i sv j i i i j yi y j i j so far we have seen how to learn a linear decision boundary but what if the linear decision boundary is not good how we can learn a non linear decision boundaries with the svm the non linear case can be handled by using a set of features essentially we map input vectors to larger feature vectors x φ x example polynomial expansions note that feature expansions are typically high dimensional given the nonlinear feature mappings we can use the linear svm on the expanded feature vectors xt x φ x t φ x kernel function measures similarity k x x φ x t φ x support vector machines solution for nonlinear decision boundaries the decision boundary wˆ t x w ˆi i sv yi k xi x classification yˆ sign wˆ t x w sign ˆ y k x x w i i i i sv decision on a new x requires to compute the kernel function defining the similarity between the examples similarly the optimization depends on the kernel j n i i n i j yi y i j feature mapping kernel trick x φ x kernel function defines the inner product in the expanded high dimensional feature vectors and let us use the svm k x x φ x t φ x problem after expansion we need to perform inner products in a very high dimensional φ x space kernel trick if we choose the kernel function k x x wisely we can compute linear separation in the high dimensional feature space implicitly by working in the original input space assume x and a feature mapping that maps the input into a quadratic feature set x φ x x t kernel function for the feature space k x x φ x t φ x x x x x x x xt x the computation of the linear separation in the higher dimensional space is performed implicitly in the original input space linear separator in the expanded feature space linear kernel k x x xt x polynomial kernel k x x radial basis kernel xt x k k x x exp x x ml researchers have proposed kernels for comparison of variety of objects strings trees graphs cool thing svm algorithm can be now applied to classify a variety of objects evaluation of binary classifiers roc analysis for any data set we use to test the classification model on we can build a confusion matrix counts of examples with class label j that are classified with a label i target predict for any data set we use to test the model we can build a confusion matrix target predict accuracy for any data set we use to test the model we can build a confusion matrix target predict accuracy error accuracy entries in the confusion matrix for binary classification have names target predict tp true positive hit fp false positive false alarm tn true negative correct rejection fn false negative a miss sensitivity recall sens tp tp fn specificity spec tn tn fp positive predictive value precision ppt tp tp fp negative predictive value npv tn tn fn confusion matrix target predict row and column quantities sensitivity sens specificity spec positive predictive value ppv negative predictive value npv classifiers project datapoints to one dimensional space defined for example by wtx or p y x w decision boundary wtx wtx wtx decision boundary wtx binary decisions receiver operating curves x probabilities sens spec threshold p x p x x x x x receiver operating characteristic roc roc curve plots sn p x x x sp p x x x for different x x sens p x x x spec p x x x roc curve case case case 04 02 02 15 p x x x 5 5 0 0 p x x x receiver operating characteristic roc shows the discriminability between the two classes under different decision biases decision bias can be changed using different loss function quality of a classification model area under the roc best value worst no discriminability 0 5 nonparametric methods parametric distribution models are restricted to specific forms which may not always be suitable example modelling a multimodal distribution with a single unimodal model nonparametric approaches make few assumptions about the overall shape of the distribution being modelled cs machine learning nonparametric methods problem we have a set d of data points xi for i n we want to calculate p x for a target value of x parametric approach represents p x using a parametric density model with parameters θ fits the parameters θ wrt the data nonparametric approach does not make any parametric assumption estimates p x from all datapoints in d as if all d are parameters histogram methods partition the data space into distinct bins with widths i and count the number of observations ni in each bin p ni i nδ often the same width is used for all bins i acts as a smoothing parameter md bins assume observations drawn from a density p x and consider a small region r containing x such that if the volume of r v is sufficiently small p x is approximately constant over r and p r p x dx p p x v the probability that k out of n observations lie inside r is bin k n p and if n is large thus p x p v k np kernel density estimation fix v estimate k from the data let r be a hypercube centred on x and define the kernel function parzen window k x xn xi xni h i d otherwise h it follows that and hence k n x xn k h p x n n n hd x xn cs machine learning to avoid discontinuities in p x because of sharp boundaries use a smooth kernel e g a gaussian any kernel such that h acts as a smoother will work nearest neighbour density estimation fix k estimate v from the data consider a hyper sphere centred on x and let it grow to a volume v that includes k of the given n data points then k acts as a smoother nonparametric models more flexibility no density model is needed but require storing the entire dataset and the computation is performed with all data examples parametric models once fitted only parameters need to be stored they are much more efficient in terms of computation but the model needs to be picked in advance we have a set d of x y pairs we have a new data point x and want to assign it a class y how algorithm step estimate p y and p y 0 step estimate p x y and p x y 0 using nonparametric estimation methods and labels step choose a class by comparing p x y p y with p x y 0 p y we have a set d of x y pairs we have a new data point x and want to assign it a class y how algorithm k nearest neighbors step find the closest k examples to x step choose a class by considering the majority of the class labels a special case the nearest neighbour algorithm cs machine learning lecture density estimation milos hauskrecht sennott square probability well defined theory for representing and manipulating uncertainty axioms of probability let a and b be two events then p a p true and p false p a b p a p b p a b probability let a be an event and a its complement then p a p a p a a p false p a a p true joint probability joint probability let a and b be two events the probability of an event a b occurring jointly p a b p a b we can add more events say a b c p a b c p a b c independence independence let a b be two events the events are independent if p a b p a p b conditional probability conditional probability let a b be two events the conditional probability of a given b is defined as p a b p a b p b product rule a rewrite of the conditional probability p a b p a b p b cs machine learning bayes theorem bayes theorem p a b p b a p a p b why p a b p a b p b p a b p a b p b a p a p b a p a p b random variable a function that maps observed quantities to real valued outcomes binary random variables mapped to example tail mapped to head mapped to note only one value for each outcome either or p x p x probability of tail probability of head probability distribution p x assigns a probability to each possible outcome cs machine learning discrete random variable x based on tail head coin toss x based on the roll of a dice p x assigns a probability to each possible outcomes continuous x height of a person p x defined in terms of the probability density function p x dx cs machine learning data density estimation d dn di xi a vector of attribute values objective estimate the underlying probability distribution over variables x p x using examples in d standard iid assumptions samples are independent of each other come from the same identical distribution fixed p x learning via parameter estimation in this lecture we consider parametric density estimation basic settings a set of random variables x xd a model of the distribution over variables in x with parameters pˆ x data d dn objective find parameters the best such that p x fits data d ml parameter estimation model pˆ x p x θ data d dn maximum likelihood ml find that maximizes max p d p d p d p dn independent p p p dn examples i p di n log p d logp di i parameter estimation coin example coin example we have a coin that can be biased outcomes two possible values head or tail data d a sequence of outcomes xi such that head tail xi xi model probability of a head probability of a tail objective we would like to estimate the probability of a head ˆ from data probability of an outcome data d a sequence of outcomes xi such that head tail xi xi model probability of a head probability of a tail assume we know the probability probability of an outcome of a coin flip xi p xi xi xi bernoulli distribution combines the probability of a head and a tail so that xi is going to pick its correct probability gives for xi gives for xi probability of a sequence of outcomes data d a sequence of outcomes xi such that head tail xi xi model probability of a head probability of a tail assume a sequence of coin flips d h h t h t h encoded as d what is the probability of observing a data sequence d p d maximum likelihood ml estimate likelihood of data p d i xi xi maximum likelihood estimate ml arg max p d optimize log likelihood the same as maximizing likelihood n l d log p d log xi xi n i n n xi i log xi log log xi i log i xi number of heads seen number of tails seen maximum likelihood ml estimate optimize log likelihood l d log log set derivative to zero l d solving maximum likelihood estimate example assume the unknown and possibly biased coin probability of the head is data h h t t h h t h t h t t t h t h h h h t h h h h t heads tails what is the ml estimate of the probability of a head and a tail maximum likelihood estimate example assume the unknown and possibly biased coin probability of the head is data h h t t h h t h t h t t t h t h h h h t h h h h t heads tails what is the ml estimate of the probability of head and tail head 0 ml n n n tail 0 ml n n n maximum a posteriori estimate maximum a posteriori estimate selects the mode of the posterior distribution map likelihood of data arg max p d prior p d p d p p d via bayes rule normalizing factor p d i xi xi p is the prior probability on how to choose the prior probability cs machine learning prior distribution choice of prior beta distribution p beta x a gamma function x x x for integer values of x n n why to use beta distribution beta distribution fits bernoulli trials conjugate choices p d posterior distribution is again a beta distribution p d p d beta p d beta cs machine learning beta distribution p beta a b a b a b a b cs machine learning posterior distribution beta beta p d p d beta p d beta n cs machine learning maximum a posterior probability maximum a posteriori estimate selects the mode of the posterior distribution p d p d beta p d beta n n2 notice that parameters of the prior act like counts of heads and tails sometimes they are also referred to as prior counts cs machine learning map estimate example assume the unknown and possibly biased coin probability of the head is data h h t t h h t h t h t t t h t h h h h t h h h h t heads tails assume p beta what is the map estimate cs machine learning map estimate example assume the unknown and possibly biased coin probability of the head is data h h t t h h t h t h t t t h t h h h h t h h h h t heads tails assume p beta what is the map estimate n1 map n n1 n2 cs machine learning map estimate example note that the prior and data fit data likelihood are combined the map can be biased with large prior counts it is hard to overturn it with a smaller sample size data h h t t h h t h t h t t t h t h h h h t h h h h t heads tails assume p p beta beta 5 5 5 20 map map cs machine learning non linear extension of logistic regression use feature basis functions to model nonlinearities the same trick as used for the linear regression linear regression logistic regression f x wj j x j f x g wj j x j j x an arbitrary function of x x x xd wm m x cs machine learning rules for submitting programs http people cs pitt edu milos courses programsubmissions html cs rules for submitting programming assignments rules programs all programs submitted to us should be written in matlab files with m extension data please avoid sending us back data files unless your are explictly asked to do so in the assignment submission format windows submit the source code for all programs and files requested in a single zip file created with winzip pkzip use yourfirstnameyourlastname zip to name the file for example janedoe zip unix submit all programs and files requested in a single tar archive file use yourfirstnameyourlastname tar to name the file for example janedoe tar creating tar files in unix to create a tar file use tar cvf janedoe tar in unix which archives all the files in the current directory and stores them in the file janedoe tar to archive only m files use tar cvf janedoe tar m to extract the archived files in the current directory use tar xvf janedoe tar command please avoid including directory structure into the archive files submission the file yourfirstnameyourlastname tar or zip should be submitted using the courseweb system http courseweb pitt edu maintained by the university of pittsburgh once you login to the system please select cs from the course menu and use the assignments button to see the assignment and the assignment submission page the above file should be submitted using the file attachment option in exceptional cases when courseweb system does not work you may submit your archive file via email sent to the ta of the course last updated by milos on university of pittsburgh introduction to machine learning handout professor milos hauskrecht march problem assignment due thursday march problem bayesian belief networks assume the bayesian belief network in the figure below assume that every variable in the network is binary representing t f values except variable d that can take on three possible values t f x x stands for undecided assume you want to compute p b t e t show how would you compute the expression more efficiently you do not have to provide counts of operations it is sufficient to illustrate what you would do to improve the efficiency of the inference problem pneumonia diagnosis assume a bayesian belief network with the naive bayes structure for a simplified version of the pneumonia problem the variable pneumonia is at the root of the naive bayes structure and features attributes fever paleness cough and highwbccount are conditionally independent given pneumonia assume that random variables in our model are discrete with the following set of values pneumonia true false fever true false paleness true false cough true false highwbccount true false part a write and submit a program m that takes the data in the file pneumonia tex and learns the ml estimates of parameters of the bbn network the pneumonia file includes examples in rows the features are in columns in the fol lowing order fever paleness cough highwbccount last column represents pneumonia variable the data are represented such that true corresponds to and false to report the parameters of the network part b assume that the patient comes with the following set symptoms fever and cough are true paleness and highwbccount are false what is the probability p p neumonia t f ever t p aleness f cough t highw bccount f that is the probability that the patient suffers from pneumonia given the symptoms simplify the expression as much as possible before plugging in the values part c assume that the patient reports cough and a fever they are true and values of paleness and highwbccount are not known simplify the expression as much as possi ble compute the probability p p neumonia t f ever t cough t of the patient suffering from the pneumonia given the symptoms part d write and submit a matlab program that reads in a combi nation of patient symptoms the values of fever paleness cough highwbccount from the file example txt and computes the probability of p p neumonia t rue currentsymptoms the values of symptoms for the current patient case are given in the following order f ever p aleness cough highw bccount the values are encoded as follows for false for true and for not given not known for example the symptoms of the patient case in part b are encoded as a vector of values 0 0 and symptoms in part c as note please note that your program should work on an arbitrary combination of input values in example txt file university of pittsburgh introduction to machine learning handout professor milos hauskrecht january problem assignment due thursday january problem install matlab on your computer or access it in one of the cssd labs problem exploratory data analysis in this problem we will explore and analyze the dataset pima txt provided on the course web page to do the analysis you will need to write short programs keep the code you write for future problem sets the pima txt is described in the file pima desc txt the dataset consists of attributes and a binary attribute defining the class label the presence of diabetes data entries are organized in rows such that attributes come first and the class label is last answer the following questions with the help of matlab a what is the range minimum and maximum value for each of the attributes b what are the means and variances of every attribute c calculate and report correlations between the first attributes in columns and the target class attribute column use matlab corrcoef function to do the calculations what is the attribute with the highest positive correlation to the target attribute do you think it is the most or the least helpful attribute in predicting the target class explain d calculate all correlations between attributes using the corrcoef function which two attributes have the largest mutual correlation in the dataset e assume we want to predict a target class given all attributes what do you think does it help or not in prediction to have attributes that are fully correlated explain while the analysis using basic statistics as performed above conveys a lot of information about the data and lets us make some conclusions about the importance of attributes or their relation it is often very useful to inspect the data visually and get more insight into various shapes and patterns they hide in the following we will inspect the data using histograms and scatter plots f histogram analysis gives us more information about the distribution of attribute values write and submit a matlab function histogram analysis that takes the data for an attribute as a vector and plots a histogram with bins using matlab hist function analyze attributes in the data using the function answer the following questions which histogram resambles most the normal distribution in your report show at least two histograms including the choice you picked as the most normally distributed attribute g scatter plots plots let us inspect the relations between pairs of attributes write and submit a function scatter plot that takes pairs of values for two attributes and plots them as points in use matlab function scatter to do the plot analyze the pairwise relations between attributes in the pima dataset using the scatter plot function answer the following questions is there a scatter plot that indicates possible linear dependency between two variables select two random scatter plots and include them in the report with every plot include the corresponding attribute names problem data preprocessing before applying learning algorithms some data preprocessing may be necessary to practice matlab we will write programs for two possible preprocessing tasks normalization and discretization of continuous values a write and submit a function normalize that takes an unnormalized vector of attribute values and returns the vector of values normalized according to the data mean and standard deviation the normalized value should be x µx xnorm σx where x is an unnormalized value µx is the mean value of the attribute in the data and σx its standard deviation test your function on attribute of the pima dataset report normalized values of the attribute for the first five entries in the dataset b write and submit a function discretize attribute that takes a vector of attribute values a number k number of bins and assings each value to one of the k bins bins are of equal length and should cover the range of values that is determined by the min and the max operations on the vector every bin is given a numerical label such that the smallest value is in bin and the largest attribute value is in bin k the bin label represents the result of discretization test your function on attribute of the pima dataset assume we use bins report new discretized values of the attribute for the first five entries in the dataset problem data set splitting in this problem we practice a splitting of the dataset along an attribute value and b a random splitting of the dataset into the training and testing sets a split pima txt data into two data subsets one that includes only examples with class label the other one with class values calculate and report the mean and standard deviation of each attribute in these two subsets hint try to use matlab function find to split the data b write and submit a function that takes the dataset represented as a matrix and the probability ptrain of selecting the data entry a row in the matrix into the training set the function should return two nonoverlapping datasets the training and testing data such that every entry is selected to the training set randomly with probability ptrain test your divideset function on the pima dataset run the function times with probability ptrain and report the average length of the training dataset c if your code to part b is correct you should see some variation in the size of the training sets write and submit a function that takes the dataset represented as a matrix and the probability ptrain and returns two nonoverlapping datasets the training and testing data that mimic closely the distribution defined by ptrain basically your function should decide first on the number of examples that will go into training and test sets and after that choose randomly examples that will go into each set the algorithm if you run it should always give you different training and test sets but their sizes should stay the same for the same dataset university of pittsburgh machine learning handout professor milos hauskrecht january problem assignment due thursday january problem mean estimates and the effect of the sample size in this problem we study the influence of the sample size on the estimate of the mean the data for this experiments are in file mean study data txt in the homework assignment folder the data were generated from the normal distribution with mean and standard deviation part load the data in the mean study data txt calculate and report the mean and standard deviation of the data part write and submit a function newdata subsample data k that randomly selects k instances from the data in the mean study data txt part use the above function to randomly generate subsamples of the data of size for each subsample calculate its mean and save the results in the vector of means plot a histogram of the mean values using bins part include the histogram in your report analyze the mean that was calculated in step on all examples in the dataset and the means calculated on subsamples of size report your observations part repeat step part but now generate subsamples of size include the histogram in the report and compare it to the histogram generated in for subsamples of size and to the mean of the original data what are the differences what conclusions can you make by comparing means for subsamples of size and problem train test splitting using k fold crossvalidation when we testing the performance of a learning algorithm using a simple holdout method the results may be biased by the training testing data split to alleviate the problem various random resampling schemes such as k fold cross validation random subsampling or bootstrap see lecture notes for class can be applied to estimate the statistics of interest by averaging the results across multiple train test splits please do the following tasks part please write and submit the function train test kfold crossvalidation data k m that takes the data k the number of folds and m the target fold as inputs and returns the training and testing data sets such that the testing set corresponds to m th fold under the k th fold cross validation scheme to implement the procedure please place the folds over indexes of the data by assuring that each fold has equal number of entries that do not overlap if this is not possible the fold sizes number of instances in each fold should differ by at most one the file should be named kfold crossvalidation m part run test your function on data in the file resampling data txt more specif ically run your kfold crossvalidation function on all data in the file by setting k number of folds to and by varying the test fold index parameter m from to for each test data generated for the different value of m that were returned by your function calculate the mean and std and report them problem function derivatives machine learning as a field builds upon knowledge of math statistics control and decision theories in many cases the learning process is formulated as an optimization problem with some objective function say minθ f θ where θ are parameters we want to optimize if this function is differentiable the optimum either local or global must satisfy f θ dθ in this problem we practice the calculation of function derivatives please derive a b d dx d c dx x d d ex dx d dx sin x e d f dx x d g dx x d h j dx ln x d dx d ln xi university of pittsburgh introduction to machine learning handout professor milos hauskrecht january problem assignment due thursday february problem bernoulli trials assume we have conducted a coin toss experiment with coin flips the results of the experiment are in file coin txt where means a head and means a tail assume that θ represents the probability of observing a head a what is an ml estimate of θ b assume the prior on θ is defined by a beta distribution beta θ plot and report both the prior and the posterior distribution on θ c calculate and report the map estimate of the θ for the prior in part b d repeat part b and c by assuming that the prior on θ follows beta θ problem multivariate gaussian assume the pairs of real valued measurements in file gaussian txt a plot the data using the scatter plot matlab function b calculate and report the ml estimate of the mean and the covariance matrix from the data plot and report the resulting gaussian distribution note you need to plot this in c now consider each measurement in gaussian txt separately calculate the ml estimate of the mean and variance of these measurements plot and report the indi vidual distributions d do you believe the mutlivariate gaussian model is a better than two separate univariate gaussian models explain why yes or why not problem poisson distribution the poisson distribution is used to model the number of random arrivals to a system over a fixed period of time examples of systems in which events are determined by random arrivals are arrivals of customers requesting the service occurence of natural disasters such as floods etc the poisson distribution is defined as e λλx p x λ x for x where λ is a parameter the mean of the poisson distribution is λ answer the following questions a plot and report the probability function for poisson distributions with parameters λ and λ note that the poisson model is defined over nonenegative integers only b given a set of independent observations xn from a poisson distribution the ml estimate of the parameter λ is n λml xi n i assume the data in poisson txt file that represent the number of incoming phone calls received over a fixed period of time compute and report the ml estimate of the parameter λ also plot and report the probability fuction for the ml parameter c the conjugate prior for λ defining the poisson distribution is gamma distribution it is defined as p λ a b baγ a λ a e λ plot and report the gamma distribution for the following set of parameters a b and a b d assuming the prior distribution on λ is gamma λ a b the posterior distribution for λ after seeing observations d xn is again gamma distribution n p λ d gamma λ a xi b i nb please use data in poisson txt to calculate and plot the posterior distributions of λ for both priors in part c note it is not neccessary to submit any matlab code for this assignment just include the plots in your report university of pittsburgh introduction to machine learning handout professor milos hauskrecht february problem assignment due thursday february linear regression in this problem set we use the boston housing dataset from the cmu statlib library that concerns prices of housing in boston suburbs a data sample consists of attribute values indicating parameters like crime rate accessibility to major highways etc and the median value of housing in thousands we would like to predict the data are in the file housing txt the description of the data is in the file housing desc txt on the course web page part exploratory data analysis examine the dataset housing txt using matlab answer the following questions a how many binary attributes are in the data set list the attributes b calculate and report correlations in between the first attributes columns and the target attribute column what are the attribute names with the highest positive and negative correlations to the target attribute c note that the correlation is a linear measure of similarity examine scatter plots for attributes and the target attribute using the function you wrote in problem set which scatter plot looks the most linear and which looks the most nonlinear plot these scatter plots and briefly in sentences explain your choice d calculate all correlations between the columns using the corrcoef function which two attributes have the largest mutual correlation in the dataset part linear regression our goal is to predict the median value of housing based on the values of attributes for your convenience the data has been divided into two datasets a training dataset housing train txt you should use in the learning phase and a testing dataset housing test txt to be used for testing assume that we choose a linear regression model to predict the target attribute using matlab a write a function lr solve that takes x and y components of the data x is a matrix of inputs where rows correspond to examples and returns a vector of coeffi cients w with the minimal mean square fit hint you can use backslash operator to do the least squares regression directly check matlab help b write a function lr predict that takes input components of the test data x and a fixed set of weights w and computes vector of linear predictions y c write and submit the program m that loads the train and test set learns the weights for the training set and computes the mean squared error of your predictor on both the training and testing data set see rules for submission of programs on the course webpage d in your report please list the resulting weights and both mean square errors compare the errors for the training and testing set which one is better part online gradient descent the linear regression model can be also learned using the gradient descent method implement an online gradient descent procedure for finding the regression coefficients your program should start with zero weights all weights set to at the beginning update weights using the annealed learning rate t where t denotes the t th update step thus for the first data point the learning rate is for the second it is for the rd is and so on repeat the update procedure for steps reusing the examples in the training data if neccessary hint the index of the i th example in the training set of size n can be obtained by i mod n operation return the final set of weights write a program m that runs the gradient procedure on the data and at the end prints the mean test and train errors your program should normalize the data before running the method by finding the mean and std of the input attributes on the train data see the lecture notes and correcting normalizing both the train and test inputs run it and report the results give the mean errors for both the training and test set is the result better or worse than the one obtained by solving the regression problem exactly run the gradient descent on the un normalized dataset what happened modify m from part b such that it lets you to progressively observe changes in the mean train and test errors use functions init progress graph and add to progress graph on the course web page the init progress graph initializes the graph structure and add to progress graph lets you add new data entries on fly to the graph using the two functions plot the mean squared errors for the training and test test for every iteration steps submit the program and include the graph in the report d experiment with the gradient descent procedure try to use fixed learning rate say or different number of update steps say and you may want to change the learning rate schedule as well try for example n report your results and any interesting behaviors you observe part regression with polynomials assume we are not happy with the predictive accuracy of the linear model and we decided to explore a more complex model for predicting housing values assume we decide to use a quadratic polynomial to model the relation between y and x f x w wixi wijxixj i i j i a write a function extendx that takes an input x and returns an expanded x that includes all linear and degree two polynomials b what happened to the binary attribute after the transformation c write and submit a matlab program 4 m that computes the regression coefficients for the extended input and both train and test errors for the result d report both errors in your report and compare them with the results in part what do you see which method would you use for the prediction why please do not turn in the weights for this part in your report university of pittsburgh introduction to machine learning handout professor milos hauskrecht february problem assignment due thursday february problem data analysis the dataset we use in this problem set is very simple and consists of a two dimensional input and a class label the data are available on the course web page and are di vided into two files one used for training classification train txt the other for testing classification test txt the data in files are in rows such that first two columns represent inputs and the last third column the class label or since inputs are only two dimensional we can easily visualize the data for the two classes in a plot write a program that plots the input data points in classification train txt such that the plot distinguishes between data points with different class labels use different color and symbol for a point e g x or o include the plot in your report is it possible to separate the two classes perfectly with a linear decision boundary problem logistic regression we are interested in building a classifier based on the logistic regression model and the gradient optimization methods a during the class you were given the expression for the gradient of the logistic regression model use the loglikelihood setup from the lecture to derive the expression show clearly the steps of the derivation please remember that the default gradient takes into account all datapoints in the training set b write and submit a gradient procedure glr m for updating the parameters of the logistic regression model your gradient procedure should start from unit weights all weights set to at the beginning use the annealed learning rate k executes for k steps where k is the parameter of the procedures c write and submit a program m that runs the glr function for steps and after the training computes mean misclassification errors for both the training and test set in your report include the resulting weights and misclassification errors d update the m with plot functions that let you observe the progress of the errors after every update steps use functions defined in ps for this purpose include the resulting graph in your report e experiment with the glr function by i changing the number of steps k and ii trying different learning rates in particular try some constant learning rates and k learning rate schedule report the results and graph from your experiments and explanations of behaviors you have observed problem online gradient descent a write an on line gradient procedure for learning the weight parameters the procedure should select sequentially examples from the training set repeating the examples whenever necessary b write a program m that performs the on line gradient method for steps uses progress plots and at the end computes the confusion matrices and mean misclassifications errors for both datasets c repeat the analysis in problem compare the results of problem and problem what are the differences in the behavior of the two gradient methods problem generative classification model an alernative approach is to learn a generative model with class conditional densities and class priors and use the parameters of such a model to do the prediction assume that an input x for each class c or follows a multivariate normal distribution that is p x c n p x c n further assume that the prior probability of a class is represented by a bernoulli distribution parameters of the generative model can be computed from the training data using the density estimation techniques such as maximum likelihood estimation once this is ac complished we can use the estimates to make class predictions for new inputs let θ µ σ µ σ θ c represent parameter estimates to predict the class we use discriminant functions based on the posterior probability of a class given the input and model parameters this can be computed via bayes rule g x p c x θ p x µ σ θ c p x µ σ θ c p x µ σ θ c assume we want to use a generative model in which the two class conditional densities share the same covariance matrix σ that is p x c n σ p x c n σ provide the following answers a give the formula for computing ml estimates of means of class conditional den sities b how would you go about computing the estimate of the covariance matrix σ note that the estimate of σ must combine both class and class examples c how would you estimate the prior of class θ c d implement function m ax likelihood that computes the estimates of the model parameters using the training set e implement the function p redict class that chooses the class using the discriminant functions based on class posteriors f write and submit a program m that learns the generative model and then uses it to compute the predictions the program should compute mean misclassifica tion errors for both training and testing datasets g report the results parameters of the generative model and errors compare them to the results obtained in problem problem the naive bayes model the naive bayes model is a special case of the generative classification model in this case p x c d p xi c and p x c d p xi c where p xi c are class conditional distributions for each input xi for the problem we study in this assign ment assume the class conditional distributions are univariate gaussians with parameters µi c σi c µi c σi c for every input xi a implement function m ax likelihood n b that computes the estimates of the model parameters using the training set b implement the function p redict class n b that chooses the class using the dis criminant functions based on class posteriors c write and submit a program m that learns the naive bayes model and then uses it to compute the predictions the program should compute mean misclas sification errors for both the training and testing datasets d report the results parameters of the model and errors compare them to the previous results university of pittsburgh intro to machine learning handout professor milos hauskrecht february problem assignment due thursday february in this problem we shall investigate the pima dataset and learn classification models for it recall we performed some exploratory analysis of the pima dataset in problem set you can download the dataset pima txt and its description pima desc txt from the course web page in addition to the complete dataset pima txt you have pima train txt and pima test txt you will need to use for training and testing purposes the dataset has been obtained from the uc irvine machine learning repository http ics uci edu mlearn m lrepository html problem logistic regression model first we try the logistic regression model in combination with gradient methods give solutions to the following tasks a write a program that normalizes inputs in the pima dataset there is no need to normalize outputs based on the data in the training set apply the procedure to normalize both the training and test set data while generating two new files pima train norm txt and pima test norm txt b familiarize yourself with a batch mode gradient procedure in file log regression m in which all data points are considered at the same time recall you were asked to write the procedure in problem set c implement and submit a program m that runs the gradient procedure on the training dataset for i teration steps also called epochs initialize all weights to at the beginning use i learning rate schedule d include graph functions for monitoring the progress of errors in m as used in the previous problem set hw compute mean misclassification error for both the training and testing data at the end in the report include final training and test misclassification errors confusion matrices for the train and test sets sensitivity and specificity of the model on the test set e experiment with the learning algorithm by changing initial weights learning sched ule number of epochs report training and test misclassification errors what was the best result you could get problem naive bayes model the naive bayes model defines a generative classifier model in which all features are inde pendent given the class label in such a case the class conditional densities over many input variables can be decomposed into a set of independent class conditional densities one for ev ery input variable for example the conditional probability of an input x xd given class in the naive bayes model is decomposed as d p x y p xi y i one important concern is the choice of an appropriate parameterization of class conditional densities typically we do not choose the distributions arbitrarily instead we want to make a good educated guess exploratory data analysis can help us greatly to recognize types of densities that appear to match the data the best problem exploratory data analysis we have performed the exploratory analysis of the pima dataset in problem set here we reuse the programs created there and apply them to study the density models we choose to parameterize our naive bayes model part a write and submit a program m that divides pima txt data into two subsets one with all examples with class and another with all examples with class analyzes examples in two subsets using histograms histograms should give you more information about the shape of the distribution of attributes you can use the function histogram analysis m for this purpose part b what distribution density would you use to fit the values of attributes to in the pima dataset choices one typically considers are bernoulli binomial multinomial normal poisson gamma exponential distributions problem learning of the naive bayes classifier the learning of the naive bayes model corresponds to the estimation of parameters of class conditional distributions p xi y p xi y for all input components i from data and estimation of class priors p y p y thus the learning boils down to a number of smaller density estimation problems assume that class conditional densities for pima dataset have the following form class conditionals for inputs 5 take the form of exponential distribution the exponential distribution is defined as p x µ exp x where µ is the parameter exponential distribution is a special case of the gamma distribution and belongs to the exponential family class conditionals for inputs follow univariate normal distributions p x µ σ σ exp x µ with mean and standard deviation being the two parameters in addition assume that priors on classes follow a bernoulli distribution θ θx θ x for x part a write and submit a program m that computes and returns the estimates of the parameters of the naive bayes model using the training set pima train txt the parameters include priors on classes class conditionals one for every input component and class label to fit exponential distibutions use matlab function expfit to fit normal distributions use function normfit see also matlab help example application of expfit and normfit functions all inputs x with label class all inputs x with label class fit the exponential class conditional for input attribute and class p y expfit fit the exponential class conditional for input attribute and class p y expfit fitting of the class conditional of the second attribute with normal distribution class condtional for class p y normfit etc part b list parameters found by your program in the report problem classification with the naive bayes model once the parameters of the naive bayes model are learned estimated the decision about the class for a specific input x can be made by designing the appropriate discriminant functions typically there are based on class posteriors thus a classification problems boils down to the problem of comparison of posteriors of classes for x these are computed through the bayes rule p y x d d i p xi y p y d i p xi y l p y i p xi y l p y note that in order to make the best posterior choice it is sufficient to compare the following discriminant functions based on log posteriors d x log p xi y log p y i d x log p xi y log p y i part a write and submit a program m that calls a function predict n b that predicts class labels for inputs based on class poste rior the discriminant functions you need to use here are given in expressions and and use parameters obtained in problem 2 uses predict n b to compute the misclassification error of the naive bayes classifier on both training and test datasets report the errors calculates and reports a confusion matrix for the test and training sets use function accuracy m part b in your report include training and test misclassification errors confusion matrices for the train and test sets sensitivity and specificity of the model on the test set part c compare results for the mean misclassification errors for the logistic regression model to the naive bayes classifier problem support vector machines support vector machines represent yet another technique one can apply to the problem of binary classification the idea is to find the hyperplane that separates the examples in two classes the best the best hyperplane is defined in terms of the maximum margin the learning problem reduces as usually to optimization in this case a quadratic optimization problem there is a number of implementations of svm algorithms with better or worse running time performances here we use a matlab code implementing svm solver for the linear decision boundary proposed by o l mangasarian and d musicant the paper describing this method can be downloaded electronically at the svm solver is in files svml m and svml itsol m that can be downloaded from the course web page svml itsol m is a slightly modified version of the original program by o l mangasarian and musicant to run it you call svml m that takes care of converting outputs from class labels to and sets other parameters of the lagrangian svm write and submit a matlab program m that loads training and test data calls linear svm solver to learn the linear decision boundary computes the mean misclassification error for both the training and test data computes the confusion matrix for the test set write a special function confusion matrix that takes class labels from the data and compares them to those computed by the classifier in your report include the misclassification errors and confusion matrix obtained for the train and test sets compare the result to the results of the logistic regression and neural network models optional if you are interested experimenting with existing svms tools including tools supporting non linear kernels please check out the following sofware packages liblinear libsvm and svmlight all these can be interfaced with matlab problem roc analysis the roc analysis let us explore the ability of the classification model to discriminate in between the two classes including possible sensitivity and specificity trade offs in the roc analysis we assume a changing threshold for calling class based on the projection defined by the model this can be either p y x for the logistic regression and the naive bayes or wt x b for the svm part a familiarize yourself with the function perfcurve in matlab that lets you calculate coordinates of points defining the roc curve as well as the area under the roc curve auroc part b use the function perfcurve to plot the roc curve and calculate auc on the testing set for the models you build in problems 1 2 all models should be trained on the training set part c please include the roc curves and the auc statistics in the report compare the roc curves and their auc statistics what do you think which model is better university of pittsburgh intro to machine learning handout professor milos hauskrecht february problem assignment due thursday march in this problem we shall continue our investigation of the pima dataset using new classifi cation models multilayer neural networks decision trees and the nearest neighbor classifier you can download the dataset pima txt and its description pima desc txt from the course web page in addition to the complete dataset pima txt you have pima train txt and pima test txt you will need to use for training and testing purposes the dataset has been obtained from the uc irvine machine learning repository http ics uci edu mlearn m lrepository html problem neural network toolbox in matlab we start with the neural network toolbox part a in homework and you were asked to implement and run a gradient al gorithm for learning the logistic regression model however the logistic regression model is also supported and implemented in matlab within its neural network tool box please familiarize yourself and run logistic n n m function that is given to you and implements the logistic regression model using the toolbox functions try to change the parameters of the model such as the optimization method and the num ber of epochs report the weights with the best mean misclassification rate for the test set and any graphs you have found interesting part b multilayer neural network the limitation of the logistic regression model is that it uses a linear decision boundary one way around this is problem is to use non linear features in combination with a linear model however in this case feature function must be fixed and selected in advance multilayer neural networks allow us to represent non linear models by cascading multiple nonlinear units multilayer neural networks can be built with the nn matlab toolbox write a program m that implements a neural network with two hidden units that is there are two nonlinear units we feed the input to and one unit that combines their results run the program for epochs calculate the mean misclassification errors for the training and testing data report errors and compare them to results obtained for the logistic regression model for part a which model is better why part c experiment with neural networks with and hidden units while changing other learning parameters e g the optimization method or the number of epochs analyze and compare and report the results problem decision trees the decision tree approach is yet another classification methods we covered in the course is the decision tree method the method builds a tree by recursively splitting the training set using one of the attributes by optimizing the gain with respect to some impurity measure part a the script run dt m shows how to train display and apply the decision tree the script first builds a default tree with minimal restrictions on its size and after that the tree obtained by restricting the number of nodes in the tree please run and familiarize yourself with the code what do you think which tree is better for prediction why should we always try to backprune it part b experiment with the decision tree function fitctree m and its optional pa rameters modifying the algorithm and the tree built report the results of your investigations in the report by listing the settings used for the tree learning algorithm and obtained results you can find the different settings in the matlab help documents problem k nearest neighbor classifier another classification method covered in the course is the k nearest neighbor classifier knn part a the script run kn n m shows how to classify examples with the knnclassify method implemented in matlab the script uses euclidean metric and the number of neighbors is set to please run and familiarize yourself with the code please experiment with knn by modifying the number of neighbors and report the results on the test set please attempt and neighbors in addition to part b please normalize the data both the train and test set before running the knn classifier with the euclidean distance again did the result improve part c the knn classifier assumes that only k closest points are used to determine the class label for each test data point moreover all these neighbors carry equal weight that is each training datapoint among the top k neighbors carries weight to account for training point differences the knn classifier can be modified using smooth kernels such as the gaussian kernel in this case each datapoint in the training set is used to calculate the class label and its contribution is weighted proportionally to its distance from the target point we want to classify write and submit function soft nn m that takes as arguments x component of the training data d y component of d the new datapoint x and the smoothness parameter h defining the gaussian kernel the function returns the label for the new datapoint x please experiment with the new function on pima dataset while varying the parameter h please normalize the data before using them report the results of the experiment in your report how does the result compares to the knn classifier problem bayesian belief networks assume the bayesian belief network in the figure below assume that every variable in the network is binary representing t f values except variable d that can take on three possible values t f x x stands for undecided the belief network encodes the full joint distribution over random variables represented by nodes by exploiting conditional independences that hold among variables part a give examples of at least five independences marginal or conditional that hold among variables in the network part b what is the number of parameters needed to define the full joint distribution over variables in the problem domain without the belief network representation part c show how to compute the joint probability p a t b t c f d f e f f t using the parameters of the belief network model part d what is the number of parameters needed to define the belief network in the figure introduction to machine learning cs spring course description the goal of the field of machine learning is to build computer systems that learn from experience and that are capable to adapt to their environments learning techniques and methods developed by researchers in this field have been successfully applied to a variety of learning tasks in a broad range of areas including for example text classification gene discovery financial forecasting credit card fraud detection collaborative filtering design of adaptive web agents and others this introductory machine learning course will give an overview of many models and algorithms used in modern machine learning including linear models multi layer neural networks support vector machines density estimation methods bayesian belief networks clustering ensemble methods and reinforcement of learning the course will give the student the basic ideas and intuition behind these methods as well as a more formal understanding of how and why they work through homework assignments students will have an opportunity to experiment with many machine learning techniques and apply them to various real world datasets prerequisites stat or or equivalent and cs or the permission of the instructor textbook chris bishop pattern recognition and machine learning springer homework assignments homework assignments will have mostly a character of projects and will require you to implement some of the learning algorithms covered during lectures programming assignments will be implemented in matlab please visit to see how to get a free copy of matlab license for students the assignments both reports and programming parts are due at the beginning of the class on the day specified on the assignment in general no extensions will be granted policy on collaboration no collaboration on homework assignments programs and exams unless you are specifically instructed to work in groups is permitted grading the final grade for the course will be determined based on homework assignments exams and your lecture attendance and activity the midterm exam will be held prior to the spring break in late feb early march policy on cheating cheating and any other anti intellectual behavior including giving your work to someone else will be dealt with severely and will result in the fail f grade if you feel you may have violated the rules speak to us as soon as possible please make sure you read understand and abide by the academic integrity code for the university of pittsburgh and faculty and college of arts and sciences students with disabilities if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and drs www drs pitt edu william pitt union for asl users as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course tentative syllabus machine learning introduction density estimation supervised learning linear and logistic regression generative classification models multi layer neural networks support vector machines unsupervised learning bayesian belief networks bbns learning parameters and structure of bbns expectation maximization clustering dimensionality reduction feature selection feature filtering wrapper methods pca ensemble methods mixtures of experts bagging and boosting reinforcement learning table of contents architecture overview convnet layers convolutional layer pooling layer normalization layer fully connected layer converting fully connected layers to convolutional layers convnet architectures layer patterns layer sizing patterns case studies lenet alexnet zfnet googlenet vggnet computational considerations additional references convolutional neural networks cnns convnets convolutional neural networks are very similar to ordinary neural networks from the previous chapter they are made up of neurons that have learnable weights and biases each neuron receives some inputs performs a dot product and optionally follows it with a non linearity the whole network still expresses a single differentiable score function from the raw image pixels on one end to class scores at the other and they still have a loss function e g svm softmax on the last fully connected layer and all the tips tricks we developed for learning regular neural networks still apply so what does change convnet architectures make the explicit assumption that the inputs are images which allows us to encode certain properties into the architecture these then make the forward function more efકcient to implement and vastly reduce the amount of parameters in the network architecture overview recall regular neural nets as we saw in the previous chapter neural networks receive an input a single vector and transform it through a series of hidden layers each hidden layer is made up of a set of neurons where each neuron is fully connected to all neurons in the previous layer and where neurons in a single layer function completely independently and do not share any connections the last fully connected layer is called the output layer and in classiકcation settings it represents the class scores regular neural nets don t scale well to full images in cifar images are only of size wide high color channels so a single fully connected neuron in a કrst hidden layer of a regular neural network would have weights this amount still seems manageable but clearly this fully connected structure does not scale to larger images for example an image of more respectible size e g would lead to neurons that have weights moreover we would almost certainly want to have several such neurons so the parameters would add up quickly clearly this full connectivity is wasteful and the huge number of parameters would quickly lead to overકtting volumes of neurons convolutional neural networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way in particular unlike a regular neural network the layers of a convnet have neurons arranged in dimensions width height depth note that the word depth here refers to the third dimension of an activation volume not to the depth of a full neural network which can refer to the total number of layers in a network for example the input images in cifar are an input volume of activations and the volume has dimensions width height depth respectively as we will soon see the neurons in a layer will only be connected to a small region of the layer before it instead of all of the neurons in a fully connected manner moreover the કnal output layer would for cifar have dimensions because by the end of the convnet architecture we will reduce the full image into a single vector of class scores arranged along the depth dimension here is a visualization left a regular layer neural network right a convnet arranges its neurons in three dimensions width height depth as visualized in one of the layers every layer of a convnet transforms the input volume to a output volume of neuron activations in this example the red input layer holds the image so its width and height would be the dimensions of the image and the depth would be red green blue channels a convnet is made up of layers every layer has a simple api it transforms an input volume to an output volume with some differentiable function that may or may not have parameters layers used to build convnets as we described above a simple convnet is a sequence of layers and every layer of a convnet transforms one volume of activations to another through a differentiable function we use three main types of layers to build convnet architectures convolutional layer pooling layer and fully connected layer exactly as seen in regular neural networks we will stack these layers to form a full convnet architecture example architecture overview we will go into more details below but a simple convnet for cifar classiકcation could have the architecture input conv relu pool fc in more detail input will hold the raw pixel values of the image in this case an image of width height and with three color channels r g b conv layer will compute the output of neurons that are connected to local regions in the input each computing a dot product between their weights and a small region they are connected to in the input volume this may result in volume such as if we decided to use કlters relu layer will apply an elementwise activation function such as the max x thresholding at zero this leaves the size of the volume unchanged pool layer will perform a downsampling operation along the spatial dimensions width height resulting in volume such as fc i e fully connected layer will compute the class scores resulting in volume of size where each of the numbers correspond to a class score such as among the categories of cifar as with ordinary neural networks and as the name implies each neuron in this layer will be connected to all the numbers in the previous volume in this way convnets transform the original image layer by layer from the original pixel values to the કnal class scores note that some layers contain parameters and other don t in particular the conv fc layers perform transformations that are a function of not only the activations in the input volume but also of the parameters the weights and biases of the neurons on the other hand the relu pool layers will implement a કxed function the parameters in the conv fc layers will be trained with gradient descent so that the class scores that the convnet computes are consistent with the labels in the training set for each image in summary a convnet architecture is in the simplest case a list of layers that transform the image volume into an output volume e g holding the class scores there are a few distinct types of layers e g conv fc relu pool are by far the most popular each layer accepts an input volume and transforms it to an output volume through a differentiable function each layer may or may not have parameters e g conv fc do relu pool don t each layer may or may not have additional hyperparameters e g conv fc pool do relu doesn t the activations of an example convnet architecture the initial volume stores the raw image pixels left and the last volume stores the class scores right each volume of activations along the processing path is shown as a column since it difકcult to visualize volumes we lay out each volume slices in rows the last layer volume holds the scores for each class but here we only visualize the sorted top scores and print the labels of each one the full is shown in the header of our website the architecture shown here is a tiny vgg net which we will discuss later we now describe the individual layers and the details of their hyperparameters and their connectivities convolutional layer the conv layer is the core building block of a convolutional network that does most of the computational heavy lifting overview and intuition without brain stuff lets કrst discuss what the conv layer computes without brain neuron analogies the conv layer parameters consist of a set of learnable કlters every કlter is small spatially along width and height but extends through the full depth of the input volume for example a typical કlter on a કrst layer of a convnet might have size i e pixels width and height and because images have depth the color channels during the forward pass we slide more precisely convolve each કlter across the width and height of the input volume and compute dot products between the entries of the કlter and the input at any position as we slide the કlter over the width and height of the input volume we will produce a dimensional activation map that gives the responses of that કlter at every spatial position intuitively the network will learn કlters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the કrst layer or eventually entire honeycomb or wheel like patterns on higher layers of the network now we will have an entire set of કlters in each conv layer e g કlters and each of them will produce a separate dimensional activation map we will stack these activation maps along the depth dimension and produce the output volume the brain view if you re a fan of the brain neuron analogies every entry in the output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially since these numbers all result from applying the same કlter we now discuss the details of the neuron connectivities their arrangement in space and their parameter sharing scheme local connectivity when dealing with high dimensional inputs such as images as we saw above it is impractical to connect neurons to all neurons in the previous volume instead we will connect each neuron to only a local region of the input volume the spatial extent of this connectivity is a hyperparameter called the receptive કeld of the neuron equivalently this is the કlter size the extent of the connectivity along the depth axis is always equal to the depth of the input volume it is important to emphasize again this asymmetry in how we treat the spatial dimensions width and height and the depth dimension the connections are local in space along width and height but always full along the entire depth of the input volume example for example suppose that the input volume has size e g an rgb cifar image if the receptive કeld or the કlter size is then each neuron in the conv layer will have weights to a region in the input volume for a total of weights and bias parameter notice that the extent of the connectivity along the depth axis must be since this is the depth of the input volume example suppose an input volume had size then using an example receptive કeld size of every neuron in the conv layer would now have a total of connections to the input volume notice that again the connectivity is local in space e g but full along the input depth left an example input volume in red e g a cifar image and an example volume of neurons in the કrst convolutional layer each neuron in the convolutional layer is connected only to a local region in the input volume spatially but to the full depth i e all color channels note there are multiple neurons in this example along the depth all looking at the same region in the input see discussion of depth columns in text below right the neurons from the neural network chapter remain unchanged they still compute a dot product of their weights with the input followed by a non linearity but their connectivity is now restricted to be local spatially spatial arrangement we have explained the connectivity of each neuron in the conv layer to the input volume but we haven t yet discussed how many neurons there are in the output volume or how they are arranged three hyperparameters control the size of the output volume the depth stride and zero padding we discuss these next first the depth of the output volume is a hyperparameter it corresponds to the number of કlters we would like to use each learning to look for something different in the input for example if the કrst convolutional layer takes as input the raw image then different neurons along the depth dimension may activate in presence of various oriented edged or blobs of color we will refer to a set of neurons that are all looking at the same region of the input as a depth column some people also prefer the term કbre second we must specify the stride with which we slide the કlter when the stride is then we move the કlters one pixel at a time when the stride is or uncommonly or more though this is rare in practice then the કlters jump pixels at a time as we slide them around this will produce smaller output volumes spatially as we will soon see sometimes it will be convenient to pad the input volume with zeros around the border the size of this zero padding is a hyperparameter the nice feature of zero padding is that it will allow us to control the spatial size of the output volumes most commonly as we ll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same we can compute the spatial size of the output volume as a function of the input volume size w the receptive કeld size of the conv layer neurons f the stride with which they are applied s and the amount of zero padding used p on the border you can convince yourself that the correct formula for calculating how many neurons કt is given by w f s for example for a input and a કlter with stride and pad we would get a output with stride we would get a output lets also see one more graphical example illustration of spatial arrangement in this example there is only one spatial dimension x axis one neuron with a receptive કeld size of f the input size is w and there is zero padding of p left the neuron strided across the input in stride of s giving output of size right the neuron uses stride of s giving output of size notice that stride s could not be used since it wouldn t કt neatly across the volume in terms of the equation this can be determined since is not divisible by the neuron weights are in this example shown on very right and its bias is zero these weights are shared across all yellow neurons see parameter sharing below use of zero padding in the example above on left note that the input dimension was and the output dimension was equal also this worked out so because our receptive કelds were and we used zero padding of if there was no zero padding used then the output volume would have had spatial dimension of only because that it is how many neurons would have કt across the original input in general setting zero padding to be p f when the stride is s ensures that the input volume and output volume will have the same size spatially it is very common to use zero padding in this way and we will discuss the full reasons when we talk more about convnet architectures constraints on strides note again that the spatial arrangement hyperparameters have mutual constraints for example when the input has size w no zero padding is used p and the કlter size is f then it would be impossible to use stride s since w f s i e not an integer indicating that the neurons don t કt neatly and symmetrically across the input therefore this setting of the hyperparameters is considered to be invalid and a convnet library could throw an exception or zero pad the rest to make it કt or crop the input to make it કt or something as we will see in the convnet architectures section sizing the convnets appropriately so that all the dimensions work out can be a real headache which the use of zero padding and some design guidelines will signiકcantly alleviate real world example the architecture that won the imagenet challenge in accepted images of size on the કrst convolutional layer it used neurons with receptive કeld size f stride s and no zero padding p since and since the conv layer had a depth of k the conv layer output volume had size each of the neurons in this volume was connected to a region of size in the input volume moreover all neurons in each depth column are connected to the same region of the input but of course with different weights as a fun aside if you read the actual paper it claims that the input images were which is surely incorrect because is quite clearly not an integer this has confused many people in the history of convnets and little is known about what happened my own best guess is that alex used zero padding of extra pixels that he does not mention in the paper parameter sharing parameter sharing scheme is used in convolutional layers to control the number of parameters using the real world example above we see that there are neurons in the કrst conv layer and each has weights and bias together this adds up to parameters on the કrst layer of the convnet alone clearly this number is very high it turns out that we can dramatically reduce the number of parameters by making one reasonable assumption that if one feature is useful to compute at some spatial position x y then it should also be useful to compute at a different position in other words denoting a single dimensional slice of depth as a depth slice e g a volume of size has depth slices each of size we are going to constrain the neurons in each depth slice to use the same weights and bias with this parameter sharing scheme the કrst conv layer in our example would now have only unique set of weights one for each depth slice for a total of unique weights or parameters biases alternatively all neurons in each depth slice will now be using the same parameters in practice during backpropagation every neuron in the volume will compute the gradient for its weights but these gradients will be added up across each depth slice and only update a single set of weights per slice notice that if all neurons in a single depth slice are using the same weight vector then the forward pass of the conv layer can in each depth slice be computed as a convolution of the neuron weights with the input volume hence the name convolutional layer this is why it is common to refer to the sets of weights as a કlter or a kernel that is convolved with the input example કlters learned by krizhevsky et al each of the કlters shown here is of size and each one is shared by the neurons in one depth slice notice that the parameter sharing assumption is relatively reasonable if detecting a horizontal edge is important at some location in the image it should intuitively be useful at some other location as well due to the translationally invariant structure of images there is therefore no need to relearn to detect a horizontal edge at every one of the distinct locations in the conv layer output volume note that sometimes the parameter sharing assumption may not make sense this is especially the case when the input images to a convnet have some speciકc centered structure where we should expect for example that completely different features should be learned on one side of the image than another one practical example is when the input are faces that have been centered in the image you might expect that different eye speciકc or hair speciકc features could and should be learned in different spatial locations in that case it is common to relax the parameter sharing scheme and instead simply call the layer a locally connected layer numpy examples to make the discussion above more concrete lets express the same ideas but in code and with a speciકc example suppose that the input volume is a numpy array then a depth column or a કbre at position would be the activations a depth slice or equivalently an activation map at depth would be the activations conv layer example suppose that the input volume has shape suppose further that we use no zero padding p that the કlter size is f and that the stride is s the output volume would therefore have spatial size giving a volume with width and height of the activation map in the output volume call it then look as follows only some of the elements are computed in this example would remember that in numpy the operation above denotes elementwise multiplication between the arrays notice also that the weight vector is the weight vector of that neuron and is the bias here is assumed to be of shape since the કlter size is and the depth of the input volume is notice that at each point we are computing the dot product as seen before in ordinary neural networks also we see that we are using the same weight and bias due to parameter sharing and where the dimensions along the width are increasing in steps of i e the stride to construct a second activation map in the output volume we would have v np sum x v np sum x v np sum x v np sum x v np sum x example of going along y v np sum x or along both where we see that we are indexing into the second depth dimension in at index because we are computing the second activation map and that a different set of parameters is now used in the example above we are for brevity leaving out some of the other operations the conv layer would perform to કll the other parts of the output array v additionally recall that these activation maps are often followed elementwise through an activation function such as relu but this is not shown here summary to summarize the conv layer accepts a volume of size requires four hyperparameters number of કlters k their spatial extent f the stride s the amount of zero padding p produces a volume of size where f s f s i e width and height are computed equally by symmetry k with parameter sharing it introduces f f weights per કlter for a total of f f k weights and k biases in the output volume the d th depth slice of size is the result of performing a valid convolution of the d th કlter over the input volume with a stride of s and then offset by d th bias a common setting of the hyperparameters is f s p however there are common conventions and rules of thumb that motivate these hyperparameters see the convnet architectures section below convolution demo below is a running demo of a conv layer since volumes are hard to visualize all the volumes the input volume in blue the weight volumes in red the output volume in green are visualized with each depth slice stacked in rows the input volume is of size and the conv layer parameters are k f s p that is we have two કlters of size and they are applied with a stride of therefore the output volume size has spatial size moreover notice that a padding of p is applied to the input volume making the outer border of the input volume zero the visualization below iterates over the output activations green and shows that each element is computed by elementwise multiplying the highlighted input blue with the કlter red summing it up and then offsetting the result by the bias input volume pad x filter filter output vol o implementation as matrix multiplication note that the convolution operation essentially performs dot products between the કlters and local regions of the input a common implementation pattern of the conv layer is to take advantage of this fact and formulate the forward pass of a convolutional layer as one big matrix multiply as follows the local regions in the input image are stretched out into columns in an operation commonly called for example if the input is and it is to be convolved with કlters at stride then we would take blocks of pixels in the input and stretch each block into a column vector of size iterating this process in the input at stride of gives locations along both width and height leading to an output matrix of of size x where every column is a stretched out receptive કeld and there are of them in total note that since the receptive કelds overlap every number in the input volume may be duplicated in multiple distinct columns the weights of the conv layer are similarly stretched out into rows for example if there are કlters of size this would give a matrix of size x the result of a convolution is now equivalent to performing one large matrix multiply which evaluates the dot product between every કlter and every receptive કeld location in our example the output of this operation would be x giving the output of the dot product of each કlter at each location the result must કnally be reshaped back to its proper output dimension this approach has the downside that it can use a lot of memory since some values in the input volume are replicated multiple times in however the beneકt is that there are many very efકcient implementations of matrix multiplication that we can take advantage of for example in the commonly used api moreover the same idea can be reused to perform the pooling operation which we discuss next backpropagation the backward pass for a convolution operation for both the data and the weights is also a convolution but with spatially ぱipped કlters this is easy to derive in the dimensional case with a toy example not expanded on for now convolution as an aside several papers use convolutions as કrst investigated by some people are at કrst confused to see convolutions especially when they come from signal processing background normally signals are dimensional so convolutions do not make sense it just pointwise scaling however in convnets this is not the case because one must remember that we operate over dimensional volumes and that the કlters always extend through the full depth of the input volume for example if the input is then doing convolutions would effectively be doing dimensional dot products since the input depth is channels dilated convolutions a recent development e g see is to introduce one more hyperparameter to the conv layer called the dilation so far we ve only dicussed conv કlters that are contiguous however it possible to have કlters that have spaces between each cell called dilation as an example in one dimension a કlter of size would compute over input the following n of for dilation the કlter would inst in other words there is a gap of between the applications this can be very useful in some settings to use in conjunction with dilated કlters because it allows you to merge spatial information across the inputs much more agressively with fewer layers for example if you stack two conv layers on top of each other than you can convince yourself that the neurons on the layer are a function of a patch of the input we would say that the effective receptive કeld of these neurons is if we use dilated convolutions then this effective receptive કeld would grow much quicker pooling layer it is common to periodically insert a pooling layer in between successive conv layers in a convnet architecture its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network and hence to also control overકtting the pooling layer operates independently on every depth slice of the input and resizes it spatially using the max operation the most common form is a pooling layer with કlters of size applied with a stride of downsamples every depth slice in the input by along both width and height discarding of the activations every max operation would in this case be taking a max over numbers little region in some depth slice the depth dimension remains unchanged more generally the pooling layer accepts a volume of size requires two hyperparameters their spatial extent f the stride s produces a volume of size where f s f s introduces zero parameters since it computes a કxed function of the input note that it is not common to use zero padding for pooling layers it is worth noting that there are only two commonly seen variations of the max pooling layer found in practice a pooling layer with f s also called overlapping pooling and more commonly f s pooling sizes with larger receptive કelds are too destructive general pooling in addition to max pooling the pooling units can also perform other functions such as average pooling or even norm pooling average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation which has been shown to work better in practice pooling layer downsamples the volume spatially independently in each depth slice of the input volume left in this example the input volume of size is pooled with કlter size stride into output volume of size notice that the volume depth is preserved right the most common downsampling operation is max giving rise to max pooling here shown with a stride of that is each max is taken over numbers little square backpropagation recall from the backpropagation chapter that the backward pass for a max x y operation has a simple interpretation as only routing the gradient to the input that had the highest value in the forward pass hence during the forward pass of a pooling layer it is common to keep track of the index of the max activation sometimes also called the switches so that gradient routing is efકcient during backpropagation getting rid of pooling many people dislike the pooling operation and think that we can get away without it for example proposes to discard the pooling layer in favor of architecture that only consists of repeated conv layers to reduce the size of the representation they suggest using larger stride in conv layer once in a while discarding pooling layers has also been found to be important in training good generative models such as variational autoencoders vaes or generative adversarial networks gans it seems likely that future architectures will feature very few to no pooling layers normalization layer many types of normalization layers have been proposed for use in convnet architectures sometimes with the intentions of implementing inhibition schemes observed in the biological brain however these layers have since fallen out of favor because in practice their contribution has been shown to be minimal if any for various types of normalizations see the discussion in alex krizhevsky fully connected layer neurons in a fully connected layer have full connections to all activations in the previous layer as seen in regular neural networks their activations can hence be computed with a matrix multiplication followed by a bias offset see the neural network section of the notes for more information converting fc layers to conv layers it is worth noting that the only difference between fc and conv layers is that the neurons in the conv layer are connected only to a local region in the input and that many of the neurons in a conv volume share parameters however the neurons in both layers still compute dot products so their functional form is identical therefore it turns out that it possible to convert between fc and conv layers for any conv layer there is an fc layer that implements the same forward function the weight matrix would be a large matrix that is mostly zero except for at certain blocks due to local connectivity where the weights in many of the blocks are equal due to parameter sharing conversely any fc layer can be converted to a conv layer for example an fc layer with k that is looking at some input volume of size can be equivalently expressed as a conv layer with f p s k in other words we are setting the કlter size to be exactly the size of the input volume and hence the output will simply be since only a single depth column કts across the input volume giving identical result as the initial fc layer fc conv conversion of these two conversions the ability to convert an fc layer to a conv layer is particularly useful in practice consider a convnet architecture that takes a image and then uses a series of conv layers and pool layers to reduce the image to an activations volume of size in an alexnet architecture that we ll see later this is done by use of pooling layers that downsample the input spatially by a factor of two each time making the કnal spatial size from there an alexnet uses two fc layers of size and કnally the last fc layers with neurons that compute the class scores we can convert each of these three fc layers to conv layers as described above replace the કrst fc layer that looks at volume with a conv layer that uses કlter size f giving output volume replace the second fc layer with a conv layer that uses કlter size f giving output volume replace the last fc layer similarly with f giving કnal output each of these conversions could in practice involve manipulating e g reshaping the weight matrix w in each fc layer into conv layer કlters it turns out that this conversion allows us to slide the original convnet very efકciently across many spatial positions in a larger image in a single forward pass for example if image gives a volume of size i e a reduction by then forwarding an image of size through the converted architecture would give the equivalent volume in size since following through with the next conv layers that we just converted from fc layers would now give the કnal volume of size since note that instead of a single vector of class scores of size we re now getting and entire array of class scores across the image evaluating the original convnet with fc layers independently across crops of the image in strides of pixels gives an identical result to forwarding the converted convnet one time naturally forwarding the converted convnet a single time is much more efકcient than iterating the original convnet over all those locations since the evaluations share computation this trick is often used in practice to get better performance where for example it is common to resize an image to make it bigger use a converted convnet to evaluate the class scores at many spatial positions and then average the class scores lastly what if we wanted to efકciently apply the original convnet over the image but at a stride smaller than pixels we could achieve this with multiple forward passes for example note that if we wanted to use a stride of pixels we could do so by combining the volumes received by forwarding the converted convnet twice first over the original image and second over the image but with the image shifted spatially by pixels along both width and height an ipython notebook on shows how to perform the conversion in practice in code using caffe convnet architectures we have seen that convolutional networks are commonly made up of only three layer types conv pool we assume max pool unless stated otherwise and fc short for fully connected we will also explicitly write the relu activation function as a layer which applies elementwise non linearity in this section we discuss how these are commonly stacked together to form entire convnets layer patterns the most common form of a convnet architecture stacks a few conv relu layers follows them with pool layers and repeats this pattern until the image has been merged spatially to a small size at some point it is common to transition to fully connected layers the last fully connected layer holds the output such as the class scores in other words the most common convnet architecture follows the pattern where the indicates repetition and the pool indicates an optional pooling layer moreover and usually k and usually for example here are some common convnet architectures you may see that follow this pattern implements a linear classiકer here here we see two conv layers stacked before every pool layer this is generally a good idea for larger and deeper networks because multiple stacked conv layers can develop more complex features of the input volume before the destructive pooling operation prefer a stack of small કlter conv to one large receptive કeld conv layer suppose that you stack three conv layers on top of each other with non linearities in between of course in this arrangement each neuron on the કrst conv layer has a view of the input volume a neuron on the second conv layer has a view of the કrst conv layer and hence by extension a view of the input volume similarly a neuron on the third conv layer has a view of the conv layer and hence a view of the input volume suppose that instead of these three layers of conv we only wanted to use a single conv layer with receptive કelds these neurons would have a receptive કeld size of the input volume that is identical in spatial extent but with several disadvantages first the neurons would be computing a linear function over the input while the three stacks of conv layers contain non linearities that make their features more expressive second if we suppose that all the volumes have c channels then it can be seen that the single conv layer would contain c c parameters while the three conv layers would only contain c c parameters intuitively stacking conv layers with tiny કlters as opposed to having one conv layer with big કlters allows us to express more powerful features of the input and with fewer parameters as a practical disadvantage we might need more memory to hold all the intermediate conv layer results if we plan to do backpropagation recent departures it should be noted that the conventional paradigm of a linear list of layers has recently been challanged in google inception architectures and also in current state of the art residual networks from microsoft research asia both of these see details below in case studies section feature more intricate and different connectivity structures in practice use whatever works best on imagenet if you re feeling a bit of a fatigue in thinking about the architectural decisions you ll be pleased to know that in or more of applications you should not have to worry about these i like to summarize this point as don t be a hero instead of rolling your own architecture for a problem you should look at whatever architecture currently works best on imagenet download a pretrained model and કnetune it on your data you should rarely ever have to train a convnet from scratch or design one from scratch i also made this point at the layer sizing patterns until now we ve omitted mentions of common hyperparameters used in each of the layers in a convnet we will કrst state the common rules of thumb for sizing the architectures and then follow the rules with a discussion of the notation the input layer that contains the image should be divisible by many times common numbers include e g cifar e g stl or e g common imagenet convnets and the conv layers should be using small કlters e g or at most using a stride of s and crucially padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input that is when f then using p will retain the original size of the input when f p for a general f it can be seen that p f preserves the input size if you must use bigger કlter sizes such as or so it is only common to see this on the very કrst conv layer that is looking at the input image the pool layers are in charge of downsampling the spatial dimensions of the input the most common setting is to use max pooling with receptive કelds i e f and with a stride of i e s note that this discards exactly of the activations in an input volume due to downsampling by in both width and height another slightly less common setting is to use receptive કelds with a stride of but this makes it is very uncommon to see receptive કeld sizes for max pooling that are larger than because the pooling is then too lossy and aggressive this usually leads to worse performance reducing sizing headaches the scheme presented above is pleasing because all the conv layers preserve the spatial size of their input while the pool layers alone are in charge of down sampling the volumes spatially in an alternative scheme where we use strides greater than or don t zero pad the input in conv layers we would have to very carefully keep track of the input volumes throughout the cnn architecture and make sure that all strides and કlters work out and that the convnet architecture is nicely and symmetrically wired why use stride of in conv smaller strides work better in practice additionally as already mentioned stride allows us to leave all spatial down sampling to the pool layers with the conv layers only transforming the input volume depth wise why use padding in addition to the aforementioned beneકt of keeping the spatial sizes constant after conv doing this actually improves performance if the conv layers were to not zero pad the inputs and only perform valid convolutions then the size of the volumes would reduce by a small amount after each conv and the information at the borders would be washed away too quickly compromising based on memory constraints in some cases especially early in the convnet architectures the amount of memory can build up very quickly with the rules of thumb presented above for example કltering a image with three conv layers with કlters each and padding would create three activation volumes of size this amounts to a total of about million activations or of memory per image for both activations and gradients since gpus are often bottlenecked by memory it may be necessary to compromise in practice people prefer to make the compromise at only the કrst conv layer of the network for example one compromise might be to use a કrst conv layer with કlter sizes of and stride of as seen in a zf net as another example an alexnet uses કler sizes of and stride of case studies there are several architectures in the કeld of convolutional networks that have a name the most common are lenet the કrst successful applications of convolutional networks were developed by yann lecun in of these the best known is the architecture that was used to read zip codes digits etc alexnet the કrst work that popularized convolutional networks in computer vision was the developed by alex krizhevsky ilya sutskever and geoff hinton the alexnet was submitted to the in and signiકcantly outperformed the second runner up top error of compared to runner up with error the network had a very similar architecture to lenet but was deeper bigger and featured convolutional layers stacked on top of each other previously it was common to only have a single conv layer always immediately followed by a pool layer zf net the ilsvrc winner was a convolutional network from matthew zeiler and rob fergus it became known as the short for zeiler fergus net it was an improvement on alexnet by tweaking the architecture hyperparameters in particular by expanding the size of the middle convolutional layers and making the stride and કlter size on the કrst layer smaller googlenet the ilsvrc winner was a convolutional network from from google its main contribution was the development of an inception module that dramatically reduced the number of parameters in the network compared to alexnet with additionally this paper uses average pooling instead of fully connected layers at the top of the convnet eliminating a large amount of parameters that do not seem to matter much there are also several followup versions to the googlenet most recently vggnet the runner up in ilsvrc was the network from karen simonyan and andrew zisserman that became known as the its main contribution was in showing that the depth of the network is a critical component for good performance their કnal best network contains conv fc layers and appealingly features an extremely homogeneous architecture that only performs convolutions and pooling from the beginning to the end their is available for plug and play use in caffe a downside of the vggnet is that it is more expensive to evaluate and uses a lot more memory and parameters most of these parameters are in the કrst fully connected layer and it was since found that these fc layers can be removed with no performance downgrade signiકcantly reducing the number of necessary parameters resnet developed by kaiming he et al was the winner of ilsvrc it features special skip connections and a heavy use of the architecture is also missing fully connected layers at the end of the network the reader is also referred to kaiming presentation and some that reproduce these networks in torch resnets are currently by far state of the art convolutional neural network models and are the default choice for using convnets in practice as of may in particular also see more recent developments that tweak vggnet in detail lets break down the in more detail as a case study the whole vggnet is composed of conv layers that perform convolutions with stride and pad and of pool layers that perform max pooling with stride and no padding we can write out the size of the representation at each step of the processing and keep track of both the representation size and the total number of weights memory weights memory weights memory weights memory weights memory weights memory weights memory weights memory weights memory weights memory weights conv3 memory weights conv3 memory 100k weights 7x7x512 memory 512 25k weights fc memory weights total memory bytes image only forward for bwd total params parameters as is common with convolutional networks notice that most of the memory and also compute time is used in the early conv layers and that most of the parameters are in the last fc layers in this particular case the કrst fc layer contains weights out of a total of computational considerations the largest bottleneck to be aware of when constructing convnet architectures is the memory bottleneck many modern gpus have a limit of memory with the best gpus having about of memory there are three major sources of memory to keep track of from the intermediate volume sizes these are the raw number of activations at every layer of the convnet and also their gradients of equal size usually most of the activations are on the earlier layers of a convnet i e કrst conv layers these are kept around because they are needed for backpropagation but a clever implementation that runs a convnet only at test time could in principle reduce this by a huge amount by only storing the current activations at any layer and discarding the previous activations on layers below from the parameter sizes these are the numbers that hold the network parameters their gradients during backpropagation and commonly also a step cache if the optimization is using momentum adagrad or rmsprop therefore the memory to store the parameter vector alone must usually be multiplied by a factor of at least or so every convnet implementation has to maintain miscellaneous memory such as the image data batches perhaps their augmented versions etc once you have a rough estimate of the total number of values for activations gradients and misc the number should be converted to size in gb take the number of values multiply by to get the raw number of bytes since every ぱoating point is bytes or maybe by for double precision and then divide by multiple times to get the amount of memory in kb mb and કnally gb if your network doesn t કt a common heuristic to make it કt is to decrease the batch size since most of the memory is usually consumed by the activations additional resources additional resources related to implementation allows you to play with convnet architectures and see the results and computations in real time in the browser one of the popular convnet libraries distinctive image features from scale invariant keypoints abstract this paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene the features are invariant to image scale and rotation and are shown to provide robust matching across a a substantial range of affine dis tortion change in viewpoint addition of noise and change in illumination the features are highly distinctive in the sense that a single feature can be cor rectly matched with high probability against a large database of features from many images this paper also describes an approach to using these features for object recognition the recognition proceeds by matching individual fea tures to a database of features from known objects using a fast nearest neighbor algorithm followed by a hough transform to identify clusters belonging to a sin gle object and finally performing verification through least squares solution for consistent pose parameters this approach to recognition can robustly identify objects among clutter and occlusion while achieving near real time performance accepted for publication in the international journal of computer vision introduction image matching is a fundamental aspect of many problems in computer vision including object or scene recognition solving for structure from multiple images stereo correspon dence and motion tracking this paper describes image features that have many properties that make them suitable for matching differing images of an object or scene the features are invariant to image scaling and rotation and partially invariant to change in illumination and camera viewpoint they are well localized in both the spatial and frequency domains re ducing the probability of disruption by occlusion clutter or noise large numbers of features can be extracted from typical images with efficient algorithms in addition the features are highly distinctive which allows a single feature to be correctly matched with high probability against a large database of features providing a basis for object and scene recognition the cost of extracting these features is minimized by taking a cascade filtering approach in which the more expensive operations are applied only at locations that pass an initial test following are the major stages of computation used to generate the set of image features scale space extrema detection the first stage of computation searches over all scales and image locations it is implemented efficiently by using a difference of gaussian function to identify potential interest points that are invariant to scale and orientation keypoint localization at each candidate location a detailed model is fit to determine location and scale keypoints are selected based on measures of their stability orientation assignment one or more orientations are assigned to each keypoint lo cation based on local image gradient directions all future operations are performed on image data that has been transformed relative to the assigned orientation scale and location for each feature thereby providing invariance to these transformations keypoint descriptor the local image gradients are measured at the selected scale in the region around each keypoint these are transformed into a representation that allows for significant levels of local shape distortion and change in illumination this approach has been named the scale invariant feature transform sift as it transforms image data into scale invariant coordinates relative to local features an important aspect of this approach is that it generates large numbers of features that densely cover the image over the full range of scales and locations a typical image of size pixels will give rise to about stable features although this number depends on both image content and choices for various parameters the quantity of features is partic ularly important for object recognition where the ability to detect small objects in cluttered backgrounds requires that at least features be correctly matched from each object for reli able identification for image matching and recognition sift features are first extracted from a set of ref erence images and stored in a database a new image is matched by individually comparing each feature from the new image to this previous database and finding candidate match ing features based on euclidean distance of their feature vectors this paper will discuss fast nearest neighbor algorithms that can perform this computation rapidly against large databases the keypoint descriptors are highly distinctive which allows a single feature to find its correct match with good probability in a large database of features however in a cluttered image many features from the background will not have any correct match in the database giving rise to many false matches in addition to the correct ones the correct matches can be filtered from the full set of matches by identifying subsets of keypoints that agree on the object and its location scale and orientation in the new image the probability that several features will agree on these parameters by chance is much lower than the probability that any individual feature match will be in error the determination of these consistent clusters can be performed rapidly by using an efficient hash table implementation of the generalized hough transform each cluster of or more features that agree on an object and its pose is then subject to further detailed verification first a least squared estimate is made for an affine approxi mation to the object pose any other image features consistent with this pose are identified and outliers are discarded finally a detailed computation is made of the probability that a particular set of features indicates the presence of an object given the accuracy of fit and number of probable false matches object matches that pass all these tests can be identified as correct with high confidence related research the development of image matching by using a set of local interest points can be traced back to the work of moravec on stereo matching using a corner detector the moravec detector was improved by harris and stephens to make it more repeatable under small image variations and near edges harris also showed its value for efficient motion tracking and structure from motion recovery harris and the harris corner detector has since been widely used for many other image matching tasks while these feature detectors are usually called corner detectors they are not selecting just corners but rather any image location that has large gradients in all directions at a predetermined scale the initial applications were to stereo and short range motion tracking but the approach was later extended to more difficult problems zhang et al showed that it was possi ble to match harris corners over a large image range by using a correlation window around each corner to select likely matches outliers were then removed by solving for a funda mental matrix describing the geometric constraints between the two views of rigid scene and removing matches that did not agree with the majority solution at the same time a similar approach was developed by torr for long range motion matching in which geometric constraints were used to remove outliers for rigid objects moving within an image the ground breaking work of schmid and mohr showed that invariant local fea ture matching could be extended to general image recognition problems in which a feature was matched against a large database of images they also used harris corners to select interest points but rather than matching with a correlation window they used a rotationally invariant descriptor of the local image region this allowed features to be matched under arbitrary orientation change between the two images furthermore they demonstrated that multiple feature matches could accomplish general recognition under occlusion and clutter by identifying consistent clusters of matched features the harris corner detector is very sensitive to changes in image scale so it does not provide a good basis for matching images of different sizes earlier work by the author lowe extended the local feature approach to achieve scale invariance this work also described a new local descriptor that provided more distinctive features while being less sensitive to local image distortions such as viewpoint change this current paper provides a more in depth development and analysis of this earlier work while also presenting a number of improvements in stability and feature invariance there is a considerable body of previous research on identifying representations that are stable under scale change some of the first work in this area was by crowley and parker who developed a representation that identified peaks and ridges in scale space and linked these into a tree structure the tree structure could then be matched between images with arbitrary scale change more recent work on graph based matching by shokoufandeh marsic and dickinson provides more distinctive feature descriptors using wavelet co efficients the problem of identifying an appropriate and consistent scale for feature detection has been studied in depth by lindeberg he describes this as a problem of scale selection and we make use of his results below recently there has been an impressive body of work on extending local features to be invariant to full affine transformations baumberg tuytelaars and van gool mikolajczyk and schmid schaffalitzky and zisserman brown and lowe this allows for invariant matching to features on a planar surface under changes in ortho graphic projection in most cases by resampling the image in a local affine frame how ever none of these approaches are yet fully affine invariant as they start with initial feature scales and locations selected in a non affine invariant manner due to the prohibitive cost of exploring the full affine space the affine frames are are also more sensitive to noise than those of the scale invariant features so in practice the affine features have lower repeatability than the scale invariant features unless the affine distortion is greater than about a degree tilt of a planar surface mikolajczyk wider affine invariance may not be important for many applications as training views are best taken at least every degrees rotation in view point meaning that recognition is within degrees of the closest training view in order to capture non planar changes and occlusion effects for objects while the method to be presented in this paper is not fully affine invariant a different approach is used in which the local descriptor allows relative feature positions to shift signif icantly with only small changes in the descriptor this approach not only allows the descrip tors to be reliably matched across a considerable range of affine distortion but it also makes the features more robust against changes in viewpoint for non planar surfaces other advantages include much more efficient feature extraction and the ability to identify larger numbers of features on the other hand affine invariance is a valuable property for matching planar surfaces under very large view changes and further research should be performed on the best ways to combine this with non planar viewpoint invariance in an efficient and stable manner many other feature types have been proposed for use in recognition some of which could be used in addition to the features described in this paper to provide further matches under differing circumstances one class of features are those that make use of image contours or region boundaries which should make them less likely to be disrupted by cluttered back grounds near object boundaries matas et al have shown that their maximally stable extremal regions can produce large numbers of matching features with good stability miko lajczyk et al have developed a new descriptor that uses local edges while ignoring unrelated nearby edges providing the ability to find stable features even near the boundaries of narrow shapes superimposed on background clutter nelson and selinger have shown good results with local features based on groupings of image contours similarly pope and lowe used features based on the hierarchical grouping of image contours which are particularly useful for objects lacking detailed texture the history of research on visual recognition contains work on a diverse set of other image properties that can be used as feature measurements carneiro and jepson describe phase based local features that represent the phase rather than the magnitude of local spatial frequencies which is likely to provide improved invariance to illumination schiele and crowley have proposed the use of multidimensional histograms summarizing the distribution of measurements within image regions this type of feature may be particularly useful for recognition of textured objects with deformable shapes basri and jacobs have demonstrated the value of extracting local region boundaries for recognition other useful properties to incorporate include color motion figure ground discrimination region shape descriptors and stereo depth cues the local feature approach can easily incorporate novel feature types because extra features contribute to robustness when they provide correct matches but otherwise do little harm other than their cost of computation therefore future systems are likely to combine many feature types detection of scale space extrema as described in the introduction we will detect keypoints using a cascade filtering approach that uses efficient algorithms to identify candidate locations that are then examined in further detail the first stage of keypoint detection is to identify locations and scales that can be repeatably assigned under differing views of the same object detecting locations that are invariant to scale change of the image can be accomplished by searching for stable features across all possible scales using a continuous function of scale known as scale space witkin it has been shown by koenderink and lindeberg that under a variety of reasonable assumptions the only possible scale space kernel is the gaussian function there fore the scale space of an image is defined as a function l x y σ that is produced from the convolution of a variable scale gaussian g x y σ with an input image i x y l x y σ g x y σ i x y where is the convolution operation in x and y and g x y σ e to efficiently detect stable keypoint locations in scale space we have proposed lowe using scale space extrema in the difference of gaussian function convolved with the image d x y σ which can be computed from the difference of two nearby scales separated by a constant multiplicative factor k d x y σ g x y kσ g x y σ i x y l x y kσ l x y σ there are a number of reasons for choosing this function first it is a particularly efficient function to compute as the smoothed images l need to be computed in any case for scale space feature description and d can therefore be computed by simple image subtraction scale next octave scale first octave gaussian difference of gaussian dog figure for each octave of scale space the initial image is repeatedly convolved with gaussians to produce the set of scale space images shown on the left adjacent gaussian images are subtracted to produce the difference of gaussian images on the right after each octave the gaussian image is down sampled by a factor of and the process repeated in addition the difference of gaussian function provides a close approximation to the scale normalized laplacian of gaussian as studied by lindeberg lindeberg showed that the normalization of the laplacian with the factor is required for true scale invariance in detailed experimental comparisons mikolajczyk found that the maxima and minima of produce the most stable image features compared to a range of other possible image functions such as the gradient hessian or harris corner function the relationship between d and can be understood from the heat diffusion equa tion parameterized in terms of σ rather than the more usual t g σ σ from this we see that can be computed from the finite difference approximation to g σ using the difference of nearby scales at kσ and σ σ g g x y kσ g x y σ and therefore σ kσ σ g x y kσ g x y σ k this shows that when the difference of gaussian function has scales differing by a con stant factor it already incorporates the scale normalization required for the scale invariant figure maxima and minima of the difference of gaussian images are detected by comparing a pixel marked with x to its neighbors in regions at the current and adjacent scales marked with circles laplacian the factor k in the equation is a constant over all scales and therefore does not influence extrema location the approximation error will go to zero as k goes to but in practice we have found that the approximation has almost no impact on the stability of extrema detection or localization for even significant differences in scale such as k an efficient approach to construction of d x y σ is shown in figure the initial image is incrementally convolved with gaussians to produce images separated by a constant factor k in scale space shown stacked in the left column we choose to divide each octave of scale space i e doubling of σ into an integer number of intervals so k we must produce images in the stack of blurred images for each octave so that final extrema detection covers a complete octave adjacent image scales are subtracted to produce the difference of gaussian images shown on the right once a complete octave has been processed we resample the gaussian image that has twice the initial value of σ it will be images from the top of the stack by taking every second pixel in each row and column the accuracy of sampling relative to σ is no different than for the start of the previous octave while computation is greatly reduced local extrema detection in order to detect the local maxima and minima of d x y σ each sample point is compared to its eight neighbors in the current image and nine neighbors in the scale above and below see figure it is selected only if it is larger than all of these neighbors or smaller than all of them the cost of this check is reasonably low due to the fact that most sample points will be eliminated following the first few checks an important issue is to determine the frequency of sampling in the image and scale do mains that is needed to reliably detect the extrema unfortunately it turns out that there is no minimum spacing of samples that will detect all extrema as the extrema can be arbitrar ily close together this can be seen by considering a white circle on a black background which will have a single scale space maximum where the circular positive central region of the difference of gaussian function matches the size and location of the circle for a very elongated ellipse there will be two maxima near each end of the ellipse as the locations of maxima are a continuous function of the image for some ellipse with intermediate elongation there will be a transition from a single maximum to two with the maxima arbitrarily close to number of scales sampled per octave number of scales sampled per octave figure the top line of the first graph shows the percent of keypoints that are repeatably detected at the same location and scale in a transformed image as a function of the number of scales sampled per octave the lower line shows the percent of keypoints that have their descriptors correctly matched to a large database the second graph shows the total number of keypoints detected in a typical image as a function of the number of scale samples each other near the transition therefore we must settle for a solution that trades off efficiency with completeness in fact as might be expected and is confirmed by our experiments extrema that are close together are quite unstable to small perturbations of the image we can determine the best choices experimentally by studying a range of sampling frequencies and using those that provide the most reliable results under a realistic simulation of the matching task frequency of sampling in scale the experimental determination of sampling frequency that maximizes extrema stability is shown in figures and these figures and most other simulations in this paper are based on a matching task using a collection of real images drawn from a diverse range including outdoor scenes human faces aerial photographs and industrial images the image domain was found to have almost no influence on any of the results each image was then subject to a range of transformations including rotation scaling affine stretch change in brightness and contrast and addition of image noise because the changes were synthetic it was possible to precisely predict where each feature in an original image should appear in the transformed image allowing for measurement of correct repeatability and positional accuracy for each feature figure shows these simulation results used to examine the effect of varying the number of scales per octave at which the image function is sampled prior to extrema detection in this case each image was resampled following rotation by a random angle and scaling by a random amount between of times the original size keypoints from the reduced resolution image were matched against those from the original image so that the scales for all keypoints would be be present in the matched image in addition image noise was added meaning that each pixel had a random number added from the uniform interval where pixel values are in the range equivalent to providing slightly less than bits of accuracy for image pixels prior smoothing for each octave sigma figure the top line in the graph shows the percent of keypoint locations that are repeatably detected in a transformed image as a function of the prior image smoothing for the first level of each octave the lower line shows the percent of descriptors correctly matched against a large database the top line in the first graph of figure shows the percent of keypoints that are detected at a matching location and scale in the transformed image for all examples in this paper we define a matching scale as being within a factor of of the correct scale and a matching location as being within σ pixels where σ is the scale of the keypoint defined from equation as the standard deviation of the smallest gaussian used in the difference of gaussian function the lower line on this graph shows the number of keypoints that are correctly matched to a database of keypoints using the nearest neighbor matching procedure to be described in section this shows that once the keypoint is repeatably located it is likely to be useful for recognition and matching tasks as this graph shows the highest repeatability is obtained when sampling scales per octave and this is the number of scale samples used for all other experiments throughout this paper it might seem surprising that the repeatability does not continue to improve as more scales are sampled the reason is that this results in many more local extrema being detected but these extrema are on average less stable and therefore are less likely to be detected in the transformed image this is shown by the second graph in figure which shows the average number of keypoints detected and correctly matched in each image the number of keypoints rises with increased sampling of scales and the total number of correct matches also rises since the success of object recognition often depends more on the quantity of correctly matched keypoints as opposed to their percentage correct matching for many applications it will be optimal to use a larger number of scale samples however the cost of computation also rises with this number so for the experiments in this paper we have chosen to use just scale samples per octave to summarize these experiments show that the scale space difference of gaussian func tion has a large number of extrema and that it would be very expensive to detect them all fortunately we can detect the most stable and useful subset even with a coarse sampling of scales frequency of sampling in the spatial domain just as we determined the frequency of sampling per octave of scale space so we must de termine the frequency of sampling in the image domain relative to the scale of smoothing given that extrema can be arbitrarily close together there will be a similar trade off between sampling frequency and rate of detection figure shows an experimental determination of the amount of prior smoothing σ that is applied to each image level before building the scale space representation for an octave again the top line is the repeatability of keypoint detection and the results show that the repeatability continues to increase with σ however there is a cost to using a large σ in terms of efficiency so we have chosen to use σ which provides close to optimal repeatability this value is used throughout this paper and was used for the results in figure of course if we pre smooth the image before extrema detection we are effectively dis carding the highest spatial frequencies therefore to make full use of the input the image can be expanded to create more sample points than were present in the original we dou ble the size of the input image using linear interpolation prior to building the first level of the pyramid while the equivalent operation could effectively have been performed by us ing sets of subpixel offset filters on the original image the image doubling leads to a more efficient implementation we assume that the original image has a blur of at least σ the minimum needed to prevent significant aliasing and that therefore the doubled image has σ relative to its new pixel spacing this means that little additional smoothing is needed prior to creation of the first octave of scale space the image doubling increases the number of stable keypoints by almost a factor of but no significant further improvements were found with a larger expansion factor accurate keypoint localization once a keypoint candidate has been found by comparing a pixel to its neighbors the next step is to perform a detailed fit to the nearby data for location scale and ratio of principal curvatures this information allows points to be rejected that have low contrast and are therefore sensitive to noise or are poorly localized along an edge the initial implementation of this approach lowe simply located keypoints at the location and scale of the central sample point however recently brown has developed a method brown and lowe for fitting a quadratic function to the local sample points to determine the interpolated location of the maximum and his experiments showed that this provides a substantial improvement to matching and stability his approach uses the taylor expansion up to the quadratic terms of the scale space function d x y σ shifted so that the origin is at the sample point d x d d t x xt x x where d and its derivatives are evaluated at the sample point and x x y σ t is the offset from this point the location of the extremum xˆ is determined by taking the derivative of this function with respect to x and setting it to zero giving d xˆ x a figure this figure shows the stages of keypoint selection a the pixel original image b the initial keypoints locations at maxima and minima of the difference of gaussian function keypoints are displayed as vectors indicating scale orientation and location c after applying a threshold on minimum contrast keypoints remain d the final keypoints that remain following an additional threshold on ratio of principal curvatures as suggested by brown the hessian and derivative of d are approximated by using dif ferences of neighboring sample points the resulting linear system can be solved with minimal cost if the offset xˆ is larger than in any dimension then it means that the ex tremum lies closer to a different sample point in this case the sample point is changed and the interpolation performed instead about that point the final offset xˆ is added to the location of its sample point to get the interpolated estimate for the location of the extremum the function value at the extremum d xˆ is useful for rejecting unstable extrema with low contrast this can be obtained by substituting equation into giving d xˆ d d t xˆ x for the experiments in this paper all extrema with a value of d xˆ less than were discarded as before we assume image pixel values in the range figure shows the effects of keypoint selection on a natural image in order to avoid too much clutter a low resolution by pixel image is used and keypoints are shown as vectors giving the location scale and orientation of each keypoint orientation assignment is described below figure a shows the original image which is shown at reduced contrast behind the subsequent figures figure b shows the keypoints at all detected maxima and minima of the difference of gaussian function while c shows the keypoints that remain following removal of those with a value of d xˆ less than part d will be explained in the following section eliminating edge responses for stability it is not sufficient to reject keypoints with low contrast the difference of gaussian function will have a strong response along edges even if the location along the edge is poorly determined and therefore unstable to small amounts of noise a poorly defined peak in the difference of gaussian function will have a large principal curvature across the edge but a small one in the perpendicular direction the principal curva tures can be computed from a hessian matrix h computed at the location and scale of the keypoint h dxx dxy dxy dyy the derivatives are estimated by taking differences of neighboring sample points the eigenvalues of h are proportional to the principal curvatures of d borrowing from the approach used by harris and stephens we can avoid explicitly computing the eigenvalues as we are only concerned with their ratio let α be the eigenvalue with the largest magnitude and β be the smaller one then we can compute the sum of the eigenvalues from the trace of h and their product from the determinant tr h dxx dyy α β det h dxxdyy dxy αβ in the unlikely event that the determinant is negative the curvatures have different signs so the point is discarded as not being an extremum let r be the ratio between the largest magnitude eigenvalue and the smaller one so that α rβ then tr h det h α β αβ rβ β r r which depends only on the ratio of the eigenvalues rather than their individual values the quantity r r is at a minimum when the two eigenvalues are equal and it increases with r therefore to check that the ratio of principal curvatures is below some threshold r we only need to check tr h det h r r this is very efficient to compute with less than floating point operations required to test each keypoint the experiments in this paper use a value of r which eliminates keypoints that have a ratio between the principal curvatures greater than the transition from figure c to d shows the effects of this operation orientation assignment by assigning a consistent orientation to each keypoint based on local image properties the keypoint descriptor can be represented relative to this orientation and therefore achieve in variance to image rotation this approach contrasts with the orientation invariant descriptors of schmid and mohr in which each image property is based on a rotationally invariant measure the disadvantage of that approach is that it limits the descriptors that can be used and discards image information by not requiring all measures to be based on a consistent rotation following experimentation with a number of approaches to assigning a local orientation the following approach was found to give the most stable results the scale of the keypoint is used to select the gaussian smoothed image l with the closest scale so that all compu tations are performed in a scale invariant manner for each image sample l x y at this scale the gradient magnitude m x y and orientation θ x y is precomputed using pixel differences m x y i l x y l x y l x y l x y θ x y tan l x y l x y l x y l x y an orientation histogram is formed from the gradient orientations of sample points within a region around the keypoint the orientation histogram has bins covering the degree range of orientations each sample added to the histogram is weighted by its gradient magni tude and by a gaussian weighted circular window with a σ that is times that of the scale of the keypoint peaks in the orientation histogram correspond to dominant directions of local gradients the highest peak in the histogram is detected and then any other local peak that is within of the highest peak is used to also create a keypoint with that orientation therefore for locations with multiple peaks of similar magnitude there will be multiple keypoints created at the same location and scale but different orientations only about of points are assigned multiple orientations but these contribute significantly to the stability of matching finally a parabola is fit to the histogram values closest to each peak to interpolate the peak position for better accuracy figure shows the experimental stability of location scale and orientation assignment under differing amounts of image noise as before the images are rotated and scaled by random amounts the top line shows the stability of keypoint location and scale assign ment the second line shows the stability of matching when the orientation assignment is also required to be within degrees as shown by the gap between the top two lines the orientation assignment remains accurate of the time even after addition of pixel noise equivalent to a camera providing less than bits of precision the measured vari ance of orientation for the correct matches is about degrees rising to degrees for noise the bottom line in figure shows the final accuracy of correctly matching a keypoint descriptor to a database of keypoints to be discussed below as this graph shows the sift features are resistant to even large amounts of pixel noise and the major cause of error is the initial location and scale detection image noise figure the top line in the graph shows the percent of keypoint locations and scales that are repeat ably detected as a function of pixel noise the second line shows the repeatability after also requiring agreement in orientation the bottom line shows the final percent of descriptors correctly matched to a large database the local image descriptor the previous operations have assigned an image location scale and orientation to each key point these parameters impose a repeatable local coordinate system in which to describe the local image region and therefore provide invariance to these parameters the next step is to compute a descriptor for the local image region that is highly distinctive yet is as invariant as possible to remaining variations such as change in illumination or viewpoint one obvious approach would be to sample the local image intensities around the key point at the appropriate scale and to match these using a normalized correlation measure however simple correlation of image patches is highly sensitive to changes that cause mis registration of samples such as affine or viewpoint change or non rigid deformations a better approach has been demonstrated by edelman intrator and poggio their pro posed representation was based upon a model of biological vision in particular of complex neurons in primary visual cortex these complex neurons respond to a gradient at a particular orientation and spatial frequency but the location of the gradient on the retina is allowed to shift over a small receptive field rather than being precisely localized edelman et al hypoth esized that the function of these complex neurons was to allow for matching and recognition of objects from a range of viewpoints they have performed detailed experiments using computer models of object and animal shapes which show that matching gradients while allowing for shifts in their position results in much better classification under rotation for example recognition accuracy for objects rotated in depth by degrees increased from for correlation of gradients to using the complex cell model our implementation described below was inspired by this idea but allows for positional shift using a different computational mechanism image gradients keypoint descriptor figure a keypoint descriptor is created by first computing the gradient magnitude and orientation at each image sample point in a region around the keypoint location as shown on the left these are weighted by a gaussian window indicated by the overlaid circle these samples are then accumulated into orientation histograms summarizing the contents over subregions as shown on the right with the length of each arrow corresponding to the sum of the gradient magnitudes near that direction within the region this figure shows a descriptor array computed from an set of samples whereas the experiments in this paper use descriptors computed from a sample array descriptor representation figure illustrates the computation of the keypoint descriptor first the image gradient mag nitudes and orientations are sampled around the keypoint location using the scale of the keypoint to select the level of gaussian blur for the image in order to achieve orientation invariance the coordinates of the descriptor and the gradient orientations are rotated relative to the keypoint orientation for efficiency the gradients are precomputed for all levels of the pyramid as described in section these are illustrated with small arrows at each sample location on the left side of figure a gaussian weighting function with σ equal to one half the width of the descriptor win dow is used to assign a weight to the magnitude of each sample point this is illustrated with a circular window on the left side of figure although of course the weight falls off smoothly the purpose of this gaussian window is to avoid sudden changes in the descriptor with small changes in the position of the window and to give less emphasis to gradients that are far from the center of the descriptor as these are most affected by misregistration errors the keypoint descriptor is shown on the right side of figure it allows for significant shift in gradient positions by creating orientation histograms over sample regions the figure shows eight directions for each orientation histogram with the length of each arrow corresponding to the magnitude of that histogram entry a gradient sample on the left can shift up to sample positions while still contributing to the same histogram on the right thereby achieving the objective of allowing for larger local positional shifts it is important to avoid all boundary affects in which the descriptor abruptly changes as a sample shifts smoothly from being within one histogram to another or from one orientation to another therefore trilinear interpolation is used to distribute the value of each gradient sample into adjacent histogram bins in other words each entry into a bin is multiplied by a weight of d for each dimension where d is the distance of the sample from the central value of the bin as measured in units of the histogram bin spacing the descriptor is formed from a vector containing the values of all the orientation his togram entries corresponding to the lengths of the arrows on the right side of figure the figure shows a array of orientation histograms whereas our experiments below show that the best results are achieved with a array of histograms with orientation bins in each therefore the experiments in this paper use a element feature vector for each keypoint finally the feature vector is modified to reduce the effects of illumination change first the vector is normalized to unit length a change in image contrast in which each pixel value is multiplied by a constant will multiply gradients by the same constant so this contrast change will be canceled by vector normalization a brightness change in which a constant is added to each image pixel will not affect the gradient values as they are computed from pixel differences therefore the descriptor is invariant to affine changes in illumination however non linear illumination changes can also occur due to camera saturation or due to illumination changes that affect surfaces with differing orientations by different amounts these effects can cause a large change in relative magnitudes for some gradients but are less likely to affect the gradient orientations therefore we reduce the influence of large gradient magnitudes by thresholding the values in the unit feature vector to each be no larger than and then renormalizing to unit length this means that matching the magnitudes for large gradients is no longer as important and that the distribution of orientations has greater emphasis the value of was determined experimentally using images containing differing illuminations for the same objects descriptor testing there are two parameters that can be used to vary the complexity of the descriptor the number of orientations r in the histograms and the width n of the n n array of orientation histograms the size of the resulting descriptor vector is as the complexity of the descriptor grows it will be able to discriminate better in a large database but it will also be more sensitive to shape distortions and occlusion figure shows experimental results in which the number of orientations and size of the descriptor were varied the graph was generated for a viewpoint transformation in which a planar surface is tilted by degrees away from the viewer and image noise is added this is near the limits of reliable matching as it is in these more difficult cases that descriptor performance is most important the results show the percent of keypoints that find a correct match to the single closest neighbor among a database of keypoints the graph shows that a single orientation histogram n is very poor at discriminating but the results continue to improve up to a array of histograms with orientations after that adding more orientations or a larger descriptor can actually hurt matching by making the descriptor more sensitive to distortion these results were broadly similar for other degrees of view point change and noise although in some simpler cases discrimination continued to improve from already high levels with and higher descriptor sizes throughout this paper we use a descriptor with orientations resulting in feature vectors with dimensions while the dimensionality of the descriptor may seem high we have found that it consistently performs better than lower dimensional descriptors on a range of matching tasks and that the computational cost of matching remains low when using the approximate nearest neighbor methods described below width n of descriptor angle deg noise figure this graph shows the percent of keypoints giving the correct match to a database of keypoints as a function of width of the n n keypoint descriptor and the number of orientations in each histogram the graph is computed for images with affine viewpoint change of degrees and addition of noise sensitivity to affine change the sensitivity of the descriptor to affine change is examined in figure the graph shows the reliability of keypoint location and scale selection orientation assignment and nearest neighbor matching to a database as a function of rotation in depth of a plane away from a viewer it can be seen that each stage of computation has reduced repeatability with increas ing affine distortion but that the final matching accuracy remains above out to a degree change in viewpoint to achieve reliable matching over a wider viewpoint angle one of the affine invariant detectors could be used to select and resample image regions as discussed in section as mentioned there none of these approaches is truly affine invariant as they all start from initial feature locations determined in a non affine invariant manner in what appears to be the most affine invariant method mikolajczyk has proposed and run detailed experiments with the harris affine detector he found that its keypoint repeatability is below that given here out to about a degree viewpoint angle but that it then retains close to repeatability out to an angle of degrees which provides better performance for extreme affine changes the disadvantages are a much higher computational cost a reduction in the number of keypoints and poorer stability for small affine changes due to errors in assigning a consistent affine frame under noise in practice the allowable range of rotation for objects is considerably less than for planar surfaces so affine invariance is usually not the limiting factor in the ability to match across viewpoint change if a wide range of affine invariance is desired such as for a surface that is known to be planar then a simple solution is to adopt the approach of pritchard and heidrich in which additional sift features are generated from affine transformed versions of the training image corresponding to degree viewpoint changes this allows for the use of standard sift features with no additional cost when processing the image to be recognized but results in an increase in the size of the feature database by a factor of viewpoint angle degrees figure this graph shows the stability of detection for keypoint location orientation and final matching to a database as a function of affine distortion the degree of affine distortion is expressed in terms of the equivalent viewpoint rotation in depth for a planar surface matching to large databases an important remaining issue for measuring the distinctiveness of features is how the re liability of matching varies as a function of the number of features in the database being matched most of the examples in this paper are generated using a database of images with about keypoints figure shows how the matching reliability varies as a func tion of database size this figure was generated using a larger database of images with a viewpoint depth rotation of degrees and image noise in addition to the usual random image rotation and scale change the dashed line shows the portion of image features for which the nearest neighbor in the database was the correct match as a function of database size shown on a logarithmic scale the leftmost point is matching against features from only a single image while the rightmost point is selecting matches from a database of all features from the images it can be seen that matching reliability does decrease as a function of the number of distractors yet all indications are that many correct matches will continue to be found out to very large database sizes the solid line is the percentage of keypoints that were identified at the correct match ing location and orientation in the transformed image so it is only these points that have any chance of having matching descriptors in the database the reason this line is flat is that the test was run over the full database for each value while only varying the portion of the database used for distractors it is of interest that the gap between the two lines is small indicating that matching failures are due more to issues with initial feature localization and orientation assignment than to problems with feature distinctiveness even out to large database sizes number of keypoints in database log scale figure the dashed line shows the percent of keypoints correctly matched to a database as a function of database size using a logarithmic scale the solid line shows the percent of keypoints assigned the correct location scale and orientation images had random scale and rotation changes an affine transform of degrees and image noise of added prior to matching application to object recognition the major topic of this paper is the derivation of distinctive invariant keypoints as described above to demonstrate their application we will now give a brief description of their use for object recognition in the presence of clutter and occlusion more details on applications of these features to recognition are available in other papers lowe lowe se lowe and little object recognition is performed by first matching each keypoint independently to the database of keypoints extracted from training images many of these initial matches will be incorrect due to ambiguous features or features that arise from background clutter therefore clusters of at least features are first identified that agree on an object and its pose as these clusters have a much higher probability of being correct than individual feature matches then each cluster is checked by performing a detailed geometric fit to the model and the result is used to accept or reject the interpretation keypoint matching the best candidate match for each keypoint is found by identifying its nearest neighbor in the database of keypoints from training images the nearest neighbor is defined as the keypoint with minimum euclidean distance for the invariant descriptor vector as was described in section however many features from an image will not have any correct match in the training database because they arise from background clutter or were not detected in the training im ages therefore it would be useful to have a way to discard features that do not have any good match to the database a global threshold on distance to the closest feature does not perform well as some descriptors are much more discriminative than others a more ef fective measure is obtained by comparing the distance of the closest neighbor to that of the ratio of distances closest next closest figure the probability that a match is correct can be determined by taking the ratio of distance from the closest neighbor to the distance of the second closest using a database of keypoints the solid line shows the pdf of this ratio for correct matches while the dotted line is for matches that were incorrect second closest neighbor if there are multiple training images of the same object then we define the second closest neighbor as being the closest neighbor that is known to come from a different object than the first such as by only using images known to contain different ob jects this measure performs well because correct matches need to have the closest neighbor significantly closer than the closest incorrect match to achieve reliable matching for false matches there will likely be a number of other false matches within similar distances due to the high dimensionality of the feature space we can think of the second closest match as providing an estimate of the density of false matches within this portion of the feature space and at the same time identifying specific instances of feature ambiguity figure shows the value of this measure for real image data the probability density functions for correct and incorrect matches are shown in terms of the ratio of closest to second closest neighbors of each keypoint matches for which the nearest neighbor was a correct match have a pdf that is centered at a much lower ratio than that for incorrect matches for our object recognition implementation we reject all matches in which the distance ratio is greater than which eliminates of the false matches while discarding less than of the correct matches this figure was generated by matching images following random scale and orientation change a depth rotation of degrees and addition of image noise against a database of keypoints efficient nearest neighbor indexing no algorithms are known that can identify the exact nearest neighbors of points in high di mensional spaces that are any more efficient than exhaustive search our keypoint descriptor has a dimensional feature vector and the best algorithms such as the k d tree friedman et al provide no speedup over exhaustive search for more than about dimensional spaces therefore we have used an approximate algorithm called the best bin first bbf algorithm beis and lowe this is approximate in the sense that it returns the closest neighbor with high probability the bbf algorithm uses a modified search ordering for the k d tree algorithm so that bins in feature space are searched in the order of their closest distance from the query location this priority search order was first examined by arya and mount and they provide further study of its computational properties in arya et al this search order requires the use of a heap based priority queue for efficient determination of the search order an approximate answer can be returned with low cost by cutting off further search after a specific number of the nearest bins have been explored in our implementation we cut off search after checking the first nearest neighbor candidates for a database of keypoints this provides a speedup over exact nearest neighbor search by about orders of magnitude yet results in less than a loss in the number of correct matches one reason the bbf algorithm works particularly well for this problem is that we only consider matches in which the nearest neighbor is less than times the distance to the second nearest neighbor as described in the previous section and therefore there is no need to exactly solve the most difficult cases in which many neighbors are at very similar distances clustering with the hough transform to maximize the performance of object recognition for small or highly occluded objects we wish to identify objects with the fewest possible number of feature matches we have found that reliable recognition is possible with as few as features a typical image contains or more features which may come from many different objects as well as background clutter while the distance ratio test described in section will allow us to discard many of the false matches arising from background clutter this does not remove matches from other valid objects and we often still need to identify correct subsets of matches containing less than inliers among outliers many well known robust fitting methods such as ransac or least median of squares perform poorly when the percent of inliers falls much below fortunately much better performance can be obtained by clustering features in pose space using the hough transform hough ballard grimson the hough transform identifies clusters of features with a consistent interpretation by using each feature to vote for all object poses that are consistent with the feature when clusters of features are found to vote for the same pose of an object the probability of the interpretation being correct is much higher than for any single feature each of our keypoints specifies parameters location scale and orientation and each matched keypoint in the database has a record of the keypoint parameters relative to the training image in which it was found therefore we can create a hough transform entry predicting the model location orientation and scale from the match hypothesis this prediction has large error bounds as the similarity transform implied by these parameters is only an approximation to the full degree of freedom pose space for a object and also does not account for any non rigid deformations therefore we use broad bin sizes of degrees for orientation a factor of for scale and times the maximum projected training image dimension using the predicted scale for location to avoid the problem of boundary effects in bin assignment each keypoint match votes for the closest bins in each dimension giving a total of entries for each hypothesis and further broadening the pose range in most implementations of the hough transform a multi dimensional array is used to represent the bins however many of the potential bins will remain empty and it is difficult to compute the range of possible bin values due to their mutual dependence for example the dependency of location discretization on the selected scale these problems can be avoided by using a pseudo random hash function of the bin values to insert votes into a one dimensional hash table in which collisions are easily detected solution for affine parameters the hough transform is used to identify all clusters with at least entries in a bin each such cluster is then subject to a geometric verification procedure in which a least squares solution is performed for the best affine projection parameters relating the training image to the new image an affine transformation correctly accounts for rotation of a planar surface under orthographic projection but the approximation can be poor for rotation of non planar objects a more general solution would be to solve for the fundamental matrix luong and faugeras hartley and zisserman however a fundamental matrix solution requires at least point matches as compared to only for the affine solution and in practice requires even more matches for good stability we would like to perform recognition with as few as feature matches so the affine solution provides a better starting point and we can account for errors in the affine approximation by allowing for large residual errors if we imagine placing a sphere around an object then rotation of the sphere by degrees will move no point within the sphere by more than times the projected diameter of the sphere for the examples of typical objects used in this paper an affine solution works well given that we allow residual errors up to times the maximum projected dimension of the object a more general approach is given in brown and lowe in which the initial solution is based on a similarity transform which then progresses to solution for the fundamental matrix in those cases in which a sufficient number of matches are found the affine transformation of a model point x y t to an image point u v t can be written as u x tx where the model translation is tx ty t and the affine rotation scale and stretch are repre sented by the mi parameters we wish to solve for the transformation parameters so the equation above can be rewrit ten to gather the unknowns into a column vector x y u v tx this equation shows a single match but any number of further matches can be added with each match contributing two more rows to the first and last matrix at least matches are needed to provide a solution we can write this linear system as ax b figure the training images for two objects are shown on the left these can be recognized in a cluttered image with extensive occlusion shown in the middle the results of recognition are shown on the right a parallelogram is drawn around each recognized object showing the boundaries of the original training image under the affine transformation solved for during recognition smaller squares indicate the keypoints that were used for recognition the least squares solution for the parameters x can be determined by solving the correspond ing normal equations x ata which minimizes the sum of the squares of the distances from the projected model locations to the corresponding image locations this least squares approach could readily be extended to solving for pose and internal parameters of articulated and flexible objects lowe outliers can now be removed by checking for agreement between each image feature and the model given the more accurate least squares solution we now require each match to agree within half the error range that was used for the parameters in the hough transform bins if fewer than points remain after discarding outliers then the match is rejected as outliers are discarded the least squares solution is re solved with the remaining points and the process iterated in addition a top down matching phase is used to add any further matches that agree with the projected model position these may have been missed from the hough transform bin due to the similarity transform approximation or other errors the final decision to accept or reject a model hypothesis is based on a detailed probabilis tic model given in a previous paper lowe this method first computes the expected number of false matches to the model pose given the projected size of the model the number of features within the region and the accuracy of the fit a bayesian analysis then gives the probability that the object is present based on the actual number of matching features found we accept a model if the final probability for a correct interpretation is greater than for objects that project to small regions of an image features may be sufficient for reli able recognition for large objects covering most of a heavily textured image the expected number of false matches is higher and as many as feature matches may be necessary recognition examples figure shows an example of object recognition for a cluttered and occluded image con taining objects the training images of a toy train and a frog are shown on the left figure this example shows location recognition within a complex scene the training images for locations are shown at the upper left and the pixel test image taken from a different viewpoint is on the upper right the recognized regions are shown on the lower image with keypoints shown as squares and an outer parallelogram showing the boundaries of the training images under the affine transform used for recognition the middle image of size pixels contains instances of these objects hidden behind others and with extensive background clutter so that detection of the objects may not be im mediate even for human vision the image on the right shows the final correct identification superimposed on a reduced contrast version of the image the keypoints that were used for recognition are shown as squares with an extra line to indicate orientation the sizes of the squares correspond to the image regions used to construct the descriptor an outer parallel ogram is also drawn around each instance of recognition with its sides corresponding to the boundaries of the training images projected under the final affine transformation determined during recognition another potential application of the approach is to place recognition in which a mobile device or vehicle could identify its location by recognizing familiar locations figure gives an example of this application in which training images are taken of a number of locations as shown on the upper left these can even be of such seemingly non distinctive items as a wooden wall or a tree with trash bins the test image of size by pixels on the upper right was taken from a viewpoint rotated about degrees around the scene from the original positions yet the training image locations are easily recognized all steps of the recognition process can be implemented efficiently so the total time to recognize all objects in figures or is less than seconds on a pentium processor we have implemented these algorithms on a laptop computer with attached video camera and have tested them extensively over a wide range of conditions in general textured planar surfaces can be identified reliably over a rotation in depth of up to degrees in any direction and under almost any illumination conditions that provide sufficient light and do not produce excessive glare for objects the range of rotation in depth for reliable recognition is only about degrees in any direction and illumination change is more disruptive for these reasons object recognition is best performed by integrating features from multiple views such as with local feature view clustering lowe these keypoints have also been applied to the problem of robot localization and map ping which has been presented in detail in other papers se lowe and little in this application a trinocular stereo system is used to determine estimates for keypoint loca tions keypoints are used only when they appear in all images with consistent disparities resulting in very few outliers as the robot moves it localizes itself using feature matches to the existing map and then incrementally adds features to the map while updating their positions using a kalman filter this provides a robust and accurate solution to the problem of robot localization in unknown environments this work has also addressed the problem of place recognition in which a robot can be switched on and recognize its location anywhere within a large map se lowe and little which is equivalent to a implementation of object recognition conclusions the sift keypoints described in this paper are particularly useful due to their distinctive ness which enables the correct match for a keypoint to be selected from a large database of other keypoints this distinctiveness is achieved by assembling a high dimensional vector representing the image gradients within a local region of the image the keypoints have been shown to be invariant to image rotation and scale and robust across a substantial range of affine distortion addition of noise and change in illumination large numbers of keypoints can be extracted from typical images which leads to robustness in extracting small objects among clutter the fact that keypoints are detected over a complete range of scales means that small local features are available for matching small and highly occluded objects while large keypoints perform well for images subject to noise and blur their computation is efficient so that several thousand keypoints can be extracted from a typical image with near real time performance on standard pc hardware this paper has also presented methods for using the keypoints for object recognition the approach we have described uses approximate nearest neighbor lookup a hough transform for identifying clusters that agree on object pose least squares pose determination and fi nal verification other potential applications include view matching for reconstruction motion tracking and segmentation robot localization image panorama assembly epipolar calibration and any others that require identification of matching locations between images there are many directions for further research in deriving invariant and distinctive image features systematic testing is needed on data sets with full viewpoint and illumination changes the features described in this paper use only a monochrome intensity image so fur ther distinctiveness could be derived from including illumination invariant color descriptors funt and finlayson brown and lowe similarly local texture measures appear to play an important role in human vision and could be incorporated into feature descriptors in a more general form than the single spatial frequency used by the current descriptors an attractive aspect of the invariant local feature approach to matching is that there is no need to select just one feature type and the best results are likely to be obtained by using many different features all of which can contribute useful matches and improve overall robustness another direction for future research will be to individually learn features that are suited to recognizing particular objects categories this will be particularly important for generic object classes that must cover a broad range of possible appearances the research of we ber welling and perona and fergus perona and zisserman has shown the potential of this approach by learning small sets of local features that are suited to recogniz ing generic classes of objects in the long term feature sets are likely to contain both prior and learned features that will be used according to the amount of training data that has been available for various object classes peekaboom a game for locating objects in images luis von ahn ruoran liu and manuel blum computer science department carnegie mellon university forbes avenue pittsburgh pa biglou royliu mblum cs cmu edu abstract we introduce peekaboom an entertaining web based game that can help computers locate objects in images people play the game because of its entertainment value and as a side effect of them playing we collect valuable image metadata such as which pixels belong to which object in the image the collected data could be applied towards constructing more accurate computer vision algorithms which require massive amounts of training and testing data not currently available peekaboom has been played by thousands of people some of whom have spent over hours a day playing and thus far has generated millions of data points in addition to its purely utilitarian aspect peekaboom is an example of a new emerging class of games which not only bring people together for leisure purposes but also exist to improve artificial intelligence such games appeal to a general audience while providing answers to problems that computers cannot yet solve author keywords distributed knowledge acquisition object segmentation object recognition computer vision web based games acm classification keywords learning knowledge acquisition h hci web based interaction introduction humans understand and analyze everyday images with little effort what objects are in the image where they are located what is the background what is the foreground etc computers on the other hand still have trouble with such basic visual tasks as reading distorted text or finding where in the image a simple object is located although researchers have proposed and tested many impressive algorithms for computer vision none have been made to work reliably and generally most of the best approaches for computer vision e g rely on machine learning train an algorithm to perform a visual task by showing it example images in which the task has already been performed for example permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee chi april montréal québec canada copyright acm training an algorithm for testing whether an image contains a dog would involve presenting it with multiple images of dogs each annotated with the precise location of the dog in the image after processing enough images the algorithm learns to find dogs in arbitrary images a major problem with this approach however is the lack of training data which obviously must be prepared by hand databases for training computer vision algorithms currently have hundreds or at best a few thousand images orders of magnitude less than what is required in this paper we address the problem of constructing a massively large database for training computer vision algorithms the target database will contain millions of images all fully annotated with information about what objects are in the image where each object is located and how much of the image is necessary to recognize it our database will be similar to those previously shown to be useful for training computer vision algorithms e g to construct such a database we follow the approach taken by the esp game and introduce a new game called peekaboom peekaboom is an extremely enjoyable networked game in which simply by playing people help construct a database for training computer vision algorithms we guarantee the database correctness even if the people playing the game don t intend it as we will show in this paper our game is also very enjoyable with some people having played over hours a week we will further show that this game can be used to improve image search results and to calculate object bounding boxes similar to those in flickr see figure the esp game is an interactive system that allows people to label images while having fun the esp game collects random images from the web and outputs word labels describing the contents of the images the game has already collected millions of labels for arbitrary images given an image the esp game can be used to determine what objects are in the image but cannot be used to determine where in the image each object is located such location information is necessary for training and testing computer vision algorithms so the data collected by the esp game is not sufficient for our purposes the game introduced in this paper peekaboom improves on the data collected by the esp game and for each object in the image outputs precise location information as well as other information useful for training computer vision algorithms by playing a game people help us collect data figure peek and boom boom gets an image along with a word related to it and must reveal parts of the image for peek to guess the correct word peek can enter multiple guesses that boom can see not because they want to be helpful but because they have fun indeed peekaboom or the esp game or any game built on this premise can be treated as a human algorithm on input an image it outputs with arbitrarily high probability a correct annotation of the image instead of using a silicon processor this algorithm runs on a processor consisting of regular humans interacting throughout the web in addition to applications in computer vision and image search our system makes a significant contribution to hci because of the way it addresses the problem peekaboom presents an example of a new line of research devoted to solving large scale problems with human computing power where people interact with computers to extend the computational abilities of machines basic game play peekaboom as the name may suggest is a game with two main components peek and boom two random players from the web participate by taking different roles in the game when one player is peek the other is boom peek starts out with a blank screen while boom starts with an image and a word related to it see figure the goal of the game is for boom to reveal parts of the image to peek so that peek can guess the associated word boom reveals circular areas of the image by clicking a click reveals an area with a pixel radius peek on the other hand can enter guesses of what boom word is boom can see peek guesses and can indicate whether they are hot or cold when peek correctly guesses the word the players get points and switch roles play then proceeds on a new image word pair if the image word pair is too difficult the two players can pass or opt out of the current image passing creates the same effect as a correct guess from peek except that the players get no points to maximize points boom has an incentive to reveal only the areas of the image necessary for peek to guess the correct word for example if the image contains a car and a dog and the word associated to the image is dog then boom will reveal only those parts of the image that contain the dog thus given an image word pair data from the game yield the area of the image pertaining to the word pings another component of the game are pings ripples that appear on peek screen when boom right clicks on the image see figure if two players were playing with the image on figure then many correct words are possible from peek point of view elephant trunk tusk ear suppose the correct word is trunk to get peek to guess correctly boom can ping the trunk of the elephant by right clicking on it in doing so boom helps to disambiguate the trunk from the rest of the elephant figure pings to help peek boom can ping parts of the image by right clicking on them figure hints boom can further help peek by giving hints about how the word relates to the image is it a noun describing something in the image a noun related to the image text on the image or a verb hints another feature of the game are buttons that allow boom to give hints to peek about how the word relates to the image see figures and upon boom pressing of one of the hint buttons a corresponding flashing placard appears on peek screen the reason for having hints is that often the words can relate to the image in multiple ways as nouns verbs text or related nouns something not in the image but related to it the origin of images and labels all words presented to the players are related to their corresponding image on input an image word pair peekaboom outputs a region of the image that is related to the word we obtain millions of images with associated keyword labels from the esp game which we now describe in more detail as mentioned before the esp game is a two player online game that pairs random players from the web from the player perspective the goal of the esp game is to guess the word that their partner is typing for each image once both players have typed the same string they move on to a next image since the players can t communicate and don t know anything about each other the easiest way for both to type the same string is by typing something related to the common image the string upon which the two players agree is a very good label for the image we use the labels collected from the esp game as the words we present to the players in peekaboom game points and the bonus round although the exact number of points given to the players for different actions is not important we mention it to show the relative proportions furthermore we mention the different point strategies used by peekaboom to keep players engaged points are given to both peek and boom equally whenever peek guesses the correct word in the current implementation both obtain points points are not subtracted for passing points are also given to both peek and boom for using the hint buttons although this might appear counterintuitive since using hints deducts points in many other games we actually want the players to use the hint buttons as mentioned above hints give us additional information about the relationship between the word and the image and therefore we encourage players to use them twenty five extra points are given to both peek and boom whenever peek guesses the correct word and boom had used a hint points are not given for usage of the hot cold buttons every time the players correctly complete four images they are sent to a bonus round the bonus round is different in nature from the rest of the game and allows players to obtain up to points in the bonus round see figure players simply click on an object in the image the closer they are to each other clicks the more points they get for example both players could obtain an image of a car and be told click on the car figure the peekaboom bonus round players must click on the specified object within the image they obtain points proportional to how close their clicks are to each other clicks players obtain between and points for every click in the bonus round depending on how far the click is from their partner corresponding click the bonus round is timed players have to click on the same place as their partner as many times as they can in seconds if the object is not in the image players can pass because some images do not contain the object related to the word passing in the bonus round generates points for both players so we can learn whether the object is there players cannot pass after they have clicked on the image there are two reasons for the peekaboom bonus round first by giving players bite size milestones getting four images correctly we reinforce their incremental success in the game and thus encourage them to continue playing second the bonus round is an alternative approach to collecting training data for computer vision in it players click inside specific objects within an image such clicks give additional information for training computer vision algorithms in this paper we do not concern ourselves with such information but remark that it is also useful collecting image metadata our goal is to construct a database for training computer vision algorithms here we discuss exactly what information is collected by peekaboom and how it is collected on input an image word pair coming directly from the esp game peekaboom collects the following information how the word relates to the image is it an object person or animal in the image is it text in the image is it a verb describing an action in the image is it an object person or animal not in the image but related to it the esp game associates words to images but does not say how the word is related to the image figure for instance shows multiple ways in which a word can be related to an image hint buttons in peekaboom allow us to determine the relation of the word to the image this is useful in multiple ways but for the purposes of constructing training sets for computer vision it allows us to weed out related nouns and to treat text separately pixels necessary to guess the word when peek enters the correct word the area that boom has revealed is precisely enough to guess the word that is we can learn exactly what context is necessary to determine what the word refers to this context information is absolutely necessary when attempting to determine what type of object a set of pixels constitutes see figure the pixels inside the object animal or person if the word is a noun directly referring to something in the image pings give us pixels that are inside the object person or animal figure the image on the left contains a car driving through the street while the one on the right has a person crossing the same street both the car and the person are exactly the same set of pixels up to a rotation by degrees example taken from the most salient aspects of the objects in the image by inspecting the sequence of boom clicks we gain information about what parts of the image are salient with respect to the word boom typically reveals the most salient parts of the image first e g face of a dog instead of the legs etc elimination of poor image word pairs if many independent pairs of players agree to pass on an image without taking action on it then likely they found it impossibly hard because of poor picture quality or a dubious relation between the image and its label by implementing an eviction policy for images that we discover are bad we can improve the quality of the data collected as well as the fun level of the game when multiple players have gone through the same image these pieces of information can be combined intelligently to give extremely accurate and useful annotations for computer vision later in the paper for example we show how a simple algorithm can use the data produced by peekaboom to calculate accurate object bounding boxes see figure the single player game peekaboom is a two player game oftentimes however there will be an odd number of people attempting to play the game so the remaining person cannot be paired to prevent their frustration we also have a single player version of the game in which the player is matched with a server side bot our bot acts intelligently to simulate a human player by being based on pre recorded games in other words we take data collected from pairs of humans and use it as the basis for the computer player logic emulating a boom player is fairly simple the bot can regurgitate the sequence of recorded clicks to the human emulating peek is much more complicated the bot needs to have some concept of closeness of the human clicks to the set of recorded clicks for instance if the human does not reveal the dog in the picture the bot should not guess dog our bot only reveals a certain pre recorded guess if enough area has been revealed towards this end it employs a spatial data structure whose members are circles each of which corresponds to a click elements of the data structure are removed as they are clicked on by the human player when the data structure becomes empty the bot gives the correct answer moreover it has the ability to make incorrect guesses along the way based on the relative emptiness of the spatial data structure cheating peekaboom is a collaborative game partners work together to maximize their score when both partners do not communicate outside the game environment we obtain correct information however if the two partners collude to cheat on the game the data could be poisoned for instance if boom and peek know each other and have an outside means of communication then boom can simply tell peek what words to type peekaboom contains multiple anti cheating mechanisms through a combination of online in game enforcement and offline analysis we are able to detect and deal with cheating before detailing peekaboom anti cheating measures we mention that cheating attempts are uncommon although a minority of players might obtain satisfaction from gaming the system the majority of them just want to play the game honestly indeed as anecdotal evidence when peekaboom was tested in a room with children of ages they would cover the word with their hand to prevent others in the room from seeing the answers nevertheless peekaboom does have a full set of measures to prevent collusion the player queue when players log on to the game server they are not immediately paired off instead the server makes them wait n seconds where n is the number of seconds until the next matching interval currently matching intervals happen every seconds and when they do the server matches everyone in the queue with a partner any odd person out will be paired with a bot with a large number of players in the system we can ensure that a player partner is random and prevent colluders from getting matched just because they clicked start playing at the same time ip address checks we also check player ip addresses to ensure that they are not paired with themselves or with others that have a similar address similarity in ip addresses can imply geographical proximity seed images because our system is a web based game one point of concern is that bots i e automated players might play the game and pollute the pool of collected data to detect them we introduce seed images into the system in other words those for which we have hand verified metadata on being presented seed images if a player consistently fails to click on the relevant parts when playing boom or to guess the correct words when playing peek they will be added to a blacklist we discard all current and future game play data associated with anyone on the blacklist notice that almost by definition a computer program cannot successfully play peekaboom if it were able to do so then it would be able to recognize the objects in the images therefore this strategy prevents bots as well as otherwise malicious players from poisoning our data limited freedom to enter guesses since boom can see all of peek guesses the game allows a limited form of communication between the players indeed many of the peekaboom players use the guess field as a way to communicate with their partner it is not uncommon for the first guess in a game to be hi or for the first guess after passing on an image to be the correct word associated with the previous image it is also not uncommon for players to type sorry after taking too long on an image a possible cheating strategy is to exchange im screen names through the guess field and then using im communicate the correct words although we have never observed attempts to execute such a strategy we can mitigate it by not allowing peek to enter any non alphabetical characters such as numbers similarly we can prevent boom from seeing any guesses that are not words in the dictionary currently we do allow boom to see such guesses because we have not seen players attempt to cheat in this way however even if players are successful in such a strategy the other anti collusion mechanisms can deal with the corrupted data aggregating data from multiple players in addition to the above strategies we aggregate data from multiple players for a given image word pair by doing this we can eliminate outliers implementation we implemented the architecture of the game under the client server model the client application is delivered as a java applet while the server is written purely in java applets connect to a server which then matches the players with games of peekaboom upon two players completion of a match the server writes their game play data and scores to disk we then compile the collected data into desired formats our implementation of the game contains many features to improve game play spelling check incorrectly spelled words are displayed in a different color to notify players this is important because the peek player usually types multiple guesses in a short time often making spelling mistakes inappropriate word replacement since boom can see peek guesses we do not allow peek to enter inappropriate words whenever one of peek guesses is among a list of possible inappropriate words we substitute it with another word chosen from a list of innocent words such as love caring iluvpeekaboom etc top scores list and ranks the peekaboom website prominently displays the cumulative top scores of the day as well as the top scores of all time furthermore players are given a rank based on the total number of points they have accumulated throughout time see figure the different ranks are fresh meat points newbie points player points gangster points and godfather or more points we remark that ranks have proven an important component of peekaboom incentive strategy of the players that have obtained an account of them have scores that fall within points of the rank cutoffs given that these intervals cover less than of the space of possible cumulative scores this strongly suggests that many players simply play to reach a new rank figure top scores and player ranks players are shown their current rank and the number of points remaining for the next rank additional applications before going to the evaluation section we mention two additional applications for the data collected by peekaboom a benefit of these applications is that they are direct in that they do not require the training of machine learning algorithms improving image search results peekaboom gives an accurate estimate of the fraction of the image related to the word in question this estimate can be calculated from the area revealed by boom the fraction of the image related to a word can be used to order image search results images in which the word refers to a higher fraction of the total pixels should be ranked higher much like the goal of the esp game is to label all images on the web we can imagine peekaboom doing the same and thus further improving image search object bounding boxes in the same vein peekaboom can be used to directly calculate object bounding boxes similar to those used in flickr see figure flickr is a photo sharing service that allows users to tag images with keywords and to associate keywords with rectangular areas in the image the areas and tags however are not guaranteed to be correct since a user can enter anything they wish for their own images to exhibit the power of the data collected by peekaboom we show how to use it calculate such rectangles we emphasize however that the data collected by peekaboom is significantly richer and that to calculate the rectangles we discard vast amounts of the information collected by our game since peekaboom annotates arbitrary images on the web its data allows for an image search engine in which the results are highlighted similar to the highlighted words in google text search results using the data obtained in the first two weeks of game play we have implemented a prototype of such a search engine see figure the search engine can be accessed from the peekaboom website the bounding boxes were calculated as follows for a single play of an image word pair we create a matrix of and the dimensions of the matrix are the same as the dimensions of the image in pixels at first every entry in the matrix is a we add a in every pixel clicked by boom as well as in the circle of radius pixels around the click we thus obtain a matrix of and corresponding to the exact area that was revealed in a single game play next we combine different plays of the same image word pair by adding their corresponding matrices this gives a matrix whose entries are integers corresponding to the number of different players that revealed each pixel of the image on this combined matrix we apply a threshold of meaning that we substitute every value less than with and every value greater than with this gives a matrix corresponding to all the pixels that have been revealed by at least players next we cluster these pixels and calculate figure object bounding boxes obtained from peekaboom data the bounding boxes by taking for each cluster the leftmost rightmost topmost and bottommost points this algorithm may produce multiple bounding boxes for a single image word pair for instance in figure we can see that many of the results for eyes have two bounding boxes one corresponding to each eye as we will see the results produced by this simplistic algorithm are extremely accurate such results could be improved by making intelligent use of the additional data given by peekaboom such as pings the precise order of the areas revealed etc but for the purposes of this paper we use the simplistic algorithm alternative using ping data for pointing instead of showing bounding boxes calculated from revealed areas we could show arrows or lines pointing to the objects see figure such pointers can be easily calculated from the ping data the simplest algorithm for doing so is to select a ping at random and assume it is a good pointer for the object we will show that this simplistic algorithm gives very accurate results figure shows an image in which the different objects have been located using ping data more elaborate algorithms could give even better results we remark however that simply averaging the pings over multiple players to obtain a single pointer does not give accurate results for instance if the object was eyes averaging the pings gives a pointer to a region that is not an eye figure calculation of object pointers using pings evaluation user statistics the evaluation of our claims consists of two parts first we must show that the game is indeed enjoyable second we must show that the data produced by the game is accurate it is difficult to evaluate how enjoyable a game really is one approach is to ask participants a series of questions regarding how much they enjoyed playing the game our data for such an approach were extremely positive but we follow a different approach in this paper we present usage statistics from arbitrary people playing our game online this same approach was used by the esp game usage statistics peekaboom was released to a general audience on august of we present the usage statistics from the period starting august and ending september a total of different people played the game during this time generating pieces of data by different people we mean different user ids by a piece of data we mean a successful round of peekaboom in which peek correctly guessed the word given boom revealed region we mention that an image word pair can have multiple pieces of data associated to it if it occurs in multiple games if people gave us pieces of data then on average each person played on images since each session of the game lasts minutes and on average players go through images during a session in this one month period each person played on average minutes without counting time spent waiting for a partner etc over of the people played on more than one occasion that is more than of the people played on different dates furthermore every player in the top scores list played over games that over hours without including the time they spent waiting for a partner this undoubtedly attests to how enjoyable the game is user comments to give a further sense for how much the players enjoyed the game we include below some quotes taken from comments submitted by players using a link on the website the game itself is extremely addictive as there is an element of pressure involved in beating the clock a drive to score more points the feeling that you could always do better next time and a curiosity about what is going to come up next i would say that it gives the same gut feeling as combining gambling with charades while riding on a roller coaster the good points are that you increase and stimulate your intelligence you don t lose all your money and you don t fall off the ride the bad point is that you look at your watch and eight hours have just disappeared one unfortunate side effect of playing so much in such a short time was a mild case of carpal tunnel syndrome in my right hand and forearm but that dissipated quickly this game is like crack i ve been peekaboom free for hours unlike other games peekaboom is cooperative rather than competitive evaluation accuracy of collected data the usefulness of peekaboom as a data collection method rests in the quality of the data we collect although the design of the game inherently ensures correctness of the data we wanted to test whether it is as good as what would be collected directly from volunteers in a non game setting to do so we conducted two experiments to test first the accuracy of the bounding boxes we defined and second the utility of the pointing behavior in the game notice that these experiments are meant to analyze the correctness of the data and not whether such data can be used to train computer vision algorithms the usefulness of data about location of objects for training computer vision algorithms has been previously established experiment accuracy of bounding boxes in the first experiment we tested whether the bounding boxes for objects within an image that are calculated from peekaboom are as good as bounding boxes people would make around an object in a non game setting we selected at random image word pairs from the data pool that had been successfully played on by at least two independent pairs of people the images selected all had nouns as their word as opposed to text in the image or an adjective etc see figure all the images chosen had the word refer to a single object in the image for each image peekaboom data was used to calculate object bounding boxes using the method explained in previous sections we then had four volunteers make bounding boxes around the objects for each image providing us with bounding boxes drawn by volunteers the volunteers were asked for each image to draw a bounding box around the object that the word referred to we then selected at random one of the four volunteer bounding boxes for each image so as to end up with one volunteer generated bounding box for every one of the images finally we tested the amount of overlap between the bounding boxes generated by peekaboom and those generated by our volunteers the amount of overlap was determined using the formula overlap a b area a b area a b where a and b are the bounding boxes notice that if a b then overlap a b and if a is disjoint from b then overlap a b we calculated the average overlap across the images as well as the standard deviation results on average the overlap between the peekaboom bounding boxes and the volunteer generated ones was with standard deviation this means that the peekaboom bounding boxes were very close to those generated by the volunteers to illustrate we show in figure the bounding box that obtained the lowest overlap score figure experiment image with lowest overlap between a volunteer generated bounding box solid lines and one generated by peekaboom dashed lines given that peekaboom was not directly built to calculate bounding boxes this shows the wide applicability of the data collected experiment accuracy of pings in the second experiment we tested whether the object pointers that are calculated from peekaboom are indeed inside the objects as in the previous experiment we selected at random image label pairs from the data pool that have been successfully played on by at least two independent pairs of people the images selected all had the word as a noun as opposed to as text in the image or an adjective etc see figure all the images chosen had the word refer to a single object in the image for each image peekaboom data was used to calculate object pointers using the method explained in previous sections we then asked three volunteer raters to determine for each pointer whether it was inside the object or not the raters were shown examples of pointers inside and outside the object and were told that near an object does not count as inside the object results according to all the raters of the pointers were inside the object referred to by the word this gives positive evidence that ping data is accurate especially since it was calculated using such a simplistic algorithm generalizing our approach the approach presented in this paper solving a problem by having people play games online can be generalized to many other problems in artificial intelligence in follow up work for example we have created two other games verbosity and phetch in which players solve problems that computers cannot yet solve verbosity collects common sense facts to train reasoning algorithms for instance for the word milk the game outputs facts such as it is white people usually eat cereal with it etc verbosity is a two player game in which one player attempts to make the other say a target word e g milk without using the word they do so by saying many facts without using the word itself in their statements e g it is a white liquid the underlying game mechanism of verbosity is similar in nature to that of peekaboom much like designing an algorithm to solve a problem designing a game to harness valuable human cycles is to a large extent an art problems usually require a specifically tailored game in addition to an original idea creating such a game also depends on a broader set of criteria including looks the fluidity of the game graphics ease of use an intuitive user interface cognitive load the amount of user attention required to play the game and action the extent to which the game absorbs the user in the experience all of these aspects have been treated in this paper and we believe many of the techniques here presented generalize to creating other games with a purpose finally we believe that these design principles like the scientific method don t just provide ideas but a way of thinking games provide a valuable vehicle to solve problems that computers cannot yet solve ethical considerations as with all systems soliciting input from humans we must address the ethical issues behind the usage of the collected data towards this end we inform the players of the game purpose on the peekaboom website players participate willingly and knowingly indeed many people play because they like the fact that the game has a purpose furthermore we state on the record that the game purpose is to obtain accurate segmentations of objects from backgrounds and to train computer vision algorithms to recognize simple objects we have no intention of applying our data towards for example military surveillance related work we have presented a method for annotating arbitrary images and we have presented evidence that it produces high quality data we now survey the related work the esp game as mentioned before the esp game is two player game that collects word labels for arbitrary images peekaboom is similar to the esp game and in fact was inspired by it we consider peekaboom an extension of esp whereas esp gives data to determine which objects are in the image peekaboom can augment this data with information about where in the image objects are located in terms of game mechanics peekaboom is different from the esp game in several ways first peekaboom is asymmetric whereas both players in the esp game are performing the same role players of peekaboom alternate in performing different roles second peekaboom allows a significantly higher level of interaction among the players whereas in the esp game players cannot communicate at all in peekaboom one of the players can freely communicate with the other third the usage of hint buttons has proven very successful in peekaboom and such buttons could as well be incorporated into esp such differences in game mechanics reflect the difference in purpose of peekaboom and esp the open mind initiative perhaps less so peekaboom is also similar at least in spirit to the open mind initiative e g a worldwide effort to develop intelligent software open mind collects data from regular internet users referred to as netizens and feeds it to machine learning algorithms volunteers participate by answering questions and teaching concepts to computer programs peekaboom is similar to open mind in that we use regular people on the internet to annotate images however as with the esp game we put much greater emphasis on our method being fun we don t expect volunteers to annotate millions of images on the web we expect images to be annotated because people want to play our game whereas a typical open mind activity would ask participants to point to the object in question we transform the activity into a two player game in which players are not even asked to point to the object they do so only as a side effect of playing the game labelme labelme is a web based tool for image annotation anybody can annotate data using this tool and thus contribute to constructing a large database of annotated objects the incentive to annotate data is the data itself you can only have access to the database once you have annotated a certain number of images the main difference between peekaboom and labelme is the game aspect whereas labelme simply asks users to annotate an image peekaboom transforms the process into an enjoyable game labelme relies on people desire to help and thus assumes that the entered data is correct on the other hand peekaboom has multiple mechanisms to prevent players from polluting the data interactive machine learning another area of related work is that of interactively training machine learning algorithms e g in these systems a user is given immediate feedback about how well an algorithm is learning from the examples provided by them as with labelme peekaboom differs from these systems in the gaming aspect as well as in the assumption that our users are interested in training an algorithm conclusions and future work peekaboom is a novel complete game architecture for collecting image metadata segmenting objects in images is a unique challenge and we have tailored a game specifically to this end in the very near future we would like to make our pieces of data available to the world by formatting it as an image segmentation library like the esp game peekaboom encompasses much more than just a java applet delivered from a website rather the ideas behind the design and implementation of the game generalize to a way of harnessing and directing the power of the most intricate computing device in the world the human mind some day computers will be able to segment objects in images unassisted but that day is not today today we have engines like peekaboom that use the wisdom of humans to help naïve computers get to that point the actual process of making computers smarter given segmentation metadata is beyond the scope of this paper since it would require a far more sophisticated interpretation of the data than the simple bounding box derivation we have presented thus we see great potential in future work at the crossroads of human computer interaction and artificial intelligence where the output of our interactive system helps advance the state of the art in computer vision prof adriana kovashka university of pittsburgh september course info course website instructor adriana kovashka kovashka cs pitt edu please use at the beginning of your subject office sennott square office hours tuesday and thursday grader nils murrugarra llerena nineil cs pitt edu textbooks by richard szeliski by kristen grauman and bastian leibe course goals to learn about the basic computer vision tasks and approaches to get experience with some computer vision techniques to learn absolute basics of machine learning to think critically about vision approaches and to see connections between works and potential for improvement plan for today introductions what is computer vision why do we care what are the challenges course structure and policies overview of topics introductions introductions what is your name what is your department major and year what one thing outside of school are you passionate about what do you hope to get out of this class what do you plan to do when you graduate computer vision what is computer vision done we see with our brains not with our eyes oliver sacks and others kristen grauman adapted what is computer vision automatic understanding of images and video computing properties of the world from visual data measurement algorithms and representations to allow a machine to recognize objects people scenes and activities perception and interpretation algorithms to mine search and interact with visual data search and organization kristen grauman vision for measurement multi view stereo for real time stereo structure from motion pollefeys et al kristen grauman community photo collections goesele et al slide credit l lazebnik vision for perception interpretation objects activities scenes locations text writing faces gestures motions emotions visual search organization query image or video archives relevant content related disciplines graphics image processing artificial intelligence computer vision algorithms machine learning cognitive science vision and graphics images vision model graphics inverse problems analysis and synthesis why vision as image sources multiply so do applications relieve humans of boring easy tasks human computer interaction perception for robotics autonomous agents organize and give access to visual content why vision images and video are everywhere personal photo albums surveillance and security movies news sports medical and scientific images faces and digital cameras camera waits for everyone to smile to take a photo canon setting camera focus via face detection face recognition linking to info with a mobile device situated search yeh et al mit msr lincoln kooaba exploring photo collections snavely et al special visual effects the matrix what dreams may come mocap for pirates of the carribean industrial light and magic source s seitz interactive systems video based interfaces human joystick newsbreaker live assistive technology systems camera mouse boston college vision for medical neuroimages image guided surgery mit ai vision group fmri data golland et al safety security navigation driver safety monitoring pool poseidon pedestrian detection merl viola et al surveillance obstacles what the computer gets why is vision difficult ill posed problem real world much more complex than what we can measure in images impossible to literally invert image formation process challenges many nuisance parameters illumination object pose clutter occlusions intra class appearance viewpoint challenges intra class variation challenges importance of context challenges complexity thousands to millions of pixels in an image human recognizable object categories degrees of freedom in the pose of articulated objects humans billions of images indexed by google image search billion prints produced from digital camera images in million camera phones sold in about half of the cerebral cortex in primates is devoted to processing visual information felleman and van essen challenges limited supervision less more ok clearly the vision problem is deep and challenging time to give up active research area with exciting progress datasets today imagenet categories images microsoft coco categories images pascal categories images sun categories images some visual recognition problems recognition what is this recognition what is this building balcony street truck carriage horse table person person car detection where are the cars activity what is this person doing scene is this an indoor scene instance which city which building visual persuasion evaluating action quality transferring art style answering visual questions course structure and policies course components homework problem sets midterm exam final exam in class participation course schedule homework submission we will use courseweb navigate to the courseweb page for click on assignments and the corresponding hw attach a zip file with your written responses and code name the file as zip or tar homework is due at on the due grades will appear on courseweb exams one mid term and one final exam the final exam will focus on the latter half of the course exams will be preceded by review sessions if our schedule allows it exam is tentatively scheduled for monday december participation of grade will be based on attendance and participation two free absences let me know and explain beyond that answer questions asked by instructor and others ask meaningful questions bring in relevant articles about recent developments in computer vision feedback is welcome late policy you get free late days i e you can submit homework a total of days late for example you can submit one problem set hours late and another hours late once you ve used up your free late days you will incur a penalty of from the total project credit possible for each late day a late day is anything from minute to hours collaboration policy you will work individually the work you turn in must be your own work you can discuss the problem sets with your classmates but do not look at their code you cannot use posted solutions search for code on the internet or use matlab implementations of something you are asked to write when in doubt ask the instructor plagiarism will cause you to fail the class and receive disciplinary penalty disabilities if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services drs william pitt union for asl users as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course medical conditions if you have a medical condition which will prevent you from doing a certain assignment or coming to class you must inform the instructor of this before the deadline you must then submit documentation of your condition within a week of the assignment deadline questions overview of topics features and filters transforming and describing images textures colors edges kristen grauman features and filters detecting repeatable features describing images with local statistics indexing and search matching features and regions across images grouping and fitting clustering segmentation fitting what parts belong together image formation how does light in world project to form images multiple views multi view geometry matching invariant features stereo vision hartley and zisserman visual recognition recognizing objects and categories learning techniques object detection detecting novel instances of objects classifying regions as one of several categories attribute based search describing the high level properties of objects searching for objects with relative attributes crowdsourcing annotations using non expert labelers to collect data actively requesting labels deep learning alexnet google inceptionism motion and tracking tracking objects video analysis low level motion optical flow kristen grauman tomas izo pose and actions automatically annotating a human pose recognizing actions in first person video next time matlab tutorial out prof adriana kovashka university of pittsburgh september plan for today course basics refresher matlab tutorial overview of homework start on image filtering course website instructor adriana kovashka kovashka cs pitt edu please use at the beginning of your subject office sennott square office hours tuesday and thursday grader nils murrugarra llerena nineil cs pitt edu by richard szeliski by kristen grauman and bastian leibe please do the assigned readings before class we will use courseweb navigate to the courseweb page for click on assignments and the corresponding hw attach a zip file with your written responses and code name the file as zip or tar homework is due at on the due grades will appear on courseweb you get free late days i e you can submit homework a total of days late for example you can submit one problem set hours late and another hours late once you ve used up your free late days you will incur a penalty of from the total project credit possible for each late day a late day is anything from minute to hours you will work individually the work you turn in must be your own work you can discuss the problem sets with your classmates but do not look at their code you cannot use posted solutions search for code on the internet or use matlab implementations of something you are asked to write when in doubt ask the instructor plagiarism will cause you to fail the class and receive disciplinary penalty part i matlab exercise today lecture part ii short answers lectures on and part iii implementing image pyramids lecture on part iv implementing seam carving today lecture matlab tutorial image formation digital camera a digital camera replaces film with a sensor array each cell in the array is light sensitive diode that converts photons to electrons digital images digital images sample the space on a regular grid quantize each sample round to nearest integer image thus represented as a matrix of integer values digital color images digital color images color images rgb color space r g b slide credit kristen grauman images in matlab images represented as a matrix suppose we have a nxm rgb image called im im top left pixel value in r channel im y x b y pixels down x pixels to right in the bth channel im n m bottom right pixel in b channel imread filename returns a image values to convert to double format values to with row column r slide credit derek hoiem homework seam carving content aware resizing traditional resizing intuition content aware resizing to reduce or increase size in one dimension remove irregularly shaped seams optimal solution via dynamic programming energy f want to remove seams where they won t be very noticeable measure energy as gradient magnitude choose seam based on minimum total energy path across image subject to connectedness energy f let a vertical seam consist of h positions that form an connected path let the cost of a seam be cost energy f i si optimal seam minimizes this cost min cost compute it efficiently with dynamic programming how to identify the minimum cost seam first consider a greedy approach kristen grauman ut austin energy matrix gradient magnitude seam carving algorithm compute the cumulative minimum energy for all possible connected seams at each entry i j m i j energy i j min m i j m i j m i j energy matrix gradient magnitude m matrix cumulative min energy for vertical seams then min value in last row of m indicates end of the minimal connected vertical seam backtrack up from there selecting min of above in m m i j energy i j min m i j m i j m i j m i j energy i min m i j m i j m i j original image energy map blue low energy red high energy other notes on seam carving analogous procedure for horizontal seams can also insert seams to increase size of image in either dimension duplicate optimal seam averaged with neighbors other energy functions may be plugged in e g color based interactive can use combination of vertical and horizontal seams example results from classes at ut austin results from eunho yang results from suyog jain original image conventional resize seam carving result original image conventional resize seam carving result original image by conventional resize by seam carving by results from jay hennig next time image filtering reading for tuesday szeliski sec reading for today was szeliski sec prof adriana kovashka university of pittsburgh september slides from kristen grauman plan for today filters cont d texture description texture synthesis sampling template matching filtering example image filtering compute a function of the local neighborhood at each pixel in the image function specified by a filter or mask saying how to combine values from neighbors uses of filtering enhance an image denoise resize etc extract information texture edges etc detect patterns template matching motivation noise reduction even multiple images of the same static scene will not be identical common types of noise salt and pepper noise random occurrences of black and white pixels impulse noise random occurrences of white pixels gaussian noise variations in intensity drawn from a gaussian normal distribution source s seitz gaussian noise noise randn size im sigma output im noise what is impact of the sigma fig m hebert even multiple images of the same static scene will not be identical how could we reduce the noise i e give an estimate of the true intensities what if there only one image let replace each pixel with an average of all the values in its neighborhood assumptions expect pixels to be like their neighbors expect noise processes to be independent from pixel to pixel let replace each pixel with an average of all the values in its neighborhood moving average in can add weights to our moving average weights non uniform weights say the averaging window size is x attribute uniform weight to each pixel loop over all pixels in neighborhood around image pixel f i j now generalize to allow different weights depending on neighboring pixel relative position non uniform weights this is called cross correlation denoted filtering an image replace each pixel with a linear combination of its neighbors the filter kernel or mask h u v is the prescription for the weights in the linear combination what values belong in the kernel h for the moving average example box filter depicts box filter white high value black low value original filtered what if the filter size was x instead of x what is the size of the output matlab output size shape options shape full output size is sum of sizes of f and g shape same output size is same as f shape valid output size is difference of sizes of f and g full same valid what about near the edge the filter window falls off the edge of the image need to extrapolate methods clip filter black wrap around copy edge reflect across edge what about near the edge the filter window falls off the edge of the image need to extrapolate methods matlab clip filter black imfilter f g wrap around imfilter f g circular copy edge imfilter f g replicate reflect across edge imfilter f g symmetric gaussian filter what if we want nearest neighboring pixels to have the most influence on the output removes high frequency components from the image low pass filter source s seitz smoothing with a gaussian gaussian filters what parameters matter here size of kernel or mask note gaussian function has infinite support but discrete filters use finite kernels σ with x kernel σ with x kernel practical matters how big should the filter be values at edges should be near zero important rule of thumb for gaussian set filter half width to about σ source derek hoiem gaussian filters what parameters matter here variance of gaussian determines extent of smoothing σ with x kernel σ with x kernel matlab hsize sigma h fspecial gaussian hsize sigma mesh h imagesc h outim imfilter im h correlation imshow outim outim smoothing with a gaussian parameter σ is the scale width spread of the gaussian kernel and controls the amount of smoothing for sigma h fspecial gaussian fsize sigma out imfilter im h imshow out pause end properties of smoothing filters smoothing values positive sum to constant regions same as input amount of smoothing proportional to mask size remove high frequency components low pass filter convolution convolution flip the filter in both dimensions bottom to top right to left then apply cross correlation notation for convolution operator convolution vs correlation convolution cross correlation for a gaussian or box filter how will the outputs differ predict the outputs using correlation filtering original original filtered no change original original shifted left by pixel with correlation original original blur with a box filter original original sharpening filter accentuates differences with local average filtering examples sharpening shift invariant operator behaves the same everywhere i e the value of the output depends on the pattern in the image neighborhood not the position of the neighborhood superposition h h h commutative f g g f associative f g h f g h distributes over addition f g h f g f h scalars factor out kf g f kg k f g identity unit impulse e f e f in some cases filter is separable and we can factor into two steps convolve all rows convolve all columns separability example f g h f g h filtering center location only the filter factors into a product of filters perform filtering along rows followed by filtering along the remaining column effect of smoothing filters additive gaussian noise salt and pepper noise no new pixel values introduced removes spikes good for impulse salt pepper noise non linear filter salt and pepper noise plots of a row of the image matlab output im im h w median filtered source m hebert median filter is edge preserving aude oliva antonio torralba philippe g schyns siggraph gaussian filter oliva a torralba p g schyns siggraph laplacian filter unit impulse gaussian laplacian of gaussian filters summary linear filters and convolution useful for enhancing images smoothing removing noise box filter gaussian filter impact of scale width of smoothing filter detecting features separable filters are more efficient median filter a non linear filter edge preserving plan for today filters cont d texture description texture synthesis sampling template matching texture what defines a texture shape from texture estimate surface orientation or shape from image texture use deformation of texture from point to point to estimate surface shape pics from a loh shape from texture estimate surface orientation or shape from image texture segmentation classification from texture cues analyze represent texture group image regions with consistent texture synthesis generate new texture patches images given some examples why analyze texture images bill freeman a efros why analyze texture importance to perception often indicative of a material properties can be important appearance cue especially if shape is similar across objects aim to distinguish between shape boundaries and texture technically representation wise we want a feature one step above building blocks of filters edges psychophysics of texture some textures distinguishable with preattentive perception without scrutiny eye movements julesz same or different lr lr ii t l ll i t t lr t ii t ltt lt lr lt ll l t l l l t lr l ll l lt lr lr lr lr lr lr lr lr r li lr lr ii t l ll i t t lr t ii t ltt lt lr lt ll l t l l l t lr l ll l lt lr lr lr lr lr lr lr lr r lr lr rl_j7 capturing the local patterns with image measurements bergen adelson nature scale of patterns influences discriminability size tuned linear filters texture representation textures are made up of repeated local patterns so find the patterns use filters that look like patterns spots bars raw patches consider magnitude of response describe their statistics within each local window mean standard deviation histogram histogram of prototypical feature occurrences kristen grauman original image original image original image original image dimension mean d dx value statistics to summarize patterns in small windows with primarily horizontal edges both windows with small gradient in both directions windows with primarily vertical edges statistics to summarize patterns in small original image kristen grauman derivative filter responses squared far dissimilar text close similar text dimension mean d dx value kristen grauman statistics to summarize patterns in small windows d a b dimension a b dimension distance reveals how dissimilar textu b from window a is from texture in window b texture representation window scale we re assuming we know the relevant window size for which we collect these statistics possible to perform scale selection by looking for window scale where texture description not changing our previous example used two filters and resulted in a dimensional feature vector to describe texture in a window x and y derivatives revealed something about local structure we can generalize to apply a collection of multiple d filters a filter bank then our feature vectors will be d dimensional still can think of nearness farness in feature space scales what filters to put in the bank typically we want a combination of scales and orientations different types of patterns matlab code available for these examples multivariate gaussian filter bank d d d d d ej i l l l j lj j image from 200 200 350 you try can you match the texture to the response filters a mean abs responses b c derek hoiem representing texture by mean abs response filters mean abs responses derek hoiem we can form a feature vector from the list of responses at each pixel d dimensional features d a b euclidean distance example uses of texture in vision analysis classifying materials stuff figure by varma zisserman texture features for image retrieval y rubner c tomasi and l j guibas the earth mover distance as a metric for image retrieval international journal of computer vision november characterizing scene categories by texture l w renninger and j malik when is scene identification just texture recognition vision research kristen grauman segmenting aerial imagery by textures kristen grauman cs intro to computer vision texture and other uses of filters prof adriana kovashka university of pittsburgh september slides from kristen grauman and derek hoiem plan for today texture cont d review of texture description texture synthesis uses of filters sampling template matching reading for today szeliski sec for next time szeliski sec pages get started now on reading for pages i will finalize the reading for each class by the day of the class preceding it readings finalized until inclusive f convolution u v h i j f convolution u v v h i j f convolution u v v v h i j f convolution u v v v u v h i j cross correlation u v f h i j cross correlation u v v f h i j cross correlation u v v v f h i j cross correlation f i j u v v v u v h no new pixel values introduced removes spikes good for impulse salt pepper noise non linear filter median filter is edge preserving salt and pepper noise plots of a row of the image matlab output im im h w median filtered source m hebert what defines a texture texture representation textures are made up of repeated local patterns so find the patterns use filters that look like patterns spots bars raw patches consider magnitude of response describe their statistics within each local window mean standard deviation histogram kristen grauman original image original image original image dimension mean d dx value statistics to summarize patterns in small windows with primarily horizontal edges both windows with small gradient in both directions windows with primarily vertical edges statistics to summarize patterns in small original image kristen grauman derivative filter responses squared our previous example used two filters and resulted in a dimensional feature vector to describe texture in a window x and y derivatives revealed something about local structure we can generalize to apply a collection of multiple filters a filter bank then our feature vectors will be d dimensional scales what filters to put in the bank typically we want a combination of scales and orientations different types of patterns matlab code available for these examples representing texture by mean abs response filters mean abs responses derek hoiem we can form a feature vector from the list of responses at each pixel kristen grauman texture related tasks shape from texture estimate surface orientation or shape from image texture segmentation classification from texture cues analyze represent texture group image regions with consistent texture texture synthesis goal create new samples of a given texture many applications virtual environments hole filling texturing surfaces the challenge need to model the whole spectrum from repeated to stochastic texture alexei a efros and thomas k leung texture synthesis by non parametric sampling proc international conference on computer vision iccv repeated stochastic both markov chain a sequence of random variables is the state of the model at time t a dog is a man best friend it a dog eat dog world out there a dog is man best friend it eat world out there a create plausible looking poetry love letters term papers etc most basic algorithm build probability histogram find all blocks of n consecutive words letters in training documents we need to eat cake results as i ve commented before really relating to someone involves standing next to impossible one morning i shot an elephant in my arms and kissed him i spent an interesting evening recently with a grain of salt dewdney a potpourri of programmed prose and prosody scientific american slide from alyosha efros iccv synthesizing computer vision text what do we get if we extract the probabilities from a chapter on linear filters and then synthesize new statements check out yisong yue website implementing text generation build your own text markov chain for a given text corpus synthesized text this means we cannot obtain a separate copy of the best studied regions in the sum all this activity will result in the primate visual system the response is also gaussian and hence isn t bandlimited instead we need to know only its response to any data vector we need to apply a low pass filter that strongly reduces the content of the fourier transform of a very large standard deviation it is clear how this integral exist it is sufficient for all pixels within a required for the images separately markov random field a markov random field mrf generalization of markov chains to two or more dimensions first order mrf probability that pixel x takes a certain value given the values of neighbors a b c and d source s seitz texture synthesis can apply version of text synthesis texture corpus sample output texture synthesis intuition before we inserted the next word based on existing nearby words now we want to insert pixel intensities based on existing nearby pixel values corpus place we want to insert next distribution of a value of a pixel is conditioned on its neighbors alone synthesizing one pixel p input image synthesized image what is find all the windows in the image that match the neighborhood to synthesize x pick one matching window at random assign x to be the center pixel of that window an exact neighbourhood match might not be present so find the best matches using ssd error and randomly choose between them preferring better matches with higher probability slide from alyosha efros iccv input increasing window size french canvas rafia weave white bread brick wall growing texture starting from the initial image grow the texture one pixel at a time hole filling extrapolation texture summary texture is a useful property that is often indicative of materials appearance cues texture representations attempt to summarize repeating patterns of local structure filter banks useful to measure redundant variety of structures in local neighborhood feature spaces can be multi dimensional neighborhood statistics can be exploited to sample or synthesize new texture regions example based technique kristen grauman plan for today texture cont d review of texture description texture synthesis uses of filters sampling template matching sampling why does a lower resolution image still make sense to us what do we lose image throw away every other row and column to create a size image example sinewave example sinewave sub sampling may be dangerous characteristic errors may appear wagon wheels rolling the wrong way in movies checkerboards disintegrate in ray tracing striped shirts look funny on color television sampling and aliasing when sampling a signal at discrete intervals the sampling frequency must be fmax fmax max frequency of the input signal this will allows to reconstruct the original perfectly from the sampled version good bad solutions sample more often get rid of all frequencies that are greater than half the new sampling frequency will lose information but it better than aliasing apply a smoothing filter algorithm for downsampling by factor of start with image h w apply low pass filter imfilter image fspecial gaussian sample every other pixel end end anti aliasing forsyth and ponce subsampling without pre filtering zoom zoom subsampling with gaussian pre filtering gaussian g g texture cont d review of texture description texture synthesis uses of filters sampling template matching goal find in image main challenge what is a good similarity or distance measure between two patches correlation zero mean correlation sum square difference normalized cross correlation goal find in image method filter the image with eye patch h m n g k l k l f m k n l f image g filter what went wrong goal find in image method filter the image with zero mean eye h m n g k l g f m k n l k l mean of template g goal find in image method ssd h m n g k l k l f m k n l goal find in image method ssd what the potential downside of ssd h m n g k l k l f m k n l goal find in image method normalized cross correlation mean template mean image patch h m n g k l k l g f m k n l fm n g k l g f m k n l m n k l k l matlab template im goal find in image method normalized cross correlation goal find in image method normalized cross correlation a depends zero mean filter fastest but not a great matcher ssd next fastest sensitive to overall intensity normalized cross correlation slowest invariant to local average intensity and contrast q what if we want to find larger or smaller eyes a image pyramid sampling gaussian filter sample source forsyth input image template match template at current scale downsample image in practice scale step of to repeat until image is very small take responses above some threshold laplacian filter unit impulse gaussian laplacian of gaussian can we reconstruct the original from the laplacian pyramid image smooth then downsample downsample smooth downsample smooth g gn ln smooth upsample smooth upsample smooth upsample use same filter for smoothing in each step e g gaussian with 𝜎 downsample upsample with nearest interpolation application hybrid images aude oliva antonio torralba philippe g schyns siggraph application hybrid images gaussian filter a oliva a torralba p g schyns siggraph laplacian filter unit impulse gaussian laplacian of gaussian slide credit kristen grauman uses of filters summary texture description texture synthesis image compression image pyramids template matching uses in object recognition detecting stable interest points scale search next time edge detection prof adriana kovashka university of pittsburgh september edge detection binary image analysis due on review slides from regarding how to do part iv there are different ways to compute image gradients office hours for will be at on edge detection goal map image from array of pixels to a set of curves or line segments or contours why figure from j shotton et al pami main idea look for strong gradients post process origin of edges surface normal discontinuity depth discontinuity surface color discontinuity illumination discontinuity edges are caused by a variety of factors what causes an edge reflectance change appearance information texture depth discontinuity object boundary change in surface orientation shape cast shadows edges gradients and invariance an edge is a place of rapid change in the image intensity function image intensity function along horizontal scanline first derivative edges correspond to extrema of derivative intensity profile gradient consider a single row or column of the image plotting intensity as a function of position gives a signal where is the edge difference filters respond strongly to noise image noise results in pixels that look very different from their neighbors generally the larger the noise the stronger the response what can we do about it solution smooth first f g f g d f dx g to find edges look for peaks in d f dx g differentiation is convolution and convolution is associative d f dx g f d g dx this saves us one operation f d g dx f d g dx is this filter separable tradeoff between smoothing and localization pixel pixels pixels smoothed derivative removes noise but blurs edge also finds edges at different scales source d forsyth designing an edge detector criteria for a good edge detector good detection find all real edges ignoring noise or other artifacts good localization detect edges as close as possible to the true edges return one point only for each true edge point cues of edge detection differences in color intensity or texture across the boundary continuity and closure high level knowledge source l fei fei gradients edges primary edge detection steps smoothing suppress noise edge enhancement filter for contrast edge localization determine which local maxima from filter output are actually edges vs noise threshold thin thresholding choose a threshold value t set any pixels less than t to zero off set any pixels greater than or equal to t to one on original image source k grauman gradient magnitude image source k grauman source k grauman source k grauman canny edge detector this is probably the most widely used edge detector in computer vision theoretical model step edges corrupted by additive gaussian noise canny has shown that the first derivative of the gaussian closely approximates the operator that optimizes the product of signal to noise ratio and localization canny ieee trans pattern analysis and machine intelligence source l fei fei canny edge detector filter image with derivative of gaussian find magnitude and orientation of gradient non maximum suppression thin wide ridges down to single pixel width linking and thresholding hysteresis define two thresholds low and high use the high threshold to start edge curves and the low threshold to continue them matlab edge image canny help edge source d lowe l fei fei input image lena derivative of gaussian y derivative of gaussian gradient magnitude threshold at minimum level get orientation theta gy gx norm of the gradient thresholding how to turn these thick regions of the gradient into curves check if pixel is local maximum along gradient direction select single max across width of the edge requires checking interpolated pixels p and r bilinear interpolation the canny edge detector problem pixels along this edge didn t survive the thresholding thinning non maximum suppression use a high threshold to start edge curves and a low threshold to continue them original image high threshold strong edges low threshold weak edges hysteresis threshold high threshold strong edges low threshold weak edges hysteresis threshold filter image with derivative of gaussian find magnitude and orientation of gradient non maximum suppression thin wide ridges down to single pixel width linking and thresholding hysteresis define two thresholds low and high use the high threshold to start edge curves and the low threshold to continue them matlab edge image canny help edge effect of gaussian kernel spread size original canny with canny with the choice of depends on desired behavior large detects large scale edges small detects fine features source s seitz low level edges vs perceived contours background texture shadows low level edges vs perceived contours image human segmentation gradient magnitude berkeley segmentation database pb boundary detector figure from fowlkes learn from humans which combination of features is most indicative of a good contour d martin et al pami human marked segment boundaries pb boundary detector figure from fowlkes brightness color texture combined human for more plan for today edge detection binary image analysis binary images binary image analysis basic steps convert the image into binary form thresholding clean up the thresholded image morphological operators extract separate blobs connected components describe the blobs with region properties two pixel values foreground and background mark region of interest grayscale binary mask useful if object of interest intensity distribution is distinct from background simplebinary html given a grayscale image or an intermediate matrix threshold to create a binary output example edge detection gradient magnitude find t looking for pixels where gradient is strong given a grayscale image or an intermediate matrix threshold to create a binary output example background subtraction looking for pixels that differ significantly from the empty background source k grauman find diff t given a grayscale image or an intermediate matrix threshold to create a binary output example intensity based detection find im looking for dark pixels given a grayscale image or an intermediate matrix threshold to create a binary output example color based detection find hue hue looking for pixels within a certain hue range a nice case bimodal intensity histograms ideal histogram light object on dark background actual observed histogram with noise issues what to do with noisy binary outputs holes extra small fragments how to demarcate multiple regions of interest count objects compute further features per object morphological operators change the shape of the foreground regions via intersection union operations between a scanning structuring element and binary image useful to clean up result from thresholding basic operators are dilation erosion expands connected components grow features fill holes before dilation after dilation erode connected components shrink features remove bridges branches noise before erosion after erosion masks of varying shapes and sizes used to perform morphology for example scan mask across foreground pixels to transform the binary image help strel at each position dilation if current pixel is foreground or the structuring element with the input image input image structuring ele g x f x se output image input image structuring element output image input image structuring element output image input image structuring element output image input image structuring element output image input image structuring element output image input image structuring element output image input image structuring element output image input image structuring element output image note that the object gets bigger and holes are filled help imdilate source shapiro and stockman at each position dilation if current pixel is foreground or the structuring element with the input image erosion if every pixel under the structuring element nonzero entries is foreground or the current pixel with s input image structuring element g x f x output image input image structuring element g x f x output image input image structuring element output image input image structuring element output image input image structuring element output image input image structuring element output image input image structuring element output image input image structuring element output image input image structuring element output image input image structuring element output image note that the object gets smaller help imerode erode then dilate remove small objects keep original shape before opening after opening closing dilate then erode fill holes but keep original shape before closing after closing applet can use filters to process and describe local neighborhood derivatives to locate gradients convolution properties will influence efficiency edge detection processes the image gradient to find curves clean up thresholding outputs with binary image morphology operations computing image features detecting and describing keypoints cs intro to computer vision local image features extraction and description prof adriana kovashka university of pittsburgh september what cs welcome party when friday september at where sensq why to learn about the cs department the cs major and other interesting useful things why to meet other students and faculty why to have some pizza feature extraction keypoint detection feature description homework due on tuesday office hours today moved to tomorrow an image is a set of pixels adapted from s narasimhan problems with pixel representation not invariant to small changes translation illumination etc some parts of an image are more important than others what do we want to represent note interest points keypoints also sometimes called features many applications tracking which points are good to track recognition find patches likely to tell us something about object category reconstruction find correspondences across different views yarbus eye tracking suppose you have to click on some point go away and come back after i deform the image and click on the same points again which points would you choose original if you wanted to meet a friend would you say let meet on campus let meet on green street let meet at green and wright corner detection or if you were in a secluded area let meet in the plains of akbar let meet on the side of mt doom let meet on top of mt doom blob valley peak detection where would you tell your friend to meet you where would you tell your friend to meet you what points would you choose overview of keypoint matching d f a fb t grauman b leibe find a set of distinctive key points define a region around each keypoint extract and normalize the region content compute a local descriptor from the normalized region match local descriptors goals for keypoints detect points that are repeatable and distinctive key trade offs detection more repeatable more points precise localization robust to occlusion description more distinctive more flexible minimize wrong matches robust to expected variations maximize correct matches motivation panorama stitching we have two images how do we combine them motivation panorama stitching we have two images how do we combine them step extract features step match features motivation panorama stitching we have two images how do we combine them step extract features step match features step align images goal interest operator repeatability we want to detect at least some of the same points in both images no chance to find true matches yet we have to be able to run the detection procedure independently per image goal descriptor distinctiveness we want to be able to reliably determine which point goes with which must provide some invariance to geometric and photometric differences between the two views corners as distinctive interest points we should easily recognize the point by looking through a small window shifting a window in any direction should give a large change in intensity corners as distinctive interest points we should easily recognize the point by looking through a small window shifting a window in any direction should give a large change in intensity corners as distinctive interest points we should easily recognize the point by looking through a small window shifting a window in any direction should give a large change in intensity corners as distinctive interest points we should easily recognize the point by looking through a small window shifting a window in any direction should give a large change in intensity corners as distinctive interest points we should easily recognize the point by looking through a small window shifting a window in any direction should give a large change in intensity window averaged squared change of intensity induced by shifting the image data by u v window function w x y or in window outside gaussian window averaged squared change of intensity induced by shifting the image data by u v e u v expanding i x y in a taylor series expansion we have for small shifts u v a quadratic approximation to the error surface between a patch and itself shifted by u v where m is a matrix computed from image derivatives m w x y i x i x i x i y x y i y i y notation i i x x i i y y ix i y i i x y first consider an axis aligned corner first consider an axis aligned corner i i i m i i x y i this means dominant gradient directions align with x or y axis look for locations where both λ are large if either λ is close to then this is not corner like what if we have a corner that is not aligned with the image axes what does this matrix reveal t since m is symmetric we have m x x mxi i xi the eigenvalues of m reveal the amount of intensity change in the two principal orthogonal gradient directions in the window corner response function edge corner and are large flat region and are small harris detector mathematics measure of corner response k empirical constant k harris detector summary compute image gradients ix and iy for all pixels for each pixel compute by looping over neighbors x y compute k empirical constant k find points with large corner response function r r threshold take the points of locally maximum r as the detected feature points i e pixels where r is bigger than for all the or neighbors compute corner response at every pixel harris detector responses effect a very precise corner detector harris detector responses i a i b only derivatives are used invariance to intensity shift i i b intensity scaling i a i r r threshold x image coordinate x image coordinate derivatives and window function are shift invariant second moment ellipse rotates but its shape i e eigenvalues remains the same invariant to image scale image zoomed image corner all points will be classified as edges lazebnik scale invariant detection the problem how do we choose corresponding circles independently in each image do objects in the image have a characteristic scale that we can identify frolova d simakov f im x f im x how to find corresponding patch sizes function responses for increasing scale scale signature f im x f im x function responses for increasing scale scale signature f im x f im x function responses for increasing scale scale signature f im x f im x function responses for increasing scale scale signature f im x f im x function responses for increasing scale scale signature f im x f im x function responses for increasing scale scale signature f im x f im x f d g dx f d g dx edge derivative of gaussian edge max of derivative f d g d f g dx edge second derivative of gaussian edge zero crossing of derivative edge ripple blob superposition of two ripples maximum spatial selection the magnitude of the laplacian response will achieve a maximum at the center of the blob provided the scale of the laplacian is matched to the scale of the blob l lazebnik laplacian of gaussian circularly symmetric operator for blob detection in g g g grauman what is a useful signature function laplacian of gaussian blob detector k grauman b leibe we can approximate the laplacian with a difference of gaussians more efficient to implement laplacian dog g x y k g x y difference of gaussians computation in gaussian scale pyramid sampling with step original image find local maxima in position scale space of difference of gaussian lxx lyy list of x y local features desired properties repeatability the same feature can be found in several images despite geometric and photometric transformations saliency each feature has a distinctive description compactness and efficiency many fewer features than image pixels locality a feature occupies a relatively small area of the image robust to clutter and occlusion plan for today feature extraction keypoint detection corners blobs feature description e g scale translation rotation tuytelaars the ideal descriptor should be robust distinctive compact efficient most available descriptors focus on edge gradient information capture texture information color rarely used lowe iccv histogram of oriented gradients captures important texture information robust to small translations affine deformations run difference of gaussian keypoint detector find maxima in location scale space find all major orientations bin orientations weight by gradient magnitude weight by distance to center gaussian weighted mean return orientations within of peak for each x y scale orientation create descriptor sample gradient magnitude and relative orientation bin samples into histograms threshold values to max of divide by norm final descriptor normalized histograms lowe ijcv scale invariant feature transform basic idea take square window around detected feature compute gradient orientation for each pixel create histogram over edge orientations weighted by magnitude full version divide the window into a grid of cells case shown below compute an orientation histogram for each cell cells orientations dimensional descriptor full version divide the window into a grid of cells case shown below compute an orientation histogram for each cell cells orientations dimensional descriptor threshold normalize the descriptor such that making descriptor rotation invariant rotate patch according to its dominant gradient orientation this puts the patches into a canonical orientation grauman image from matthew brown sift descriptor lowe extraordinarily robust matching technique can handle changes in viewpoint up to about degree out of plane rotation can handle significant changes in illumination sometimes even day vs night below fast and efficient can run in real time lots of code available s seitz matching local features image image to generate candidate matches find patches that have the most similar appearance e g lowest ssd simplest approach compare them all take the closest or closest k or within a thresholded distance grauman ambiguous matches image image at what ssd value do we have a good match to add robustness to matching can consider ratio distance to best match distance to second best match if low first match looks good k grauman if high could be ambiguous match nearest neighbor euclidean distance threshold ratio of nearest to nearest descriptor extract features extract features compute putative matches extract features compute putative matches loop hypothesize transformation t small group of putative matches that are related by t extract features compute putative matches loop hypothesize transformation t small group of putative matches that are related by t verify transformation search for other matches consistent with t extract features compute putative matches loop hypothesize transformation t small group of putative matches that are related by t verify transformation search for other matches consistent with t applications of local invariant features image alignment indexing and retrieval recognition robot navigation reconstruction wide baseline stereo panoramas motion tracking recognition of specific objects scenes schmid and mohr sivic and zisserman lowe grauma image from t tuytelaars eccv tutorial summary keypoint detection repeatable and distinctive corners blobs stable regions laplacian of gaussian automatic scale selection descriptors robust and selective histograms for robustness to small shifts and translations sift descriptor prof adriana kovashka university of pittsburgh september today review sift features physics and perception of color color matching color spaces uses of color in computer vision announcement homework released small changes made homework due tonight at review late policy reminder do not look for or use existing implementations harris detector summary compute image gradients ix and iy for all pixels for each pixel compute by looping over neighbors x y compute k empirical constant k find points with large corner response function r r threshold take the points of locally maximum r as the detected feature points i e pixels where r is bigger than for all the or neighbors d frolova d simakov example of harris application grauman lowe iccv histogram of oriented gradients captures important texture information robust to small translations affine deformations k grauman b leibe tan α 𝑜𝑝𝑝𝑜𝑠𝑖𝑡𝑒 𝑠𝑖𝑑𝑒 𝑎𝑑𝑗𝑎𝑐𝑒𝑛𝑡 𝑠𝑖𝑑𝑒 m x y sqrt θ x y atan m x y sqrt θ x y atan m x y sqrt θ x y atan scale invariant feature transform basic idea take square window around detected feature compute gradient orientation for each pixel create histogram over edge orientations weighted by magnitude full version divide the window into a grid of cells case shown below compute an orientation histogram for each cell cells orientations dimensional descriptor full version divide the window into a grid of cells case shown below compute an orientation histogram for each cell cells orientations dimensional descriptor threshold normalize the descriptor such that making descriptor rotation invariant rotate patch according to its dominant gradient orientation this puts the patches into a canonical orientation grauman image from matthew brown today review sift features physics and perception of color color matching color spaces uses of color in computer vision color and light color of light arriving at camera depends on spectral reflectance of the surface light is leaving spectral radiance of light falling on that patch color perceived depends on physics of light visual system receptors brain processing environment grauman the human eye is a camera lens changes shape by using ciliary muscles to focus on objects at different distances pupil the hole aperture whose size is controlled by the iris iris colored annulus with radial muscles retina photoreceptor cells slide by steve seitz light d hoiem color sensing in cameras bayer grid estimate rgb at each cell from neighboring values slide by steve seitz two types of light sensitive receptors cones cone shaped less sensitive operate in high light color vision rods rod shaped highly sensitive operate at night gray scale vision slower to respond rod cone sensitivity distribution of rods and cones rods responsible for intensity cones responsible for color fovea small region or at the center of the visual field containing the highest density of cones and no rods less visual acuity in the periphery night sky why are there more stars off center adapted from a efros k grauman s seitz p duygulu electromagnetic spectrum human luminance sensitivity function grauman image credit nasa gov the physics of light any patch of light can be completely described physically by its spectrum the number of photons per time unit at each wavelength nm photons per ms wavelength nm stephen e palmer the physics of light some examples of the spectra of light sources ruby laser b gallium phosphide crystal wavelength nm wavelength nm tungsten lightbulb normal daylight stephen e palmer the physics of light some examples of the reflectance spectra of surfaces wavelength nm stephen e palmer physiology of color vision three kinds of cones s nm m l wavelength nm why are m and l cones so close stephen e palmer is better than m and l on the x chromosome why men are more likely to be color blind see what it like l has high variation so some women are tetrachromatic some animals have night animals e g dogs fish birds pigeons some reptiles amphibians or even mantis shrimp d hoiem human photoreceptors possible evolutionary pressure for developing receptors for different wavelengths in primates osorio vorobyev k grauman measuring spectra spectroradiometer separate input light into its different wavelengths and measure the energy at each k grauman foundations of vision b wandell metamers spectral reflectances for some natural objects how much of each wavelength is reflected for that surface grauman forsyth ponce measurements by e koivisto we don t perceive a spectrum or even rgb we perceive hue mean wavelength color saturation variance vividness intensity total amount of light same perceived color can be recreated with combinations of three primary colors trichromacy d hoiem color mixing cartoon spectra for color names additive color mixing colors combine by adding color spectra light adds to black examples of additive color systems crt phosphors multiple projectors grauman subtractive color mixing colors combine by multiplying color spectra pigments remove color from incident light white source w freeman examples of subtractive color systems printing on paper crayons most photographic film grauman fun with color brightness perception edward adelson edward adelson edward adelson color constancy interpret surface in terms of true color rather than observed intensity humans are good at it computers are not nearly as good look at blue squares content r beau lotto look at yellow squares content r beau lotto content r beau lotto content r beau lotto content r beau lotto content r beau lotto name that color high level interactions affect perception and processing reasons for illusions chromatic adaptation we adapt to a particular illuminant assimilation contrast effects chromatic induction nearby colors affect what is perceived receptor excitations interact across image and time afterimages tired receptors produce negative response color matching color appearance physics of light perception of light chromatic adaptation the visual system changes its sensitivity depending on the luminances prevailing in the visual field the exact mechanism is poorly understood adapting to different brightness levels changing the size of the iris opening i e the aperture changes the amount of light that can enter the eye think of walking into a building from full sunshine adapting to different color temperature the receptive cells on the retina change their sensitivity for example if there is an increased amount of red light the cells receptive to red decrease their sensitivity until the scene looks white again we actually adapt better in brighter scenes this is why candlelit scenes still look yellow today review sift features physics and perception of color color matching color spaces uses of color in computer vision goal find out what spectral radiances produce same response in human observers observer adjusts weight intensity for primary lights fixed spd to match appearance of test light foundations of vision by brian wandell sinauer assoc after judd wyszecki goal find out what spectral radiances produce same response in human observers assumption simple viewing conditions where we say test light alone affects perception ignoring additional factors for now like adaptation complex surrounding scenes etc k grauman slide credit freeman the primary color amounts needed for a match we say a negative amount of was needed to make the match because we added it to the test color side the primary color amounts needed for a match what must we require of the primary lights chosen how are three numbers enough to represent entire spectrum trichromacy in color matching experiments most people can match any given light with three primaries primaries must be independent for the same light and same primaries most people select the same weights exception color blindness trichromatic color theory three numbers seem to be sufficient for encoding color dates back to century thomas young grassman laws if two test lights can be matched with the same set of weights then they match each other suppose a and b then a b if we scale the test light then the matches get scaled by the same amount suppose a then ka if we mix two test lights then mixing the matches will match the result superposition suppose a and b then a b p2 here means matches how do we compute the weights that will yield a perceptual match for any test light using a given set of primaries select primaries estimate their color matching functions observer matches series of monochromatic lights one at each wavelength c c color matching functions for a particular set of primaries nm nm nm rows of matrix c foundations of vision by brian wandell sinauer assoc slide credit w freeman i matches i i i now have matching functions for all monochromatic light sources so we know how to match a unit of each wavelength arbitrary new spectral signal is a linear combination of the monochromatic sources t t k grauman t so given any set of primaries and their associated matching functions c we can compute weights e needed on each primary to give a perceptual match to any test light t spectral signal grauman fig from b wandell why is computing the color match for any color signal for a given set of primaries useful want to paint a carton of kodak film with the kodak yellow color want to match skin color of a person in a photograph printed on an ink jet printer to their true skin color want the colors in the world on a monitor and in a print format to all look the same adapted from w freeman image credit pbs org today review sift features physics and perception of color color matching color spaces uses of color in computer vision why specify color numerically accurate color reproduction is commercially valuable many products are identified by color golden arches few color names are widely recognized by english speakers black blue brown grey green orange pink purple red white and yellow other languages have fewer more common to disagree on appropriate color names color reproduction problems increased by prevalence of digital imaging e g digital libraries of art how to ensure that everyone perceives the same color forsyth ponce standard color spaces use a common set of primaries color matching functions linear color space examples rgb cie xyz non linear color space hsv linear color spaces defined by a choice of three primaries the coordinates of a color are given by the weights of the primaries used to match it mixing two lights produces colors that lie along a straight line in color space mixing three lights produces colors that lie within the triangle they define in color space default color space some drawbacks strongly correlated channels non perceptual hoiem r g b g r b b r g image from http en wikipedia org wiki file png perceptual equivalents with rgb perceptual equivalents with cie xyz rgb portion is in triangle are distances between points in a color space perceptually meaningful not necessarily cie xyz is not a uniform color space so magnitude of differences in coordinates are poor indicator of color distance mcadam ellipses just noticeable differences in color attempt to correct this limitation by remapping color space so that just noticeable differences are contained by circles distances more perceptually meaningful examples cie u v cie lab cie xyz cie u v color spaces cie l a b perceptually uniform color space luminance brightness chrominance color l a b a l b b l a intuitive color space h s v s h v v h s hsv color space hue saturation value brightness nonlinear reflects topology of colors by coding hue as an angle matlab grauman image from mathworks com today review sift features physics and perception of color color matching color spaces uses of color in computer vision color as a low level cue for cbir r g b color intensity color histograms use distribution of colors to describe image no spatial info invariant to translation rotation scale k grauman color as a low level cue for cbir r g b given two histogram vectors sum the minimum counts per bin n i x y min i xi yi grauman given collection database of images extract and store one color histogram per image given new query image extract its color histogram for each database image compute intersection between query histogram and database histogram sort intersection values highest score most similar rank database items relative to query based on this sorted order example database grauman m jones and j rehg statistical color models with application to skin detection ijcv color based segmentation for robot soccer towards eliminating manual color calibration at robocup mohan sridharan and peter stone robocup robot soccer world cup ix springer verlag http www cs utexas edu users austinvilla p research color perception differs from the physics of color various color spaces exist with different strengths and weaknesses color has limited application in computer vision segmentation clustering figure by j shi prof adriana kovashka university of pittsburgh september goals grouping in vision gather features that belong together obtain an intermediate representation that compactly describes key image parts examples of grouping in vision figure by j shi determine image regions jpg group video frames into shots figure by wang suter figure ground figure by grauman darrell object level grouping image human segmentation group together similar looking pixels for efficiency of further processing superpixels x ren and j malik iccv oversegmentation undersegmentation multiple segmentations bottom up group tokens with similar features top down group tokens that likely belong to the same object source d hoiem levin and weiss goals grouping in vision gather features that belong together obtain an intermediate representation that compactly describes key image video parts top down vs bottom up segmentation top down pixels belong together because they are from the same object bottom up pixels belong together because they look similar hard to measure success what is interesting depends on the app bottom up segmentation via clustering features color texture quantization for texture summaries algorithms mode finding and mean shift k means mean shift graph based normalized cuts other gestalt whole is greater than sum of its parts relationships among parts can yield new properties features psychologists identified series of factors that predispose set of elements to be grouped by human visual system gestaltism the muller lyer illusion we perceive the interpretation not the senses source d hoiem from steve lehar the constructive aspect of visual perception gestaltists do not believe in coincidence grouping by invisible completion source d hoiem from steve lehar the constructive aspect of visual perception continuity explanation by occlusion figure ground in vision d marr from j l marroquin human visual perception of structure gestalt cues good intuition and basic principles for grouping basis for many ideas in segmentation and occlusion reasoning some e g symmetry are difficult to implement in practice source d hoiem similarity common fate image credit arthus bertrand via f durand source k grauman proximity today inspiration from human perception gestalt properties bottom up segmentation via clustering features color texture quantization for texture summaries algorithms mode finding and mean shift k means mean shift graph based normalized cuts other image segmentation toy example black pixels gray pixels white pixels input image intensity these intensities define the three groups we could label every pixel in the image according to which of these primary intensities it is i e segment the image based on the intensity feature what if the image isn t quite so simple input image input image intensity intensity input image intensity now how to determine the three main intensities that define our groups we need to cluster intensity goal choose three centers as the representative intensities and label every pixel according to which of these centers it is nearest to best cluster centers are those that minimize ssd between all points and their nearest cluster center ci clustering with this objective it is a chicken and egg problem if we knew the cluster centers we could allocate points to groups by assigning each to its closest center if we knew the group memberships we could get the centers by computing the mean per group k means clustering basic idea randomly initialize the k cluster centers and iterate between the two steps we just saw randomly initialize the cluster centers ck given cluster centers determine points in each cluster for each point p find the closest ci put p into cluster i given points in each cluster solve for ci set ci to be the mean of points in cluster i if ci have changed repeat step properties will always converge to some solution can be a local minimum does not always find the global minimum of objective function source steve seitz k means ask user how many cllusters they d nke e g k k means l ask user how clusters they d lliike e g k guess k clluster center locations k means ask user how many clusters they d like e g k s guess k cluster ce nt e r locat ions each dlatapoint finds out wh ich center it closest to thus each cente r a set of datapo ints l k means ask user how many clusters they d like e g k guess k cluster center locations each datapoint finds out which cent er it closest to each center finds the centroid of the points it owns o l k means clustering matlab demo java demos today inspiration from human perception gestalt properties bottom up segmentation via clustering features color texture quantization for texture summaries algorithms graph based normalized cuts other k means pros and cons pros simple fast to compute converges to local minimum of within cluster squared error cons issues setting k sensitive to initial centers sensitive to outliers detects spherical clusters assuming means can be computed an aside smoothing out cluster assignments assigning a cluster label per pixel may yield outliers original labeled by cluster center intensity how to ensure they are segmentation as clustering depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on intensity similarity feature space intensity value d k k quantization of the feature space segmentation label map depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on color similarity r g b r g b r g r b r g b feature space color value d source k grauman depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on intensity similarity clusters based on intensity similarity don t have to be spatially coherent source k grauman depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on intensity position similarity source k grauman x both regions are black but if we also include position x y then we could group the two into distinct segments way to encode both similarity proximity segmentation as clustering color brightness position alone are not enough to distinguish all regions source l lazebnik segmentation as clustering depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on texture similarity filter bank of filters feature space filter bank responses e g d source k grauman recall texture representation windows with primarily horizontal edges both source k grauman windows with small gradient in both directions windows with primarily vertical edges statistics to summarize patterns in small windows find textons by clustering vectors of filter bank outputs describe texture in a window based on texton histogram image texton map texton index texton index malik belongie leung and shi ijcv source l lazebnik pixel properties vs neighborhood properties these look very similar in terms of their color distributions histograms how would their texture distributions compare for an image of a single texture we can classify it according to its global image wide texton histogram figure from varma zisserman ijcv source k grauman nearest neighbor classification label the input according to the nearest known example label board source k grauman manik varma k means pros and cons pros simple fast to compute converges to local minimum of within cluster squared error cons issues setting k sensitive to initial centers sensitive to outliers detects spherical clusters assuming means can be computed the mean shift algorithm seeks modes or local maxima of density in the feature space image feature space l u v color values estimated density search window center of mass mean shift vector source d hoiem cluster all data points in the attraction basin of a mode attraction basin the region for which all trajectories lead to the same mode slide by y ukrainitz b sarel compute features for each point color texture etc initialize windows at individual feature points perform mean shift for each window until convergence merge windows that end up near the same peak or mode source d hoiem pros mean shift does not assume shape on clusters one parameter choice window size generic technique find multiple modes robust to outliers cons selection of window size does not scale well with dimension of feature space mean shift reading nicely written mean shift explanation with math includes m code for mean shift clustering mean shift paper by comaniciu and meer adaptive mean shift in higher dimensions today inspiration from human perception gestalt properties bottom up segmentation via clustering features color texture quantization for texture summaries algorithms mode finding and mean shift k means mean shift other fully connected graph node vertex for every pixel link between every pair of pixels p q affinity weight wpq for each link edge wpq measures similarity similarity is inversely proportional to difference in color and position a b c break graph into segments want to delete links that cross between segments easiest to break links that have low similarity low weight similar pixels should be in the same segments dissimilar pixels should be in different segments b link cut set of links whose removal makes a graph disconnected cost of a cut find minimum cut cut a b w p a q b p q gives you a segmentation fast algorithms exist for doing this minimum cut problem with minimum cut weight of cut proportional to number of edges in the cut tends to produce small isolated components shi malik pami source k grauman b normalized cut fix bias of min cut by normalizing for size of segments cut a b assoc a v cut a b assoc b v assoc a v sum of weights of all edges that touch a ncut value small when we get two clusters with many edges with high weights and few edges of low weight between them approximate solution for minimizing the ncut value eigenvalue problem shi and j malik cvpr source steve seitz normalized cuts pros and cons pros generic framework flexible to choice of function that computes weights affinities between nodes does not require model of the data distribution cons time complexity can be high dense highly connected graphs many affinity computations solving eigenvalue problem preference for balanced partitions today inspiration from human perception gestalt properties bottom up segmentation via clustering features color texture quantization for texture summaries algorithms mode finding and mean shift k means mean shift graph based normalized cuts e borenstein and s ullman eccv levin and y weiss eccv slide credit lana lazebnik define a labeling l as an assignment of each pixel with a label background or foreground find the labeling l that minimizes data term smoothness term how similar is each labeled pixel to the foreground or background encourage spatially coherent segments slide credit n snavely image gradient watershed boundaries image parsing or semantic segmentation j tighe and s lazebnik eccv ijcv segmentation to find object boundaries or mid level regions bottom up segmentation via clustering general choices features affinity functions and clustering algorithms grouping also useful for quantization can create new feature summaries texton histograms for texture within local region example clustering methods k means mean shift graph cut normalized cuts finding correspondences between features line and model fitting prof adriana kovashka university of pittsburgh september today fitting models lines to points i e find the parameters of a model that best fits the data least squares hough transform ransac matching finding correspondences between points i e find the parameters of the transformation that best aligns points homework is due fitting want to associate a model with observed features fig from marszalek schmid for example the model could be a line a circle or an arbitrary shape example line fitting why fit lines many objects characterized by presence of straight lines why aren t we done just by running edge detection difficulty of line fitting extra edge points clutter multiple models which points go with which line if any only some parts of each line detected and some parts are missing how to find a line that bridges missing evidence noise in measured edge points orientations how to detect true underlying parameters least squares line fitting data xn yn line equation yi m xi b find m b to minimize xi yi y mx b m m e n x y ap y i i b i b x y n n hypothesize and test propose parameters try all possible each point votes for all consistent parameters repeatedly sample enough points to solve for parameters score the given parameters number of consistent points possibly weighted by distance choose from among the set of parameters global or local maximum of scores possibly refine parameters using inliers voting it not feasible to check all combinations of features by fitting a model to each possible subset voting is a general technique where we let the features vote for all models that are compatible with it cycle through features cast votes for model parameters look for model parameters that receive a lot of votes noise clutter features they will cast votes too but typically their votes should be inconsistent with the majority of good features fitting lines hough transform given points that belong to a line what is the line how many lines are there which points belong to which lines hough transform is a voting technique that can be used to answer all of these questions main idea record vote for each possible line on which each edge point lies look for lines that get many votes y b x m image space hough parameter space connection between image x y and hough m b spaces a line in the image corresponds to a point in hough space y b x m image space hough parameter space connection between image x y and hough m b spaces a line in the image corresponds to a point in hough space what does a point in the image space map to answer the solutions of b this is a line in hough space to go from image space to hough space given a set of points x y find all m b such that y mx b y b x m image space hough parameter space what are the line parameters for the line that contains both and it is the intersection of the lines b and b y b x m image space hough parameter space how can we use this to find the most likely parameters m b for the most prominent line in the image space let each edge point in image space vote for a set of possible parameters in hough space accumulate votes in discrete set of bins parameters with the most votes indicate line in image space hough transform y m x b y m x b problems with the m b space unbounded parameter domains vertical lines require infinite m problems with the m b space unbounded parameter domains vertical lines require infinite m alternative polar representation y sin each point x y will add a sinusoid in the parameter space hough transform hough machine analysis of bubble chamber pictures proc int conf high energy accelerators and instrumentation use a polar representation for the parameter space y x hough space x cos ysin algorithm outline initialize accumulator h to all zeros for each feature point x y in the image for θ to ρ x cos θ y sin θ h θ ρ h θ ρ end end find the value of θ ρ where h θ ρ is a local maximum the detected line in the image is given by ρ x cos θ y sin θ hough transform example derek hoiem x image space edge coordinates votes y d x image space edge coordinates votes what difficulty does this present for an implementation features votes need to adjust grid size or smooth image space edge coordinates votes here everything appears to be noise or random edge points but we still see peaks in the vote space initialize accumulator h to all zeros for each feature point x y in the image for θ to ρ x cos θ y sin θ h θ ρ h θ ρ end end find the value of θ ρ where h θ ρ is a local maximum the detected line in the image is given by ρ x cos θ y sin θ recall when we detect an edge point we also know its gradient direction but this means that the line is uniquely determined modified hough transform for each edge point x y θ gradient orientation at x y ρ x cos θ y sin θ h θ ρ h θ ρ end hough transform for circles circle center a b and radius r xi a yi b for a fixed radius r unknown gradient direction b hough space a hough transform for circles circle center a b and radius r xi a yi b for a fixed radius r unknown gradient direction intersection most votes for center occur here image space hough space hough transform for circles circle center a b and radius r xi a yi b for an unknown radius r unknown gradient direction b circle center a b and radius r xi a yi b for an unknown radius r unknown gradient direction b image space hough space for every edge pixel x y for all a for all b r h a b r end end end circle center a b and radius r xi a yi b for an unknown radius r known gradient direction image space hough space kristen grauman a circle with radius r and center a b can be described as x a r cos θ y b r sin θ a for every edge pixel x y for each possible radius value r x a r cos θ y b r sin θ for each possible gradient direction θ or use estimated gradient at x y a x r cos θ column b y r sin θ row h a b r end end end modified from kristen grauman original edges votes penny note a different hough transform with separate accumulators was used for each circle radius quarters vs penny comboinreigdindaeltections edges votes quarter note a different hough transform with separate accumulators was used for each circle radius quarters vs penny example iris detection gradient threshold hough space fixed radius max detections hemerson pistori and eduardo rocha costa voting practical tips minimize irrelevant tokens first choose a good grid discretization too fine too coarse too coarse large votes obtained when too many different lines correspond to a single bucket too fine miss lines because points that are not exactly collinear cast votes for different buckets vote for neighbors also smoothing in accumulator array use direction of edge to reduce parameters by to read back which points voted for winning peaks keep tags on the votes we want to find a template defined by its reference point center and several distinct types of landmark points in stable spatial configuration template what if we want to detect arbitrary shapes intuition model image novel image x vote space now suppose those colors encode gradient directions define a model shape by its boundary points and a reference point offline procedure at each boundary point compute displacement vector r a pi store these vectors in a table indexed by gradient orientation θ kristen grauman dana h ballard generalizing the hough transform to detect arbitrary shapes detection procedure for each edge point use its gradient orientation θ to index into stored table use retrieved r vectors to vote for reference point novel image assuming translation is the only transformation here i e orientation and scale are fixed template representation for each type of landmark point store all possible displacement vectors towards the center template model detecting the template for each feature in a new image look up that feature type in the model and vote for the possible center locations associated with that type in the model test image model svetlana lazebnik training image visual codeword with displacement vectors test image build codebook of patches around extracted interest points using clustering more on this later in the course template representation for each type of landmark point store all possible displacement vectors towards the center template model build codebook of patches around extracted interest points using clustering map the patch around each interest point to closest codebook entry build codebook of patches around extracted interest points using clustering map the patch around each interest point to closest codebook entry for each codebook entry store all positions it was found relative to object center hough transform pros and cons pros all points are processed independently so can cope with occlusion gaps some robustness to noise noise points unlikely to contribute consistently to any single bin can detect multiple instances of a model in a single pass cons complexity of search time increases exponentially with the number of model parameters non target shapes can produce spurious peaks in parameter space quantization can be tricky to pick a good grid size today fitting models lines to points i e find the parameters of a model that best fits the data least squares hough transform ransac matching finding correspondences between points i e find the parameters of the transformation that best aligns points homework is due outliers outliers can hurt the quality of our parameter estimates e g an erroneous pair of matching points from two images an edge point that is noise or doesn t belong to the line we are fitting random sample consensus approach we want to avoid the impact of outliers so let look for inliers and use those only intuition if an outlier is chosen to compute the current fit then the resulting line won t have much support from rest of the points ransac general form ransac loop randomly select a seed group of points on which to base model estimate fit model to these points find inliers to this model i e points whose distance from the line is less than t if there are d or more inliers re compute estimate of model on all of the inliers repeat n times keep the model with the largest number of inliers least squares fit randomly select minimal subset of points randomly select minimal subset of points hypothesize a model randomly select minimal subset of points hypothesize a model compute error function randomly select minimal subset of points hypothesize a model compute error function select points consistent with model randomly select minimal subset of points hypothesize a model compute error function select points consistent with model repeat hypothesize and verify loop randomly select minimal subset of points hypothesize a model compute error function select points consistent with model repeat hypothesize and verify loop uncontaminated sample randomly select minimal subset of points hypothesize a model compute error function select points consistent with model repeat hypothesize and verify loop randomly select minimal subset of points hypothesize a model compute error function select points consistent with model repeat hypothesize and verify loop fischler bolles in algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence line fitting example algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence ransac line fitting example algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence line fitting example n i algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence algorithm n i sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model how to choose parameters number of samples n choose n so that with probability p at least one random sample is free from outliers e g p outlier ratio e number of sampled points minimum number needed to fit the model distance threshold choose so that a good point with noise is likely e g prob within threshold explanation in szeliski ransac pros and cons pros simple and general applicable to many different problems often works well in practice cons lots of parameters to tune doesn t work well for low inlier ratios too many iterations or can fail completely can t always get a good initialization of the model based on the minimum number of samples common applications image stitching relating two views svetlana lazebnik cs intro to computer vision feature matching indexing and retrieval prof adriana kovashka university of pittsburgh october today matching points retrieving object instances indexing by visual words spatial verification fitting vs matching alignment fitting models lines to points i e find the parameters of a model that best fits the data least squares hough transform ransac matching finding correspondences between points i e find the parameters of the transformation that best aligns points least squares line fitting data xn yn line equation yi m xi b find m b to minimize xi yi y mx b m m e n x y ap y i i b i b x y n n modified from svetlana lazebnik y b x m image space hough parameter space what are the line parameters for the line that contains both and it is the intersection of the lines b and b y b x m image space hough parameter space how can we use this to find the most likely parameters m b for the most prominent line in the image space let each edge point in image space vote for a set of possible parameters in hough space accumulate votes in discrete set of bins parameters with the most votes indicate line in image space problems with the m b space unbounded parameter domains vertical lines require infinite m alternative polar representation y sin each point x y will add a sinusoid in the parameter space hough transform hough machine analysis of bubble chamber pictures proc int conf high energy accelerators and instrumentation use a polar representation for the parameter space y x hough space x cos ysin initialize accumulator h to all zeros for each feature point x y in the image for θ to ρ x cos θ y sin θ h θ ρ h θ ρ end end find the value of θ ρ where h θ ρ is a local maximum the detected line in the image is given by ρ x cos θ y sin θ recall when we detect an edge point we also know its gradient direction but this means that the line is uniquely determined modified hough transform for each edge point x y θ gradient orientation at x y ρ x cos θ y sin θ h θ ρ h θ ρ end hough transform for circles circle center a b and radius r xi a yi b for a fixed radius r unknown gradient direction b hough space a circle center a b and radius r xi a yi b for a fixed radius r unknown gradient direction intersection most votes for center occur here image space hough space for every edge pixel x y for each possible radius value r for each possible gradient direction θ or use estimated gradient at x y a x r cos θ column b y r sin θ row h a b r end end end generalized hough transform define a model shape by its boundary points and a reference point offline procedure at each boundary point compute displacement vector r a pi store these vectors in a table indexed by gradient orientation θ kristen grauman dana h ballard generalizing the hough transform to detect arbitrary shapes hough transform pros and cons pros all points are processed independently so can cope with occlusion gaps some robustness to noise noise points unlikely to contribute consistently to any single bin can detect multiple instances of a model in a single pass cons complexity of search time increases exponentially with the number of model parameters if parameters and choices for each search is o non target shapes can produce spurious peaks in parameter space quantization can be tricky to pick a good grid size modified from kristen grauman fischler bolles in algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence line fitting example algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence ransac line fitting example algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence line fitting example n i algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence algorithm n i sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model pros ransac pros and cons simple and general applicable to many different problems often works well in practice cons lots of parameters to tune doesn t work well for low inlier ratios too many iterations or can fail completely can t always get a good initialization of the model based on the minimum number of samples common applications image stitching relating two views today review fitting hough transform ransac retrieving object instances indexing by visual words spatial verification alignment problem we have previously considered how to fit a model to image evidence e g a line to edge points in alignment we will fit the parameters of some transformation according to a set of matching feature pairs correspondences difficulties xi noise outliers xi examples of parametric warps translation rotation aspect affine perspective p x y p x y transformation t is a coordinate changing machine p t p what does it mean that t is global is the same for any point p can be described by just a few numbers parameters let represent t as a matrix p mp x y x y scaling a coordinate means multiplying each of its components by a scalar uniform scaling means this scalar is the same for all components non uniform scaling different scalars per component x y scaling operation or in matrix form x ax y by xx yy a xx b yy scaling matrix s x y a c b x d y only linear transformations can be represented with a matrix linear transformations are combinations of scale rotation shear and mirror x a b c x affine transformations are combinations of linear transformations and y d e f translations properties of affine transformations lines map to lines or parallel lines remain parallel ratios are preserved x y a b d e c x f y closed under composition assuming we know the correspondences how do we get the transformation m xi xi y m m y t m i i alyosha efros what are the correspondences compare content in local patches find best matches e g simplest approach scan with template and compute ssd or correlation between list of pixel intensities in the patch kristen grauman n pixels d f a fb t grauman b leibe find a set of distinctive key points define a region around each keypoint extract and normalize the region content compute a local descriptor from the normalized region match local descriptors given matched points in a and b estimate the translation of the object x b x a tx i i i i t y derek hoiem tx ty least squares solution x b x a tx write down objective function in form ax b i i solve using pseudo inverse or eigenvalue decomposition i i t y x b x a y b y a tx y x b x a n n derek hoiem y b y a n n tx ty problem outliers ransac solution x b x a tx sample a set of matching points pair i i solve for transformation parameters score parameters with number of inliers repeat steps n times solve using least squares with inliers i i t y tx ty problem outliers multiple objects and or many to one matches hough transform solution x b x a tx initialize a grid of parameter values i i each matched pair casts a vote for consistent values find the parameters with the most votes solve using least squares with inliers i i board t y each feature match gives an alignment hypothesis for scale translation and orientation of model in image assuming we use scale rotation and translation invariant local features a hypothesis generated by a single match may be unreliable so let each match vote for a hypothesis in hough space gen hough transform details lowe system training phase for each model feature record location scale and orientation of model relative to normalized feature frame test phase let each match between a test sift feature and a model feature vote in a hough space x y location orientation scale find all bins with at least three votes and perform geometric verification estimate least squares affine transformation search for additional features that agree with the alignment object found if at least t matched points found david g lowe ijcv pp example result background subtract for model boundaries lowe objects recognized recognition in spite of occlusion fitting and matching summary fitting problems require finding any supporting evidence for a model even within clutter and missing features voting and inlier approaches such as the hough transform and ransac make it possible to find likely model parameters without searching all combinations of features can use these approaches to compute robust feature alignment matching and to match object templates adapted from kristen grauman and derek hoiem john phillips today review fitting hough transform ransac matching points spatial verification each patch region has a descriptor which is a point in some high dimensional feature space e g sift kristen grauman when we see close points in feature space we have similar descriptors which indicates similar local content kristen grauman database images with potentially thousands of features per image and hundreds to millions of images to search how to efficiently find those that are relevant to a new image kristen grauman indexing local features inverted file index for text documents an efficient way to find all pages on which a word occurs is to use an index we want to find all images in which a feature occurs to use this idea we ll need to map our features to visual words kristen grauman extract some local features from a number of images e g sift descriptor space each point is dimensional each point is a local descriptor e g sift vector example each visual words group of patches belongs to the same visual word figure from sivic zisserman iccv index displacements by visual codeword test image learning in computer vision build codebook of patches around extracted interest points using clustering more on this later in the course visual words map high dimensional descriptors to tokens words by quantizing the feature space quantize via clustering let cluster centers be the prototype words determine which word to assign to each new image region by finding the closest cluster center database images are loaded into the index mapping words to image numbers when will this give us a significant gain in efficiency new query image is mapped to indices of database images that share a word we can call this retrieval process instance recognition instance recognition remaining issues how to summarize the content of an entire image and gauge overall similarity how large should the vocabulary be how to perform quantization efficiently is having the same set of visual words enough to identify the object scene how to verify spatial agreement analogy to documents of all the sensory impressions proceeding to the brain the visual experiences are the dominant ones our perception of the world around us is based essentially on the messages that reach the brain from our eyes for a long time it was thought that the retinal image was trasnsemnittseod proyi ntbbryapionin t to visual centers in thveisbruaian l thpe ecerrceberapl tcioortnex was a movie screen so to speak upon which the image inrtehteienyae lw acseprroejebctreadl tchororutgehxth e discoveries oef hyueb elcaendllw oiepsetliwceanlow know that behind the origin of the visual perception in the brain there is a considerably more complicatheducbouersle owf eiveesntes lby following the visual impulses along their path to the various cell layers of the optical cortex hubel and wiesel have been able to demonstrate that the message about the image falling on the retina undergoes a step wise analysis in a system of nerve cells stored in columns in this system each cell has its specific function and is responsible for a specific detail in the pattern of the retinal image china is forecasting a trade surplus of to this year a threefold increase on the commerce ministry said the surplus would be created by a predicted jump in exports to compared with a rise in imports to the figcurhesinarae likterlay dtoefu rther annoy thesuusr pwhluichsh acsolomngmargeurecdeth at china exports are unfairly helped by a deliberateelyxupnoderrtvsal ueimd ypuaonr tbse ijuings agrees tyheusaunrp lubs ias ntoko hidgho mbutessaytsicth e yuan is only one factor bank of china governor zhou xiaochuan said the country also needed to dtormaodreet ovbaoolustedomestic demand so more goods stayed within the country china increased the value of the yuan against the dollar by in july and permitted it to trade within a narrow band but the us wants the yuan to be allowed to trade freely however beijing has made it clear that it will take its time and tread carefully before allowing the yuan to rise further in value iccv short course l fei fei bags of visual words summarize entire image based on its distribution histogram of word occurrences analogous to bag of words representation commonly used for documents comparing bags of words rank frames by normalized scalar product between their possibly weighted occurrence counts nearest neighbor search for similar images 𝑠𝑖𝑚 𝑉 𝑑𝑗 𝑖 𝑞 𝑖 for vocabulary of v words j kristen grauman bags of words pros and cons flexible to geometry deformations viewpoint compact summary of image content very good results in practice basic model ignores geometry must verify afterwards or encode via features background and foreground mixed when bag covers whole image optimal vocabulary formation remains unclear adapted from kristen grauman inverted file index and bags of words similarity extract words in query inverted file index to find relevant frames compare word counts adapted from kristen grauman tf idf weighting term frequency inverse document frequency describe frame by frequency of each word within it downweight words that appear often in the database standard weighting for text retrieval number of occurrences of word i in document d number of words in document d total number of documents in database number of documents word i occurs in in whole database kristen grauman bags of words for content based image retrieval slide from andrew zisserman example retrieved shots r i ilvk t t j stru t fnun e key frame end frame st ut fnune f key frame f ti c t r i r shut frame fra me ji k ey craine end frame o i t r i i i i j f slide from andrew zisserman frame l r i frame ke fr e l o r i p key frame etl l f encl fnune video google system collect all words within query region inverted file index to find relevant frames compare word counts spatial verification sivic zisserman iccv demo online at esearch vgoogle index html k grauman b leibe scoring retrieval quality query database size images relevant total images results ordered precision relevant returned recall relevant total relevant recall instance recognition remaining issues how to summarize the content of an entire image and gauge overall similarity how large should the vocabulary be how to perform quantization efficiently is having the same set of visual words enough to identify the object scene how to verify spatial agreement vocabulary size results for recognition task with images influence on performance sparsity nister stewenius cvpr vocabulary trees hierarchical clustering for large vocabularies tree construction nister stewenius cvpr slide credit david nister vocabulary tree training filling the tree nister stewenius cvpr vocabulary tree training filling the nister stewenius cvpr vocabulary tree training filling the tree nister stewenius cvpr vocabulary tree recognition nister stewenius cvpr slide credit david nister complexity what is the computational advantage of the hierarchical representation bag of words vs a flat vocabulary complexity depends on branching factor and number of levels instance recognition remaining issues how to summarize the content of an entire image and gauge overall similarity how large should the vocabulary be how to perform quantization efficiently is having the same set of visual words enough to identify the object scene how to verify spatial agreement today review fitting hough transform ransac matching points retrieving object instances indexing by visual words db image with high bow similarity db image with high bow similarity both image pairs have many visual words in common db image with high bow similarity db image with high bow similarity only some of the matches are mutually consistent spatial verification two basic strategies ransac typically sort by bow similarity as initial filter verify by checking support inliers for possible transformations e g success if find a transformation with n inlier correspondences generalized hough transform let each matched feature cast a vote on location scale orientation of the model object verify parameters with enough votes video google system collect all words within query region inverted file index to find relevant frames compare word counts spatial verification sivic zisserman iccv demo online at esearch vgoogle index html kristen grauman example applications mobile tourist guide self localization object building recognition photo video augmentation b leibe quack leibe van gool civr indexing and retrieval summary bag of words representation quantize feature space to make discrete set of visual words summarize image by distribution of words index individual words inverted index pre compute index to enable faster search at query time recognition of instances via alignment matching local features followed by spatial verification robust fitting ransac ght adapted from kristen grauman cs intro to computer vision projective transformations image stitching prof adriana kovashka university of pittsburgh october part iii b degree bins degree bins due on thursday wrap up indexing and retrieval image transformations linear and affine image formation image stitching computing homographies indexing local features inverted file index for text documents an efficient way to find all pages on which a word occurs is to use an index we want to find all images in which a feature occurs to use this idea we ll need to map our features to visual words kristen grauman extract some local features from a number of images e g sift descriptor space each point is dimensional example each group of patches belongs to the same visual word figure from sivic zisserman iccv map high dimensional descriptors to tokens words by quantizing the feature space quantize via clustering let cluster centers be the prototype words determine which word to assign to each new image region by finding the closest cluster center comparing bags of words rank frames by normalized scalar product between their possibly weighted occurrence counts nearest neighbor search for similar images 𝑠𝑖𝑚 𝑉 𝑑𝑗 𝑖 𝑞 𝑖 for vocabulary of v words j inverted file index and bags of words similarity extract words in query inverted file index to find relevant frames compare word counts bags of words for content based image retrieval slide from andrew zisserman example retrieved shots r i ilvk t t j stru t fnun e key frame end frame st ut fnune f key frame f ti c t r i r shut frame fra me ji k ey craine end frame o i t r i i i i j f slide from andrew zisserman frame l r i frame ke fr e l o r i p key frame etl l f encl fnune video google system collect all words within query region inverted file index to find relevant frames compare word counts spatial verification sivic zisserman iccv demo online at esearch vgoogle index html k grauman b leibe db image with high bow similarity db image with high bow similarity both image pairs have many visual words in common db image with high bow similarity db image with high bow similarity only some of the matches are mutually consistent spatial verification two basic strategies ransac typically sort by bow similarity as initial filter verify by checking support inliers for possible transformations e g success if find a transformation with n inlier correspondences generalized hough transform let each matched feature cast a vote on location scale orientation of the transformation verify parameters with enough votes tx ty problem outliers ransac solution x b x a tx sample a set of matching points pair i i solve for transformation parameters score parameters with number of inliers repeat steps n times i i t y tx ty problem outliers multiple objects and or many to one matches hough transform solution x b x a tx initialize a grid of parameter values i i each matched pair casts a vote for consistent values find the parameters with the most votes solve using least squares with inliers i i t y scoring retrieval quality query database size images relevant total images results ordered precision relevant returned recall relevant total relevant recall example applications mobile tourist guide object building recognition self localization photo video augmentation b leibe quack leibe van gool civr indexing and retrieval summary bag of words representation quantize feature space to make discrete set of visual words summarize image by distribution of words index individual words inverted index pre compute index to enable faster search at query time recognition of instances via alignment matching local features followed by spatial verification robust fitting ransac ght today wrap up indexing and retrieval image formation image stitching computing homographies examples of parametric warps translation rotation aspect affine perspective p x y p x y transformation t is a coordinate changing machine p t p what does it mean that t is global is the same for any point p can be described by just a few numbers parameters let represent t as a matrix p mp x y x y scaling x x x x sx x y y y sy y y rotate around x cos x sin y x cos sin x y sin x cos y y sin cos y shear x x shx y x shx x y sh x y y sh y y y mirror about y axis x x x x y y y y mirror over x x x x y y y y translation x y x tx y t y no linear transformations x y a c b x d y only linear transformations can be represented with a matrix linear transformations are combinations of scale rotation shear and mirror homogeneous coordinates to convert to homogeneous coordinates homogeneous image coordinates converting from homogeneous coordinates homogeneous coordinates xx tx xx xx ttxx yy y y t ty y y tyy tx ty x a b c x y d e f y w w affine transformations are combinations of linear transformations and translations parallel lines remain parallel assuming we know the correspondences how do we get the transformation m xi xi y m m y t m i i alyosha efros m xi yi xi x y m y i i i how many matches correspondence pairs do we need to solve for the transformation parameters once we have solved for the parameters how do we compute x new y new given xnew ynew where do the matches come from recall fitting an affine transformation the affine model approximates perspective projection projective transformations x a b c x y w d e g h f y i w projective transformations affine transformations and projective warps parallel lines do not necessarily remain parallel main questions alignment given two images what is the transformation between them warping given a source image and a transformation what does the transformed output look like motivation for feature based alignment image mosaics today wrap up indexing and retrieval image transformations linear and affine image stitching computing homographies how are objects in the world captured in an image let design a camera idea put a piece of film in front of an object do we get a reasonable image idea add a barrier to block off most of the rays this reduces blurring the opening is known as the aperture how does this transform the image pinhole camera is a simple model to approximate imaging process perspective projection if we treat pinhole as a point only one ray from any given point can enter the camera camera obscura the pre camera camera obscura dark room first idea mo ti china to first built alhazen iraq egypt to illustration of camera obscura freestanding camera obscura at unc chapel hill modified from derek hoiem photo by seth ilys camera obscura at home how can you make your own portable camera sketch from camera obscura used for tracing lens based camera obscura derek hoiem camera obscura jetty at margate england an attraction in the late century adapted from r duraiswami around oldest surviving photograph took hours on pewter plate joseph niepce photograph of the first photograph stored at ut austin derek hoiem boulevard du temple louis daguerre dimensionality reduction machine to world image point of observation projection can be tricky honda commercial far away objects appear smaller forsyth and ponce parallel lines in the scene intersect in the image converge in image on horizon line image plane virtual pinhole scene many to one any points along same ray map to same point in image points points lines lines collinearity preserved distances and angles are not preserved degenerate cases line through focal point projects to a point plane through focal point projects to line plane perpendicular to image plane projects to part of the image projection world coordinates image coordinates p x y optical center y f z x camera center z x x p y x y scene point image coordinates homogeneous coordinates is this a linear transformation no division by z is nonlinear trick add one more coordinate homogeneous image coordinates homogeneous scene coordinates converting from homogeneous coordinates homogeneous coordinates invariant to scaling x kx kx x k y ky kw w ky y w kw kw w homogeneous coordinates cartesian coordinates point in cartesian is ray in homogeneous perspective projection matrix projection is a matrix multiplication using homogeneous coordinates x y x x y y f f f z z f z z divide by the third coordinate to convert back to non homogeneous coordinates weak perspective assumes scene depth average distance to camera adding a lens a lens focuses light onto the film rays passing through the center are not deviated all parallel rays converge to one point on a plane located at the focal length f slide by steve seitz cameras with lenses f focal point optical center center of projection a lens focuses parallel rays onto a single focal point gather more light while keeping focus make pinhole perspective projection practical today wrap up indexing and retrieval image transformations linear and affine image formation mosaics obtain a wider angle view by combining multiple images how to stitch together a panorama a k a mosaic basic procedure take a sequence of images from the same position rotate the camera about its optical center compute transformation between second image and first transform the second image to overlap with the first blend the two together to create a mosaic if there are more images repeat use the geometry of the scene combine two or more overlapping images to make one larger image camera center mosaics generating synthetic views real camera synthetic camera can generate any synthetic camera view as long as it has the same center of projection mosaic pp the mosaic has a natural interpretation in the images are reprojected onto a common plane the mosaic is formed on this plane mosaic is a synthetic wide angle camera basic question how to relate two images from the same camera center how to map a pixel from to answer cast a ray through each pixel in draw the pixel where that ray intersects observation rather than thinking of this as a reprojection think of it as a image warp from one image to another a projective transform is a mapping between any two pps with the same center of projection rectangle should map to arbitrary quadrilateral parallel lines aren t but must preserve straight lines called homography wx wy x y w p h p homography xn yn xn yn to compute the homography given pairs of corresponding points in the images we need to set up an equation where the parameters of h are the unknowns solving for homographies p hp wx a b c x wy d e f y w g h i can set scale factor i so there are unknowns set up a system of linear equations ah b where vector of unknowns h a b c d e f g h t need at least eqs but the more the better solve for h if overconstrained solve using least squares min ah b help lmdivide x y homography wx w wy w x y to apply a given homography h compute p hp regular matrix multiply convert p from homogeneous to image wx wy w x y coordinates kristen grauman p h p y given a coordinate transform and a source image f x y how do we compute a transformed image g x y f t x y y send each pixel f x y to its corresponding location x y t x y in the second image q what if pixel lands between two pixels y x f x y x g x y send each pixel f x y to its corresponding location x y t x y in the second image q what if pixel lands between two pixels a distribute color among neighboring pixels x y known as splatting y get each pixel g x y from its corresponding location x y t x y in the first image q what if pixel comes from between two pixels y x f x y x g x y get each pixel g x y from its corresponding location x y t x y in the first image q what if pixel comes from between two pixels a interpolate color value from neighbors nearest neighbor bilinear help recap how to stitch together a panorama a k a mosaic basic procedure take a sequence of images from the same position rotate the camera about its optical center compute transformation homography between second image and first using corresponding points transform the second image to overlap with the first blend the two together to create a mosaic if there are more images repeat homography example image rectification to unwarp rectify an image solve for homography h given p and p p hp assume we have four matched points how do we compute homography h direct linear transformation dlt w x p w y h h h h p hp a x y w xx yx x h x y xy yy y h h h apply svd udvt a h h vsmallest column of v corr to smallest singular value h assume we have matched points with outliers how do we compute homography h automatic homography estimation with ransac choose number of samples n choose random potential matches compute h using normalized dlt project points from x to x for each potentially matching pair x i hxi count points with projected distance t e g t pixels repeat steps n times choose h with most inliers derek hoiem compute interest points on each image find candidate matches estimate homography h using matched points and ransac with normalized dlt project each image onto the same surface and blend matlab maketform imtransform summary write transformations as matrix vector multiplication including translation when we use homogeneous coordinates projection equations express how world points mapped to image perform image warping forward inverse fitting transformations solve for unknown parameters given corresponding points from two views affine projective homography mosaics uses homography and image warping to merge views taken from same center of projection cs intro to computer vision epipolar geometry and stereo vision prof adriana kovashka university of pittsburgh october today review projective transforms image stitching homography epipolar geometry multiple views from different cameras stereo vision estimating depth from disparities exam and homework info x y a c b x d y only linear transformations can be represented with a matrix linear transformations are combinations of scale rotation shear and mirror x a b c x y d e f y w w affine transformations are combinations of linear transformations and translations parallel lines remain parallel x a b c x y w d e g h f y i w projective transformations affine transformations and projective warps parallel lines do not necessarily remain parallel m xi yi xi x y m y i i i how many matches correspondence pairs do we need to solve for the transformation parameters once we have solved for the parameters how do we compute x new y new given xnew ynew x k r t x x image coordinates u v k intrinsic matrix r rotation t translation x world coordinates x y z extrinsic params r t intrinsic params k focal length pixel sizes mm etc we ll assume that these parameters are given and fixed intrinsic assumptions unit aspect ratio optical center at no skew extrinsic assumptions no rotation camera at k u f x y x k i x w v f z u f u r r r t x x y x k r t x w v ty r r r t z z x k r t x wu x y wv z w obtain a wider angle view by combining multiple images two images with rotation zoom but no translation derek hoiem camera center how to stitch together a panorama a k a mosaic basic procedure take a sequence of images from the same position rotate the camera about its optical center compute the homography transformation between second image and first transform the second image to overlap with the first blend the two together to create a mosaic if there are more images repeat modified from steve seitz mosaic plane the mosaic has a natural interpretation in the images are reprojected onto a common plane the mosaic is formed on this plane mosaic is a synthetic wide angle camera a projective transform is a mapping between any two pps with the same center of projection rectangle should map to arbitrary quadrilateral parallel lines aren t but must preserve straight lines called homography wx wy x y w p h p xn yn xn yn to compute the homography given pairs of corresponding points in the images we need to set up an equation where the parameters of h are the unknowns p hp wx a b c x wy d e f y w g h i can set scale factor i so there are unknowns set up a system of linear equations ah b where vector of unknowns h a b c d e f g h t need at least eqs but the more the better solve for h if overconstrained solve using least squares min ah b help lmdivide how to stitch together a panorama a k a mosaic basic procedure take a sequence of images from the same position rotate the camera about its optical center compute the homography transformation between second image and first transform the second image to overlap with the first blend the two together to create a mosaic if there are more images repeat x y image image canvas wx w wy w x y to apply a given homography h compute p hp regular matrix multiply wx wy x y convert p from homogeneous to image coordinates w modified from kristen grauman p h p image image canvas y forward warping send each pixel f x y to its corresponding location x y h x y in the right image image image canvas y inverse warping get each pixel g x y from its corresponding location x y h x y in the left image q what if pixel comes from between two pixels a interpolate color value from neighbors ransac for homography today review projective transforms image stitching homography stereo vision estimating depth from disparities exam and homework info last class vs this class last class same camera center but camera rotates this class camera center is not the same we have multiple cameras epipolar geometry relates cameras from two positions stereo depth estimation recover depth from two images adapted from derek hoiem why multiple views structure and depth are inherently ambiguous from single views multiple views help us to perceive shape and depth take two pictures of the same subject from two slightly different viewpoints and display so that each eye sees only one of the images invented by sir charles wheatstone image from fisher price com stereo vision two cameras simultaneous views single moving camera and static scene goal recover depth by finding image coordinate x that corresponds to x x x z c baseline c b goal recover depth by finding image coordinate x that corresponds to x sub problems calibration how do we recover the relation of the cameras if not already known correspondence how do we search for the matching point x x geometry for a simple stereo system assume parallel optical axes known camera parameters i e calibrated cameras what is expression for z similar triangles pl p pr and ol p or t xl xr t z f z depth is inversely proportional to disparity depth disparity z f t xr xl depth from disparity we have two images taken from cameras with different intrinsic and extrinsic parameters how do we match a point in the first image to a point in the second image i x y disparity map d x y image i x y so if we could find the corresponding points in two images we could estimate relative depth given p in left image where can corresponding point p be geometry of two views constrains where the corresponding pixel for some image point in the first view must occur in the second view it must be on the line carved out by a plane connecting the world point and optical centers potential matches for p have to lie on the corresponding line l potential matches for p have to lie on the corresponding line l epipolar geometry notation derek hoiem baseline line connecting the two camera centers epipoles intersections of baseline with image planes projections of the other camera center epipolar plane plane containing baseline epipolar lines intersections of epipolar plane with image planes always come in corresponding pairs note all epipolar lines intersect at the epipole epipolar constraint this is useful because it reduces the correspondence problem to a search along an epipolar line kristen grauman image from andrew zisserman if the stereo rig is calibrated we know how to rotate and translate camera reference frame to get to camera reference frame rotation matrix r translation vector t if the stereo rig is calibrated we know how to rotate and translate camera reference frame to get to camera reference frame x c rxc t an aside cross product vector cross product takes two vectors and returns a third vector that perpendicular to both inputs so here c is perpendicular to both a and b which means the dot product from geometry to algebra x rx t x t x x t rx t x normal to the plane t rx another aside matrix form of cross product a a b a a b c can be expressed as a matrix multiplication a a a x from geometry to algebra x rx t x t x x t rx t x t rx t t normal to the plane t rx essential matrix x tx rx let e t x r x t ex e is called the essential matrix and it relates corresponding image points between both cameras given the rotation and translation if we observe a point in one image its position in other image is constrained to lie on line defined by above ex is the epipolar line through x in the first image corresponding to x note these points are in camera coordinate systems essential matrix example parallel cameras r p x y f t e t x r p x y f p ep for the parallel cameras image of any point must lie on same horizontal line in each image plane image i x y disparity map d x y x y x d x y y image i x y what about when cameras optical axes are not parallel reproject image planes onto a common plane parallel to the line between camera centers pixel motion is horizontal after this transformation two homographies transform one for each input image reprojection and pattern recognition stereo image rectification example what if we don t know the camera parameters want to estimate world geometry without requiring calibrated cameras archival videos photos from multiple unrelated users weak calibration estimate epipolar geometry from a redundant set of point correspondences between two uncalibrated cameras computing f from correspondences each point correspondence generates one constraint on f collect n of constraints solve for f im right im left fundamental matrix relates pixel coordinates in the two views more general form than essential matrix we remove need to know intrinsic parameters properties of the fundamental matrix xt f x with f k t ek f x is the epipolar line associated with x l f x ftx is the epipolar line associated with x l ftx f e and fte f is singular rank two det f f has seven degrees of freedom entries but defined up to scale det f let recap today review projective transforms image stitching homography epipolar geometry multiple views from different cameras exam and homework info moving on to stereo fuse a calibrated binocular stereo pair to produce a depth image image image dense depth map basic stereo matching algorithm for each pixel in the first image find corresponding epipolar scanline in the right image if necessary rectify the two stereo images to transform epipolar lines into scanlines search along epipolar line and pick the best match x compute disparity x x and set depth x f t x x correspondence search left right scanline matching cost disparity slide a window along the right scanline and compare contents of that window with the reference window in the left image matching cost ssd or normalized correlation geometry for a simple stereo system assume parallel optical axes known camera parameters i e calibrated cameras what is expression for z similar triangles pl p pr and ol p or t xl xr t z f z depth disparity z f t xr xl kristen grauman results with window search data window based matching ground truth how can we improve uniqueness for any point in one image there should be at most one matching point in the other image ordering corresponding points should be in the same order in both views smoothness we expect disparity values to change slowly for the most part many of these constraints can be encoded in an energy function and solved using graph cuts before graph cuts ground truth for the latest and greatest projective structure from motion given m images of n fixed points xij pi xj i m j n problem estimate m projection matrices pi and n points xj from the mn corresponding points xij xj photo synth building rome in a day agarwal et al point x in left image corresponds to epipolar line l in right image epipolar line passes through the epipole the intersection of the cameras baseline with the image plane fundamental matrix maps from a point in one image to a line in the other if x and x correspond to the same point x recap stereo with calibrated cameras given image pair r t detect some features compute essential matrix e match features using the epipolar and other constraints triangulate for structure and get depth kristen grauman summary epipolar geometry epipoles are intersection of baseline with image planes matching point in second image is on a line passing through its epipole epipolar constraint limits where points from one view will be imaged in the other which makes search for correspondences quicker fundamental matrix maps from a point in one image to a line its epipolar line in the other can solve for f given corresponding points e g interest points stereo depth estimation find corresponding points along epipolar scanline estimate disparity depth is inverse to disparity modified from kristen grauman and derek hoiem today review projective transforms image stitching homography epipolar geometry multiple views from different cameras stereo vision estimating depth from disparities next thursday midterm exam in class review on tuesday email me with topics you want me to review or with questions format mostly short answer questions from easier shorter to longer harder some exercises to show you can apply some of the clustering and matching algorithms we discussed homework grades due tonight review late policy beyond free late days total for the class minute late late day penalty notes on part iii a the x y scores you output should correspond to the final set of keypoints after non max suppression if you re getting a negative mean r you can ignore the threshold and output the top n keypoints e g top matlab tips released due october part i hough transform for circles part ii video google system data and starter code provided prof adriana kovashka university of pittsburgh october image formation digital images digital images sample the space on a regular grid quantize each sample round to nearest integer image thus represented as a matrix of integer values digital color images digital color images color images rgb color space r g b slide credit kristen grauman images in matlab images represented as a matrix suppose we have a nxm rgb image called im im top left pixel value in r channel im y x b y pixels down x pixels to right in the bth channel im n m bottom right pixel in b channel imread filename returns a image values to convert to double format values to with row column r slide credit derek hoiem salt and pepper noise random occurrences of black and white pixels impulse noise random occurrences of white pixels gaussian noise variations in intensity drawn from a gaussian normal distribution source s seitz let replace each pixel with an average of all the values in its neighborhood moving average in can add weights to our moving average weights non uniform weights depicts box filter white high value black low value original filtered what if the filter size was x instead of x slide credit kristen grauman gaussian filter what if we want nearest neighboring pixels to have the most influence on the output removes high frequency components from the image low pass filter source s seitz smoothing with a gaussian size of kernel or mask note gaussian function has infinite support but discrete filters use finite kernels σ with x kernel σ with x kernel variance of gaussian determines extent of smoothing σ with x kernel σ with x kernel practical matters how big should the filter be values at edges should be near zero important rule of thumb for gaussian set filter half width to about σ source derek hoiem smoothing with a gaussian parameter σ is the scale width spread of the gaussian kernel and controls the amount of smoothing for sigma h fspecial gaussian fsize sigma out imfilter im h imshow out pause end slide credit kristen grauman predict the outputs using correlation filtering slide credit kristen grauma original original filtered no change original original shifted left by pixel with correlation original original blur with a box filter original original sharpening filter accentuates differences with local average separability example f g h f g h filtering center location only the filter factors into a product of filters perform filtering along rows followed by filtering along the remaining column no new pixel values introduced removes spikes good for impulse salt pepper noise non linear filter median filter is edge preserving what defines a texture original image original image original image original image dimension mean d dx value statistics to summarize patterns in small windows with primarily horizontal edges both windows with small gradient in both directions windows with primarily vertical edges statistics to summarize patterns in small original image kristen grauman derivative filter responses squared d a b dimension kristen grauman filter banks scales what filters to put in the bank typically we want a combination of scales and orientations different types of patterns matlab code available for these examples slide credit kristen grauman filter bank d d d d d ej i l l 201hil 20 l j lj j 20 20 20 20 20 20 20 20 20 20 20 20 20 slide credit kristen grauman 200 50 50 150 200 250 50 100 150 200 250 50 50 100 150 200 250 50 100 150 200 250 50 50 100 150 200 250 we can form a feature vector from the list of responses at each pixel classifying materials stuff figure by varma zisserman texture synthesis goal create new samples of a given texture many applications virtual environments hole filling texturing surfaces slide credit kristen grauman markov random field a markov random field mrf generalization of markov chains to two or more dimensions first order mrf probability that pixel x takes a certain value given the values of neighbors a b c and d source s seitz texture synthesis intuition before we inserted the next word based on existing nearby words now we want to insert pixel intensities based on existing nearby pixel values corpus place we want to insert next distribution of a value of a pixel is conditioned on its neighbors alone slide credit kristen grauman synthesizing one pixel p input image synthesized image what is find all the windows in the image that match the neighborhood to synthesize x pick one matching window at random assign x to be the center pixel of that window an exact neighbourhood match might not be present so find the best matches using ssd error and randomly choose between them preferring better matches with higher probability varying window size increasing window size throw away every other row and column to create a size image slide credit derek hoiem example sinewave example sinewave when sampling a signal at discrete intervals the sampling frequency must be fmax fmax max frequency of the input signal this will allows to reconstruct the original perfectly from the sampled version good bad goal find in image main challenge what is a good similarity or distance measure between two patches correlation zero mean correlation sum square difference normalized cross correlation goal find in image method filter the image with eye patch h m n g k l k l f m k n l f image g filter input filtered image what went wrong goal find in image method ssd h m n g k l k l f m k n l input sqrt ssd thresholded image goal find in image method normalized cross correlation mean template mean image patch h m n g k l k l g f m k n l fm n g k l g f m k n l m n k l k l matlab template im goal find in image method normalized cross correlation input normalized x correlation thresholded image gaussian pyramid laplacian filter unit impulse gaussian laplacian of gaussian laplacian pyramid what causes an edge reflectance change appearance information texture depth discontinuity object boundary change in surface orientation shape cast shadows edges gradients and invariance characterizing edges an edge is a place of rapid change in the image intensity function image intensity function along horizontal scanline first derivative edges correspond to extrema of derivative intensity profile with a little gaussian noise gradient solution smooth first f g f g d f dx g to find edges look for peaks in d f dx g derivative theorem of convolution differentiation is convolution and convolution is associative d f dx g f d g dx this saves us one operation f d g dx f d g dx gradients edges primary edge detection steps smoothing suppress noise edge enhancement filter for contrast edge localization determine which local maxima from filter output are actually edges vs noise threshold thin thresholding choose a threshold value t set any pixels less than t to zero off set any pixels greater than or equal to t to one on original image source k grauman gradient magnitude image source k grauman source k grauman source k grauman threshold at minimum level get orientation theta gy gx norm of the gradient thresholding how to turn these thick regions of the gradient into curves check if pixel is local maximum along gradient direction select single max across width of the edge requires checking interpolated pixels p and r brightness color texture combined human given a grayscale image or an intermediate matrix threshold to create a binary output example edge detection gradient magnitude find t looking for pixels where gradient is strong given a grayscale image or an intermediate matrix threshold to create a binary output example intensity based detection find im looking for dark pixels given a grayscale image or an intermediate matrix threshold to create a binary output example color based detection find hue hue looking for pixels within a certain hue range structuring elements masks of varying shapes and sizes used to perform morphology for example scan mask across foreground pixels to transform the binary image help strel expands connected components grow features fill holes before dilation after dilation erode connected components shrink features remove bridges branches noise before erosion after erosion source shapiro and stockman erode then dilate remove small objects keep original shape before opening after opening closing dilate then erode fill holes but keep original shape before closing after closing applet suppose you have to click on some point go away and come back after i deform the image and click on the same points again which points would you choose original hoiem d f a fb t grauman b leibe find a set of distinctive key points define a region around each keypoint extract and normalize the region content compute a local descriptor from the normalized region match local descriptors goals for keypoints detect points that are repeatable and distinctive adapted from d hoiem corners as distinctive interest points we should easily recognize the point by looking through a small window shifting a window in any direction should give a large change in intensity corners as distinctive interest points we should easily recognize the point by looking through a small window shifting a window in any direction should give a large change in intensity corners as distinctive interest points we should easily recognize the point by looking through a small window shifting a window in any direction should give a large change in intensity corners as distinctive interest points we should easily recognize the point by looking through a small window shifting a window in any direction should give a large change in intensity corners as distinctive interest points we should easily recognize the point by looking through a small window shifting a window in any direction should give a large change in intensity window averaged squared change of intensity induced by shifting the image data by u v window function w x y or in window outside gaussian window averaged squared change of intensity induced by shifting the image data by u v e u v expanding i x y in a taylor series expansion we have for small shifts u v a quadratic approximation to the error surface between a patch and itself shifted by u v where m is a matrix computed from image derivatives edge corner and are large flat region and are small compute image gradients ix and iy for all pixels for each pixel compute by looping over neighbors x y compute k empirical constant k find points with large corner response function r r threshold take the points of locally maximum r as the detected feature points i e pixels where r is bigger than for all the or neighbors frolova d simakov compute corner response at every pixel i a i b only derivatives are used invariance to intensity shift i i b intensity scaling i a i r r threshold x image coordinate x image coordinate derivatives and window function are shift invariant second moment ellipse rotates but its shape i e eigenvalues remains the same corner all points will be classified as edges scale invariant detection the problem how do we choose corresponding circles independently in each image do objects in the image have a characteristic scale that we can identify f d g dx f d g dx edge derivative of gaussian edge max of derivative f d g d f g dx edge second derivative of gaussian edge zero crossing of derivative edge ripple blob superposition of two ripples maximum spatial selection the magnitude of the laplacian response will achieve a maximum at the center of the blob provided the scale of the laplacian is matched to the scale of the blob lazebnik what is a useful signature function laplacian of gaussian blob detector difference of gaussian laplacian we can approximate the laplacian with a difference of gaussians more efficient to implement laplacian dog g x y k g x y difference of gaussians find local maxima in position scale space of difference of gaussian lxx lyy list of x y lowe iccv histogram of oriented gradients captures important texture information robust to small translations affine deformations grauman b leibe scale invariant feature transform basic idea take square window around detected feature compute gradient orientation for each pixel create histogram over edge orientations weighted by magnitude full version divide the window into a grid of cells case shown below compute an orientation histogram for each cell cells orientations dimensional descriptor full version divide the window into a grid of cells case shown below compute an orientation histogram for each cell cells orientations dimensional descriptor threshold normalize the descriptor such that making descriptor rotation invariant rotate patch according to its dominant gradient orientation this puts the patches into a canonical orientation grauman image from matthew brown local features desired properties repeatability the same feature can be found in several images despite geometric and photometric transformations saliency each feature has a distinctive description compactness and efficiency many fewer features than image pixels locality a feature occupies a relatively small area of the image robust to clutter and occlusion matching local features image image to generate candidate matches find patches that have the most similar appearance e g lowest ssd simplest approach compare them all take the closest or closest k or within a thresholded distance ambiguous matches image image at what ssd value do we have a good match to add robustness to matching can consider ratio distance to best match distance to second best match if low first match looks good k grauman if high could be ambiguous match extract features extract features compute putative matches extract features compute putative matches loop hypothesize transformation t small group of putative matches that are related by t extract features compute putative matches loop hypothesize transformation t small group of putative matches that are related by t verify transformation search for other matches consistent with t extract features compute putative matches loop hypothesize transformation t small group of putative matches that are related by t verify transformation search for other matches consistent with t two types of light sensitive receptors cones cone shaped less sensitive operate in high light color vision rods rod shaped highly sensitive operate at night gray scale vision slower to respond slide credit efros the physics of light some examples of the spectra of light sources a ruby laser b gallium phosphide crystal wavelength nm wavelength nm tungsten lightbulb normal daylight stephen e palmer the physics of light some examples of the reflectance spectra of surfaces 700 700 700 700 wavelength nm stephen e palmer physiology of color vision three kinds of cones 100 50 s nm m l 650 wavelength nm why are m and l cones so close stephen e palmer metamers spectral reflectances for some natural objects how much of each wavelength is reflected for that surface grauman forsyth ponce measurements by e koivisto color constancy interpret surface in terms of true color rather than observed intensity humans are good at it computers are not nearly as good how do we compute the weights that will yield a perceptual match for any test light using a given set of primaries select primaries estimate their color matching functions observer matches series of monochromatic lights one at each wavelength c c i matches i i i now have matching functions for all monochromatic light sources so we know how to match a unit of each wavelength arbitrary new spectral signal is a linear combination of the monochromatic sources t t t image human segmentation group together similar looking pixels for efficiency of further processing superpixels x ren and j malik iccv oversegmentation undersegmentation multiple segmentations image segmentation toy example black pixels gray pixels white pixels input image intensity these intensities define the three groups we could label every pixel in the image according to which of these primary intensities it is i e segment the image based on the intensity feature what if the image isn t quite so simple input image input image intensity intensity input image intensity now how to determine the three main intensities that define our groups we need to cluster intensity goal choose three centers as the representative intensities and label every pixel according to which of these centers it is nearest to best cluster centers are those that minimize ssd between all points and their nearest cluster center ci clustering with this objective it is a chicken and egg problem if we knew the cluster centers we could allocate points to groups by assigning each to its closest center if we knew the group memberships we could get the centers by computing the mean per group k means clustering basic idea randomly initialize the k cluster centers and iterate between the two steps we just saw randomly initialize the cluster centers ck given cluster centers determine points in each cluster for each point p find the closest ci put p into cluster i given points in each cluster solve for ci set ci to be the mean of points in cluster i if ci have changed repeat step properties will always converge to some solution can be a local minimum does not always find the global minimum of objective function source steve seitz k means l ask user how many cllusters they d nke e g k k means l ask user how clusters they d lliike e g k guess k clluster center locations k means ask user how many clusters they d like e g k s guess k cluster ce nt e r locat ions each dlatapoint finds out wh ich center it closest to thus each cente r a set of datapo ints l k means ask user how many clusters they d like e g k guess k cluster center locations each datapoint finds out which cent er it closest to each center finds the centroid of the points it owns o l segmentation as clustering depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on intensity similarity feature space intensity value d k k quantization of the feature space segmentation label map segmentation as clustering depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on intensity position similarity source k grauman x both regions are black but if we also include position x y then we could group the two into distinct segments way to encode both similarity proximity segmentation as clustering color brightness position alone are not enough to distinguish all regions source l lazebnik segmentation as clustering depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on texture similarity filter bank of filters feature space filter bank responses e g d source k grauman find textons by clustering vectors of filter bank outputs describe texture in a window based on texton histogram image texton map texton index texton index malik belongie leung and shi ijcv source l lazebnik the mean shift algorithm seeks modes or local maxima of density in the feature space image feature space l u v color values source k grauman search window center of mass mean shift vector source d hoiem cluster all data points in the attraction basin of a mode attraction basin the region for which all trajectories lead to the same mode slide by y ukrainitz b sarel fully connected graph node vertex for every pixel link between every pair of pixels p q affinity weight wpq for each link edge wpq measures similarity similarity is inversely proportional to difference in color and position a b c break graph into segments want to delete links that cross between segments easiest to break links that have low similarity low weight similar pixels should be in the same segments dissimilar pixels should be in different segments b normalized cut fix bias of min cut by normalizing for size of segments cut a b assoc a v cut a b assoc b v assoc a v sum of weights of all edges that touch a ncut value small when we get two clusters with many edges with high weights and few edges of low weight between them approximate solution for minimizing the ncut value eigenvalue problem shi and j malik cvpr source steve seitz difficulty of line fitting extra edge points clutter multiple models which points go with which line if any only some parts of each line detected and some parts are missing how to find a line that bridges missing evidence noise in measured edge points orientations how to detect true underlying parameters kristen grauman least squares line fitting data xn yn line equation yi m xi b find m b to minimize xi yi y mx b m m e n x y ap y i i b i b x y n n modified from svetlana lazebnik y b x m image space hough parameter space connection between image x y and hough m b spaces a line in the image corresponds to a point in hough space y b x m image space hough parameter space connection between image x y and hough m b spaces a line in the image corresponds to a point in hough space what does a point in the image space map to answer the solutions of b this is a line in hough space to go from image space to hough space given a set of points x y find all m b such that y mx b y b x m image space hough parameter space what are the line parameters for the line that contains both and it is the intersection of the lines b and b y b x m image space hough parameter space how can we use this to find the most likely parameters m b for the most prominent line in the image space let each edge point in image space vote for a set of possible parameters in hough space accumulate votes in discrete set of bins parameters with the most votes indicate line in image space problems with the m b space unbounded parameter domains vertical lines require infinite m alternative polar representation y sin each point x y will add a sinusoid in the parameter space hough transform hough machine analysis of bubble chamber pictures proc int conf high energy accelerators and instrumentation use a polar representation for the parameter space y x hough space x cos ysin initialize accumulator h to all zeros for each feature point x y in the image for θ to ρ x cos θ y sin θ h θ ρ h θ ρ end end find the value of θ ρ where h θ ρ is a local maximum the detected line in the image is given by ρ x cos θ y sin θ recall when we detect an edge point we also know its gradient direction but this means that the line is uniquely determined modified hough transform for each edge point x y θ gradient orientation at x y ρ x cos θ y sin θ h θ ρ h θ ρ end hough transform for circles circle center a b and radius r xi a yi b for a fixed radius r unknown gradient direction b hough space a hough transform for circles circle center a b and radius r xi a yi b for a fixed radius r unknown gradient direction intersection most votes for center occur here image space hough space ransac general form ransac loop randomly select a seed group of points on which to base model estimate fit model to these points find inliers to this model i e points whose distance from the line is less than t if there are d or more inliers re compute estimate of model on all of the inliers repeat n times keep the model with the largest number of inliers least squares fit randomly select minimal subset of points randomly select minimal subset of points hypothesize a model randomly select minimal subset of points hypothesize a model compute error function randomly select minimal subset of points hypothesize a model compute error function select points consistent with model randomly select minimal subset of points hypothesize a model compute error function select points consistent with model repeat hypothesize and verify loop randomly select minimal subset of points hypothesize a model compute error function select points consistent with model repeat hypothesize and verify loop uncontaminated sample randomly select minimal subset of points hypothesize a model compute error function select points consistent with model repeat hypothesize and verify loop randomly select minimal subset of points hypothesize a model compute error function select points consistent with model repeat hypothesize and verify loop alignment problem we have previously considered how to fit a model to image evidence e g a line to edge points in alignment we will fit the parameters of some transformation according to a set of matching feature pairs correspondences difficulties xi noise outliers xi kristen grauman and derek hoiem given matched points in a and b estimate the translation of the object x b x a tx i i i i t y tx ty problem outliers ransac solution x b x a tx sample a set of matching points pair i i solve for transformation parameters score parameters with number of inliers repeat steps n times i i t y tx ty problem outliers multiple objects and or many to one matches hough transform solution x b x a tx initialize a grid of parameter values i i each matched pair casts a vote for consistent values find the parameters with the most votes i i t y indexing local features inverted file index for text documents an efficient way to find all pages on which a word occurs is to use an index we want to find all images in which a feature occurs to use this idea we ll need to map our features to visual words kristen grauman extract some local features from a number of images e g sift descriptor space each point is dimensional each point is a local descriptor e g sift vector example each visual words group of patches belongs to the same visual word figure from sivic zisserman iccv adapted from kristen grauman inverted file index and bags of words similarity extract words in query inverted file index to find relevant frames compare word counts adapted from kristen grauman bags of visual words summarize entire image based on its distribution histogram of word occurrences analogous to bag of words representation commonly used for documents comparing bags of words rank frames by normalized scalar product between their possibly weighted occurrence counts nearest neighbor search for similar images 𝑠𝑖𝑚 𝑉 𝑑𝑗 𝑖 𝑞 𝑖 for vocabulary of v words j kristen grauman tf idf weighting term frequency inverse document frequency describe frame by frequency of each word within it downweight words that appear often in the database standard weighting for text retrieval number of occurrences of word i in document d number of words in document d total number of documents in database number of documents word i occurs in in whole database kristen grauman video google system collect all words within query region inverted file index to find relevant frames compare word counts spatial verification sivic zisserman iccv demo online at esearch vgoogle index html grauman b leibe scoring retrieval quality query database size images relevant total images results ordered precision relevant returned recall relevant total relevant recall db image with high bow similarity db image with high bow similarity both image pairs have many visual words in common db image with high bow similarity db image with high bow similarity only some of the matches are mutually consistent examples of parametric warps translation rotation aspect affine perspective p x y p x y transformation t is a coordinate changing machine p t p what does it mean that t is global is the same for any point p can be described by just a few numbers parameters let represent t as a matrix p mp x y x y what transformations can be represented with a matrix scaling x x x x sx x y y y sy y y rotate around x cos x sin y x cos sin x y sin x cos y y sin cos y shear x x shx y x shx x y sh x y y sh y y y homogeneous coordinates to convert to homogeneous coordinates homogeneous image coordinates converting from homogeneous coordinates homogeneous coordinates xx tx xx xx ttxx yy y y t ty y y tyy tx ty m xi yi xi x y m y i i i how many matches correspondence pairs do we need to solve for the transformation parameters once we have solved for the parameters how do we compute x new y new given xnew ynew where do the matches come from y send each pixel f x y to its corresponding location x y t x y in the second image q what if pixel lands between two pixels y x f x y x g x y send each pixel f x y to its corresponding location x y t x y in the second image q what if pixel lands between two pixels a distribute color among neighboring pixels x y known as splatting y get each pixel g x y from its corresponding location x y t x y in the first image q what if pixel comes from between two pixels y x f x y x g x y get each pixel g x y from its corresponding location x y t x y in the first image q what if pixel comes from between two pixels a interpolate color value from neighbors nearest neighbor bilinear help obtain a wider angle view by combining multiple images kristen grauman two images with rotation zoom but no translation derek hoiem camera center how to stitch together a panorama a k a mosaic basic procedure take a sequence of images from the same position rotate the camera about its optical center compute the homography transformation between second image and first transform the second image to overlap with the first blend the two together to create a mosaic if there are more images repeat modified from steve seitz a projective transform is a mapping between any two pps with the same center of projection rectangle should map to arbitrary quadrilateral parallel lines aren t but must preserve straight lines called homography wx wy x y w p h p alyosha efros xn yn xn yn to compute the homography given pairs of corresponding points in the images we need to set up an equation where the parameters of h are the unknowns p hp wx a b c x wy d e f y w g h i can set scale factor i so there are unknowns set up a system of linear equations ah b where vector of unknowns h a b c d e f g h t need at least eqs but the more the better solve for h if overconstrained solve using least squares min ah b help lmdivide x y image image canvas wx w wy w x y to apply a given homography h compute p hp regular matrix multiply wx wy x y convert p from homogeneous to image coordinates w modified from kristen grauman p h p homography example image rectification to unwarp rectify an image solve for homography h given p and p p hp derek hoiem stereo photography and stereo viewers kristen grauman goal recover depth by finding image coordinate x that corresponds to x x x z c baseline c b baseline line connecting the two camera centers epipoles intersections of baseline with image planes projections of the other camera center epipolar plane plane containing baseline epipolar lines intersections of epipolar plane with image planes always come in corresponding pairs note all epipolar lines intersect at the epipole epipolar constraint this is useful because it reduces the correspondence problem to a search along an epipolar line essential matrix x tx rx let e t x r x t ex e is called the essential matrix and it relates corresponding image points between both cameras given the rotation and translation if we observe a point in one image its position in other image is constrained to lie on line defined by above ex is the epipolar line through x in the first image corresponding to x note these points are in camera coordinate systems basic stereo matching algorithm for each pixel in the first image find corresponding epipolar scanline in the right image if necessary rectify the two stereo images to transform epipolar lines into scanlines search along epipolar line and pick the best match x compute disparity x x and set depth x f t x x correspondence search left right scanline matching cost disparity slide a window along the right scanline and compare contents of that window with the reference window in the left image matching cost ssd or normalized correlation image i x y disparity map d x y x y x d x y y image i x y what about when cameras optical axes are not parallel kristen grauman results with window search data derek hoiem window based matching ground truth projective structure from motion given m images of n fixed points xij pi xj i m j n problem estimate m projection matrices pi and n points xj from the mn corresponding points xij xj svetlana lazebnik photo synth cs intro to computer vision intro to machine learning visual recognition part i prof adriana kovashka university of pittsburgh october out of median mean due october due november up to points of extra credit overview of some methods for classification challenges and trade offs full system spatial pyramid match scene recognition bag of words feature vector descriptor representation recognition often involves classification classes categories hence classification categorization training learning a model e g classifier happens at training time from training data classification prediction happens at test time machine learning problems supervised learning unsupervised learni ng q cj cl q c c classification given a feature representation for images how do we learn a model for distinguishing features from different classes decision boundary zebra non zebra classification assign input vector to one of two or more classes any decision rule divides the input space into decision regions separated by decision boundaries example spam filter examples of categorization in vision part or object detection e g for each window face or non face scene categorization indoor vs outdoor urban forest kitchen etc action recognition picking up vs sitting down vs standing emotion recognition region classification label pixels into different object surface categories boundary classification boundary vs non boundary etc etc what do you see in this image trees bear camera man rabbit grass forest describe predict or interact with the object based on visual cues is it dangerous how fast does it run is it alive is it soft does it have a tail can i poke with it slide credit d hoiem cat vs dog object recognition caltech average object images fine grained recognition place recognition places database dating historical photos image style recognition slide credit d hoiem layout prediction assign regions to orientation geometric context assign regions to depth material recognition a farhadi i endres d hoiem and d forsyth describing objects by their attributes cvpr kovashka s vijayanarasimhan and k grauman actively selecting annotations among objects and attributes iccv kovashka d parikh and k grauman whittlesearch image search with relative attribute feedback cvpr generic categorization problem instance level recognition problem john car visual object categories basic level categories in human categorization rosch lakoff the highest level at which category members have similar perceived shape the highest level at which a single mental image reflects the entire category the level at which human subjects are usually fastest at identifying category members the first level named and understood by children the highest level at which a person uses similar motor actions for interaction with category members visual object categories basic level categories in humans seem to be defined predominantly visually there is evidence that humans usually start with basic level categorization before doing identification basic level categorization is easier and faster for humans than object identification how does this transfer to automatic abstract levels animal quadruped classification algorithms basic level dog cat cow german shepherd doberman individual level fido object categorization task description given a small number of training images of a category recognize a priori unknown instances of that category and assign the correct category label which categories are feasible visually fido german shepherd dog animal living being how many object categories are there source fei fei li rob fergus antonio torralba biederman other types of categories functional categories e g chairs something you can sit on other types of categories ad hoc categories e g something you can find in an office environment prototype or sum of exemplars prototype model exemplars model category judgments are made by comparing a new exemplar to the prototype category judgments are made by comparing a new exemplar to all the old exemplars of a category or to the exemplar that is the most appropriate slide credit torralba why recognition recognition a fundamental part of perception e g robots autonomous agents organize and give access to visual content connect to information detect trends and themes slide credit k grauman recognition a machine learning approach apply a prediction function to a feature representation of the image to get the desired output f apple f tomato f cow y f x output prediction function image feature training given a training set of labeled examples xn yn estimate the prediction function f by minimizing the prediction error on the training set testing apply f to a never before seen test example x and output the predicted value y f x training steps testing test image slide credit d hoiem and l lazebnik popular global image features raw pixels and simple functions of raw pixels oliva and torralba histograms bags of features hog dalal and triggs slide credit l lazebnik recognizing a beach recognizing cloth fabric recognizing a mug fine grained recognition what breed is this dog what are the right features depend on what you want to know object shape local shape info shading shadows texture scene geometric layout linear perspective gradients line segments material properties albedo feel hardness color texture action motion optical flow tracked points color l a b color space hsv color space texture filter banks or hog over regions histograms of descriptors sift lowe ijcv bag of visual words bag of visual words image patches cluster patches get codewords bow histogram codewords training steps testing test image slide credit d hoiem and l lazebnik recognition training data images in the training set must be annotated with the correct answer that the model is expected to produce motorbike slide credit l lazebnik datasets today imagenet categories images microsoft coco categories images pascal categories images sun categories images the pascal visual object classes challenge challenge classes person person animal bird cat cow dog horse sheep vehicle aeroplane bicycle boat bus car motorbike train indoor bottle chair dining table potted plant sofa tv monitor dataset size by 11 training validation images bounding boxes segmentations classification for each of the twenty classes predicting presence absence of an example of that class in the test image detection predicting the bounding box and label of each object from the twenty target classes in the test image segmentation generating pixel wise segmentations giving the class of the object visible at each pixel or background otherwise person layout predicting the bounding box and label of each part of a person head hands feet slide credit l lazebnik illumination object pose clutter occlusions intra class appearance viewpoint realistic scenes are crowded cluttered have overlapping objects challenges complexity thousands to millions of pixels in an image human recognizable object categories degrees of freedom in the pose of articulated objects humans billions of images indexed by google image search billion prints produced from digital camera images in million camera phones sold in about half of the cerebral cortex in primates is devoted to processing visual information felleman and van essen challenges learning with minimal supervision less cs intro to computer vision intro to machine learning visual recognition part ii prof adriana kovashka university of pittsburgh october homework push back to nov push back to nov make half length still of overall grade out nov due dec take out a small piece of paper and vote yes no on this proposal other announcements piazza can get participation credit by asking answering feedback from surveys plan for today overview of some methods for classification challenges and trade offs some translations feature vector descriptor representation recognition often involves classification classes categories hence classification categorization training learning a model e g classifier happens at training time from training data classification prediction happens at test time classification given a feature representation for images learn a model for distinguishing features from different classes decision boundary zebra non zebra slide credit l lazebnik image categorization cat vs dog image categorization object recognition caltech average object images image categorization place recognition places database region categorization material recognition recognition a machine learning approach the machine learning framework apply a prediction function to a feature representation of the image to get the desired output f apple f tomato f cow the machine learning framework y f x output prediction function image feature training given a training set of labeled examples xn yn estimate the prediction function f by minimizing the prediction error on the training set testing apply f to a never before seen test example x and output the predicted value y f x training steps board testing test image slide credit d hoiem and l lazebnik popular global image features raw pixels and simple functions of raw pixels oliva and torralba histograms bags of features hog dalal and triggs slide credit l lazebnik what kind of things do we compute histograms of color l a b color space hsv color space texture filter banks or hog over regions what kind of things do we compute histograms of histograms of descriptors sift lowe ijcv bag of visual words training steps testing test image slide credit d hoiem and l lazebnik recognition training data images in the training set must be annotated with the correct answer that the model is expected to produce motorbike challenges robustness illumination object pose clutter occlusions intra class appearance viewpoint challenges importance of context painter identification how would you learn to identify the author of a painting goya kirchner klimt marc monet van gogh plan for today visual recognition problems recognition pipeline features and data challenges one way to think about it training labels dictate that two examples are the same or different in some sense features and distances define visual similarity goal of training is to learn feature weights so that visual similarity predicts label similarity linear classifier confidence in positive label is a weighted sum of features what are the weights board we want the simplest function that is confidently correct supervised classification given a collection of labeled examples come up with a function that will predict the labels of new examples four nine training examples novel input how good is some function that we come up with to do the classification depends on mistakes made cost associated with the mistakes supervised classification given a collection of labeled examples come up with a function that will predict the labels of new examples consider the two class binary decision problem l loss of classifying a as a l loss of classifying a as a risk of a classifier is expected loss r pr using l pr using l we want to choose a classifier so as to minimize this total risk supervised classification optimal classifier will minimize total risk feature value x at decision boundary either choice of label yields same expected loss if we choose class four at boundary expected loss is p class is x l if we choose class nine at boundary expected loss is p class is x l so best decision boundary is at point x where p class is x l p class is x l supervised classification optimal classifier will minimize total risk feature value x at decision boundary either choice of label yields same expected loss to classify a new point choose class with lowest expected loss i e choose four if p x l p x l loss for choosing four loss for choosing nine disclaimers we will often assume the same loss for all possible types of misclassifications we won t always build probability distributions often we ll just find a decision boundary using discriminative methods what the simplest classifier you can think of nearest neighbor classifier training examples from class test example training examples from class f x label of the training example nearest to x all we need is a distance function for our inputs no training required k nearest neighbors classification for a new point find the k closest points from training data labels of the k points vote to classify black negative red positive k if query lands here the nn consist of negatives and positives so we classify it as negative nearest neighbor nearest neighbor nearest neighbor what are the tradeoffs of having a too large k too small k a nearest neighbor recognition example estimating geographic information from a single image james hays and alexei efros cvpr where in the world where in the world where in the world how much can an image tell about its geographic location nearest neighbors according to gist bag of sift color histogram a few others million geotagged photos by photographers slides james hays spatial envelope theory of scene representation oliva torralba slide credit aude olivia scene matches hays and efros estimating geographic information from a single image cvpr slides james hays slides james hays scene matches hays and efros estimating geographic information from a single image cvpr slides james hays eographic information from a single image c scene matches the importance of data hays and efros estimating geographic information from a single image cvpr slides james hays nearest neighbor classifier training examples from class test example training examples from class f x label of the training example nearest to x all we need is a distance function for our inputs no training required slide credit l lazebnik evaluating classifiers accuracy correctly classified all test examples precision recall precision retrieved positives retrieved recall retrieved positives positives f measure p r discriminative classifiers learn a simple function of the input features that correctly predicts the true labels on the training set 𝑦 𝑓 training goals accurate classification of training data correct classifications are confident classification function is simple slide credit d hoiem linear classifier what about this line find a linear function to separate the classes f x sgn wdxd sgn w x nn vs linear classifiers nn pros simple to implement decision boundaries not necessarily linear works for any number of classes nonparametric method nn cons need good distance function slow at test time large search problem to find neighbors storage of data linear pros low dimensional parametric representation very fast at test time linear cons works for two classes how to train the linear function what if data is not linearly separable prof adriana kovashka university of pittsburgh october homework due nov note you need to perform k means on features from multiple frames joined together not one k means per frame due nov half length still of grade out nov due dec plan for today support vector machines bias variance tradeoff scene recognition spatial pyramid matching the machine learning framework y f x output prediction function image feature training given a training set of labeled examples xn yn estimate the prediction function f by minimizing the prediction error on the training set testing apply f to a never before seen test example x and output the predicted value y f x nearest neighbor classifier training examples from class test example training examples from class f x label of the training example nearest to x all we need is a distance function for our inputs no training required k nearest neighbors classification for a new point find the k closest points from training data labels of the k points vote to classify black negative red positive k if query lands here the nn consist of negatives and positives so we classify it as negative scene matches hays and efros estimating geographic information from a single image cvpr slides james hays nearest neighbors pros and cons nn pros simple to implement decision boundaries not necessarily linear works for any number of classes nonparametric method nn cons need good distance function slow at test time large search problem to find neighbors storage of data adapted from l lazebnik discriminative classifiers learn a simple function of the input features that correctly predicts the true labels on the training set 𝑦 𝑓 training goals accurate classification of training data correct classifications are confident classification function is simple slide credit d hoiem linear classifier find a linear function to separate the classes f x sgn wdxd sgn w x svetlana lazebnik lines in let a w c x x y ax cy b lines in let a w c x x y ax cy b w x b lines in let a w c x x y ax cy b w x b lines in let a w c x x y ax cy b w x b d b distance from point to line lines in let a w c x y ax cy b w x b b w x b distance from d w point to line linear classifiers find linear function to separate positive and negative examples xi positive xi negative w b xi w b which line is best support vector machines discriminative classifier based on optimal separating line for case maximize the margin between the positive and negative training examples support vector machines want line that maximizes the margin xi positive yi xi w b xi negative yi xi w b support vectors margin for support vectors xi w b support vector machines want line that maximizes the margin xi positive yi xi w b xi negative yi xi w b for support vectors xi w b distance between point and line for support vectors xi w b w support vectors margin wτ x b m support vector machines want line that maximizes the margin xi positive yi xi w b xi negative yi xi w b for support vectors xi w b distance between point and line xi w b w support vectors margin therefore the margin is w finding the maximum margin line maximize margin w correctly classify all training data points xi positive yi xi w b xi negative yi xi w b quadratic optimization problem one constraint for each training point note sign trick finding the maximum margin line solution w i i yi xi finding the maximum margin line solution w i i yi xi b yi w xi for any support vector classification function f x sign w x b sign i i yi xi x b if f x classify as negative otherwise classify as positive notice that it relies on an inner product between the test point x and the support vectors xi solving the optimization problem also involves computing the inner products xi xj between all pairs of training points nonlinear svms datasets that are linearly separable work out great x but what if the dataset is just too hard x we can map it to a higher dimensional space nonlinear svms general idea the original input space can always be mapped to some higher dimensional feature space where the training set is separable nonlinear kernel example consider the mapping x x x y x y xy k x y xy the kernel trick the linear classifier relies on dot product between vectors k xi xj xi xj if every data point is mapped into high dimensional space via some transformation φ xi φ xi the dot product becomes k xi xj φ xi φ xj a kernel function is similarity function that corresponds to an inner product in some expanded feature space the kernel trick instead of explicitly computing the lifting transformation φ x define a kernel function k such that k xi xj φ xi φ xj examples of kernel functions linear k xi x j i j gaussian rbf k xi x j exp histogram intersection k xi x j min k xi k x j k allowing misclassifications misclassification cost data samples slack variable the w that minimizes maximize margin minimize misclassification what about multi class svms unfortunately there is no definitive multi class svm formulation in practice we have to obtain a multi class svm by combining multiple two class svms one vs others training learn an svm for each class vs the others testing apply each svm to the test example and assign it to the class of the svm that returns the highest decision value one vs one training learn an svm for each pair of classes testing each learned svm votes for a class to assign to the test example svms for recognition define your representation for each example select a kernel function compute pairwise kernel values between labeled examples use this kernel matrix to solve for svm support vectors weights to classify a new example compute kernel values between new input and support vectors apply weights check sign of output example learning gender with svms moghaddam and yang learning gender with support faces tpami moghaddam and yang face gesture learning gender with svms training examples males females experiment with various kernels select gaussian rbf k xi xj exp support faces moghaddam and yang learning gender with support faces tpami moghaddam and yang learning gender with support faces tpami gender perception experiment how well can humans do subjects people male female ages mid to mid test data face images males females low res and high res versions task classify as male or female forced choice no time limit gender perception experiment how well can humans do error error human vs machine svms performed better than any single human test subject at either resolution kristen grauman hardest examples for humans moghaddam and yang face gesture svms pros and cons pros many publicly available svm packages or use built in matlab version but slower kernel based framework is very powerful flexible often a sparse set of support vectors compact at test time work very well in practice even with very small training sample sizes cons no direct multi class svm must combine two class svms can be tricky to select best kernel function for a problem computation memory during training time must compute matrix of kernel values for every pair of examples learning can take a very long time for large scale problems adapted from lana lazebnik cs intro to computer vision interactive image search with attributes prof adriana kovashka university of pittsburgh november hours of video uploaded to youtube daily million photos uploaded to flickr daily million top level domains this data pool is too big to simply browse who did the witness see at the crime scene find all images depicting shiny objects which plant is this keywords work well for categories with known name unclear how to search without name or image keywords are not enough potential to communicate more precisely the desired visual content iteratively refine the set of retrieved images key questions how to open up communication how is interactive search done today keywords binary relevance feedback traditional binary feedback imprecise allows only coarse communication between user and system rui et al zhou et al tong chang cox et al ferecatu geman our idea search via comparisons like this but with curlier hair allow user to whittle away irrelevant images via comparative feedback on properties of results kovashka parikh and grauman cvpr visual attributes high level semantic properties shared by objects metallic long hair ornaments red we want to be able to infer something about unfamiliar objects if we can infer category names familiar objects new object cat horse dog farhadi et al cvpr slide credit derek hoiem we want to be able to infer something about unfamiliar objects if we can infer properties familiar objects new object has stripes has ears has eyes has four legs has mane has tail has snout brown muscular has snout has stripes like cat has mane and tail like horse has snout like horse and dog farhadi et al cvpr slide credit derek hoiem bright not bright smiling not smiling natural not natural we need ability to compare images by attribute strength bright at test time predict attribute strength of each database image input image features x output real valued attribute strength am x at training time learn a mapping between image features and attribute strength input pairs of ordered images with features output ranking functions am parikh and grauman iccv we want to learn a spectrum ranking model for an attribute e g brightness supervision from human annotators consists of ordered pairs similar pairs parikh and grauman iccv learn a ranking function image features learned parameters that best satisfies the constraints max margin learning to rank formulation image relative attribute score parikh and grauman iccv joachims kdd relative attributes we need ability to compare images by attribute strength bright kovashka parikh and grauman cvpr i want something more natural than this i want something less natural than this natural i want something with more perspective than this kovashka parikh and grauman cvpr query i want a bright open shoe that is short on the leg more open than selected fe round less orna round match en than datasets data from users shoes shoe images attributes pointy bright high heeled feminine etc osr scene images attributes natural perspective open air close depth etc pubfig face images attributes masculine young smiling round face etc whittlesearch results summary binary feedback represents status quo rui et al cox et al ferecatu geman whittlesearch finds relevant results faster than traditional binary feedback whittlesearch demo cktime player file edit vi ew wi ndow help biil o a j wed am q wld l find visual ly appealing shoes o ur recommendations are based on app ear anc e not d ick tracking womens girls mens boys users retrieve fonts that match requested attributes fonts sorted by relative attribute scores o donovan et al exploratory font selection using crowdsourced attributes siggraph the most relevant images might not be most informative existing active methods largely focus on binary relevance feedback and suffer from expensive selection procedures tong chang li et al cox et al ferecatu geman relative questions game select series of most informative visual comparisons that user should make to help deduce target kovashka and grauman iccv are the shoes you seek more or less feminine than traditional active approach using pivots pointy open bright x m feminine n pointy open bright m feminine pivots o mn comparisons total m n o m comparisons total how to account for ambiguity of search terms standard attribute learning one generic model formal not formal learn a generic model by pooling training data regardless of the annotator identity inter annotator disagreement treated as noise problem one model does not fit all binary attribute relative attribute there may be valid perceptual differences within an attribute yet existing methods assume monolithic attribute sufficient lampert et al cvpr farhadi et al cvpr branson et al eccv kumar et al pami scheirer et al cvpr parikh grauman iccv personalization from scratch formal not formal collect data from the user and train a classifier idea learn user specific attributes treat learning perceived attributes as an adaptation problem adapt generic attribute model with minimal user specific labeled examples kovashka and grauman iccv adapting binary attribute classifiers given user labeled data and generic model learn adapted model j yang et al icdm formal not formal sun attributes scene images attributes sailing hiking vacationing open area vegetation etc shoes 658 shoe images attributes pointy bright high heeled feminine etc result over datasets attributes and total users our user adaptive method most accurately captures perceived attributes with adapted attributes 20 shoes binary sun generic generic user exclusive user adaptive which images most influence adaptation open feminine sailing camping vegetation horizon far attribute model spectrum no personalization personalized to each user assumes all users have the same notion of the attribute assumes each user has a unique notion of the attribute robustness to noise via majority vote no robustness to noise problem user specific extreme different groups of users might subscribe to different shades of meaning of an attribute term how can we discover these shades automatically color is our idea discovering shades of attributes discover schools of thought among users based on latent factors behind their use of attribute terms allows discovery of the attribute shades of meaning kovashka and grauman ijcv show users definitions but no example images ask them to label images with presence of attribute also ask for explanations of some responses is the attribute open present in the image given a partially observed attribute specific label matrix want to recover its latent factors via matrix factorization gives a representation of each user attribute open cluster users in the space of latent factor representations use k means select k automatically gives a representation of each shade for this attribute approach using shades to predict attributes results accuracy of attribute prediction user adp kovashka and grauman iccv attr disc rastegari et al eccv shades make personalization more robust shades advantage transfers to more accurate search liu and kovashka wacv idea learn model for new domain by adapting a model for an existing domain idea rely on features that are similar between the two domains to make the adaptation possible liu and kovashka wacv liu and kovashka wacv a naïve adaptation method fails to outperform a method that learns from scratch but feature selection makes our method most useful when data is scarce prof adriana kovashka university of pittsburgh november today window based generic object detection basic pipeline boosting classifiers face detection as case study basic framework build train object model choose a representation learn or fit parameters of model classifier generate candidates in new image score the candidates representation choice part based simple holistic descriptions of image content grayscale color histogram vector of pixel intensities pixel based representations sensitive to small shifts color or grayscale based appearance description can be sensitive to illumination and intra class appearance variation summarize local distribution of gradients with histogram locally orderless offers invariance to small shifts and rotations contrast normalization try to correct for variable illumination given the representation train a binary classifier noy enso tcaarc ar discriminative classifier construction slide adapted from antonio torralba window based models generating and scoring candidates window based object detection recap training obtain training data define features define classifier given new image slide window score by classifier feature extraction face detection and recognition sally challenges of face detection sliding window detector must evaluate tens of thousands of location scale combinations faces are rare per image a megapixel image has pixels and a comparable number of candidate face locations for computational efficiency we should try to spend as little time as possible on the non face windows to avoid having a false positive in every image our false positive rate has to be less than viola jones face detector weak classifier weights increased weak classifier weights increased weak classifier final classifier is a combination of weak classifiers boosting training initially weight each training example equally in each boosting round find the weak learner that achieves the lowest weighted training error raise weights of training examples misclassified by current weak learner compute final classifier as linear combination of all weak learners weight of each learner is directly proportional to its accuracy exact formulas for re weighting and combining weak learners depend on the particular boosting scheme e g adaboost viola jones face detector main idea represent local texture with efficiently computable rectangular features within window of interest select discriminative features to be weak classifiers use boosted combination of them as final classifier form a cascade of such classifiers rejecting clear negatives quickly viola jones detector features rectangular filters feature output is difference between adjacent regions value pixels in white area pixels in black area efficiently computable with integral image any sum can be computed in constant time value at x y is sum of pixels above and to the left of x y integral image source result the integral image computes a value at each pixel x y that is the sum of the pixel values above and to the left of x y inclusive this can quickly be computed in one pass through the image cumulative row sum x y x y i x y integral image ii x y ii x y x y computing sum within a rectangle let a b c d be the values of the integral image at the corners of a then the sum of original image values within the rectangle can be computed as sum a b c d only additions are required for any size of rectangle viola jones detector features considering all possible filter parameters position scale and type possible features associated with each x window which subset of these features should we use to determine if a window has a face use adaboost both to select the informative features and to form the classifier viola jones detector adaboost want to select the single rectangle feature and threshold that best separates positive faces and negative non faces training examples in terms of weighted error resulting weak classifier outputs of a possible rectangle feature on faces and non faces for next round reweight the examples according to errors choose another filter threshold combo adaboost algorithm start with uniform weights on training examples for m rounds xn evaluate weighted error for each feature pick best classifier re weight the examples incorrectly classified more weight correctly classified less weight final classifier is combination of weak ones weighted according to error they had slide adapted from k grauman figure from szeliski boosting for face detection first two features selected by boosting this feature combination can yield detection rate and false positive rate boosting pros and cons advantages of boosting integrates classification with feature selection complexity of training is linear in the number of training examples flexibility in the choice of weak learners boosting scheme testing is fast easy to implement disadvantages needs many training examples often found not to work as well as an alternative discriminative classifier support vector machine svm are we done even if the filters are fast to compute each new image has a lot of possible windows to search how to make the detection more efficient cascading classifiers for detection form a cascade with low false negative rates early on apply less accurate but faster classifiers first to immediately discard windows that clearly appear to be negative we start with simple classifiers which reject many of the negative sub windows while detecting almost all positive sub windows positive response from the first classifier triggers the evaluation of a second more complex classifier and so on a negative outcome at any point leads to the immediate rejection of the sub window the detection rate and the false positive rate of the cascade are found by multiplying the respective rates of the individual stages a detection rate of and a false positive rate on the order of can be achieved by a stage cascade if each stage has a detection rate of and a false positive rate of about example problem decide whether to wait for a table at a restaurant based on the following attributes alternate is there an alternative restaurant nearby bar is there a comfortable bar area to wait in fri sat is today friday or saturday hungry are we hungry patrons number of people in the restaurant none some full price price range raining is it raining outside reservation have we made a reservation type kind of restaurant french italian thai burger waitestimate estimated waiting time train with positives negatives real time detector using layer cascade features in all layers adapted from kristen grauman a seminal approach to real time object detection training is slow but detection is very fast key ideas integral images for fast feature evaluation boosting for feature selection attentional cascade of classifiers for fast rejection of non face windows p viola and m jones cvpr viola and m jones ijcv kristen grauman example using viola jones detector frontal faces detected and then tracked character names inferred with alignment of script and subtitles everingham m sivic j and zisserman a hello my name is buffy automatic naming of characters in tv video bmvc et asia re ttdlnology means blli i n ss home i in sight ireviews i t ec ttgu i des ijobs ibiogs ivideos i comm unity idownloads iit l jibrary i so ft w ar e hardw ar e secur ity communicat ion bus iness in t e rn et phato i sea rch zdnet sia new i nt ern et google now view by elin or mills cnet news com frid ay a u gust pt i goo g le h a gotl e a lo t o f fl ac k fro m p ri va cy ad ro c at e fo r phot p hrirngi f ac e ancll new fro m co unt ri esjreg ion i hailand i indon es ia wh at ilo t latest new is ebay fa cingi eller rev oit i a ia pac ifii l i o e n e pl at e n umbe r an d d i spl ayi t h e m t h e s t r e et vi ew i n goo maps o liig i n all y t h e c o rnpany said o nly p e o p l e v lh o i d e n t i fi ed u e m se v e c o u a sk th e c o mp any to r e m o v e t h ei r im a g e but oo gle h as quietly changed poli cy partly in res pon se to critici sm and now anyon e can alert the comp any and have an im age of a li cen se plate or a rec ognizable face rem ove d not just the o er of th e face or car ays mariss a n a yer vi ce pr esident of sea rch pro ducts and us er exp erien ce at oog le repo rt a maz on may again b e mullin p netflix bu mo z illa map out jetp ac k add an tr ans ition pl an ga a gle begins earch far middl e east labby ist goo gle st ill thinks it ca n chang e china advertisement broug th a good poli cy for us er and also clarifies the intent of th e prod uc t she said in an inte f i e n foll o i i n g her keynot e at the se arch engin e strategies conf erence in sa n j os e calif n ednes day th e poli cy ch ang e l a m a de a bo ut da y after the launch of the prod uct in la te rl a y but l a not publi cly announ ced ac cord ing to r yer the com pany i rem oving im ages on ly w hen orn e on e notifi e them and not pro a ctively she said wa defin itely a big po li cy change in side cisco colloboratlon soluf rtorr face detection and recognition sally can be trained to recognize pets slide credit lana lazebnik surveillance derek hoiem cs intro to computer vision detection ii deformable part models prof adriana kovashka university of pittsburgh november today object category detection window based approaches review viola jones detector dalal triggs pedestrian detector part based approaches implicit shape model deformable parts model object category detection focus on object search where is it build templates that quickly differentiate object patch from background patch dog or non dog derek hoiem rectangular filters feature output is difference between adjacent regions value pixels in white area pixels in black area efficiently computable with integral image any sum can be computed in constant time value at x y is sum of pixels above and to the left of x y integral image adapted from kristen grauman and lana lazebnik considering all possible filter parameters position scale and type possible features associated with each x window which subset of these features should we use to determine if a window has a face use adaboost both to select the informative features and to form the classifier viola jones detector adaboost want to select the single rectangle feature and threshold that best separates positive faces and negative non faces training examples in terms of weighted error resulting weak classifier outputs of a possible rectangle feature on faces and non faces for next round reweight the examples according to errors choose another filter threshold combo cascading classifiers for detection demo form a cascade with low false negative rates early on apply less accurate but faster classifiers first to immediately discard windows that clearly appear to be negative extract fixed sized pixel window at each position and scale compute hog histogram of gradient features within each window score the window with a linear svm classifier perform non maxima suppression to remove overlapping detections with lower scores navneet dalal and bill triggs histograms of oriented gradients for human detection non max suppression adapted from derek hoiem map each grid cell in the input window to a histogram counting the gradients per orientation train a linear svm using training set of pedestrian vs non pedestrian windows code available slide by pete barnum kristen grauman navneet dalal and bill triggs histograms of oriented gradients for human detection histogram of gradient orientations orientation bins for unsigned angles histograms in pixel cells votes weighted by magnitude bilinear interpolation between cells slide by pete barnum navneet dalal and bill triggs histograms of oriented gradients for human detection cells pos w neg w pedestrian single rigid template usually not enough to represent a category many objects e g humans are articulated or have parts that can vary in configuration many object categories look very different from different viewpoints or from instance to instance slide by n snavely images from caltech images from d ramanan dataset define object by collection of parts modeled by appearance spatial configuration one extreme fixed template object model sum of scores of features at fixed positions non object object another extreme bag of words star shaped model star shaped model articulated parts model object is configuration of parts each part is detectable and can move around adapted from derek hoiem images from felzenszwalb visual vocabulary is used to index votes for object position a visual word part training image annotated with object localization info visual codeword with displacement vectors learning in computer vision lana lazebnik visual vocabulary is used to index votes for object position a visual word part test image learning in computer vision build vocabulary of patches around extracted interest points using clustering build vocabulary of patches around extracted interest points using clustering map the patch around each interest point to closest word build vocabulary of patches around extracted interest points using clustering map the patch around each interest point to closest word for each word store all positions it was found relative to object center given new test image extract patches match to vocabulary words cast votes for possible positions of object center search for maxima in voting space example results on cows k grauman b leibe detection results qualitative performance recognizes different kinds of objects robust to clutter occlusion noise low contrast k grauman b leibe root filter part filters deformation weights multiple components the score of a hypothesis is the sum of appearance scores minus the sum of deformation costs subwindow n features n displacements score p p f h p d dx dy n i i i i i i i i i appearance weights deformation weights training data consists of images with labeled bounding boxes need to learn the filters and deformation parameters f x w h x w are model parameters z are latent hypotheses latent svm training initialize w and iterate fix w and find the best z for each training example fix z and solve for w standard svm training component component object detection system overview our system takes an input image extracts around bottom up region proposals computes features for each proposal using a large convolutional neural network cnn and then classifies each region using class specific linear svms r cnn achieves a mean average precision map of on pascal voc for comparison uijlings et al report map using the same region proposals but with a spatial pyramid and bag of visual words approach the popular deformable part models perform at lana lazebnik summary window based approaches assume object appears in roughly the same configuration in different images look for alignment with a global template part based methods allow parts to move somewhat from their usual locations look for good fits in appearance for both the global template and the individual part templates next time analyzing and debugging detection methods cs intro to computer vision detection iii analyzing and debugging detection methods prof adriana kovashka university of pittsburgh november today in what ways does detection fail how can we visualize features and models define object by collection of parts modeled by appearance spatial configuration star shaped model build vocabulary of patches around extracted interest points using clustering map the patch around each interest point to closest word for each word store all positions it was found relative to object center given new test image extract patches match to vocabulary words cast votes for possible positions of object center search for maxima in voting space bin gradients from pixel neighborhoods into orientations dalal triggs cvpr root filter part filters deformation weights p felze lana lazebnik the score of a hypothesis is the sum of appearance scores minus the sum of deformation costs subwindow n features n displacements score p p f h p d dx dy n i i i i i i i i i appearance weights deformation weights adapted from lana lazebnik what is an object b alexe t deselaers and v ferrari computer vision and pattern recognition cvpr speeding up detection restrict set of windows we pass through svm to those w high objectness objectness cue where people look objectness cue color contrast at boundary objectness cue no segments straddling the object box boxes found to have high objectness cyan ground truth bounding boxes yellow correct and red incorrect predictions for objectness only run the sheep horse chair etc classifier on the yellow red boxes today review deformable part models how can we speed up detection how can we visualize features and detections diagnosing error in object detectors d hoiem y chodpathumwan and q dai european conference on computer vision eccv intra class variation for airplane occlusion shape viewpoint distance confusing distractors for airplane background similar categories dissimilar categories localization error other objects similar objects bird boat car background other objects additional annotations for seven categories occlusion level parts visible sides visible occlusion poor robustness to occlusion but little impact on overall performance easier none harder heavy size strong preference for average to above average sized airplanes large medium x large small small aspect ratio better at detecting wide side views than tall views x wide wide medium x tall tall easier wide harder tall sides parts best performance direct side view with all parts visible easier side harder non side most errors that detectors make are reasonable localization error and confusion with similar objects misdetection of occluded or small objects detectors have different sensitivity to different factors e g less sensitive to truncation than to size differences code and annotations are available online today review deformable part models how can we speed up detection in what ways does detection fail hoggles visualizing object detection features c vondrick a khosla t malisiewicz and a torralba international conference on computer vision iccv why did the detector fail car vondrick et al iccv what information is lost vondrick et al iccv what information is lost vondrick et al iccv recovering image from neighbors image hog top detections vondrick et al iccv recovering image from neighbors image hog top detections vondrick et al iccv recovering image from neighbors image hog top detections vondrick et al iccv recovering image from neighbors image hog top detections vondrick et al iccv better recovery using paired dictionary vondrick et al iccv a microscope to view hog more intuitive vondrick et al iccv vs human vision hog vision vondrick et al iccv vondrick et al iccv vondrick et al iccv vondrick et al iccv vondrick et al iccv vondrick et al iccv vondrick et al iccv the hoggles chal enge humans detect dpms detect vondrick et al iccv the hoggles challenge humans miss dpm miss vondrick et al iccv chair detections vondrick et al iccv chair detections vondrick et al iccv car detections vondrick et al iccv car detections vondrick et al iccv hog human detector chair loss due to rgb hog recall hog dpm hog human rgb human vondrick et al iccv why did the detector fail car vondrick et al iccv why did the detector fail car vondrick et al iccv why did the detector fail car vondrick et al iccv visualizing learned models car person bottle bicycle motorbike chair tv horse vondrick et al iccv we can speed up object detection by using the notion of objectness to prune windows unlikely to contain any object some failure modes are more important than others and fixing them could increase the overall detection performance even humans cannot produce correct classifications with imperfect features cs intro to computer vision human factors crowdsourcing labels gaze saliency prof adriana kovashka university of pittsburgh november today games with a purpose esp game peekaboom visual saliency predict where people will look amazon mechanical turk workers annotation protocols type keywords select relevant images click on landmarks outline something anything else custom annotations x large scale low price g a m azon m echan i ca l turk x amazonmechanical turk artificial artificial intelligence your account hits qualificat ions introduction i dashboard i status i account settings mechanical turk is a marketplace for work d already have an account sign in as a worker i requester we give businesses and developers access to an on demand scalable workforce workers select from mousands of tasks a a work whenever it convenient hits availab le view them now make money by working on hits h ts hum an i ntelligence tasks are individual tasks that you work on find hits no w as a mechanical turk worker you can work from home choose your own work h ours get paid for doing good w ork ge resul from mechanical turk workers ask workers to complete hits human i ntelligence tasks and get r esult u sing mechanical turk get started as a mechanical turk requester you have access to a global on demand x workforce get thousands of hits completed in minutes pay only when you re satisfied wit h th e results find an interesting task work earn money find hits now fund your load your account tasks get started get results or l earn more about being a worker faq i co ntact us i ca r eers at mec h anical turk i dev elop ers i press i policies i state licensing i blog i se rv ice hea lt h d ashbo ar d ama w n com i n c or its aff ili at es an ama on com company issues quality how good is it how to be sure price how to price it ensuring annotation quality consensus multiple annotation wisdom of the crowd qualification exam gold standard questions grading tasks a second tier of workers who grade others crowdsourcing object bounding boxes pricing trade off between throughput and cost higher pay can actually attract scammers some studies find that the most accurate results are achieved if turkers do tasks for free c p q q o u cl u q ti i o of rrect r tect io n o c error p u o u u i error cl ti s c i error error f rate o correct n c a ii t eriror er ror i er ror ii c i o er ror lo c p u q o q c e rro r e rro r i erro r o c error p u dj dj o u dj e rro r u dj c c i e rro r l o rate of rrect r tect io n o c 6 e rro r p q q e rro r o u q u 6 q c e rro r e rro r 2rate of rrect r tectio n o c p u t o 6 6 eriror i error q c adver aries i error error rateof rrect r tect io n o today collecting annotations from humans mturk pipeline and challenges visual saliency predict where people will look luis von ahn associate professor at cmu one of the fathers of crowdsourcing created the esp game peekaboom and several other games with a purpose two player online game partners don t know each other and can t communicate object of the game type the same word the only thing in common is an image luis von ahn and laura dabbish labeling images with a computer game chi player player guessing car guessing boy guessing hat guessing kid success you agree on car guessing car success you agree on car the esp game is fun million labels with players there are many people that play over hours a week labeling the entire web people playing simultaneously can label all images on google in days individual games in yahoo and msn average over 000 players at a time a few million labels can improve image search can be used to improve computer vision single player game a single person can play with pre recorded actions as their partner 12 car hat 08 boy we emulate partner by playing pre recorded moves 21 kid 23 car notice that this when people play we record every action with timing information doesn t stop the labeling process what about cheating if a pair plays too fast we don t record the words they agree on what about cheating we give players test images for which we know all the common labels we only store a player s guesses if they successfully label the test images why do people like the esp game the esp game gives its players a weird and beautiful sense of anonymous intimacy on the one hand you have no idea who your partner is on the other hand the two of you are bringing your minds together in a way that lovers would envy strangely addictive it s so much fun tryng to guess what others think you have to step outside of yourself to match it s fast paced helps me learn english there are many fascinating things about the esp game how it uses a game structure to build an index of images on the web what words become taboo from frequent use how interesting some of the images pulled randomly from the web are but by far the most intriguing aspect of the game is how often your random unknown partner is a complete idiot other games locating objects in images the esp game tells us if an image contains a specific object but doesn t say where in the image the object is such information would be extremely useful for computer vision research players shoot at objects on the image shoot the car we give points and check accuracy by giving players images for which we already know where the object is x of images x of images don t know guesser revealer guess car bcraursh partner s guess the revealer clicks on parts of the image and shows them to the guesser the guesser guesses flower petal butterfly server correct butterfly why peekaboom works by getting the guesser to guess correctly the revealer locates objects by clicking on the relevant parts of the image summary collecting annotations from humans crowdsourcing allows very cheap data collection getting high quality annotations can be tricky but there are many ways to ensure quality one way to obtain high quality data fast is by phrasing your data collection as a game today collecting annotations from humans mturk pipeline and challenges games with a purpose esp game peekaboom natural images from flickr and labelme tilke judd chris thomas eye tracking experiments screen resolution each image shown for seconds user rests head in chin rest eye tracker measures location of eye fixation several times a second people free viewed the images actually done in dark room calibrate check every 50 images divide the experiment into two sessions of randomly ordered images initial fixation is discarded tilke judd chris thomas how do eye trackers work just convolve a gaussian over the fixation positions can do this for a single person or an aggregate of many people can also threshold choose top n yields a binary map labeled faces text with bounding box and horizon line if any based on the annotations fixation on faces on text of fixations are within the center of the image animals cars human body parts features labels predict where people will look for a new image by running model on every pixel of the image saliency model features illumination color orientation horizon line face detector viola jones person detector dpm distance to center salient samples from top non salient samples from bottom from each of training images learns weights for each feature that best predict a saliency label for each pixel non photorealistic rendering application saliency map from fixations of one person is thresholded to become a binary classifier if pixel at x y over threshold otherwise by varying the threshold we can get a roc curve receiver operating characteristic curve average over all users and all images receiver operating characteristic curve percent salient human fixations white cloud predicted saliency strength red dots true human fixations receiver operating characteristic curve thresholded saliency map percent salient we calculate the percentage of fixations that lie within the salient portion of the map white cloud predicted saliency strength red dots true human fixations receiver operating characteristic curve thresholded saliency map percent salient we calculate the percentage of fixations that lie within the salient portion of the map white cloud predicted saliency strength red dots true human fixations receiver operating characteristic curve thresholded saliency map we calculate the percentage of fixations that lie within the salient portion of the map white cloud predicted saliency strength red dots true human fixations receiver operating characteristic curve thresholded saliency map percent salient we calculate the percentage of fixations that lie within the salient portion of the map summary visual saliency collect human gaze data and train a classifier to predict which pixels will be fixated fixated regions interesting regions can use this to restrict region of image that we want to examine e g for object detection also useful for graphics applications prof adriana kovashka university of pittsburgh november announcements homework due tonight just save some positive detections not all homework out due december worth points total i e half the work still of final grade up to points of extra credit fixations for one person fixation map just convolve a gaussian over the fixation positions can do this for a single person or an aggregate of many people can also threshold choose top n yields a binary map where do people actually look labeled faces text with bounding box and horizon line if any based on the annotations fixation on faces on text of fixations are within the center of the image animals cars human body parts learning a classifier from eye tracking data features labels predict where people will look for a new image by running model on every pixel of the image saliency model features illumination color orientation horizon line face detector viola jones person detector dpm distance to center salient samples from top non salient samples from bottom from each of training images learns weights for each feature that best predict a saliency label for each pixel evaluating saliency maps amazon mechanical turk workers 01 the esp game two player online game partners don t know each other and can t communicate object of the game type the same word the only thing in common is an image luis von ahn and laura dabbish labeling images with a computer game chi the esp game player player guessing car guessing boy guessing hat guessing kid success you agree on car guessing car success you agree on car revealing images guesser revealer guess car bcraursh partner s guess using gaze to collect object bounding boxes viewing task was to determine which object category is shown compute a bounding box from locations that were fixated by users faster than drawing bounding boxes papadopoulos et al training object class detectors from eye tracking data eccv learning attributes using human gaze use human gaze to learn where attributes live to learn attribute models extract features only from fixated regions today what to do when data is expensive to obtain human in the loop recognition choose labeling questions at test time develop unsupervised methods that don t require labels next time overview of recent research concepts will go fast crowdsourcing training james hays active learning traditional active learning reduces supervision by obtaining labels for the most informative or uncertain examples first mackay freund et al tong koller lindenbaum et al kapoor et al multi level active learning choose not only which images to label but at what level to label them weak labels informing about presence of an object strong labels outlines demarking the object stronger labels informing about labels of parts of objects approach multi level active visual learning best use of manual resources may call for combination of annotations at different levels choice must balance cost of varying annotations with their information gain requirements the approach requires a classifier that can deal with annotations at multiple levels an active learning criterion to deal with multiple types of annotation queries variable cost associated with different queries results 100 92 category ajaxorange 40 category apple 60 20 40 category banana 20 40 cost cost cost category checkeredscarf category cokecan 93 category dirtyworkgloves 42 sample learning curves per class each averaged over five trials multi level active selection performs the best for most classes today what to do when data is expensive to obtain use data intelligently only label useful data active learning choose labeling questions at training time develop unsupervised methods that don t require labels next time human in the loop recognition test image testing adapted from james hays visual recognition with humans in the loop eccv crete greece steve branson catherine wah florian schroff boris babenko serge belongie peter welinder pietro perona field guides difficult for average users computer vision doesn t work perfectly yet research mostly on basic level categories what type of bird is this visual recognition with humans in the loop parakeet auklet motivation supplement visual recognition with the human capacity for visual feature extraction to tackle difficult fine grained recognition problems typical progress is viewed as increasing data difficulty while maintaining full autonomy here the authors view progress as reduction in human effort on difficult data brian o neill categories of recognition basic level subordinate parts attributes easy for humans hard for computers hard for humans hard for computers easy for humans hard for computers visual 20 questions game hard classification problems can be turned into a sequence of easy ones recognition with humans in the loop computers reduce number of required questions humans drive up accuracy of vision algorithms implementation assembled visual questions encompassing visual attributes extracted from mechanical turk users asked to answer questions and provide confidence scores brian o neill example questions example questions example questions basic algorithm input image x computer vision max expected information gain question a no p c x is the belly black max expected information gain question a yes p c x is the bill hooked p c x some definitions q qn set of possible questions board ai ai ri v possible answers to question i possible confidence in answer i guessing probably definitely ui u t ai ri user response history of user responses at time t question selection board seek the question e g what color is the belly of the bird that gives the maximum information gain entropy reduction given the image and the set of previous user responses i c u x u t p u x u t h c x u u t h c x u t i i i ui ai v probability of obtaining response ui to evaluated question given image and response history entropy when response is added to history entropy at this iteration before response to evaluated question is added to history where h c x u t c p c x u t log p c x u t birds dataset classes images binary attributes black footed albatross groove billed ani parakeet auklet field sparrow vesper sparrow arctic tern forster tern common tern baird sparrow henslow sparrow results users drive performance fewer questions asked if cv used just computer vision examples user input helps correct computer vision magnolia warbler common yellowthroat computer vision common yellowthroat is the breast pattern solid no definitely magnolia warbler summary human in the loop training and testing to make intelligent use of the human labeling effort during training have the computer vision algorithm learn actively by selecting those questions that are most informative to combine strengths of human and imperfect vision algorithms use a human in the loop at recognition time prof adriana kovashka university of pittsburgh december today deep neural networks architectures and basic operations applications visualizing deep neural networks breaking deep learning packages take model trained on e g imagenet training set take outputs of or layer optional fine tune features and or classifier on new dataset to train model from scratch need lots of data classify test set of new dataset why now we have lots of data and deep nets can be trained in reasonable time with gpus language deep learning deep neural nets convolutional neural nets image video pixels object class features are key to recent progress in recognition but flawed where next better classifiers or keep building more features what about learning the features learn a feature hierarchy all the way from pixels to classifier each layer extracts features from the output of previous layer train all layers jointly image video pixels simple classifier shallow vs deep architectures traditional recognition shallow architecture image video pixels object class image video pixels deep learning deep architecture object class background perceptrons input weights d background multi layer neural networks nonlinear classifier training find network weights w to minimize the error between true training labels yi and estimated labels fw xi n e w yi i fw xi minimization can be done by gradient descent provided f is differentiable this training method is called back propagation biological neuron and perceptrons a biological neuron an artificial neuron perceptron a linear classifier jia bin huang hubel wiesel architecture d hubel and t wiesel nobel prize visual cortex consists of a hierarchy of simple complex and hyper complex cells hierarchy of feature detectors in the visual cortex with higher level features responding to patterns of activation in lower level cells and propagating activation upwards to still higher level cells lana lazebnik jia bin huang hubel wiesel architecture and multi layer neural network hubel and weisel architecture multi layer neural network a non linear classifier jia bin huang today deep neural networks background applications visualizing deep neural networks breaking deep learning packages neural network with specialized connectivity structure stack multiple stages of feature extractors higher stages compute more global more invariant more abstract features classification layer at the end adapted from rob fergus convolve input with learned filters apply non linearity spatial pooling downsample normalization optional supervised training of convolutional filters by back propagating classification error adapted from lana lazebnik convolution apply learned filter weights one feature map per filter stride can be greater than faster less memory non linearity per element independent options tanh sigmoid exp x rectified linear unit relu simplifies backpropagation makes learning faster avoids saturation issues preferred option works well spatial pooling sum or max non overlapping overlapping regions role of pooling invariance to small transformations larger receptive fields see more of input normalization within or across feature maps before or after spatial pooling feature maps feature maps after contrast normalization image pixels lowe ijcv feature vector training convolutional neural networks backpropagation stochastic gradient descent initialization transfer learning dropout data augmentation randomly turn off some neurons allows individual neurons to independently be responsible for performance dropout a simple way to prevent neural networks from overfitting create virtual training samples horizontal flip random crop color casting geometric distortion deep image today deep neural networks background architectures and basic operations visualizing deep neural networks breaking deep learning packages handwritten text digits mnist error ciresan et al arabic chinese ciresan et al simpler recognition benchmarks cifar error wan et al traffic sign recognition error vs for humans ciresan et al but until recently less good at more complex datasets caltech few training examples deng et al cvpr million labeled images classes images gathered from internet human labels via amazon turk challenge million training images classes alexnet similar framework to lecun but bigger model hidden layers units 000 000 params more data vs images gpu implementation speedup over cpu trained on two gpus for a week better regularization for training dropout krizhevsky et al error top next best non convnet error imagenet challenge best non convnet in jia bin huang caltech samples per class caltech ucsd birds decaf sun dataset decaf mit indoor scenes dataset overfeat j donahue y j a razavian h azizpour j sullivan lana lazebnik object detection system overview our system takes an input image extracts around bottom up region proposals computes features for each proposal using a large convolutional neural network cnn and then classifies each region using class specific linear svms r cnn achieves a mean average precision map of on pascal voc for comparison uijlings et al report map using the same region proposals but with a spatial pyramid and bag of visual words approach the popular deformable part models perform at lana lazebnik improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned jia bin huang learning and transferring mid level image representations using convolutional neural networks detection segmentation regression pose estimation matching patches synthesis and many more jia bin huang who took this photograph deep net features achieve accuracy chance is less than human performance is method learns what proto objects scenes authors shoot can use this to develop field guides for human use can generate novel photographs by given author some misclassifications dead pho tog ra p hers work x th stach lot i i i i i university of pittsburgh emtler at pitt compute r scientists have resurrected fa med photographe rs world new t o g tr o mdead photo graphers with convolutional neur al networks artin anderson thu nov researchers out of the university of p irtt sbur gh have used convolutional neurail networks cnns to id entify th e st ran g e obsessions and unique styles o f weu known photographers and even to generate new photographs whkh accord with their unique today deep neural networks background architectures and basic operations applications packages patches from validation images that give maximal activation of a given feature map layer layer visualizing and understanding convolutional networks layer layer and visualizing and understanding convolutional networks lana lazebnik lana lazebnik diagnosing problems visualization of krizhevsky et al architecture showed some problems with layers and large stride of used alter architecture smaller stride filter size visualizations look better performance improves lana lazebni architecture of krizhevsky et al layers total trained on imagenet top error layer conv pool lana lazebnik remove top fully connected layer layer drop million parameters only drop in performance layer conv pool lana lazebnik remove both fully connected layers layer drop million parameters drop in performance layer conv pool lana lazebnik now try removing upper feature extractor layers layers drop million parameters drop in performance layer conv pool layer conv pool layer conv pool lana lazebnik input image now try removing upper feature extractor layers fully connected layers now only layers drop in performance layer conv pool layer conv pool depth of network is key layer conv pool lana lazebnik input image tapping off features at each layer plug features from each layer into linear svm features are neuron activations at that level adapted from matt zeiler jia bin huang intriguing properties of neural networks jia bin huang deep neural networks are easily fooled high confidence predictions for unrecognizable images fooling a linear classifier to fool a linear classifier add a small multiple of the weight vector to the training example x x αw jia bin huang today deep neural networks background architectures and basic operations applications visualizing deep neural networks breaking deep learning new version of decaf things to remember overview neuroscience perceptron multi layer neural networks convolutional neural network cnn convolution nonlinearity max pooling cnn for classification and beyond understanding and visualizing cnn find images that maximize some class scores visualize individual neuron activation and input patterns breaking cnns training cnn dropout data augmentation transfer learning using cnns for your own task basic first step try the pre trained caffenet layers as features adapted from jia bin huang prof adriana kovashka university of pittsburgh december today tracking tracking how an object moves examples of tracking applications probabilistic approach to tracking kalman filters tracking challenges ta will have graded on tuesday due next thursday use a random sample of images to train the attribute classifiers even impoverished motion data can evoke a strong percept johansson visual perception of biological motion and a model for its analysis perception and psychophysics 1973 traffic soccer face body eye gaze how would you do it body pose tracking activity recognition censusing a bat population video based interfaces medical apps surveillance extract visual features corners textured areas and track them over multiple frames many problems e g structure from motion require matching points many challenges e g points may change appearance over time points may appear or disappear i x y t i x y t given two subsequent frames estimate the point translation the lucas kanade optical flow method assumes brightness constancy projection of the same point looks the same in every frame small motion points do not move very far spatial coherence points move like their neighbors recognizing human facial expression by yaser yacoob larry s davis kristen grauman features optical flow within a region of interest the pixel man efros berg mori malik kristen grauman interesting point best matching neighborhood search window time t time t search window is centered at the point where we last saw the feature in image best match position where we have the highest normalized cross correlation value kristen grauman video interface use feature tracking as mouse replacement user clicks on the feature to be tracked take the pixel square of the feature in the next image do a search to find the region with the highest correlation move the mouse pointer accordingly repeat in the background every of a second james gips and margrit betke kristen grauman specialized software for communication games james gips and margrit betke feature based matching for motion for a discrete matching search what are the tradeoffs of the chosen search window size which patches to track select interest points e g corners where should the search window be placed near match at previous frame more generally taking into account the expected dynamics of the object things that make visual tracking difficult small few visual features erratic movements moving very quickly occlusions leaving and coming back surrounding similar looking objects tracking by repeated detection works well if object is easily detectable e g face or colored glove and there is only one need some way to link up detections best you can do if you can t predict motion key idea based on a model of expected motion predict where objects will occur in next frame before even seeing the image restrict search for the object measurement noise is reduced by trajectory smoothness robustness to missing or weak observations assumptions camera is not moving instantly to new viewpoint objects do not disappear and reappear in different places in the scene t t t t detection we detect the object independently in each frame and can record its position over time e g based on blob centroid or detection window coordinates tracking with dynamics we use image measurements to estimate position of object but also incorporate position predicted by dynamics i e our expectation of object motion pattern time t time t belief measurement corrected prediction time t time t state x the actual state of the moving object that we want to estimate but cannot observe state could be any combination of position pose viewpoint velocity acceleration etc observations y our actual measurement or observation of state x which can be very noisy at each time t the state changes to xt and we get a new observation yt amin sadeghi our goal recover most likely state xt given all observations seen so far i e yt knowledge about dynamics of state transitions p x yt t p x yt t correction compute an updated estimate of the state from prediction and measurements p x yt yt yt yt only the immediate past matters p x x t dynamics model only the immediate past matters p x x t dynamics model measurements depend only on the current state p y x x t yt x obs el only the immediate past matters p x x t dynamics model measurements depend only on the current state p y x x t yt x p y x observation model we have models for likelihood of next state given current state dynamics model p x x likelihood of observation given the state observation or measurement model p y x we want to recover for each t p x y notation reminder x n μ σ random variable with gaussian probability distribution that has the mean vector μ and covariance matrix σ x and μ are d dimensional σ is d x d d d if x is d we just have one σ parameter the variance kristen grauman dynamics and observation models dynamics model represents evolution of state over time p xt xt observation or measurement model at every time step we get a noisy measurement of the state p yt x t this is how these models are defined in the kalman filter the kalman filter linear dynamics model state undergoes linear transformation plus gaussian noise observation model measurement is linearly transformed state plus gaussian noise the predicted corrected state distributions are gaussian you only need to maintain the mean and covariance the calculations are easy time state vector position p and velocity v x pt pt t v t x t pt noise t v t measurement is position only y mx noise pt noise t t v t base case start with initial prior that predicts state in absence of any evidence p for the first frame correct this given the first measurement base case start with initial prior that predicts state in absence of any evidence p for the first frame correct this given the first measurement p x y y p x p x p base case start with initial prior that predicts state in absence of any evidence p for the first frame correct this given the first measurement given corrected estimate for frame t predict for frame t p x yt observe yt correct for frame t p x t yt yt predict correct prediction involves representing p x t given t t p x y t p x x t t t law of total probability p x y t p x x t t t p x t x t yt p x t yt dx t p x y t p x x t t t p x t yt dx t p x t yt dx t independence assumption only immediate past state matters adapted from amin sadeghi p x y t p x x t t t p x t x t yt p x t yt dx t p x t x t p x t yt dx t dynamics model corrected estimate from previous step p correction correction involves computing p x y given predicted value p p x t p y yt p x t yt p yt x t p x t yt p yt yt correction correction involves computing p x y given predicted value p x y t p x y y p yt x t yt p x y y p yt yt t t p yt x t p x t yt p yt yt p yt x t p x t yt p yt x t p x t yt dx t conditioning on xt amin sadeghi correction involves computing p x y given predicted value p x y t p x y y p yt x t yt p x y y p yt yt t t p yt x t p x t yt observation p yt yt predicted model p yt x t p x t yt estimate p yt x t p x t yt dx t normalization factor amin sadeghi prediction know corrected state from previous time step and all measurements up to excluding the current one predict distribution over next state advances t p x receive measurement correction know prediction of state and next measurement update distribution over current state p x y prediction p xt yt p xt xt p xt yt dxt dynamics model corrected estimate from previous step correction observation model predicted estimate p x y y p yt xt p xt yt t t p yt xt p xt yt dxt kalman filter processing kalman filter processing kalman filter processing kalman filter processing how to predict if missing a detection in the current frame prediction only depends on previous frames how to correct if missing detection in the current frame don t correct just use prediction what if we have two detections in a frame determine which of the two is more likely correct example kalman filter ground truth observation correction amin sadeghi a bat census kristen grauman tracking issues initialization manual background subtraction detection initialization obtaining observation and dynamics models observation model match a template or use a trained detector dynamics model usually specify using domain knowledge initialization obtaining observation and dynamics models uncertainty of prediction vs correction if the dynamics model is too strong will end up ignoring the data if the observation model is too strong tracking is reduced to repeated detection too strong dynamics model too strong observation model initialization getting observation and dynamics models prediction vs correction data association when tracking multiple objects need to assign right objects to right tracks particle filters good for this initialization getting observation and dynamics models prediction vs correction data association drift errors can accumulate over time d ramanan d forsyth and a zisserman tpami tracking objects detection prediction probabilistic framework predict next state update current state based on observation two simple but effective methods kalman filters gaussian distribution particle filters multimodal distribution for multiple objects challenges prof adriana kovashka university of pittsburgh december today human pose and actions introduction estimating human pose recognizing human actions using specialized features using pose using objects from ego centric video next time last class review for the final exam omets by wednesday night post on piazza questions or anything you want me to review for participation credit extra office hours on friday final exam monday dec same room sennott square similar to midterm exam mostly short questions and a few problems but longer points will only cover topics discussed after midterm but some of these use topics from first half mean median max due thursday see piazza for correction about how to get probabilities for part iii tentative grades entered on courseweb median is action a transition from one state to another who is the actor how is the state of the actor changing what if anything is being acted on how is that thing changing what is the purpose of the action if any could be more or less complex no universal terminology but approximately actions atomic motion patterns often gesture like single clear cut trajectory single nameable behavior e g sit wave arms activity series or composition of actions e g interactions between people event combination of activities or actions e g a football game a traffic accident categories walking hammering dancing skiing sitting down standing up jumping poses nouns and predicates man swings hammer man hits nail w hammer motion pose held objects nearby objects today human pose and actions introduction recognizing human actions using specialized features using pose using objects from ego centric video jamie shotton andrew fitzgibbon mat cook toby sharp mark finocchio richard moore alex kipman andrew blake best paper award at cvpr adapted from jamie shotton recognize large variety of human poses all shapes sizes limited compute budget super real time on xbox to allow games to run concurrently no temporal information frame by frame local pose estimate of parts each pixel each body joint treated independently reduced training data and computation time very fast simple depth image features parallel decision forest classifier capture depth image remove bg infer body parts per pixel cluster pixels to hypothesize body joint positions fit model track skeleton compute p ci wi pixels i x y body part ci image window wi discriminative approach learn classifier p ci wi from training data train invariance to depth comparisons very fast to compute 𝑑𝐼 x δ toy example distinguish left l and right r sides of the body to classify pixel x start here fθ i x no yes fθ i x no yes p c l r p c l r p c l r input depth ground truth parts inferred parts soft depth 12 depth of trees 15 20 depth of trees amit geman breiman geurts et al tree 𝐼 x 𝐼 x tree t pt c c c c trained on different random subset of images bagging helps avoid over fitting 𝑇 average tree posteriors 𝑃 𝑇 𝑃𝑡 𝑐 𝐼 x 𝑡 55 50 number of trees input depth inferred body parts front view side view top view jamie shotton inferred joint positions modes found using mean shift no tracking or smoothing input depth inferred body parts front view side view top view jamie shotton inferred joint positions modes found using mean shift no tracking or smoothing today human pose and actions introduction estimating human pose using pose using objects from ego centric video tracked points space time interest points corner detectors in space time talk on phone get out of car space time interest point detectors descriptors hog hof pyramid histograms svms with chi squared kernel spatio temporal binning interest points today human pose and actions introduction estimating human pose recognizing human actions using specialized features from ego centric video integrated reasoning human pose estimation integrated reasoning human pose estimation object detection tennis racket integrated reasoning human pose estimation object detection action categorization tennis racket head torso activity tennis forehand human pose estimation is challenging difficult part appearance self occlusion image region looks like a body part human pose estimation is challenging human pose estimation object detection facilitate given the object is detected object detection is challenging viola jones lampert et al divvala et al vedaldi et al object detection is challenging viola jones lampert et al divvala et al vedaldi et al facilitate given the pose is estimated learning results tennis forehand tennis serve volleyball smash activity classification results 6 3 our model 9 gupta et al bag of words sift svm cricket shot tennis forehand today human pose and actions introduction estimating human pose recognizing human actions using specialized features using pose using objects detecting activities of daily living in first person camera views hamed pirsiavash deva ramanan cvpr hamed pirsiavash motivation a sample video of activities of daily living tele rehabilitation long term at home monitoring kopp et al arch of physical medicine and rehabilitation catz et al spinal cord life logging so far mostly write only memory this is the right time for computer vision community to get involved gemmell et al mylifebits a personal database for everything communications of the acm hodges et al sensecam a retrospective memory aid ubicomp wearable adl detection it is easy to collect natural data low level features weak semantics high level features strong semantics space time interest points laptev ijcv human pose difficulties of pose detectors are not accurate enough not useful in first person camera views high level features strong semantics long scale temporal structure classic data boxing wearable data making tea appearance feature bag of objects fridge stove tv video clip fridge bag of detected objects stove tv temporal pyramid coarse to fine correspondence matching with a multi layer pyramid inspired by spatial pyramid cvpr and pyramid match kernels iccv video clip temporal pyramid descriptor hamed pirsiavash time accuracy on action categories our model 40 6 stip baseline 8 hamed pirsiavash summary human actions action recognition still an open problem how to represent actions types of data atomic and more complex actions ego centric video common representations space time interest points pose objects and temporal pyramids of objects pose can be approached as a classification problem using depth data prof adriana kovashka university of pittsburgh december today omets min review quiz returned average was solutions posted on courseweb extended to friday tomorrow at 59pm hws submitted on tuesday have been graded ask me if you need to know how many free late days left anyone working on part i describe each search final exam monday dec review quizzes make sure you know and understand svms i am not reviewing tracking and actions pose today since we covered them recently but they will be on the exam extra office hours tomorrow at recognition some translations feature vector descriptor representation recognition often involves classification classes categories hence classification categorization training learning a model e g classifier happens at training time from training data classification prediction happens at test time ground truth labels for test images their true labels that we cannot observe when developing our system machine learning problems supervised learning unsupervised learni ng q cj cl q s c c classification given a feature representation for images how do we learn a model for distinguishing features from different classes decision boundary zebra non zebra classification assign input vector to one of two or more classes any decision rule divides the input space into decision regions separated by decision boundaries what do you see in this image trees bear camera man rabbit grass forest slide credit d hoiem describe predict or interact with the object based on visual cues is it dangerous how fast does it run is it alive is it soft does it have a tail can i poke with it slide credit d hoiem binary cat vs dog or cat vs no cat object recognition caltech average object images fine grained recognition place recognition places database object categorization task description given a small number of training images of a category recognize a priori unknown instances of that category and assign the correct category label categorize at which level fido german shepherd dog grauman b leibe animal living being apply a prediction function to a feature representation of the image to get the desired output f apple f tomato f cow y f x output prediction function image feature training given a training set of labeled examples xn yn estimate the prediction function f by minimizing the prediction error on the training set testing apply f to a never before seen test example x and output the predicted value y f x training steps testing test image slide credit d hoiem and l lazebnik popular global image features raw pixels and simple functions of raw pixels oliva and torralba histograms bags of features e g sift hog dalal and triggs recognizing a beach recognizing cloth fabric datasets today imagenet categories images microsoft coco categories images pascal categories images sun categories images challenges robustness illumination object pose clutter occlusions intra class appearance viewpoint nearest neighbor classifier training examples from class test example training examples from class f x label of the training example nearest to x all we need is a distance function for our inputs no training required k nearest neighbors classification for a new point find the k closest points from training data labels of the k points vote to classify black negative red positive k if query lands here the nn consist of negatives and positives so we classify it as negative nearest neighbors pros and cons nn pros simple to implement decision boundaries not necessarily linear works for any number of classes nonparametric method nn cons need good distance function slow at test time large search problem to find neighbors storage of data discriminative classifiers learn a simple function of the input features that correctly predicts the true labels on the training set 𝑦 𝑓 training goals accurate classification of training data correct classifications are confident classification function is simple linear classifier find a linear function to separate the classes confidence in positive label is a weighted sum of features f x sgn wdxd sgn w x lines in let a w c x x y ax cy b w x b lines in let a w c x y ax cy b w x b d b w x b distance from w point to line find linear function to separate positive and negative examples xi positive xi negative w b xi w b which line is best discriminative classifier based on optimal separating line for case maximize the margin between the positive and negative training examples xi positive yi xi w b xi negative yi xi w b support vectors margin for support vectors xi w b xi positive yi xi w b xi negative yi xi w b for support vectors xi w b distance between point and line for support vectors xi w b w support vectors margin wτ x b m want line that maximizes the margin xi positive yi xi w b xi negative yi xi w b for support vectors xi w b distance between point and line xi w b w support vectors margin therefore the margin is w maximize margin w correctly classify all training data points xi positive yi xi w b xi negative yi xi w b quadratic optimization problem one constraint for each training point note sign trick solution w i i yi xi solution w i i yi xi b yi w xi for any support vector classification function f x sign w x b sign i i yi xi x b if f x classify as negative otherwise classify as positive notice that it relies on an inner product between the test point x and the support vectors xi nonlinear svms datasets that are linearly separable work out great x but what if the dataset is just too hard x we can map it to a higher dimensional space andrew moore nonlinear kernel example consider the mapping x x x y x y xy k x y xy y2 the kernel trick the linear classifier relies on dot product between vectors k xi xj xi xj if every data point is mapped into high dimensional space via some transformation φ xi φ xi the dot product becomes k xi xj φ xi φ xj a kernel function is similarity function that corresponds to an inner product in some expanded feature space the kernel trick instead of explicitly computing the lifting transformation φ x define a kernel function k such that k xi xj φ xi φ xj examples of kernel functions linear k xi x j i j gaussian rbf k xi x j exp histogram intersection k xi x j min k xi k x j k allowing misclassifications misclassification cost data samples slack variable the w that minimizes maximize margin minimize misclassification what about multi class svms unfortunately there is no definitive multi class svm formulation in practice we have to obtain a multi class svm by combining multiple two class svms one vs others training learn an svm for each class vs the others testing apply each svm to the test example and assign it to the class of the svm that returns the highest decision value one vs one training learn an svm for each pair of classes testing each learned svm votes for a class to assign to the test example svms for recognition define your representation for each example select a kernel function compute pairwise kernel values between labeled examples use this kernel matrix to solve for svm support vectors weights to classify a new example compute kernel values between new input and support vectors apply weights check sign of output example learning gender with svms moghaddam and yang learning gender with support faces tpami moghaddam and yang face gesture support faces moghaddam and yang learning gender with support faces tpami human vs machine svms performed better than any single human test subject at either resolution kristen grauman svms pros and cons pros many publicly available svm packages or use built in matlab version but slower kernel based framework is very powerful flexible often a sparse set of support vectors compact at test time work very well in practice even with very small training sample sizes cons no direct multi class svm must combine two class svms can be tricky to select best kernel function for a problem computation memory during training time must compute matrix of kernel values for every pair of examples learning can take a very long time for large scale problems adapted from lana lazebnik evaluating classifiers accuracy correctly classified all test examples precision recall precision predicted true pos predicted pos recall predicted true pos true pos f measure p r want evaluation metric to be in some range e g worst possible classifier best possible classifier training set labels known test set labels unknown how well does a learned model generalize from the data it was trained on to a new test set components of generalization error bias how much the average model over all training sets differs from the true model error due to inaccurate assumptions simplifications made by the model variance how much models estimated from different training sets differ from each other underfitting model is too simple to represent all the relevant class characteristics high bias and low variance overfitting model is too complex and fits irrelevant characteristics noise in the data low bias and high variance models with too few parameters are inaccurate because of a large bias not enough flexibility models with too many parameters are inaccurate because of a large variance too much sensitivity to the sample underfitting overfitting high bias low variance low bias high variance high bias low variance low bias high variance need validation set validation set is separate from the test set high bias low variance low bias high variance number of training examples how to reduce variance choose a simpler classifier use fewer features get more training data regularize the parameters want to minimize overall absolute value of weights slide credit d hoiem remember no free lunch machine learning algorithms are tools three kinds of error inherent unavoidable bias due to over simplifications variance due to inability to perfectly estimate parameters from limited data try simple classifiers first better to have smart features and simple classifiers than simple features and smart classifiers use increasingly powerful classifiers with more training data bias variance tradeoff adapted from d hoiem bag of features steps extract local features learn visual vocabulary using clustering quantize local features using visual vocabulary represent images by frequencies of visual words slide credit l lazebnik learning the visual vocabulary compute histogram in each spatial bin attributes we want to be able to infer something about unfamiliar objects if we can infer properties familiar objects new object has stripes has ears has eyes has four legs has mane has tail has snout brown muscular has snout has stripes like cat has mane and tail like horse has snout like horse and dog whittlesearch search via comparisons like this but with curlier hair allow user to whittle away irrelevant images via comparative feedback on properties of results relative attributes we need ability to compare images by attribute strength bright learning relative attributes we want to learn a spectrum ranking model for an attribute e g brightness supervision from human annotators consists of ordered pairs similar pairs learning user specific attributes treat learning perceived attributes as an adaptation problem adapt generic attribute model with minimal user specific labeled examples generic category recognition representation choice part based consider edges contours and oriented intensity gradients summarize local distribution of gradients with histogram given the representation train a binary classifier noy enso tcaarc ar face detection and recognition sally boosting training initially weight each training example equally in each boosting round find the weak learner that achieves the lowest weighted training error raise weights of training examples misclassified by current weak learner compute final classifier as linear combination of all weak learners weight of each learner is directly proportional to its accuracy review illustration with red blue circles of different sizes viola jones face detector main idea represent local texture with efficiently computable rectangular features within window of interest select discriminative features to be weak classifiers use boosted combination of them as final classifier form a cascade of such classifiers rejecting clear negatives quickly viola jones detector features rectangular filters feature output is difference between adjacent regions value pixels in white area pixels in black area efficiently computable with integral image any sum can be computed in constant time value at x y is sum of pixels above and to the left of x y integral image example source result computing sum within a rectangle let a b c d be the values of the integral image at the corners of a then the sum of original image values within the rectangle can be computed as sum a b c d only additions are required for any size of rectangle viola jones detector features considering all possible filter parameters position scale and type 000 possible features associated with each x window which subset of these features should we use to determine if a window has a face use adaboost both to select the informative features and to form the classifier viola jones detector adaboost want to select the single rectangle feature and threshold that best separates positive faces and negative non faces training examples in terms of weighted error resulting weak classifier outputs of a possible rectangle feature on faces and non faces for next round reweight the examples according to errors choose another filter threshold combo cascading classifiers for detection form a cascade with low false negative rates early on apply less accurate but faster classifiers first to immediately discard windows that clearly appear to be negative dalal triggs pedestrian detector extract fixed sized pixel window at each position and scale compute hog histogram of gradient features within each window score the window with a linear svm classifier perform non maxima suppression to remove overlapping detections with lower scores histogram of gradient orientations orientation bins for unsigned angles histograms in pixel cells histograms of oriented gradients hog cells dalal and b triggs cvpr image credit n snavely images from caltech define object by collection of parts modeled by appearance spatial configuration object model sum of scores of features at fixed positions non object 7 5 object star shaped model root filter part filters deformation weights p felze the score of a hypothesis is the sum of appearance scores minus the sum of deformation costs subwindow n features n displacements score p p f h p d dx dy n i i i i i i i i i appearance weights deformation weights adapted from lana lazebnik training data consists of images with labeled bounding boxes need to learn the filters and deformation parameters f x w h x w are model parameters z are latent hypotheses latent svm training initialize w and iterate fix w and find the best z for each training example fix z and solve for w standard svm training object detection system overview our system takes an input image extracts around bottom up region proposals computes features for each proposal using a large convolutional neural network cnn and then classifies each region using class specific linear svms r cnn achieves a mean average precision map of 7 on pascal voc for comparison uijlings et al report map using the same region proposals but with a spatial pyramid and bag of visual words approach the popular deformable part models perform at lana lazebnik hog human detector chair loss due to rgb hog 0 7 0 0 5 0 0 0 0 0 0 0 0 recall 0 8 hog dpm hog human rgb human vondrick et al iccv why did the detector fail car vondrick et al iccv why did the detector fail car vondrick et al iccv summary detection window based approaches assume object appears in roughly the same configuration in different images look for alignment with a global template part based methods allow parts to move somewhat from their usual locations look for good fits in appearance for both the global template and the individual part templates amazon mechanical turk workers 0 01 annotation protocols type keywords select relevant images click on landmarks outline something anything else ensuring annotation quality consensus multiple annotation wisdom of the crowd qualification exam gold standard questions grading tasks a second tier of workers who grade others the esp game player player guessing car guessing boy guessing hat guessing kid success you agree on car guessing car success you agree on car revealing images guesser revealer guess car bcraursh partner s guess summary collecting annotations from humans crowdsourcing allows very cheap data collection getting high quality annotations can be tricky but there are many ways to ensure quality one way to obtain high quality data fast is by phrasing your data collection as a game crowdsourcing training james hays active learning traditional active learning reduces supervision by obtaining labels for the most informative or uncertain examples first board entropy mackay freund et al tong koller lindenbaum et al kapoor et al human in the loop recognition test image testing basic algorithm input image x computer vision max expected information gain question a no p c x high information gain low entropy is the belly black max expected information gain question 2 a yes p c x is the bill hooked p c x features labels predict where people will look for a new image by running model on every pixel of the image saliency model features illumination color orientation horizon line face detector viola jones person detector dpm distance to center salient samples from top non salient samples from bottom from each of training images learns weights for each feature that best predict a saliency label for each pixel take model trained on e g imagenet training set take outputs of or layer optional fine tune features and or classifier on new dataset to train model from scratch need lots of data classify test set of new dataset why now we have lots of data and deep nets can be trained in reasonable time with gpus language deep learning deep neural nets convolutional neural nets shallow vs deep architectures traditional recognition shallow architecture image video pixels object class image video pixels deep learning deep architecture object class background perceptrons input weights d neural network with specialized connectivity structure stack multiple stages of feature extractors higher stages compute more global more invariant more abstract features classification layer at the end adapted from rob fergus convolve input with learned filters apply non linearity spatial pooling downsample normalization optional supervised training of convolutional filters by back propagating classification error adapted from lana lazebnik convolution apply learned filter weights one feature map per filter stride can be greater than faster less memory non linearity per element independent options tanh sigmoid 1 1 exp x rectified linear unit relu simplifies backpropagation makes learning faster avoids saturation issues preferred option works well max pooling spatial pooling downsampling sum or max non overlapping overlapping regions role of pooling invariance to small transformations larger receptive fields see more of input krizhevsky et al 4 error top 5 next best non convnet 2 error machine learning proceedings of the thirteenth international conference experiments with a new boosting algorithm yoav freund robert e schapire at t laboratories mountain avenue murray hill nj fyoav abstract in an earlier paper we introduced a new boosting algorithm called adaboost which theoretically can be used to significantly reduce the error of any learning algorithm that con sistently generates classifiers whose performance is a little better than random guessing we also introduced the related notion of a pseudo loss which is a method for forcing a learning algorithm of multi label conceptsto concentrate on the labels that are hardest to discriminate in this paper we describe experiments we carried out to assess how well adaboost with and without pseudo loss performs on real learning problems we performed two sets of experiments the first set compared boosting to breiman bagging method when used to aggregate various classifiers including decision trees and single attribute value tests we compared the performance of the two methods on a collection of machine learning benchmarks in the second set of experiments we studied in more detail the performance of boosting using a nearest neighbor classifier on an ocr problem introduction boosting is a general method for improving the perfor mance of any learning algorithm in theory boostingcan be used to significantly reduce the error of any weak learning algorithm that consistently generates classifiers which need only be a little bit better than random guessing despite the potential benefits of boosting promised by the theoret ical results the true practical value of boosting can only be assessed by testing the method on real machine learning problems in this paper we present such an experimental assessment of a new boosting algorithm called adaboost boosting works by repeatedly running a given learning algorithm on various distributions over the train ing data and then combining the classifiers produced by the weak learner into a single composite classifier the first provably effective boosting algorithms were presented by schapire and freund more recently we de scribed and analyzed adaboost and we argued that this new boosting algorithm has certain properties which make it more practical and easier to implement than its prede cessors this algorithm which we used in all our experiments is described in detail in section home page expected to change to some time in the near future for uid efyoav schapireg use the term weak learning algorithm even though in practice boosting might be combined with a quite strong learning algorithm such as this paper describes two distinct sets of experiments in the first set of experiments described in section we compared boosting to bagging a method described by breiman which works in the same general fashion i e by repeatedly rerunning a given weak learning algorithm and combining the computed classifiers but which con structs each distribution in a simpler manner details given below we compared boosting with bagging because both methods work by combining many classifiers this com parison allows us to separate out the effect of modifying the distribution on each round which is done differently by each algorithm from the effect of voting multipleclassifiers which is done the same by each in our experiments we compared boosting to bagging using a number of different weak learning algorithms of varying levels of sophistication these include an algorithm that searches for very simple prediction rules which test on a single attribute similar to holte very sim ple classification rules an algorithm that searches for a single good decision rule that tests on a conjunction of attribute tests similar in flavor to the rule formation part of cohen ripper algorithm and fu rnkranz and widmer irep algorithm and quinlan decision tree algorithm we tested these algorithms on a collection of benchmark learning problems taken from the uci repository the main conclusion of our experiments is that boost ing performs significantly and uniformly better than bag ging when the weak learning algorithm generates fairly simple classifiers algorithms and above when combined with boosting still seems to outperform bagging slightly but the results are less compelling we also found that boosting can be used with very sim ple rules algorithm to construct classifiers that are quite good relative say to kearns and mansour argue that can itself be viewed as a kind of boosting algo rithm so a comparison of adaboost and can be seen as a comparison of two competing boostingalgorithms see dietterich kearns and mansour paper for more detail on this point in the second set of experiments we test the perfor mance of boosting on a nearest neighbor classifier for hand written digit recognition in this case the weak learning algorithm is very simple and this lets us gain some insight into the interaction between the boosting algorithm and the nearest neighbor classifier we show that the boosting al gorithm is an effective way for finding a small subset of prototypes that performs almost as well as the complete set we also show that it compares favorably to the standard method of condensed nearest neighbor in terms of its test error there seem to be two separate reasons for the improve ment in performance that is achieved by boosting the first and better understood effect of boosting is that it generates a hypothesis whose error on the training set is small by com bining many hypotheses whose error may be large but still better than random guessing it seems that boostingmay be helpful on learning problems having either of the following two properties the first property which holds for many real world problems is that the observed examples tend to have varying degrees of hardness for such problems the boosting algorithm tends to generate distributions that con centrate on the harder examples thus challenging the weak learning algorithm to perform well on these harder parts of algorithm adaboost input sequence of mexamples h xm ym i with labels yiey kg weak learning algorithm weaklearn integer tspecifying number of iterations initialize i mfor all i do for t t call weaklearn providing it with the distribution dt get back a hypothesis ht x y calculate the error of ht tt dt i i ht xi y i if tt then set t t and abort loop set tt tt update distribution dt dt i dt i o ht xi y i where ztis a normalization constant chosen so that dt will be a distribution output the final hypothesis the sample space the second property is that the learning algorithm be sensitive to changes in the training examples h n x arg max yey t hx t x y log so that significantly different hypotheses are generated for different training sets in this sense boosting is similar to breiman bagging which performs best when the weak learner exhibits such unstable behavior however unlike bagging boosting tries actively to force the weak learning algorithm to change its hypotheses by changing the distri bution over the training examples as a function of the errors made by previously generated hypotheses the second effect of boosting has to do with variance re duction intuitively taking a weighted majority over many hypotheses all of which were trained on different samples taken out of the same training set has the effect of re ducing the random variability of the combined hypothesis thus like bagging boosting may have the effect of produc ing a combined hypothesis whose variance is significantly lower than those produced by the weak learner however unlike bagging boosting may also reduce the bias of the learning algorithm as discussed above see kong and di etterich for further discussion of the bias and variance reducing effects of voting multiple hypotheses as well as breiman very recent work comparing boosting and bagging in terms of their effects on bias and variance in our first set of experiments we compare boosting and bag ging and try to use that comparison to separate between the bias and variance reducing effects of boosting previous work drucker schapire and simard performed the first experiments using a boosting algorithm they used schapire original boosting algorithm com bined with a neural net for an ocr problem follow up comparisons to other ensemble methods were done by drucker et al more recently drucker and cortes used adaboost with a decision tree algorithm for an ocr task jackson and craven used adaboost to learn classifiers represented by sparse perceptrons and tested the algorithm on a set of benchmarks finally quinlan recently conducted an independent comparison of boosting and bagging combined with on a collection of uci benchmarks figure the algorithm adaboost the boosting algorithm in this section we describe our boosting algorithm called adaboost see our earlier paper for more details about the algorithm and its theoretical properties we describe two versions of the algorithm which we denote adaboost and adaboost the two ver sions are equivalent for binary classification problems and differ only in their handling of problems with more than two classes adaboost we begin with the simpler version adaboost the boosting algorithm takes as input a training set of mexam ples s h xm ym iwhere xiis an instance drawn from some space xand represented in some man ner typically a vector of attribute values and yieyis the class label associated with xi in this paper we al ways assume that the set of possible labels yis of finite cardinality k in addition the boosting algorithmhas access to another unspecified learning algorithm called the weak learning algorithm which is denoted generically as weaklearn the boosting algorithm calls weaklearn repeatedly in a series of rounds on round t the booster provides weaklearn with a distribution dtover the training set s in response weaklearn computes a classifier or hy pothesis ht x ywhich should correctly classify a fraction of the training set that has large probability with respect to dt that is the weak learner goal is to find a hypothesis htwhich minimizes the training error it pri dtht xi yi note that this error is measured with respect to the distribution dtthat was provided to the weak learner this process continues for trounds and at last the booster combines the weak hypotheses ht into a single final hypothesis h n algorithm adaboost input sequence of mexamples h xm ym i with labels yiey kg weak learning algorithm weaklearn then the following upper bound holds on the error of the final hypothesis hfin jfi hfin x ygjtt initialize i y jbjfor i y eb do for t t call weaklearn providing it with mislabel distribution dt get back a hypothesis ht xoy calculate the pseudo loss of ht theorem implies that the training error of the final hy pothesis generated by adaboost is small this does not necessarily imply that the test error is small however if the weak hypotheses are simple and t not too large tt i x y eb dt i y ht xi yi tht xi y then the difference between the training and test errors can also be theoretically bounded see our earlier paper for set tt tt update dt d i y dt i y ht xi yi ht xi y more on this subject the experiments in this paper indicate that the theoreti cal bound on the training error is often weak but generally t ztt correct qualitatively however the test error tends to be where ztis a normalization constant chosen so that dt will be a distribution output the final hypothesis hfin x arg max x log hh x y much better than the theory would suggest indicating a clear defect in our theoretical understanding the main disadvantage of adaboost is that it is unable to handle weak hypotheses with error greater than yey t the expected error of a hypothesis which randomly guesses the label is where kis the number of figure the algorithm adaboost still unspecified are the manner in which dtis computed on each round and how h nis computed different boosting schemes answer these two questions in different ways adaboost uses the simple rule shown in figure the initial distribution is uniform over sso i all i to compute distribution dt from dtand the last weak hypothesis ht we multiply the weight of example iby some number jjte if htclassifies xi correctly and otherwise the weight is left unchanged the weights are then renormalized by dividing by the normal ization constant zt effectively easy examples that are correctly classified by many of the previous weak hypothe ses get lower weight and hard examples which tend often to be misclassified get higher weight thus adaboost fo cuses the most weight on the examples which seem to be hardest for weaklearn the number jjtis computed as shown in the figure as a function of it the final hypothesis h nis a weighted vote i e a weighted linear threshold of the weak hypotheses that is for a given instance x h noutputs the label ythat maximizes the sum of the weights of the weak hypotheses predicting that label the weight of hypothesis htis defined to be log so that greater weight is given to hypotheses with lower error the importanttheoretical propertyabout adaboost is stated in the following theorem this theorem shows that if the weak hypotheses consistently have error only slightly better than then the training error of the final hypothesis h ndrops to zero exponentially fast for binary classifi cation problems this means that the weak hypotheses need be only slightly better than random theorem suppose the weak learning algorithm weaklearn when called by adaboost generates hy potheses with errors it where itis as defined in figure assume each it and let t it possible labels thus for k the weak hypotheses need to be just slightly better than random guessing but when k the requirement that the error be less than is quite strong and may often be hard to meet adaboost the second version of adaboost attempts to overcome this difficulty by extending the communication between the boosting algorithm and the weak learner first we allow the weak learner to generate more expressive hypotheses which rather than identifying a single label in y instead choose a set of plausible labels this may often be easier than choosing just one label for instance in an ocr setting it may be hard to tell if a particular image is or a but easy to eliminate all of the other possibilities in this case rather than choosing between and the hypothesis may output the set that both labels are plausible we also allow the weak learner to indicate a degree of plausibility thus each weak hypothesis outputs a vector k where the components with values close to or correspond to those labels considered to be plausible or implausible respectively note that this vector of values is not a probability vector i e the components need not sum to one while we give the weak learning algorithm more ex pressive power we also place a more complex requirement on the performance of the weak hypotheses rather than using the usual prediction error we ask that the weak hy potheses do well with respect to a more sophisticated error measure that we call the pseudo loss unlike ordinary error which is computed with respect to a distributionover exam ples pseudo loss is computed with respect to a distribution deliberately use the term plausible rather than prob able to emphasize the fact that these numbers should not be interpreted as the probability of a given label over the set of all pairs of examples and incorrect labels by manipulating this distribution the boosting algorithm can focus the weak learner not only on hard to classify ex amples but more specifically on the incorrect labels that are hardest to discriminate we will see that the boosting algorithm adaboost which is based on these ideas achieves boosting if each weak hypothesis has pseudo loss slightly better than random guessing more formally a mislabel is a pair i y where iis the index of a training example and yis an incorrect label associated with example i let bbe the set of all misla bels b f i y mg y yig a mislabel distribution is a distribution defined over the set bof all mislabels only that the weak hypotheses have pseudo loss less than i e only slightly better than a trivial constant valued hypothesis regardless of the number of classes also al though the weak hypotheses htare evaluated with respect to the pseudo loss we of course evaluate the final hypothesis h nusing the ordinary error measure theorem suppose the weak learning algorithm weaklearn when called by adaboost generates hy potheses with pseudo losses it where itis as de fined in figure let t it then the following upper bound holds on the error of the final hypothesis hfin jfi h x ygj on each round tof boosting adaboost figure fin i m i k supplies the weak learner with a mislabel distribution dt in response the weak learner computes a hypothesis htof t t the form ht xry there is norestriction on yht x y in particular the prediction vector does not have to define a probability distribution intuitively we interpret each mislabel i y as repre senting a binary question of the form do you predict that the label associated with example xiis yi the correct k exp where kis the number of classes boosting and bagging t t label or y one of the incorrect labels with this in terpretation the weight dt i y assigned to this mislabel represents the importance of distinguishing incorrect label yon example xi a weak hypothesis htis then interpreted in the following manner if ht xi yi and ht xi y then hthas correctly predicted that xi label is yi not y since ht deems yito be plausible and y implausible similarly if ht xi yi and ht xi y then hthas incorrectly made the opposite prediction if ht xi yi ht xi y then ht prediction is taken to be a random guess values for htin are interpreted probabilistically this interpretation leads us to define the pseudo loss of hypothesis htwith respect to mislabel distribution dtby the formula in this section we describe our experiments comparing boosting and bagging on the uci benchmarks we first mention briefly a small implementation issue many learning algorithms can be modified to handle ex amples that are weighted by a distribution such as the one created by the boosting algorithm when this is possi ble the booster distribution dtis supplied directly to the weak learning algorithm a method we call boosting by reweighting however some learning algorithms require an unweighted set of examples for such a weak learn ing algorithm we instead choose a set of examples from s independently at random according to the distribution dt with replacement the number of examples to be chosen on each round is a matter of discretion in our experiments we chose mexamples on each round where mis the size of the original training set s we refer to this method as it i y eb dt i y ht xi yi ht xi y boosting by resampling boosting by resampling is also possible when using the space limitations prevent us from giving a complete deriva tion of this formula which is explained in detail in our earlier paper it can be verified though that the pseudo loss is minimized when correct labels yiare assigned the value and incorrect labels y yiassigned the value fur ther note that pseudo loss is trivially achieved by any constant valued hypothesis ht the weak learner goal is to find a weak hypothesis htwith small pseudo loss thus standard off the shelf learning algorithms may need some modification to be used in this manner although this modification is often straight forward after receiving ht the mislabel distribution is up dated using a rule similar to the one used in adaboost the final hypothesis h noutputs for a given instance x the label ythat maximizes a weighted average of the weak hypothesis values ht x y the following theorem gives a bound on the training er ror of the final hypothesis note that this theorem requires pseudo loss in this case a set of mislabels are chosen from the set bof all mislabels with replacement according to the given distribution dt such a procedure is consistent with the interpretation of mislabels discussed in section in our experiments we chose a sample of size jbj m k on each round when using the resampling method the weak learning algorithms as mentioned in the introduction we used three weak learn ing algorithms in these experiments in all cases the exam ples are described by a vector of values which corresponds to a fixed set of features or attributes these values may be discrete or continuous some of the examples may have missing values all three of the weak learners build hy potheses which classify examples by repeatedly testing the values of chosen attributes the first and simplest weak learner which we call findattrtest searches for the single attribute value test boosting findattrtest boosting finddecrule bagging findattrtest bagging finddecrule pseudo loss table the benchmark machine learning problems used in the experiments with minimum error or pseudo loss on the training set more precisely findattrtest computes a classifier which is defined by an attribute a a value vand three predictions and p this classifier classifies a new example x as follows if the value of attribute ais missing on x then predict p if attribute ais discrete and its value on example xis equal to v or if attribute ais continuous and its value on xis at most v then predict otherwise predict if using ordinary error adaboost these predictions p would be simple classifications for pseudo loss the predictions would be vectors in k where kis the number of classes the algorithm findattrtest searches exhaustively for the classifier of the form given above with minimum error or pseudo loss with respect to the distribution provided by the booster in other words all possible values of a v and p are considered with some preprocessing this search can be carried out for the error based implementation in o nm time where nis the number of attributes and mthe number of examples as is typical the pseudo loss implementation adds a factor of o k where kis the number of class labels for this algorithm we used boosting with reweighting the second weak learner does a somewhat more sophis ticated search for a decision rule that tests on a conjunction of attribute value tests we sketch the main ideas of this algorithm which we call finddecrule but omit some of the finer details for lack of space these details will be provided in the full paper first the algorithm requires an unweighted training set so we use the resampling version of boosting the given training set is randomly divided into a growing set using of the data and a pruning set with the remaining figure comparison of using pseudo loss versus ordinary error on multi class problems for boosting and bagging in the first phase the growing set is used to grow a list of attribute value tests each test compares an attribute ato a value v similar to the tests used by findattrtest we use an entropy based potential function to guide the growth of the list of tests the list is initially empty and one test is added at a time each time choosing the test that will cause the greatest drop in potential after the test is chosen only one branch is expanded namely the branch with the highest remaining potential the list continues to be grown in this fashion until no test remains which will further reduce the potential in the second phase the list is pruned by selecting the prefix of the list with minimum error or pseudo loss on the pruning set the third weak learner is quinlan decision tree algorithm we used all the default options with pruning turned on since expects an unweighted training sam ple we used resampling also we did not attempt to use adaboost since is designed to minimize error not pseudo loss furthermore we did not expect pseudo loss to be helpful when using a weak learning algorithm as strong as since such an algorithm will usually be able to find a hypothesis with error less than bagging we compared boosting to breiman bootstrap aggre gating or bagging method for training and combining multiple copies of a learning algorithm briefly the method works by training each copy of the algorithm on a bootstrap sample i e a sample of size mchosen uniformly at random with replacement from the original training set s of size m the multiple hypotheses that are computed are then combined using simple voting that is the final composite hypothesis classifies an example xto the class most often assigned by the underlying weak hypotheses see his paper for more details the method can be quite effective especially according to breiman for unstable learning algorithms for which a small change in the data effects a large change in the computed hypothesis in order to compare adaboost which uses pseudo loss to bagging we also extended bagging in a natural way for use with a weak learning algorithm that minimizes pseudo loss rather than ordinary error as described in section such a weak learning algorithm expects to be provided with a distribution over the set bof all mislabels on each round of bagging we construct this distribution using the bootstrap method that is we select jbjmislabels from b chosen uniformly at random with replacement findattrtest finddecrule boosting boosting findattrtest boosting finddecrule boosting bagging figure comparison of boosting and bagging for each of the weak learners and assign each mislabel weight the number of times it was chosen the hypotheses htcomputed in this manner are then combined using voting in a natural manner namely given x the combined hypothesis outputs the label ywhich maximizes tht x y for either error or pseudo loss the differences between bagging and boosting can be summarized as follows bagging always uses resampling rather than reweighting bagging does not modify the distribution over examples or mislabels but instead always uses the uniform distribution and in forming the final hypothesis bagging gives equal weight to each of the weak hypotheses the experiments we conducted our experiments on a collection of machine learning datasets available from the repository at university of california at irvine a summary of some of the proper ties of these datasets is given in table some datasets are provided with a test set for these we reran each algorithm times since some of the algorithms are randomized and averaged the results for datasets with no provided test set we used fold cross validation and averaged the re sults over runs for a total of runs of each algorithm on each dataset in all our experiments we set the number of rounds of boosting or bagging to be t results and discussion the results of our experiments are shown in table the figures indicate test error rate averaged over mul tiple runs of each algorithm columns indicate which weak learning algorithm was used and whether pseudo loss adaboost or error adaboost was used note that pseudo loss was not used on any two class prob lems since the resulting algorithm would be identical to the corresponding error based algorithm columns labeled indicate that the weak learning algorithm was used by itself with no boosting or bagging columns using boosting or bagging are marked boost and bag respectively one of our goals in carrying out these experiments was to determine if boosting using pseudo loss rather than er ror is worthwhile figure shows how the different al gorithms performed on each of the many class k problems using pseudo loss versus error each point in the scatter plot represents the error achieved by the two compet ing algorithms on a given benchmark so there is one point html figure comparison of versus various other boosting and bagging methods for each benchmark these experiments indicate that boost ing using pseudo loss clearly outperforms boosting using error using pseudo loss did dramatically better than error on every non binary problem except it did slightly worse on iris with three classes because adaboost did so much better than adaboost we will only discuss adaboost henceforth as the figure shows using pseudo loss with bagging gave mixed results in comparison to ordinary error over all pseudo loss gave better results but occasionally using pseudo loss hurt considerably figure shows similar scatterplots comparing the per formance of boosting and bagging for all the benchmarks and all three weak learner for boosting we plotted the er ror rate achieved using pseudo loss to present bagging in the best possible light we used the error rate achieved using either error or pseudo loss whichever gave the better result on that particular benchmark for the binary problems and experiments with only error was used for the simpler weak learning algorithms findattr test and finddecrule boosting did significantly and uni formly better than bagging the boosting error rate was worse than the bagging error rate using either pseudo loss or error on a very small number of benchmark problems and on these the difference in performance was quite small on average for findattrtest boosting improved the error rate over using findattrtest alone by compared to bagging which gave an improvement of only using pseudo loss or using error for finddecrule boost ing improved the error rate by bagging by only using pseudo loss using error when using as the weak learning algorithm boost ing and bagging seem more evenly matched although boosting still seems to have a slight advantage on av erage boosting improved the error rate by bagging by boosting beat bagging by more than on of the benchmarks while bagging did not beat boostingby this amount on any benchmark for the remaining bench marks the difference in performance was less than figure shows in a similar manner how performed compared to bagging with and compared to boosting with each of the weak learners using pseudo loss for the non binary problems as the figure shows using boosting with findattrtest does quite well as a learning algorithm in its own right in comparison to this algorithm beat on of the benchmarks by at least tied on and lost on as mentioned above its average performance relative to using findattrtest by itself was in comparison improvement in performance table test error rates of various algorithms on benchmark problems over findattrtest was using boosting with finddecrule did somewhat bet ter the win tie lose numbers for this algorithm compared to were and its average improvement over findattrtest was boosting a nearest neighbor classifier in this section we study the performance of a learning al gorithm which combines adaboost and a variant of the nearest neighbor classifier we test the combined algorithm on the problem of recognizing handwritten digits our goal is not to improve on the accuracy of the nearest neighbor classifier but rather to speed it up speed up is achieved by reducing the number of prototypes in the hypothesis and thus the required number of distance calculations without increasing the error rate it is a similar approach to that of nearest neighbor editing in which one tries to find the minimal set of prototypes that is sufficient to label all the training set correctly the dataset comes from the us postal service usps and consists of training examples and test exam ples the training and test examples are evidently drawn from rather different distributions as there is a very signifi cant improvement in the performance if the partition of the data into training and testing is done at random rather than using the given partition we report results both on the original partitioning and on a training set and a test set of the same sizes that were generated by randomly partitioning the union of the original training and test sets each image is represented by a matrix of bit pixels the metric that we use for identifying the near est neighbor and hence for classifying an instance is the standard euclidean distance between the images viewed as vectors in r this is a very naive metric but it gives reasonably good performance a nearest neighbor classifier which uses all the training examples as prototypes achieves a test error of on randomly partitioned data using the more sophisticated tangent distance is in our future plans each weak hypothesis is defined by a subset pof the training examples and a mapping p k given a new test point x such a weak hypothesis predicts the vector where epis the closest point to x on each round of boosting a weak hypothesis is gener ated by adding one prototype at a time to the set puntil the set reaches a prespecified size given any set p we always choose the mapping minimizes the pseudo loss of the resulting weak hypothesis with respect to the given mislabel distribution initially the set of prototypes pis empty next ten candidate prototypes are selected at random according to the current marginal distribution over the training exam ples of these candidates the one that causes the largest decrease in the pseudo loss is added to the set p and the process is repeated the boosting process thus influences the weak learning algorithm in two ways first by changing the way the ten random examples are selected and second by changing the calculation of the pseudo loss it often happens that on the following round of boost ing the same set pwill have pseudo loss significantly less than with respect to the new mislabel distribution but possibly using a different mapping in this case rather than choosing a new set of prototypes we reuse the same set pin additional boosting steps until the advantage that can be gained from the given partition is exhausted details figure a sample of the examples that have the largest weight after of the boosting iterations the first line is after itera tion the second after iteration and the third after iteration underneath each image we have a line of the form d where dis the label of the example and are the labels that get the highest and second highest vote from the combined hy pothesis at that point in the run of the algorithm and are the corresponding normalized votes omitted we ran iterations of the boosting algorithm and the number of prototypes we used were for the first weak hypothesis for the second for the third for the next five and for the remaining twenty two weak hypotheses these sizes were chosen so that the errors of all of the weak hypotheses are approximately equal we compared the performance of our algorithm to a strawman algorithm which uses a single set of prototypes similar to our algorithm the prototype set is generated in crementally comparing ten prototype candidates at each step and always choosing the one that minimizes the em pirical error we compared the performance of the boosting algorithm to that of the strawman hypothesis that uses the same number of prototypes we also compared our per formance to that of the condensed nearest neighbor rule cnn a greedy method for finding a small set of prototypes which correctly classify the entire training set results and discussion the results of our experiments are summarized in ta ble and figure table describes the results from ex periments with adaboost each experiment was repeated times using different random seeds the strawman al gorithm each repeated times and cnn times we compare the results using a random partition of the data into training and testing and using the partition that was defined by usps we see that in both cases after more than examples the training error of adaboost is much better than that of the strawman algorithm the performance on the test set is similar with a slight advantage to adaboost when the hypotheses include more than examples but a slight advantage to strawman if fewer rounds of boostingare used after examples the error of adaboost on the random partition is on average while the error achieved by using the whole training set is on the random partition the final error is while the error using the whole training set is err figure graphs of the performance of the boosting algorithm on a randomly partitioned usps dataset the horizontal axis indicates the total number of prototypes that were added to the combined hypothesis and the vertical axis indicates error the topmost jagged line indicates the error of the weak hypothesis that is trained at this point on the weighted training set the bold curve is the bound on the training error calculated using theorem the lowest thin curve and the medium bold curve show the performance of the combined hypothesis on the training set and test set respectively comparing to cnn we see that both the strawman algorithm and adaboost perform better than cnn even when they use about examples in their hypotheses larger hypotheses generated by adaboost or strawman are much better than that generated by cnn the main problem with cnn seems to be its tendency to overfit the training data adaboost and the strawman algorithm seem to suffer less from overfitting figure shows a typical run of adaboost the upper most jagged line is a concatenation of the errors of the weak hypotheses with respect to the mislabel distribution each peak followed by a valley corresponds to the beginning and end errors of a weak hypothesis as it is being constructed one prototype at a time the weighted error always starts around at the beginning of a boosting iteration and drops to around the heaviest line describes the upper bound on the training error that is guaranteed by the orem and the two bottom lines describe the training and test error of the final combined hypothesis it is interesting that the performance of the boosting algorithm on the test set improved significantly after the error on the training set has already become zero this is surprising because an occam razor argument would predict that increasing the complexity of the hypothesis after the error has been reduced to zero is likely to degrade the performance on the test set figure shows a sample of the examples that are given large weights by the boosting algorithm on a typical run there seem to be two types of hard examples first are examples which are very atypical or wrongly labeled such as example on the first line and examples and on the second line the second type which tends to dominate on later iterations consists of examples that are very similar to each other but have different labels such as examples versus on the third line although the algorithm at this point was correct on all training examples it is clear from the votes it assigned to different labels for these example pairs that it was still trying to improve the discrimination table average error rates on training and test sets in percent for columns labeled random partition a random partition of the union of the training and test sets was used usps partition means the usps provided partition into training and test sets was used columns labeled theory give theoretical upper bounds on training error calculated using theorem size indicates number of prototypes defining the final hypothesis between similar examples this agrees with our intuition that the pseudo loss is a mechanism that causes the boosting algorithm to concentrate on the hard to discriminate labels of hard examples conclusions we have demonstrated that adaboost can be used in many settings to improve the performance of a learning algorithm when starting with relatively simple classifiers the im provement can be especially dramatic and can often lead to a composite classifier that outperforms more complex one shot learning algorithms like this improvement is far greater than can be achieved with bagging note how ever that for non binary classification problems boosting simple classifiers can only be done effectively if the more sophisticated pseudo loss is used when starting with a complex algorithm like boosting can also be used to improve performance but does not have such a compelling advantage over bagging boosting combined with a complex algorithm may give the greatest improvement in performance when there is a rea sonably large amount of data available note for instance boosting performance on the letter recognition problem with training examples naturally one needs to consider whether the improvement in error is worth the ad ditional computation time although we used rounds of boosting quinlan got good results using only rounds boosting may have other applications besides reducing the error of a classifier for instance we saw in section that boosting can be used to find a small set of prototypes for a nearest neighbor classifier as described in the introduction boosting combines two effects it reduces the bias of the weak learner by forcing the weak learner to concentrate on different parts of the instance space and it also reduces the variance of the weak learner by averaging several hypotheses that were generated from different subsamples of the training set while there is good theory to explain the bias reducing effects there is need for a better theory of the variance reduction when logistic regression fails example in which the logistic regression model fails cs machine learning multiclass classification approach vs vs vs cs machine learning discriminative approach methods based on binary classification methods assume we have classes labeled approach a binary logistic regression on all pairs vs vs vs d class decision class label based on who gets the majority does not work all the time cs machine learning multiclass classification approach vs vs vs cs machine learning learning of the softmax model learning of parameters w statistical view x softmax network p y x k p y k x multi way y coin toss assume outputs y are transformed as follows y k y cs machine learning on line learning example cs machine learning practical concerns input normalization input normalization makes the data vary roughly on the same scale can make a huge difference in on line learning assume on line update delta rule for two weights j k wj wj i yi f xi xi j w w i y f x x change depends on the magnitude of the input k k i i i k for inputs with a large magnitude the change in the weight is huge changes to the inputs with high magnitude disproportional as if the input was more important cs machine learning cs machine learning http people cs pitt edu milos courses machine learning issp time monday wednesday location sennott square room announcements course syllabus matlab tutorial the final exam for the course will be on wednesday april the exam is a closed book cumulative exam with a slight bias towards the material covered in the second half of the course the term project presentations will be held on monday april and wednesday april the slide presentations should be minutes long and should briefly summarize the problem the data used the methodology learning methods the results obtained during the investigations and main conclusions the presentation order that was generated with the help of randperm function can be found here please submit the presentations in ppt pptx or pdf format by of the day of your presentationvia courseweb the term projects are due on wednesday april at the project report should be organized like a conference papers with abstract introduction related work methodology experimental results discussion of results and conclusion sections the reports should be selfexplanatory the methodology section should explain briefly the methods used and provide relevant references homework assignments the printed reports should be submitted in the class before the lecture programs should be submitted electronically via course web before please follow the rules for the submission of programs homework assignment solutions homework assignment homework assignment homework assignment cs machine learning http people cs pitt edu milos courses homework assignment homework assignment homework assignment homework assignment homework assignment homework assignment homework assignment links course description lectures homeworks term projects matlab abstract the goal of the field of machine learning is to build computer systems that learn from experience and that are capable to adapt to their environments learning techniques and methods developed by researchers in this field have been successfully applied to a variety of learning tasks in a broad range of areas including for example text classification gene discovery financial forecasting credit card fraud detection collaborative filtering design of adaptive web agents and others this introductory machine learning course will give an overview of many models and algorithms used in modern machine learning including linear models multilayer neural networks support vector machines density estimation methods bayesian belief networks mixture models clustering ensamble methods and reinforcement learning the course will give the student the basic ideas and intuition behind these methods as well as a more formal understanding of how and why they work students will have an opportunity to experiment with machine learning techniques and apply them a selected problem in the context of a term project course syllabus prerequisites knowledge of matrices and linear algebra probability cs and statistics cs programming cs or equivalent or the permission of the instructor textbook cs machine learning http people cs pitt edu milos courses chris bishop pattern recognition and machine learning springer other ml books k murphy machine learning a probabilistic perspective mit press r o duda p e hart d g stork pattern classification second edition john wiley and sons j han m kamber data mining concepts and techniques morgan kauffman t mitchell machine learning mc graw hill lectures lectures topic assignments january introduction to machine learning readings bishop chapter january introduction to machine learning readings bishop chapter january density estimation i readings bishop chapter january matlab tutorial readings homework assignment data for homework january density estimation ii readings bishop chapter homework assignment data for homework january density estimation iii readings bishop chapter january linear regression readings bishop chapter homework assignment data for homework february classification learning logistic regression generative classification models readings bishop chapter february classification learning ii evaluation of classifiers readings bishop chapter homework assignment data for homework february fisher linear discriminant support vector machines readings bishop chapter chapter cs machine learning http people cs pitt edu milos courses february support vector machines for regression nonparametric instance based classification methods readings bishop chapter homework assignment data for homework february multilayer neural networks readings bishop chapter february multiclass classification decision trees readings bishop chapter homework assignment data for homework february bayesian belief networks readings bishop chapter february bayesian belief networks inference and learning readings bishop chapter homework assignment data for homework march midterm exam readings everything covered before or on february march expectation maximization algorithm readings bishop chapter homework assignment data for homework march expectation maximization algorithm mixture of gaussians readings bishop chapter march clustering readings bishop chapter homework assignment data for homework march ensemble methods mixture of experts bagging readings bishop chapter l breiman arcing classifiers y freund r schapire experiments with a new boosting algorithm march ensemble methods boosting readings bishop chapter l breiman arcing classifiers y freund r schapire experiments with a new boosting algorithm homework assignment data for homework cs machine learning http people cs pitt edu milos courses march dimensionality reduction feature selection readings bishop chapter april dimensionality reduction ii reinforcement learning readings bishop chapter kaebling littman moore reinforcement learning a survey april reinforcement learning readings kaebling littman moore reinforcement learning a survey april concept learning readings april final exam readings all semester april and term project presentations readings all semester homeworks the homework assignments will have mostly a character of projects and will require you to implement some of the learning algorithms covered during lectures programming assignments will be implemented in matlab see rules for the submission of programs the assignments both written and programming parts are due at the beginning of the class on the day specified on the assignment in general no extensions will be granted collaborations no collaboration on homework assignments programs and exams is permitted unless you are specifically instructed to work in groups term projects the term project is due at the end of the semester and accounts for a significant portion of your grade matlab cs machine learning http people cs pitt edu milos courses matlab is a mathematical tool for numerical computation and manipulation with excellent graphing capabilities it provides a great deal of support and capabilities for things you will need to run machine learning experiments the cssd at upitt offers student licenses for matlab to obtain the licence please check the following link to the matlab cssd page in addition upitt has a number of matlab licences running on both unix and windows platforms see the following web page for the details matlab tutorial file other matlab resources on the web online matlab documentation online mathworks documentation including matlab toolboxes cheating policy cheating and any other antiintellectual behavior including giving your work to someone else will be dealt with severely and will result in the fail f grade if you feel you may have violated the rules speak to us as soon as possible please make sure you read understand and abide by the academic integrity code for the faculty and college of arts and sciences students with disabilities if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union as early as possible in the term drs will verify your disability and determine reasonable accomodations for this course course webpages from spring spring spring spring spring and spring last updated by milos on machine learning cs issp spring lecture meeting time mondays wednesdays am pm classroom sennott square sensq course description the goal of the field of machine learning is to build computer systems that learn from experience and that are capable to adapt to their environments learning techniques and methods developed by researchers in this field have been successfully applied to a variety of learning tasks in a broad range of areas including for example text classification gene discovery financial forecasting credit card fraud detection collaborative filtering design of adaptive web agents and others this introductory machine learning course will give an overview of many models and algorithms used in modern machine learning including linear models multi layer neural networks support vector machines density estimation methods bayesian belief networks mixture models clustering ensemble methods and reinforcement learning the course will give the student the basic ideas and intuition behind these methods as well as a more formal understanding of how and why they work students will have an opportunity to experiment with machine learning techniques and apply them a selected problem in the context of a term project prerequisites knowledge of matrices and linear algebra cs probability cs statistics cs programming cs or equivalent or the permission of the instructor textbook chris bishop pattern recognition and machine learning springer homework assignments homework assignments will have mostly a character of projects and will require you to implement some of the learning algorithms covered during lectures programming assignments will be implemented in matlab please visit to obtain a matlab license for students the assignments both written and programming parts are due at the beginning of the class on the day specified on the assignment in general no extensions will be granted collaborations no collaboration on homework assignments programs term projects and exams unless you are specifically instructed to work in groups is permitted grading the final grade for the course will be determined based on homework assignments exams the term project and your lecture attendance and activity the midterm exam will be in early march and the final exam will be the week of april the term project presentations will be held the week of april policy on cheating cheating and any other anti intellectual behavior including giving your work to someone else will be dealt with severely and will result in the fail f grade if you feel you may have violated the rules speak to us as soon as possible please make sure you read understand and abide by the academic integrity code for the faculty and college of arts and sciences students with disabilities if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union tty as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course tentative syllabus machine learning introduction density estimation supervised learning linear and logistic regression generative classification models multi layer neural networks support vector machines unsupervised learning bayesian belief networks bbns learning parameters and structure of bbns expectation maximization clustering dimensionality reduction feature selection feature filtering wrapper methods pca ensemble methods mixtures of experts bagging and boosting reinforcement learning changsheng liu university of april plan for today review some quesdons from hw density esdmadon mixture of gaussian naïve bayesian hw please see whiteboard maximum likelihood maximum a posteriori esdmadon a set of random variables x xd a model of distribudon over variables in x with parameters θ p x θ data d dn objecdve find parameter θ that p x θ ﬁts data d the best maximum likelihood maximize p d θ ξ maximum a posteriori probability map a model of distribudon over variables in x with parameters θ p θ d ξ a biased coin with the probability of a head θ data hhtthhththttththhhhthhhht heads tails what is a good esdmate of θ use the frequency of occurrences this is the maximum likelihood esdmate the likelihood of the data maximum likelihood choose from the same family for convienence diagonal covariance matrix covariance matrix propordonal to the idendty matrix old faithful data set single gaussian mixture of two gaussians combine simple models into a complex model mixing coeﬃcient k directed acyclic graph dag nodes are random variables edges indicate causal inﬂuences each node has a condi onal probability table cpt that gives the probability of each of its values given every possible combinadon of values for its parents condidoning case roots sources of the dag that have no parents are given prior probabilides a is independent of b given c equivalently notadon condi onally independent via d separa on d separa on in the graph let x y and z be three sets of nodes if x and y are d separated by z then x and y are condidonally independent give z d separa on a is d separated from b give c if every undirected path between them is blocked with c naïve bayes as a bayes net naïve bayes is a simple bayes net priors p y and condidonals p xi y for naïve bayes provide cpts for the network slide credit ray mooney a course in machine learning hal daumé iii unsupervised learning if you have access to labeled training data you know what to do this is the supervised setting in which you have a teacher telling you the right answers unfortunately finding such a teacher is often difficult expensive or down right impossible in those cases you might still want to be able to analyze your data even though you do not have labels unsupervised learning is learning without a teacher one basic thing that you might want to do with data is to visualize it sadly it is difficult to visualize things in more than two or three dimensions and most data is in hundreds of dimensions or more dimension ality reduction is the problem of taking high dimensional data and embedding it in a lower dimension space another thing you might want to do is automatically derive a partitioning of the data into clusters you ve already learned a basic approach for doing this the k means algorithm chapter here you will analyze this algorithm to see why it works you will also learn more advanced clustering approaches k means clustering revisited the k means clustering algorithm is re presented in algorithm there are two very basic questions about this algorithm does it converge and if so how quickly how sensitive it is to initializa tion the answers to these questions detailed below are yes it converges and it converges very quickly in practice though slowly in theory yes it is sensitive to initialization but there are good ways to initialize it consider the question of convergence the following theorem states that the k means algorithm converges though it does not say how quickly it happens the method of proving the convergence is to specify a clustering quality objective function and then to show that the k means algorithm converges to a local optimum of that objective function the particular objective function that k means dependencies unsupervised learning algorithm k means d k for k to k do µk some random location randomly initialize mean for kth cluster end for repeat for n to n do zn argmink µk xn assign example n to closest center end for for k to k do µk mean xn zn k re estimate mean of cluster k end for until converged return z return cluster assignments is optimizing is the sum of squared distances from any data point to its assigned center this is a natural generalization of the definition of a mean the mean of a set of points is the single point that minimizes the sum of squared distances from the mean to every point in the data formally the k means objective is l z µ d µzn k n zn k xn µk theorem k means convergence theorem for any dataset d and any number of clusters k the k means algorithm converges in a finite num ber of iterations where convergence is measured by l ceasing the change proof of theorem the proof works as follows there are only two points in which the k means algorithm changes the values of µ or z lines and we will show that both of these operations can never increase the value of l assuming this is true the rest of the argu ment is as follows after the first pass through the data there are are only finitely many possible assignments to z and µ because z is discrete and because µ can only take on a finite number of values means of some subset of the data furthermore l is lower bounded by zero together this means that l cannot decrease more than a finite number of times thus it must stop decreasing at some point and at that point the algorithm has converged it remains to show that lines and decrease l for line when looking at example n suppose that the previous value of zn is a and the new value is b it must be the case that xn µb xn µb thus changing from a to b can only decrease l for line consider the second form of l line computes µk as the mean of the data points for which zn k which is precisely the point that minimizes squared sitances thus this update to µk can only decrease l there are several aspects of k means that are unfortunate first the convergence is only to a local optimum of l in practice this a course in machine learning means that you should usually run it times with different initial izations and pick the one with minimal resulting l second one can show that there are input datasets and initializations on which it might take an exponential amount of time to converge fortu nately these cases almost never happen in practice and in fact it has recently been shown that roughly if you limit the floating point pre cision of your machine k means will converge in polynomial time though still only to a local optimum using techniques of smoothed analysis the biggest practical issue in k means is initialization if the clus ter means are initialized poorly you often get convergence to uninter esting solutions a useful heuristic is the furthest first heuristic this gives a way to perform a semi random initialization that attempts to pick initial means as far from each other as possible the heuristic is sketched below pick a random example m and set xm for k k find the example m that is as far as possible from all previ ously selected means namely m arg maxm mink k xm µk and set µk xm in this heuristic the only bit of randomness is the selection of the first data point after that it is completely deterministic except in the rare case that there are multiple equidistant points in step it is extremely important that when selecting the mean you select that point that maximizes the minimum distance to the closest other mean you want the point that as far away from all previous means as possible the furthest first heuristic is just that a heuristic it works very well in practice though can be somewhat sensitive to outliers which will often get selected as some of the initial means however this outlier sensitivity is usually reduced after one iteration through the k means algorithm despite being just a heuristic it is quite useful in practice you can turn the heuristic into an algorithm by adding a bit more randomness this is the idea of the k means algorithm which is a simple randomized tweak on the furthest first heuristic the idea is that when you select the kth mean instead of choosing the absolute furthest data point you choose a data point at random with probability proportional to its distance squared this is made formal in algorithm if you use k means as an initialization for k means then you are able to achieve an approximation guarantee on the final value unsupervised learning algorithm k means d k xm for m chosen uniformly at random randomly initialize first point for k to k do dn mink k xn µk n compute distances p d normalize to probability distribution m random sample from p pick an example at random µk xm end for run k means using µ as initial centers of the objective this doesn t tell you that you will reach the global optimum but it does tell you that you will get reasonably close in particular if lˆ is the value obtained by running k means then this will not be too far from l opt the true global minimum theorem k means approximation guarantee the expected value of the objective returned by k means is never more than o log k from optimal and can be as close as o from optimal even in the former case with random restarts one restart will be o from optimal with high probability formally e log k moreover if the data is well suited for clustering then e lˆ o l opt the notion of well suited for clustering informally states that the advantage of going from k clusters to k clusters is large formally it means that lk opt opt where lk opt is the optimal value for clustering with k clusters and is the desired degree of approximation the idea is that if this condition does not hold then you shouldn t bother clustering the data one of the biggest practical issues with k means clustering is choosing k namely if someone just hands you a dataset and asks you to cluster it how many clusters should you produce this is difficult because increasing k will always decrease lk opt until k n and so simply using l as a notion of goodness is insuffi cient analogous to overfitting in a supervised setting a number of information criteria have been proposed to try to address this problem they all effectively boil down to regularizing k so that the model cannot grow to be too complicated the two most popular are the bayes information criteria bic and the akaike information criteria aic defined below in the context of k means bic arg min ˆk k log d k aic arg min ˆk k the informal intuition behind these criteria is that increasing k is going to make lk go down however if it doesn t go down by enough then it not worth doing in the case of bic by enough a course in machine learning means by an amount proportional to log d in the case of aic it proportional to thus aic provides a much stronger penalty for many clusters than does bic especially in high dimensions a more formal intuition for bic is the following you ask yourself the question if i wanted to send this data across a network how many bits would i need to send clearly you could simply send all of the n examples each of which would take roughly log d bits to send this gives n log d to send all the data alternatively you could first cluster the data and send the cluster centers this will take k log d bits then for each data point you send its center as well as its deviation from that center it turns out this will cost exactly lˆk bits therefore the bic is precisely measuring how many bits it will take to send your data using k clusters the k that minimizes this number of bits is the optimal value linear dimensionality reduction dimensionality reduction is the task of taking a dataset in high di mensions say and reducing it to low dimensions say while retaining the important characteristics of the data since this is an unsupervised setting the notion of important characteristics is difficult to define consider the dataset in figure which lives in high dimensions two and you want to reduce to low dimensions one in the case of linear dimensionality reduction the only thing you can do is to project the data onto a vector and use the projected distances as the embeddings figure shows a projection of this data onto the vector that points in the direction of maximal variance of the original dataset intuitively this is a reasonable notion of importance since this is the direction in which most information is encoded in the data for the rest of this section assume that the data is centered namely the mean of all the data is at the origin this will sim ply make the math easier suppose the two dimensional data is xn and you re looking for a vector u that points in the direc tion of maximal variance you can compute this by projecting each point onto u and looking at the variance of the result in order for the projection to make sense you need to constrain u in this case the projections are u xn u call these values pn the goal is to compute the variance of the pn and then choose u to maximize this variance to compute the variance you first need to compute the mean because the mean of the xns was zero the unsupervised learning figure mean of the ps is also zero this can be seen as follows pn xn u xn u u the variance of the pn is then just n finding the optimal u from the perspective of variance maximization reduces to the following optimization problem max u xn u subj to u in this problem it becomes apparent why keeping u unit length is important if not u would simply stretch to have infinite length to maximize the objective it is now helpful to write the collection of datapoints xn as a n d matrix x if you take this matrix x and multiply it by u which has dimensions d you end up with a n vector whose values are exactly the values p the objective in eq is then just the squared norm of p this simplifies eq to max u xu subj to u where the constraint has been rewritten to make it amenable to con structing the lagrangian doing so and taking gradients yields l u λ xu λ u ul λu xtx u you can solve this expression λu xtxu by computing the first eigenvector and eigenvalue of the matrix xtx this gives you the solution to a projection into a one dimensional space to get a second dimension you want to find a new vector v on which the data has maximal variance however to avoid redundancy you want v to be orthogonal to u namely u v this gives max v xv subj to v and u v following the same procedure as before you can construct a la a course in machine learning algorithm pca d k µ m ean x compute data mean for centering λk uk top k eigenvalues eigenvectors of d return x u project data using u grangian and differentiate l v xv u v ul xtx v however you know that u is the first eigenvector of xtx so the solution to this problem for and v is given by the second eigen value eigenvector pair of xtx repeating this analysis inductively tells you that if you want to project onto k mutually orthogonal dimensions you simply need to take the first k eigenvectors of the matrix xtx this matrix is often called the data covariance matrix because xtx i j n m xn i xm j which is the sample covariance between features i and j this leads to the technique of principle components analysis or pca for completeness the is depicted in algorithm the important thing to note is that the eigenanalysis only gives you the projection directions it does not give you the embedded data to embed a data point x you need to compute its embedding as x x x uk if you write u for the d k matrix of us then this is just xu there is an alternative derivation of pca that can be informative based on reconstruction error consider the one dimensional case again where you are looking for a single projection direction u if you were to use this direction your projected data would be z xu each zn gives the position of the nth datapoint along u you can project this one dimensional data back into the original space by multiplying it by ut this gives you reconstructed values zut instead of maximizing variance you might instead want to minimize the reconstruction error defined by definition of z x xuut quadratic rule unsupervised learning x xuut quadratic rule x x u is a unit vector c xu join constants rewrite last term minimizing this final term is equivalent to maximizing xu which is exactly the form of the maximum variance derivation of pca thus you can see that maximizing variance is identical to minimiz ing reconstruction error the same question of what should k be arises in dimension ality reduction as in clustering if the purpose of dimensionality reduction is to visualize then k should be or however an alter native purpose of dimensionality reduction is to avoid the curse of dimensionality for instance even if you have labeled data it might be worthwhile to reduce the dimensionality before applying super vised learning essentially as a form of regularization in this case the question of an optimal k comes up again in this case the same criteria aic and bic that can be used for clustering can be used for pca the only difference is the quality measure changes from a sum of squared distances to means for clustering to a sum of squared distances to original data points for pca in particular for bic you get the reconstruction error plus k log d for aic you get the recon struction error plus manifolds and graphs what is a manifold graph construction non linear dimensionality reduction isomap lle mvu mds non linear clustering spectral methods what is a spectrum spectral clustering a course in machine learning exercises exercise todo computer vision algorithms and applications http szeliski org book computer vision algorithms and applications richard szeliski welcome to the web site http szeliski org book for my computer vision textbook which you can now purchase at a variety of locations including springer springerlink doi amazon and barnes noble the book is also available in chinese and japanese translated by prof toru tamaki this book is largely based on the computer vision courses that i have cotaught at the university of washington and stanford with steve seitz and david fleet you are welcome to download the pdf from this web site for personal use but not to repost it on any other web site please post a link to this url http szeliski org book instead an electronic version of this manuscript will continue to be available even after the book is published note however that while the content of the electronic and hardcopy versions are the same the page layout pagination is different since the electronic version is optimized for online reading computer vision algorithms and applications http szeliski org book the pdfs should be enabled for commenting directly in your viewer also hyperlinks to sections equations and references are enabled to get back to where you were use altleftarrow in acrobat if you have any comments or feedback on the book please send me email this web site will also eventually contain supplementary materials for the textbook such as figures and images from the book slides sets pointers to software and a bibliography electronic draft september errata see here for a list of errors that people have noticed and reported last updated prof toru tamaki has compiled a more extensive list during his translation of the book into japanese slide sets there are not yet any slide sets to go with the book please feel free to look at the university of washington cse graduate computer vision slides that steve seitz and i have put together additional good sources for related slides include trevor darrell cs computer vision class at berkeley antonio torralba advances in computer vision class at mit michael black cs introduction to computer vision class at brown kristen grauman cs computer vision class at ut austin alyosha efros computational photography and learningbased methods in vision classes at carnegie mellon pascal fua introduction to computer vision class at epfl cs machine learning project suggestions https www cs utexas edu mooney projecttopics html cs machine learning project suggestions page project proposal due nov final project report due december general information and resources these are just suggestions gathered from various ongoing ut research projects related to machine learning feel free to propose your own idea particularly one that relates to your own ongoing research interests and projects i encourage you to talk to me about your project ideas during office hours the only real restriction is that projects should involve doing some research in machine learning rather than a writing a survey or discussion paper the ideal goal is to produce a research result that could be published as a scientific conference paper like the one you read for homework perhaps with some addtional followup work in addition to weka java ml code there is c code for decision trees in u mooney foil c code for ilp in u mooney foil prolog code for ilp available for aleph and ml systems in c available in mlc also checkout data resources available at the irvine ml repository the kdd cup repository and the conll shared task descriptions for the conference on natural language learning markov logic networks the alchemy system recently developed at the univ of washington combines statistical and relational learning to construct markov logic networks a cross between logic programs and markov nets local contact is cs ph d student lily mihalkova lilyanam cs utexas edu improve the speed or accuracy of alchemy current greedy beam search algorithm implement and test various ways of limiting the search space or improving the output of the beam search algorithm implement and test costsensitive discriminative weight training of mlns by adapting the algorithm from sen and getoor for use in alchemy bioinformatics there is an active local group of researchers working on data mining for bioinformatics in particular there is a group competing in a scientific competition called mousefunc i for predicting the function of mouse genes from various sources of information about the gene that is interested in help local contacts are prof edward marcotte in biochemistry marcotte icmb utexas edu and ece ph d student chase krumpleman krump lans ece utexas edu below is another project idea from prof edward marcotte and biology postdoc christine vogel cvogel mail utexas edu contact them for details we are interested in a particular version of these called synthetic lethal interactions in which either gene can be removed individually without affecting the cell but the removal of both kills the cell in particular the hubs in a synthetic lethal interaction network are important they compensate for the loss of many other genes i suspect these may often be genes critical for suppressing tumor formation although we haven t formally tested this idea a good project would be to try to predict which genes are hubs in these networksthat is learn what properties separate hubs from nonhubs with features based on expression data interaction data functions etc this would require playing with various classifiers etc on functional genomics data if anyone got nice results on this we have a collaborator in england that would be cs machine learning project suggestions https www cs utexas edu mooney projecttopics html willing to experimentally assay the genes for their participation in synthetic lethal interactions so the project would continue on beyond the class with real results computer graphics below is an idea from cs prof okan arikan okan cs utexas edu for applying machine learning to computer graphics please contact him for more details graphics applications films games etc require very detailed digital content such as geometric models textures animations creating such content often involves repetitive operations for example if we re creating an old decrepit house the artist would have to create the same dusty eroded appearance with spider webs distributed accordingly everywhere in the house the idea of this project is to learn a model of the user edits and generalizing these edits to cut down the creation time for digital environments in our old decrepit house example if we let the user create a single room of this house can we learn from this what it means to be old and decrepit and apply this to the rest of the house automatically can we learn and help the user as the user is performing these edits can we ever reach a state where after sufficient use of this system we can develop a model of user desired appearance and recreate it from very simple inputs on novel environments computer architecture there is a large architecture project in the department called trips cs prof kathryn mckinley mckinley cs utexas edu has proposed two potential topics for machine learning research in this area please contact her for details we have a bunch of data from simulated annealing of different schedules on trips we examined this data by hand to extract scheduling heuristics it would be interested to see if learning could do better i would also would like to use the same framework to generate register bank allocation and scheduling data from simulated annealing and see if we can derive some register bank assignment heuristics either independently or together with schedules business applications prof maytal saartsechansky maytal saartsechansky mccombs utexas edu in the school of business works in machine learning and data mining and has proposed the following topics please see her for details active information acquisition predictive models play a dominant role in numerous business intelligence tasks a critical factor affecting the knowledge captured by such a model is the quality of the information i e the training data from which the model is induced for many tasks potentially pertinent information is not immediately available but can be acquired at a cost traditionally information acquisition and modeling are addressed independently data are collected irrespective of the modeling objectives however information acquisition and predictive modeling in fact are mutually dependent newly acquired information affects the model induced from the data and the knowledge captured by the model can help determine what new information would be most useful to acquire information acquisition policies take advantage of this relationship to producing acquisition schedules an acquisition schedule is a ranking of potential information acquisitions in this case currently unknown feature values or missing labels target variables an ideal acquisition schedule would rank most highly those acquisitions that would yield the largest improvement in model quality per unit cost prior work proposed algorithms for acquisition of either class labels i e dependent target variables of training examples or of feature values in this project we propose and evaluate new comprehensive approaches for information acquisition to rank the acquisition of both labels and feature values that may be missing we will evaluate new approaches on several business data sets where feature values and class labels are costly to acquire information acquisition for compliance management domains compliance management pertains to the selection of tax reports to be audited or health care claims to be scrutinized for fraud noncompliance is a cs machine learning project suggestions https www cs utexas edu mooney projecttopics html substantial source of revenue loss that afflicts a variety of industries the u department of health and human services reported in that medicare alone lost billion to fraud or other improper payments to providers in the internal revenue service reported recently that the gap between taxes paid and taxes owed was between billion and billion in the latest year for which figures are available with about one sixth of this amount eventually collected substantial losses have also been reported by the auto insurance industry adding billions of dollars to auto premiums each year in all these scenarios at each point in time an analyst must decide whether to audit a case in order to recover or prevent a revenue loss such decisions are increasingly being guided by predictive models built based on historical audit outcomes due to the vast transaction volume review of all cases that might be related to fraud is prohibitively expensive thus effective predictive models that identify strong leads for further investigation are essential however there exists a hurdle shared by all compliance management sectors that renders model induction in this domain particularly challenging as in all supervised learning settings in order to induce a model it is necessary for training examples to be labeled meaning that the value of the target variable e g whether or not a particular claim is fraudulent must be acquired however audits are carefully chosen to target only cases which are predicted to have a high probability of being fraudulent and thus leading to revenue recovery this leads to a severely biased training sample which cripples model induction and revenue collection such models do not detect new pockets of noncompliance effectively furthermore rather than helping improve the model new audit outcomes merely reinforce existing perceptions and do not provide useful information an alternative approach is to complement the biased sample with additional audits carefully selected to significantly improve the model itself rather than to avoid imminent losses because audits are costly it is essential to devise selective information acquisition mechanism that would identify informative audits that will particularly improve model performance for a given acquisition cost existing active learning policies either assume that no data is available exante and that the sample is constructed exclusively by the active learner or that a representative sample of the underlying population is available exante these assumptions are fundamental to the acquisition policies subsequent actions and are severally violated in the compliance management domain due to the biased audit data thus the active learner ought to leverage this knowledge of biasness to help identify new informative acquisitions that can improve the model performance we have obtained a real data set on companies sales tax audits which we will use in this study to evaluate new policies the data include information about firms in a given state sales tax audit results and the amounts of money paid by companies following the audits computational linguistics natural language processing prof katrin erk katrin erk gmail com in the linguistics department has suggested the following two projects contact her for details guess word senses for items outside the lexicon suppose you want to tag each content word in your text with a sense but your lexicon is lacking entries a lot of words are not covered however the senses that you have in the lexicon are actually described as sets of words for example for the word bank you might have the senses depository financial institution bank banking concern banking company and bank cant camber so if you encounter a word that is not covered by the lexicon what you can do is to try to find another similar word that is covered by the lexicon and just use its sense the idea would be to use a semantic similarity method that you learn from corpus data e g the one proposed by lin for this you can evaluate your method on words that are covered by the lexicon by pretending that they are not covered and seeing which similar words are proposed detect occurrences of an idiom it is becoming more and more obvious lately that multiword expressions and idioms are not weird exceptions they occur a lot and if you cannot spot them your meaning analysis of the text will go wrong a text talking about letting the cat out of the bag need not be about cats now suppose you have training data labeled for whether a multiword expression or idiom is present can you build a classifier that will spot the idiom in new text the idea would be to use an existing parser as a basis occurrences of an idiom may vary syntactically either because they are syntactically variable let the cat out of the bag cat is out of the bag or because the parser doesn t treat occurrences uniformly idioms may also vary lexically for example you will find occurrences of let the secret out of the bag or even let catbert out of the cs machine learning project suggestions https www cs utexas edu mooney projecttopics html bag this is an attested occurrence features of the classifier could be syntactic subtrees local to the idiom in the training data and semantic classes of words that tend to occur with the idiom data mining the following suggestions come from prof inderjit dhillon data mining research group nonnegative matrix factorization nonnegative matrix factorization attempts to find a factorization of a matrix a into the product of two matrices b and c where the entries of b and c are nonnegative this has been used recently in unsupervised learning for various applications one project possibility is to code up and use nonnegative matrix factorization for some application possibilities include text analysis modelling topics in text image face processing or for problems in bioinformatics a second idea would be explore sparse nonnegative matrix factorization which has been recently proposed the contact for these projects is suvrit sra suvrit cs utexas edu powerlaw graphs much largescale graph data has node degrees that are powerlaw distributed a standard example is a web graph clustering of such graphs is somewhat difficult due to these distributionsone project idea would be to explore clustering such graphs some work has been done recently using min balanced cuts to cluster these graphs and there has been promise in using normalized cuts in this domain an option would be to compare graclus software for computing the normalized cut in a graph to other graph clustering methods to determine the most effective ways to compute clusters in powerlaw graphs the contact for this project is brian kulis kulis cs utexas edu experiments with svms several projects are possible for support vector machines one possibility is to find a good application create a support vector machine implementation and perform experiments another possibility is to compare different existing implementations of svms including svm code developed at ut in terms of speed and accuracy a third idea is to explore different methods of regularization for svms this would require some knowledge of optimization and compare performance contacts for this project are dongmin kim dmkim cs utexas edu suvrit sra suvrit cs utexas edu and brian kulis kulis cs utexas edu homework http people cs pitt edu kovashka htm homework due instructions please provide your code and your written answers your written answers should be in the form of a pdf or word document doc or docx your code should be written in matlab zip or tar your written answers and m files and upload the zip or tar file on courseweb assignments homework name the file zip or tar if you do not see the assignments button notify the instructor note if you are asked to implement something by yourself it is not ok to use or even look at existing matlab code for anything you are not asked to implement feel free to look up relevant matlab functions if you have questions about what you can use ask the instructor or the ta part i short answers points a pts bishop exercises and b pts propose a new problem that can be solved with machine learning one that we have not discussed in class and describe how you would go about solving it make sure to describe both the algorithm design as well as the data collection and evaluation of your algorithm of course you know very little about machine learning at this point unless you have prior experience with machine learning which will be taken into account when grading what are some potential problems with your approach c pts describe what overfitting is how we can detect it if we had access to the test data which usually we don t why it is a problem and discuss three possible ways to deal with it part ii matlab basics points generate two five thousand by one vectors of random numbers from a gaussian distribution with mean approximately and standard deviation approximately use matlab randn function add the nth value of the second vector to the nth value of the first vector by using a loop to get the number of loops you need to run use matlab size function time the above operation print the number and write the total time taken in your answer sheet use matlab tic and toc functions now do the same addition but without using a loop time this operation print the time and write it down read in this image into matlab as a matrix and write down its dimensions compute the mean along the third dimension and save the resulting matrix in a matrix variable find the largest value in the matrix and write it your answer sheet convert the matrix to a vector first and use matlab function replace any of the pixels equal to the largest pixel value and its neighbors by the value without using a loop use matlab function repmat find the indices in the resulting matrix that are larger than and report their count in your writeup plot a histogram of the values in the matrix and include the figure in your writeup part iii computing an image representation points in this problem you will compute a descriptor for an image the x in machine learning notation to do so you will compute the response of an image to each of a set of filters and then compute a histogram over the response values download the leungmalik filter bank code one function and read the description at the top of how to run it run the code to obtain the filter bank f and apply each of filters i e each f i to an image of your choice resulting in one resp im f i valid value for each filter homework http people cs pitt edu kovashka htm i note that im imread image filename im im is an image read into matlab and converted to grayscale compute a histogram for each of the i responses with bin sizes of your choice and concatenate the histograms this is the final representation for your image report its dimensionality in your writeup what test can you run to determine if the representation you computed is a useful representation include the answer in your writeup optional if you want to visualize each response call figure imshow resp part iv the classification pipeline points in this problem you will train a multiclass model to distinguish between different animals first copy the animals with attributes dataset originally appearing here from the following pitt afs directory you need to be on campus or use vpn to access the local copy which contains four feature type folders plus a classes txt file which you can also download from the original base package the dataset includes animal categories attributes and images for this problem we will not use the attributes you can use textread classes txt u to read in the class names four feature types are provided cqhist which are color histograms phoghist and sifthist which are two types of histograms of gradients and decaf which are features extracted from a deep neural network pick any one feature type to use for any feature type there is one text file for every image and files are organized by animal categories normalize your features divide all values in the descriptor for image i by the sum of values in that descriptor and do this for all images separately split the images features available for each animal into three groups a training set a validation set and a test set now you will train a multiway svm animal classifier you fill use matlab function model fitcecoc where of size nxd are your features and of size are the ground truth labels for the training samples the label values should be integers between and for animals to use the model you just learned you will call label predict model where of size mxd is the descriptor for the m samples whose labels you want to predict you might want to use a template svm t templatesvm standardize model fitcecoc learners t you have so far used the default misclassification parameters you will now automatically choose the best value for one particular parameter the misclassification cost something we will discuss later you will set all offdiagonal values in the cost matrix to the same value c that parameter can be modified by adding cost at the end of your call to fitcecoc where repmat c logical eye size the cost can be modified by adding boxconstraint at the end of your call to templatesvm try three different values of c for example and and pick that value for your final model that gives the best performance accuracy on the validation set to determine the accuracy of a model on some set compute what fraction of the images in that set were assigned the correct label i e the ground truth label that came with the dataset once you have fixed the cost parameter run your model on the test data and report that final accuracy in your writeup also report the error on the training data finally experiment with different amounts of training data and report how the error on the test data and training data changes as you add more training data include a plot in your writeup with at least five different values for the size of the training data on the xaxis and accuracy on the yaxis for the training and test sets separately i e show two curves explain what you are observing and why it might be happening grading rubric homework http people cs pitt edu kovashka htm a loading the features splitting the data and running the training testing with default parameters pts b using the validation set to pick the best c value pts c demonstrating how train test error changes as more training data is added pts d your writeup including explanations pts part v segmentation via clustering points for this problem you will perform image clustering with your favorite clustering method which you will implement you have great freedom in how you go about this problem we only care how good qualitatively your segmentation results are the features and segmentation method you use will be relatively simple so of course we don t expect the results to be anywhere close to perfect download both the images and optionally for qualitative evaluation human segmentations from the the berkeley segmentation dataset and benchmark to perform segmentation you need a representation for every image pixel for simplicity the only two representations you use will be the r g and b values of each pixel and the vertical and horizontal gradients of each pixel for the latter you can convert the image to grayscale and at each pixel i i j compute the difference in intensity between i i and i i j as the horizontal gradient and between i j and i i j as the vertical gradient experiment with different combinations of rgb values and gradients as your pixel representation you can include the x y location of each pixel in your clustering representation if you wish implement and perform clustering over the resulting representation space and place each pixel in some cluster you can choose to implement kmeans mean shift graph cuts or agglomerative clustering in your writeup explain how you chose which method to implement easiness is a good reason to pick a method just make sure to explain why you think one is easier than another see the following two functions for fast implementations of distance computations and distsqr if your method is still very slow feel free to downsample the image you can generate as many clusters as you d like and whatever distance function you like finally recolor the input images according to the cluster they belong to for example you can color all the pixels in a cluster with the mean color of pixels in that cluster include the recoloring result for images in your writeup discuss the quality of the segmentation explain any choices you made in how you implemented the clustering method and what parameters e g number of clusters and pixel representation you picked grading rubric a the correctness of your clustering method implementation pts b applying your clustering method on images including representing pixels and recoloring depending on cluster membership pts c your writeup including explanations pts homework http people cs pitt edu kovashka htm homework due instructions please provide your code and your written answers your written answers should be in the form of a pdf or word document doc or docx your code should be written in matlab zip or tar your written answers and m files and upload the zip or tar file on courseweb assignments homework name the file zip or tar note if you are asked to implement something by yourself it is not ok to use or even look at existing matlab code for anything you are not asked to implement feel free to look up relevant matlab functions if you have questions about what you can use ask the instructor or the ta part i knearest neighbors points in this example you will explore knn classification using matlab builtin knn function first read about how knn classification works in matlab here however do not matlab builtin kfold crossvalidation functionality you will use the pima indians diabetes dataset same as used in the version of cs taught by prof hauskrecht you will use the pimaindiansdiabetes data file and should also read the pimaindiansdiabetes names file both found at the data folder yellow link at the top the last value in each row contains the target value y for that row before you begin split the data into approximately equallysized folds your results reported below should be an average of the results when you train on the first folds and test on the remaining then if you train on the folds numbered through and the fold and testing on the fold etc for simplicity you can also just use folds of size and drop the remaining instances hint you can use the matlab function setdiff to find which fold you re supposed to use for training given what your test fold is you can get the indices of data points in a fold as follows if z is the size of a fold and i is the fold id z z also before you begin make sure to normalize the data x by subtracting the mean and dividing by the standard deviation over each dimension note that you should compute the mean and stdev using the training data only and then apply them on the test data this is because in a real application we do not see the test data until after we ship off our program code first use the default parameter values e g the value of k and the choice of distance functions and compute the accuracy on your test set hint use model fitcknn x y plot the test accuracy as a function of k use the following values of k explain what you observe hint fit the model just once and then use model numneighbors k before applying it for each value of k running this entire part should take less than a minute so far we have been weighing neighbors equally now we want to experiment with weighing them according to their distance to the test sample of interest implement a gaussianweighed knn classifier using the equation given in class experiment with three different values of the bandwidth parameter σ from the equations on the board and report the results consider multiplying the denominator in the exponential by if you want to use the exact same form as a gaussian kernel equivalent to scaling σ by for one fixed choice of parameter values use just one fold as opposed to nine folds as training data and report the accuracy why do you observe what you observe don t just say more data is better but you can informally talk about why more data increases the chance of finding good representatives for the test data grading rubric make sure to include the results in your written report homework http people cs pitt edu kovashka htm a points default parameter values solution including code to set up crossvalidation and compute average result over testing on test folds b points plot using different values of k discussion c points gaussian weighing solution d points less data experiment discussion part ii linear regression points we ll discuss this topic on in this problem you will solve a regression problem in two ways using the direct leastsquares solution or using gradient descent you will use the wine quality dataset use only the red wine data the goal is to find the quality score of some wine based on its attributes first divide the data into a training and test set using approximately for training you don t need to use crossvalidation for this problem normalize the data before you begin use the linear system of equations least squares solution in matlab using the backslash operator ax b x a b what are a x and b in the case of linear regression use the resulting solution to find the wine quality scores on the test data then measure and report in your written answers the distance between the true and predicted scores now implement the gradient descent solution for this you will need to initialize the weights in some way use either random values or all zeros then you repeat the following some number of times for this problem repeat times in each iteration compute the error function gradient using all training data points then adjust the weights in the direction opposite to the gradient apply the solution to the test set then compute and report the distance as above experiment with different learning rates e g ones in the range i e and report your observations grading rubric a points data set up and least squares solution b points gradient descent solution part iii fisher linear discriminant points we ll discuss this topic on you have the following twodimensional data the first five data points belong to one class and the second set of five to a second class download the starter code here add the following to that code compute the direction of the w vector corresponding to the fisher linear discriminant of the data points run the starter code which will plot the points in along with the fisher discriminant direction save the figure and include it in your written report part iv short answers points we ll discuss these topics roughly on 15 and a points bishop exercise b points in the discussion of svms we describe how we can account for nonlinearlyseparable data using slack variables see equations and in bishop one can also use slack variables for regression write down the optimization objective and constraints for linear regression similar to equations and for classification that results if you add slack variables homework http people cs pitt edu kovashka htm homework due instructions please provide your code and your written answers your written answers should be in the form of a pdf or word document doc or docx your code should be written in matlab zip or tar your written answers and m files and upload the zip or tar file on courseweb assignments homework name the file zip or tar note if you are asked to implement something by yourself it is not ok to use or even look at existing matlab code for anything you are not asked to implement feel free to look up relevant matlab functions if you have questions about what you can use ask the instructor or the ta do not discuss the solutions of this assignment until a week after the deadline part i short answers points what you need for parts a d has been discussed already and what you need for parts e g will be discussed on and a pts bishop exercise hints expand the norm notation remembering that the norm of a vector x is xtx that xtx is the same as an inner product of x with itself and transpose distribution properties then express the expanded form with kernel notation what type of kernel do you see b pts bishop exercise c pts bishop exercise hints this is a twoclass problem you have two constraints total one for the positive instance and one for the negative instance and these constraints are equalities because your positive and negative points have to lie on the margin and be support vectors use lagrange multipliers to make the constraints part of the optimization problem and find what w and b equal d pts show that linear regression with an objective to minimize the squared difference between true and predicted labels tn when regularized is equivalent to maximumaposteriori map estimation we ll model the noise between in the target labels tn with a gaussian distribution see figure in bishop examine the following questions in bishop when log is taken and which combines and show how these equations can be used to achieve the map objective in equation no need to compute its derivative what log base you use does not matter e pts the first part only of bishop exercise hint transform the righthand side into the lefthand side and work with discrete random variables f pts bishop exercise g pts just the last part of bishop exercise draw the graphical model corresponding to the joint probability part ii svm as a quadratic program points what you need has been discussed already in this part you will implement an svm as a quadratic program qp using matlab quadprog function but not the builtin matlab svm you will use the pima indians diabetes dataset from read under description for quadprog what qp matlab expects your job is to define the matrix h discussed in class on and in my notes for that day on page and to transform a maximization into a minimization what is the ft vector in the case of an svm first adjust your data by replacing labels of by labels of since the pima dataset denotes negative labels by and we want them to be do crossvalidation as in include a bias by appending a in front of the feature representation for every data sample that a lazy way but not fully correct way to not worry about the bias explicitly and you don t have to do it you can deal with the bias explicitly instead as we did in class and in my notes homework http people cs pitt edu kovashka htm make sure to standardize the data you will create a softmargin svm assuming a nonlinearly separable dataset which corresponds to setting the bounds on the alphas to be α c where the scalar c is the misclassification cost use c use the call x quadprog h f a b aeq beq lb ub and set the third through sixth argument to you can also use aeq beq to define the constraint that the alphas times the labels should sum to i forgot about this constraint when i wrote this up so it fine to skip it note that you might find that all of your training samples are support vectors that ok it also ok if you get warnings from matlab compute and report the accuracy on the test set and on the training set the latter is just for comparison now train and apply just on the test set the builtin matlab svm however remove any changes to the feature representation that were made to account for the bias use the same c value as before report the builtin matlab svm accuracy on the test set if the wtx for some test instance is exactly zero set its sign to time how long it takes to learn the model solve for w with the qp approach versus the builtin matlab svm using the qp approach to training an svm create a function with the signature function w c as well as another function function accuracy w the are the predicted labels and are the the raw wtx scores before taking the sign of that expression name your script for part ii m make calls to your and functions in your script your accuracy should be around or over useful matlab functions for this and the next part quadprog sign size length fprintf tic toc find randperm strcmp unique setdiff nchoosek mode you can ignore outputs from a function using e g a b part iii multiclass svm out of twoclass svms points what you need has been discussed already in this part you will use your code from part ii to solve a multiclass prediction task on the multiclass iris dataset this dataset has the same format as the pima indians dataset but you need to read the data differently use the iris data file read it as classes textread iris data f f f f which will give you vectors of real numbers through which you concatenate horizontally to form your feature matrix x and a vector of strings classes first you need to map the last attribute in each row to an integer between and that denotes the class label find the unique class names and then for each sample call y i find strcmp classes i use a single training test split rather than and choose images for training and for testing however repeat the experiment times i e with different train test splits and report the average results build two types of multiclass svms onevsall and onevsone the methodology for constructing onevsall and onevsone can be found in the slide deck from towards the end for the onevsone svm don t forget to map the labels back to labels for the iris classes for example you can say labels labels labels labels then record the votes in a x number of onevsone classifiers matrix and call mode votes note that for onevsall you need to use the decision values to get the argmax in each row of a x number of onevsall classifiers scores matrix use inds max scores homework http people cs pitt edu kovashka htm write your two classifiers in two separate scripts one with a signature accuracy c and the other accuracy c same input output but different function name then call these functions in a script called m report the accuracy of both the onevsall and onevsone svms averaged over the runs also output what the performance of a classifier that guesses randomly what the class is for comparison there no need to implement this random classifier just give the expected number part iv perceptron points what you need has been discussed already in this part you will trace through the a run of the perceptron algorithm use the following data x y ones ones this data is linearly separable and can be plotted in using its two feature dimensions your goal is to create figures similar to figure in bishop we are providing code that you can use to plot the points w and the decision boundary which passes through the origin and is perpendicular to w download this code here it will plot positive points as hollow circles and negatives as filled circles it will also plot correctly classified points in green and misclassified ones in red your goal is to implement the perceptron algorithm and use the provided code to trace through several iterations of the method use all the data for training for the purposes of this exercise set η to remember that in each step the weight vector w is adjusted using one misclassified example you will use the equations from the slides for perceptron on 15 instead of a basis function you will just use x i e φ x x you can use your function to compute the prediction and accuracy on a test sample given a w of course you have to call that on the training set in your code in each iteration output the iteration id and the two feature dimensions for the misclassified example that is being used to correct the w use the feature dimensions to identify which point is being used in your writeup word or pdf file for three consecutive iterations of the method include a plot of the misclassified points and current w then point out the point that was used for the update of w briefly describe what you are observing if too many or too few iterations are taking place terminate the run and start another run homework http people cs pitt edu kovashka htm homework due instructions please provide your code and your written answers your written answers should be in the form of a pdf or word document doc or docx your code should be written in matlab zip or tar your written answers and m files and upload the zip or tar file on courseweb assignments homework name the file zip or tar note if you are asked to implement something by yourself it is not ok to use or even look at existing matlab code for anything you are not asked to implement feel free to look up relevant matlab functions if you have questions about what you can use ask the instructor or the ta do not discuss the solutions of this assignment until a week after the deadline part i short answers points what you need has been discussed already a pts consider the following graphical model the variables in this model are the following r can take values or depending on whether it rained or not can take values or depending on whether the sprinkler in tom yard was on or not t can take values or depending on whether the grass in tom yard is wet or not j can take values or depending on whether the grass in julia yard is wet or not let the prior probability that it rained be and the prior probability that the sprinkler in tom yard was on be we also have the following conditional probability tables r j t f r t t t t f f t f f compute the probability that the sprinkler was on given that tom grass is wet keep in mind how you can compute the probability of individual variables from a joint probability how you express a joint probability from a graphical model and the axioms of probability also compute the probability that the sprinkler was on given that the grass in both tom and julia yards homework http people cs pitt edu kovashka htm is wet how do these two probabilities compare what is the intuitive explanation for their difference b pts in this exercise we ll do some crossdomain recommendation where we assume that there is a correlation between a user taste in music and film we ll only consider one music genre namely jazz which we ll denote by j and four films waking life denoted by w borat denoted by b cinema paradiso denoted by c and requiem for a dream denoted by r we ll assume that conditioned on whether the user likes jazz the movie likes dislikes are independent the prior probability of liking jazz is we ve defined the following combined conditional probability table where means likes j w b c r t f what is the probability the user likes jazz given that she likes the first and fourth movies but dislikes the second and third how about the probability that the user likes jazz given that she likes all the movies c pts bishop exercise part ii hidden markov model for partofspeech tagging points what you need will be discussed on march we ll use the hmm from our inclass partofspeech tagging example whose states are propnoun noun verb det we ll denote propnoun as a state noun as a state verb as a state and det as a state remember that we also have the start state and end state sf the transition probabilities are similar to those given in page of this slide deck but with one correction they can be obtained in this file you can change how the probabilities are stored if you prefer we ll define the observation probabilities as follows these are also given in the file state observation john mary cat saw ate a the propnoun noun verb det a pts write code to compute the probability of observing the following sentences in two ways using the naive solution and using the efficient solution you can map each word to a number that is its index into our vocabulary the union of the column headers except the first one then a sentence is just a vector of numbers john saw the cat or using our mapping to numbers sent john ate john saw mary mary saw john cat saw the john john saw the saw john ate the cat homework http people cs pitt edu kovashka htm instructions create two functions function prob a b n m sent and a function with the same signature the inputs are defined in our slides you can use this code to get combinations with replacement to get your list of possible state sequences also you can use fprintf p t n vocab sent prob to print the probability of a sentence with decimal places if you include the probability of transfering to the end state in the efficient solution make sure you also include it in the naive solution include the probabilities of the above sentences from both methods and discuss some greater than less than relations between the resulting probabilities for the different sentences include any scripts you used b pts write out the efficient solution by hand just for the sentence john ate and show your work then check your answer with the answer you got from your program part iii adaboost points what you need will be discussed on march and in this exercise you ll implement the adaboost method defined on pages in bishop section use decision stumps as your weak classifiers for each decision stump use at least thresholds use the pima dataset from and and the code for crossvalidation and feature normalization from submit the following code you can add inputs outputs to the signatures function mistakes weights where are the labels on the test set output by your best decision stump mistakes is the indicator value vector used to compute the quantity jm from bishop eq and weights are the wn m function accuracy adaboost m where accuracy are the final accuracy and labels on the test data a script m to set up your train test splits run your adaboost function and evaluate accuracy machine learning http people cs pitt edu kovashka machine learning announcements overview policies schedule resources machine learning spring location sennott square time monday and wednesday instructor adriana kovashka email kovashka at cs dot pitt dot edu use at the beginning of the subject line office sennott square office hours monday and wednesday ta changsheng liu email changsheng at cs dot pitt dot edu use at the beginning of the subject line ta office sennott square ta office hours wednesday and thursday announcements the projects grading rubric has been posted the presentation schedule has been posted is out and is due top overview course description the course will cover the following topics learning basics unsupervised learning supervised learning classification regression clustering dimensionality reduction nearest neighbor classification support vector machines density estimation bayesian belief networks hidden markov models expectation maximization decision trees ensembles deep learning active and transfer learning and information retrieval the course will include many examples of how machine learning is used in computer vision the homework assignments will have some bias towards applying machine learning techniques to computer vision problems and datasets there will be two exams and a final project prerequisites knowledge of matrices and linear algebra cs probability stat statistics stat programming and algorithm development and analysis cs or equivalent or the permission of the instructor programming homework assignments will be written in matlab the final project can be written in any language textbook christopher m bishop pattern recognition and machine learning springer book resources top policies grading grading will be based on the following components homework project status presentation and report final presentation and report midterm exam final exam participation homework there will be four homework assignments you will submit your homework using courseweb navigate to the courseweb page for then click on assignments on the left and the corresponding homework number attach in a single zip file your written responses and code name the file as zip or tar homework is due at on the due date project machine learning http people cs pitt edu kovashka students are encouraged to work in groups of two see exceptions below for their final project a project can be design of a new method for some problem which may or may not extend existing methods you don t have to invent a new svm but can for example show an algorithm for solving some problem that incorporates or modifies svms or other classifiers organized and interleaved in some fashion an implemenation of some method discussed in class or other method with instructor approval note that you should also discuss algorithmic choices and tradeoffs an application of techniques we studied in class or another method with instructor approval to a new problem that we have not discussed in class note that you should also describe why this problem is important and challenging i expect this to be a significant amount of work and not just a straightup run of some package on existing data and you are allowed to use existing code for known methods an implementation of a real working system e g an app that can solve some machine learning task note you should also discuss design challenges and evaluate your system quantitatively and qualitatively with real users experimental comparison of a number of existing techniques on a known problem and detailed discussion and analysis of the results this one can only be done by students working individually an extensive literature review and analysis on one of the topics covered in class this one can only be done by students working individually other speak with the instructor all projects should include some experimental results that validate your method application also think about what data and or code you will use a project can become a subsequent conference publication ideas for computer vision project ideas you can look at the list of datasets and tasks below for inspiration or read some paper abstracts on this page for nlp project ideas see this page from christopher manning also look at the following list of project suggestions from ray mooney but please do not contact any of the contacts given this one from carlos guestrin this one from andreas krause and this one from andrew ng timeline and deliverables you will submit a project proposal in february and receive feedback from the instructor in the proposal describe what techniques and data you plan to use and what existing work there is on the subject in late march you will present your progress to your classmates for feedback describe your progress on the project and any problems encountered along the way at the end of the semester you will present your final project and submit a final project report using the cvpr latex template the final report should resemble a conference paper and should include clear problem definition and argumentation of why this problem is important overview of related work detailed explanation of the approach wellmotivated experimental evaluation including setup description and a description of what each team member did in the final presentation describe your approach and experimental findings in a clear and engaging fashion please look at this project grading rubric all project written items are due at on courseweb status report persentations will be minutes long and final presentations will be minutes long if you described something in the status report presentation don t repeat it except maybe with one sentence in the final presentation exams there will be both a midterm exam and a final exam the latter of which will mostly focus on material from the second part of the class but will be cumulative participation students are expected to regularly attend the class lectures and should actively engage in inclass discussions your participation grade will be based on how actively you participated in class you can actively participate by for example responding to the instructor or others questions asking questions or making meaningful remarks and comments about the lecture or posting questions or responses on piazza you are also encouraged to bring in relevant articles you saw in the news late policy you get free late days i e you can submit homework a total of days late for example you can submit one problem set hours late and another hours late once you ve used up your free late days you will incur a penalty of from the total project credit possible for each late day a late day is anything from minute to hours collaboration policy and academic honesty you will do your work exams and homework individually the work you turn in must be your own work you are allowed to discuss the problem sets with your classmates but do not look at code they might have written for the problem sets you are also not allowed to search for code on the internet use solutions posted online unless you are explicitly allowed to look at those or to use matlab implementation if you are asked to write your own code when in doubt about what you can or cannot use ask the instructor plagiarism will cause you to fail the class and receive disciplinary penalty please consult the university guidelines on academic integrity note on disabilities if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services drs william pitt union drsrecep pitt edu for asl users as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course machine learning http people cs pitt edu kovashka note on medical conditions if you have a medical condition which will prevent you from doing a certain assignment or coming to class you must inform the instructor of this before the deadline you must then submit documentation of your condition within a week of the assignment deadline statement on classroom recording to ensure the free and open discussion of ideas students may not record classroom lectures discussion and or activities without the advance written permission of the instructor and any such recording properly approved in advance can be used solely for the student own private use top schedule date chapter topic readings lecture slides due basics introduction and administrativia this website pptx pdf out matlab ml tasks notation and challenges bishop ch sec pptx pdf more matlab biasvariance tradeoff pptx pdf no class mlk day biasvariance tradeoff cont d data representations szeliski sec grauman leibe ch pptx pdf unsupervised learning clustering bishop ch pptx pdf dimensionality reduction bishop sec daume pptx pdf supervised learning intro and linear models nearest neighbors bishop sec pptx pdf notes due out nearest neighbors cont d linear algebra review pptx pdf review cont d bishop sec pptx pdf linear regression bishop sec pptx pdf linear models for classification bishop sec pptx pdf notes linear models for classification cont d support vector machines bishop sec pptx pdf notes proposal due support vector machines cont d support vector machines optimization solution notes classification probabilistic models probability review density estimation bishop sec pptx pdf pptx pdf due out midterm exam no class spring break bayesian belief networks bishop sec pptx pdf bayesian belief networks cont d machine learning http people cs pitt edu kovashka markov random fields hidden markov models bishop sec skip extra jurafsky martin ppt pdf hidden markov models cont d pptx pdf due out hidden markov models cont d pptx pdf expectation maximization bishop sec pptx pdf review changsheng pptx pdf project status report presentations status report due classification other topics ensembles bagging and boosting decision trees bishop sec skip pptx pdf neural networks bishop sec skip pptx pdf neural networks cont d active learning and crowdsourcing pptx pdf due final exam project presentations final report due friday top resources this course was inspired by the following courses machine learning by milos hauskrecht university of pittsburgh spring introduction to machine learning by dhruv batra virginia tech spring machine learning by tommi jaakkola mit machine learning by subhransu maji umass amhrest spring machine learning by erik sudderth brown university fall computer vision by kristen grauman ut austin spring computer vision by derek hoiem uiuc spring natural language processing by ray mooney ut austin tutorials matlab tutorial linear algebra review by feifei li brief machine learning intro by aditya khosla and joseph lim resources list compiled by devi parikh some computer vision datasets and tasks microsoft coco common objects in context object recognition segmentation image description imagenet object recognition sun database scenes caltechucsd birds finegrained object recognition msrc annotations active learning animals with attributes attributebased recognition apascal ayahoo attributebased recognition shoes attributebased search inria movie actions action recognition adl egocentric action recognition action quality evaluating action quality cardb historical cars style classification of cars recognizing image style photographic style classification judd gaze visual saliency prediction visual persuasion predicting subtle messages in images vqa visual questionanswering recognition datasets list compiled by kristen grauman human activity datasets list compiled by chaoyeh chen some code of interest libsvm by chihchung chang and chihjen lin machine learning http people cs pitt edu kovashka svm light by thorsten joachims vlfeat feature extraction tutorials and more by andrea vedaldi gist feature extraction by aude oliva and antonio torralba caffe deep learning code by yangqing jia et al top notes on k nearest neighbors february formal definition let x be our test data point and nk x be the indices of the k nearest neighbors of x in rd classification in classification we are trying to predict some discrete target label e g for spam no spam or in multi class problems some class id e g this movie is a drama i e class id action i e class id indie i e class id etc then y argmaxc yi c where c is the class id this can also be written as y argmaxc i nk x i yi c regression in regression we want to predict some continuous label e g price of a house based on its size year of construction etc then y i nk x yi distance metrics euclidean d d mahalanobis we want to weigh different dimensions differently e g because in some dimen sions points are naturally spread out so distances are on average larged then we can define a distance as d x z d i th dimension xi zi where σ i is the variance in the more generally we can write d x z x z ta x z where a is a diag onal matrix with the σi along the diagonal this is an example of a mahalanobis distance for a distance of this form to be a mahalanobis distance a has to be positive semi definine i e it has to be a symmetric matrix for which xtax for all x rd minkowski we define d x z d xi zi p p note that if p this becomes the manhattan distance and if p it is the euclidean distance weighted k nn for this generalization we will use all n training points i e k n let wi be the weight of the i th instance note that this wi depends on the particular query sample x one choice of a weighting function is a gaussian kernel wi x xi e we need a minus sign because we want a point close to x i e a point with a small distance to x to get a high weight note that σ is the bandwidth parameter and it expresses how quickly our weight function drops off as points becomes further and further from the query x classification the weighted prediction becomes y argmaxc n i wii yi c regression the weighted average becomes y n wiyi wi volume of a shell with width e to be discussed on wednesday the volume of a sphere with radius r in d dimensions is kdrd see bishop section say we have a sphere with radius and a thin shell of the sphere with thickness e then v shell kd d kd e d kd d e d this tends to as d tends to infinity thus most of the volume of the sphere is in that thin outer shell which is counter intuitive acknowledgement these notes are based on dhruv batra notes prof adriana kovashka university of pittsburgh february solution via least squares solution via gradient descent regularized least squares statistical view of regression dealing with outliers sometimes want to add a bias term can add as such that x xd figure from milos hauskrecht f x t at training time use given xn tn to estimate mapping function f objective minimize f xi ti for all i n xi are the input features d dimensional ti is the target output label given by human oracle at test time use f to make prediction for some new xtest problem is called classification if does your patient have cancer should your bank give this person a credit card is it going to rain tomorrow what animal is in this image problem is called regression if how much should you ask for this house what is the temperature going to be tomorrow what score should your system give to this person figure skating performance y and t will be used interchangeably regression the goal is to make quantitative real valued predictions on the basis of a vector of features or attributes example predicting vehicle fuel efficiency mpg from attributes we need to specify the class of functions e g linear select how to measure pred ict io n loss solve the resulting minimization problem linear regression x we begin by considering linear regression easy to extend to more com plex predict ions lat er on f r r f a r f x w wo w f x w wo w x wdxd where w wo w w d t are parameters we need to set fit line to points use parameters of line to predict the y coordinate of a new data point xnew find parameters of plane linear regression squared loss x f x w wo f x w wo w x wdxd we can measure the prediction loss in terms of squared error loss y y y y so that the empirical loss on n training samples becomes mean squared error l n w i l yi j xi w tommi jaakkola mit csail plan for today linear regression definition regularized least squares statistical view of regression dealing with outliers linear regression estimation we have to minimize the empirical squared loss optimality conditions derivation a a l n jn w yi w i w w n i l now using d dimensions j a system of equations in w using d l dimensions linear regression matrix notation we can express the solution a bit more generally by resorting to a matrix notation y yn so that x w n n yn n lly n inear regression so ution by setting the derivatives of iiy n to zero we get the same optimality conditions as before now expressed in a matrix form t n l y xwll w n y xw y xw which gives the solution is a linear function of the outputs y now using d dimensions id e iva t ive of loss l w y xw t y xw l w yt wt xt y x w id e iri v a t i v e of loss l w y xw t y xw l w yt w t xt y x w w t t t t t t w j t w y y w x y y xw w x xw l ll e ast aire soll u t i o a l w x t y x t x w challenges computing the pseudoinverse might be slow for large matrices cubic in number of features d due to pseudoinverse linear in number of samples n we might want to adjust solution as new examples come in without recomputing the pseudoinverse for each new sample that comes in another solution gradient descent cost linear in both d and n if d use gradient descent the same thing but do it for each training sample separately rather than as a batch a k a sequential a k a online w τ w τ α jni w τ α yi w τ txi xi global vs local minima what happens when we get into a local minimum linear regression definition solution via least squares solution via gradient descent dealing with outliers example polynomial curve fitting m y x w w j x w j x j o where q x are known as basis functions typica ly x so that acts as a bias lest we f unc tions j x xd consider the error function data term regularization term with the sum of squares error function and a quadratic regularizer we get which is minimized by with a more general regularizer we have isosurfaces w p constant lasso quadratic lasso tends to generate sparser solutions than a quadratic regularizer statistical view of regression the mean squared prediction error setting is equivalent to the maximum likelihood estimation setting see hidden slides for more plan for today linear regression definition solution via least squares solution via gradient descent regularized least squares statistical view of regression hypothesize and test try all possible parameter combinations repeatedly sample enough points to solve for parameters each point votes for all consistent parameters e g each point votes for all possible lines on which it might lie score the given parameters number of consistent points choose from among the set of parameters noise clutter features they will cast votes too but typically their votes should be inconsistent with the majority of good features two methods hough transform and ransac adapted from derek hoiem and kristen grauman y b x m image space hough parameter space connection between image x y and hough m b spaces a line in the image corresponds to a point in hough space y b x m image space hough parameter space connection between image x y and hough m b spaces a line in the image corresponds to a point in hough space what does a point in the image space map to answer the solutions of b this is a line in hough space to go from image space to hough space given a set of points x y find all m b such that y mx b y b x m image space hough parameter space how can we use this to find the most likely parameters m b for the most prominent line in the image space let each edge point in image space vote for a set of possible parameters in hough space accumulate votes in discrete set of bins parameters with the most votes indicate line in image space ransac random sample consensus approach we want to avoid the impact of outliers so let look for inliers and use those only intuition if an outlier is chosen to compute the current fit then the resulting line won t have much support from rest of the points ransac general form ransac loop randomly select a seed group of points on which to base model estimate fit model to these points find inliers to this model i e points whose distance from the line is less than t if there are d or more inliers re compute estimate of model on all of the inliers repeat n times keep the model with the largest number of inliers fischler bolles in algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence line fitting example algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence ransac line fitting example algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence line fitting example n i algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence algorithm n i sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model prof adriana kovashka university of pittsburgh february plan for today regression for classification fisher linear discriminant perceptron logistic regression multi way classification generative vs discriminative models classification example digit recognition binary digits binary digit actual label target label in learning ii classification via regression suppose we ignore the fact that the target output y is binary e g rather than a continuous variable so we will estimate a linear regression function j x w wq wdxd wo xt w based on the available data as before assuming y f x w e e rv n o a then the ml objective for the parameters w reduces to least squares fitting we can use the resulting regression function to classify any new test example x according to label if f x w and label otherwise f x w therefore defines a linear decision boundary that partitions the input space into two class specific regions half spaces given the dissociation between the objective classification and the estimation criterion regression it is not clear that this approach leads to sens ible results fn sometimes good sometimes bad the effect of outliers magenta least squares green logistic regression with three classes left least squares right logistic regression projected point in r z xi w w we can study how well the projected points zn viewed as functions of w are separated across the classes we can study how well the projected points zn viewed as functions of are separated across the classes by varying w we get different levels of separation between the projected points we would like to find that somehow maximizes the separation of the projected points across classes we can quantify the separation overlap in terms of means and variances of the resulting dimensional class distributions r class descriptions in rd class n samples mean covariance o class n samples mean covariance projected class descriptions in r class n samples mean variance w f class n samples mean µf w variance w f w estimation criterion we find that maximizes separation of projected means j fi sh er w sum of within class variances µf solution class separation fisher linear discriminant figures from bishop plan for today regression for classification fisher linear discriminant logistic regression multi way classification generative vs discriminative models rosenblatt prediction rule where loss just using the misclassified examples loss learning algorithm update rule interpretation if sample is being misclassified make the weight vector more like it figures from bishop plan for today regression for classification fisher linear discriminant perceptron multi way classification generative vs discriminative models suppose we know the class conditional densit ies p xly for y l as well as the overall class frequencies p y how do we decide which class a new example x belongs to so as to minimize the overall probability of error suppose we know the class conditional densities p xly for y l as well as the overall class frequencies p y how do we decide which class a new example x belongs to so as to minimize the overall probability of error the minimum probability of error decisions are given by y argmax p x ly p y y o l arg max p ylx y o l the optimal decisions are based on the posterior class probabilit iesp y lx for binary classification problems we can write these decisions as p y l l x y if log p y oxl and y otherwise the optimal decisions are based on the posterior class probabilities p y lx for binary classification problems we can write these decisions as p y l l x y if log p y oxl and y otherwise we generally don t know p ylx but we can parameterize the possible decisions according to p y llx t log w w w py ox our log odds model p y l lx t log p y oxl w x w gives rise to a specific form for the conditional probability over the labels the logistic model p y l l x w g wo x t where g z exp z is a logistic squashing function that turns linear predictions int o probabilit ies z logistic regression models imply a linear decision boundary p y llx wo xt p y olx t o oo o ooo ao o rr i o o l j o o o o o f o j o o i a s o o cf o d o o o o o oo r od o fl odb qo i oo cxi cxi o a a ao es a o a cb ooo o rl class as with the linear regression models we can fit the logistic models using the maximum conditional log likelihood criterion where n l d w logp yi lx i w i l p y l lx w g wo stochastic gradient ascent we can try to maximize the log likelihood in an on line or incremental fashion given each training input xi and the binary label yi we can change the parameters w slightly to increase the corresponding log probability w w rj aw log p yilxi w w tj yi p yi l lx i w prediction error where tj is the learning rate gradient ascent of the log likelihood we can also perform gradient ascent steps on the log likelihood of all the training labels given examples at the same time in other words a w w owl d w w t yi p yi llxi w i i l we have a probabilistic model m of some phenomena but we do not know its parameters each execution of m produces an observation x i according to the unknown distribution induced by m goal after observing x x n estimate the model parameters that generated the observed data this vector parameter can be used to predict future data mle principle choose parameters that maximize the likelihood of the data the likelihood of the observed data given the model parameters is the conditional probability that the model m with parameters produces x x n l pr x x n m in mle we seek the model parameters that maximize the likelihood when tossed it can land in one of two positions head h or tail t we denote by the unknown probability p h how good is a particular it depends on how likely it is to generate the observed data ld p d p x m m the likelihood for the sequence h t t h h is ld ld nh log nt log taking derivative and equating it to we get nh nt ˆ nh nh nt overconfidence better maximum a posteriori map plan for today regression for classification fisher linear discriminant perceptron logistic regression generative vs discriminative models instead of just two classes we now have c classes e g predict which movie genre a viewer likes best possible answers action drama indie thriller etc two approaches one vs all one vs one one vs all a k a one vs others train k classifiers in each pos data from class i neg data from classes other than i the class with the most confident prediction wins example you have classes train classifiers vs others score vs others score vs others score vs other score final prediction class issues one vs one a k a all vs all train k k binary classifiers all pairs of classes they all vote for the label example you have classes then train classifiers vs vs vs vs vs vs votes final prediction is class what are some problems with this approach to doing multi class there are natively multi class methods figures from bishop plan for today regression for classification fisher linear discriminant perceptron logistic regression multi way classification binary case mutli class case why are these called generative can use them to generate new samples x perhaps this is overkill consider again a binary classification task with y l labels not as before and linear discriminant functions f x w wo xt parameterized by wa and wd t the predicted label is simply given by the sign of the discriminant function y sign f x w we are only interested in getting the labels correct no probabilities are associated with the predictions when the training set xn yn is linearly separable we can find parameters w such that yi wo xf w i n i e the sign of the discriminant function agrees with the label there are many possible solutions perhaps we can find a better discriminant boundary by requiring that the training examples are separated with a fixed margin yi wo xf w i n we minimize the regularization penalty d l wi subject to the classification constraints i l x x x x x x x q x x x x x for i n x x x x x prof adriana kovashka university of pittsburgh february homework deadline is now we ll have covered everything you need today or at the latest on monday project proposal due tonight on courseweb how many of you want me to print handouts for next time linear support vector machines non linear svms and the kernel trick soft margin svms example use of svms advanced topics very briefly structured svms latent variables how to solve the svm problem next class let a w c x x y ax cy b let a w c x x y ax cy b w x b lines in let a w c x x y ax cy b w x b lines in let a w c x x y ax cy b w x b d b distance from point to line lines in let a w c x y ax cy b w x b b w x b distance from d w point to line find linear function to separate positive and negative examples xi positive xi negative w b xi w b which line is best discriminative classifier based on optimal separating line for case maximize the margin between the positive and negative training examples xi positive yi xi w b xi negative yi xi w b support vectors margin for support vectors xi w b xi positive yi xi w b xi negative yi xi w b for support vectors xi w b distance between point and line for support vectors xi w b w support vectors margin wτ x b m want line that maximizes the margin xi positive yi xi w b xi negative yi xi w b for support vectors xi w b distance between point and line xi w b w support vectors margin therefore the margin is w maximize margin w correctly classify all training data points xi positive yi xi w b xi negative yi xi w b quadratic optimization problem one constraint for each training point note sign trick solution w i i yi xi solution w i i yi xi b yi w xi for any support vector classification function f x sign w x b sign i yi xi x b if f x classify as negative otherwise classify as positive notice that it relies on an inner product between the test point x and the support vectors xi solving the optimization problem also involves computing the inner products xi xj between all pairs of training points f x sign w x b sign i i yi xi x b adapted from milos hauskrecht plan for today linear support vector machines soft margin svms example use of svms advanced topics very briefly structured svms latent variables how to solve the svm problem next class datasets that are linearly separable work out great x but what if the dataset is just too hard x we can map it to a higher dimensional space general idea the original input space can always be mapped to some higher dimensional feature space where the training set is separable consider the mapping x x x y x y xy k x y xy svetlana lazebnik the kernel trick the linear classifier relies on dot product between vectors k xi xj xi xj if every data point is mapped into high dimensional space via some transformation φ xi φ xi the dot product becomes k xi xj φ xi φ xj a kernel function is similarity function that corresponds to an inner product in some expanded feature space the kernel trick instead of explicitly computing the lifting transformation φ x define a kernel function k such that k xi xj φ xi φ xj andrew moore examples of kernel functions linear k xi x j i j polynomials of degree up to d 𝐾 𝑥𝑖 𝑥𝑗 𝑥𝑖𝑇𝑥𝑗 𝑑 gaussian rbf k xi x j exp histogram intersection k xi x j min k xi k x j k andrew moore carlos guestrin probllem chedki n g if a g i ven k x x x jr f u llfi llls the condit i o n for a lkernell i d ifficuit we need to prove or di spr o ve for any set w ork ar ou n d l tik xi xj tj i j l it i easy to construct functi o n k t h a t are p osit i ve definite lkernells we can construct kerne ls fr o m scratch for any cp x jrm k x x cp x cp x ntm i a lkerne i if d x x x jr i a distance fu nct ion i e d x x for a lll x x e x d x x onlly for x x d x x d x x for allll x x e x d x x d x x d x x for allll x x ux e x then k x x exp d x x i a kernell we can construct kernels fr o m other kernels if k i a lkernell and a then ak a nd k a are lkernells if are kerne ls then and are lkernells plan for today linear support vector machines non linear svms and the kernel trick advanced topics very briefly structured svms latent variables how to solve the svm problem next class the w that minimizes maximize margin misclassification cost data samples slack variable the w that minimizes maximize margin minimize misclassification board what about multi class svms in practice we obtain a multi class svm by combining two class svms one vs others training learn an svm for each class vs the others testing apply each svm to the test example and assign it to the class of the svm that returns the highest decision value one vs one training learn an svm for each pair of classes testing each learned svm votes for a class to assign to the test example there are also natively multi class formulations crammer and singer jmlr svms for recognition define your representation for each example select a kernel function compute pairwise kernel values between labeled examples use this kernel matrix to solve for svm support vectors weights to classify a new example compute kernel values between new input and support vectors apply weights check sign of output example learning gender with svms moghaddam and yang learning gender with support faces tpami moghaddam and yang face gesture support faces moghaddam and yang learning gender with support faces tpami human vs machine svms performed better than any single human test subject at either resolution kristen grauman plan for today linear support vector machines non linear svms and the kernel trick soft margin svms example use of svms how to solve the svm problem next class y is a vector tsochantaridis et al jmlr adapted from s nowozin and c lampert svm vs logistic regression when viewed from the point of view of regularized empirical loss minimization svm and logistic regression appear quite similar svm logp yi lx w l n logistic n l logg yi wo xf w j i l where g z exp z is the logistic function note that we have transformed the problem maximizing the penalized log likelihood into minimizing negative penalized log likelihood svm vs logistic regression cont d the difference comes from how we penalize errors n az both n l loss i wo ll i l svm loss z z regularized logistic reg loss z log l exp z o l j j j z pros svms pros and cons kernel based framework is very powerful flexible often a sparse set of support vectors compact at test time work very well in practice even with very small training sample sizes solution can be formulated as a quadratic program next time many publicly available svm packages e g libsvm liblinear svmlight or use built in matlab version but slower cons can be tricky to select best kernel function for a problem computation memory at training time must compute kernel values for all example pairs learning can take a very long time for large scale problems adapted from lana lazebnik min ll subject to yi wo xf i n yi wo xf i n let start by representing the constraints as losses max a a o yi wo xit yi wo xf oo ot yi wo xf i n let start by representing the constraints as losses max a yi wo xit yi wo xf a o oo ot and rewrite the minimization problem in terms of these min w min ll subject to yi wo xf w i n let start by representing the constraints as losses max a yi wo xi t yi wo xf a o oo ot and rewrite the minimization problem in terms of these max min o i o w n lai yi wo xf i l j w o as a result we have to be able to minimize j w a with respect to parameters w for any fixed setting of the lagrange multipliers ai max ai o w n l cxi l yi wo xf i l j w a we can find the optimal w as a function of ai by setting the derivatives to zero a j w a a j w a wo l aiyixi i l n laiyi i l we can then substitute the solution a n j w a a l o iyixi i l n j w a aiyi owo i l back into the objective and get after some algebra n max a l ai l yi wo xf i l i i a iy i max a i i o iyi subject to ai and ei aiyi only a corresponding to pport vectors will be non zero subject to ai and ei aiyi only a corresponding to pport vectors will be non zero we can make predictions on any new example x according to the sign of the discriminant function subject to ai and ei aiyi only a corresponding to pport vectors will be non zero we can make predictions on any new example x according to the sign of the discriminant function w xt wa xt l iyixi i l ai by maximizing subject to ai and ei aiyi only a corresponding to pport vectors will be non zero we can make predictions on any new example x according to the sign of the discriminant function wa xt wa xt l iyixi wo l iy i xt x i i l iesv sequential minimal optimization convergence all αi satisfy karush kuhn tucker kkt conditions used to determine if at optimal solution repeat until convergence pick αi that violates the conditions pick another αj recompute new values for αi and αj proposed by john platt in fast training of support vector machines using sequential minimal optimization further reading prof adriana kovashka university of pittsburgh february plan for today and next two classes probability review density estimation naïve bayes and bayesian belief networks procedural view training stage raw data x feature extraction training data x y f learning testing stage raw data x feature extraction test data x f x apply function evaluate error statistical estimation view probabilities to rescue x and y are random variables d xn yn p x y iid independent identically distributed both training testing data sampled iid from p x y learn on training set have some hope of generalizing to test set probability a is non deterministic event can think of a as a boolean valued variable examples a your next patient has cancer a rafael nadal wins us open interpreting probabilities what does p a mean frequentist view limit n a is true n limiting frequency of a repeating non deterministic event bayesian view p a is your belief about a market design view p a tells you how much you would bet axioms of probability theory all probabilities between and p a true proposition has probability false has probability p true p false the probability of disjunction is p a b p a p b p a b slide credit ray mooney p a p false p true p a v b p a p b p a b event space of all possible worlds its area is p a area of reddish oval p a p false p true p a v b p a p b p a b the area of a can t get any smaller than and a zero area would mean no world could ever have a true p a p false p true p a v b p a p b p a b the area of a can t get any bigger than and an area of would mean all worlds will have a true p a p false p true p a v b p a p b p a b simple addition and subtraction the joint probability distribution for a set of random variables xn gives the probability of every combination of values an n dimensional array with vn values if all variables are discrete with v values all vn values must sum to p xn positive negative the probability of all possible conjunctions assignments of values to some subset of variables can be calculated by summing the appropriate subset of values from the joint distribution p red p red circle therefore all conditional probabilities can also be calculated p positive red slide credit ray mooney circle y z sum rule dhruv batra slide credit erik suddherth p a b in worlds where b is true fraction where a is true p a b p a b p b example h have a headache f coming down with flu p h p f p h f headaches are rare and flu is rarer but if you re coming down with flu there a chance you ll have a headache p y y x x what do you believe about y y if i tell you x x p rafael nadal wins us open what if i tell you he has won the us open twice novak djokovic is ranked just won australian open product rule dhruv batra slide credit erik sudderth generalized product rule example a and b are independent iff p a p b b p a p b these two constraints are logically equivalent therefore if a and b are independent p a b p a b p a p a b p b p a p b marginal p satisfies x y if and only if p x x y y p x x p y y x val x y val y conditional p satisfies x y z if and only if p x y z p x z p y z x val x y val y z val z expectation variance covariance equations from bishop bayes theorem p h e p e h p h p e simple proof from definition of conditional probability p h e p h e def cond prob p e p e h p h e def cond prob p h e p h p e h p h p h e p e qed p h e p e h p h adapted from ray mooney p e probabilistic classification let y be the random variable for the class which takes values ym let x be the random variable describing an instance consisting of a vector of values for n features xn let xk be a possible value for x and xij a possible value for xi for classification we need to compute p y yi x xk for i m however given no other assumptions this requires a table giving the probability of each category for each possible instance in the instance space which is impossible to accurately estimate from a reasonably sized training set assuming y and all xi are binary we need entries to specify p y pos x xk for each of the possible xk since p y neg x xk p y pos x xk compared to entries for the joint distribution p y xn slide credit ray mooney bayesian categorization determine category of xk by determining for each yi prior likelihood p y y x x p y yi p x xk y yi i k posterior p x xk p x xk can be determined since categories are complete and disjoint m p x xk p y i yi p x xk y yi p y y x x p y yi p x xk y yi i i k i p x xk bayesian categorization cont need to know priors p y yi conditionals likelihood p x xk y yi p y yi are easily estimated from data if ni of the examples in d are in yi then p y yi ni d too many possible instances e g for binary features to estimate all p x xk y yi need to make some sort of independence assumptions about the features to make learning tractable more details later a hypothesis is denoted as h it is one member of the hypothesis space h a set of training examples is denoted as d a collection of x y pairs for training pr h the prior probability of the hypothesis without observing any training data what the probability that h is the target function we want pr d the prior probability of the observed data chance of getting the particular set of training examples d pr h d the posterior probability of h what is the probability that h is the target given that we ve observed d pr d h the probability of getting d if h were true a k a likelihood of the data pr h d pr d h pr h pr d maximum a posteriori map estimation hmap argmaxh pr h d argmaxh pr d h pr h pr d argmaxh pr d h pr h maximum likelihood estimation mle hml argmax pr d h prof adriana kovashka university of pittsburgh march basic building blocks need to determine given curve fitting coin flipping heads tails bernoulli distribution n coin flips binomial distribution ml for bernoulli given example prediction all future tosses will land heads up overfitting to d distribution over the beta distribution provides the conjugate prior for the bernoulli distribution the hyperparameters an and bn are the effective number of observations of x and x need not be integers the posterior distribution in turn can act as a prior as more data is observed interpretation l n m the fraction of real and fictitious prior observations corresponding to x prior likelihood posterior of k coding scheme given conjugate prior for the multinomial distribution diagonal covariance matrix covariance matrix proportional to the identity matrix given i i d data the log likeli hood function is given by set the derivative of the log likelihood function to zero and solve to obtain similarly mixtures of gaussians old faithful data set single gaussian mixture of two gaussians combine simple models into a complex model mixing coefficient k prof adriana kovashka university of pittsburgh march plan for today and next week today and next time bayesian networks bishop sec conditional independence bishop sec next week markov random fields bishop sec hidden markov models bishop sec expectation maximization bishop ch graphical models if no assumption of independence is made then an exponential number of parameters must be estimated for sound probabilistic inference no realistic amount of training data is sufficient to estimate so many parameters if a blanket assumption of conditional independence is made efficient training and inference is possible but such a strong assumption is rarely warranted graphical models use directed or undirected graphs over a set of random variables to explicitly specify variable dependencies and allow for less restrictive independence assumptions while limiting the number of parameters that must be estimated bayesian networks directed acyclic graphs indicate causal structure markov networks undirected graphs capture general dependencies structure learning learn the graphical structure of the network parameter learning learn the real valued parameters of the network cpts for bayes nets potential functions for markov nets if values for all variables are available during training then parameter estimates can be directly estimated using frequency counts over the training data if there are hidden variables some form of gradient descent or expectation maximization em must be used to estimate distributions for hidden variables directed acyclic graph dag directed acyclic graph dag nodes are random variables edges indicate causal influences each node has a conditional probability table cpt that gives the probability of each of its values given every possible combination of values for its parents conditioning case roots sources of the dag that have no parents are given prior probabilities probability of false not given since rows must add to example requires parameters rather than for specifying the full joint distribution number of parameters in the cpt for a node is exponential in the number of parents given known values for some evidence variables determine the posterior probability of some query variables example given that john calls what is the probability that there is a burglary john calls of the time there burglary earthquake alarm johncalls marycalls is an alarm and the alarm detects of burglaries so people generally think it should be fairly high however this ignores the prior probability of john calling example given that john calls what is the probability that there is a burglary john also calls of the time when there is no alarm so over days we expect burglary and john will probably call however he will also call with a false report times on average so the call is about times more likely a false report p burglary johncalls polynomial plate input variables and explicit hyperparameters condition on data predictive distribution where generative approach model use bayes theorem discriminative approach model directly causal process for generating images general joint distribution k parameters independent joint distribution k parameters general joint distribution over m variables km parameters m node markov chain k m k k parameters a is independent of b given c equivalently notation node c is tail to tail for path from a to b path makes a and b dependent node c is tail to tail for path from a to b c blocks the path thus making a and b conditionally independent node c is head to tail for path from a to b path makes a and b dependent node c is head to tail for path from a to b c blocks the path thus making a and b conditionally independent node c is head to head for path from a to b c blocks the path thus making a and b independent note this is the opposite of example with c unobserved node c is head to head for path from a to b c unblocks the path thus making a and b conditionally dependent note this is the opposite of example with c observed b battery flat fully charged f fuel tank empty full g fuel gauge reading empty full probability of an empty tank increased by observing g probability of an empty tank reduced by observing b this referred to as explaining away d separation a b and c are non intersecting subsets of nodes in a directed graph a path from a to b is blocked if it contains a node such that either the arrows on the path meet either head to tail or tail to tail at the node and the node is in the set c or the arrows meet head to head at the node and neither the node nor any of its descendants are in the set c if all paths from a to b are blocked a is said to be d separated from b by c if a is d separated from b by c the joint distribution over all variables in the graph satisfies the xi conditionally independent are the xi marginally independent conditioned on the class z the distributions of the input variables xd are independent are the xd marginally independent factors independent of xi cancel between numerator and denominator the parents children and co parents of xi form its markov blanket the minimal set of nodes that isolate xi from the rest of the graph slide from bishop bayes nets represent a subclass of joint distributions that capture non cyclic causal dependencies between variables a markov net can represent any joint distribution slide credit ray mooney in general first order markov chain second order markov chain undirected graph over a set of random variables where an edge represents a dependency the markov blanket of a node x in a markov net is the set of its neighbors in the graph nodes that have an edge connecting to x every node in a markov net is conditionally independent of every other node given its markov blanket slide credit ray mooney markov blanket a node is conditionally independent of all other nodes conditioned only on the neighboring nodes clique maximal clique distribution for a markov network the distribution of a markov net is most compactly described in terms of a set of potential functions φk for each clique k in the graph for each joint assignment of values to the variables in clique k φk assigns a non negative real value that represents the compatibility of these values the joint distribution of a markov is then defined by p xn z k k x k where x k represents the joint assignment of the variables in clique k and z is a normalizing constant that makes a joint distribution that sums to z k x k x k slide credit ray mooney original image noisy image yi in labels in observed noisy image xi in labels in noise free image i is the index over pixels noisy image restored image icm prof adriana kovashka university of pittsburgh march all slides are from ray mooney motivating example part of speech tagging annotate each word in a sentence with a part of speech marker lowest level of syntactic analysis john saw the saw and decided to take it to the table nnp vbd dt nn cc vbd to vb prp in dt nn useful for subsequent syntactic parsing and word sense disambiguation english parts of speech noun person place or thing singular nn dog fork plural nns dogs forks proper nnp nnps john springfields personal pronoun prp i you he she it wh pronoun wp who what verb actions and processes base infinitive vb eat past tense vbd ate gerund vbg eating past participle vbn eaten non person singular present tense vbp eat person singular present tense vbz eats modal md should can to to to to eat english parts of speech cont adjective modify nouns basic jj red tall comparative jjr redder taller superlative jjs reddest tallest adverb modify verbs basic rb quickly comparative rbr quicker superlative rbs quickest preposition in on in by to with determiner basic dt a an the wh determiner wdt which that coordinating conjunction cc and but or particle rp off took off up put up ambiguity in pos tagging like can be a verb or a preposition i like vbp candy time flies like in an arrow around can be a preposition particle or adverb i bought it at the shop around in the corner i never got around rp to getting a car a new prius costs around rb classification learning typical machine learning addresses the problem of classifying a feature vector description into a fixed number of classes there are many standard learning methods for this task decision trees and rule learning naïve bayes and bayesian networks logistic regression maximum entropy maxent perceptron and neural networks support vector machines svms nearest neighbor instance based beyond classification learning standard classification problem assumes individual cases are disconnected and independent i i d independently and identically distributed many nlp problems do not satisfy this assumption and involve making many connected decisions each resolving a different ambiguity but which are mutually dependent more sophisticated learning and inference techniques are needed to handle such situations in general sequence labeling problem many nlp problems can viewed as sequence labeling each token in a sequence is assigned a label labels of tokens are dependent on the labels of other tokens in the sequence particularly their neighbors not i i d foo bar blam zonk zonk bar blam identify phrases in language that refer to specific types of entities and relations in text named entity recognition is task of identifying names of people places organizations etc in text people organizations places michael dell is the ceo of dell computer corporation and lives in austin texas extract pieces of information relevant to a specific application e g used car ads make model year mileage price for sale toyota prius mi or best offer available starting july for each clause determine the semantic role played by each noun phrase that is an argument to the verb agent patient source destination instrument john drove mary from austin to dallas in his toyota prius the hammer broke the window also referred to a case role analysis thematic analysis and shallow semantic parsing sequence labeling also valuable in labeling genetic sequences in genome analysis extron intron agctaacgttcgatacggattacagcct classify each token independently but use as input features information about the surrounding tokens sliding window john saw the saw and decided to take it to the table nnp classify each token independently but use as input features information about the surrounding tokens sliding window john saw the saw and decided to take it to the table vbd classify each token independently but use as input features information about the surrounding tokens sliding window john saw the saw and decided to take it to the table dt classify each token independently but use as input features information about the surrounding tokens sliding window john saw the saw and decided to take it to the table nn classify each token independently but use as input features information about the surrounding tokens sliding window john saw the saw and decided to take it to the table cc classify each token independently but use as input features information about the surrounding tokens sliding window john saw the saw and decided to take it to the table vbd classify each token independently but use as input features information about the surrounding tokens sliding window john saw the saw and decided to take it to the table to classify each token independently but use as input features information about the surrounding tokens sliding window john saw the saw and decided to take it to the table vb classify each token independently but use as input features information about the surrounding tokens sliding window john saw the saw and decided to take it to the table prp classify each token independently but use as input features information about the surrounding tokens sliding window john saw the saw and decided to take it to the table in classify each token independently but use as input features information about the surrounding tokens sliding window john saw the saw and decided to take it to the table dt classify each token independently but use as input features information about the surrounding tokens sliding window john saw the saw and decided to take it to the table nn sequence labeling as classification using outputs as inputs better input features are usually the categories of the surrounding tokens but these are not available yet can use category of either the preceding or succeeding tokens by going forward or back and using previous output john saw the saw and decided to take it to the table nnp nnp john saw the saw and decided to take it to the table vbd nnp vbd john saw the saw and decided to take it to the table dt nnp vbd dt john saw the saw and decided to take it to the table nn nnp vbd dt nn john saw the saw and decided to take it to the table cc nnp vbd dt nn cc john saw the saw and decided to take it to the table vbd nnp vbd dt nn cc vbd john saw the saw and decided to take it to the table to nnp vbd dt nn cc vbd to john saw the saw and decided to take it to the table vb nnp vbd dt nn cc vbd to vb john saw the saw and decided to take it to the table prp nnp vbd dt nn cc vbd to vb prp john saw the saw and decided to take it to the table in nnp vbd dt nn cc vbd to vb prp in john saw the saw and decided to take it to the table dt nnp vbd dt nn cc vbd to vb prp in dt john saw the saw and decided to take it to the table nn disambiguating to in this case would be even easier backward john saw the saw and decided to take it to the table nn disambiguating to in this case would be even easier backward nn john saw the saw and decided to take it to the table dt disambiguating to in this case would be even easier backward dt nn john saw the saw and decided to take it to the table in disambiguating to in this case would be even easier backward in dt nn john saw the saw and decided to take it to the table prp disambiguating to in this case would be even easier backward prp in dt nn john saw the saw and decided to take it to the table vb disambiguating to in this case would be even easier backward vb prp in dt nn john saw the saw and decided to take it to the table to disambiguating to in this case would be even easier backward to vb prp in dt nn john saw the saw and decided to take it to the table vbd disambiguating to in this case would be even easier backward vbd to vb prp in dt nn john saw the saw and decided to take it to the table cc disambiguating to in this case would be even easier backward cc vbd to vb prp in dt nn john saw the saw and decided to take it to the table nn disambiguating to in this case would be even easier backward vbd cc vbd to vb prp in dt nn john saw the saw and decided to take it to the table dt disambiguating to in this case would be even easier backward dt vbd cc vbd to vb prp in dt nn john saw the saw and decided to take it to the table vbd disambiguating to in this case would be even easier backward vbd dt vbd cc vbd to vb prp in dt nn john saw the saw and decided to take it to the table nnp problems with sequence labeling as classification not easy to integrate information from category of tokens on both sides difficult to propagate uncertainty between decisions and collectively determine the most likely joint assignment of categories to all of the tokens in a sequence probabilistic sequence models allow integrating uncertainty over multiple interdependent classifications and collectively determine the most likely global assignment two standard models hidden markov model hmm conditional random field crf a finite state machine with probabilistic state transitions makes markov assumption that next state only depends on the current state and independent of previous history det noun stop start propnoun verb det noun stop start propnoun verb p propnoun verb det noun probabilistic generative model for sequences assume an underlying set of hidden unobserved states in which the model can be e g parts of speech assume probabilistic transitions between states over time e g transition from pos to another pos as sequence is generated assume a probabilistic generation of tokens from states e g words generated for each pos the cat a a the dog bed the a the car apple that det pen noun bit saw played stop tom hit gave start johnmary alice jerry propnoun verb the cat a a the dog bed the a the car apple that det pen noun bit saw played stop tom hit gave start johnmary alice jerry propnoun verb the cat a a the dog bed the a the car apple that det pen noun bit saw played stop tom hit gave start johnmary alice jerry propnoun verb the cat a a the dog bed the a the car apple that det pen noun bit saw played stop tom hit gave start john johnmary alice jerry propnoun verb the cat a a the dog bed the a the car apple that det pen noun bit saw played stop tom hit gave start john johnmary alice jerry propnoun verb the cat a a the dog bed the a the car apple that det pen noun bit saw played stop tom hit gave johnmary alice jerry propnoun verb start john bit the cat a a the dog bed the a the car apple that det pen noun bit saw played stop tom hit gave johnmary alice jerry propnoun verb start john bit the cat a a the dog bed the a the car apple that det pen noun bit saw played stop tom hit gave johnmary alice jerry propnoun verb start john bit the the cat a a the dog bed the a the car apple that det pen noun bit saw played stop tom hit gave johnmary alice jerry propnoun verb start john bit the the cat a a the dog bed the a the car apple that det pen noun bit saw played stop tom hit gave johnmary alice jerry propnoun 25 verb start john bit the apple the cat a a the dog bed the a the car apple that det pen noun bit saw played stop tom 25 hit gave johnmary alice jerry propnoun 25 verb start john bit the apple a set of n states s sn sf distinguished start state distinguished final state sf a set of m possible observations v vm a state transition probability distribution a aij aij n p qt j qt si i j n and i j f aij j aif i n observation probability distribution for each state j b bj k bj k p vk at t qt j j n k m total parameter set λ a b to generate a sequence of t observations o ot set initial state for t to t transit to another state qt sj based on transition distribution aij for state qt pick an observation ot vk based on being in state qt using distribution bqt k observation likelihood to classify and order sequences most likely state sequence decoding to tag each token in a sequence with a label maximum likelihood training learning to train models to fit empirical training data given a sequence of observations o and a model with a set of parameters λ what is the probability that this observation was generated by this model p o λ allows hmm to be used as a language model a formal probabilistic model of a language that assigns a probability to each string saying how likely that string was to have been generated by the language useful for two tasks sequence classification most likely sequence assume an hmm is available for each category i e language what is the most likely category for a given observation sequence i e which category hmm is most likely to have generated it used in speech recognition to find most likely word model to have generate a given sound or phoneme sequence austin p o austin p o boston boston of two or more possible sequences which one was most likely generated by a given model used to score alternative word sequence interpretations in speech recognition ordinary english p o ordenglish p o ordenglish naïve solution consider all possible state sequences q of length t that the model could have traversed in generating the given observation sequence compute the probability of a given state sequence from a and multiply it by the probabilities of generating each of given observations in each of the corresponding states in this sequence to get p o q λ p o q λ p q λ sum this over all possible state sequences to get p o λ computationally complex o tnt efficient solution due to the markov assumption the probability of being in any state at any given time t only relies on the probability of being in each of the possible states at time t forward algorithm uses dynamic programming to exploit this fact to efficiently compute observation likelihood in o time compute a forward trellis that compactly and implicitly encodes information about all possible state paths let t j be the probability of being in state j after seeing the first t observations by summing over all initial paths leading to j t j p ot qt j consider all possible ways of getting to sj at time t by coming from all possible states si and determine probability of each j sum these to get the total probability of being in state sj at n time t while accounting for the t i t i first t observations then multiply by the probability of actually observing ot in sj sf tt tt continue forward in time until reaching final time point and sum probability of ending in final state initialization j recursion jbj j n j n i a b o j n t t t i t ij j t termination n p o t sf t i aif requires only o time to compute the probability of an observed sequence given a model exploits the fact that all state sequences must merge into one of the n possible states at any point in time and the markov assumption that only the last state effects the next one given an observation sequence o and a model λ what is the most likely state sequence q qt that generated this sequence from this model used for sequence labeling assuming each state corresponds to a tag it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in probability theory given an observation sequence o and a model λ what is the most likely state sequence q qt that generated this sequence from this model used for sequence labeling assuming each state corresponds to a tag it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in probability theory given an observation sequence o and a model λ what is the most likely state sequence q qt that generated this sequence from this model used for sequence labeling assuming each state corresponds to a tag it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in probability theory given an observation sequence o and a model λ what is the most likely state sequence q qt that generated this sequence from this model used for sequence labeling assuming each state corresponds to a tag it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in probability theory given an observation sequence o and a model λ what is the most likely state sequence q qt that generated this sequence from this model used for sequence labeling assuming each state corresponds to a tag it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in probability theory given an observation sequence o and a model λ what is the most likely state sequence q qt that generated this sequence from this model used for sequence labeling assuming each state corresponds to a tag it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in probability theory given an observation sequence o and a model λ what is the most likely state sequence q qt that generated this sequence from this model used for sequence labeling assuming each state corresponds to a tag it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in probability theory hmm most likely state sequence efficient solution obviously could use naïve algorithm based on examining every possible state sequence of length t dynamic programming can also be used to exploit the markov assumption and efficiently determine the most likely state sequence for a given observation and model standard procedure is called the viterbi algorithm viterbi and also has o time complexity viterbi scores recursively compute the probability of the most likely subsequence of states that accounts for the first t observations and ends in state sj vt j max qt p qt ot qt j also record backpointers that subsequently allow backtracing the most probable state sequence btt j stores the state at time t that maximizes the probability that system was in state sj at time t given the observed sequence computing the viterbi scores initialization j recursion n jbj j n vt j max i vt i aijbj ot j n t t termination n p vt sf max i vt i aif analogous to forward algorithm except take max instead of sum computing the viterbi backpointers initialization j j n recursion n btt j argmax i vt i aijbj ot j n t t termination n qt btt sf argmax i vt i aif final state in the most probable state sequence follow backpointers to initial state to construct full sequence sf tt tt sf tt tt most likely sequence sn sf supervised learning all training sequences are completely labeled tagged unsupervised learning all training sequences are unlabelled but generally know the number of tags i e states semisupervised learning some training sequences are labeled most are unlabeled if training sequences are labeled tagged with the underlying state sequences that generated them then the parameters λ a b can all be estimated directly training sequences det noun propnoun verb estimate state transition probabilities based on tag bigram and unigram statistics in the labeled data a c qt si qt j c qt si estimate the observation probabilities based on tag word co occurrence statistics in the labeled data b k c qi j oi vk c qi j use appropriate smoothing if training data is sparse use a corpus of labeled sequence data to easily construct an hmm using supervised training given a novel unlabeled test sequence to tag use the viterbi algorithm to predict the most likely globally optimal tag sequence unsupervised maximum likelihood training training sequences austin maximum likelihood training given an observation sequence o what set of parameters λ for a given model maximizes the probability that this data was generated from this model p o λ used to train an hmm model and properly induce its parameters from a set of training data only need to have an unannotated observation sequence or set of sequences generated from the model does not need to know the correct state sequence for the observation sequence in this sense it is unsupervised hmm maximum likelihood training efficient solution there is no known efficient algorithm for finding the parameters λ that truly maximizes p o λ however using iterative re estimation the baum welch algorithm a k a forward backward a version of a standard statistical procedure called expectation maximization em is able to locally maximize p o λ in practice em is able to find a good set of parameters that provide a good fit to the training data in many cases em algorithm iterative method for learning probabilistic categorization model from unsupervised data initially assume random assignment of examples to categories learn an initial probabilistic model by estimating model parameters from this randomly labeled data iterate following two steps until convergence expectation e step compute p ci e for each example given the current model and probabilistically re label the examples based on these posterior probability estimates maximization m step re estimate the model parameters from the probabilistically re labeled data initialize assign random probabilistic labels to unlabeled data unlabeled examples initialize give soft labeled training data to a probabilistic learner initialize produce a probabilistic classifier e step relabel unlabeled data using the trained classifier m step retrain classifier on relabeled data continue em iterations until probabilistic labels on unlabeled data converge sketch of baum welch em algorithm for training hmms assume an hmm with n states randomly set its parameters λ a b making sure they represent legal distributions until converge i e λ no longer changes do e step use the forward backward procedure to determine the probability of various possible state sequences for generating the training data m step use these probability estimates to re estimate values for all of the parameters λ backward probabilities let t i be the probability of observing the final set of observations from time t to t given that one is in state i at time t t i p ot ot ot qt si computing the backward probabilities initialization t i aif i n recursion n t i aijbj ot t j j i n t t termination n p o t sf jbj j j estimating probability of state transitions let t i j be the probability of being in state i at time t and state j at time t t i j p qt si qt j o i p qt si qt j o t i aijbj ot t j t p o p o a b o j ani sn t i ij j t sn t t t t re estimating a aˆ expected number of transitio ns from state i to j ij expected number t of transitio ns from state i t i j aˆij t t n t i j t j estimating observation probabilities let t i be the probability of being in state i at time t given the observations and the model j p q o p qt j o t j t j t t j p o p o re estimating b bˆ v expected number of times in state j observing vk j k expected number of times in state j bˆj vk t j t t ot vk t t j t pseudocode for baum welch em algorithm for training hmms assume an hmm with n states randomly set its parameters λ a b making sure they represent legal distributions until converge i e λ no longer changes do e step m step compute values for t j and t i j using current values for parameters a and b re estimate parameters aij aˆij bj vk bˆj vk em properties each iteration changes the parameters in a way that is guaranteed to increase the likelihood of the data p o anytime algorithm can stop at any time prior to convergence to get approximate solution converges to a local maximum prof adriana kovashka university of pittsburgh march mixtures of gaussians form let z latent variable be of k representation then responsibility of component k for explaining x generating samples sample value z from p z then sample a value for x from p x z color generated samples using z left color samples using responsibilities right finding parameters of mixture want to maximize set derivative with respect to means to get finding parameters of mixture set derivative wrt covariance to set derivative wrt mixing coefficients to reminder responsibilities so parameters of gaussian depend on responsibilities and vice versa remember k means iterative algorithm from bishop from bishop a lb c d e f from bishop general algorithm cs machine learning ensembles bagging and boosting decision trees prof adriana kovashka university of pittsburgh april plan for today ensemble methods bagging boosting boosting application face detection decision trees learn multiple alternative definitions of a concept using different training data or different learning algorithms train several classifiers svm knn logistic regression decision tree neural network etc call these classifiers x x fm x take majority of predictions y majority x x fm x for regression use mean or median of the predictions averaging is a form of regularization each model can individually overfit but the average is able to overcome the overfitting learn multiple alternative definitions of a concept using different training data or different learning algorithms combine decisions of multiple definitions when combing multiple independent and diverse decisions each of which is at least more accurate than random guessing random errors cancel each other out correct decisions are reinforced human ensembles are demonstrably better how many jelly beans in the jar individual estimates vs group average who wants to be a millionaire expert friend vs audience vote use a single arbitrary learning algorithm but manipulate training data to make it learn multiple models data m learner m different methods for changing training data bagging resample training data boosting reweight training data create ensembles by repeatedly randomly resampling the training data brieman given a training set of size n create m samples of size n by drawing n examples from the original data with replacement combine the m resulting models using simple majority vote decreases error by decreasing the variance in the results due to unstable learners algorithms like decision trees whose output can change dramatically when the training data is slightly changed however often the errors of the different models are correlated which defies the purpose of bagging originally developed by computational learning theorists to guarantee performance improvements on fitting training data for a weak learner that only needs to generate a hypothesis with a training accuracy greater than schapire revised to be a practical algorithm adaboost for building ensembles that empirically improves generalization performance freund shapire examples are given weights at each iteration a new hypothesis is learned and the examples are reweighted to focus the system on examples that the most recently learned classifier got wrong general loop set all examples to have equal uniform weights for m from to m do find the weak learner hm that achieves lowest weighted training error increase the weights of examples that hm classifies incorrectly during testing each of the m classifiers gets a weighted vote proportional to its accuracy on the training data final classifier is a linear combination of all weak learners base weak learner must focus on correctly classifying the most highly weighted examples while strongly avoiding over fitting weak learners must perform better than chance weak classifier weights increased weak classifier weights increased weak classifier final classifier is a combination of weak classifiers start with uniform weights on training examples for m rounds evaluate weighted error for each weak learner pick best learner figure from c bishop notes from k grauman normalize the weights so they sum to re weight the examples incorrectly classified get more weight correctly classified get less weight final classifier is combination of weak ones weighted according to error they had learning with weighted examples generic approach is to replicate examples in the training set proportional to their weights e g replicas of an example with a weight of and for one with weight most algorithms can be enhanced to efficiently incorporate weights directly in the learning algorithm so that the effect is the same for decision trees for calculating information gain when counting example i simply increment the corresponding count by wi rather than by experimental results on ensembles freund schapire quinlan ensembles have been used to improve generalization accuracy on a wide variety of problems on average boosting provides a larger increase in accuracy than bagging boosting on rare occasions can degrade accuracy bagging more consistently provides a modest improvement boosting is particularly subject to over fitting when there is significant noise in the training data issues in ensembles parallelism in ensembles bagging is easily parallelized boosting is not variants of boosting to handle noisy data how weak should a base learner for boosting be what is the theoretical explanation of boosting ability to improve generalization exactly how does the diversity of ensembles affect their generalization performance combining boosting and bagging sliding window detector must evaluate tens of thousands of location scale combinations faces are rare per image a megapixel image has pixels and a comparable number of candidate face locations for computational efficiency we should try to spend as little time as possible on the non face windows main idea represent local texture with efficiently computable rectangular features within window of interest select discriminative features to be weak classifiers use boosted combination of them as final classifier form a cascade of such classifiers rejecting clear negatives quickly not discussed see hidden slides rectangular filters feature output is difference between adjacent regions value pixels in white area pixels in black area efficiently computable with integral image any sum can be computed in constant time value at x y is sum of pixels above and to the left of x y integral image considering all possible filter parameters position scale and type possible features associated with each x window which subset of these features should we use to determine if a window has a face use adaboost both to select the informative features and to form the classifier want to select the single rectangle feature and threshold that best separates positive faces and negative non faces training examples in terms of weighted error resulting weak classifier outputs of a possible rectangle feature on faces and non faces for next round reweight the examples according to errors choose another filter threshold combo first two features selected by boosting this feature combination can yield detection rate and false positive rate like the thresholded features classifiers in face detection a single level decision tree discussed next for use a total of of these and don t worry about them having better than chance performance figure from wikipedia ensemble methods bagging boosting boosting application face detection tree based classifiers for instances represented as feature vectors nodes test features there is one branch for each value of the feature and leaves specify the category shape color red blue green neg pos shape color red blue green b c circle square triangle pos neg neg circle square triangle a b c can represent arbitrary conjunction and disjunction can represent any classification function over discrete feature vectors can be rewritten as a set of rules red circle pos red circle a blue b red square b green c red triangle c continuous real valued features can be handled by allowing nodes to split a real valued feature into two ranges based on a threshold e g length and length classification trees have discrete class labels at the leaves recursively build a tree top down by divide and conquer big red circle small red circle small red square big blue circle color red blue green big red circle small red circle small red square recursively build a tree top down by divide and conquer big red circle big red circle small red circle small red square big blue circle color red blue green small red circle shape neg neg small red square circle pos big red circle small red circle big blue circle square triangle neg pos small red square dtree examples features returns a tree if all examples are in one category return a leaf node with that category label else if the set of features is empty return a leaf node with the category label that is the most common in examples else pick a feature f and create a node r for it for each possible value vi of f let examplesi be the subset of examples that have value vi for f add an out going edge e to node r labeled with the value vi if examplesi is empty then attach a leaf node to edge e labeled with the category that is the most common in examples else call dtree examplesi features f and attach the resulting tree as the subtree under edge e return the subtree rooted at r goal is to have the resulting tree be as small as possible per occam razor finding a minimal decision tree nodes leaves or depth is an np hard optimization problem top down divide and conquer method does a greedy search for a simple tree but does not guarantee to find the smallest that ok want to pick a feature that creates subsets of examples that are relatively pure in a single class so they are closer to being leaf nodes there are a variety of heuristics for picking a good test a popular one is based on information gain that originated with the system of quinlan entropy disorder impurity of a set of examples s relative to a binary classification is entropy s log log where is the fraction of positive examples in s and is the fraction of negatives if all examples are in one category entropy is zero we define log if examples are equally mixed entropy is a maximum of for multi class problems with c categories entropy generalizes to c entropy s i pi log pi the information gain of a feature f is the expected reduction in entropy resulting from splitting on this feature gain s f entropy s v values f entropy sv where sv is the subset of s having value v for feature f entropy of each resulting subset weighted by its relative size example big red circle small red circle small red square big blue circle e size big small e e gain slide credit ray mooney e color red blue e e gain 311 e shape circle square e 918 e gain 918 311 example problem decide whether to wait for a table at a restaurant based on the following attributes alternate is there an alternative restaurant nearby bar is there a comfortable bar area to wait in fri sat is today friday or saturday hungry are we hungry patrons number of people in the restaurant none some full price price range raining is it raining outside reservation have we made a reservation type kind of restaurant french italian thai burger waitestimate estimated waiting time 60 learning a tree that classifies the training data perfectly may not lead to the tree with the best generalization to unseen data there may be noise in the training data that the tree is erroneously fitting the algorithm may be making poor decisions towards the leaves of the tree that are based on very little data and may not reflect reliable trends on training data on test data hypothesis complexity category or feature noise can easily cause overfitting add noisy instance medium blue circle pos but really neg color red green blue shape neg neg circle square triangle pos neg pos category or feature noise can easily cause overfitting add noisy instance medium blue circle pos but really neg color shape red green neg blue big blue circle medium blue circle circle square triangle small med big pos neg pos neg pos neg noise can also cause different instances of the same feature vector to have different classes impossible to fit this data and must label leaf with the majority class big red circle neg but really pos conflicting examples can also arise if the features are incomplete and inadequate to determine the class or if the target concept is non deterministic overfitting prevention pruning methods two basic approaches for decision trees prepruning stop growing tree as some point during top down construction when there is no longer sufficient data to make reliable decisions postpruning grow the full tree then remove subtrees that do not have sufficient evidence label leaf resulting from pruning with the majority class of the remaining data or a class probability distribution method for determining which subtrees to prune cross validation reserve some training data as a hold out set validation set to evaluate utility of subtrees statistical test use a statistical test on the training data to determine if any observed regularity can be dismisses as likely due to random chance minimum description length mdl determine if the additional complexity of the hypothesis is less complex than just explicitly remembering any exceptions resulting from pruning cs machine learning active learning and crowdsourcing prof adriana kovashka university of pittsburgh april collecting data on amazon mechanical turk workers annotation protocols type keywords select relevant images click on landmarks outline something anything else type keywords select examples outline something motivation custom annotations x large scale low price g a m azon m echan i ca l turk x amazonmechanical turk artificial artificial intelligence your account hits qualificat ions introduction i dashboard i status i account settings mechanical turk is a marketplace for work d already have an account sign in as a worker i requester we give businesses and developers access to an on demand scalable workforce workers select from mousands of tasks a a work whenever it convenient hits availab le view them now make money by working on hits h ts hum an i ntelligence tasks are individual tasks that you work on find hits no w as a mechanical turk worker you can work from home choose your own work h ours get paid for doing good w ork ge resul from mechanical turk workers ask workers to complete hits human i ntelligence tasks and get r esult u sing mechanical turk get started as a mechanical turk requester you have access to a global on demand x workforce get thousands of hits completed in minutes pay only when you re satisfied wit h th e results find an interesting task work earn money find hits now fund your load your account tasks get started get results or l earn more about being a worker faq i co ntact us i ca r eers at mec h anical turk i dev elop ers i press i policies i state licensing i blog i se rv ice hea lt h d ashbo ar d ama w n com i n c or its aff ili at es an ama on com company issues quality how good is it how to be sure price how to price it ensuring annotation quality consensus multiple annotation wisdom of the crowd qualification exam gold standard questions grading tasks a second tier of workers who grade others pricing trade off between throughput and cost higher pay can actually attract scammers some studies find that the most accurate results are achieved if turkers do tasks for free games with a purpose luis von ahn associate professor at cmu one of the fathers of crowdsourcing created the esp game peekaboom and several other games with a purpose the esp game two player online game partners don t know each other and can t communicate object of the game type the same word the only thing in common is an image luis von ahn and laura dabbish labeling images with a computer game chi the esp game player player guessing car guessing boy guessing hat guessing kid success you agree on car guessing car success you agree on car the esp game is fun million labels with players there are many people that play over hours a week why do people like the esp game the esp game gives its players a weird and beautiful sense of anonymous intimacy on the one hand you have no idea who your partner is on the other hand the two of you are bringing your minds together in a way that lovers would envy strangely addictive it s so much fun tryng to guess what others think you have to step outside of yourself to match it s fast paced helps me learn english locating objects in images the esp game tells us if an image contains a specific object but doesn t say where in the image the object is such information would be extremely useful for computer vision research paintball game players shoot at objects on the image shoot the car we give points and check accuracy by giving players images for which we already know where the object is revealing images guesser revealer guess car bcraursh partner s guess summary collecting annotations from humans crowdsourcing allows very cheap data collection getting high quality annotations can be tricky but there are many ways to ensure quality one way to obtain high quality data fast is by phrasing your data collection as a game what to do when data is expensive to obtain crowdsourcing training james hays active learning traditional active learning reduces supervision by obtaining labels for the most informative or uncertain examples first mackay freund et al tong koller lindenbaum et al kapoor et al visual recognition with humans in the loop eccv crete greece steve branson catherine wah florian schroff boris babenko serge belongie peter welinder pietro perona field guides difficult for average users computer vision doesn t work perfectly yet research mostly on basic level categories what type of bird is this visual recognition with humans in the loop parakeet auklet motivation supplement visual recognition with the human capacity for visual feature extraction to tackle difficult fine grained recognition problems typical progress is viewed as increasing data difficulty while maintaining full autonomy here the authors view progress as reduction in human effort on difficult data brian o neill categories of recognition basic level subordinate parts attributes easy for humans hard for computers hard for humans hard for computers easy for humans hard for computers visual questions game hard classification problems can be turned into a sequence of easy ones recognition with humans in the loop computers reduce number of required questions humans drive up accuracy of vision algorithms example questions example questions example questions basic algorithm input image x computer vision max expected information gain question a no p c x is the belly black max expected information gain question a yes p c x is the bill hooked p c x some definitions q qn set of possible questions ai ai ri v possible answers to question i possible confidence in answer i guessing probably definitely ui u t ai ri user response history of user responses at time t question selection seek the question e g what color is the belly of the bird that gives the maximum information gain entropy reduction given the image and the set of previous user responses i c u x u t p u x u t h c x u u t h c x u t i i i ui ai v probability of obtaining response ui to evaluated question given image and response history entropy when response is added to history entropy at this iteration before response to evaluated question is added to history where h c x u t c p c x u t log p c x u t results users drive performance fewer questions asked if cv used just computer vision adapted from steve branson summary human in the loop learning to make intelligent use of the human labeling effort during training have the computer vision algorithm learn actively by selecting those questions that are most informative to combine strengths of human and imperfect vision algorithms use a human in the loop at recognition time visual object recognition synthesis lectures on artificial intelligence and machine learning april pages doi kristen grauman university of texas at austin bastian leibe rwth aachen university abstract the visual recognition problem is central to computer vision research from robotics to information retrieval many desired applications demand the ability to identify and localize categories places and objects this tutorial overviews computer vision algorithms for visual object recognition and image classification we introduce primary representations and learning approaches with an emphasis on recent advances in the field the target audience consists of researchers or students working in ai robotics or vision who would like to understand what methods and representations are available for these problems this lecture summarizes what is and isn t possible to do reliably today and overviews key concepts that could be employed in systems requiring visual categorization table of contents introduction overview recognition of specific objects local features detection and description matching local features geometric verification of matched features example systems specificobject recognition overview recognition of generic object categories representations for object categories generic object detection finding and scoring candidates learning generic object category models example systems generic object recognition other considerations and current challenges conclusions homework http people cs pitt edu kovashka htm homework due instructions see homework submission mechanics on the main course page if you do not see the assignments button notify the instructor note if you are asked to implement something by yourself it is not ok to use or even look at existing matlab or python code for anything you are not asked to implement feel free to look up relevant functions if you have questions about what you can use ask the instructor or the ta part i short answers points propose three new problems that can be solved with machine learning ones that we have not discussed in class and describe how you would go about solving each for each problem discuss what features would you use what would the labels be how would you collect data why might the problem turn out to be challenging provide your answers in a text file titled txt part ii the classification pipeline points in this problem you will train a multiclass model to distinguish between different types of flowers you will use the iris dataset from the uci machine learning repository the data is contained the iris data file under data folder while the file iris names contains a description of the data the features x are given as the first four commaseparated values in each row in the data file the labels y are the last entry in each row but you should convert the strings to integer ids first split the data into three groups a training set a validation set and a test set you will train a multiway svm classifier we will talk about svm classifiers in great length later in the class the goal in this assignment is to use svms as a black box so that you can experiment with the machine learning pipeline we discussed in the introduction svm is one of the most popular classifiers you might need to use it for your project and it might be one of the few things you remember how to use after this class is over pick an svm software package to use among libsvm liblinear svm light the svm builtin to matlab or the svm in scikitlearn for python look at different packages to see which one you would feel most comfortable using read the documentation and find which function in the package you are using performs learning fitting training and which function performs prediction classification testing you can also look for examples of how these functions are used include a text file txt in your submission in which you describe why you chose the package that you chose to use then copypaste the parts of the documentation that show how to train and use an svm pick some svm parameter that the package allows you to tune or specify values for for now you won t know what these parameters do but they will affect the success of your learning algorithm in some way your goal is to pick the best value of the parameter by tuning your model on a validation set in other words you will train one model for every value of the parameter of your choice try different values and pick which value is best according to the accuracy on the validation set you will then use the chosen value of the parameter and apply it to classify the samples in the test set in the text file mentioned above report the final accuracy on the training set and the test set the difference between these two is called the generalization error if the code you are using does not include a homework http people cs pitt edu kovashka htm function to compute accuracy you should write such a function yourself it will only be a few lines to compute accuracy you will compare the predicted labels for your test samples to the groundtruth provided with the dataset labels for those same samples and compute the fraction of samples that were labeled correctly finally experiment with different amounts of training data and report how the error on the training data and test data changes as you add more training data include a plot in your writeup with at least different values for the size of the training data on the xaxis and accuracy on the yaxis for the training and test sets separately i e show two curves in the text file explain what you are observing and why it might be happening grading rubric a loading and splitting the data into train test validation and x y parts pts b package choice and documentation excerpts pts c training with different values of a parameter and using the validation set to pick the best value of that parameter pts d demonstrating how train test error changes as more training data is added including plot pts part iii segmentation via clustering points for this problem you will implement the kmeans algorithm you will then use it to perform image clustering to test your implementation write code to perform clustering over a an nxd data matrix where n is the number of samples and d is the dimensionality of your feature representation that you receive as input from the user your code should output an output containing the data memberships of each sample denoted by an index from to k where k is the number of clusters a kxd matrix containing the mean center for each cluster and the final ssd error of the clustering i e the sum of the squared distances between points and their assigned means summed over all clusters in your kmeans function try random restarts and return the clustering with the lowest ssd error you will next test your implementation by applying clustering to segment and recolor an image download images from the the berkeley segmentation dataset and benchmark to make sure running your method doesn t take a long time downsample reduce the size of your chosen images to perform segmentation you need a representation for every image pixel for simplicity you will use a threedimensional feature representation for each pixel consisting of the r g and b values of each pixel you can also include the x y location of each pixel in the feature representation if you wish perform clustering over the pixels of the image then recolor the pixels of each image according to their cluster membership in particular replace each pixel with the average r g b values for the center to which the pixel belongs include the recoloring result for images and different values of k for each image in your submission grading rubric a the correctness of your clustering method implementation pts b applying your clustering method on images and recoloring depending on cluster membership pts part iv linear regression points in this problem you will solve a regression problem in two ways using the direct leastsquares solution and using gradient descent you will use the wine quality dataset use only the red wine data the goal is to find the quality score of some wine based on its attributes first divide the data into a training and test set using approximately for training you don t need to use crossvalidation for this problem homework http people cs pitt edu kovashka htm use the linear system of equations least squares solution for the language of your choice in matlab that the backslash operator ax b x a b you need to decide what are a x and b in the case of linear regression use the resulting solution to find the wine quality scores on the test data then measure and report in a file txt the distance between the true and predicted scores now implement the gradient descent solution for this you will need to initialize the weights in some way use either random values or all zeros then you repeat the following some number of times for this problem repeat times in each iteration compute the error function gradient using all training data points then adjust the weights in the direction opposite to the gradient apply the solution to the test set then compute and report the distance as above experiment with different learning rates e g ones in the range i e and report your observations grading rubric a data set up and least squares solution pts b gradient descent solution pts homework http people cs pitt edu kovashka htm homework due note if you are asked to implement something by yourself it is not ok to use or even look at existing matlab or python code unless it utility code if you have questions about what you can use ask the instructor or the ta part i knearest neighbors points in this example you will implement and explore knn classification you will use the pima indians diabetes dataset see the pimaindiansdiabetes data file the last value in each row contains the target label for that row and the pimaindiansdiabetes names file both found at the data folder yellow link at the top before you begin split the data into approximately equallysized folds your results reported below should be an average of the results when you train on the first folds and test on the remaining then if you train on the folds numbered through and the fold and testing on the fold etc for simplicity you can also just use folds of size and drop the remaining instances make sure to normalize the data x by subtracting the mean and dividing by the standard deviation over each dimension note that you should compute the mean and stdev using the training data only and then apply them on the test data this is because in a real application we do not see the test data until after we ship off our program code implement knn your function should take in as inputs a scalar k and matrices vectors of size ntrainxd where ntrain is the number of training instances and d is the feature dimension of size containing the labels of the training instances and of size ntestxd it should output a vector of size for each test instance compute its distance to all training instances pick the closest k training instances pick the most common among their labels and return it as the label for that test instance it ok to use builtin functions that compute distances sort compute the most common member of a list etc in your submission report in a text file the test accuracy when k remember to average the test accuracy over the folds so far we have been weighing neighbors equally now we want to experiment with weighing them according to their distance to the test sample of interest implement a gaussianweighed knn classifier using the equation given in class experiment with different values of the bandwidth parameter σ from the equations on the board and report the results remember that your plot should be averaged over the test folds part ii fisher linear discriminant points you have the following twodimensional data the first five data points belong to one class and the second set of five to a second class write a function that computes the direction of the w vector corresponding to the fisher linear discriminant of the data points apply the function to the given data plot both the points and the direction along which they are projected save the figure and include it in your submission hint the slope of the line w along which the points are plotted is w w have a look at this file for starter plotting code do not hesitate to ask the ta for help with plotting part iii perceptron points in this part you will trace through a run of the perceptron algorithm use the data samples and labels from part homework http people cs pitt edu kovashka htm ii but subtract from their coordinates to center them around the origin or add a bias this data is linearly separable and can be plotted in using its two feature dimensions your goal is to create figures similar to figure in bishop implement the perceptron algorithm and use it along with some plotting code you write to trace through several iterations of the method use the equations from the slides for perceptron on use all the data for training you need to keep track of which instances are misclassified in each step the weight vector w is adjusted using one misclassified example instead of a basis function you will just use x i e φ x x set η to in your submission zip file for three consecutive iterations of the method show which points are misclassified and the current w in each iteration output the iteration id and the two feature dimensions for the misclassified example that is being used to correct the w use the feature dimensions to identify which point is being used plot positive points as hollow circles and negatives as filled circles plot correctly classified points in green and misclassified ones in red if too many or too few iterations are taking place terminate the run and start another run do not hesitate to ask the ta for help with plotting refer to the plotting code from part ii part iv short answers support vector machines points pts bishop exercise pts bishop exercise hints expand the norm notation remembering that the norm of a vector x is xtx that xtx is the same as an inner product of x with itself and transpose distribution properties then express the expanded form with kernel notation what type of kernel do you see pts bishop exercise pts bishop exercise hints this is a twoclass problem you have two constraints total one for the positive instance and one for the negative instance and these constraints are equalities because your positive and negative points have to lie on the margin and be support vectors use lagrange multipliers to make the constraints part of the optimization problem and find what w and b equal pts examine the matlab function quadprog it can be used train an svm find the optimal w consider the input variables h f a b aeq beq lb ub write pseudocode that shows how you should set each of them so that the quadratic program that is solved is the solution to an svm also include pseudocode that shows how to compute the weight vector w from the output of quadprog make sure to explain in your pseudocode what notation you are using for the train test feature label matrices homework http people cs pitt edu kovashka htm homework due note if you are asked to implement something by yourself it is not ok to use or even look at existing matlab or python code unless it utility code if you have questions about what you can use ask the instructor or the ta part i neural networks points in this exercise you will train and evaluate a very simple neural network you will train a network with a single hidden layer your network will have a single output dimension i e k your network should have a tanh activation function at the hidden layer and identity activation function i e yk ak at the output layer this is exactly the function we used to illustrate the backprop algorithm in other words if you directly follow the slides from class slides from slide deck this will be very quick to implement the network will be trained for a regression task using the wine quality dataset from include one function which computes activations forward pass and another function which performs training using backpropagation and calls the activationcomputation function as it iterates also use the forward pass function to evaluate your network after training call both of these functions from a main function which sets up your train test splits trains the neural network computes predictions on the train test data and prints your accuracy on the training and test sets initialize your weights to small random numbers e g on a scale of experiment with different values of the number of hidden neurons m the number of iterations before you terminate training the learning rate show plots in your submission that demonstrate what happens as you vary each of these three factors while keeping the other two factors the same you can start with the following values m lr part ii convolutional neural networks points you don t need to write any code for this just do it by hand in this part you will compute the output from applying a single set of convolution nonlinearity and pooling operations on a toy example below are your image with size n and your filter with size f a first show the output of applying convolution use no padding and a stride of in both the horizontal and vertical directions homework http people cs pitt edu kovashka htm b second show the output of applying a rectified linear unit relu activation c third show the output of applying max pooling over regions part iii adaboost points in this exercise you will implement the adaboost method defined on pages in bishop section use the pima dataset from use decision stumps as your weak classifiers each decision stump operates on some feature dimension and uses some threshold over that feature dimension to make positive negative predictions for each decision stump use at least thresholds include one function which determines the best decision stump the one with the lowest weighted error on the training data include another function which implements the adaboost loop using decision stumps and outputs the final set of classifiers and weights alpha associated with each also include a third function which sets up your train test splits calls the adaboost loop to train computes predictions on the train test data and prints accuracy on the training and test sets part iv probability review points a bishop exercise b bishop exercise c bishop exercise first part only hint transform the righthand side into the lefthand side machine learning http people cs pitt edu kovashka machine learning overview policies project schedule resources machine learning spring overview course description the course will cover the following topics learning basics unsupervised learning supervised learning classification regression clustering dimensionality reduction nearest neighbor classification support vector machines neural networks density estimation bayesian belief networks hidden markov models expectation maximization decision trees and ensembles there will be homework assignments two exams and a final project prerequisites math stat stat the expectation is that you can program and analyze the efficiency and performance of programs you should also be able to compute derivatives of functions further some experience with linear algebra matrix and vector operations and probability is expected piazza sign up for it here note that we will use piazza for two main purposes for announcements and for classmatetoclassmates discussion of homework problems etc the instructor will monitor piazza infrequently the time when you should ask the instructor or ta questions is during office hours programming languages for homework assignments you can use matlab or python for the course project you can use any language of your choice textbooks we will have required readings from two textbooks in some cases the readings will be overlapping but it helps to read two phrasings of the same idea in other cases one reading is more complete or sometimes more intuitive than the other christopher m bishop pattern recognition and machine learning springer kevin p murphy machine learning a probabilistic perspective mit press full text available online through the pitt library you can also refer to the following two textbooks for additional examples and explanations trevor hastie robert tibshirani and jerome friedman the elements of statistical learning springer available online on the second author page david barber bayesian reasoning and machine learning cambridge university press available online on the author page top policies grading grading will be based on the following components homework assignments assignments x each course project midterm and final exam midterm final participation homework submission mechanics you will submit your homework using courseweb navigate to the courseweb page for then click on assignments on the left and the corresponding homework id your written answers should be a single pdf doc docx file your source code should be a single zip file also including images results if requested name the file extension please comment your code homework is due at on the due date grades will be posted on courseweb exams machine learning http people cs pitt edu kovashka there will be one inclass midterm exam and a final exam which will focus on material from the latter part of the course there will be no makeup exams unless you or a close relative is seriously ill participation students are expected to regularly attend the class lectures and should actively engage in inclass discussions attendance will not be taken but keep in mind that if you don t attend you cannot participate you can actively participate by for example responding to the instructor or others questions asking questions or making meaningful remarks and comments about the lecture and answering others questions on piazza you are also encouraged to bring in relevant articles you saw in the news late policy on your programming assignments only you get free late days counted in minutes i e you can submit a total of hours late for example you can submit one homework hours late and another hours late once you ve used up your free late days you will incur a penalty of from the total assignment credit possible for each late day a late day is anything from minute to hours note this policy does not apply to components of the project collaboration policy and academic honesty you will do your work exams and homework individually the only exception is the project which can be done in pairs the work you turn in must be your own work you are allowed to discuss the assignments with your classmates but do not look at code they might have written for the assignments or at their written answers you are also not allowed to search for code on the internet use solutions posted online unless you are explicitly allowed to look at those or to use matlab or python implementation if you are asked to write your own code when in doubt about what you can or cannot use ask the instructor plagiarism will cause you to fail the class and receive disciplinary penalty please consult the university guidelines on academic integrity note on disabilities if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services drs william pitt union drsrecep pitt edu for asl users as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course note on medical conditions if you have a medical condition which will prevent you from doing a certain assignment you must inform the instructor of this before the deadline you must then submit documentation of your condition within a week of the assignment deadline statement on classroom recording to ensure the free and open discussion of ideas students may not record classroom lectures discussion and or activities without the advance written permission of the instructor and any such recording properly approved in advance can be used solely for the student own private use top project a project can be type a design of a new method for an existing problem or an application of techniques we studied in class or another method to a new problem that we have not discussed in class type b experimental comparison of a number of existing techniques on a known problem and detailed discussion and analysis of the results type c an extensive literature review and analysis on one of the topics covered in class milestones for the project proposal not for a grade but should be submitted aim for at least pages the better thoughtout this is the more feedback the instructor can give also think about what data and or code you will use draft of final grade this should be like a final version of your final report and should have all sections that your final report would have although some of them will be incomplete yet showing as much progress as you can the expectation is that at this point you have done or of the required work for this project presentation of final grade aim to be clear enthusiastic and concise you need to submit on courseweb the presentation file on the day of your presentation for the instructor reference final report of final grade use some existing conference template the final report should resemble a conference paper and should include as applicable clear problem definition and argumentation of why this problem is important overview of related work detailed explanation of the approach wellmotivated experimental evaluation including setup description and a description of what each team member did general rules students are encouraged to work in groups of two for their final project the only exception is the literature review which can only be done by students working individually machine learning http people cs pitt edu kovashka the project should include some amount of novelty you are encouraged to use any external expertise you might have e g biology physics etc so that your project makes the best use of areas you know well and is as interesting as possible combining your final project for this class and another class is generally permitted but the project proposal and final report should clearly outline what part of the work was done to get credit in this class and the instructor should approve the proposed breakdown of work between this and another class the final report should be selfcontained i e the instructor should not have to read any other papers to understand what you did all project written items are due at on courseweb if you are proposing a new problem or a new solution to an existing problem the project should include some amount of novelty for example you cannot just reimplement an existing paper or project you should come up with a new method or apply an existing method for a new problem do not rely on data collection to be the novel component of your work if you are proposing to tackle a new problem you might need to collect data but while this is a contribution it will not be enough to earn a good project grade you still have to come up with a solid method idea i e your project has to have sufficient technical novelty you must show that your method is in some sense better quantitatively than at least some relatively recent existing methods for example you can show that your method achieves superior accuracy in some prediction task compared to prior methods or that it achieves comparable accuracy but is faster this outcome is not guaranteed to come out the way you intended during the limited timespan of a course project so whether or not your outperform the state of the art will only be a small component of your grade further if you propose a sufficiently interesting method rather than an extremely simple method it will be less of a problem if your method does not outperform other existing approaches to the problem each of the following components will be graded how well you introduced and motivated the problem in your presentation and final report how well you researched and presented the relevant work in the area you are tackling how technically solid and novel your method is how well you experimentally tested your method and analytically discussed your experimental findings how well you were able to draw conclusions from your work and discuss potential future work to further improve on the problem you proposed to tackle you are allowed to use existing code for known methods but again notice that your project is expected to be a significant amount of work and not just a straightup run of some package this type of project has the highest chance of turning into a published workshop or conference paper even for this type of project you should present a very brief literature review during your presentation so your classmates know the space in which you are working a good source for learning about what work has been done in your domain of interest are search engines google scholar and arxiv org if you are proposing a literature review or are proposing to experimentally compare existing solutions to a known problem what you are proposing to do should not already have been done in another published paper including papers on arxiv org you have to properly introduce and motivate the problem you chose to study i e why it is important and why it is challenging for experimental comparisons you still need to present a detailed literature review for the topic at hand you must review and include detailed descriptions in your final report of at least papers if code is not available for most of the papers you chose to implement you need to experimentally compare at least papers in your implementation of papers without code you do not have to follow the papers in every detail but your implementation should be faithful to the paper you are implementing in spirit you should implement rather than use existing code for at least one of the methods you compare against e g you might use code for papers and implement additional paper make sure to include a careful justification why these are the ones you chose to implement make sure to include a detailed analysis of the strengths and weaknesses of each paper you chose to compare based on both the published papers as well as the experimental findings you collected over the course of the project for literature reviews your final report should include at least references it should show a sensible organization of these references and at least one paragraph containing details about each paper including at least sentences describing the method in each of the referenced works make sure to describe both the technical details and the experimental techniques used in each of the papers you present make sure to discuss some strengths and weaknesses of each paper you include in your review also include a synthesis summary of what has been accomplished in the community on the problem you chose to study grouped by the themes of the papers and what future work might be literature reviews can only be done in teams of one for sources of ideas for computer vision project ideas you can look at the list of datasets and tasks below for inspiration or read some paper abstracts on this page for nlp project ideas see this page from christopher manning also look at the following list of project suggestions from ray mooney but please do not contact any of the contacts given this one from carlos guestrin this one from andreas krause and this one from andrew ng top schedule date chapter topic readings lecture slides due introduction murphy ch bishop sec pptx pdf machine learning http people cs pitt edu kovashka intro linear algebra and matlab pptx pdf unsupervised learning clustering bishop sec murphy sec pptx pdf out dimensionality reduction bishop sec murphy sec pptx pdf notes regression line fitting biasvariance bishop sec pptx pdf linear regression bishop sec murphy sec sec pptx pdf notes classification intro and linear models nearest neighbors bishop sec linear models for classification bishop sec murphy ch due support vector machines bishop sec murphy sec midterm exam classification nonlinear models neural networks bishop sec skip murphy sec convolutional neural networks karpathy notes module project proposal due spring break no class recurrent neural networks decision trees bagging and boosting bishop sec skip murphy sec classification probabilistic models probability review density estimation bishop sec murphy sec bayesian belief networks bishop sec murphy ch 28 markov random fields hidden markov models bishop sec jurafsky martin skip murphy sec project draft due expectation maximization bishop sec murphy ch wrapup projects exam research topics project presentations 25 project final report due machine learning http people cs pitt edu kovashka final exam top resources this course was inspired by the following courses machine learning by milos hauskrecht university of pittsburgh spring introduction to machine learning by dhruv batra virginia tech spring machine learning by tommi jaakkola mit machine learning by subhransu maji umass amhrest spring machine learning by erik sudderth brown university fall computer vision by kristen grauman ut austin spring computer vision by derek hoiem uiuc spring natural language processing by ray mooney ut austin tutorials matlab tutorial linear algebra review by feifei li brief machine learning intro by aditya khosla and joseph lim resources list including code and data tutorials and other related courses compiled by devi parikh some computer vision datasets microsoft coco common objects in context object recognition segmentation image description imagenet object recognition sun database scenes caltechucsd birds finegrained object recognition msrc annotations active learning animals with attributes attributebased recognition apascal ayahoo attributebased recognition shoes attributebased search inria movie actions action recognition adl egocentric action recognition action quality evaluating action quality cardb historical cars style classification of cars recognizing image style photographic style classification judd gaze visual saliency prediction visual persuasion predicting subtle messages in images vqa visual questionanswering recognition datasets list compiled by kristen grauman human activity datasets list compiled by chaoyeh chen some code of interest libsvm by chihchung chang and chihjen lin svm light by thorsten joachims vlfeat feature extraction tutorials and more by andrea vedaldi caffe deep learning code by yangqing jia et al top prof adriana kovashka university of pittsburgh january about the instructor born in sofia bulgaria got ba in at pomona college ca computer science media studies got phd in at university of texas at austin computer vision course info course website instructor adriana kovashka kovashka cs pitt edu use at the beginning of your subject office sennott square office hours tue thu ta longhao li cs pitt edu office sennott square office hours tbd do the doodle by the end of friday note longhao is out of the country until jan please email him any questions textbooks christopher m bishop pattern recognition and machine learning springer kevin p murphy machine learning a probabilistic perspective mit press more resources available on course webpage your notes from class are your best study material slides are not complete with notes course goals to learn the basic machine learning techniques both from a theoretical and practical perspective to learn how to apply these techniques on toy problems to get experience with these techniques on a real world problem policies and schedule should i take this class it will be a lot of work but you will learn a lot some parts will be hard and require that you pay close attention but i will have periodic ungraded pop quizzes to see how you re doing i will also pick on students randomly to answer questions use instructor and ta office hours questions introductions what is machine learning example problems and tasks ml in a nutshell challenges measuring performance what is your name what one thing outside of school are you passionate about do you have any prior experience with machine learning what do you hope to get out of this class every time you speak please remind me your name finding patterns and relationships in data we can apply these patterns to make useful predictions e g we can predict how much a user will like a movie even though that user never rated that movie netflix challenge given lots of data about how users rated movies training data but we don t know how user i will rate movie j and want to predict that test data spam or not vs weather prediction who will win contest of your choice machine translation speech recognition pose estimation face recognition image categorization pizza wine stove is it dangerous how fast does it run is it alive is it soft slide credit derek hoiem does it have a tail can i poke with it attribute based image retrieval dating car photographs inferring visual persuasion answering questions about images what else what are some problems in your area of research or from your everyday life that can be helped by machine learning tens of thousands of machine learning algorithms decades of ml research oversimplified learn a mapping from input to output f x y x emails y spam notspam slide credit pedro domingos y f x output prediction function features training given a training set of labeled examples xn yn estimate the prediction function f by minimizing the prediction error on the training set testing apply f to a never before seen test example x and output the predicted value y f x apply a prediction function to a feature representation of the image to get the desired output f apple f tomato f cow training testing test image slide credit d hoiem and l lazebnik training vs testing what do we want high accuracy on training data no high accuracy on unseen new test data why is this tricky training data features x and labels y used to learn mapping f test data features used to make a prediction labels only used to see how well we ve learned f validation data held out set of the training data can use both features and labels to tune parameters of the model we re learning why do we hope this would work statistical estimation view x and y are random variables d xn yn p x y both training testing data sampled iid from p x y iid independent and identically distributed learn on training set have some hope of generalizing to test set ml in a nutshell every machine learning algorithm has data representation x y problem representation evaluation objective function optimization data representation let brainstorm what our x should be for various y prediction tasks problem representation decision trees sets of rules logic programs instances graphical models bayes markov nets neural networks support vector machines model ensembles etc evaluation objective function accuracy precision and recall squared error likelihood posterior probability cost utility margin entropy k l divergence etc optimization discrete combinatorial optimization e g graph algorithms continuous optimization e g linear programming types of learning supervised learning training data includes desired outputs unsupervised learning training data does not include desired outputs weakly or semi supervised learning training data includes a few desired outputs reinforcement learning rewards from sequence of actions supervised learning x y discrete x y continuous unsupervised learning x x discrete id x x continuous improve on task t with respect to performance metric p based on experience e t categorize email messages as spam or legitimate p percentage of email messages correctly classified e database of emails some with human given labels t recognizing hand written words p percentage of words correctly classified e database of human labeled images of handwritten words t playing checkers p percentage of games won against an arbitrary opponent e playing practice games against itself t driving on four lane highways using vision sensors p average distance traveled before a human judged error e a sequence of images and steering commands recorded while observing a human driver spam or not vs spam emails a lot of words like money free bank account regular emails word usage pattern is more spread out simple strategy let count this is x this is y оr weigh counts and sum to get prediction why these words klingon vs mlingon classification training data klingon klix kour koop mlingon moo maa mou testing data kap which language why board why not just hand code these weights we re letting the data do the work rather than develop hand code classification rules the machine is learning to program itself but there are challenges i saw her duck with a telescope what humans see what computers see some challenges ambiguity and context machines take data representations too literally humans are much better than machines at generalization which is needed since test data will rarely look exactly like the training data why might it be hard to predict if a viewer will like a movie recognize cars in images translate between languages many basic effective and efficient algorithms available large amounts of on line data available large amounts of computational resources available if y is discrete accuracy correctly classified all test examples true positive false positive true negative false negative weighted misclassification via a confusion matrix if y is discrete precision recall precision tp tp fp predicted true pos predicted pos recall tp tp fn predicted true pos true pos f measure p r want evaluation metric to be in some range e g worst possible classifier best possible classifier true positives images that contain people true negatives images that do not contain people predicted positives images predicted to contain people predicted negatives images predicted not to contain people precision recall f measure accuracy if y is continuous sum of squared differences ssd error between predicted and true y e n i f xi yi fill out doodle read entire course website do first reading linear algebra review matlab tutorial prof adriana kovashka university of pittsburgh january announcement ta won t be back until the last week of january skype office hours tuesday wednesday to username please do doodle linear algebra primer professor fei fei li stanford vectors and matrices vectors and matrices are just collections of ordered numbers that represent something movements in space scaling factors word counts movie ratings pixel brightnesses etc we ll define some common uses and standard operations on them a column vector where a row vector where denotes the transpose operation you ll want to keep track of the orientation of your vectors when programming in matlab you can transpose a vector v in matlab by writing v vectors can represent an offset in or space points are just vectors from the origin data can also be treated as a vector such vectors don t have a geometric interpretation but calculations like distance still have value a matrix is an array of numbers with size 𝑚 by 𝑛 i e m rows and n columns if we say that is square addition can only add a matrix with matching dimensions or a scalar scaling inner product dot product of vectors multiply corresponding entries of two vectors and add up the result x y is also x y cos the angle between x and y inner product dot product of vectors if b is a unit vector then a b gives the length of a which lies in the direction of b projection if b is unit length hence norm is norm norm lp norm for real numbers p multiplication the product ab is each entry in the result is that row of a dot product with that column of b multiplication example each entry of the matrix product is made by taking the dot product of the corresponding row in the left matrix with the corresponding column in the right one transpose flip matrix so row becomes column a useful identity identity matrix i square matrix along diagonal elsewhere i another matrix that matrix diagonal matrix square matrix with numbers along diagonal elsewhere a diagonal another matrix scales the rows of that matrix symmetric matrix given a matrix a its inverse a is a matrix such that aa a i e g inverse does not always exist if a exists a is invertible or non singular otherwise it singular matlab example say you have the matrix equation ax b where a and b are known and you want to solve for x you could use matlab to calculate the inverse and premultiply by it a a x a matlab command would be inv a b but calculating the inverse for large matrices often brings problems with computer floating point resolution or your matrix might not even have an inverse fortunately there are workarounds instead of taking an inverse directly ask matlab to solve for x in ax b by typing a b matlab will try several appropriate numerical methods including the pseudoinverse if the inverse doesn t exist matlab will return the value of x which solves the equation if there is no exact solution it will return the closest one if there are many solutions it will return the smallest one matrix addition is commutative and associative a b b a a b c a b c matrix multiplication is associative and distributive but not commutative a b c a b c a b c a b a c a b b a suppose we have a set of vectors vn if we can express as a linear combination of the other vectors vn then is linearly dependent on the other vectors the direction can be expressed as a combination of the directions vn e g if no vector is linearly dependent on the rest of the set the set is linearly independent common case a set of vectors vn is always linearly independent if each vector is perpendicular to every other vector and non zero linearly independent set not linearly independent column row rank column rank always equals row rank matrix rank if a matrix is not full rank inverse doesn t exist inverse also doesn t exist for non square matrices there are several computer algorithms that can factor a matrix representing it as the product of some other matrices the most useful of these is the singular value decomposition represents any matrix a as a product of three matrices uσvt matlab command u s v svd a uσvt a where u and v are rotation matrices and σ is a scaling matrix for example in general if a is m x n then u will be m x m σ will be m x n and vt will be n x n u and v are always rotation matrices geometric rotation may not be an applicable concept depending on the matrix so we call them unitary matrices each column is a unit vector σ is a diagonal matrix the number of nonzero entries rank of a the algorithm always sorts the entries high to low m uσvt illustration from wikipedia we ve discussed svd in terms of geometric transformation matrices but svd of a data matrix can also be very useful to understand this we ll look at a less geometric interpretation of what svd is doing svd applications look at how the multiplication works out left to right column of u gets scaled by the first value from σ the resulting vector gets scaled by row of vt to produce a contribution to the columns of a svd applications each product of column i of u value i from σ row i of vt produces a component of the final a svd applications we re building a as a linear combination of the columns of u using all columns of u we ll rebuild the original matrix perfectly but in real world data often we can just use the first few columns of u and we ll get something close e g the first apartial above svd applications we can call those first few columns of u the principal components of the data they show the major patterns that can be added to produce the columns of the original matrix the rows of vt show how the principal components are mixed to produce the columns of the matrix svd applications we can look at σ to see that the first column has a large effect while the second column has a much smaller effect in this example principal component analysis remember columns of u are the principal components of the data the major patterns that can be added to produce the columns of the original matrix one use of this is to construct a matrix where each column is a separate data sample run svd on that matrix and look at the first few columns of u to see patterns that are common among the columns this is called principal component analysis or pca of the data samples principal component analysis often raw data samples have a lot of redundancy and patterns pca can allow you to represent data samples as weights on the principal components rather than using the original raw form of the data by representing each sample as just those weights you can represent just the meat of what different between samples this minimal representation makes machine learning and other algorithms much more efficient example eigenfaces images of faces represent each as the concatenation of its pixels column vectors stack together horizontally to get matrix a u s v svd a first four columns of u can represent each face as a linear combination of the first few columns of u image from alexander ihler addendum how is svd computed for this class tell matlab to do it use the result but if you re interested one computer algorithm to do it makes use of eigenvectors the following material is presented to make svd less of a magical black box but you will do fine in this class if you treat svd as a magical black box as long as you remember its properties from the previous slides suppose we have a square matrix a we can solve for vector x and scalar λ such that ax λx in other words find vectors where if we transform them with a the only effect is to scale them with no change in direction these vectors are called eigenvectors german for self vector of the matrix and the scaling factors λ are called eigenvalues an m x m matrix will have m eigenvectors where λ is nonzero computers can find an x such that ax λx using this iterative algorithm x random unit vector while x hasn t converged x ax normalize x x will quickly converge to an eigenvector some simple modifications will let this algorithm find all eigenvectors eigenvectors are for square matrices but svd is for all matrices to do svd a computers can do this take eigenvectors of aat matrix is always square these eigenvectors are the columns of u square root of eigenvalues are the singular values the entries of σ take eigenvectors of ata matrix is always square these eigenvectors are columns of v or rows of vt moral of the story svd is fast even for large matrices it useful for a lot of stuff there are also other algorithms to compute svd or part of the svd matlab svd command has options to efficiently compute only what you need if performance becomes an issue matlab tutorial please cover whatever we don t finish at home tutorials and exercises do problems most also have solutions ask the ta if you have any problems prof adriana kovashka university of pittsburgh january grouping items that belong together i e have similar features unsupervised we only use the features x not the labels y this is useful because we may not have any labels but we can still detect patterns summarizing data look at large amounts of data represent a large continuous vector with the cluster number counting computing feature histograms prediction data points in the same cluster may have the same labels slide credit j hays d hoiem counting and classification via clustering compute a histogram to summarize the data after clustering ask a human to label each group cluster cat panda group id giraffe image segmentation via clustering separate image into coherent objects image human segmentation unsupervised discovery we don t know what the objects in red boxes are but we know they tend to occur in similar context if features the context objects in red will cluster together then ask human for a label on one example from the cluster and keep learning new object categories iteratively today and next class clustering motivation and applications algorithms mean shift find modes in the data hierarchical clustering start with all points in separate clusters and merge normalized cuts split nodes in a graph based on similarity image segmentation toy example black pixels gray pixels white pixels input image intensity these intensities define the three groups we could label every pixel in the image according to which of these primary intensities it is i e segment the image based on the intensity feature what if the image isn t quite so simple input image intensity intensity goal choose three centers as the representative intensities and label every pixel according to which of these centers it is nearest to best cluster centers are those that minimize ssd between all points and their nearest cluster center ci clustering with this objective it is a chicken and egg problem if we knew the cluster centers we could allocate points to groups by assigning each to its closest center if we knew the group memberships we could get the centers by computing the mean per group k means clustering basic idea randomly initialize the k cluster centers and iterate between the two steps we just saw randomly initialize the cluster centers ck given cluster centers determine points in each cluster for each point p find the closest ci put p into cluster i given points in each cluster solve for ci set ci to be the mean of points in cluster i if ci have changed repeat step properties will always converge to some solution can be a local minimum does not always find the global minimum of objective function source steve seitz k means l ask user how many cllusters they d nke e g k 7 k means l ask user how clusters they d lliike e g k guess k clluster center locations k means ask user how many clusters they d like e g k s guess k cluster ce nt e r locat ions each dlatapoint finds out wh ich center it closest to thus each cente r a set of datapo ints l k means ask user how many clusters they d like e g k guess k cluster center locations each datapoint finds out which cent er it closest to each center finds the centroid of the points it owns o l k means converges to a local minimum k means clustering java demo matlab demo time complexity let n number of instances d dimensionality of the features k number of clusters assume computing distance between two instances is o d reassigning clusters o kn distance computations or o knd computing centroids each instance vector gets added once to a centroid o nd assume these two steps are each done once for a fixed number of iterations i o iknd linear in all relevant factors adapted from ray mooney another way of writing objective k means let rnk if instance n belongs to cluster k otherwise k medoids more general distances distance metrics euclidian distanmce norm norm x y xi i m yi x y i xi yi cosine similarity transform to a distance by subtracting from x y x y segmentation as clustering depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on intensity similarity feature space intensity value d k k quantization of the feature space segmentation label map segmentation as clustering depending on what we choose as the feature space we can group pixels in different ways grouping pixels based on color similarity r g b r g b r g r b r g b feature space color value d source k grauman k means pros and cons pros simple fast to compute converges to local minimum of within cluster squared error cons issues setting k one way silhouette coefficient sensitive to initial centers use heuristics or output of another method sensitive to outliers detects spherical clusters adapted from k grauman today clustering motivation and applications algorithms k means iterate between finding centers and assigning points hierarchical clustering start with all points in separate clusters and merge normalized cuts split nodes in a graph based on similarity the mean shift algorithm seeks modes or local maxima of density in the feature space image feature space l u v color values kernel estimated density data d search window center of mass mean shift vector source d hoiem cluster all data points in the attraction basin of a mode attraction basin the region for which all trajectories lead to the same mode slide by y ukrainitz b sarel simple mean shift procedure compute mean shift vector translate the kernel window by m x adapted from y ukrainitz b sarel compute features for each point intensity word counts etc initialize windows at individual feature points perform mean shift for each window until convergence merge windows that end up near the same peak or mode source d hoiem pros mean shift does not assume shape on clusters robust to outliers cons issues need to choose window size expensive o i d search for neighbors could be sped up in lower dimensions does not scale well with dimension of feature space mean shift reading nicely written mean shift explanation with math includes m code for mean shift clustering mean shift paper by comaniciu and meer adaptive mean shift in higher dimensions source k grauman today clustering motivation and applications algorithms k means iterate between finding centers and assigning points mean shift find modes in the data normalized cuts split nodes in a graph based on similarity assumes a similarity function for determining the similarity of two instances starts with all instances in separate clusters and then repeatedly joins the two clusters that are most similar until there is only one cluster the history of merging forms a binary tree or hierarchy start with all instances in their own cluster until there is only one cluster among the current clusters determine the two clusters ci and cj that are most similar replace ci and cj with a single cluster ci cj how many clusters clustering creates a dendrogram a tree to get final clusters pick a threshold max number of clusters or max distance within clusters y axis how to compute similarity of two clusters each possibly containing multiple instances single link similarity of two most similar members sim ci c j max x ci y c j sim x y complete link similarity of two least similar members sim ci c j min x ci y c j sim x y group average average similarity between members today clustering motivation and applications algorithms k means iterate between finding centers and assigning points mean shift find modes in the data hierarchical clustering start with all points in separate clusters and merge fully connected graph node vertex for every pixel link between every pair of pixels p q affinity weight wpq for each link edge wpq measures similarity similarity is inversely proportional to difference in color and position a b c break graph into segments want to delete links that cross between segments easiest to break links that have low similarity low weight similar pixels should be in the same segments dissimilar pixels should be in different segments b link cut set of links whose removal makes a graph disconnected cost of a cut find minimum cut cut a b w p a q b p q gives you a segmentation fast algorithms exist for doing this minimum cut problem with minimum cut weight of cut proportional to number of edges in the cut tends to produce small isolated components shi malik pami cuts in a graph normalized cut b normalize for size of segments cut a b wp q p a q b cut a b assoc a v cut a b assoc b v assoc a v sum of weights of all edges that touch a ncut value small when we get two clusters with many edges with high weights within them and few edges of low weight between them shi and j malik cvpr adapted from steve seitz might depend on application purity where is the set of clusters and is the set of classes see murphy sec for another two metrics rand index and mutual information clustering strategies k means iteratively re assign points to the nearest cluster center mean shift clustering estimate modes graph cuts split the nodes in a graph based on assigned links with similarity weights agglomerative clustering start with each point as its own cluster and iteratively merge the closest clusters prof adriana kovashka university of pittsburgh january plan for today dimensionality reduction motivation principal component analysis pca applications of pca other methods for dimensionality reduction why reduce dimensionality data may intrinsically live in a lower dim space too many features and too few data lower computational expense memory train test time want to visualize the data in a lower dim space want to use data of different dimensionality input data in a high dim feature space output projection of same data into a lower dim space f high dim x low dim x slide credit erik sudderth find a projection where the data has low reconstruction error high variance of the data see hand written notes for how we find the optimal projection slide credit subhransu maji demo with eigenfaces covariance matrix is huge for d pixels but typically examples n d simple trick x is nxd matrix of normalized training data solve for eigenvectors u of xxt instead of xtx then xu is eigenvector of covariance xtx need to normalize each vector of xu into unit length adapted from derek hoiem one goal can be to pick k such that p of the variance of the data is preserved e g let λ a vector containing the eigenvalues of the covariance matrix total variance can be obtained from entries of λ sum λ take as many of these entries as needed k find cumsum λ p variance preserved at i th eigenvalue figure a from bishop application face recognition face recognition once you ve detected and cropped a face try to recognize it sally verification a person is claiming a particular identity verify whether that is true e g security closed world identification assign a face to one person from among a known set general identification assign a face to a known person or to unknown when viewed as vectors of pixel values face images are extremely high dimensional image dimensions slow and lots of storage but very few dimensional vectors are valid face images we want to effectively model the subspace of face images face x in face space coordinates reconstruction x µ process labeled training images find mean µ and covariance matrix σ find k principal components eigenvectors of σ uk project each training image xi onto subspace spanned by principal components wik u txi u txi given novel image x project onto subspace wk u tx u tx classify as closest training face in k dimensional subspace m turk and a pentland cvpr singular value decomposition alternative method to calculate still subtract mean decompose x u s vt orthogonal xr x vs s vr v d vr x xr u ljt u d ljt u s matrix provides coefficients example xi ui gives the least squares approximation to x of this form i k ki i k eigen x represent x using pca ex viola jones data set images of faces dimensional measurements eigen x represent x using pca ex viola jones data set images of faces dimensional measurements take first k pca components n x l mean v l v v v eigen x represent x using pca ex viola jones data set images of faces dimensional measurements take first k pca components mean dir k k dir dir dir k4 k collaborative filtering netflix of bellkor team cd users i ji k j latent space models model ratings matrix as user and movie positions cj users infer values from known ratings extrapolate to unranked users d cj latent space models ous braveheart the color purple i amadeus chick flicks the princess diaries the lion king independence day escapist dumb and dumber text representations bag of words remember word counts but not order example rain and chilly weather didn t keep thousands of paradegoers from camping out friday night for the tournament of roses spirits were high among the street party crowd as they set up for curbside seats for today parade i want to party all night said tyne gaudielle of glendale who spent the last night of the year along colorado boulevard with a group of friends whether they came for the partying or the parade campers were in for a long night rain continued into the evening and tern peratures were expected to dip down into the low bag of words remember word counts but not order example examplel txt rain and chilly weather didn t keep thousa ra n paradegoers from camping out friday nig chilly of roses weather didn spirits were high among the street party er keep for curbside seats for today parade an ds parad egoers i want to party all night said tyne gau camping glendale who spent the last night of the ye out boulevard with a group of friends friday night whether they came for the partying or the l l l th in for a long night rain continued into the tournament tern peratures were expected to dip down in roses spirits pca for text data create a giant matrix of words in docs word j appears feature xj in document i data example i wordj doc i huge matrix mostly zeros typically normalize by e g sum over j to control for short docs typically don t subtract mean or normalize by variance might transform counts in some way log etc pca on this matrix provides a new representation document comparison fuzzy search concept instead of word matching plan for today dimensionality reduction motivation principal component analysis pca applications of pca pca general dimensionality reduction technique preserves most of variance with a much more compact representation lower storage requirements eigenvectors a few numbers per face faster matching what are some problems the direction of maximum variance is not always good for classification pca preserves maximum variance a more discriminative subspace fisher linear discriminants fld preserves discrimination find projection that maximizes scatter between classes and minimizes scatter within classes fisher linear discriminant using two classes as example poor projection good comparison with pca other dimensionality reduction methods non linear kernel pca schölkopf et al neural computation independent component analysis comon signal processing lle locally linear embedding roweis and saul science isomap isometric feature mapping tenenbaum et al science t sne t distributed stochastic neighbor embedding van der maaten and hinton jmlr figure from genevieve patterson ijcv cs machine learning line fitting bias variance trade off prof adriana kovashka university of pittsburgh january generalization training set labels known test set labels unknown how well does a learned model generalize from the data it was trained on to a new test set slide credit l lazebnik generalization components of expected loss noise in our observations unavoidable bias how much the average model over all training sets differs from the true model error due to inaccurate assumptions simplifications made by the model variance how much models estimated from different training sets differ from each other underfitting model is too simple to represent all the relevant class characteristics high bias and low variance high training error and high test error overfitting model is too complex and fits irrelevant characteristics noise in the data low bias and high variance low training error and high test error adapted from l lazebnik models with too few parameters are inaccurate because of a large bias not enough flexibility models with too many parameters are inaccurate because of a large variance too much sensitivity to the sample purple dots possible test points red dots training data all that we see before we ship off our model green curve true underlying model blue curve our predicted model fit adapted from d hoiem root mean square rms error order polynomial order polynomial penalize large coefficient values remember we want to minimize this expression no regularization huge regularization regularization vs training vs test error underfitting overfitting high bias low variance low bias high variance high bias low variance low bias high variance number of training examples choosing the trade off between bias and variance need validation set separate from the test set high bias low variance low bias high variance figure from chris bishop get more training data regularize the parameters choose a simpler classifier slide credit d hoiem remember three kinds of error inherent unavoidable bias due to over simplifications variance due to inability to perfectly estimate parameters from limited data try simple classifiers first use increasingly powerful classifiers with more training data bias variance trade off adapted from d hoiem prof adriana kovashka university of pittsburgh january your ta will be back this wednesday office hours tuesday and thursday friday office hours start this thursday solution via least squares solution via gradient descent regularized least squares dealing with outliers sometimes want to add a bias term can add as such that x xd figure from milos hauskrecht f x w x y at training time use given xn yn to estimate mapping function f objective minimize yi f w xi for all i n xi are the input features d dimensional yi is the target continuous output label given by human oracle at test time use f to make prediction for some new xtest problem is called classification if y is discrete does your patient have cancer should your bank give this person a credit card is it going to rain tomorrow what animal is in this image problem is called regression if y is continuous what price should you ask for this house what is the temperature going to be tomorrow what score should your system give to this person figure skating performance linear regression r r x we begin by considering linear regression easy to extend to more complex predict ions later on f r r f x w w o w b mx tommi jaakkola mit csail fit line to points use parameters of line to predict the y coordinate of a new data point xnew find parameters of plane derivation on board test test challenges computing the pseudoinverse might be slow for large matrices cubic in number of features d due to svd linear in number of samples n we might want to adjust solution as new examples come in without recomputing the pseudoinverse for each new sample that comes in another solution gradient descent cost linear in both d and n if d use gradient descent global vs local minima fortunately least squares is convex linear regression definition solution via least squares solution via gradient descent dealing with outliers example polynomial curve fitting m y x w w j x w j x j o where q x are known as basis functions typica ly x so that acts as a bias lest we f unc tions j x xd consider the error function data term regularization term with the sum of squares error function and a quadratic regularizer we get which is minimized by with a more general regularizer we have isosurfaces w q constant e g lasso quadratic lasso tends to generate sparser solutions than a quadratic regularizer plan for today linear regression definition solution via least squares solution via gradient descent regularized least squares hypothesize and test try all possible parameter combinations repeatedly sample enough points to solve for parameters each point votes for all consistent parameters e g each point votes for all possible lines on which it might lie score the given parameters number of consistent points choose the optimal parameters using the scores noise clutter features they will cast votes too but typically their votes should be inconsistent with the majority of good features two methods hough transform and ransac adapted from derek hoiem and kristen grauman y b x m image space hough parameter space connection between image x y and hough m b spaces a line in the image corresponds to a point in hough space y b x m image space hough parameter space connection between image x y and hough m b spaces a line in the image corresponds to a point in hough space what does a point in the image space map to answer the solutions of b this is a line in hough space y b x m image space hough parameter space how can we use this to find the most likely parameters m b for the most prominent line in the image space let each edge point in image space vote for a set of possible parameters in hough space accumulate votes in discrete set of bins parameters with the most votes indicate line in image space random sample consensus ransac ransac loop randomly select a seed group of points on which to base model estimate fit model to these points find inliers to this model i e points whose distance from the line is less than t if there are d or more inliers re compute estimate of model on all of the inliers repeat n times keep the model with the largest number of inliers fischler bolles in algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence line fitting example algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence ransac line fitting example algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence line fitting example n i algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence algorithm n i sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model prof adriana kovashka university of pittsburgh february just for today my office hours will be slightly shifted 30 basic formulation of the simplest classifier k nearest neighbors example use generalizing the distance metric and weighting neighbors differently problems the curse of dimensionality picking k approximation strategies a type of supervised learning we want to learn to predict for a new data point x its label y e g spam not spam don t learn an explicit function f x y keep all training data x y for a test example x find the training example xi closest to it e g using euclidean distance then copy the target label yi as the label for x instance based methods exemplar methods memory based methods non parametric methods four things make a memory based learner a distance metric how many nearby neighbors to look at a weighting function optional how to fit with the local points four things make a memory based learner a distance metric euclidean and others how many nearby neighbors to look at a weighting function optional not used how to fit with the local points just predict the same output as the nearest neighbor training examples from class test example training examples from class f x label of the training example nearest to x four things make a memory based learner a distance metric euclidean and others how many nearby neighbors to look at k a weighting function optional not used how to fit with the local points just predict the average output among the nearest neighbors for a new point find the k closest points from training data e g k labels of the k points vote to classify black negative red positive if query lands here the nn consist of negatives and positives so we classify it as negative what are the tradeoffs of having a too large k too small k let x be our test data point and nk x be the indices of the k nearest neighbors of x classification regression million geotagged photos by photographers scene matches hays and efros estimating geographic information from a single image cvpr scene matches hays and efros estimating geographic information from a single image cvpr hays and efros estimating geographic information from a single image cvpr scene matches the importance of data k nearest neighbor four things make a memory based learner how many nearby neighbors to look at k how to fit with the local points just predict the average output among the nearest neighbors distances suppose i want to charge my overall distance more for differences in direction as opposed to direction setup a equal weighing on all directions setup b more weight on direction will my neighborhoods be longer in the or direction voronoi partitioning nearest neighbor regions all points in a region are closer to the seed in that region than to any other seed black dots seeds figure from wikipedia multivariate distance metrics suppose the input vectors xn are two dimensional x11 x22 xn x x dist xi xj x x x x dist xi xj x x the relative scalings in the distance metric affect region shapes adapted from carlos guestrin euclidean minkowski mahalanobis where a is a positive semidefinite matrix symmetric matrix with all non negative eigenvalues manhattan figures from wikipedia neighbors weighted differently use all samples i e k n weight on i th sample σ the bandwidht parameter expresses how quickly our weight function drops off as points get further and further from the query x classification regression extremes bandwidth infinity prediction is dataset average bandwidth zero prediction becomes nn kernel regression classification four things make a memory based learner a distance metric euclidean and others how many nearby neighbors to look at all of them a weighting function optional wi exp d xi query σ2 nearby points to the query are weighted strongly far points weakly the σ parameter is the kernel width bandwidth how to fit with the local points predict the weighted average of the outputs problems with instance based learning too many features doesn t work well if large number of irrelevant features distances overwhelmed by noisy features distances become meaningless in high dimensions the curse of dimensionality what is the impact of the value of k expensive no learning most real work done during testing for every test sample must search through all dataset very slow must use tricks like approximate nearest neighbor search need to store all training data consider sphere of radius in d dims consider an outer ε shell in this sphere what is shell volume sphere volume slide credit dhruv batra the volume of a sphere with radius r in d dimensions is kdrd bishop sec what is shell volume sphere volume as d tends to infinity this ratio tends to i e most of the volume of the sphere is in that thin outer shell which is counter intuitive figure from bishop problem in very high dimensions all points are equally close this problem applies to all types of classifiers not just k nn problems with instance based learning too many features doesn t work well if large number of irrelevant features distances overwhelmed by noisy features distances become meaningless in high dimensions the curse of dimensionality what is the impact of the value of k expensive no learning most real work done during testing for every test sample must search through all dataset very slow must use tricks like approximate nearest neighbor search need to store all training data adapted from dhruv batra slide credit alexander ihler increasing k simplifies decision boundary majority voting means less emphasis on individual points k k increasing k simplifies decision boundary majority voting means less emphasis on individual points k k iknn decision boundary increasing k simplifies decision boundary majority voting means less emphasis on individual points k use a validation set to pick k too complex slide credit alexander ihler problems with instance based learning too many features doesn t work well if large number of irrelevant features distances overwhelmed by noisy features distances become meaningless in high dimensions the curse of dimensionality what is the impact of the value of k expensive no learning most real work done during testing for every test sample must search through all dataset very slow must use tricks like approximate nearest neighbor search need to store all training data adapted from dhruv batra an approximate distance method build a balanced tree of data points kd tree splitting along different dimensions go down tree starting from root to find in which bin a query point lives declare that leaf current best neighbor go up the tree checking for and exploring branches as needed when a closer neighbor could exist in that branch check for possibility of better neighbor by intersecting regions with hypersphere defined by radius of current best eliminate parts of the search space if they cannot contain a better current best only search for neighbors up until some budget exhausted k d tree o log n query time fruit data i i i i c ca k d tree o log n query time fruit data i i i i i i sp lit at the median c ca c k d tree o log n query time e a c a c k d tree o log n query time e a c a c k d tree o log n query time fruit data i i i i i sp lit at the median e a j c c a i i i k d tree o log n query time fruit data i i i i 10 sp lit at the median 9 e a j c o a c i i figure from szeliski summary k nearest neighbor is the most basic and simplest to implement classifier cheap at training time expensive at test time unlike other methods we ll see later naturally works for any number of classes pick k through a validation set use approximate methods for finding neighbors success of classification depends on the amount of data and the meaningfulness of the distance function prof adriana kovashka university of pittsburgh february plan for next two lectures regression for classification fisher s linear discriminant perceptron logistic regression multi way classification generative vs discriminative models classification via regression suppose we ignore the fact that the target output y is binary e g rather than a continuous variable so we will estimate a linear regression function j x w wq wdxd wo xt w based on the available data as before objective we want to minimize classification via regression cont d we can use the resulting regression function to classify any new test example x according to label if f x w and label otherwise f x w therefore defines a linear decision boundary that partitions the input space into two class specific regions half spaces figures adapted from from andrew ng classification via regression cont d given the dissociation between the objective classification and the estimation criterion regression it is not clear that this approach leads to sens ible results fn 12 8 sometimes good sometimes bad tommi jaakkola mit csail the effect of outliers another example magenta least squares green logistic regression projected point in r y xi y y n xnt we can study how well the projected points yn viewed as functions of are separated across the classes by varying w we get different levels of separation between the projected points 5 5 5 5 we would like to find that somehow maximizes the separation of the projected points across classes 5 s 5 5 we can quantify the separation overlap in terms of means and variances of the resulting dimensional class distributions estimation criterion we find that maximizes separation of projected means j fi sher w sum of within class variances fisher s linear discriminant figures from bishop plan for today regression for classification fisher s linear discriminant logistic regression multi way classification generative vs discriminative models rosenblatt prediction rule where want loss or just using the misclassified examples loss learning algorithm update rule interpretation if sample is being misclassified make the weight vector more like it figures from bishop plan for today regression for classification fisher s linear discriminant perceptron multi way classification generative vs discriminative models suppose we know the class conditional densit ies p xly for y l as well as the overall class frequencies p y how do we decide which class a new example x belongs to so as to minimize the overall probability of error suppose we know the class conditional densities p xly for y l as well as the overall class frequencies p y how do we decide which class a new example x belongs to so as to minimize the overall probability of error the minimum probability of 2 2 error decisions are given by y argmax p x ly p y y o l arg max p ylx y o l the optimal decisions are based on the posterior class probabilit iesp y lx for binary classification problems we can write these decisions as p y l l x y if log p y oxl and y otherwise the optimal decisions are based on the posterior class probabilities p y lx for binary classification problems we can write these decisions as p y l l x y if log p y oxl and y otherwise we generally don t know p ylx but we can parameterize the possible decisions according to p y llx t log w w w py ox our log odds model p y l lx t log p y oxl w x w gives rise to a specific form for the conditional probability over the labels the logistic model p y l l x w a w o xt where a z exp z is a logistic squashing function that turns linear predictions int o probabilit ies z logistic regression models imply a linear decision boundary p y llx wo xt p y olx t o oo o ooo ao o rr i o o l j s o o o o o f o j o o i a s o o cf o d o o o o o oo r od o fl odb qo i oo cxi cxi o a a ao es a o a cb ooo o 0 00 0 0 0 0 rl 0 0 class 0 0 as with the linear regression models we can fit the logistic models using the maximum conditional log likelihood criterion where n l d w logp yi lx i w i l stochastic gradient ascent we can try to maximize the log likelihood in an on line or incremental fashion given each training input xi and the binary 0 label yi we can change the parameters w slightly to increase the corresponding log probability w w rj aw log p yilxi w w tj yi p yi l lx i w prediction error where tj is the learning rate plan for today regression for classification fisher s linear discriminant perceptron logistic regression generative vs discriminative models instead of just two classes we now have c classes e g predict which movie genre a viewer likes best possible answers action drama indie thriller etc two approaches one vs all one vs one one vs all a k a one vs others train c classifiers in each pos data from class i neg data from classes other than i the class with the most confident prediction wins example you have classes train classifiers vs others score 5 2 vs others score 2 vs others score 1 vs other score 5 5 final prediction class 2 issues one vs one a k a all vs all train c c 1 2 binary classifiers all pairs of classes they all vote for the label example you have 4 classes then train classifiers 1 vs 2 1 vs 1 vs 4 2 vs 3 2 vs 4 3 vs 4 votes 1 1 4 2 4 4 final prediction is class 4 what are some problems with this approach to doing multi class there are natively multi class methods figures from bishop single k class classifier fk x wkt x prediction assign x to class ck if fk x fj x for all j k benefit decision regions would not overlap plan for today regression for classification fisher s linear discriminant perceptron logistic regression multi way classification binary case multi class case why are these called generative model probability of the data features x can use them to generate new samples x can model which data points are outliers don t always need to model the probability of the features in svms coming next we don t tradeoffs in discussed methods regression for classification generally not a good idea fisher s linear discriminant finds a discriminative projection of the data can be coupled with a threshold to allow classification perceptron decision boundary corresponds to wt x 0 simple update rule won t converge for non linearly separable data logistic regression a classification method same decision boundary as perceptron models the probability of a label given the data data driven visual similarity for cross domain image matching abhinav shrivastava carnegie mellon university tomasz malisiewicz mit abhinav gupta carnegie mellon university alexei a efros carnegie mellon university figure in this paper we are interested in defining visual similarity between images across different domains such as photos taken in different seasons paintings sketches etc what makes this challenging is that the visual content is only similar on the higher scene level but quite dissimilar on the pixel level here we present an approach that works well across different visual domains abstract the goal of this work is to find visually similar images even if they appear quite different at the raw pixel level this task is particu larly important for matching images across visual domains such as photos taken over different seasons or lighting conditions paint ings hand drawn sketches etc we propose a surprisingly simple method that estimates the relative importance of different features in a query image based on the notion of data driven uniqueness we employ standard tools from discriminative object detection in a novel way yielding a generic approach that does not depend on a particular image representation or a specific visual domain our approach shows good performance on a number of difficult cross domain visual tasks e g matching paintings or sketches to real photographs the method also allows us to demonstrate novel ap plications such as internet re photography and while at present the technique is too computationally intensive to be practical for interactive image retrieval we hope that some of the ideas will eventually become applicable to that domain as well cr categories i artificial intelligence vision and scene understanding learning i image processing and computer vision image representation statistical keywords image matching visual similarity saliency image re trieval paintings sketches re photography visual memex links introduction powered by the availability of internet scale image and video col lections coupled with greater processing speeds the last decade has witnessed the rise of data driven approaches in computer graphics and computational photography unlike traditional methods which employ parametric models to capture visual phenomena the data driven approaches use visual data directly without an explicit inter mediate representation these approaches have shown promising results on a wide range of challenging computer graphics problems including super resolution and de noising freeman et al buades et al hacohen et al texture and video syn thesis efros and freeman schodl et al image analo gies hertzmann et al automatic colorization torralba et al scene and video completion wexler et al hays and efros whyte et al photo restoration dale et al as sembling photo realistic virtual spaces kaneva et al chen et al and even making cg imagery more realistic johnson et al to give but a few examples the central element common to all the above approaches is search ing a large dataset to find visually similar matches to a given query be it an image patch a full image or a spatio temporal block however defining a good visual similarity metric to use for match ing can often be surprisingly difficult granted in many situations where the data is reasonably homogeneous e g different patches within the same texture image efros and freeman or dif ferent frames within the same video schodl et al a simple pixel wise sum of squared differences matching works quite well but what about the cases when the visual content is only sim ilar on the higher scene level but quite dissimilar on the pixel level for instance methods that use scene matching e g hays and efros dale et al often need to match images across different illuminations different seasons different cameras etc likewise retexturing an image in the style of a painting hertzmann et al efros and freeman requires making visual correspon dence between two very different domains photos and paintings cross domain matching is even more critical for applications such as chen et al and johnson et al which aim to bring domains as different as sketches and cg renderings into correspondence with natural photographs in all of these cases pixel wise matching fares quite poorly because small perceptual differences can result in arbitrarily large pixel wise dif ferences what is needed is a visual metric that can capture the important visual structures that make two images appear similar yet show robustness to small unimportant visual details this is precisely what makes this problem so difficult the visual similar ity algorithm somehow needs to know which visual structures are important for a human observer and which are not currently the way researchers address this problem is by using var figure in determining visual similarity the central question is which visual structures are important for a human observer and which are not in the painting above the brush strokes in the sky are as thick as those on the ground yet are perceived as less important in this paper we propose a simple data driven learning method for determining which parts of a given image are more informative for visual matching ious image feature representations sift lowe gist oliva and torralba hog dalal and triggs wavelets etc that aim to capture the locally salient i e high gradient and high contrast parts of the image while downplaying the rest such rep resentations have certainly been very helpful in improving image matching accuracy for a number of applications e g hays and efros kaneva et al dale et al johnson et al however what these features encode are purely local trans formations mapping pixel patches from one feature space into an other independent of the global image content the problem is that the same local feature might be unimportant in one context but cru cially important in another consider for example the painting in figure in local appearance the brush strokes on the alleyway on the ground are virtually the same as the brush strokes on the sky yet the former are clearly much more informative as to the content of the image than the latter and should be given a higher importance when matching figure to do this algorithmically requires not only considering the local features within the context of a given query image but also having a good way of estimating the importance of each feature with respect to the particular scene overall visual impression what we present in this paper is a very simple yet surprisingly effective approach to visual matching which is particularly well suited for matching images across different domains we do not propose any new image descriptors or feature representations in stead given an image represented by some features we will be us ing the spatially rigid hog dalal and triggs descriptor for most of this paper the aim is to focus the matching on the features that are the most visually important for this particular image the central idea is the notion of data driven uniqueness we hypothe size following boiman and irani that the important parts of the image are those that are more unique or rare within the visual world represented here by a large dataset for example in fig ure the towers of the temple are very unique whereas the wispy clouds in the sky are quite common however since the same local features could represent very different visual content depending of context unlike boiman and irani our notion of uniqueness is scene dependent i e each query image decides what is the best way to weight its constituent parts figure demonstrates the dif ference between image matching using a standard uniform feature weighting vs our uniqueness based weighting we operationalize this data driven uniqueness by using ideas from machine learning training a discriminative classifier to discover which parts of an image are most discriminative in relationship to the rest of the dataset this simple approach results in visual matching that is surprisingly versatile and robust by focusing on the globally salient parts of the image the approach can be suc cessfully used for generic cross domain matching without making figure example of image matching using the sift descriptor while sift works very well at matching fine image structure left it fails miserably when there is too much local change such as a change of season right any domain specific changes as shown on figure the rest of the paper is organized as follows we first give a brief overview of the related work section then describe our approach in detail section present an evaluation on several public datasets sec tion and finally show some of the applications that our algorithm makes possible section background in general visual matching approaches can be divided into three broad classes with different techniques tailored for each exact matching for finding more images of the exact same phys ical object e g a pepsi can or scene e g another photo of eif fel tower under similar illumination researchers typically use the general bag of words paradigm introduced by the video google work sivic and zisserman where a large histogram of quan tized local image patches usually encoded with the sift descrip tor lowe is used for image retrieval this paradigm gener ally works extremely well especially for heavily textured objects and has led to many successful applications such as google gog gles however these methods usually fail when tasked with find ing similar but not identical objects e g try using google gog gles app to find a cup or a chair this is because sift being a local descriptor captures the minute details of a particular object well but not its overall global properties as seen in figure approximate matching the task of finding images that are merely visually similar to a query image is significantly more difficult and none of the current approaches can claim to be par ticularly successful most focus on employing various image rep resentations that aim to capture the important salient parts of the image some of the popular ones include the gist oliva and tor ralba descriptor the histogram of gradients hog descrip tor dalal and triggs various other wavelet and gradient based decompositions or agglomerations such as the spatial pyra mid lazebnik et al of visual words also related is the vast field of content based image retrieval cbir see datta et al for overview however in cbir the goals are somewhat different the aim is to retrieve semantically relevant images even if they do not appear to be visually similar e g a steam engine would be considered semantically very similar to a bullet train even though visually there is little in common as a result most modern cbir methods combine visual information with textual annotations and user input cross domain matching a number of methods exists for match ing between particular domains such as sketches to photographs e g chen et al eitz et al drawings paintings to photographs e g russell et al or photos under different illuminants e g chong et al etc however these typically present very domain specific solutions that do not easily generalize across multiple domains of the general solutions the most am bitious is work by shechtman and irani which proposes to describe an image in terms of local self similarity descriptors that are invariant across visual domains this work is complementary to ours since it focuses on the design of a cross domain local descrip tor while we consider relative weighting between the descriptors for a given image so it might be interesting to combine both within the text retrieval community the tf idf normaliza tion baeza yates and ribeiro neto used in the bag of words approaches shares the same goals as our work trying to re weight the different features words in text or visual words in im ages sivic and zisserman based on their relative frequency the main difference is that in tf idf each word is re weighted inde pendently of all the others whereas our method takes the interac tions between all of the features into account most closely related to ours are approaches that try to learn the statistical structure of natural images by using large unlabeled im age sets as a way to define a better visual similarity in the context of image retrieval hoiem et al estimate the un conditional probability density of images off line and use it in a bayesian framework to find close matches tieu and viola use boosting at query time to discriminatively learn query specific features however these systems require multiple positive query images and or user guidance whereas most visual matching tasks that we are interested in need to work automatically and with only a single input image fortunately recent work in visual recognition has shown that it possible to train a discriminative classifier using a single positive instance and a large body of negatives wolf et al malisiewicz et al provided that the negatives do not contain any images similar to the positive instance in this work we adapt this idea to image retrieval where one cannot guarantee that the negative set will not contain images similar to the query on the contrary it most probably will what we show is that surpris ingly this assumption can be relaxed without adversely impacting the performance approach the problem considered in this paper is the following how to com pute visual similarity between images which would be more con sistent with human expectations one way to attack this is by de signing a new more powerful image representation however we believe that existing representations are already sufficiently power ful but that the main difficulty is in developing the right similarity distance function which can pick which parts of the representa tion are most important for matching in our view there are two requirements for a good visual similarity function it has to fo cus on the content of the image the what rather that the style the how e g the images on figure should exhibit high vi sual similarity despite large pixel wise differences it should be scene dependent that is each image should have its own unique similarity function that depends on its global content this is im portant since the same local feature can represent vastly different visual content depending on what else is depicted in the image data driven uniqueness the visual similarity function that we propose is based on the idea of data driven the low ranked last samples which are either highly occluded blurred or not good rep resentatives of side facing class source motorbike target bicycle test set pascal test procedure pascal side only in all the tables test configuration information is given similar to the line above the values for the source and target are the ap scores of the source i e motorbike and full target i e bicycle trained with all available samples detectors on the target task table average precision ap comparison of baseline and transfer svms on the one shot learning task models are learned using one sample of bicycle class and the motorbike clas sifier as the source the top row displays the average ap results using one of the top high ranked samples ranked by the source classifier next rows display the next in the ranking note the tremendous boost obtained by the transfer method compared to the base svm without transfer outperforms pmt svm we conclude that pmt svm is highly sensitive to bad low ranked samples da svm per forms very similarly to a svm for all the samples to our knowledge there is no previous work on how to select or weight samples of the target class while perform ing transfer learning under the assumption that the source class is visually similar to the target class ranking samples with the source detector provides an idea about the qual ity of available samples note the ranking of samples using the source and full target i e trained with all available sam ples classifier has overlap in the top samples and overlap for the last samples this source ranking can help during the sample selection or weighting of the samples for transfer learning since bad samples clearly de teriorate the performance see table and low scored samples either should be removed or at least should be as signed small weights source cow target horse test set pascal test procedure pascal side only table ap comparison of baseline and transfer svms on the one shot learning task models are learned using one sample of horse class and the cow classifier as the source source cow target horse test set pascal test procedure pascal side only a source motorbike target bicycle test set pascal test procedure pascal side only b source horse target bicycle test set pascal test procedure pascal side only c table ap results of baseline svm and model transfer meth ods transfers are performed a from cow to horse b from mo torbike to bicycle and c from horse to bicycle negative transfer leftmost column displays the number of positive samples used for learning in this and all the following tables and figures the ex periments are performed five times with different randomized or derings of the positive samples multiple shot learning for the experiments we use a fixed but random order ing for four learning methods target samples only trans fer from the rigid source template using a svm and pmt svm and transfer from the deformable source template us ing da svm each experiment is repeated times with a different random order the aps are averaged for each number of samples the average removes any idiosyn crasies due to particularly good or bad samples turning up early in the training these fluctuations in ap can be seen in the standard deviations of the results given in table the transition of the learned template is illustrated in figure as is clear from table transfer from the source class using a svm da svm and pmt svm performs signifi cantly better than the baseline svm especially for a small number of positive samples note that the standard devi ations of all three methods are smaller than the baseline svm showing that the idiosyncrasies due to good and bad training samples are better tolerated as the number of positive samples increases the improvements from transfer learning methods over the baseline decreases as can be seen from table pmt svm works better for a small number of samples in table a one sam ple pmt svm appears worse than a svm in fact this is caused by one of the orderings where a bad sample is intro duced when we remove that ordering and average the other orderings for the one sample case pmt svm clearly outperforms a svm with performance to respectively this incident also shows that pmt svm is good for one shot learning but it is highly sensitive to sam ple quality pmt svm also doesn t perform well with large number of samples probably caused by the introduction of bad samples in addition to positive transfer experiments we also compared the transfer methods in a negative transfer case where we learn a bicycle classifier using a horse classifier as the source as can be seen from table c a svm and da svm perform worse than the baseline since horse is not a suitable class for transfer learning of a bicycle class the deterioration of a svm and da svm is expected how ever pmt svm still manages to perform some boost over the baseline svm which shows that this weaker model can transfer at the coarse objectness level as well as another type of baseline for the transfer experiments we include the performance of the source classifier without transfer for detecting the target category this measures the confusion between the two categories as shown in ta ble if more than positive sample is used then the transfer methods outperform the source classifier for the target cat egory detection we also evaluated a svm and da svm using a larger scale of positive samples on the pascal complete test set see table and figure in all the cases a svm and da svm perform better that the baseline svm da svm is superior to a svm except for the samples case in summary the results show a significant improvement through transfer learning in terms of higher start and higher slope refer to figure multiple shot learning with multiple components aspects these experiments are conducted on classifiers trained with multiple components aspects similar to we compare two methods baseline svm a classifier with multiple com source motorbike pascal side only pascal default target bicycle pascal side only pascal default test set pascal complete table ap results of baseline svm and model transfer methods for the bicycle detection task source motorbike target bicycle test set pascal complete test procedure pascal side only table ap results of multiple component baseline svm and a svm for bicycle detection task for a svm the transfer is performed from a multiple component motorbike classifier source motorbike target bicycle test set pascal complete test procedure pascal side only figure ap comparison between baseline svm and model trans fer methods on bicycle detection task ponents for the root filter learned from the positive samples only and a svm a multiple component classifier learned by transferring from a multiple component source classifier the experimental settings are as above using the pascal complete test set except now in training positive train ing samples are selected from all poses not just those of the side views a model with two distinct components corre sponding to four components once mirrored is used for the source and the target classifiers transfer is performed from the motorbike class to bicycle class in the transfer training each positive training sample is assigned to one of the components of the source classifier depending on the score of the sample obtained from the source components then training is performed in a similar fashion to except we use transfer svm training instead of classical svm training similarly to the other transfer experiments transfer methods achieve a good performance improving over the classical svm training particularly for a small number of samples see table the boost gradu ally decreases when we increase the number of samples specialization superior to subordinate cate gory transfer these transfer experiments are conducted on the horse cow and sheep categories of pascal voc a quadruped category detector is trained from randomly selected examples from the horse cow and sheep cate gories it is then specialized to one of those categories by transfer learning using the model transfer methods we omitted pmt svm since it doesn t perform well for a large number of samples the experimental settings and proce dures are the same as multiple shot learning experiments on pascal complete test set table shows that specializing from the quadruped class to a subordinate class using model transfer again gives a sig nificant performance improvement over the baseline svm especially for a small number of positive samples occa sionally using a large number of samples the baseline svm can perform better than transfer methods in our experi ments the transfer parameter γ is fixed decreasing γ when we have large number of samples would solve the problem since our models converges to the classical svm formula tion when γ discussion the benefits of training a superior class quadruped in this case are that the training samples can come from multiple subordinate classes this is similar to the case of attribute training indeed the superior class need not involve any training from the target class however we are restricted here in using pascal voc since there are so few categories it is not possible to train a superior class without also including all subordinate classes neverthe less the benefit of specializing by transfer learning is well demonstrated conclusions and future work almost all object category detection methods to date learn the classifier from scratch tabula rasa we have proposed a straightforward modification of the learning ob jective function which retains the benefits of i convexity ii optimization methods honed to the special structure of an svm and also brings the benefit of learning with fewer training samples the model transfer methods can act as a power boost plug in to any svm training scheme there are a number clear extensions to the model i so far the transfer learning has been applied to the root filter and multiple component scenario of the next step is to extend it to multiple parts ii so far a single feature type has been used but the model can also be extended to multi ple features such as are used in acknowledgements we are very grateful to andrea vedaldi for insightful discussions and the vlfeat li brary financial support was provided by the royal academy of engineering microsoft and erc grant vis rec no big data phuong pham april learning everything about anything webly supervised visual concept learning based on santosh divvala slide horse rearing horse rolling horse reining horse eye horse what are these without human supervision biased non comprehensive concept specific expertise scalability approach overall questions or we will go into detail of each component retrieve concept variations approximately n grams per concept several visually non salient n grams e g last horse particular horse classifier based pruning train test thumbnail images if a p threshold discard n gram find good quality n gram quality coverage merge into s if i j are different not for finding super ngrams merging similar good quality ngrams train separate dpm per super ngram pruning noisy components merging similar appearance clustering pruning noisy components noisy components pruned based on a p threshold and frequency merging similar components as super ngram their system is comparable to better in avg is the best model at the cost of fully supervised learning limitation of annotation the webly learning give reasonable good result advantages cut annotation cost combine linguistic and visual semantic may lead to more interesting research topics disadvantages still not defeat state of the art even with training set at large scale questions scene completion using millions of photographs based on james hays slides input image scene descriptor image collection completions context matching blending matches gist scene descriptor from orientation and scales find its nearest neighbors in million images total context matching graph cut poisson blending hays and efros siggraph for missing regions of the existing image for regions of the image not covered be the scene match for other pixels we assign each of the results a score which is the sum of the scene matching distance the context matching distance color texture the graph cut cost our baseline discussions total recall automatic query expansion with a generative feature model for object retrieval ondˇrej james josef michael and andrew geometry group department of engineering science university of oxford research silicon valley ondra james josef az robots ox ac uk abstract given a query image of an object our objective is to re trieve all instances of that object in a large image database we adopt the bag of visual words architecture which has proven successful in achieving high precision at low recall unfortunately feature detection and quantiza tion are noisy processes and this can result in variation in the particular visual words that appear in different images of the same object leading to missed results in the text retrieval literature a standard method for im proving performance is query expansion a number of the highly ranked documents from the original query are reis sued as a new query in this way additional relevant terms can be added to the query this is a form of blind rele vance feedback and it can fail if outlier false positive documents are included in the reissued query in this paper we bring query expansion into the visual domain via two novel contributions firstly strong spatial constraints between the query image and each result allow us to accurately verify each return suppressing the false positives which typically ruin text based query expansion secondly the verified images can be used to learn a latent feature model to enable the controlled construction of ex panded queries we illustrate these ideas on the annotated im age oxford building database together with more than flickr images we show that the precision is substantially boosted achieving total recall in many cases introduction the leading methods for object retrieval from large im age corpora all rely on variants of the same technique first each image in the corpus is processed to extract features in some high dimensional descriptor space these descriptors are quantized or clustered to map every feature to a visual word in some much smaller discrete figure a sample of challenging results returned by our method in answer to a visual query for the tom tower christ church col lege oxford top left which weren t found by a simple bag of visual words method this query was performed on a large dataset of images vocabulary the corpus is then summarized using an index where each image is represented by the visual words that it contains at query time the system is presented with a query in the form of an image region this region is it self processed to extract feature descriptors that are mapped onto the visual word vocabulary and these words are used to query the index the response set of the query is a set of images from the corpus that contain a large number of visual words in common with the query region these re sponse images may subsequently be ranked using spatial information to ensure that the response and the query not only contain similar features but that the features occur in compatible spatial configurations this procedure can be interpreted probabilistically as follows the system extracts a generative model of an ob ject from the query region then forms the response set from those images in the corpus that are likely to have been gen erated from that model the generative model in this case is a spatial configuration of visual words extracted from the query region together with a background distribution of words that encodes the overall frequency statistics of the corpus in this paper we explore ways to derive better object models given the query region in order to improve retrieval performance we keep the form of the model fixed it is still a configuration of visual words however rather than simply extracting the model from the single input query re gion we enrich it with additional information from the cor pus we refer to this as a latent model of the object this richer model achieves substantially better retrieval perfor mance than the state of the art on the oxford buildings dataset the latent model is a generalization of the idea of query expansion a well known technique from the field of text based information retrieval in text based query ex pansion a number of the high ranked documents from the original response set are used to generate a new query that can be used to obtain a new response set this is a form of blind relevance feedback in that it allows additional relevant terms to be added to the query it is particularly well suited to our problem domain for two reasons first the spatial structure of images allows us to be very robust to false positives in text retrieval relevance feed back attempts to construct a topic model of relevance based on terms in the documents due to the complexities of natural language the relevant terms may be spread arbi trarily throughout the returned documents and the task is complicated by the dramatic changes in meaning that can arise from subtle rearrangement of language terms conse quently there is substantial danger of topic drift where an incorrect model is inferred from the initial result set lead ing to divergence as the process is iterated in the image retrieval case we are greatly assisted by the fact that we can construct a model of a region rather than the whole image and that the image data within the region is very likely to correspond to the object of interest while there may be occlusions obscuring parts of some matching regions it is reasonable to expect them to be independent in different re sponse images simplifying the task of inferring the latent model second the baseline image search without query expan sion suffers more acutely from false negatives than most text retrieval systems because the visual words used to index images are a synthetic projection from a high dimensional descriptor space they suffer from substantial noise and drop outs two very similar image instances of the same object typically have only partial overlap of their visual words especially when the features are sampled sparsely as is common to many systems for performance reasons consequently as we show in section we can substantially improve recall at a given threshold of precision simply by forming the union of features common to a transitive closure of the response images an outline of our approach is as follows given a query region search the corpus and retrieve a set of image regions that match the query object we use bag of visual words retrieval together with spatial verification however the approach would apply to re trieval systems that use different object models combine the retrieved regions along with the original query to form a richer latent model of the object of in terest re query the corpus using this expanded model to re trieve an expanded set of matching regions repeat the process as necessary alternating between model refinement and re querying in the following we briefly outline our implementation of the bag of visual words retrieval in section and spatial verification in section section then describes several al ternative mechanisms for constructing latent models in the iterative framework described above in section the per formance of these mechanisms is assessed on a very chal lenging dataset of over flickr images since our generative model outputs only visual words our system presents the results to the user as a set of match ing image regions from the corpus however as we argue in section there is a natural avenue of extensions to this work that lead toward more complex models that might include detailed intensity or structural information about the object with these more sophisticated models we could imagine re turning a synthesis of the queried object directly rather than a set of matching images real time object retrieval this section overviews our bag of visual words real time object retrieval engine further details can be found in image description for each image in the dataset see sec tion we find multi scale hessian interest points and fit an affine invariant region to each using the semi local second moment matrix on average there are regions detected on an image of size for each of these affine regions we compute dimensional sift descrip tors the number of descriptors generated for each of our datasets is shown in table quantization a visual vocabulary of words is gener ated using an approximate k means clustering method based on randomized trees this produces visual vocabu laries which perform as well as those generated by exact k means at a fraction of the computational cost each visual descriptor is assigned via approximate nearest neighbour search to a single cluster centre giving a standard bag of figure sample of query images used in the ground truth eval uation for all query images see visual words model these quantized visual features are then used to index the images for the search engine search engine our search engine uses the vector space model of information retrieval the query and each doc ument in the corpus is represented as a sparse vector of term visual word occurrences and search then proceeds by calculating the similarity between the query vector and each document vector we use the standard tf idf weighting scheme which down weights the contribution that com monly occurring and therefore less discriminative words make to the relevance score for computational speed the engine stores word occur rences in an index which maps individual words to the doc uments in which they occur for sparse queries this can result in a substantial speedup over examining every doc ument vector as only documents which contain common to the query words need to be examined the scores for each document are accumulated so that they are identical to explicitly computing the similarity with large corpora of images memory usage becomes a major concern to help ameliorate this problem the in verted file is stored in a space efficient binary packed struc ture additionally when main memory is exhausted the en gine can be switched to use an inverted file flattened to disk which caches the data for the most frequently requested words spatial verification the output from performing a query on the inverted file described previously is a ranked list of images for a sig nificant section of the corpus until now we have consid ered the features in each image as a visual bag of words and have ignored their spatial configurations it is vital for query expansion that we do not expand using false positives or use features which occur in the result image but not on the object of interest to achieve this we use a fast ro bust hypothesize and verify procedure to estimate an affine homography between a query region and target image each interest point has an affine invariant semi local re gion associated with it and we use this extra information to hypothesize transformations using single correspondences this makes our procedure both fast the number of hy potheses to test is simply the number of putative correspon dences and deterministic we examine every possible hy pothesis a ransac like scoring mechanism is used to select the hypothesis with the greatest number of inliers each single correspondence hypothesizes a three degree of freedom dof transformation isotropic scale trans lation for a typical query of features with a dis criminative vocabulary the number of correspondences and hence hypotheses to test will be of the order of a few thou sand the number of inliers to this transformation is found using a symmetric transfer error coupled with a scale threshold which prevents mis sized regions from scoring as inliers each hypothesis is stored in a priority queue keyed by the number of inliers for the top hypotheses found we iteratively use a least squares re estimation method on the initially found inliers to generate a full dof affine trans formation returning the best hypothesis as the one with the most inliers after re estimation empirically we find that results with more than inliers reliably contain the object being sought for we call such results spatially veri fied the spatial verification is applied up to a maximum of the top results returned from the search engine at each result a decision is made about whether to proceed with the verification further down the ranked list based on how recently a verified image has been seen if no verified result has been seen in the last ranked images then we stop returning the verified images seen so far empirically we find that increasing this threshold further does not sig nificantly increase the number of positively verified results this prevents us from needlessly verifying images for re sults where all the true positive images have already been seen or from prematurely bailing out of verification when there are more true positives waiting to be found the out put is a list of images ranked in non increasing order of the number of inliers the threshold of inliers is used to produce a list of verified results and their associated trans formations this list of known good results is essential for the query expansion generative model in this section we describe several methods for com puting latent object models these are based on generative models of the features and their configuration with different levels of complexity we account for quantization and de tection noise and the effect of different image resolutions each method starts by evaluating the original query composed of all the visual words which fall inside the query region a latent model is then constructed from the verified images returned from and a new query or several new queries issued this immediately raises two issues i how far should this sequence extend should a new latent model be built from the returns of and another query issued etc ii how should the ranked lists returned from be combined we explore both these questions note that the bag of visual word result set from must be verified against for example cannot be used for verification since we are aiming to obtain images that were not verified against methods the methods can be divided into those that issue a single new query and those that issue multiple queries in the latter case it is necessary to combine the returned ranked lists for each query query expansion baseline this method is a straight for ward na ıve application of query expansion as is used in text retrieval we take the top m results from the origi nal query without spatial verification average the term frequency vectors computed from the entire result image and requery once the results of are appended to those of the top transitive closure expansion a priority queue of verified images is keyed by the number of inliers then an image is taken from the top of the queue and the region correspond ing to the original query region is used to issue a new query verified results of the expanded query that have not been inserted to the queue before are inserted again in the order of the number of inliers the procedure repeats until the queue is empty the images in the final result are in the same order in which they entered the queue average query expansion a new query is constructed by averaging verified results of the original query first the top m verified results returned by the search engine are selected a new query qavg is then formed by taking the average of the original query and the m results recursive average query expansion this method im proves on the average query expansion method by recur sively generating queries qi from all spatially verified re sults returned so far the method stops once more than verified images have been found or after no new images have been positively verified multiple image resolution expansion the generative model in this case also takes account of the probability of observing a feature given an image of an object and its res olution features covering a small area of the object are seen only in close up images or images with high resolu tion similarly features covering the whole object are not seen on detailed views the latent image is constructed as before by back pro jecting verified regions of using the hi transformations the number of pixels of the projected region defines the resolution of each result image an image with median resolution is chosen as a resolution reference image and a relative change of the resolution with respect to the resolu tion reference image is computed for each result image the resolution bands are given by the relative resolution change as and we construct an average query for each of the three different resolution bands using only images that have resolution within that scale band the queries are executed independently and the results are merged verified images from are returned first results from expanded queries follow in order of the number of inliers the maximum is taken if an image is re trieved in more than one resolution band experiments to evaluate our system we use the oxford dataset avail able from this is a relatively small set of images with an extensive associated ground truth we also use two additional unlabeled datasets and which are assumed not to contain images of the ground truth land marks these additional datasets are used as distractors for the system and provide an important test for the scal ability of our method these three datasets are described below and compared in table the set of images down loaded from two or more of flickr tags will not in general be disjoint so we remove exact duplicate images from all davg m di i our datasets the oxford dataset this dataset was crawled from where is the normalized tf vector of the query region and di is the normalized tf vector of the i th result for this average we take the union of features of the original query combined with regions back projected into the query region by hi the estimated transformation this is the simplest form of latent model since no account is taken of the stabil ity of the features or the resolution of the images again we requery once and the results of qavg are appended to those top m of flickr using queries for famous oxford landmarks such as oxford christ church and oxford radcliffe camera it consists of high resolution images ground truth labelling is provided for landmarks with four possible labels as follows good a nice clear picture of the object building ok more than of the object is clearly visible bad the object is not present junk less than of the object is visible or there is a very high level of occlusion or distortion for each table the number of descriptors for each dataset landmark five standard queries are defined for evaluation a sample of query images is shown in figure for the rest see dataset this dataset was crawled from flickr most popular tags and consists of high resolu tion images our search engine can query the combined datasets of oxford and flickr consisting of images in around for a typical query and the index consumes of main memory dataset this dataset consists of medium resolution downloaded from flickr most popular tags the index for the combined oxford and corpus is so we use an offline version of the index which does not have to sit in main memory querying this corpus from disk takes around for a typical query evaluation procedure to evaluate performance we use average precision ap computed as the area under the precision recall curve pre cision is the number of retrieved positive images relative to the total number of images retrieved recall is the number of retrieved positive images relative to the total number of positives in the corpus an ideal precision recall curve has precision over all recall levels which corresponds to an average precision of note a precision recall curve does not have to be monotonically decreasing to illustrate this say there are positives out of the first retrieved which corresponds to precision then if the next image is positive the precision increases to we compute an average precision score for each of the queries for a landmark and then average these to obtain a mean average precision map for the landmark for some experiments in addition to the map we also display precision recall curves which can sometimes better illus trate the success of our system in improving recall in the evaluation the good and ok images are treated as positives bad images as negative and junk images as don t care the don t care images are han dled as if they were not present in the corpus so that if our system returns them the score is not affected we evaluate our system on two databases composed of oxford datasets images and ox ford datasets images the effect of the size of the database on the performance is dis cussed in section figure precision recall curves before left and after right query expansion on experiment these results are for reso lution expansion our best method in each case the five curves correspond to the five queries for that landmark retrieval performance in this section we discuss some quantitative results of our method evaluated against the ground truth gathered from the oxford dataset table summarizes the results of using our different query expansion methods measuring their relative perfor mance in terms of the map score from the table we can table summary of ground truth and the relative performance of the different expansion methods the methods are as follows ori original query qeb query expansion baseline trc transitive closure avg average query expansion rec recursive average query expansion sca resolution expansion the shade of each cell shows relative performance to the worst dark and the best white result for a particular query row ap figure histograms of the average precision for all queries in experiment note that the query expansion moves the mass of the histogram towards the right hand side i e towards total recall figure some false positive images for magdalen tower query the tower shown is actually part of merton college chapel see that all proposed query expansion with the exception of the query expansion baseline methods perform much bet ter than the original bag of visual words method showing a gain in the map from to on and from to on with the best method figure shows selected precision recall curves for the plain bag of words method on the left with the curves from the resolution based query expansion shown on the right in almost all cases the precision recall curve hugs to the right of the graph much more after the query expansion demon strating the method power in dramatically improving the recall of a query additionally in the original bag of words query each individual query for the landmarks shows con siderable variance in the precision recall plots whereas af ter query expansion has been applied in most cases this variance has been reduced improving all the component queries to a similar level of retrieval performance the plot for magdalen in figure shows failure in achieving total recall if the initial results returned from the bag of words method are too bad so that there are no verified images with which to query expand our method is unable to improve the performance this occurs for two of the magdalen queries note that since we are measuring the map over all queries the average result in table is lowered by such non expandable queries to eliminate the averaging effect we study each query independently figure compares two histograms of ap for each of the queries on experiment the top histogram displays results of the original query and the bottom results of the best query expansion method the plot clearly shows the significant improvement brought by the query expansion the performance of the system is hurt by incorrectly ver ified retrievals no verification method is perfect especially when one has to deal with partial occlusions some of the false positives are indeed difficult to distinguish even for a human as demonstrated in figure figure shows some example images returned by our method which were not found in the original bag of words query after query expansion we get many more examples figure demonstrating the performance of the method on a number of different queries the image to the left shows the original query image the four images in the middle show the first four results returned by the original query before query expansion the images to the right show true positive images returned after query expansion which were not found from the bag of words method of the object some of which would be extremely challeng ing to the traditional method with in some cases very high levels of occlusion or large scale changes method comparison we now compare our different query expansion methods referring to table for the relative performances query expansion baseline qeb this method does worse than not using query expansion at all as expected blindly choosing the top m documents for expansion does not take into account whether or not any of the top m are correct so the method suffers from serious drift we can see this by noting that queries which return lots of true positives from the initial query such as radcliffe camera and hert ford perform much better than those with fewer initial true positives such as ashmolean and keble transitive closure trc the method uses a single image to query with each time since both the feature detection and vocabulary generation are noisy processes transitive clo sure has lower performance than methods constructing la tent image representation from several images this method the slowest since it generates by far the highest number of query reissues average query expansion avg this method performs significantly better than just using the results from the stan dard bag of words methods scoring on average as opposed to in the case of additionally the method improves the results for every query in our scoring this method performs so much better mainly because the spatial verification allows us to exclude false positives from results to the original query preventing the drift which ruined the baseline method recursive average query expansion rec this method improves on the avg method by recursively generating and querying the system with spatially verified results by querying recursively we can more thoroughly explore the space of object features giving us instances of the object whose visual appearance can differ greatly from the origi nal query resolution expansion sca the resolution expansion method performs the best on our data by grouping re sults based on the resolution of the object of interest we query expand using only features which reliably fire on the object at a particular resolution this prevents us from in cluding features which fire at different scales which can raise the chance of a false positive image being verified this method gets an map score of on and on and most of the queries exhibit near total re call see figure the percentage is brought down by a few queries which due to the initial bad performance of the bag of words method are unable to be successfully ex panded such queries lie on the left hand side of the lower histogram in figure also note that the merging strategy does not rank all im ages in the database this can be observed on the precision recall figure where the curve does not reach the right side of the plot dataset comparison vs the average precision measure is designed to capture quality of retrieval with strong emphasis on the top ranked results note that additional negative images can only de crease or leave unchanged the average precision measure in the best case if all of the additional negative images were correctly classified they could be appended at the tail of the results which would leave average precision unchanged however correct classification of all images rarely happens our experiments show varying drop of performance after increasing the size of the database negative images times the decrease in performance relative and absolute is lower for query expansion methods than for the original method discussion given the set of retrieved images which often cover a variety of viewpoints we now have the potential to con struct much richer latent feature models of the query re gion much previous work ferrari et al lowe rothganger et al has explored combining features from multiple views and this can now be harnessed for la tent model construction it is also possible to move from features to surfaces where the latent model would consist of a textured surface reconstruction which can be built using standard methods we view image retrieval systems such as video google as one extreme and the photo tourism system as another of examples drawn from a spectrum of possible image based object retrieval techniques the common fea ture unifying this family of methods is that they construct a latent model of the query object with the aid of the image corpus and return to the user some representation of that latent model the work of this paper defines another point on the spectrum acknowledgements we are grateful for support from an epsrc platform grant the royal academy of engineering eu project class and microsoft classification experiments january visual recognition bhavin modi bag of features bag of features outline extract features learn visual vocabulary quantize features using visual vocabulary represent images by frequencies of visual words slide credits li fei fei bag of features summary what about spatial information slide credits cordelia schmid beyond bag of features slide credits li fei fei spatial pyramid matching image representation slide credits li fei fei kernel function histogram intersection function final kernel is the sum of the separate channels spatial pyramid vector dimensions weakness of the model experiments conducted datasets used scene caltech and graz strong features sift descriptors of pixel patches computed over a grid with spacing of pixels weak features oriented edge points i e points whose gradient magnitude in a given direction exceeds a minimum threshold dictionary size and levels are tested for different values m and l not in all cases scene one of the most complete scene categories at the time each category has to images conclusions made using all levels together confers a statistically significant benefit for strong features single level performance drops as we go from l to l while weak features improve performance at l and l is almost equivalent moving from m to m has a very small performance increase performs better with classes than classes at l caltech has geometric stability and lack of clutter contains to images per category slide credits cordelia schmid caltech conclusions prone to intra class variations results shown for m m shows no significant improvement best performance with l m with strong features best classification rate for scene was and it is for caltech graz dataset has object categories bikes and people with heavy clutter and pose changes m l and l for strong features conclusions improvement for l to l is relatively small since it is difficult to find useful global features performance at is higher than scene and caltech new experiments conducted used the caltech dataset categories to check if performance decreases on increasing the number of classes vary the size of dictionary m to see the effects on accuracy values used m and is said to be the optimal control parameters present default shown image size grid spacing patch size dictionary size number of texton images pyramid levels why caltech caltech weaknesses the dataset is too clean images are very uniform in presentation aligned from left to right and usually not occluded limited number of categories some categories contain few images certain categories are not represented as well as others containing as few as images for example binocular wild cat caltech is another image dataset created at the california institute of technology in a successor to caltech it is intended to address some of the weaknesses inherent to caltech 0027 06 077 005 001 001 002 005_0140 0040 0162 slide credits vision caltech edu experiment results dataset caltech multiple categories considered training images per category test images per category l m experiment same as above but categories considered m m m number of categories m dictionary size problems as we can see the accuracy is very low which leads to believe that there is some error in implementation so we try to figure out the reason by performing three debugging steps all debugging is done on the catech dataset for categories m l no of training images per category no of testing images per category accuracy on test set accuracy on train set compute the big kernel using the inbuilt linear kernel and rbf kernel calculating kernel means values debugging results calculating the big kernel accuracy no change using a linear or rbf kernel on the test data and doing a sanity check on the training data calculating the ratio of the mean k sample other samples from same class values and the mean k sample samples from different classes ratio values for both the train and test kernels debugging ii we check the predicted labels on the test set to see the which category was assigned to majority of the images we see category that basketball hoops and drinking straw have more than images assigned to these two categories evaluation on other datasets slide credits cordelia schmid summary discussion spatial pyramid representation appearance of local image patches coarse global position information substantial improvement over bag of features depends on the similarity of image layout future work done packing more information in the pyramid bosch et al used descriptors phow and phog germett et al kernel codebook uses a gaussian kernel over every centroid w every bin gets if descriptor ri is assigned nearest to its centroid w every descriptor contributes some information to every bin depending on σ shengye yan et al beyond spatial pyramid uses a two level feature extraction method using encoding and pooling procedures on the window based features to acquire new image features thank you peekaboom a game for locating objects in images luis von ahn ruoran liu and manuel blum computer science department carnegie mellon university presented by nils murrugarra university of pittsburgh object location in images given an image determine what objects are present in the image and locate them woman man umbrella tree sailboat dog let use human power math is hard let go shopping barbie on similar line of thinking programming computers to locate objects in images is hard so let not think about that instead humans can do the work for us problems wait human probably wants enjoyment they want to have a good time incentives they want something in return how to address them a game people can do the work for us by playing a game many questions appears what will be the core idea of the game how do we collect data how do we ensure the quality of the data an earlier idea luis von ahn esp game core idea two players without communication watch a particular image each one tries to guess what the other is thinking about the image if they agree on a word the game moves on and increases both players scores a sample run player guesses pants lady player guesses woman shirt girl model server agreed model why esp works data collection and quality when two players agrees say what it is in other words this is a label to the shown image the fact that two players agree on a label means that this label has a high quality limitations of esp the esp game can label images what in them but it cannot where the objects are determine the way in which the object appears does the label car refer to the text car or an actual car in the image completing the image cycle unlabeled images esp game server labeled images a new idea peekaboom core idea two players are assigned the roles of revealer boom and guesser peek the revealer sees an image with a label the guesser sees nothing the revealer shows the guesser parts of the image if the guesser guesses correctly the game continues with new images peekaboom interface peek guesser boom revealer statement of purpose the authors would like to collect data of a lot images automatically the authors hope that these data can be used to train computer vision algorithms let do an example the revealer clicks on parts of the image and shows them to the guesser the guesser guesses flower petal butterfly server correct butterfly let play https wgcm feature youtu be t why peekaboom works to help as much as possible the guesser to guess correctly the revealer locates relevant parts of the object in the image but wait there more peekaboom not only locates objects it gives the context necessary to identify them it classifies the image as text noun or verb using the hints option let learn more about these functionalities object context the label nose pings help separate the context of object with the object itself they help the guesser distinguish nose from other possibly correct labels like elephant and ear hints the role of hints how to involve more participants in the game game points game points peek guesses the correct word points are not subtracted for passing peek guesses the correct word and boom had used a hint extra points are not given for usage of the hot cold buttons bonus points obtain up to get points points depend on how far one participant click is from his her partner corresponding click lf the object are not in the image players can pass collecting image metadata data collection data from area revealed which pixels are necessary to guess the word data from hints what is the relation between word and image data from pings which pixels are inside the object data from sequence of boom clicks what are the most relevant aspects of the object data from pass button elimination of poor difficult image word pairs cheating data quality why to worried if the two players cheat on the game the data is not reliable multiple anti cheating mechanisms to avoid match participants that start at the same time the player queue to avoid geographically proximity ip address checks to avoid bots blacklists after consistent failure on seed images to avoid cheating communication limited freedom to enter guesses applications improving image search results object bounding boxes given an image create a matrix of for each click in its surrounding area radius pixels add to the matrix position combine different games for the same image word pair apply a threshold of at least players agree cluster the pixels to get bounding boxes using ping data for pointing select a random ping evaluation is this an effective way to collect data yes game is enjoyable each person played average of images that minutes per person in one month user reviews usage statistics august september people and pieces of data evaluation accuracy of collected data accuracy of bounding boxes are they good compared to bounding boxes collected in a non game setup it was performed in image word nouns pairs given a word four volunteers were asked to draw a bounding box around the object that the word refers to average overlap standard deviation accuracy of pings it was verified if the peekaboom object pointers are indeed inside the objects given a pointer three volunteer determine if it is inside the object or not of the pointers were inside the object referred by the word discussion what are some disadvantages weaknesses of peekaboom can you think of any other applications of peekaboom conclusion peekaboom is an enjoyable game to collect image data achieving low costs one game server data with good quality accurately locate objects in images large quantity of data locate objects in millions of images questions references von ahn l liu r blum m april peekaboom a game for locating objects in images in proceedings of the sigchi conference on human factors in computing systems pp acm slides version of peekaboom a game for locating objects in images source objects in images slides version of peekaboom a game for locating objects in images source slides version of peekaboom a game for locating objects in images source http cgit nutn edu tw cgit pptdl pdf slides version of peekaboom a game for locating objects in images source video human computation source https crowdsourcing annotations for visual object detection hao su jia deng li fei fei computer science department stanford university presented by nils murrugarra university of pittsburgh motivation motivation a large quantity of precise bounding boxes are required to learn good object detectors goal crowd source bounding boxes annotations challenges control the data quality with minimal cost method overview qualification test qualification control good bounding boxes bad bounding boxes method drawing task method drawing task method drawing task method quality verification task good annotation bad annotation method coverage verification task evaluation dataset images were selected over categories on the imagenet database overall quality it was manually inspected of images are completely covered with bounding boxes the remaining are difficult cases are accurate tight as possible overall cost the proposed method is cheaper consensus is more expensive evaluation quality control drawing task acceptance ratio quality verification task it was employed a gold standard validation images acceptance ratio coverage verification task it was employed a gold standard validation images acceptance ratio effectiveness of worker training conclusion it was presented a method that collects bounding boxes annotation using crowdsourcing it is composed by tasks drawing task quality verification task coverage verification task it achieves high quality data with low cost beyond bags of features spatial pyramid matching for recognizing natural scene categories svetlana institute university of illinois cordelia rhoˆne alpes montbonnot france jean normale supe rieure paris france abstract this paper presents a method for recognizing scene cat egories based on approximate global geometric correspon dence this technique works by partitioning the image into increasingly fine sub regions and computing histograms of local features found inside each sub region the result ing spatial pyramid is a simple and computationally effi cient extension of an orderless bag of features image rep resentation and it shows significantly improved perfor mance on challenging scene categorization tasks specifi cally our proposed method exceeds the state of the art on the caltech database and achieves high accuracy on a large database of fifteen natural scene categories the spa tial pyramid framework also offers insights into the success of several recently proposed image descriptions including torralba gist and lowe sift descriptors introduction in this paper we consider the problem of recognizing the semantic category of an image for example we may want to classify a photograph as depicting a scene forest street office etc or as containing a certain object of in terest for such whole image categorization tasks bag of features methods which represent an image as an orderless collection of local features have recently demonstrated im pressive levels of performance however because these methods disregard all information about the spatial layout of the features they have severely limited de scriptive ability in particular they are incapable of captur ing shape or of segmenting an object from its background unfortunately overcoming these limitations to build effec tive structural object descriptions has proven to be quite challenging especially when the recognition system must be made to work in the presence of heavy clutter occlu sion or large viewpoint changes approaches based on generative part models and geometric correspondence search achieve robustness at significant computa tional expense a more efficient approach is to augment a basic bag of features representation with pairwise relations between neighboring local features but existing implemen tations of this idea have yielded inconclusive re sults one other strategy for increasing robustness to geo metric deformations is to increase the level of invariance of local features e g by using affine invariant detectors but a recent large scale evaluation suggests that this strat egy usually does not pay off though we remain sympathetic to the goal of develop ing robust and geometrically invariant structural object rep resentations we propose in this paper to revisit global non invariant representations based on aggregating statis tics of local features over fixed subregions we introduce a kernel based recognition method that works by computing rough geometric correspondence on a global scale using an efficient approximation technique adapted from the pyramid matching scheme of grauman and darrell our method involves repeatedly subdividing the image and computing histograms of local features at increasingly fine resolutions as shown by experiments in section this simple oper ation suffices to significantly improve performance over a basic bag of features representation and even over meth ods based on detailed geometric correspondence previous research has shown that statistical properties of the scene considered in a holistic fashion without any anal ysis of its constituent objects yield a rich set of cues to its semantic category our own experiments confirm that global representations can be surprisingly effective not only for identifying the overall scene but also for categorizing images as containing specific objects even when these ob jects are embedded in heavy clutter and vary significantly in pose and appearance this said we do not advocate the direct use of a global method for object recognition except for very restricted sorts of imagery instead we envision a subordinate role for this method it may be used to capture the gist of an image and to inform the subsequent search for specific objects e g if the image based on its global description is likely to be a highway we have a high probability of finding a car but not a toaster in addition the simplicity and efficiency of our method in combina tion with its tendency to yield unexpectedly high recogni tion rates on challenging data could make it a good base line for calibrating new datasets and for evaluating more sophisticated recognition approaches previous work in computer vision histograms have a long history as a method for image description see e g koen derink and van doorn have generalized histograms to locally orderless images or histogram valued scale spaces i e for each gaussian aperture at a given location and scale the locally orderless image returns the histogram of image features aggregated over that aperture our spatial pyramid approach can be thought of as an alternative for mulation of a locally orderless image where instead of a gaussian scale space of apertures we define a fixed hier archy of rectangular windows koenderink and van doorn have argued persuasively that locally orderless images play an important role in visual perception our retrieval exper iments fig confirm that spatial pyramids can capture perceptually salient features and suggest that locally or derless matching may be a powerful mechanism for esti mating overall perceptual similarity between images it is important to contrast our proposed approach with multiresolution histograms which involve repeatedly subsampling an image and computing a global histogram of pixel values at each new level in other words a mul tiresolution histogram varies the resolution at which the fea tures intensity values are computed but the histogram res olution intensity scale stays fixed we take the opposite subdivision scheme although a regular grid seems to be the most popular implementation choice and what is the right balance between subdividing and disordering the spatial pyramid framework suggests a possible way to address this issue namely the best results may be achieved when multiple resolutions are combined in a principled way it also suggests that the reason for the empirical success of subdivide and disorder techniques is the fact that they ac tually perform approximate geometric matching spatial pyramid matching we first describe the original formulation of pyramid matching and then introduce our application of this framework to create a spatial pyramid image representation pyramid match kernels let x and y be two sets of vectors in a d dimensional feature space grauman and darrell propose pyramid matching to find an approximate correspondence between these two sets informally pyramid matching works by placing a sequence of increasingly coarser grids over the feature space and taking a weighted sum of the number of matches that occur at each level of resolution at any fixed resolution two points are said to match if they fall into the same cell of the grid matches found at finer resolutions are weighted more highly than matches found at coarser resolu tions more specifically let us construct a sequence of grids at resolutions l such that the grid at level has cells along each dimension for a total of d cells let f and hf denote the histograms of x and y at this res olution so that hf i and hf i are the numbers of points approach of fixing the resolution at which the features are x y computed but varying the spatial resolution at which they are aggregated this results in a higher dimensional rep resentation that preserves more information e g an image consisting of thin black and white stripes would retain two modes at every level of a spatial pyramid whereas it would become indistinguishable from a uniformly gray image at all but the finest levels of a multiresolution histogram fi nally unlike a multiresolution histogram a spatial pyramid when equipped with an appropriate kernel can be used for from x and y that fall into the ith cell of the grid then the number of matches at level is given by the histogram intersection function f f f f x y x y i in the following we will abbreviate i hf hf to if the operation of subdivide and disorder i e par tition the image into subblocks and compute histograms or histogram statistics such as means of local features in these subblocks has been practiced numerous times in computer vision both for global image description and for local description of interest regions thus though the operation itself seems fundamental pre vious methods leave open the question of what is the right note that the number of matches found at level also in cludes all the matches found at the finer level there fore the number of new matches found at level is given by f f for l the weight associated with level is set to which is inversely proportional to cell width at that level intuitively we want to penalize matches found in larger cells because they involve increas ingly dissimilar features putting all the pieces together we get the following definition of a pyramid match kernel l level level level κl x y i if if f l if both the histogram intersection and the pyramid match ker nel are mercer kernels spatial matching scheme as introduced in a pyramid match kernel works with an orderless image representation it allows for pre cise matching of two collections of features in a high dimensional appearance space but discards all spatial in formation this paper advocates an orthogonal approach perform pyramid matching in the two dimensional image space and use traditional clustering techniques in feature space specifically we quantize all feature vectors into m discrete types and make the simplifying assumption that only features of the same type can be matched to one an other each channel m gives us two sets of two dimensional vectors xm and ym representing the coordinates of fea tures of type m found in the respective images the final kernel is then the sum of the separate channel kernels m kl x y κl xm ym m this approach has the advantage of maintaining continuity with the popular visual vocabulary paradigm in fact it reduces to a standard bag of features when l because the pyramid match kernel is simply a weighted sum of histogram intersections and because c min a b min ca cb for positive numbers we can implement kl as a single histogram intersection of long vectors formed by concatenating the appropriately weighted histograms of all channels at all resolutions fig for l levels and m channels the resulting vector has dimen sionality m l m several experi figure toy example of constructing a three level pyramid the image has three feature types indicated by circles diamonds and crosses at the top we subdivide the image at three different lev els of resolution next for each level of resolution and each chan nel we count the features that fall in each spatial bin finally we weight each spatial histogram according to eq the final implementation issue is that of normalization for maximum computational efficiency we normalize all histograms by the total weight of all features in the image in effect forcing the total number of features in all images to be the same because we use a dense feature representation see section and thus do not need to worry about spuri ous feature detections resulting from clutter this practice is sufficient to deal with the effects of variable image size feature extraction this section briefly describes the two kinds of features used in the experiments of section first we have so called weak features which are oriented edge points i e points whose gradient magnitude in a given direction ex ceeds a minimum threshold we extract edge points at two scales and eight orientations for a total of m chan nels we designed these features to obtain a representation similar to the gist or to a global sift descriptor of the image for better discriminative power we also utilize higher dimensional strong features which are sift descriptors of pixel patches computed over a grid with spacing ments reported in section use the settings of m and l resulting in dimensional histogram in tersections however these operations are efficient because the histogram vectors are extremely sparse in fact just as in the computational complexity of the kernel is linear in the number of features it must also be noted that we did not observe any significant increase in performance beyond m and l where the concatenated histograms are only dimensional principle it is possible to integrate geometric information directly into the original pyramid matching framework by treating image coordi nates as two extra dimensions in the feature space of pixels our decision to use a dense regular grid in stead of interest points was based on the comparative evalu ation of fei fei and perona who have shown that dense features work better for scene classification intuitively a dense image description is necessary to capture uniform re gions such as sky calm water or road surface to deal with low contrast regions we skip the usual sift normalization procedure when the overall gradient magnitude of the patch is too weak we perform k means clustering of a random subset of patches from the training set to form a visual vo cabulary typical vocabulary sizes for our experiments are m and m office kitchen living room bedroom store industrial tall building inside city street highway coast open country mountain forest suburb figure example images from the scene category database the starred categories originate from oliva and torralba table classification results for the scene category database see text the highest results for each kind of feature are shown in bold experiments in this section we report results on three diverse datasets fifteen scene categories caltech and graz we perform all processing in grayscale even when color images are available all experiments are re peated ten times with different randomly selected training and test images and the average of per class recognition is recorded for each run the final result is reported as the mean and standard deviation of the results from the in dividual runs multi class classification is done with a sup port vector machine svm trained using the one versus all rule a classifier is learned to separate each class from the rest and a test image is assigned the label of the classifier with the highest response alternative performance measure the percentage of all test im ages classified correctly can be biased if test set sizes for different classes vary significantly this is especially true of the caltech dataset where some of the easiest classes are disproportionately large scene category recognition our first dataset fig is composed of fifteen scene cat egories thirteen were provided by fei fei and perona eight of these were originally collected by oliva and tor ralba and two industrial and store were collected by ourselves each category has to images and av erage image size is pixels the major sources of the pictures in the dataset include the corel collection personal photographs and google image search this is one of the most complete scene category dataset used in the literature thus far table shows detailed results of classification experi ments using images per class for training and the rest for testing the same setup as first let us examine the performance of strong features for l and m corresponding to a standard bag of features our classi fication rate is for the classes inherited from fei fei and perona which is much higher than their best results of achieved with an orderless method and a feature set comparable to ours we conjecture that fei fei and perona approach is disadvantaged by its re office kitchen living room bedroom store industrial tall building inside city street highway coast open country mountain forest suburb figure confusion table for the scene category dataset average classification rates for individual classes are listed along the diag onal the entry in the ith row and jth column is the percentage of images from class i that were misidentified as class j liance on latent dirichlet allocation lda which is essentially an unsupervised dimensionality reduction tech nique and as such is not necessarily conducive to achiev ing the highest classification accuracy to verify this we have experimented with probabilistic latent semantic analy sis plsa which attempts to explain the distribution of features in the image as a mixture of a few scene topics or aspects and performs very similarly to lda in prac tice following the scheme of quelhas et al we run plsa in an unsupervised setting to learn a aspect model of half the training images next we apply this model to the other half to obtain probabilities of topics given each image thus reducing the dimensionality of the feature space from to finally we train the svm on these reduced features and use them to classify the test set in this setup our average classification rate drops to from the original for the classes inherited from fei fei and perona it drops to from which is now very similar to their results thus we can see that la tent factor analysis techniques can adversely affect classifi cation performance which is also consistent with the results of quelhas et al next let us examine the behavior of spatial pyramid matching for completeness table lists the performance achieved using just the highest level of the pyramid the single level columns as well as the performance of the complete matching scheme using multiple levels the pyra mid columns for all three kinds of features results im prove dramatically as we go from l to a multi level setup though matching at the highest pyramid level seems to account for most of the improvement using all the levels together confers a statistically significant benefit for strong features single level performance actually drops as we go from l to l this means that the highest level of the l pyramid is too finely subdivided with individ ual bins yielding too few matches despite the diminished discriminative power of the highest level the performance of the entire l pyramid remains essentially identical to that of the l pyramid this then is the main advantage of the spatial pyramid representation because it combines multiple resolutions in a principled fashion it is robust to failures at individual levels it is also interesting to compare performance of differ ent feature sets as expected weak features do not per form as well as strong features though in combination with the spatial pyramid they can also achieve acceptable levels of accuracy note that because weak features have a much higher density and much smaller spatial extent than strong features their performance continues to improve as we go from l to l increasing the visual vocabulary size from m to m results in a small perfor mance increase at l but this difference is all but elim inated at higher pyramid levels thus we can conclude that the coarse grained geometric cues provided by the pyramid have more discriminative power than an enlarged visual vo cabulary of course the optimal way to exploit structure both in the image and in the feature space may be to com bine them in a unified multiresolution framework this is subject for future research fig shows a confusion table between the fifteen scene categories not surprisingly confusion occurs between the indoor classes kitchen bedroom living room and also be tween some natural classes such as coast and open country fig shows examples of image retrieval using the spatial pyramid kernel and strong features with m these examples give a sense of the kind of visual information cap tured by our approach in particular spatial pyramids seem successful at capturing the organization of major pictorial elements or blobs and the directionality of dominant lines and edges because the pyramid is based on features com puted at the original image resolution even high frequency details can be preserved for example query image b shows white kitchen cabinet doors with dark borders three of the retrieved kitchen images contain similar cabinets the office image shows a wall plastered with white docu ments in dark frames and the inside city image shows a white building with darker window frames caltech our second set of experiments is on the caltech database fig this database contains from to images per category most images are medium resolu tion i e about pixels caltech is probably the most diverse object database available today though it kitchen living room living room living room office living room living room living room living room kitchen office inside city store mountain forest tall bldg inside city inside city tall bldg inside city mountain mountain mountain inside city tall bldg street figure retrieval from the scene category database the query images are on the left and the eight images giving the highest values of the spatial pyramid kernel for l m are on the right the actual class of incorrectly retrieved images is listed below them is not without shortcomings namely most images feature relatively little clutter and the objects are centered and oc cupy most of the image in addition a number of categories such as minaret see fig are affected by corner arti facts resulting from artificial image rotation though these artifacts are semantically irrelevant they can provide stable cues resulting in misleadingly high recognition rates we follow the experimental setup of grauman and dar rell and j zhang et al namely we train on im ages per class and test on the rest for efficiency we limit the number of test images to per class note that be cause some categories are very small we may end up with just a single test image per class table gives a break down of classification rates for different pyramid levels for weak features and strong features with m the results for m are not shown because just as for the scene category database they do not bring any signifi cant improvement for l strong features give which is slightly below the reported by grauman and darrell our best result is achieved with strong fea tures at l this exceeds the highest classification rate previously published that of reported by j zhang et al berg et al report accuracy using training images per class our average recognition rate with this setup is the behavior of weak features on this database is also noteworthy for l they give a clas sification rate of which is consistent with a naive graylevel correlation baseline but in conjunction with a four level spatial pyramid their performance rises to on par with the best results in the literature fig shows a few of the easiest and hardest object classes for our method the successful classes are either dominated by rotation artifacts like minaret have very lit tle clutter like windsor chair or represent coherent natural scenes like joshua tree and okapi the least success ful classes are either textureless animals like beaver and cougar animals that camouflage well in their environment however h zhang et al in these proceedings for an al gorithm that yields a classification rate of for training examples and for examples minaret windsor chair joshua tree okapi cougar body beaver crocodile ant figure caltech results top some classes on which our method l m achieved high performance bottom some classes on which our method performed poorly table classification results for the caltech database table top five confusions for our method l m on the caltech database table results of our method m for the graz database and comparison with two existing methods like crocodile or thin objects like ant table shows the top five of our method confusions all of which are between closely related classes to summarize our method has outperformed both state of the art orderless methods and methods based on precise geometric correspondence significantly all these methods rely on sparse features interest points or sparsely sampled edge points however because of the geometric stability and lack of clutter of caltech dense features combined with global spatial relations seem to cap ture more discriminative information about the objects the graz dataset as seen from sections and our proposed ap proach does very well on global scene classification tasks or on object recognition tasks in the absence of clutter with most of the objects assuming canonical poses however it was not designed to cope with heavy clutter and pose changes it is interesting to see how well our algorithm can do by exploiting the global scene cues that still remain under these conditions accordingly our final set of ex periments is on the graz dataset fig which is characterized by high intra class variation this dataset has two object classes bikes images and persons im ages and a background class images the image res olution is and the range of scales and poses at which exemplars are presented is very diverse e g a per son image may show a pedestrian in the distance a side view of a complete body or just a closeup of a head for this database we perform two class detection object vs back ground using an experimental setup consistent with that of opelt et al namely we train detectors for persons and bikes on positive and negative images of which are drawn from the other object class and from the back ground and test on a similarly distributed set we generate roc curves by thresholding raw svm output and report the roc equal error rate averaged over ten runs table summarizes our results for strong features with m note that the standard deviation is quite high be cause the images in the database vary greatly in their level of difficulty so the performance for any single run is depen dent on the composition of the training set in particular for l the performance for bikes ranges from to for this database the improvement from l to l is relatively small this makes intuitive sense when a class is characterized by high geometric variability it is difficult to find useful global features despite this disadvantage of our method we still achieve results very close to those of opelt et al who use a sparse locally invariant feature representation in the future we plan to combine spatial pyramids with invariant features for improved robustness against geometric changes discussion this paper has presented a holistic approach for image categorization based on a modification of pyramid match kernels our method which works by repeatedly sub dividing an image and computing histograms of image fea tures over the resulting subregions has shown promising re bike person background figure the graz database sults on three large scale diverse datasets despite the sim plicity of our method and despite the fact that it works not by constructing explicit object models but by using global cues as indirect evidence about the presence of an object it consistently achieves an improvement over an orderless image representation this is not a trivial accomplishment given that a well designed bag of features method can out perform more sophisticated approaches based on parts and relations our results also underscore the surprising and ubiquitous power of global scene statistics even in highly variable datasets such as graz they can still provide useful discriminative information it is important to develop methods that take full advantage of this information ei ther as stand alone scene categorizers as context mod ules within larger object recognition systems or as tools for evaluating biases present in newly collected datasets histograms of oriented gradients for human detection navneet dalal and bill triggs inria rhoˆne alps avenue de l europe montbonnot france navneet dalal bill triggs inrialpes fr abstract we study the question of feature sets for robust visual ob ject recognition adopting linear svm based human detec tion as a test case after reviewing existing edge and gra dient based descriptors we show experimentally that grids of histograms of oriented gradient hog descriptors sig nificantly outperform existing feature sets for human detec tion we study the influence of each stage of the computation on performance concluding that fine scale gradients fine orientation binning relatively coarse spatial binning and high quality local contrast normalization in overlapping de scriptor blocks are all important for good results the new approach gives near perfect separation on the original mit pedestrian database so we introduce a more challenging dataset containing over annotated human images with a large range of pose variations and backgrounds introduction detecting humans in images is a challenging task owing to their variable appearance and the wide range of poses that they can adopt the first need is a robust feature set that allows the human form to be discriminated cleanly even in cluttered backgrounds under difficult illumination we study the issue of feature sets for human detection showing that lo cally normalized histogram of oriented gradient hog de scriptors provide excellent performance relative to other ex isting feature sets including wavelets the proposed descriptors are reminiscent of edge orientation histograms sift descriptors and shape contexts but they are computed on a dense grid of uniformly spaced cells and they use overlapping local contrast normalizations for im proved performance we make a detailed study of the effects of various implementation choices on detector performance taking pedestrian detection the detection of mostly visible people in more or less upright poses as a test case for sim plicity and speed we use linear svm as a baseline classifier throughout the study the new detectors give essentially per fect results on the mit pedestrian test set so we have created a more challenging set containing over pedes trian images with a large range of poses and backgrounds ongoing work suggests that our feature set performs equally well for other shape based object classes we briefly discuss previous work on human detection in give an overview of our method describe our data sets in and give a detailed description and experimental evaluation of each stage of the process in the main conclusions are summarized in previous work there is an extensive literature on object detection but here we mention just a few relevant papers on human detec tion see for a survey papageorgiou et al describe a pedestrian detector based on a polynomial svm using rectified haar wavelets as input descriptors with a parts subwindow based variant in depoortere et al give an optimized version of this gavrila philomen take a more direct approach extracting edge images and matching them to a set of learned exemplars using chamfer distance this has been used in a practical real time pedes trian detection system viola et al build an efficient moving person detector using adaboost to train a chain of progressively more complex region rejection rules based on haar like wavelets and space time differences ronfard et al build an articulated body detector by incorporating svm based limb classifiers over and order gaussian filters in a dynamic programming framework similar to those of felzenszwalb huttenlocher and ioffe forsyth mikolajczyk et al use combinations of orientation position histograms with binary thresholdedgradient magni tudes to build a parts based method containing detectors for faces heads and front and side profiles of upper and lower body parts in contrast our detector uses a simpler archi tecture with a single detection window but appears to give significantly higher performance on pedestrian images overview of the method this section gives an overview of our feature extraction chain which is summarized in fig implementation details are postponed until the method is based on evaluating well normalized local histograms of image gradient orienta tions in a dense grid similar features have seen increasing use over the past decade the basic idea is that local object appearance and shape can often be characterized rather well by the distribution of local intensity gradients or input image person non person classification figure an overview of our feature extraction and object detection chain the detector window is tiled with a grid of overlapping blocks in which histogram of oriented gradient feature vectors are extracted the combined vectors are fed to a linear svm for object non object classification the detection window is scanned across the image at all positions and scales and conventional non maximum suppression is run on the output pyramid to detect object instances but this paper concentrates on the feature extraction process edge directions even without precise knowledge of the cor responding gradient or edge positions in practice this is im plemented by dividing the image window into small spatial regions cells for each cell accumulating a local d his togram of gradient directions or edge orientations over the pixels of the cell the combined histogram entries form the representation for better invariance to illumination shad owing etc it is also useful to contrast normalize the local responses before using them this can be done by accumu lating a measure of local histogram energy over somewhat larger spatial regions blocks and using the results to nor malize all of the cells in the block we will refer to the nor malized descriptor blocks as histogram of oriented gradi ent hog descriptors tiling the detection window with a dense in fact overlapping grid of hog descriptors and using the combined feature vector in a conventional svm based window classifier gives our human detection chain see fig the use of orientation histograms has many precursors but it only reached maturity when combined with local spatial histogramming and normalization in lowe scale invariant feature transformation sift approach to wide baseline image matching in which it provides the underlying image patch descriptor for matching scale invariant keypoints sift style approaches perform remark ably well in this application the shape context work studied alternative cell and block shapes albeit ini tially using only edge pixel counts without the orientation histogramming that makes the representation so effective the success of these sparse feature based representations has somewhat overshadowed the power and simplicity of hog as dense image descriptors we hope that our study will help to rectify this in particular our informal experiments sug gest that even the best current keypoint based approaches are likely to have false positive rates at least orders of mag nitude higher than our dense grid approach for human detec tion mainly because none of the keypoint detectors that we are aware of detect human body structures reliably the hog sift representation has several advantages it captures edge or gradient structure that is very characteristic of local shape and it does so in a local representation with coarse spatial sampling fine orientation sampling and strong local photometric normalization turns out to be the best strat egy presumably because it permits limbs and body segments to change appearance and move from side to side quite a lot provided that they maintain a roughly upright orientation data sets and methodology datasets we tested our detector on two different data sets the first is the well established mit pedestrian database containing training and test images of pedestri ans in city scenes plus left right reflections of these it con tains only front or back views with a relatively limited range of poses our best detectors give essentially perfect results on this data set so we produced a new and significantly more challenging data set inria containing im ages of humans cropped from a varied set of personal pho tos fig shows some samples the people are usually standing but appear in any orientation and against a wide variety of background image including crowds many are bystanders taken from the image backgrounds so there is no particular bias on their pose the database is available from for research purposes methodology we selected of the images as positive training examples together with their left right reflections images in all a fixed set of patches sampled randomly from person free training photos provided the initial negative set for each detector and parameter com bination a preliminary detector is trained and the nega tive training photos are searched exhaustively for false posi tives hard examples the method is then re trained using this augmented set initial hard examples to pro duce the final detector the set of hard examples is subsam pled if necessary so that the descriptors of the final training set fit into gb of ram for svm training this retrain ing process significantly improves the performance of each detector by at false positives per window tested fppw for our default detector but additional rounds of retraining make little difference so we do not use them to quantify detector performance we plot detection er ror tradeoff det curves on a log log scale i e miss rate an easily controllable degree of invariance to local geometric falseneg truepos falseneg versus fppw lower val and photometric transformations translations or rotations make little difference if they are much smaller that the local spatial or orientation bin size for human detection rather ues are better det plots are used extensively in speech and in nist evaluations they present the same information as receiver operating characteristics roc but allow small figure some sample images from our new human detection database the subjects are always upright but with some partial occlusions and a wide range of variations in pose appearance clothing illumination and background probabilities to be distinguished more easily we will often use miss rate at as a reference point for results this is arbitrary but no more so than e g area under roc in a multiscale detector it corresponds to a raw error rate of about false positives per image tested the full detector has an even lower false positive rate owing to non maximum suppression our det curves are usually quite shallow so even very small improvements in miss rate are equivalent to large gains in fppw at constant miss rate for example for our default detector at fppw every absolute relative reduction in miss rate is equivalent to reducing the fppw at constant miss rate by a factor of overview of results before presenting our detailed implementation and per formance analysis we compare the overall performance of our final hog detectors with that of some other existing methods detectors based on rectangular r hog or cir cular log polar c hog blocks and linear or kernel svm are compared with our implementations of the haar wavelet pca sift and shape context approaches briefly these ap proaches are as follows generalized haar wavelets this is an extended set of ori ented haar like wavelets similar to but better than that used in the features are rectified responses from and oriented and derivative box filters at inter strength and edge presence based voting were tested with the edge threshold chosen automatically to maximize detec tion performance the values selected were somewhat vari able in the region of graylevels results fig shows the performance of the various detec tors on the mit and inria data sets the hog based de tectors greatly outperform the wavelet pca sift and shape context ones giving near perfect separation on the mit test set and at least an order of magnitude reduction in fppw on the inria one our haar like wavelets outperform mit wavelets because we also use order derivatives and con trast normalize the output vector fig a also shows mit best parts based and monolithic detectors the points are in terpolated from however beware that an exact compar ison is not possible as we do not know how the database in was divided into training and test parts and the nega tive images used are not available the performances of the final rectangular r hog and circular c hog detectors are very similar with c hog having the slight edge aug menting r hog with primitive bar detectors oriented derivatives hog doubles the feature dimension but further improves the performance by at fppw replacing the linear svm with a gaussian kernel one im proves performance by about at fppw at the cost of much higher run using binary edge voting ec hog instead of gradient magnitude weighted voting c vals and the corresponding derivative xy filter hog decreases performance by at fppw while omitting orientation information decreases it by much more pca sift these descriptors are based on projecting gradi ent images onto a basis learned from training images using pca ke sukthankar found that they outperformed sift for key point based matching but this is controversial our implementation uses blocks with the same derivative scale overlap etc settings as our hog descrip tors the pca basis is calculated using positive training im ages shape contexts the original shape contexts used bi nary edge presence voting into log polar spaced bins irre spective of edge orientation we simulate this using our c hog descriptor see below with just orientation bin angular and radial intervals with inner radius pixels and outer radius pixels gave the best results both gradient even if additional spatial or radial bins are added by at fppw for both edges e shapec and gradients g shapec pca sift also performs poorly one reason is that in comparison to many more of principal vectors have to be retained to capture the same proportion of the variance this may be because the spatial registration is weaker when there is no keypoint detector implementation and performance study we now give details of our hog implementations and systematically study the effects of the various choices on de use the hard examples generated by linear r hog to train the ker nel r hog detector as kernel r hog generates so few false positives that its hard example set is too sparse to improve the generalization significantly det different descriptors on mit database det different descriptors on inria database false positives per window fppw false positives per window fppw figure the performance of selected detectors on left mit and right inria data sets see the text for details tector performance throughout this section we refer results to our default detector which has the following properties described below rgb colour space with no gamma cor rection gradient filter with no smoothing linear gradient voting into orientation bins in tive masks several smoothing scales were tested includ ing σ none masks tested included various d point derivatives uncentred centred and cubic corrected as well as sobel masks and diagonal ones the most compact cen pixel blocks of four pixel cells gaussian spatial win dow with σ pixel hys lowe style clipped norm block normalization block spacing stride of pixels hence fold coverage of each cell detection window linear svm classifier fig summarizes the effects of the various hog param eters on overall detection performance these will be exam ined in detail below the main conclusions are that for good performance one should use fine scale derivatives essen tially no smoothing many orientation bins and moderately sized strongly normalized overlapping descriptor blocks gamma colour normalization we evaluated several input pixel representations includ ing grayscale rgb and lab colour spaces optionally with power law gamma equalization these normalizations have only a modest effect on performance perhaps because the subsequent descriptor normalization achieves similar results we do use colour information when available rgb and lab colour spaces give comparable results but restricting to grayscale reduces performance by at fppw square root gamma compression of each colour channel im proves performance at low fppw by at fppw but log compression is too strong and worsens it by at fppw gradient computation detector performance is sensitive to the way in which gradients are computed but the simplest scheme turns out to be the best we tested gradients computed using gaus sian smoothing followed by one of several discrete deriva tred d derivative masks simple d masks at σ work best using larger masks always seems to de crease performance and smoothing damages it significantly for gaussian derivatives moving from σ to σ reduces the recall rate from to at fppw at σ cubic corrected d width filters are about worse than at fppw while the diagonal masks are worse using uncentred derivative masks also decreases performance by at fppw presum ably because orientation estimation suffers as a result of the x and y filters being based at different centres for colour images we calculate separate gradients for each colour channel and take the one with the largest norm as the pixel gradient vector spatial orientation binning the next step is the fundamental nonlinearity of the de scriptor each pixel calculates a weighted vote for an edge orientation histogram channel based on the orientation of the gradient element centred on it and the votes are accumu lated into orientation bins over local spatial regions that we call cells cells can be either rectangular or radial log polar sectors the orientation bins are evenly spaced over unsigned gradient or signed gradient to reduce aliasing votes are interpolated bilinearly between the neighbouring bin centres in both orientation and posi tion the vote is a function of the gradient magnitude at the pixel either the magnitude itself its square its square root or a clipped form of the magnitude representing soft pres ence absence of an edge at the pixel in practice using the det effect of gradient scale det effect of number of orientation bins det effect of normalization methods false positives per window fppw false positives per window fppw false positives per window fppw b c det effect of overlap cell size num cell wt false positives per window fppw det effect of window size false positives per window fppw det effect of kernel width on kernel svm false positives per window fppw e f figure for details see the text a using fine derivative scale significantly increases the performance c cor is the cubic corrected point derivative b increasing the number of orientation bins increases performance significantly up to about bins spaced over c the effect of different block normalization schemes see d using overlapping descriptor blocks decreases the miss rate by around e reducing the pixel margin around the detection window decreases the performance by about f using a gaussian kernel svm exp γ improves the performance by about magnitude itself gives the best results taking the square root reduces performance slightly while using binary edge pres ence voting decreases it significantly by at fppw fine orientation coding turns out to be essential for good performance whereas see below spatial binning can be rather coarse as fig b shows increasing the number of orientation bins improves performance significantly up to about bins but makes little difference beyond this this is for bins spaced over i e the sign of the gradi ent is ignored including signed gradients orientation range as in the original sift descriptor decreases the performance even when the number of bins is also doubled 3x3 to preserve the original orientation resolution for humans cell size pixels block size cells the wide range of clothing and background colours presum ably makes the signs of contrasts uninformative however note that including sign information does help substantially in some other object recognition tasks e g cars motorbikes normalization and descriptor blocks gradient strengths vary over a wide range owing to local variations in illumination and foreground background con trast so effective local contrast normalization turns out to be essential for good performance we evaluated a num figure the miss rate at fppw as the cell and block sizes change the stride block overlap is fixed at half of the block size blocks of pixel cells perform best with miss rate ber of different normalization schemes most of them are based on grouping cells into larger spatial blocks and con trast normalizing each block separately the final descriptor is then the vector of all components of the normalized cell responses from all of the blocks in the detection window in fact we typically overlap the blocks so that each scalar cell response contributes several components to the final de scriptor vector each normalized with respect to a different block this may seem redundant but good normalization is critical and including overlap significantly improves the per formance fig d shows that performance increases by at fppw as we increase the overlap from none stride to fold area fold linear coverage stride we evaluated two classes of block geometries square or rectangular ones partitioned into grids of square or rectangu lar spatial cells and circular blocks partitioned into cells in log polar fashion we will refer to these two arrangements as r hog and c hog for rectangular and circular hog r hog r hog blocks have many similarities to sift de scriptors but they are used quite differently they are computed in dense grids at a single scale without dominant orientation alignment and used as part of a larger code vector that implicitly encodes spatial position relative to the detec tion window whereas sift are computed at a sparse set of scale invariant key points rotated to align their dominant orientations and used individually sift are optimized for sparse wide baseline matching r hog for dense robust coding of spatial form other precursors include the edge orientation histograms of freeman roth we usually use square r hog i e ς ς grids of η η pixel cells each containing β orientation bins where ς η β are parameters fig plots the miss rate at fppw w r t cell size and block size in cells for human detection cell blocks of pixel cells perform best with miss rate at fppw in fact pixel wide cells do best irrespec tive of the block size an interesting coincidence as human limbs are about pixels across in our images and blocks work best beyond this the results deteriorate adaptivity to local imaging conditions is weakened when the block becomes too big and when it is too small block normalization over orientations alone valuable spatial in formation is suppressed as in it is useful to downweight pixels near the edges of the block by applying a gaussian spatial window to each pixel before accumulating orientation votes into cells this improves performance by at fppw for a gaussian c hog our circular block c hog descriptors are rem iniscent of shape contexts except that crucially each spatial cell contains a stack of gradient weighted orienta tion cells instead of a single orientation independent edge presence count the log polar grid was originally suggested by the idea that it would allow fine coding of nearby struc ture to be combined with coarser coding of wider context and the fact that the transformation from the visual field to the cortex in primates is logarithmic however small descriptors with very few radial bins turn out to give the best performance so in practice there is little inhomogeneity or context it is probably better to think of c hog simply as an advanced form of centre surround coding we evaluated two variants of the c hog geometry ones with a single circular central cell similar to the gloh feature of and ones whose cen tral cell is divided into angular sectors as in shape contexts we present results only for the circular centre variants as these have fewer spatial cells than the divided centre ones and give the same per formance in practice a technical report will provide fur ther details the c hog layout has four parameters the numbers of angular and radial bins the radius of the central bin in pixels and the expansion factor for subsequent radii at least two radial bins a centre and a surround and four angular bins quartering are needed for good performance including additional radial bins does not change the perfor mance much while increasing the number of angular bins decreases performance by at fppw when go ing from to angular bins pixels is the best radius for the central bin but and give similar results increas ing the expansion factor from to leaves the performance essentially unchanged with these parameters neither gaus sian spatial weighting nor inverse weighting of cell votes by cell area changes the performance but combining these two reduces slightly these values assume fine orientation sam pling shape contexts orientation bin require much finer spatial subdivision to work well block normalization schemes we evaluated four differ ent block normalization schemes for each of the above hog geometries let v be the unnormalized descriptor vector lvlk be its k norm for k and e be a small constant with σ block width the schemes are a norm v v b we also tried including multiple block types with differ hys norm followed by clipping limiting the maxi ent cell and block sizes in the overall descriptor this slightly mum values of v to and renormalizing as in c improves performance by around at fppw at the norm v v e and d sqrt norm fol cost of greatly increased descriptor size besides square r hog blocks we also tested vertical cell and horizontal cell blocks and a combined descriptor including both vertical and horizontal pairs verti cal and vertical horizontal pairs are significantly better than horizontal pairs alone but not as good as blocks worse at fppw lowed by square root v v e which amounts to treating the descriptor vectors as probability distributions and using the bhattacharya distance between them fig c shows that hys norm and sqrt all perform equally well while simple norm reduces performance by and omitting normalization entirely reduces it by at fppw some regularization e is needed as we evalu ate descriptors densely including on empty patches but the results are insensitive to e value over a large range centre surround normalization we also investigated an alternative centre surround style cell normalization scheme in which the image is tiled with a grid of cells and for each cell the total energy in the cell and its surrounding re gion summed over orientations and pooled using gaussian weighting is used to normalize the cell however as fig c window norm shows this decreases performance relative to the corresponding block based scheme by at fppw for pooling with σ cell widths one reason is that there are no longer any overlapping blocks so each cell is coded only once in the final descriptor including several normalizations for each cell based on different pooling scales σ provides no perceptible change in performance so it seems that it is the existence of several pooling regions with differ ent spatial offsets relative to the cell that is important here not the pooling scale to clarify this point consider the r hog detector with overlapping blocks the coefficients of the trained linear svm give a measure of how much weight each cell of each block can have in the final discrimination decision close ex amination of fig b f shows that the most important cells are the ones that typically contain major human contours es pecially the head and shoulders and the feet normalized w r t blocks lying outside the contour in other words despite the complex cluttered backgrounds that are com mon in our training set the detector cues mainly on the contrast of silhouette contours against the background not on internal edges or on silhouette contours against the fore ground patterned clothing and pose variations may make internal regions unreliable as cues or foreground to contour transitions may be confused by smooth shading and shad owing effects similarly fig c g illustrate that gradients inside the person especially vertical ones typically count as negative cues presumably because this suppresses false pos itives in which long vertical lines trigger vertical head and leg cells detector window and context our detection window includes about pixels of margin around the person on all four sides fig e shows that this border provides a significant amount of con text that helps detection decreasing it from to pixels detection window decreases performance by at fppw keeping a window but increasing the person size within it again decreasing the border causes a similar loss of performance even though the resolution of the person is actually increased classifier by default we use a soft c linear svm trained with svmlight slightly modified to reduce memory usage for problems with large dense descriptor vectors us ing a gaussian kernel svm increases performance by about at fppw at the cost of a much higher run time discussion overall there are several notable findings in this work the fact that hog greatly out performs wavelets and that any significant degree of smoothing before calculating gra dients damages the hog results emphasizes that much of the available image information is from abrupt edges at fine scales and that blurring this in the hope of reducing the sen sitivity to spatial position is a mistake instead gradients should be calculated at the finest available scale in the current pyramid layer rectified or used for orientation voting and only then blurred spatially given this relatively coarse spa tial quantization suffices pixel cells one limb width on the other hand at least for human detection it pays to sample orientation rather finely both wavelets and shape contexts lose out significantly here secondly strong local contrast normalization is essen tial for good results and traditional centre surround style schemes are not the best choice better results can be achieved by normalizing each element edge cell several times with respect to different local supports and treating the results as independent signals in our standard detector each hog cell appears four times with different normaliza tions and including this redundant information improves performance from to at fppw summary and conclusions we have shown that using locally normalized histogram of gradient orientations features similar to sift descriptors in a dense overlapping grid gives very good results for person detection reducing false positive rates by more than an order of magnitude relative to the best haar wavelet based detector from we studied the influence of various de scriptor parameters and concluded that fine scale gradients fine orientation binning relatively coarse spatial binning and high quality local contrast normalization in overlapping descriptor blocks are all important for good performance we also introduced a new and more challenging pedestrian database which is publicly available future work although our current linear svm detector is reasonably efficient processing a scale space im age detection windows in less than a second there is still room for optimization and to further speed up detections it would be useful to develop a coarse to fine or rejection chain style detector based on hog descriptors we are also working on hog based detectors that incorporate motion in formation using block matching or optical flow fields fi nally although the current fixed template style detector has proven difficult to beat for fully visible pedestrians humans are highly articulated and we believe that including a parts based model with a greater degree of local spatial invariance a b c d e f g figure our hog detectors cue mainly on silhouette contours especially the head shoulders and feet the most active blocks are centred on the image background just outside the contour a the average gradient image over the training examples b each pixel shows the maximum positive svm weight in the block centred on the pixel c likewise for the negative svm weights d a test image it computed r hog descriptor f g the r hog descriptor weighted by respectively the positive and the negative svm weights would help to improve the detection results in more general situations scene semantics from long term observation of people vincent david f ivan josef abhinav and alexei a e cole normale sup erieure paris mellon university abstract our everyday objects support various tasks and can be used by people for different purposes while object classification is a widely studied topic in computer vision recognition of object function i e what people can do with an object and how they do it is rarely addressed in this paper we construct a functional object description with the aim to recognize objects by the way people interact with them we describe scene objects sofas tables chairs by associated human poses and ob ject appearance our model is learned discriminatively from automat ically estimated body poses in many realistic scenes in particular we make use of time lapse videos from youtube providing a rich source of common human object interactions and minimizing the effort of man ual object annotation we show how the models learned from human observations significantly improve object recognition and enable predic tion of characteristic human poses in new scenes results are shown on a dataset of more than frames obtained from time lapse videos of challenging and realistic indoor scenes introduction what are people expected to do with a christmas tree just set up in a living room is it common to see a person sitting on a stove current computer vision methods provide no answers to such questions meanwhile resolving these and many other questions by recognizing functional properties of objects and scenes would be highly relevant for addressing the tasks of abnormal event detection and predicting future events in image and video data object functions can be derived from the known associations between object categories and human actions the mediated perception of function approach for example chair sittable window openable actions such as sitting however can be realized in many different forms which can be characteristic for some objects but not for others as illustrated in figure moreover some objects may not support the common function associated with their category for exam ple windows in airplanes are usually not openable these and numerous other examples suggest that the category level association between objects and their functions is not likely to scale well to the very rich variety of the types and forms of person object interactions instead we argue that the functional descriptions of objects should be learned directly from observations of visual data fig different ways of using objects while all people depicted on the left are sitting their sitting poses can be rather unambiguously associated with the objects on the right in this paper we build on this observation and learn object descriptions in terms of characteristic body poses in this work we design object descriptions by learning associations between objects and spatially co occurring human poses to capture the rich variety of person object interactions we automatically detect people and estimate body poses in long term observations of realistic indoor scenes using the state of the art method of while reliable pose estimation is still a challenging problem we circumvent the noise in pose estimation by observing many person interac tions with the same instances of objects for this purpose we use videos from hours lasting events parties house cleaning recorded with a static camera and summarized into time static objects in time lapses e g sofas can be readily associated with hundreds of co occurring human poses spanning the typ ical interactions of people with these objects see figures equipped with this data we construct statistical object descriptors which combine the signa tures of object specific body poses as well as the object appearance the model is learned discriminatively from many time lapse videos of variety of scenes to summarize our contributions we propose a new statistical model de scribing objects in terms of distributions of associated human poses notably we do not require human poses to be annotated during training and learn the rich variety of person object interactions automatically from long term observa tions of people our functional object description generalizes across realistic and challenging scenes provides significant improvements in object recognition and supports prediction of human poses in new scenes background semantic object labeling and segmentation has been mainly con sidered for outdoor scenes e g for indoor scenes the focus has been on recovering spatial layout possibly since many indoor objects are often bet ter defined by their function rather than appearance time lapse is a common media type used to summarize recordings of long events into short video clips by temporal sub sampling we use time lapses widely available on public video sharing web sites such as youtube which are typically sampled at one frame per seconds the interplay between people and objects has recently attracted significant attention interactions between people and semantic objects has been studied in still images with the focus on improving action recognition object local ization and discovery as well as pose estimation in video constraints between human actions and objects e g drinking from a coffee cup have been investigated in restricted laboratory setups or ego centric scenarios in both still images and video the focus has been typically on small objects manipulated by hands e g coffee cups footballs tennis rackets rather than scene objects such as chairs sofas or tables which exhibit large intra class variability in addition manual annotation of action categories or human poses in the training data is often required and the models typically do not allow predicting poses in new scenes without people functional scene descriptions have been developed for surveillance setups e g but the models are usually designed for specific scene instances and use only coarse level observations of object person tracks or approximate person segments obtained from background subtraction in contrast our method generalizes to new challenging scenes and uses finer grain descriptors of estimated body configuration enabling discrimination between object classes such as sofas and chairs recent attempts have inferred functions or affordances from au tomatically obtained noisy reconstructions of indoor scenes these methods infer affordance based on the geometry and physical properties of the space for example they find places where a person can sit by fitting a human skeleton in a particular pose at a particular location in the scene while people can sit at many places they tend to sit in sofas more often than on tables moreover they may sit on sofas in a different way than on a floor or on a chair in this work we aim to leverage these observations and focus on statistical affordances by learning typical human poses associated with each object in a similar setup to ours fouhey et al have looked at people actions as a cue for a coarse box like geometry of indoor scenes here we investigate the interplay between object function and object semantics rather than scene geometry in addition in the geometric person scene relations are designed manually in this work we learn semantic person object interactions from data method overview in this section we give a brief overview of the proposed approach our main goal is to learn functional object descriptions from realistic observations of person object interactions to simplify the learning task we assume input videos to contain static objects with fixed locations in each frame of the video annota tion of such objects in the whole video can be simply done by outlining object boundary in one video frame as illustrated in figure moreover person inter actions with static objects can be automatically recorded by detecting people in the spatial proximity of annotated objects we start by over segmenting input scenes into super pixels which will form the candidate object regions details given in section for each object region pose estimation appearance pose distribution appearance distribution bof location location distribution h r fig overview of the proposed person based object description input scenes are over segmented into super pixels each super pixel denoted r here is described by the distribution of co occurring human poses over time as well as by the appearance and location of the super pixel in the image r we construct a descriptor vector h r to be used for subsequent learning and recognition the particular novelty of our method is a new descriptor represent ing an object region by the temporal statistics hp r of co occurring people section this descriptor contains a distribution of human body poses and their relative location with respect to the object region we also represent each object region by appearance features denoted ha r and the absolute location in the frame denoted hl r as described in section given descriptor vectors one for each object region containing statistics of characteristic poses appearance and image locations a linear support vector ma chine svm classifier is learnt for each object class from the labelled training data in a discriminative manner at test time the same functional and appear ance representation is extracted from candidate object regions of the testing video individual candidate object regions are then classified as belonging to one of the semantic object classes modeling long term person object interactions this section presents our model of the relationship between objects and sur rounding people we start by introducing a new representation describing an object by the statistics of co occurring human poses we then explain the de tails of the extraction and quantization of human poses in time lapses describing an object by a distribution of poses we wish to characterize objects by the typical locations and poses of surrounding people while reasoning about people and scenes has some advantages reliable estimation of scene geometry and human poses in is still an open problem moreover deriving rich person object co occurrences from a single im age is difficult due to the typically limited number of people in the scene and the noise of automatic human pose estimation to circumvent these problems we fig capturing person object interactions an object region r is described by a distribution histogram over poses k left joints j middle and cells c right the grid of cells c is placed around each joint to capture the relative position of an object region r with respect to joint j the pixel overlap between the grid cell c and the object region r weights the contribution of the jth joint and the kth pose cluster take advantage of the spatial co occurrence of objects and people in the image plane moreover we accumulate many human poses by observing scenes over an extended period of time in our setup we assume a static camera and consider larger objects such as sofas and tables which are less likely to change locations over time we describe object region r in the image by the temporal statistics hp of co occurring human poses each person detection d is represented by the locations of j body joints indexed by j and the assignment qd of d pose to a vocabulary of kp discrete pose clusters see figure and sections for details to measure the co occurrence of people and objects we define a spatial grid of cells c around each body joint j we measure the overlap between the object region r and the grid cell bd by the normalized area of their intersection b r bj c r bj c we then accumulate overlaps from all person detections d in a given video and compute one entry hp r of the histogram descriptor hp r for region r as hp r d j c r qd k j c d d exp where k j and c index pose clusters body joints and grid cells respectively the contribution of each person detection in is weighted by the detection score sd the values of qd indicate the similarity of the person detection d with a pose cluster k in the case of the hard assignment of d to the pose cluster k qd for k k and qd otherwise in our experiments we found that better results can be obtained using soft pose assignment as described in the next section fig pose cluster and detection examples left example cluster means from our pose vocabulary right person detections in multiple frames of time lapse videos assigned to the pose clusters on the left building a vocabulary of poses we represent object specific human actions by a distribution of quantized human poses to compute pose quantization we build a vocabulary of poses from person detections in the training set by unsupervised clustering in order to build the pose vocabulary we first convert each detection d in the training video into a dimensional pose vector xd by concatenating mid point coordinates of all detected body joints we center and normalize all pose vectors in the training videos and cluster them by fitting a gaussian mixture model gmm with kp components via expectation maximization em the components are initialized by the result of a k means clustering and during fitting we constrain the covariances to be diagonal the resulting mean vectors µk diagonal covariance matrices σk and weights πk for each pose cluster k kp form our vocabulary of poses see figure a pose vector xd for a detection d can be described by a soft assignment to each of the µk by computing the posterior probability vector qd where qk p xd µk σk πk kp p xd µ σ π person detection and pose estimation we focus on detecting people in three body configurations common in indoor scenes standing sitting and reaching we use the person detector from yang and ramanan which was shown to perform very well at both people detection and pose estimation and train three separate models one for each body configuration we found that training separate models improved pose estimation performance over using a single generic pose estimator section the three detectors are run separately on all frames of each time lapse video in a sliding window manner at multiple scales as all our videos have fixed view point we use background subtraction section to remove some false positive detections additional false positives can be removed via geometric filtering we use the vanishing point estimation method proposed in to compute the hori zon height yh we then assume a linear relationship hp yp α yp yh between a person height hp and the feet y coordinate yp in the image and learn the scaling coefficient α via ransac and robust least square fitting we discard detections for which the difference between the detected person height and the expected person height is greater than a given threshold e finally we normalize the output of the detectors by making the mean and standard deviation of the detection scores equal to and on training videos respectively the filtering and normalization is performed separately for each detector to obtain the final set of detections we perform standard non maxima sup pression on the combined outputs of the three detectors in each frame if bound ing boxes of several person detections overlap i e have intersection over union bigger than the detection with the highest normalized response is kept this leads to a set di of confident person detections for the ith video each detection d di is represented by an associated normalized score sd and an estimated limb configuration consisting of j bounding boxes bd j j corresponding to j locations of body joints as our time lapse videos are sparsely sampled in time the reasoning about temporal evolution of human poses is not straightforward we therefore cur rently discard any temporal information about detected people nevertheless the temporal re occurrence of characteristic body poses for particular objects is a very powerful cue which we exploit to i reduce the noise in pose estimation and ii to span the rich variety of person object interactions modeling appearance and location in addition to the distribution of poses we also model the appearance and ab solute position of image regions we build on the orderless bag of features rep resentation and describe the appearance of image regions by a distribution of visual words we first densely extract sift descriptors f k from image patches bf of multiple sizes sk for k s for all training videos and quantize them into visual words by fitting a gmm with ka components each feature f is then soft assigned to this vocabulary in the same manner as described in eq this results in an assignment vector qf for each feature the ka dimensional appearance histogram ha r for region r is computed as a weighted sum of assignment vectors qf s a f f k k f fk where bf r is the number of pixels belonging to both object region r and feature patch bf similar to we also represent the absolute position of regions r within the video frame this is achieved by spatially discretizing the video into a grid of m n cells resulting in a m n dimensional histogram hl r for each region r here the ith bin of hl r is simply the proportion of pixels of the ith cell of the grid falling into r learning from long term observations we now detail how we obtain candidate object regions from multiple super pixel segmentations and learn the model of person object interactions we then show how to recognize objects in testing videos and predict likely poses in new scenes obtaining candidate object regions as described in previous sections we represent objects by accumulating statistics of human poses image appearance and location at object regions r candidate object regions are obtained by over segmenting video frames into super pixels using the method and on line imple mentation of as individual video frames may contain many people occlud ing the objects in the scene we represent each video using a single background frame containing almost no people section rather than relying on a single segmentation we follow and compute multiple overlapping segmentations by varying the parameters of the segmentation algorithm learning object model we train a classifier for each object class in a one versus all manner the training data for each classifier is obtained by collecting all potentially overlapping super pixels ri for i n from all training videos for each region we extract their corresponding pose appearance and location histograms as described in sections and the histograms are sepa rately normalized and concatenated into a single k dimensional feature vec tor x h p r h a r h l ri where h denotes normalized histogram h an object label yi is then assigned to each super pixel based on the sur face overlap with the provided ground truth object segmentation in the training videos using the surface overlap threshold of each super pixel can be as signed up to two ground truth object labels finally we train a binary support vector machine svm classifier with the hellinger kernel for each object class using the labelled super pixels as training data the hellinger kernel is efficiently implemented using the explicit feature map φ xi xi xi and a linear classifier finally the outputs of individual svm classifiers are calibrated with respect to each other by fitting a multinomial regression model from the clas sifiers output to the super pixel labels the output of the learning stage is a k dimensional weight vector wy of the calibrated linear classifier for each object class y at test time multiple super pixel segmentations are extracted from the back ground frame of the test video and the individual classifiers are applied to each super pixel this leads to a confidence measure for each label and super pixel the confidence of a single image pixel is then the mean of the confidences of all the super pixels it belongs to inferring probable pose here we wish to predict the most likely pose within a manually provided bounding box in an image given an object layout segmen tation of the scene this is achieved by choosing the pose cluster for which the sum of learnt object weights for all joints most agree with the given per pixel object labels in the image more formally denoting wy k j c the weight learnt for label y pose cluster k joint j and grid cell c we select the pose cluster kˆ that maximizes the sum of per pixel weights under each joint grid cell bk j kˆ arg max wyi k j c where yi is the label for pixel i time lapse dataset we extend the dataset of to time lapse videos containing a total of around frames each video sequence shows human actors interacting with an indoor scene over a period of time ranging from a few minutes to several hours the captured events include parties working in an office cooking or room cleaning the videos were downloaded from youtube by placing queries such as time lapse party search results were manually verified to contain only videos captured with a stationary camera and showing an indoor scene all videos are sparsely sampled in time with limited temporal continuity between consecutive frames the dataset represents a challenging uncontrolled setup where people perform natural non staged interactions with objects in a variety of real indoor scenes we manually annotated each video with ground truth segmentation masks of eight frequently occurring semantic object classes bed sofa armchair cof fee table chair table wardrobe cupboard christmas tree and other similar to the other class contains various foreground room clutter such as clothes on the floor or objects e g lamps bottles or dishes on tables in addi tion to objects we also annotated three room background classes wall ceiling and floor as the camera and majority of the objects are static we can collect hundreds or even thousands of realistic person object interactions throughout the whole time lapse sequence by providing a single object annotation per video the dataset is divided into splits of around videos with approximately the same proportion of labels for different objects the dataset including the annotations is available at experiments in this section we give the implementation details and then show results for i pose estimation ii semantic labeling of objects in time lapse videos and iii predicting likely poses for new scenes implementation details the foreground background segmentation in each video frame is estimated using a pixel wise adaptive mixture of gaussian with components with α and t we also compute a single background image for each video that contains no people by taking the median of background segments across all video frames person detections and human pose estimates in each frame are obtained using the method and code of detections in the background segments and with confidence smaller than are removed the threshold e for the ground plane based geometric filter is set to super pixels for each video are generated using the code of with parameters σ k and min sift features are extracted from patches of size pixels with spatial overlap to train the proposed model we use splits of the dataset see section to cross validate the c parameter of the svm and use the split to calibrate the outputs of the individual classifiers the resulting model is tested on the split this is repeated five times for the different test splits to obtain the mean and standard deviation of the classification performance pose estimation to evaluate person detection and pose estimation perfor mance we have annotated poses of at least ten randomly chosen person occur rences in each video resulting in pose annotations person bounding box detection performance is measured using the standard average precision ap and pose estimation performance is measured by the percentage of correct parts pcp score among the detected people as proposed in we first compare our individually trained pose estimators for each action see section with a single model trained on images from all action classes both have a similar re call of around but the individually trained models achieve an average pcp of compared to for the single model we then evaluate the effect of the background subtraction and geometric filtering for person detection the indi vidually trained models achieve an ap of which is significantly improved by background subtraction and geometric filtering semantic labeling of objects semantic labeling performance is measured by pixel wise precision recall curve and average precision ap for each object table shows the average precision for different object and room background classes for different feature combinations of our method performance is com pared to two baselines the method of trained on our data with semantic object annotations and the deformable part model dpm of trained over manually defined bounding boxes for each class at test time the dpm bound ing boxes are converted to segmentation masks by assigning to each testing pixel the maximum score of any overlapping detection note that combining the pro posed pose features with appearance a p results in a significant improvement table average precision ap for baselines of felzenszwalbet al and hedauet al compared to four different settings of our method appearance and location features only a l person features only p appearance and person features a p appearance location and person features combined a l p in overall performance but further adding location features a l p brings little additional benefit which suggests that spatial information in the scene is largely captured by the spatial relation to the human pose the proposed method a l p also significantly outperforms both baselines example classification results for the proposed method are shown in figure finally learnt weights for different objects are visualized in figure we have also evaluated our model on functional surface estimation for train ing and testing we have provided ground truth functional surface masks for the dataset of our model achieves ap of and for walkable sit table and reachable surfaces respectively averaging a gain of compared to which could be attributed to the discriminative nature of our model predicting poses in new scenes figure shows qualitative results of pre dicting likely human poses in new scenes given a person bounding box and the manually labelled object regions the most likely pose is predicted using eq as can be seen the automatically generated poses are consistent with object classes as well as with the scene geometry despite no explicit reasoning is included in our model discussion we have proposed a statistical descriptor of person object interactions and have demonstrated its benefits for recognizing objects and predicting human body poses in new scenes notably our method requires very little annotation and relies on long term observations of people in time lapse videos given the mutual dependence of objects and human poses the current method can be further extended to perform joint pose estimation and object recognition style aware mid level representation for discovering visual connections in space and time paper presentation by bhavin modi slides by yong jae lee alexei a efros and martial hebert carnegie mellon university uc berkeley iccv long before the age of data mining when historical dating where botany geography the view from your window challenge low level visual words sivic zisserman laptev lindeberg czurka et al visual world object category discovery sivic et al grauman darrell russell et al lee grauman payet todorovic faktor irani kang et al most approaches mine globally consistent patterns mid level visual elements doersch et al endres et al juneja et al fouhey et al doersch et al recent methods discover specific visual patterns much in our visual world undergoes a gradual change temporal much in our visual world undergoes a gradual change spatial mine mid level visual elements in temporally and spatially varying data and model their visual style when historical dating of cars kim et al fu et al palermo et al where geolocalization of streetview images cristani et al hays efros knopp et al chen grauman schindler et al establish connections closed world model style specific differences approach unsupervised discovery of mid level discriminative patches fig the top two detected visual words bo ttom vs mid level discrim inative pat ches t op t rained wit hout any supervisio n a nd on t he e large unlabeled dataset can we get nice parts without supervision idea k means clustering in hog space still not good enough the svm memorizes bad examples and still scores them highly however the space of bad examples is much more diverse so we can avoid overfitting if we train on a training subset but look for patches on a validation subset why k means on hog fails chicken egg problem if we know that a set of patches are visually similar we can easily learn a distance metric for them if we know the distance metric we can easily find other members start with k means train a discriminative classifier for the distance function using all other classes as negative examples re assign patches to clusters whose classifier gives highest score repeat start with k means or knn train a discriminative classifier for the distance function using detection detect the patches and assign to top k clusters repeat can we get good parts without supervision what makes a good part must occur frequently in one class representative must not occur frequently in all classes discriminative split the discovery dataset into two equal parts training and validation train on the training subset run the trained classifier on the validation set to collect examples exchange training and validation sets repeat algorit hm di cover top n disc inat ive pat ches require discovery set v atural world et n d n l div id d n int o equal sized disjoint se s rand pl t sample random patche from k km ans s l clust er pct ches u i ng k leans while not converg d do u for all i such tha siz k i do l prune out small ones cnew i svm rain k i t t rain classifier for each clust er knew i det ect op c i i find op m nevl member in ot her set end for k knew c cnew w a p swa p t swap the hvo sets end while a i purit y k i x di scrim in ati ven ess k i vi t comput e scor ret urn sele ct op c a n t sort according o scores and select op n patches doublets discover second order relationships start with high scoring patches find spatial correlations to other weaker patches rank the potential doublets on validation set doublets ap on mit indoor scene recognition dataset coming back sample patches and compute nearest neighbors dalal triggs hog style sensitive style insensitive tight uniform 1987 1930 1932 peaky low entropy clusters uniform high entropy clusters take top ranked clusters to build correspondences dataset train a detector hog linear svm singh et al natural world background dataset top detection per decade singh et al we expect style to change gradually natural world background dataset 1990s 1990s top detection per decade initial model final model initial model final model regression model regression model support vector regressors with gaussian kernels input hog output date geo location detector regression output detector regression output train image level regression model using outputs of visual element detectors and regressors as features figure we is alize he tyle hat a style jw are reores or l learned b y averaoin the for e ch d ecade results results date geo location prediction crawled from crawled from google street view images tagged with year images tagged with gps coordinate n carolina to georgia carl b date pr edid ii o n error edb geo l o c atiiion pr edic fi on error q i lo i t i wlo ii c i i i b singh e ti al sip bow ours sp singh et al ibow figure box plots showing date and location tion error on the cardb and ed b da tasets respectively lower value are bet ter qur approach the ubtle stylistic differences for each discovered ent in the data which leads to lower error rates results date geo location prediction crawled from crawled from google street view results learned styles average of top predictions per decade extra fine grained recognition mean classification accuracy on caltech ucsd birds dataset weak supervision strong supervision conclusions models visual style appearance correlated with time space first establish visual connections to create a closed world then focus on style specific differences experiment presentation for visual recognition presenter zitao liu university of pittsburgh march goal discover connections goal discover connections between recurring mid level visual elements goal discover connections between recurring mid level visual elements in historic temporal and geographic spatial image collections goal discover connections between recurring mid level visual elements in historic temporal and geographic spatial image collections and attempts to capture the underlying visual style visual style appearance variations of the same visual element due to change in time or location three steps mining style sensitive visual elements establishing correspondences training style aware regression models data three datasets are used in the paper and one dataset cardb can be downloaded from the author website cardb with photos for training and for testing cardb contains cars made in crawled from external packages voc extracting features libsvm making classification and regression decades in total goal find visual elements whose appearance correlates with the date label what code does select query images from each category random sample patches with various scales and locations from each query image each patch is represented by a histogram of gradients hog goal find visual elements whose appearance correlates with the date label what code does find top n n nearest neighbor patches in the database foreach query decade image subsets querydec foreach query image in querydec foreach match decade image subsets matchdec foreach image in matchdec i foreach detector in image i more than images goal find visual elements whose appearance correlates with the date label what code does for each cluster compute the temporal distribution of labels compute entropy based on year distribution find good clusters with peaky distributions rank good clusters and select top m m as the discovered style sensitive visual elements peaky examples etc flat examples etc goal model the change in style of the same visual element over the entire label space what code does train a linear svm with initial cluster images negative data are sampled from random flickr images run the learned svm on a new subset of the data that slightly broader range in label space keep the most confident prediction example and continue cross validation is used in each step of the incremental revision good positive training examples 1940 1941 1953 1972 1991 goal prediction what code does train a svm with rbf kernels on the correspondences obtained from step each training example is weighted by a transformed detection score keep in mind we have decades clusters for each decades neighbors in each cluster thank you q a unsupervised discovery of mid level discriminative patches saurabh singh abhinav gupta and alexei a efros carnegie mellon university pittsburgh pa usa http graphics cs cmu edu projects discriminativepatches abstract the goal of this paper is to discover a set of discriminative patches which can serve as a fully unsupervised mid level visual repre sentation the desired patches need to satisfy two requirements to be representative they need to occur frequently enough in the visual world to be discriminative they need to be di erent enough from the rest of the visual world the patches could correspond to parts objects visual phrases etc but are not restricted to be any one of them we pose this as an unsupervised discriminative clustering problem on a huge dataset of image patches we use an iterative procedure which alternates between clustering and training discriminative classi ers while applying careful cross validation at each step to prevent over tting the paper ex perimentally demonstrates the e ectiveness of discriminative patches as an unsupervised mid level visual representation suggesting that it could be used in place of visual words for many tasks furthermore discrim inative patches can also be used in a supervised regime such as scene classi cation where they demonstrate state of the art performance on the mit indoor dataset introduction consider the image in figure shown in green are the two most con dent visual words detected in this image and the corresponding visual word clusters shown in red are the two most con dent detections using our proposed mid level discriminative patches computed on the same large unlabeled image dataset as the visual words without any supervision for most people the representation at the top seems instantly more intuitive and reasonable in this paper we will show that it is also simple to compute and o ers very good discriminability broad coverage better purity and improved performance compared to visual word features finally we will also show how our approach can be used in a supervised setting where it demonstrates state of the art performance on scene classi cation beating bag of words spatial pyramids objectbank and scene deformable parts models on the mit indoor dataset what are the right primitives for representing visual information this is a question as old as the computer vision discipline itself and is unlikely to be settled anytime soon over the years researchers have proposed a plethora of di erent visual features spanning a wide spectrum from very local to full image singh et al our discrimina ve patches visual words fig the top two detected visual words bottom vs mid level discriminative patches top trained without any supervision and on the same large unlabeled dataset and from low level bottom up to semantic top down in terms of spatial resolution one extreme is using the pixel itself as a primitive however there is generally not enough information at a pixel level to make a useful feature it will re all the time at the other extreme one can use the whole image as a primitive which while showing great promise in some applications requires extraordinarily large amounts of training data since one needs to represent all possible spatial con gurations of objects in the world explicitly as a result most researchers have converged on using features at an intermediate scale that of an image patch but even if we x the resolution of the primitive there is still a wide range of choices to be made regarding what this primitive aims to represent from the low level bottom up point of view an image patch simply represents the appearance at that point either directly with raw pixels or transformed into a di erent representation lterbank response vector blurred or spatially binned feature etc at a slightly higher level combining such patches together typically by clustering and histogramming allows one to represent texture information e g textons dense bag of words etc a bit higher still are approaches that encode image patches only at sparse interest points in a scale and rotation invariant way such as in sift matching overall the bottom up approaches work very well for most problems involving exact instance matching but their record for generalization i e nding similar instances is more mixed one explanation is that at the low level it is very hard to know which parts of the representation are the important ones and which could be safely ignored as a result recently some researchers have started looking at high level fea tures which are already impregnated with semantic information needed to gen eralize well for example a number of papers have used full blown object detec tors e g as features to describe and reason about images e g others have employed discriminative part detectors such as poselets at tribute detectors visual phrases or stu detectors as features unsupervised discovery of mid level discriminative patches however there are signi cant practical barriers to the wide spread adaptation of such top down semantic techniques first they all require non trivial amounts of hand labeled training data per each semantic entity object part attribute etc second many semantic entities are just not discriminative enough visually to act as good features for example wall is a well de ned semantic category with plenty of training data available but it makes a lousy detector simply because walls are usually plain and thus not easily discriminable in this paper we consider mid level visual primitives which are more adapt able to the appearance distributions in the real world than the low level features but do not require the semantic grounding of the high level entities we propose a representation called mid level discriminative patches these patches could correspond to parts objects visual phrases etc but are not restricted to be any one of them what de nes them is their representative and discriminative property that is that they can be detected in a large number of images with high recall and precision but unlike other discriminative methods which are weakly supervised either with image labels e g or bounding box labels e g our discriminative patches can be discovered in a fully unsupervised manner given only a large pile of unlabeled the key insight of this paper is to pose this as an unsupervised discriminative clustering problem on a huge unla beled dataset of image patches we use an iterative procedure which alternates between clustering and training discriminative classi ers linear svms while applying careful cross validation at each step to prevent over tting some of the resulting discriminative patches are shown in figure priorwork our goals are very much in common with prior work on nding good mid level feature representations most notably the original visual words approach given sparse key point detections over a large dataset the idea is to cluster them in sift space in an e ort to yield meaningful common units of visual meaning akin to words in text however in practice it turns out that while some visual words do capture high level object parts most others end up encoding simple oriented bars and corners and might more appropriately be called visual phonemes or even visual letters the way addressed these shortcomings was by using image segments as a mid level unit for nding commonality since then there has been a large body of work in the general area of unsupervised object discovery while we share some of the same conceptual goals our work is quite di erent in that we do not explicitly aim to discover whole semantic units like objects or parts unlike we do not assume a single object per image whereas in object discovery there is no separate training and test set we explicitly aim to discover patches that are detectable in novel images because only visual words have all the above properties that will be our main point of comparison our paper is very much inspired by poselets both in its goal of nding representative yet discriminative regions and its use of hog descriptors and n b the term unsupervised has changed its meaning over the years e g while the award winning paper of fergus et al had unsupervised in its title it would now be considered a weakly supervised method singh et al fig examples of discovered discriminative patches that were highly ranked linear svms however poselets is a heavily supervised method employing labels at the image bounding box and part levels whereas our approach aims to solve a much harder problem without any supervision at all so direct comparisons between the two would not be meaningful our work is also informed by who show that discriminative machinery such as a linear svm could be successfully used in a fully unsupervised manner discovering discriminative patches given an arbitrary set of unlabeled images the discovery dataset d our goal is to discover a relatively small number of discriminative patches at arbitrary resolution which can capture the essence of that data the challenge is that the space of potential patches represented in this paper by hog features is extremely large since even a single image can generate tens of thousands of patches at multiple scales unsupervised discovery of mid level discriminative patches approach motivation of our two key requirements for good discriminative patches to occur fre quently and to be su ciently di erent from the rest of the visual world the rst one is actually common to most other object discovery approaches the standard solution is to employ some form of unsupervised clustering such as k means either on the entire dataset or on a randomly sampled subset however running k means on our mid level patches does not produce very good clusters as shown on figure initial kmeans the reason is that unsupervised clustering like k means has no choice but to use a low level distance metric e g euclidean cross correlation which does not work well for medium sized patches of ten combining instances which are in no way visually similar of course if we somehow knew that a set of patches were visually similar we could easily train a discriminative classi er such as a linear svm to produce an appropriate sim ilarity metric for these patches it would seem we have a classic chicken and egg problem the clustering of the patches depends on a good similarity but learning a similarity depends on obtaining good clusters but notice that we can pose this problem as a type of iterative discriminative clustering in a typical instantiation e g an initial clustering of data is followed by learning a discriminative classi er for each cluster based on the discriminatively learned similarity new cluster memberships can be computed by reassigning data points to each cluster etc in principle this procedure will satisfy both of our requirements the clustering step will latch onto frequently occurring patches while the classi cation step will make sure that the patches in the clusters are di erent enough from the rest and thus discriminative however this approach will not work on our problem as is since it is infeasible to use a discovery dataset large enough to be representative of the entire visual world it will require too many clusters to address this we turn the classi cation step of discriminative clustering into a detection step making each patch cluster into a detector trained using a linear svm to nd other patches like those it already owns this means that each cluster is now trained to be discriminative not just against the other clusters in the discovery dataset d but against the rest of the visual world which we propose to model by a natural world dataset n the only requirement of n is that it be very large thousands of images containing tens of millions of patches and drawn from a reasonably random image distribution we follow in simply using random photos from the internet note that n is not a negative set as it can and most likely will contain visual patterns also found in d we also experimented with d n it is interesting to note the similarity between this version of discriminative clustering and the root lter latent updates of there too a cluster of patches representing an object category is being iteratively re ned by making it more discriminative against millions of other image patches however whereas imposes overlap constraints preventing the cluster from moving too far from the supervised initialization in our unsupervised formulation the clusters are completely unconstrained singh et al ini al kmeans discrimina ve iter iter clustering our approach fig few examples to show how our iterative approach starting with initial k means clustering converges to consistent clusters iter while standard discriminative clus tering approach second row also converges in some cases column in vast major ity of cases it memorizes and over ts note that our approach allows clusters to move around in x y and scale space to nd better members oval in column alas our proposed discriminative clustering procedure is still not quite enough consider figure which shows three example clusters the top row is simple ini tialization using k means while the second row shows the results of the discrimi native clustering described above the left most cluster shows good improvement compared to initialization but the other two clusters see little change the cul prit seems to be the svm it is so good at memorizing the training data that it is often unwilling to budge from the initial cluster con guration to com bat this we propose an extremely simple but surprisingly e ective solution cross validation training instead of training and classifying the same data we divide our input dataset into two equal non overlapping subsets we perform a step of discriminative clustering on the training subset but then apply our learned discriminative patches on the validation subset to form clusters there in this way we are able to achieve better generalization since the errors in the training set are largely uncorrelated with errors in the validation set and hence the svm is not able to over t to them we then exchange the roles of training and validation and repeat the whole process until convergence figure shows the iterations of our algorithm for the three initial patch clusters showing top patches in each cluster note how the consistency of the clusters improves signi cantly after each iteration note also that the clusters can move around in x y and scale space to latch onto the more discriminative parts of the visual space see the circled train in the right most column approach details initialization the input to our discovery algorithm is a discovery dataset d of unlabeled images as well as a much larger natural world dataset n in this unsupervised discovery of mid level discriminative patches algorithm discover top n discriminative patches require discovery set d natural world set n d n divide d n into equal sized disjoint sets rand sample sample random patches from k kmeans cluster patches using kmeans while not converged do for all i such that size k i do prune out small ones cnew i svm train k i train classi er for each cluster knew i detect top c i m find top m new members in other set end for k knew c cnew swap swap swap the two sets end while a i purity k i discriminativeness k i i compute scores return select top c a n sort according to scores and select top n patches paper we used images randomly sampled from flickr com first we divide both d and n into two equal non overlapping subsets and for cross validation for all images in we compute hog descriptors at multiple resolutions at di erent scales to initialize our algorithm we randomly sample patches from about per image disallowing highly overlapping patches or patches with no gradient energy e g sky patches and then run standard k means clustering in hog space since we do not trust k means to generalize well we set k quite high k producing tens of thousands of clusters most with very few members we remove clusters with less than patches eliminating of the clusters ending up with about patches per image still active iterative algorithm given an initial set of clusters k we train a linear svm classi er for each cluster using patches within the cluster as positive examples and all patches of as negative examples iterative hard mining is used to handle the complexity if we exclude near duplicates from by normalized cross correlation the trained discriminative classi ers are then run on the held out validation set and new clusters are formed from the top m rings of each detector we consider all svm scores above to be rings we limit the new clusters to only m members to keep cluster purity high using more produces much less homogeneous clusters on the other hand if a cluster detector res less than times on the validation set this suggests that it might not be very discriminative and is killed the validation set now becomes the training set and the procedure is repeated until convergence i e the top m patches in a cluster do not change in practice the algorithm converges in iterations the full approach is summarized in algorithm parameters the size of our hog descriptor is cells with a stride of pixels cell so the minimum possible patch is pixels while the maximum could be as large as full image we use a linear svm c with iterations of hard negative mining for more details consult the source code on the website singh et al fig visualizing images left in terms of their most discriminative patches right the patch detectors were red on a novel image and the high scoring patch detections were averaged together weighted by their scores ranking discriminative patches our algorithm produces a dictionary of a few thousand discriminative patches of varying quality our next task is to rank them to nd a small number of the most discriminative ones our criteria for ranking consists of two terms purity ideally a good cluster should have all its member patches come from the same visual concept however measuring purity in an unsupervised setting is impossible therefore we approximate the purity of each cluster in terms of the classi er con dence of the cluster members assuming that cross validation removed over tting thus the purity score for a cluster is computed by summing up the svm detection scores of top r cluster members where r m to evaluate the generalization of the cluster beyond the m training patches discriminativeness in an unsupervised setting the only thing we can say is that a highly discriminative patch should re rarely in the natural world therefore we de ne discriminativeness of a patch as the ratio of the number of rings on d to the number of rings on d n of course we do not want patches that never re at all but these would have already been removed in cross validation training all clusters are ranked using a linear combination of the above two scores figure shows a set of top ranked discriminative patch clusters discovered with our approach note how sometimes the patches correspond to object parts such as horse legs and horse muzzle sometimes to whole objects such as plates and sometimes they are just discriminative portions of an object similar to pose lets e g see the corner of trains also note that they exhibit surprisingly good visual consistency for a fully unsupervised approach the ability of discrimina tive patches to re on visually similar image regions is further demonstrated in figure where the patch detectors are applied to a novel image and high scoring detections are displayed with the average patch from that cluster in a way the gure shows what our representation captures about the image discovering doublets while our discriminative patch discovery approach is able to produce a number of visually good highly ranked discriminative patches some other potentially promising ones do not make it to the top due to low purity this happens when unsupervised discovery of mid level discriminative patches a clean cluster b noisy cluster c cleaned up fig cluster clean up using doublets a visually non homogeneous cluster b that has learned more than one concept when coupled into a doublet with a high quality cluster a gets cleaned up c fig examples of discovered discriminative doublets that were highly ranked a cluster converges to two or more concepts because the underlying classi er is able to generalize to both concepts simultaneously e g figure however often the two concepts have di erent ring patterns with respect to some other mid level patch in the dictionary e g motorcycle wheel in figure therefore we propose to employ second order spatial co occurrence relationships among our discriminative patches as a way of cleaning them up figure moreover discovering these second order relationships can provide us with doublets which could be further generalized to grouplets that can themselves be highly discriminative and useful as mid level features in their own right to discover doublets we start with a list of highly discriminative patches that will serve as high quality roots for each root patch we search over all the other discovered discriminative patches even poor quality ones and record their relative spatial con guration in each image where they both re the pairs that exhibit a highly spatially correlated ring pattern become potential doublets we rank the doublets by applying them on the unlabeled validation set the doublets are ranked high if in images where both patches re their relative spatial con guration is consistent with what was observed in the training set in figure we show some examples of highly discriminative doublets notice that not only is the quality of discriminative patches good but also the spatial relationships within the doublet are intuitive quantitative evaluation as with other unsupervised discovery approaches evaluation is di cult we have shown a number of qualitative results figures and there are many more on the website for the rst set of quantitative evaluations as well as for all the qualitative results except figure we have chosen a subset of of pascal voc images as our discovery dataset we picked pascal singh et al voc because it is a well known and di cult dataset with rich visual diversity and scene clutter moreover it provides annotations for a number of object classes which could be used to evaluate our unsupervised discovery however since our discovered patches are not meant to correspond to semantic objects this evaluation metric should be taken with quite a few grains of salt one way to evaluate the quality of our discriminative patch clusters is by using the standard unsupervised discovery measures of purity and coverage e g purity is de ned by what percentage of cluster members correspond to the same visual entity in our case we will use pascal semantic category annotations as a surrogate for visual similarity for each of the top discov ered patches we rst assign it to one of the semantic categories using majority membership we then measure purity as percentage of patches assigned to the same pascal semantic label coverage is de ned as the number of images in the dataset covered red on by a given cluster figure reports the purity and coverage of our approach and a number of baselines for each one the graphs show the cumulative purity coverage as number of clusters being considered is increased the clusters are sorted in the decreasing order of purity we compare our approach with visual words and russell et al baseline plus a number of intermediate results of our method hog k means visual word analog for hog features initial clustering svms trained on the k means clusters without discriminative re clustering and no cross validation iterative discriminatively trained clusters but with out cross validation in each case the numbers indicate area under the curve auc for each method overall our approach demonstrates substantial gain in purity without sacri cing much coverage as compared to the established ap proaches moreover each step of our algorithm improves purity note in par ticular the substantial improvement a orded by the cross validation training procedure compared to standard training as we mentioned however the experiment above under reports the purity of our clusters since semantic equivalence is not the same as visual similarity therefore we performed an informal perceptual experiment with human sub jects measuring the visual purity of our clusters we selected the top clusters from the dataset for each cluster we asked human labelers to mark which of the cluster top ten rings on the validation set are visually consistent with the cluster based on this measure average visual purity for these clusters was supervised image classi cation unsupervised clustering approaches such as visual words have long been used as features for supervised tasks such as classi cation in particular bag of visual words and spatial pyramids are some of the most popular current methods for image classi cation since our mid level patches could be considered the true visual words as opposed to visual letters it makes sense to see how they would perform on a supervised classi cation task we evaluate them in two di erent settings unsupervised discovery supervised classi cation and supervised discovery supervised classi cation unsupervised discovery of mid level discriminative patches number of clusters cumulative purity purity visual words russel et al hog kmeans init clust no crossval our approach number of clusters cumulative coverage coverage visual words russel et al hog kmeans init clust no crossval our approach fig quantitative comparison of discriminative patches compared to the baseline approaches quality of clustering is evaluated in terms of the area under the curve for cumulative purity and coverage gist spatial pyramid hog sphog y spatial pyramid sift sp y roi gist scene dpm mm scene object bank ours ours gist ours sp ours gist sp ours dpm ours gist dpm ours sp dpm gist sp dpm ours gist sp dpm table quantitative evaluation average classi cation on mit indoor dataset current state of the art ybest performance from various vocabulary sizes unsupervised discriminative patches using the discriminative patches discovered from the same pascal voc dis covery dataset as before we would like to see if they could make better visual words for a supervised image classi cation task our baseline is the standard spatial pyramid of visual words using visual words using their public code for our approach we construct spatial pyramid using top dis criminative patches classi cation was performed using a simple linear svm and performance was evaluated using average precision standard visual words scored ap while using our discriminative patches the score was ap we further expanded our feature representation by adding the top ranking doublets as extra visual words resulting in a slight improvement to ap supervised discriminative patches we further want to evaluate the performance of our approach when it is allowed to utilize more supervision for a fair comparison with several existing supervised approaches instead of discovering the discriminative patches from a common pool of all the images we can also discover them on a per category basis in this experiment we perform supervised scene classi cation using the challenging mit indoor dataset containing scene categories using the provided scene singh et al church closet bowling bookstore bakery bathroom wine cellar subway game room shoe shop office laundromat hair salon dining room staircase auditorium grocery store meebng room fig top discriminative patches for a sampling of scenes in the mit indoor scene dataset note how these capture various visual aspects of a typical scene labels we discover discriminative patches for each scene independently while treating all other images in the dataset as the natural world figure shows top few most discriminative patches discovered this way for a number of categories from the dataset it is interesting to see that the discrimina tive patches capture aspects of scenes that seem very intuitive to us in particular unsupervised discovery of mid level discriminative patches the discriminative patches for the church category capture the arches and the benches the ones for the meeting room capture the center table and the seats these discriminative patches are therefore capturing the essence of the scene in terms of these highly consistent and repeating patterns and hence providing a simple yet highly e ective mid level representation inspired by these results we have also applied a similar approach to discovering what makes paris look like paris using geographic labels as the weak supervisory signal to perform classi cation top discovered patches of each scene are ag gregated into a spatial pyramid using maxpooling over the discriminative patch scores as in we again use a linear svm in a one vs all classi cation the results are reported in table comparison with hog visual words sphog shows the huge performance gain resulting from our algorithm when operating in the same feature space further our simple method by itself outperforms all others that have been tested on this dataset moreover combining our method with the currently best performing combination approach of yields performance which to our knowledge is the best on this dataset acknowledgments the authors would like to thank martial hebert tomasz malisiewicz abhinav shrivastava and carl doersch for many helpful discussions this work was supported by onr grant cs visual recognition describing images with features adriana kovashka department of computer science january plan for today presentation assignments schedule changes image filtering feature detection feature description feature matching next time classification and detection adriana research announcements open door policy fixed office hours adriana travel clarification of experiment presentations presentation assignments image description an image is a set of pixels problems with pixel representation not invariant to small changes translation illumination etc some parts of an image are more important than others what do we want to represent preprocessing image filtering image filtering compute a function of the local neighborhood at each pixel in the image function specified by a filter or mask saying how to combine values from neighbors uses of filtering enhance an image denoise resize etc extract information texture edges etc detect patterns template matching motivation noise reduction even multiple images of the same static scene will not be identical common types of noise salt and pepper noise random occurrences of black and white pixels impulse noise random occurrences of white pixels gaussian noise variations in intensity drawn from a gaussian normal distribution motivation noise reduction how could we reduce the noise i e give an estimate of the true intensities what if there only one image let replace each pixel with an average of all the values in its neighborhood assumptions expect pixels to be like their neighbors expect noise processes to be independent from pixel to pixel let replace each pixel with an average of all the values in its neighborhood moving average in can add weights to our moving average weights non uniform weights say the averaging window size is x attribute uniform weight to each pixel loop over all pixels in neighborhood around image pixel f i j now generalize to allow different weights depending on neighboring pixel relative position non uniform weights this is called cross correlation denoted filtering an image replace each pixel with a linear combination of its neighbors the filter kernel or mask h u v is the prescription for the weights in the linear combination what if we want nearest neighboring pixels to have the most influence on the output removes high frequency components from the image low pass filter dali marilyn einstein describing images with features feature detection repeatability the same feature can be found in several images despite geometric and photometric transformations saliency each feature has a distinctive description compactness and efficiency many fewer features than image pixels locality a feature occupies a relatively small area of the image robust to clutter and occlusion we want to detect at least some of the same points in both images no chance to find true matches yet we have to be able to run the detection procedure independently per image we want to be able to reliably determine which point goes with which must provide some invariance to geometric and photometric differences between the two views we should easily recognize the point by looking through a small window shifting a window in any direction should give a large change in intensity flat region no change in all directions edge no change along the edge direction corner significant change in all directions window averaged squared change of intensity induced by shifting the image data by u v window function w x y or in window outside gaussian window averaged squared change of intensity induced by shifting the image data by u v e u v expanding i x y in a taylor series expansion we have for small shifts u v a quadratic approximation to the error surface between a patch and itself shifted by u v where m is a matrix computed from image derivatives m w x y i x i x i x i y x y i y i y notation i i x x i i y y i x i y i i x y since m is symmetric we have m x x t mxi i xi the eigenvalues of m reveal the amount of intensity change in the two principal orthogonal gradient directions in the window edge corner and are large flat region and are small measure of corner response k empirical constant k 06 compute image gradients ix and iy for all pixels for each pixel compute by looping over neighbors x y compute find points with large corner response function r r threshold take the points of locally maximum r as the detected feature points i e pixels where r is bigger than for all the or neighbors partial invariance to additive and multiplicative intensity changes only derivatives are used invariance to intensity shift intensity scaling fine except for the threshold that used to specify when r is large enough r r threshold x image coordinate x image coordinate invariant to image scale image zoomed image not invariant to image scale all points will be classified as edges corner the problem how do we choose corresponding circles independently in each image do objects in the image have a characteristic scale that we can identify solution design a function on the region circle which is scale invariant the same for corresponding regions even if they are at different scales take a local maximum of this function f f scale region size region size a good function for scale detection has one stable sharp peak f f f region size region size region size for usual images a good function would be a one which responds to contrast sharp local intensity change functions for determining scale kernels laplacian derivative of gaussian difference of gaussians where gaussian harris laplacian find local maximum of harris corner detector in space image coordinates laplacian in scale scale harris x describing images with features feature description raw patches as local descriptors the simplest way to describe the neighborhood around an interest point is to write down the list of intensities to form a feature vector but this is very sensitive to even small shifts rotations geometric transformations e g scale translation rotation photometric transformations sift descriptor lowe use histograms to bin pixels within sub patches according to their orientation making the descriptor rotation invariant rotate the patch according to its dominant gradient orientation this puts the patches into a canonical orientation histograms of oriented gradients hog bin gradients from pixel neighborhoods into orientations dalal triggs cvpr filter banks d d d d d ej i l l l j lj j kristen grauman image from 250 50 200 250 can you match the texture to the response a b c derek hoiem mean responses representing texture by mean response filters derek hoiem mean responses we can form a feature vector from the list of responses at each pixel shape context belongie malik and puzicha pami representation of the local shape around a feature location as histogram of edge points in an image relative to that location computed by counting edge points in log polar space color histograms representation of the distribution of colors in an image derived by counting the number of pixels of each of given set of color ranges in a typically color space rgb hsv etc gist oliva and torralba ijcv captures the global energy of the scene computes edge orientation responses for multiple orientations and scales describing images with features feature matching correspondence matching points patches edges or regions across images alignment find the parameters of the transformation that best align matched points fitting find the parameters of a model that best fit the data p v c hough machine analysis of bubble chamber pictures proc int conf high energy accelerators and instrumentation given a set of points find the curve or line that explains the data points best y m x y m x b b hough space m y b x hough transform y m x b y m x b fischler bolles in algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence line fitting example algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence ransac line fitting example algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence line fitting example n i algorithm sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model repeat until the best model is found with high confidence algorithm n i sample randomly the number of points required to fit the model solve for model parameters using samples score by the fraction of inliers within a preset threshold of the model given matched points in a and b estimate the translation of the object x b x a tx i i i i t y tx ty x b x a tx i i least squares solution write down objective function i i t y x b x a write in form ax b solve using pseudo inverse or eigenvalue tx y b y a decomposition t y x b x a n n derek hoiem y b y a n n tx ty problem outliers multiple objects and or many to one matches hough transform solution x b x a tx initialize a grid of parameter values i i each matched pair casts a vote for consistent values find the parameters with the most votes solve using least squares with inliers i i t y tx ty ransac solution problem outliers x b x a tx sample a set of matching points pair i i solve for transformation parameters score parameters with number of inliers repeat steps n times i i t y local features main components detection identify the interest points description extract vector feature descriptor surrounding each interest point x x x matching determine correspondence between descriptors in two views x x x next time classification and detection adriana research foundations and trendsqr in computer graphics and vision vol no qc t tuytelaars and k mikolajczyk doi local invariant feature detectors a survey tinne and krystian department of electrical engineering katholieke universiteit leuven kasteelpark arenberg b leuven belgium school of electronics and physical sciences university of surrey guildford surrey uk abstract in this survey we give an overview of invariant interest point detectors how they evolved over time how they work and what their respective strengths and weaknesses are we begin with deﬁning the properties of the ideal local feature detector this is followed by an overview of the literature over the past four decades organized in diﬀerent categories of feature extraction methods we then provide a more detailed analysis of a selection of methods which had a particularly signiﬁcant impact on the research ﬁeld we conclude with a summary and promising future research directions introduction in this section we discuss the very nature of local invariant fea tures what do we mean with this term what is the advantage of using local features what can we do with them what would the ideal local feature look like these are some of the questions we attempt to answer what are local features a local feature is an image pattern which diﬀers from its immediate neighborhood it is usually associated with a change of an image prop erty or several properties simultaneously although it is not necessarily localized exactly on this change the image properties commonly con sidered are intensity color and texture figure shows some exam ples of local features in a contour image left as well as in a grayvalue image right local features can be points but also edgels or small image patches typically some measurements are taken from a region centered on a local feature and converted into descriptors the descrip tors can then be used for various applications why local features fig importance of corners and junctions in visual recognition and an image example with interest points provided by a corner detector cf section why local features as discussed shortly in the preface local invariant features are a powerful tool that has been applied successfully in a wide range of systems and applications in the following we distinguish three broad categories of feature detectors based on their possible usage it is not exhaustive or the only way of categorizing the detectors but it emphasizes diﬀerent proper ties required by the usage scenarios first one might be interested in a speciﬁc type of local features as they may have a speciﬁc seman tic interpretation in the limited context of a certain application for instance edges detected in aerial images often correspond to roads blob detection can be used to identify impurities in some inspection task etc these were the ﬁrst applications for which local feature detec tors have been proposed second one might be interested in local fea tures since they provide a limited set of well localized and individually identiﬁable anchor points what the features actually represent is not really relevant as long as their location can be determined accurately and in a stable manner over time this is for instance the situation in most matching or tracking applications and especially for camera cal ibration or reconstruction other application domains include pose estimation image alignment or mosaicing a typical example here are the features used in the klt tracker finally a set of local features can be used as a robust image representation that allows to recognize objects or scenes without the need for segmentation here again it does not really matter what the features actually represent they do not even have to be localized precisely since the goal is not to match them on an individual basis but rather to analyze their statistics this way of exploiting local features was ﬁrst reported in the seminal work of and and soon became very popular especially in the context of object recognition both for speciﬁc objects as well as for category level recognition other application domains include scene classiﬁcation texture analysis image retrieval and video mining clearly each of the above three categories imposes its own con straints and a good feature for one application may be useless in the context of a diﬀerent problem these categories can be considered when searching for suitable feature detectors for an application at hand in this survey we mainly focus on the second and especially the third application scenario finally it is worth noting that the importance of local features has also been demonstrated in the context of object recognition by the human visual system more precisely experiments have shown that removing the corners from images impedes human recognition while removing most of the straight edge information does not this is illustrated in figure a few notes on terminology before we discuss feature detectors in more detail let us explain some terminology commonly used in the literature detector or extractor traditionally the term detector has been used to refer to the tool that extracts the features from the image e g a corner blob or edge detec tor however this only makes sense if it is a priori clear what the corners blobs or edges in the image are so one can speak of false detections or missed detections this only holds in the ﬁrst usage a few notes on terminology scenario mentioned earlier not for the last two where extractor would probably be semantically more correct still the term detector is widely used we therefore also stick to this terminology invariant or covariant a similar discussion holds for the use of invariant or covariant a function is invariant under a certain family of transformations if its value does not change when a transformation from this family is applied to its argument a function is covariant when it commutes with the transformation i e applying the transformation to the argu ment of the function has the same eﬀect as applying the transformation to the output of the function a few examples may help to explain the diﬀerence the area of a surface is invariant under rotations since rotating a surface does not make it any smaller or bigger but the orientation of the major axis of inertia of the surface is covariant under the same family of transformations since rotating a sur face will aﬀect the orientation of its major axis in exactly the same way based on these deﬁnitions it is clear that the so called local scale and or aﬃne invariant features are in fact only covariant the descrip tors derived from them on the other hand are usually invariant due to a normalization step since the term local invariant feature is so widely used we nevertheless use invariant in this survey rotation invariant or isotropic a function is isotropic at a particular point if it behaves the same in all directions this is a term that applies to e g textures and should not be confused with rotational invariance interest point region or local feature in a way the ideal local feature would be a point as deﬁned in geometry having a location in space but no spatial extent in practice however images are discrete with the smallest spatial unit being a pixel and discretization eﬀects playing an important role to localize features in images a local neighborhood of pixels needs to be analyzed giving all local features some implicit spatial extent for some applications e g camera calibration or reconstruction this spatial extent is completely ignored in further processing and only the location derived from the feature extraction process is used with the location sometimes determined up to sub pixel accuracy in those cases one typically uses the term interest point however in most applications those features also need to be described such that they can be identiﬁed and matched and this again calls for a local neighborhood of pixels often this neighborhood is taken equal to the neighborhood used to localize the feature but this need not be the case in this context one typically uses the term region instead of interest point however beware when a local neighborhood of pixels is used to describe an interest point the feature extraction process has to determine not only the location of the interest point but also the size and possibly the shape of this local neighborhood especially in case of geometric deformations this signiﬁcantly compli cates the process as the size and shape have to be determined in an invariant covariant way in this survey we prefer the use of the term local feature which can be either points regions or even edge segments properties of the ideal local feature local features typically have a spatial extent i e the local neigh borhood of pixels mentioned above in contrast to classical segmen tation this can be any subset of an image the region boundaries do not have to correspond to changes in image appearance such as color or texture also multiple regions may overlap and uninter esting parts of the image such as homogeneous areas can remain uncovered ideally one would like such local features to correspond to seman tically meaningful object parts in practice however this is unfeasible as this would require high level interpretation of the scene content which is not available at this early stage instead detectors select local features directly based on the underlying intensity patterns properties of the ideal local feature good features should have the following properties repeatability given two images of the same object or scene taken under diﬀerent viewing conditions a high percentage of the features detected on the scene part visible in both images should be found in both images distinctiveness informativeness the intensity patterns underlying the detected features should show a lot of varia tion such that features can be distinguished and matched locality the features should be local so as to reduce the probability of occlusion and to allow simple model approx imations of the geometric and photometric deformations between two images taken under diﬀerent viewing conditions e g based on a local planarity assumption quantity the number of detected features should be suﬃ ciently large such that a reasonable number of features are detected even on small objects however the optimal number of features depends on the application ideally the number of detected features should be adaptable over a large range by a simple and intuitive threshold the density of features should reﬂect the information content of the image to provide a compact image representation accuracy the detected features should be accurately local ized both in image location as with respect to scale and possibly shape eﬃciency preferably the detection of features in a new image should allow for time critical applications repeatability arguably the most important property of all can be achieved in two diﬀerent ways either by invariance or by robustness invariance when large deformations are to be expected the preferred approach is to model these mathematically if possible and then develop methods for feature detection that are unaﬀected by these mathematical transformations robustness in case of relatively small deformations it often suﬃces to make feature detection methods less sensitive to such deformations i e the accuracy of the detection may decrease but not drastically so typical deformations that are tackled using robustness are image noise discretization eﬀects compression artifacts blur etc also geometric and photometric deviations from the mathematical model used to obtain invariance are often overcome by including more robustness discussion clearly the importance of these diﬀerent properties depends on the actual application and settings and compromises need to be made repeatability is required in all application scenarios and it directly depends on the other properties like invariance robustness quantity etc depending on the application increasing or decreasing them may result in higher repeatability distinctiveness and locality are competing properties and cannot be fulﬁlled simultaneously the more local a feature the less information is available in the underlying intensity pattern and the harder it becomes to match it correctly especially in database applications where there are many candidate features to match to on the other hand in case of planar objects and or purely rotating cameras e g in image mosaicing applications images are related by a global homography and there are no problems with occlusions or depth discontinuities under these con ditions the size of the local features can be increased without problems resulting in a higher distinctiveness similarly an increased level of invariance typically leads to a reduced distinctiveness as some of the image measurements are used to lift the degrees of freedom of the transformation a similar rule holds for robustness versus distinctiveness as typically some information is disregarded considered as noise in order to achieve robustness as a result it is important to have a clear idea on the required level of invariance or robustness for a given application it is hard to achieve high invariance and robustness at the same time and invariance which is not adapted to the application may have a negative impact on the results accuracy is especially important in wide baseline matching regis tration and structure from motion applications where precise corre spondences are needed to e g estimate the epipolar geometry or to calibrate the camera setup quantity is particularly useful in some class level object or scene recognition methods where it is vital to densely cover the object of interest on the other hand a high number of features has in most cases a negative impact on the computation time and it should be kept within limits also robustness is essential for object class recognition as it is impossible to model the intra class variations mathematically so full invariance is impossible for these applications an accurate local ization is less important the eﬀect of inaccurate localization of a fea ture detector can be countered up to some point by having an extra robust descriptor which yields a feature vector that is not aﬀected by small localization errors global versus local features local invariant features not only allow to ﬁnd correspondences in spite of large changes in viewing conditions occlusions and image clutter wide baseline matching but also yield an interesting description of the image content for image retrieval and object or scene recognition tasks both for speciﬁc objects as well as categories to put this into context we brieﬂy summarize some alternative strategies to compute image representations including global features image segments and exhaustive and random sampling of features global features in the ﬁeld of image retrieval many global features have been proposed to describe the image content with color histograms and variations thereof as a typical example this approach works surprisingly well at least for images with distinctive colors as long as it is the overall composition of the image as a whole that the user is interested in rather than the foreground object indeed global features cannot distinguish foreground from background and mix information from both parts together global features have also been used for object recognition result ing in the ﬁrst appearance based approaches to tackle this challenging problem turk and pentland and later murase and nayar proposed to compute a principal component analysis of a set of model images and to use the projections onto the ﬁrst few principal components as descriptors compared to the purely geometry based approaches tried before the results of the novel appearance based approach were striking a whole new range of natural objects could suddenly be recognized however being based on a global description image clutter and occlusions again form a major problem limiting the usefulness of the system to cases with clean backgrounds or where the object can be segmented out e g relying on motion information image segments an approach to overcome the limitations of the global features is to segment the image in a limited number of regions or segments with each such region corresponding to a single object or part thereof the best known example of this approach is the blobworld system pro posed in which segments the image based on color and texture then searches a database for images with similar image blobs an example based on texture segmentation is the wide baseline matching work described in however this raises a chicken and egg problem as image segmen tation is a very challenging task in itself which in general requires a high level understanding of the image content for generic objects color and texture cues are insuﬃcient to obtain meaningful segmentations sampled features a way to deal with the problems encountered with global features or image segmentations is to exhaustively sample diﬀerent subparts of the image at each location and scale for each such image subpart global features can then be computed this approach is also referred to as a sliding window based approach it has been especially popu lar in the context of face detection but has also been applied for the recognition of speciﬁc objects or particular object classes such as pedes trians or cars by focusing on subparts of the image these methods are able to ﬁnd similarities between the queries and the models in spite of changing backgrounds and even if the object covers only a small percentage of the total image area on the downside they still do not manage to cope with partial occlusions and the allowed shape variability is smaller than what is feasible with a local features based approach however by far the biggest drawback is the ineﬃciency of this approach each and every subpart of the image must be analyzed resulting in thousands or even millions of features per image this requires extremely eﬃcient methods which signiﬁcantly limits the scope of possible applications to overcome the complexity problems more sparse ﬁxed grid sam pling of image patches was used e g it is however diﬃcult to achieve invariance to geometric deformations for such fea tures the approach can tolerate some deformations due to dense sam pling over possible locations scales poses etc but the individual features are not invariant an example of such approach are multi scale interest points as a result they cannot be used when the goal is to ﬁnd precise correspondences between images however for some appli cations such as scene classiﬁcation or texture recognition they may well be suﬃcient in better results are reported with a ﬁxed grid of patches than with patches centered on interest points in the context of scene classiﬁcation work this can be explained by the dense cover age as well as the fact that homogeneous areas e g sky are also taken into account in the ﬁxed grid approach which makes the representation more complete this dense coverage is also exploited in where a ﬁxed grid of patches was used on top of a set of local invariant features in the context of speciﬁc object recognition where the latter supply an initial set of correspondences which then guide the construction of correspondences for the former in a similar vein rather than using a ﬁxed grid of patches a random sampling of image patches can also be used e g this gives a larger ﬂexibility in the number of patches the range of scales or shapes and their spatial distribution good scene recognition results are shown in based on random image patches as in the case of ﬁxed grid sampling this can be explained by the dense coverage which ignores the localization properties of features random patches are in fact a subset of the dense patches and are used mostly to reduce the complexity their repeatability is poor hence they work better as an addition to the regular features rather than as a stand alone method finally to overcome the complexity problems while still providing a large number of features with better than random localization proposed to sample features uniformly from edges this proved useful for dealing with wiry objects well represented by edges and curves overview of this survey this survey article consists of two parts first in section we review local invariant feature detectors in the literature from the early days in computer vision up to the most recent evolutions next we describe a few selected representative methods in more detail we have structured the methods in a relatively intuitive manner based on the type of feature extracted in the image doing so we distinguish between corner detectors section blob detectors section and region detectors section additionally we added a section on various detectors that have been designed in a computationally eﬃcient manner section with this structure we hope the reader can easily ﬁnd the type of detector most useful for his her application we conclude the survey with a qualitative comparison of the diﬀerent methods and a discussion of future work section to the novice reader who is not very familiar with local invariant feature detectors yet we advice to skip section at ﬁrst this section has been added mainly for the more advanced reader to give further insight in how this ﬁeld evolved and what were the most important trends and to add pointers to earlier work local features in the literature in this section we give an overview of local feature detectors proposed in the literature starting from the early days of image processing and pattern recognition up to the current state of the art introduction the literature on local feature detection is vast and goes back as far as when it was ﬁrst observed by attneave that information on shape is concentrated at dominant points having high curvature it is impossible to describe each and every contribution to over years of research in detail instead we provide pointers to the literature where the interested reader can ﬁnd out more the main goal of this section is to make the reader aware of the various great ideas that have been proposed especially in the pre internet era all too often these are overlooked and then re invented we would like to give proper credit to all those researchers who contributed to the current state of the art early work on local features it is important to mention the beginnings of this research area and the ﬁrst publications which appeared after the observation on the importance of corners and junctions in visual recognition see figure since then a large number of algorithms have been sug gested for extracting interest points at the extrema of various functions computed on the digital shape also it has been understood early on in the image processing and visual pattern recognition ﬁeld that inter sections of straight lines and straight corners are strong indications of man made structures such features have been used in a ﬁrst series of applications from line drawing images and photomosaics first monographs on digital image processing by rosenfeld and by duda and hart as well as their later editions served to establish the ﬁeld on a sound theoretical foundation overview we identiﬁed a number of important research directions and struc tured the subsections of this section accordingly first many authors have studied the curvature of contours to ﬁnd corners their work is described in section others directly analyze the image intensities e g based on derivatives or regions with high variance this is the topic of section another line of research has been inspired by the human visual system and aims at reproducing the processes in the human brain see section methods focussing on the exploitation of color information are discussed in section while section describes model based approaches more recently there has been a trend toward feature detection with invariance against various geometric transfor mations including multi scale approaches and scale or aﬃne invariant methods these are discussed in section in section we focus on segmentation based methods and section describes methods which build on machine learning techniques finally section gives an overview of diﬀerent evaluation and comparison schemes proposed in the literature contour curvature based methods a ﬁrst category of interest point detectors are the contour curvature based methods originally these were mainly applied to line drawings piecewise constant regions and cad cam images rather than natural scenes the focus was especially on the accuracy of point localization they were most popular of the end of the and most of the high curvature points contour intersections and junctions often result in bi directional sig nal changes therefore a good strategy to detect features consists of extracting points along the contour with high curvature curvature of an analog curve is deﬁned as the rate at which the unit tangent vec tor changes with respect to arc length contours are often encoded in chains of points or represented in a parametric form using splines several techniques have been developed which involve detecting and chaining edges so as to ﬁnd corners in the chain by analyzing the chain code ﬁnding maxima of curvature change in direc tion or change in appearance others avoid chaining edges and instead look for maxima of curvature or change in direction at places where the gradient is large several methods for detecting edges based on gray level gradient and angular changes in digital curves were proposed in other solutions for line drawing images include methods for detecting corners in a chain coded plane curve in these works a mea sure for the cornerness of a point is based on mean angular diﬀerences between successive segment positions along the chain one general approach to feature extraction is to detect the dominant points directly through angle or corner detection using various schemes for approximating discrete curvature such as cosine or local curvature which deﬁne corners as discontinuities of an average curve slope other parametric representation like b splines curves are commonly used in rendering a curve in computer graphics compression and coding cad cam systems and also for curve ﬁtting and shape description in cubic polynomials are ﬁt to a curve and discontinuities are detected in such curve to localize interest points spline approximations of line images are used in in combination with a dynamic programming technique to ﬁnd the knots of a spline pseudo coding of line ﬁgures and a complicated vector ﬁnder to obtain interest points are proposed in in dominant points are computed at the maximum global curvature based on the iterative averaging of local discretized curvature at each point with respect to its immediate neighbors in tangential deﬂection and curvature of discrete curves are deﬁned based on the geometrical and statistical properties associated with the eigenvalue eigenvector structure of sample covariance matrices computed on chain codes another approach is to obtain a piecewise linear polygonal approx imation of the digital curve subject to certain constraints on the qual ity of ﬁt indeed it has been pointed out in that piecewise linear polygonal approximation with variable breakpoints will tend to locate vertices at actual corner points these points correspond approximately to the actual or extrapolated intersections of adjacent line segments of the polygons a similar idea was explored in more recently estimates the parameters of two lines ﬁtted to the two segments neighboring to the corner point a corner is declared if the parameters are statistically signiﬁcantly diﬀerent a similar approach is to identify edge crossings and junctions by following image gradient maxima or minima and ﬁnding gaps in edge maps dealing with scale corner detection methods by curvature estimation normally use a set of parameters to eliminate contour noise and to obtain the corners at a given scale although object corners can be found at multiple nat ural scales to solve this problem some detectors apply their algo rithms iteratively within a certain range of parameters selecting points which appear in a ﬁxed set of iterations the stability of the points and the time spent for their detection is closely related to the number of iterations initial attempts to deal with discretization and scale problems via an averaging scheme can be found in the curvature primal sketch cps proposed in is a scale space representation of signif icant changes in curvature along contours the changes are classiﬁed as basic or compound primitives such as corners smooth joints ends cranks bumps and dents the features are detected at diﬀerent scales resulting in a multiple scale representation of object contours a similar idea was explored in and later in where the curvature scale space analysis was performed to ﬁnd the local scale of curves they ﬁnd inﬂection points of the curves and represent shapes in parametric forms a b spline based algorithm was also proposed in the general idea is to ﬁt a b spline to the curve then to measure the cur vature around each point directly from the b spline coeﬃcients another algorithm dealing with scale for detecting dominant points on a digital closed curve is motivated by the angle detection pro cedure from they indicate that the detection of dominant points relies primarily on the precise determination of the region of support rather than on the estimation of discrete curvature first the region of support for each point based on its local properties is determined then a measure of relative curvature or local symmetry of each point is computed the gaussian ﬁlter is the most commonly used ﬁlter in point detection however if the scale of a gaussian ﬁlter is too small the result may include some redundant points which are unnecessary details i e due to noise if the scale is too large the points with small support regions will tend to be smoothed out to solve the problems existing in gaussian ﬁltering with ﬁxed scale scale space procedures based on multiple scale discrete curvature representation and search ing are proposed in the scheme is based on a stability criterion that states that the presence of a corner must concur with a curvature maximum observable at a majority of scales natural scales of curves were studied in to avoid exhaustive representation of curves over a full range of scales a successful scale selection mechanism for gaussian ﬁlters with a theoretical formulation was also proposed in in a nonlinear algorithm for critical point detection is pre sented they establish a set of criteria for the design of a point detection algorithm to overcome the problems arising from curvature approxima tion and gaussian ﬁltering another approach to boundary smoothing is based on simulated annealing for curvature estimation in the corner points are localized at the maxima of absolute curvature of edges the corner points are tracked through multiple curvature scale levels to improve localization chang and horng proposed an algorithm to detect corner points using a nest moving average ﬁlter is investigated in corners are detected on curves by computing the diﬀerence of blurred images and observing the shift of high curva ture points more detailed analysis of various methods for determining natural scales of curves can be found in discussion although theoretically well founded for analog curves the contour cur vature calculation is less robust in case of discrete curves possible error sources in digital curvature estimation were investigated in furthermore the objectives for the above discussed detectors were diﬀerent than the ones we typically have nowadays it was considered disadvantageous if a method detected corners on circular shapes mul tiple corners at junctions etc at that time a much stricter deﬁnition of interest points corners was used with only points corresponding to true corners in being considered as relevant nowadays in most practical applications of interest points the focus is on robust stable and distinctive points irrespective of whether they corre spond to true corners or not see also our earlier discussion in section there has been less activity in this area recently over the past ten years due to complexity and robustness problems while methods based directly on image intensity attracted more attention intensity based methods methods based on image intensity have only weak assumptions and are typically applicable to a wide range of images many of these approaches are based on ﬁrst and second order gray value derivatives while others use heuristics to ﬁnd regions of high variance diﬀerential approaches hessian based approaches one of the early intensity based detec tors is the rotation invariant hessian based detector proposed by beaudet it explores the second order taylor expansion of the inten sity surface and especially the hessian matrix containing the second order derivatives the determinant of this matrix reaches a maximum for blob like structures in the image a more detailed description of this method can be found in section it has been extended in and where the interest points are localized at the zero crossing of a curve joining local extrema of the hessian determinant around a corner similarly high curvature points can be localized by computing gaussian curvature of the image surface i e saddle points in image brightness in a local quadratic surface was ﬁt to the image inten sity function the parameters of the surface were used to determine the gradient magnitude and the rate of change of gradient direction the resulting detector uses the curvature of isophotes computed from ﬁrst and second order derivatives scaled by image gradient to make it more robust to noise a similar idea was proposed in a detailed investigation in and later in shows that the detectors of all perform the same measurements on the image and have relatively low reliability accord ing to criteria based on localization precision nevertheless the trace and determinant of the hessian matrix were successfully used later on in scale and aﬃne invariant extensions of interest point detec tors when other feature properties became more important gradient based approaches local feature detection based on ﬁrst order derivatives is also used in various applications a corner detector which returns points at the local maxima of a directional variance mea sure was ﬁrst introduced in in the context of mobile robot navigation it was a heuristic implementation of the auto correlation function also explored in the proposed corner detector investigates a local window in the image and determines the average change of inten sity which results from shifting the window by a few pixels in various directions this idea is taken further in and formalized by using ﬁrst order derivatives in a so called second moment matrix to explore local statistics of directional image intensity variations the method separates corner candidate detection and localization to improve the accuracy to subpixel precision at the cost of higher computational com plexity harris and stephens improved the approach by moravec by performing analytical expansion of the average intensity vari ance this results in a second moment matrix computed with sobel derivatives and a gaussian window a function based on the determi nant and trace of that matrix was introduced which took into account both eigenvalues of the matrix this detector is widely known today as the harris detector or plessey detector and is probably the best known interest point detector around it is described in more detail in section it has been extended in numerous papers e g by using gaussian derivatives combinations of ﬁrst and second order derivatives or an edge based second moment matrix but the underlying idea remains the same the harris detector was also investigated in and demonstrated to be optimal for l junctions based on the assumption of an aﬃne image deformation an analysis in led to the conclusion that it is more convenient to use the smallest eigenvalue of the autocorrelation matrix as the corner strength function more recently the second moment matrix has also been adopted to scale changes by parameterizing gaussian ﬁlters and normaliz ing them with respect to scale based on scale space theory also the harris detector was extended with search over scale and aﬃne space in using the laplacian operator and eigenvalues of the second moment matrix inspired by the pioneering work of linde berg see section for details the approach from performs an analysis of the computation of the second moment matrix and its approximations a speed increase is achieved by computing only two smoothed images instead of the three previously required a number of other suggestions have been made for how to compute the corner strength from the second order matrix and these have all been shown to be equiv alent to various matrix norms a generalization to images with multi dimensional pixels was also proposed in plessey electronic research ltd in the harris corner detector is extended to yield stable fea tures under more general transformations than pure translations to this end the auto correlation function was studied under rotations scalings up to full aﬃne transformations intensity variations a diﬀerent category of approaches based on intensity variations applies mathematical morphology to extract high curvature points the use of zero crossings of the shape boundary curvature in binary images detected with a morphological opening operator was investigated in mathematical morphology was also used to extract convex and concave points from edges in later on a parallel algo rithm based on an analysis of morphological residues and corner char acteristics was proposed in another approach indicates that for interest points the median value over a small neighborhood is signiﬁcantly diﬀerent from the corner point value thus the diﬀerence in intensity between the center and median gives a strong indication for corners however this method cannot deal with more complex junctions or smooth edges a simple and eﬃcient detector named susan was introduced in based on earlier work from it computes the fraction of pixels within a neighborhood which have similar intensity to the center pixel corners can then be localized by thresholding this measure and selecting local minima the position of the center of gravity is used to ﬁlter out false positives more details on the susan detector can be found in section a similar idea was explored in where pixels on a circle are considered and compared to the center of a patch more recently proposed the fast detector a point is clas siﬁed as a corner if one can ﬁnd a suﬃciently large set of pixels on a circle of ﬁxed radius around the point such that these pixels are all signiﬁcantly brighter resp darker than the central point eﬃcient classiﬁcation is based on a decision tree more details on fast can be found in section local radial symmetry has been explored in to identify interest points and its real time implementation was also proposed wavelet transformation was also investigated in the context of feature point extraction with successful results based on multi resolution analysis in saliency the idea of saliency has been used in a number of computer vision algorithms the early approach of using edge detectors to extract object descriptions embodies the idea that the edges are more signiﬁcant than other parts of the image more explicit uses of saliency can be divided into those that concentrate on low level local features e g and those that compute salient groupings of low level features e g though some approaches operate at both levels e g the technique suggested in is based on the maximization of descriptor vectors across a particular image these salient points are the points on the object which are almost unique hence they maxi mize the discrimination between the objects a related method identiﬁes salient features for use in automated generation of statistical shape appearance models the method aims to select those features which are less likely to be mismatched regions of low density in a mul tidimensional feature space generated from the image are classiﬁed as highly salient a more theoretically founded approach based on variability or com plexity of image intensity within a region was proposed in it was motivated by visual saliency and information content which we revise in the next section the method from deﬁnes saliency in terms of local signal complexity or unpredictability more speciﬁcally the use of shannon entropy of local attributes is suggested the idea is to ﬁnd a point neighborhood with high complexity as a measure of saliency or information content the method measures the change in entropy of a gray value histogram computed in a point neighborhood the search was extended to scale and aﬃne parameterized regions thus providing position scale and aﬃne shape of the region neighborhood for a detailed discussion we refer to section biologically plausible methods most systems proposed in the previous sections were mainly concerned with the accuracy of interest point localization this is important in the context of ﬁtting parametric curves to control points or image match ing for recovering the geometry in contrast the biologically plausible methods reviewed in this section were mainly proposed in the con text of artiﬁcial intelligence and visual recognition most of them did not have a speciﬁc application purpose and their main goal was to model the processes of the human brain numerous models of human visual attention or saliency have been discussed in cognitive psychol ogy and computer vision however the vast majority were only of theoretical interest and only few were implemented and tested on real images feature detection as part of the pre attentive stage one of the main models for early vision in humans attributed to neisser is that it consists of a pre attentive and an atten tive stage biologically plausible methods for feature detection usu ally refer to the idea that certain parts of a scene are pre attentively distinctive and create some form of immediate response within the early stages of the human visual system in the pre attentive stage only pop out features are detected these are local regions of the image which present some form of spatial discontinuity in the atten tive stage relationships between these features are found and group ing takes place this model has widely inﬂuenced the computer vision community mainly through the work of marr and is reﬂected in the classical computer vision approach feature detec tion and perceptual grouping followed by model matching and corre spondence search activities in the models of attention started in the mid following progress in neurophysiological and psychological research one approach inspired by neuro biological mechanisms was pro posed in they apply gabor like ﬁlters to compute local energy of the signal maxima of the ﬁrst and second order deriva tives of that energy indicate the presence of interest points the idea of using gabor ﬁlter responses from diﬀerent scales was further explored in the approach developed in was motivated by psy chophysical experiments they compute a symmetry score of the signal at each image pixel in diﬀerent directions regions with signiﬁcant sym metry are then selected as interest points theory on texture recognition and the idea of textons as simple local structures like blobs corners junctions line ends etc was introduced in he suggested that statistics over texton distributions play an important role in recognition the extraction of simple textons is done in the pre attentive stage and the construction of relations in the atten tive stage a feature integration theory based on these principles was proposed in he distinguished between a disjunctive case where the distinctive features can be directly localized in a feature map and a conjunctive case where the feature can be extracted only by processing various feature maps simultaneously this model was implemented by combining bottom up and top down measures of interest the bot tom up method merges various feature maps and looks for interesting events while in the top down process knowledge about the target is exploited the main goal of the above systems was to provide computation ally plausible models of visual attention their interest was mainly theoretical however those systems served as source of inspiration for practical solutions for real images once machine learning techniques like neural networks had grown mature enough in image pro cessing operators were combined with the attentive models to make it applicable to more realistic images he applies a laplacian of gaussians log like operator to feature maps to model the receptive ﬁelds and enhance the interesting events the image was analyzed at multiple scales the approach from uses a set of feature templates and cor relates them with the image to produce feature maps which are then enhanced with log temporal derivatives were used to detect moving objects koch and ullman proposed a very inﬂuential computational model of visual attention which accounts for several psychophysical phenomena they proposed to build a set of maps based on orienta tion color disparity and motion and to simulate the lateral inhibition mechanism by extracting locations which diﬀer signiﬁcantly from their neighborhood information from diﬀerent maps is then merged into a single saliency map a winner take all wta network was used to select the active location in the maps in a hierarchical manner using a pyramidal strategy the hypotheses suggested in were ﬁrst implemented in a similar implementation of the wta model was proposed in the extraction of globally salient structures like object outlines was investigated in by grouping local information such as contour fragments but no relation to pre attentive vision was claimed non uniform resolution and coarse to fine processing also non uniform resolution of the retina and coarse to ﬁne process ing strategies have been studied in biologically plausible models these have been simulated mostly via scale space techniques however these systems were mostly focused on the engineering and realtime aspects rather than its biological plausibility one of the ﬁrst systems to perform interest point detection in scale space was pro posed in they built a laplacian pyramid for coarse to ﬁne fea ture selection templates were used to localize the objects in the log space templates were also employed for building features maps which were then combined by a weighted sum diﬀerence of gaussians dog ﬁlters were used in the system designed in to accelerate the computation biologically inspired systems developed in explored the idea of using boundary and interest point detectors based on dog ﬁlters as well as directional diﬀerences of oﬀset gaussians doog to simulate simple cells in the system proposed in was mainly concerned with classiﬁ cation of textures studied earlier in the feature extraction part used a bank of ﬁlters based on oriented kernels dog and doog to produce feature maps similar to the next stage corresponds to a wta mechanism to suppress weak responses and simulate lateral inhibition finally all the responses are merged to detect texture boundaries spatial event detection robust statistics have also been used to detect outliers in a set of image primitives the idea is based on the observation that textures can be represented by their statistics and the locations which violate those statistics represent interesting events for example texture prim itives are represented by a number of attributes using histograms and ransac in first order statistics over feature maps computed from zero cross ings of dog at diﬀerent scales are used in for each point a histogram of gradient orientations is then constructed and the local histograms are combined into a global one which is similar in spirit to the more recent sift descriptor local histograms are then compared with the global one to provide a measure of interest another statistical model was proposed in they measure the edge density at a range of distances from the interest point to build an edge distribution histogram this idea has been used later in the shape context descriptor of cells that respond only to edges and bars which terminate within their receptive ﬁeld have ﬁrst been found in a corner detection algorithm based on a model for such end stopped cells in the visual cor tex was presented in furthermore the notion of end stopped cells was generalized to color channels in a biologically plausible way based on color opponent processes a more recent visual attention system also motivated by the early primate visual system is presented in multiscale image features detected at local spatial discontinuities in intensity color and orien tation are combined into a single topographical saliency map and a neural network selects locations depending on the saliency other recent visual recognition systems inspired by a model of visual cortex which follow models from can be found in these methods attempt to implement simple and complex cells color based methods from visual cortex which are multiscale gabor and edgel detectors fol lowed by local maxima selection methods color based methods color provides additional information which can be used in the process of feature extraction several biologically plausible methods reviewed in the previous section use color for building saliency maps given the high performance of harris corners a straightforward extension of the second moment matrix to rgb color space was intro duced in incorporating color information in the harris corner extraction process salient point detection based on color distinctiveness has been pro posed in salient points are the maxima of the saliency map which represents distinctiveness of color derivatives in a point neighborhood in related work they argue that the distinctiveness of color based salient points is much higher than for the intensity ones color ratios between neighboring pixels are used to obtain derivatives independent of illumination which results in color interest points that are more robust to illumination changes most of the proposed approaches based on color are simple exten sions of methods based on the intensity change color gradients are usually used to enhance or to validate the intensity change so as to increase the stability of the feature detectors but the pixel intensities remain the main source of information for feature detection model based methods there have been a few attempts to do an analytical study of corner detection by giving a formal representation of corner points in an image based on diﬀerential geometry techniques or contour curvature for instance it was found that a gray level corner point can be found as the point of maximal planar curvature on the line of the steepest gray level slope an analytical expression for an optimal function whose convolution with an image has signiﬁcant values at corner points was investigated in the methods presented in assume that a corner resem bles a blurred wedge and ﬁnds the characteristics of the wedge the amplitude angle and blur by ﬁtting it to the local image several mod els of junctions of multiple edges were used in the assumption is that the junctions are formed by homogeneous regions parameterized masks are used to ﬁt the intensity structure including position orienta tion intensity blurring and edges the residual is then minimized dur ing the detection the accuracy is high provided a good initialization of the parameters the eﬃciency of the approach in was improved in by using a diﬀerent blurring function and a method to initialize the parameters fitting a corner model to image data was also consid ered in for each possible intersection of lines a template was constructed based on the angle orientation and scale of the hypothe sized corner the template was then matched to the image in a small neighborhood of the interest point to verify the model a template based method for locating the saddle points was also described in where the corner points correspond to the intersections of saddle ridge and saddle valley structures a set of fuzzy patterns of contour points were established in and the corner detection was characterized as a fuzzy classiﬁcation problem of the patterns other model based methods aimed at improving the detection accuracy of the hessian based corner detector were proposed in to this end the responses of the corner detector on a the oretical model over scale space were analyzed it was observed that the operator responses at diﬀerent scales move along the bisector line it is worth to note that this observation is also valid for the pop ular harris corner detector the exact position of the corner was then computed from two responses indicating the bisector and its intersection with the zero crossing of the laplacian response an aﬃne transformation was also used to ﬁt a model of a corner to an image a diﬀerent model based approach is proposed in for each type of feature a parametric model is developed to characterize the local intensity in an image projections of intensity proﬁle onto a set of orthogonal zernike moment generating polynomials are used to esti mate model parameters and generate the feature map an interesting technique is to ﬁnd corners by ﬁtting a parameterized model with the generalized hough transform in images with extracted edges two lines appear in a parameter space for each corner and the peak occurs at the crossover real corner models in the form of templates were considered in a similarity measure and several alternative matching schemes were applied detection and localization accuracy was improved by merging the output of the diﬀerent matching techniques in general only relatively simple feature models were considered in the above methods and the generalization to images other than polyg onal is not obvious the complexity is also a major drawback in such approaches toward viewpoint invariant methods most of the detectors described so far extract features at a single scale determined by the internal parameters of the detector at the end of the as local features were more and more used in the context of wide baseline matching and object recognition there was a growing need for features that could cope with scale changes or even more general viewpoint changes multi scale methods most of the detectors described so far extract features at a single scale determined by the internal parameters of the detector to deal with scale changes a straightforward approach consists of extracting points over a range of scales and using all these points together to repre sent the image this is referred to as a multi scale or multi resolution approach in a scale adapted version of the harris operator was proposed interest points are detected at the local maxima of the harris function applied at several scales thanks to the use of normalized derivatives a comparable strength of the cornerness measure is obtained for points detected at diﬀerent scales such that a single threshold can be used to reject less signiﬁcant corners over all scales this scale adapted detector signiﬁcantly improves the repeatability of interest points under scale changes on the other hand when prior knowledge on the scale change between two images is given the detector can be adapted so as to extract interest points only at the selected scales this yields a set of points for which the respective localization and scale perfectly reﬂect the real scale change between the images in general multi scale approaches suﬀer from the same problems as dense sampling of features cf section they cannot cope well with the case where a local image structure is present over a range of scales which results in multiple interest points being detected at each scale within this range as a consequence there are many points which represent the same structure but with slightly diﬀerent localiza tion and scale the high number of points increases the ambiguity and the computational complexity of matching and recognition therefore eﬃcient methods for selecting accurate correspondences and verifying the results are necessary at further steps of the algorithms in contrast to structure from motion applications this is less of an issue in the context of recognition where a single point can have multiple correct matches scale invariant detectors to overcome the problem of many overlapping detections typical of multiscale approaches scale invariant methods have been intro duced these automatically determine both the location and scale of the local features features are typically circular regions in that case many existing methods search for maxima in the representation of an image x y and scale this idea for detecting local features in scale space was introduced in the early the pyramid repre sentation was computed with low pass ﬁlters a feature point is detected if it is at a local maximum of a surrounding cube and if its abso lute value is higher than a certain threshold since then many methods for selecting points in scale space have been proposed the existing approaches mainly diﬀer in the diﬀerential expression used to build the scale space representation a normalized log function was applied in to build a scale space representation and search for maxima the scale space representation is constructed by smoothing the high resolution image with derivatives of gaussian kernels of increasing size automatic scale selection cf section is performed by selecting local maxima in scale space the log operator is circularly symmetric it is therefore naturally invariant to rotation it is also well adapted for detecting blob like structures the experimental evaluation in shows this function is well suited for automatic scale selection the scale invariance of inter est point detectors with automatic scale selection has also been explored in corner detection and blob detection with automatic scale selec tion were also proposed in a combined framework in for feature tracking with adaptation to spatial and temporal size variations the interest point criterion that is being optimized for localization need not be the same as the one used for optimizing the scale in a scale invariant corner detector coined harris laplace and a scale invariant blob detector coined hessian laplace were introduced in these meth ods position and scale are iteratively updated until convergence more details can be found in sections and an eﬃcient algorithm for object recognition based on local extrema in the scale space pyramid built with dog ﬁlters was intro duced in the local extrema in the pyramid representation determine the localization and the scale of interest points this method is discussed further in section aﬃne invariant methods an aﬃne invariant detector can be seen as a generalization of the scale invariant ones to non uniform scaling and skew i e with a diﬀerent scaling factor in two orthogonal directions and without pre serving angles the non uniform scaling aﬀects not only the localiza tion and the scale but also the shape of characteristic local structures therefore scale invariant detectors fail in the case of signiﬁcant aﬃne transformations aﬃne invariant feature detection matching and recognition have been addressed frequently in the past here we focus on the methods which deal with invariant interest point detection one category of approaches was concerned with the localization accuracy under aﬃne and perspective transformations an aﬃne invari ant algorithm for corner localization was proposed in which builds on the observations made in aﬃne morphological multi scale anal ysis is applied to extract corners the evolution of a corner is given by a linear function formed by the scale and distance of the detected points from the real corner the location and orientation of the corner is com puted based on the assumption that the multiscale points move along the bisector line and the angle indicates the true location however in natural scenes a corner can take any form of a bi directional signal change and in practice the evolution of a point rarely follows the bisec tor the applicability of the method is therefore limited to a polygonal like world other approaches were concerned with simultaneous detection of location size and aﬃne shape of local structures the method intro duced in coined ebr edge based regions starts from harris corners and nearby intersecting edges two points moving along the edges together with the harris point determine a parallelogram the points stop at positions where some photometric quantities of the tex ture covered by the parallelogram reach an extremum the method can be categorized as a model based approach as it looks for a speciﬁc structure in images albeit not as strict as most methods described in section more details can be found in section a similar scheme has been explored in an intensity based method ibr intensity based regions was also proposed in it starts with the extraction of local inten sity extrema the intensity proﬁles along rays emanating from a local extremum are investigated a marker is placed on each ray in the place where the intensity proﬁle signiﬁcantly changes finally an ellipse is ﬁtted to the region determined by the markers this method is further discussed in section somewhat similar in spirit are the maximally stable extremal regions or mser proposed in and described in the next section a method to ﬁnd blob like aﬃne invariant features using an iter ative scheme was introduced in in the context of shape from texture this method based on the aﬃne invariance of shape adapted ﬁxed points was also used for estimating surface orientation from binoc ular data shape from disparity gradients the algorithm explores the properties of the second moment matrix and iteratively estimates the aﬃne deformation of local patterns it eﬀectively estimates the transformation that would project the patch to a frame in which the eigenvalues of the second moment matrix are equal this work provided a theoretical background for several other aﬃne invariant detectors it was combined with the harris corner detector and used in the context of matching in hand tracking in ﬁngerprint recog nition and for aﬃne rectiﬁcation of textured regions in in interest points are extracted at several scales using the harris detector and then the shape of the regions is adapted to the local image structure using the iterative procedure from this allows to extract aﬃne invariant descriptors for a given ﬁxed scale and loca tion that is the scale and the location of the points are not extracted in an aﬃne invariant way furthermore the multi scale har ris detector extracts many points which are repeated at the neighbor ing scale levels this increases the probability of a mismatch and the complexity the harris laplace detector introduced in was extended in by aﬃne normalization with the algorithm proposed in this detector suﬀers from the same drawbacks as the initial location and scale of points are not extracted in an aﬃne invariant way although the uniform scale changes between the views are handled by the scale invariant harris laplace detector beyond aﬃne transformations a scheme that goes even beyond aﬃne transformations and is invariant to projective transformations was introduced in however on a local scale the perspective eﬀect is usually neglectable more damaging is the eﬀect of non planarities or non rigid deformations this is why a theoretical framework to extend the use of local features to non planar surfaces has been proposed in based on the deﬁnition of equivalence classes however in prac tice they have only shown results on straight corners simultaneously an approach invariant to general deformations was developed in by embedding an image as a surface in space and exploiting geodesic distances segmentation based methods segmentation techniques have also been employed in the context of feature extraction these methods were either applied to ﬁnd homoge neous regions to localize junctions on their boundaries or to directly use these regions as local features for the generic feature extrac tion problem mostly bottom up segmentation based on low level pixel grouping was considered although in some speciﬁc tasks top down methods can also be applied although signiﬁcant progress has been made in the analysis and formalization of the segmentation problem it remains an unsolved problem in the general case optimal segmen tation is intractable in general due to the large search space of possi ble feature point groups in particular in algorithms based on multiple image cues moreover a multitude of deﬁnitions of optimal segmen tation even for the same image makes it diﬃcult to solve nonethe less several systems using segmentation based interest regions have been developed especially in the context of retrieval matching and recognition in early years of computer vision polygonal approximations of images were popular in scene analysis and medical image analysis these algorithms often involved edge detection and sub sequent edge following for region identiﬁcation in the vertices of a picture are deﬁned as those points which are common in three or more segmented regions it can be seen as one of the ﬁrst attempts to extract interest points using segmentation simple segmentation of patches into two regions is used in and the regions are compared to ﬁnd corners unfortunately the two region assumption makes the usefulness of the method limited another set of approaches represent real images through seg mentation well performing image segmentation methods are machine learning based methods based on graph cuts where graphs represent connected image pix els these methods allow to obtain segmentation at the required level of detail although semantic segmentation is not reliable over segmenting the image can produce many regions which ﬁt to the objects this approach was explored in and it is par ticularly appealing for image retrieval problems where the goal is to ﬁnd similar images via regions with similar properties in the goal is to create interest operators that focus on homogeneous regions and compute local image descriptors for these regions the segmentation is performed on several feature spaces using kernel based optimiza tion methods the regions can be individually described and used for recognition but their distinctiveness is low this direction has recently gained more interest and some approaches use bottom up segmentation to extract interest regions or so called superpixels see also section in general the disadvantages of this representation are that the segmentation results are still unstable and ineﬃcient for processing large amounts of images an approach which successfully deals with these problems was taken in maximally stable extremal regions mser are extracted with a watershed like segmentation algorithm the method extracts homogeneous intensity regions which are sta ble over a wide range of thresholds the regions are then replaced by ellipses with the same shape moments up to the second order recently a variant of this method was introduced in which handles the problems with blurred region boundaries by using region isophotes in a sense this method is also similar to the ibr method described in section as very similar regions are extracted more details on mser can be found in section the method was extended in with tree like representation of watershed evolution in the image machine learning based methods the progress in the domain of machine learning and the increase of available computational power allowed learning techniques to enter the feature extraction domain the idea of learning the attributes of local features from training examples and then using this information to extract features in other images has been around in the vision commu nity for some time but only recently it was more broadly used in real applications the success of these methods is due to the fact that eﬃ ciency provided by classiﬁers became a more desirable property than accuracy of detection in a neural network is trained to recognize corners where edges meet at a certain degree near to the center of an image patch this is applied to images after edge detection a similar idea was explored in to improve the stability of curvature measurement of digital curves decision trees have also been used successfully in interest point detection tasks the idea of using intensity diﬀerences between the cen tral points and neighboring points has been adopted in they construct a decision tree to classify point neigh borhoods into corners the main concern in their work is the eﬃciency in testing only a fraction of the many possible diﬀerences and the tree is trained to optimize that the approach of was also extended with log ﬁlters to detect multiscale points in they use a feature selection technique based on the repeatability of individual interest points over perspective projected images a hybrid methodology that integrates genetic algorithms and decision tree learning in order to extract discriminatory features for recognizing complex visual concepts is described in in inter est point detection is posed as an optimization problem they use a genetic programming based learning approach to construct operators for extracting features the problem of learning an interest point oper ator was posed diﬀerently in where human eye movement was studied to ﬁnd the points of ﬁxation and to train an svm classiﬁer one can easily generalize the feature detection problem to a classiﬁ cation problem and train a recognition system on image examples pro vided by one or a combination of the classical detectors any machine learning approach can be used for that haar like ﬁlters implemented with integral images to eﬃciently approximate multiscale derivatives were used in a natural extension would be to use the learning scheme from viola and jones successfully applied to face detec tion to eﬃciently classify interest points the accuracy of machine learning based methods in terms of local ization scale and shape estimation is in general lower than for the generic detectors but in the context of object recognition the eﬃciency is usually more beneﬁcial evaluations given the multitude of interest point approaches the need for indepen dent performance evaluations was identiﬁed early on and many exper imental tests have been performed over the last three decades various experimental frameworks and criteria were used one of the ﬁrst com parisons of corner detection techniques based on chain coded curves was presented in in the early papers very often only visual inspec tion was done others performed more quantitative evaluations providing scores for individual images or for small test data corner detectors were often tested on artiﬁcially generated images with diﬀerent types of junctions with varying angle length contrast noise blur etc diﬀerent aﬃne photometric and geometric transformations were used to generate the test data and to evaluate corner detectors in 128 this approach simpliﬁes the evaluation process but cannot model all the noise and deformations which aﬀect the detector performance in a real application scenario thus the performance results are often over optimistic a somewhat diﬀerent approach is taken in there performance comparison is approached as a general recognition problem corners are manually annotated on aﬃne transformed images and measures like consistency and accuracy similar to detection rate and recall are used to evaluate the detectors in sets of points are extracted from polyhedral objects and projective invariants are used to calculate a manifold of constraints on the coordinates of the corners they estimate the variance of the distance from the point coordinates to this manifold independently of camera parameters and object pose nonlinear diﬀusion was used to remove the noise and the method from performed better than the one proposed in the idea of using planar invariants is also explored in to evaluate corner detectors based on edges theoretical properties of features and localization accuracy were also tested in based on a parametric l corner model to evaluate localization accuracy also a randomized generator of corners has been used to test the localization error state of the art curve based detectors 197 are eval uated in a quantitative measure of the quality of the detected dominant points is deﬁned as the pointwise error between the digital curve and the polygon approximated from interest points the perfor mance of the proposed scale adapted approach is reported better than of the other methods the repeatability rate and information content measures were intro duced in they consider a point in an image interesting if it has two main properties distinctiveness and invariance this means that a point should be distinguishable from its immediate neighbors more over the position as well as the selection of the interest point should be invariant with respect to the expected geometric and radiometric distortions from a set of investigated detectors harris and a corner later described as susan perform best systematic evaluation of several interest point detectors based on repeatability and information content measured by the entropy of the descriptors was performed in the evaluation shows that a mod iﬁed harris detector provides the most stable results on image pairs with diﬀerent geometric transformations the repeatability rate and information content in the context of image retrieval were also eval uated in to show that a wavelet based salient point extraction algorithm outperforms the harris detector consistency of the number of corners and accuracy criteria were introduced as evaluation criteria in this overcomes the problems with the repeatability criterion of favoring detectors providing more features the introduced criterion instead favors detectors which pro vide similar number of points regardless of the object transformation even though the number of details in the image changes with scale and resolution several detectors are compared with the best performance reported for a modiﬁed implementation of tracking and the number of frames over which the corners are detected during tracking was used to compare detectors in similarly bae et al uses correlation and matching to ﬁnd repeated corners between frames and compare their numbers to the reference frame extensive evaluation of commonly used feature detectors and descriptors has been performed in the repeatability on image pairs representing planar scenes related by various geometric transformations was computed for diﬀerent state of the art scale and aﬃne invariant detectors the mser region detector based on watershed segmentation showed the highest accuracy and stability on various structured scenes the data collected by mikolajczyk and tuytelaars became a standard benchmark for evaluating interest point detectors and descriptors recently the performance of feature detectors and descriptors from has been investigated in in the context of matching object features across viewpoints and lighting condi tions a method based on intersecting epipolar constraints provides ground truth correspondences automatically in this evaluation the aﬃne invariant detectors introduced in are most robust to view point changes dog detector from was reported the best in a similar evaluation based on images of natural scenes in feature detectors were also evaluated in the context of recognition in using object category training data where direct corre spondence cannot be automatically veriﬁed clustering properties and compactness of feature clusters were measured in some speciﬁc recognition tasks like pedestrian detection were also used to compare the performance of diﬀerent features in see vgg research aﬃne corner detectors a large number of corner detector methods have been proposed in the literature to guide the reader in ﬁnding an approach suitable for a given application representative methods have been selected based on the underlying extraction technique e g based on image derivatives morphology or geometry as well as based on the level of invariance translations and rotations scale or aﬃne invariant for each cate gory we describe the feature extraction process for some of the best performing and representative methods introduction it is important to note that the term corner as used here has a speciﬁc meaning the detected points correspond to points in the image with high curvature these do not necessarily correspond to projec tions of corners corners are found at various types of junctions on highly textured surfaces at occlusion boundaries etc for many prac tical applications this is suﬃcient since the goal is to have a set of stable and repeatable features whether these are true corners or not is considered irrelevant we begin this section with a derivatives based approach the harris corner detector described in section next we explain the basic ideas of the susan detector section which is an example of a method based on eﬃcient morphological operators we then move on to detectors with higher levels of invariance starting with the scale and aﬃne invariant extensions of the harris detector harris laplace and harris aﬃne section this is followed by a discussion of edge based regions in section finally we conclude the section with a short discussion section harris detector the harris detector proposed by harris and stephens is based on the second moment matrix also called the auto correlation matrix which is often used for feature detection and for describing local image structures this matrix describes the gradient distribution in a local neighborhood of a point m with g σi x σd ix x σd iy x σd ix x σd iy x σd x σd ix x σd xg σd i x g σ e the local image derivatives are computed with gaussian kernels of scale σd the diﬀerentiation scale the derivatives are then averaged in the neighborhood of the point by smoothing with a gaussian win dow of scale σi the integration scale the eigenvalues of this matrix represent the principal signal changes in two orthogonal directions in a neighborhood around the point deﬁned by σi based on this property corners can be found as locations in the image for which the image sig nal varies signiﬁcantly in both directions or in other words for which both eigenvalues are large in practice harris proposed to use the fol lowing measure for cornerness which combines the two eigenvalues in a single measure and is computationally less expensive cornerness det m λ trace m with det m the determinant and trace m the trace of the matrix m a typical value for λ is since the determinant of a matrix is equal to the product of its eigenvalues and the trace corresponds to the sum it is clear that high values of the cornerness measure corre spond to both eigenvalues being large adding the second term with the trace reduces the response of the operator on strong straight con tours moreover computing this measure based on the determinant and the trace is computationally less demanding than actually com puting the eigenvalues this seems less relevant now but it was impor tant back in when the computational resources were still very limited subsequent stages of the corner extraction process are illustrated in figure given the original image i x y upper left the ﬁrst step consists of computing the ﬁrst order derivatives ix and iy lower left next one takes the product of these gradient images lower right then the images are smoothed with a gaussian kernel these fig illustration of the components of the second moment matrix and harris cornerness measure images contain the diﬀerent elements of the hessian matrix which are then in a ﬁnal step combined into the cornerness measure following equation upper right when used as an interest point detector local maxima of the cor nerness function are extracted using non maximum suppression such points are translation and rotation invariant moreover they are stable under varying lighting conditions in a comparative study of diﬀer ent interest point detectors the harris corner was proven to be the most repeatable and most informative additionally they can be made very precise sub pixel precision can be achieved through quadratic approximation of the cornerness function in the neighbor hood of a local maximum discussion figure shows the corners detected with this measure for two exam ple images related by a rotation note that the features found corre spond to locations in the image showing two dimensional variations in the intensity pattern these may correspond to real corners but the detector also ﬁres on other structures such as t junctions points with high curvature etc this equally holds for all other corner detectors described in this chapter when true corners are desirable model based approaches are certainly more appropriate fig harris corners detected on rotated image examples as can be seen in the ﬁgure many but not all of the features detected in the original image left have also been found in the rotated version right in other words the repeatability of the harris detector under rotations is high additionally features are typically found at locations which are informative i e with a high variability in the intensity pattern this makes them more discriminative and easier to bring into correspondence susan detector the susan corner detector has been introduced by smith and brady and relies on a diﬀerent technique rather than evaluat ing local gradients which might be noise sensitive and computationally more expensive a morphological approach is used susan stands for smallest univalue segment assimilating nucleus and is a generic low level image processing technique which apart from corner detection has also been used for edge detection and noise suppression the basic principle goes as follows see also figure for each pixel in the image we consider a circular neigh borhood of ﬁxed radius around it the center pixel is referred to as the nucleus and its intensity value is used as reference then all other fig susan corners are detected by segmenting a circular neighborhood into similar orange and dissimilar blue regions corners are located where the relative area of the similar region usan reaches a local minimum below a certain threshold susan detector pixels within this circular neighborhood are partitioned into two cat egories depending on whether they have similar intensity values as the nucleus or diﬀerent intensity values in this way each image point has associated with it a local area of similar brightness coined usan whose relative size contains important information about the structure of the image at that point see also figure in more or less homo geneous parts of the image the local area of similar brightness covers almost the entire circular neighborhood near edges this ratio drops to and near corners it decreases further to about hence corners can be detected as locations in the image where the number of pixels with similar intensity value in a local neighborhood reaches a local min imum and is below a predeﬁned threshold to make the method more robust pixels closer in value to the nucleus receive a higher weighting moreover a set of rules is used to suppress qualitatively bad features local minima of the susans smallest usans are then selected from the remaining candidates an example of detected susan corners is shown in figure discussion the features found show a high repeatability for this artiﬁcially rotated set of images however many of the features are located on edge structures and not on corners for such points the localization fig susan corners found for our example images is sensitive to noise moreover edge based points are also less discriminative the two detectors described so far are invariant under translation and rotation only this means that corners will be detected at cor responding locations only if the images are related by a translation and or rotation in the next sections we will describe detectors with higher levels of viewpoint invariance that can withstand scale changes or even aﬃne deformations apart from better matching across trans formed images these also bring the advantage of detecting features over a range of scales or shapes alternatively this eﬀect can be obtained by using a multiscale approach in that case a detector which is not scale invariant is applied to the input image at diﬀerent scales i e after smoothing and sampling harris laplace aﬃne mikolajczyk and schmid developed both a scale invariant corner detec tor referred to as harris laplace as well as an aﬃne invariant one referred to as harris aﬃne harris laplace harris laplace starts with a multiscale harris corner detector as ini tialization to determine the location of the local features the charac teristic scale is then determined based on scale selection as proposed by lindeberg et al the idea is to select the characteristic scale of a local structure for which a given function attains an extremum over scales see figure the selected scale is characteristic in the quan titative sense since it measures the scale at which there is maximum similarity between the feature detection operator and the local image structures the size of the region is therefore selected independently of the image resolution for each point as the name harris laplace sug gests the laplacian operator is used for scale selection this has been shown to give the best results in the experimental comparison of as well as in these results can be explained by the circular shape fig example of characteristic scales the top row shows images taken with diﬀerent zoom the bottom row shows the responses of the laplacian over scales for two corre sponding points the characteristic scales are and for the left and right images respectively the ratio of scales corresponds to the scale factor between the two images the radius of displayed regions in the top row is equal to times the selected scales of the laplacian kernel which acts as a matched ﬁlter when its scale is adapted to the scale of a local image structure figure shows the scale invariant local features obtained by applying the harris laplace detector for two images of the same scene related by a scale change in order not to overload the images only some of the corresponding regions that were detected in both images fig corresponding features found with the harris laplace detector only a subset of corresponding features is displayed to avoid clutter the circles indicate the scale of the features are shown a similar selection mechanism has been used for all subse quent image pairs shown in this survey harris aﬃne given a set of initial points extracted at their characteristic scales based on the harris laplace detection scheme the iterative estimation of elliptical aﬃne regions as proposed by lindeberg et al allows to obtain aﬃne invariant corners instead of circular regions these are ellipses the procedure consists of the following steps detect the initial region with the harris laplace detector estimate the aﬃne shape with the second moment matrix normalize the aﬃne region to a circular one re detect the new location and scale in the normalized image go to step if the eigenvalues of the second moment matrix for the new point are not equal the iterations are illustrated in figure fig iterative detection of an aﬃne invariant interest point in the presence of an aﬃne transformation top and bottom rows the ﬁrst column shows the points used for initial ization the consecutive columns show the points and regions after iterations and note that the regions converge after iterations to corresponding image regions fig diagram illustrating the aﬃne normalization using the second moment matrices image coordinates are transformed with matrices m and m l r the eigenvalues of the second moment matrix see equation are used to measure the aﬃne shape of the point neighborhood more precisely we determine the transformation that projects the intensity pattern of the point neighborhood to one with equal eigenvalues this transformation is given by the square root of the second moment matrix m it can be shown that if the neighborhoods of two points xr and xl are related by an aﬃne transformation then their normalized ver sions xr m and xl m are related by a simple rota r r l l tion x l rx r this process is illustrated in figure the matrices ml and mr computed in the normalized frames are rotation matrices as well note that rotation preserves the eigenvalue ratio for an image patch therefore the aﬃne deformation can be determined only up to a rotation factor the estimation of aﬃne shape can be applied to any initial point given that the determinant of the second moment matrix is larger than zero and the signal to noise ratio is suﬃciently large we can therefore use this technique to estimate the shape of initial regions provided by the harris laplace detector the output of the harris aﬃne detector on two images of the same scene is shown in figure apart from the scale also the shape of the regions is now adapted to the underlying intensity patterns so as fig harris aﬃne regions generated for two diﬀerent views of a planar scene subset in spite of the aﬃne deformation the region shapes clearly correspond to ensure that the same part of the object surface is covered in spite of the deformations caused by the viewpoint change edge based regions a more heuristic technique to obtain aﬃne invariance is to exploit the geometry of the edges that can usually be found in the proximity of a harris corner such a method has been proposed by tuytelaars and van gool the rationale behind this approach is that edges are typically rather stable image features that can be detected over a range of viewpoints scales and illumination changes moreover by exploiting the edge geometry the dimensionality of the problem can be signiﬁcantly reduced indeed as will be shown next the search problem over all possible aﬃnities or once the center point is ﬁxed can be reduced to a one dimensional problem by exploiting the nearby edges geometry in practice we start from a harris corner point p see section and a nearby edge extracted with the canny edge detector to increase the robustness to scale changes these basic features are extracted at multiple scales two points and move away from the corner in both directions along the edge as shown in figure their relative speed is coupled through the equality of relative aﬃne invariant parameters and li r abs pi si p pi si dsi edge based regions q p fig the edge based region detector starts from a corner point p and exploits nearby edge information with si an arbitrary curve parameter in both directions i pi si the ﬁrst derivative of pi si with respect to si abs the abso lute value and the determinant this condition prescribes that the areas between the joint p and the edge and between the joint p and the edge remain identical from now on we simply use l when referring to for each value l the two points l and l together with the corner p deﬁne a parallelogram ω l the parallelogram spanned by the vectors l p and l p see figure this yields a one dimensional family of parallelogram shaped regions as a function of l from this family one or a few parallelogram are selected for which the following photometric quantities of the texture go through an extremum inv abs pg pg m p p m m m p p m m m with n in x y xpyq dxdy ω pg fig originally detected region shapes for the edge based regions subset with m n the nth order p q th degree moment computed over the region ω l pg the center of gravity of the region weighted with inten sity i x y and q the corner of the parallelogram opposite to the corner point p see figure the second factor in these formula has been added to ensure invariance under an intensity oﬀset for straight edges l along the entire edge in that case the two photometric quantities given in equation are combined and locations where both functions reach a minimum value are taken to ﬁx the parameters and moreover instead of relying on the harris corner detection the straight lines intersection point can be used instead examples of detected regions are displayed in figure from parallelograms to ellipses note that the regions found with this method are parallelograms this is in contrast to many other aﬃne invariant detectors for example those based on the second moment matrix for which the output shape is an ellipse for uniformity and convenience in comparison it is some times advantageous to convert these parallelogram shaped regions into ellipses this can be achieved by selecting an ellipse with the same ﬁrst and second order moments as the originally detected region which is an aﬃne covariant construction method the elliptical regions generated with this procedure are shown in figure note though that some discussion fig edge based regions generated for the two example images represented with ellipses subset information is lost during this conversion as ellipses have a rotational degree of freedom which was ﬁxed in the original representation discussion several methods for corner detection have been described in this chap ter as discussed earlier corner based features do not necessarily cor respond to real corners in the world indeed the goal is to extract stable features that can be matched well in spite of changes in viewing conditions the harris detector was identiﬁed as the most stable one in many independent evaluations there are also multi scale as well as scale and aﬃne invariant extensions of this approach it is a convenient tool for providing a large number of features alternatively the susan detector can be used it is more eﬃcient but also more sensitive to noise an optimized susan detector using machine learn ing techniques is described in section as discussed in sections and contour based corner detectors are suitable for line drawing images but in natural scenes intensity based methods are typically more stable it is important to note that the aﬃne transformation model only holds for viewpoint changes in case of locally planar regions and assum ing the camera is relatively far from the object however corners are often found near object boundaries as this is where the intensity change usually occurs hence the region extraction process is often based on measurements on non planar structures e g including background or another facet of the object in these cases the viewpoint invariance will be limited and also the robustness to background changes will be aﬀected a possible way out has been indicated in the work of detectors that search for region boundaries like ebr are less aﬀected by this phenomenon the measurement regions can then be delimited by the detected contours thus excluding the non planar parts in many practical situations on the positive side compared to other types of features corners are typically better localized in the image plane this localization accuracy can be important for some applications e g for camera calibration or reconstruction their scale however is not well deﬁned as a corner structure changes very little over a wide range of scales the reason why scale selection still works with the harris detector is that the feature point is localized not exactly on the corner edge but slightly inside the corner structure blob detectors after corners the second most intuitive local features are blobs as it was the case in the previous section we select a few methods that have proved successful in many applications and describe these in more detail these methods typically provide complementary features to the ones discussed in the previous chapter we start with a derivative based method the hessian detector section next we consider the scale invariant and aﬃne invariant extensions of this method coined hessian laplace and hessian aﬃne section finally we describe the salient region detector which is based on the entropy of the inten sity probability distribution section we conclude the chapter with a short discussion hessian detector the second matrix issued from the taylor expansion of the image intensity function i x is the hessian matrix ixx x σd ixy x σd ixy x σd iyy x σd with ixx etc second order gaussian smoothed image derivatives these encode the shape information by describing how the normal to an isosurface changes as such they capture important properties of local image structure particularly interesting are the ﬁlters based on the determinant and the trace of this matrix the latter is often referred to as the laplacian local maxima of both measures can be used to detect blob like structures in an image the laplacian is a separable linear ﬁlter and can be approximated eﬃciently with a diﬀerence of gaussians dog ﬁlter the lapla cian ﬁlters have one major drawback in the context of blob extrac tion though local maxima are often found near contours or straight edges where the signal change is only in one direction these maxima are less stable because their localization is more sensitive to noise or small changes in neighboring texture this is mostly an issue in the context of ﬁnding correspondences for recovering image transformations a more sophisticated approach solving this prob lem is to select a location and scale for which the trace and the determinant of the hessian matrix simultaneously assume a local extremum this gives rise to points for which the second order derivatives detect signal changes in two orthogonal directions a similar idea is explored in the harris detector albeit for ﬁrst order derivatives only the feature detection process based on the hessian matrix is illus trated in figure given the original image upper left one ﬁrst computes the second order gaussian smoothed image derivatives lower part which are then combined into the determinant of the hessian upper right the interest points detected with the determinant of the hessian for an example image pair are displayed in figure the second order derivatives are symmetric ﬁlters thus they give weak responses exactly in the point where the signal change is most signiﬁcant therefore the maxima are localized at ridges and blobs for which the size of the gaussian kernel σd matches by the size of the blob structure hessian laplace aﬃne the hessian laplace and hessian aﬃne detectors are similar in spirit as their harris based counterparts harris laplace and harris aﬃne hessian laplace aﬃne fig illustration of the components of the hessian matrix and hessian determinant fig output of the hessian detector applied at a given scale to example images with rotation subset described in section except that they start from the determinant of the hessian rather than the harris corners this turns the methods into viewpoint invariant blob detectors they have also been proposed by mikolajczyk and schmid and are complementary to their harris based counterparts in the sense that they respond to a diﬀerent type of feature in the image an example of the detection result is shown in figures and for the scale invariant hessian laplace and aﬃne invariant hessian aﬃne respectively fig output of hessian laplace detector applied to example images with scale change subset fig hessian aﬃne regions generated for two views of the example scene subset like in the harris based detector the number of regions found with the hessian laplace detector can be controlled by thresholding the hessian determinant as well as the laplacian response typically a large number of features can be extracted resulting in a good cover age of the image which is one of the advantages of the hessian based detector furthermore this detector also responds to some corner structures at ﬁne scale see figure the returned locations however are more suitable for scale estimation than the harris points due to the use of similar ﬁlters for spatial and scale localization both based on second order gaussian derivatives one of the possible extensions of this work salient regions is to explore the hessian matrix to use additional shape information encoded by the eigenvalues of this matrix salient regions rather than building on the derivative information in the image the salient region detector proposed by kadir and brady is inspired by information theory the basic idea behind this feature detector is to look for salient features where saliency is deﬁned as local complexity or unpredictability it is measured by the entropy of the probability dis tribution function of intensity values within a local image region how ever looking at entropy alone does not suﬃce to accurately localize the features over scales so as an additional criterion the self dissimilarity in scale space of the feature is added as an extra weighting function favoring well localized complex features detection proceeds in two steps ﬁrst at each pixel x the entropy of the probability distribution p i is evaluated over a range of scales h p i log p i i the probability distribution p i is estimated empirically based on the intensity distribution in a circular neighbourhood of radius around x local maxima of the entropy are recorded these are candi date salient regions second for each of the candidate salient regions the magnitude of the derivative of p i with respect to scale is computed as p i the saliency y is then computed as y wh the candidate salient regions over the entire image are ranked by their saliency and the top p ranked regions are retained also an aﬃne invariant version of the detector has been proposed where local maxima over the scale and the shape parameters orien tation θ and ratio of major to minor axes λ of an elliptical region fig salient regions found for the two example images related to a change in viewpoint subset are sought simultaneously however this seriously slows down the computation examples of detected regions using the aﬃne invariant version are displayed in figure more details about this method can be found in discussion because of the weighting factor measuring the self dissimilarity over scale the detector typically ﬁres on blob like structures in the image that is why we have catalogued the method as a blob detector but note that in contrast to other blob detectors the contrast of the blobs does not have any inﬂuence on the detection the number of features found with this method is typically rela tively low unlike for many other detectors the ranking of the extracted features is meaningful due to the entropy based criteria with the ones from the top the most stable this property has been explored in the context of category level object recognition and especially in combi nation with classiﬁers where the complexity largely depends on the number of features e g discussion blob detectors have been used widely in diﬀerent application domains apart from the methods described above also dog diﬀerence of discussion gaussians and surf speeded up robust features can be catalogued as blob detectors however since their extraction processes are focussed on eﬃciency we postpone their discussion until section some of the methods described in section also share common characteristics with blob detectors especially ibr intensity based regions and mser maximally stable extremal regions often ﬁnd blob like structures in the image however apart from blob like structures they also detect other more irregularly shaped patterns which we consider their distinctive property blob detectors are in a sense complementary to corner detectors as a result they are often used together by using several complemen tary feature detectors the image is better covered and the performance becomes less dependent on the actual image content this has been exploited e g in in general blob like structures tend to be less accurately localized in the image plane than corners although their scale and shape are bet ter deﬁned than for corners the location of a corner can be identiﬁed by a single point while blobs can only be localized by their boundaries which are often irregular on the other hand the scale estimation of a corner is ill deﬁned as for example an intersection of edges exists at a wide range of scales the boundaries of a blob however even if irregu lar give a good estimate of the size thus scale of the blob this makes them less suited for e g camera calibration or reconstruction for object recognition on the other hand a precise image localization is often not necessary since the entire recognition process is very noisy a robust descriptor such as sift can match such features nev ertheless the scales of matched blobs allow then to hypothesize the size of the objects which makes them very useful in recognition applications finally the number of features detected with the methods described above varies greatly there is often just a few tens of salient regions found in an image whereas the hessian laplace or hessian aﬃne methods allow to extract up to several hundreds or thousands of fea tures depending on the application and algorithms used either case can be advantageous we refer to section for a further discussion on this issue region detectors in this chapter we discuss a number of feature detectors which directly or indirectly are concerned with extraction of image regions first we describe the intensity based regions section followed by maximally stable extremal regions section at the end we discuss superpix els section these regions are provided by diﬀerent methods but focus on similar image structures and share similar properties super pixels are traditionally not considered as local features and have limited robustness to changes in viewing conditions but they are currently more and more used in the context of image recognition we therefore include all the above features in the same category intensity based regions here we describe a method proposed by tuytelaars and van gool to detect aﬃne invariant regions it starts from intensity extrema detected at multiple scales and explores the image around them in a radial way delineating regions of arbitrary shape which are then replaced by ellipses more precisely given a local extremum in intensity the intensity function along rays emanating from the extremum is studied see i t t f t t t fig construction of intensity based regions figure the following function is evaluated along each ray f t abs i t max t abs i t dt with t an arbitrary parameter along the ray i t the intensity at posi tion t the intensity value at the extremum and d a small number which has been added to prevent a division by zero the point for which this function reaches an extremum is invariant under aﬃne geo metric and linear photometric transformations given the ray typi cally a maximum is reached at positions where the intensity suddenly increases or decreases the function f t is in itself already invari ant nevertheless points are selected where this function reaches an extremum to make a robust selection next all points corresponding to maxima of f t along rays originating from the same local extremum are linked to enclose an aﬃne invariant region this often irregularly shaped region is replaced by an ellipse having the same shape moments up to the second order this ellipse ﬁtting is an aﬃne covariant con struction an example of regions detected with this method is shown in figure maximally stable extremal regions mser or maximally stable extremal regions have been proposed by matas et al a maximally stable extremal region is a con nected component of an appropriately thresholded image the word extremal refers to the property that all pixels inside the mser have fig intensity based regions found for the graﬃti images subset either higher bright extremal regions or lower dark extremal regions intensity than all the pixels on its outer boundary the maximally stable in mser describes the property optimized in the threshold selection process the set of extremal regions i e the set of all connected compo nents obtained by thresholding has a number of desirable properties first a monotonic change of image intensities leaves unchanged sec ond continuous geometric transformations preserve topology pixels from a single connected component are transformed to a single con nected component finally there are no more extremal regions than there are pixels in the image so a set of regions was deﬁned that is preserved under a broad class of geometric and photometric changes and yet has the same cardinality as e g the set of ﬁxed sized square windows commonly used in narrow baseline matching the enumeration of the set of extremal regions is very eﬃcient almost linear in the number of image pixels the enumeration proceeds as follows first pixels are sorted by intensity after sorting pixels are marked in the image either in decreasing or increasing order and the list of growing and merging connected components and their areas is maintained using the union ﬁnd algorithm during the enumer ation process the area of each connected component as a function of intensity is stored among the extremal regions the maximally sta ble ones are those corresponding to thresholds for which the relative area change as a function of relative change of threshold is at a local minimum in other words the mser are the parts of the image where local binarization is stable over a large range of thresholds the deﬁ nition of mser stability based on relative area change is invariant to aﬃne transformations both photometrically and geometrically detection of mser is related to thresholding since every extremal region is a connected component of a thresholded image however no global or optimal threshold is sought all thresholds are tested and the stability of the connected components evaluated the output of the mser detector is not a binarized image for some parts of the image multiple stable thresholds exist and a system of nested subsets is output in this case for many of the aﬃne invariant detectors the output shape is an ellipse however for mser it is not examples of the original regions detected are given in figure using the same procedure as explained above for the ibr an ellipse can be ﬁtted based on the ﬁrst and second shape moments this results in a set of features as shown in figure alternatively a local aﬃne frame can be deﬁned based on a set of sta ble points along the region contour this provides an alternative scheme to normalize the region against aﬃne deformations discussion the mser features typically anchor on region boundaries thus the resulting regions are accurately localized compared to other fig regions detected with mser on the graﬃti images subset fig final mser regions for the graﬃti images subset blob detectors the method works best for structured images which can be segmented well ideally an image with uniform regions separated by strong intensity changes on the downside it was found to be sensi tive to image blur which can be explained by the fact that image blur undermines the stability criterion this issue was addressed in its recent extension in the method is also relatively fast it is cur rently the most eﬃcient among the aﬃne invariant feature detectors it has been used mostly for recognizing or matching speciﬁc objects e g and showed lower performance for object class recog nition segmentation based methods superpixels the two methods described above extract small regions whose intensity patterns clearly stand out with respect to their immediate surround ings this is reminiscent of traditional image segmentation techniques however image segments are typically relatively large too large in fact to be used as local features by increasing the number of segments a new image representation can be obtained where the image segments typically have the right trade oﬀ between locality and distinctiveness required in most local features based applications see figure this low level grouping of pixels into atomic regions has been advocated by mori et al and ren and malik who refer to the resulting atomic regions as superpixels this terminology refers to the fact that segmentation based methods superpixels fig superpixels generated for the example image superpixels can be considered as a more natural and perceptually more meaningful alternative for the original image pixels in superpixels are extracted from the image using nor malized cuts but any data driven segmentation methods can be used here the normalized cuts based approach is a classical image segmentation algorithm which exploits pairwise brightness color or texture aﬃnities between pixels to enforce locality only local connec tions are taken into account when constructing the aﬃnity matrix an example of superpixels is shown in figure in contrast to traditional local features by construction superpixels cover the entire image and do not overlap multiple segmentations can also be used to increase the possibility of object boundaries coinciding with boundaries between adjacent superpixels except for small contour details and invisible contours all superpixels extracted from an image have similar scale so the method is not scale invariant an alternative construction method based on constrained delauney triangulation has been proposed to obtain robustness against scale change these features are less suited for matching or object recognition as the regions are uniform therefore not discriminative and the repeata bility of boundary extraction is low they have been used successfully for modeling and exploiting mid level visual cues such as curvilinear continuity region grouping or ﬁgure ground organization for semantic image segmentation discussion the local features detected with the methods described above typi cally represent homogeneous regions while this is acceptable for the detection step it may incur problems for the later description and matching indeed homogeneous regions lack distinctiveness fortu nately this can easily be overcome by increasing the measurement region in other words we use a larger scale region to compute the descriptor such that it also contains part of the surrounding image structures and captures the shape of the region boundary this usually suﬃces to increase the discriminative power and match regions between images intensity based regions and maximally stable extremal regions typ ically give very similar features these methods are therefore not com plementary ibr may break down when the region is non convex but it is more robust to small gaps in the region contour mser on the other hand has been shown to be relatively sensitive to image blur in as this directly aﬀects the stability criterion this problem has been recently addressed in however apart from the case of image blur mser scores best with respect to repeatability in as discussed earlier region detectors often detect blob like struc tures although they are not restricted to this type of regions as a result they are less complementary to blobs then to corners region based detectors are typically quite accurate in their local ization they work especially well for images with a well structured scene clearly delineated regions such as images containing objects with printed surfaces buildings etc even though superpixels share some characteristics with the other region detectors they are not the same they are non overlapping and cover the entire image their repeatability suﬀers from the weak robustness of the segmentation methods most importantly they have been developed in a diﬀerent context where the idea is to speed up the image analysis by focussing on the superpixels only instead of ana lyzing all pixels superpixels are hence considered as a bigger equiv alent of pixels which can be described to a ﬁrst approximation by discussion a single intensity or color value this is in contrast to local features which should be distinctive and in the ideal case uniquely identiﬁable however using region boundaries to build distinctive descriptors may overcome the occlusion problem from which traditional interest points suﬀer eﬃcient implementations most feature detectors described so far involve the computation of derivatives or more complex measures such as the second moment matrix for the harris detector or entropy for the salient regions detec tor since this step needs to be repeated for each and every location in feature coordinate space which includes position scale and shape this makes the feature extraction process computationally expensive thus not suitable for many applications in this section we describe several feature detectors that have been developed with computational eﬃciency as one of the main objectives the dogs detector approximates the laplacian using multiple scale space pyramids see section surf makes use of integral images to eﬃciently compute a rough approximation of the hessian matrix section fast evaluates only a limited number of individual pixel intensities using decision trees see section diﬀerence of gaussians the diﬀerence of gaussians detector or dog for short has been pro posed in it is a scale invariant detector which diﬀerence of gaussians extracts blobs in the image by approximating the laplacian see also section based on the diﬀusion equation in scale space theory it can be shown that the laplacian corresponds to the derivative of the image in the scale direction since the diﬀer ence between neighboring points in a given direction approximates the derivative in this direction the diﬀerence between images at diﬀerent scales approximates the derivative with respect to scale furthermore gaussian blurring is often applied to generate images at various scales hence the dog images produce responses which approximate the log the computation of second order derivatives in x and y directions is then avoided as illustrated in figure the actual computation scheme is illustrated in figure the image is smoothed several times with a gaussian convolution mask these smoothed versions are combined pairwise to compute a set of dog blob response maps local maxima in these maps are located both over space and over scales with non maximal suppression and the locations are further reﬁned with quadratic interpolation after a few smoothing steps the image can be subsampled to process the next octave since the laplacian gives strong response on edges an additional ﬁltering step is added where the eigenvalues of the full hessian matrix are computed and their strengths evaluated this ﬁltering step does not aﬀect the overall processing time too much as it is only needed for a limited number of image locations and scales the dog features detected in our example images are shown in figure several frames per second can be processed with this method fig the laplacian can be approximated as a diﬀerence of two gaussian smoothed images fig overview of the dog detection scheme fig local features detected with the dog detector surf speeded up robust features in the context of realtime face detection viola and jones have proposed to use integral images which allow for very fast computation of haar wavelets or any box type convolution ﬁlter first we will describe the basic idea of integral images then we show how this technique can be used to obtain a fast approximation of the hessian matrix as used in surf speeded up robust features integral images the entry of an integral image iς x at a location x x y represents the sum of all pixels in the input image i of a rectangular region formed fig using integral images it takes only four operations to calculate the area of a rectangular region of any size by the origin and x i x j y iς x i i j i j once the integral image has been computed it takes four additions to calculate the sum of the intensities over any upright rectangular area as shown in figure moreover the calculation time is inde pendent of the size of the rectangular area surf surf or speeded up robust features have been proposed by bay et al it is a scale invariant feature detector based on the hessian matrix as is e g the hessian laplace detector see sec tion however rather than using a diﬀerent measure for selecting the location and the scale the determinant of the hessian is used for both the hessian matrix is roughly approximated using a set of box type ﬁlters and no smoothing is applied when going from one scale to the next gaussians are optimal for scale space analysis but in practice they have to be discretized figure left which introduces artifacts in particular in small gaussian kernels surf pushes the approximation even further using the box ﬁlters as shown in the right fig left to right the discretised and cropped gaussian second order partial derivative in y direction and xy direction respectively surf box ﬁlter approximation for the second order gaussian partial derivative in y direction and xy direction the gray regions are equal to zero half of figure these approximate second order gaussian deriva tives and can be evaluated very fast using integral images indepen dently of their size surprisingly in spite of the rough approximations the performance of the feature detector is comparable to the results obtained with the discretized gaussians box ﬁlters can produce a suf ﬁcient approximation of the gaussian derivatives as there are many other sources of signiﬁcant noise in the processing chain the box ﬁlters in figure are approximations for a gaussian with σ and represent the ﬁnest scale i e highest spatial resolu tion we will denote them by dxx dyy and dxy the weights applied to the rectangular regions are kept simple for computational eﬃciency but we need to further balance the relative weights in the expression for the hessian determinant with lxy f dxx yy f for the smallest scale where lxx yy f dxy f x f is the frobenius norm this yields det happrox dxxdyy the approximated determinant of the hessian represents the blob response in the image at location x these responses are stored in a blob response map and local maxima are detected and reﬁned using quadratic interpolation as with dog see section figure shows the result of the surf detector for our example images surf has been reported to be more than ﬁve times faster than dog fast features from accelerated segment test the fast detector introduced by rosten and drummond in builds on the susan detector previously discussed in section susan computes the fraction of pixels within a neighborhood which fig local features detected with the surf detector have similar intensity to the center pixel this idea is taken further by fast which compares pixels only on a circle of ﬁxed radius around the point the test criterion operates by considering a circle of pix els around the corner candidate see figure initially pixels and are compared with a threshold then and as well as the remain ing ones at the end the pixels are classiﬁed into dark similar and brighter subsets the algorithm from is used to select the pixels which yield the most information about whether the candidate pixel is a corner this is measured by the entropy of the positive and fig illustration of pixels examined by the fast detector fig local features detected with the fast detector negative corner classiﬁcation responses based on this pixel the pro cess is applied recursively on all three subsets and terminates when the entropy of a subset is zero the decision tree resulting from this partitioning is then converted into c code creating a long string of nested if then else statements which is compiled and used as a corner detector finally non maxima suppression is applied on the sum of the absolute diﬀerence between the pixels in the circle and the center pixel this results in a very eﬃcient detector which is up to times faster than the dog detector discussed in section albeit not invariant to scale changes the fast features found in our example images are displayed in figure an extension to a multi scale detector by scale selection with the laplacian function was proposed in they estimate the laplacian using gray level diﬀerences between pixels on the circle and the central one and retain only the locations where this estimate is largest this proves to be suﬃcient to produce a large number of keypoint candidates from which the unstable ones are ﬁltered out during the recognition process discussion the ultimate goal of methods focussing on eﬃciency is often realtime processing of a video stream or dealing with large amounts of data discussion however to some extent this is a moving target computation power increases rapidly over time but so does the number of features we extract or the size of the databases we deal with moreover feature detection is not the ﬁnal goal but just the ﬁrst step in a processing chain followed by matching tracking object recognition etc in many applications signiﬁcant performance improvement can be obtained just by increasing the number of training examples eﬃciency is therefore one of the major properties equally important to invariance or robust ness which should be considered when designing or selecting a feature detector coming back to the ﬁrst point especially the advent of powerful graphical processing units opens up new possibilities apart from the methods described above which obtain a speedup by platform indepen dent algorithmic changes further speedups become possible by exploit ing the special structure and parallelism that can be realized with gpus some examples of such work can be found in an fpga based implementation of the harris aﬃne feature detector see section is discussed in and of the dog detector see section in this signiﬁcantly reduces the time needed to compute all fea tures on a normal sized image and enables video frame rate processing in spite of this new trend the basic ideas and methods described in this section still hold as they are suﬃciently general and widely applicable finally more eﬃcient methods usually come at a price a trade oﬀ has to be made between eﬃciency on the one hand and accuracy or repeatability on the other hand surprisingly the dog surf and fast detectors are competitive with the standard more computation ally expensive feature detectors and may produce better results for some applications discussion and conclusion in this ﬁnal section of our survey we give an overview of the pre viously discussed methods and highlight their respective strengths and weaknesses we give some hints on how to use these features and on how to select the appropriate feature detector for a given application finally we discuss some open issues and future research directions how to select your feature detector below we give a few guidelines on what feature detector to use for a given application this does not give a precise and deﬁnitive answer but indicates a few points one needs to consider when searching for a suitable detector we refer the reader to section where we deﬁne the properties of local features often mentioned here first we organized the feature detectors in this survey based on the type of image structures they extract corners blobs or regions depending on the image content some of these image structures are more common than others thus the number of features found with a given detector may vary for diﬀerent image categories if little is known about the image content in advance it is generally recommended to how to select your feature detector combine diﬀerent complementary detectors i e extracting diﬀerent types of features second feature detectors can be distinguished based on the level of invariance there have been many evaluations which focus on this property one might be tempted to always select the highest level of invariance available so as to compensate for as much variability as possible however the discriminative power of features is reduced at increased levels of invariance as more patterns are to be judged equivalent there is more parameters to estimate thus more possible sources of noise also the feature detection process becomes more complex which aﬀects both the computational complexity as well as the repeatability as a result a basic rule of thumb is to use no more invariance than what is truly needed by the application at hand more over if the expected transformations are relatively small it is often better to count on the robustness of the feature detection and descrip tion rather than to increase the level of invariance that is also the reason why feature detectors invariant to perspective transformations are of little use all detectors discussed in this survey are invariant to translations and rotations the former automatically follows from the use of local features the latter can be achieved relatively easily at limited extra cost sometimes rotation invariance is not required e g if all images are taken upright and the objects are always upright as well buildings cars etc in these cases the rotation invariant detectors can be com bined with a rotation variant descriptor to ensure good discriminative power in all other cases a descriptor with at most the same level of invariance as the detector is preferred finally there are a number of qualitative properties of the detec tors to consider depending on the application scenario some of these properties are more crucial than others when dealing with category level object recognition robustness to small appearance variations is important to deal with the within class variability when ﬁtting a para metric model to the data as for camera calibration or modeling the localization accuracy is essential for online applications or appli cations where a large amount of data needs to be processed eﬃciency is the most important criterion summary on the detectors table gives an overview of the most important properties for the feature detectors described in sections the feature detectors in table are organized in groups accord ing to their invariance rotation similarity aﬃne and perspective we compare the properties within each group for rotation invariant features the highest repeatability and localization accuracy in many tests has been obtained by the harris detector the hessian detector ﬁnds blobs which are not as well localized and requires second order derivatives to be computed the susan detector avoids computation of derivatives and is known for its eﬃciency however the absence of smoothing makes it more susceptible to noise all the rotation invariant methods are suitable for applications where only the spatial location of the features is used and no large scale changes are expected e g structure from motion or camera calibration in the scale invariant group harris laplace shows high repeatabil ity and localization accuracy inherited from the harris detector however its scale estimation is less accurate due to the multiscale nature of corners hessian laplace is more robust than its single scale version this is due to the fact that blob like structures are bet ter localized in scale than corners and the detector beneﬁts from mul tiscale analysis although it is less accurately localized in the image plane dog and surf detectors were designed for eﬃciency and the other properties are slightly compromised however for most applica tions they are still more than suﬃcient quantity and good coverage of the image are crucial in recognition applications where localization accuracy is less important thus hessian laplace detectors have been successful in various categorization tasks although there are detectors with higher repeatability rate random and dense sampling also provide good results in this context which conﬁrms the coverage requirements of recognition methods although they result in far less com pact representations than the interest points dog detector performs extremely well in matching and image retrieval probably due to a good balance between spatial localization and scale estimation accuracy table overview of feature detectors note that for the scale and aﬃne invariant detectors the diﬀerence between corner and blob detectors becomes less outspoken with most detectors detecting a mixture of both feature types although they still show a preference for either type the aﬃne invariant harris and hessian follow the observations from previous groups salient regions require to compute a histogram and its entropy for each region candidate in scale or aﬃne space which results in large computational cost on the positive side the regions can be ranked according to their complexity or information content some applications exploit this and use only a small subset of the salient regions while still obtaining a good performance in e g recognition originally they were only scale invariant but later they have been extended to aﬃne invariance the edge based regions focus on corners formed by edge junctions which gives good localiza tion accuracy and repeatability but the number of detected features is small the region detectors are based on the idea of segmenting bound aries of uniform regions intensity based regions use a heuristic method and ﬁnd similar regions to mser superpixels are typically based on segmentation methods which are computationally expensive like nor malized cuts the level of invariance of superpixels depends mostly on the segmentation algorithm used in contrast to superpixels mser selects only the most stable regions which results in high repeatabil ity mser is also eﬃcient due to the use of a watershed segmenta tion algorithm aﬃne invariant detectors are beneﬁcial in cases where extreme geometric deformations are expected otherwise their scale invariant counterparts usually perform better in particular for category recognition given a stable location scale and orientation for each key it is now possible to describe the local image region in a manner invariant to these transformations in addition it is desirable to make this representation robust against small shifts in local geometry such as arise from affine or projection one approach to this is suggested by the response properties of complex neurons in the visual cortex in which a feature positionis allowed to vary over a small regionwhile orientation and spatial frequency specificity are maintained edelman intrator poggio have performed experiments that simulated the responses of complex neurons to different views of computer graphic models and found that the complex cell outputs provided much better discrimination than simple correlation basedmatching this can be seen for example if an affine projection stretches an image in one direction relative to another which changes the relative locations of gradient features while having a smaller effect on their orientations and spatial frequencies this robustness to local geometric distortion can be obtained by representing the local image region with multiple images representing each of a number of orientations referred to as orientation planes each orientation plane contains only the gradients corresponding to that orientation with linear interpolation used for intermediate orientations each orientation plane is blurred and resampled to allow for larger shifts in positions of the gradients this approach can be efficiently implemented by using the same precomputed gradients and orientations for each level of the pyramid that were used for orientation selection for each keypoint we use the pixel sampling from the pyramid level at which the key was detected the pixels that fall in a circle of radius pixels around the key location are inserted into the orientation planes the orientation is measured relative to that of the key by subtracting the key orientation for our experiments we used orientation planes each sampled over a grid of locations with a sample spacing times that of the pixel spacing used for gradient detection the blurring is achieved by allocating the gradient of each pixel among its closest neighbors in the sample grid using linear interpolationin orientation and the two spatial dimensions this implementation is much more efficient than performing explicit blurring and resampling yet gives almost equivalent results in order to sample the image at a larger scale the same process is repeated for a second level of the pyramid one octave higher however this time a rather than a sample region is used this means that approximately the same image region will be examined at both scales so that any nearby occlusionswill not affect one scale more than the other therefore the total number of samples in the sift key vector from both scales is or elements giving enough measurements for high specificity indexing and matching for indexing we need to store the sift keys for sample images and then identifymatching keys fromnew images the problemof identifyingthemost similar keys for high dimen sional vectors is known to have high complexity if an exact solution is required however a modification of the k d tree algorithm called the best bin first search method beis lowe can identify the nearest neighbors with high probability using only a limited amount of computation to further improve the efficiency of the best bin first algorithm the sift key samples generated at the larger scale are given twice the weight of those at the smaller scale this means that the larger scale is in effect able to filter the most likely neighbours for checking at the smaller scale this also improves recognition performance by giving more weight to the least noisy scale in our experiments it is possible to have a cut off for examining at most neighbors in a probabilisticbest bin first search of key vectors with almost no loss of performance compared to finding an exact solution an efficient way to cluster reliable model hypotheses is to use the hough transform to search for keys that agree upon a particular model pose each model key in the database contains a record of the key parameters relative to the model coordinate system therefore we can create an entry in a hash table predicting the model location orientation and scale from the match hypothesis we use a bin size of degrees for orientation a factor of for scale and times the maximum model dimension for location these rather broad bin sizes allow for clustering even in the presence of substantial geometric distortion such as due to a change in viewpoint to avoid the problem of boundary effects in hashing each hypothesis is hashed into the closest bins in each dimension giving a total of hash table entries for each hypothesis solution for affine parameters the hash table is searched to identify all clusters of at least entries in a bin and the bins are sorted into decreasing order of size each such cluster is then subject to a verification procedure in which a least squares solution is performed for the affine projection parameters relating themodel to the image the affine transformation of a model point x y t to an image point u v t can be written as u v x y tx ty where the model translation is tx ty t and the affine rotation scale and stretch are represented by themi parameters we wish to solve for the transformation parameters so figure model images of planar objects are shown in the top row recognitionresults belowshowmodel outlines and image keys used for matching the equation above can be rewritten as x y x y tx ty u v this equation shows a single match but any number of further matches can be added with each match contributing twomore rows to the first and lastmatrix at least are needed to provide a solution we can write this linear system as ax b the least squares solution for the parameters x can be deter figure top row shows model images for objects with outlines found by background segmentation bottom image shows recognitionresults for outlines and image keys used for matching mined by solving the corresponding normal equations x ata which minimizes the sum of the squares of the distances from the projected model locations to the corresponding image locations this least squares approach could readily be extended to solving for pose and internal parameters of articulated and flexible objects outliers can now be removed by checking for agreement between each image feature and themodel given the parameter solution each match must agree within degrees orientation change in scale and times maximummodel size in terms of location if fewer than points remain after discarding outliers then thematch is rejected if any outliers are discarded the least squares solutionis re solvedwith the remaining points figure examples of recognitionwith occlusion experiments the affine solution provides a good approximation to perspective projection of planar objects so planar models provide a good initial test of the approach the top row of figure shows three model images of rectangular planar faces of objects the figure also shows a cluttered image containing the planar objects and the same image is shown overlayed with the models following recognition the model keys that are displayed are the ones used for recognition and final least squares solution since only keys are needed for robust recognition it can be seen that the solutions are highly redundant and would survive substantial occlusion also shown are the rectangular borders of themodel images projected using the affine transform from the least square solution these closely agree with the true borders of the planar regions in the image except for small errors introduced by the perspective projection similar experiments have been performed formany images of planar objects and the recognition has proven to be robust to at least a degree rotation of the object in any direction away fromthe camera although the model images and affine parameters do not account for rotation in depth of objects they are still sufficient to perform robust recognition of objects over about a degree range of rotation in depth away from each model view an example of three model images is shown in figure stability of image keys is tested under differing illumination the first image is illuminated from upper left and the second from center right keys shown in the bottom image were those used to match second image to first the top row of figure themodels were photographed on a black background and object outlines extracted by segmenting out the background region an example of recognition is shown in the same figure again showing the sift keys used for recognition the object outlines are projected using the affine parameter solution but this time the agreement is not as close because the solution does not account for rotation in depth figure shows more examples in which there is significant partial occlusion the images in these examples are of size pixels the computation times for recognition of all objects in each image are about seconds on a sun sparc processor with about seconds required to build the scalespace pyramid and identify the sift keys and about seconds to perform indexing and least squares verification this does not include time to pre process each model image which would be about second per image but would only need to be done once for initial entry into a model database the illumination invariance of the sift keys is demonstrated in figure the two images are of the same scene from the same viewpoint except that the first image is illuminated from the upper left and the second from the center right the full recognition system is run to identify the second image using the first image as the model and the second image is correctly recognized as matching the first only sift keys that were part of the recognition are shown there were keys that were verified as part of the final match which means that in each case not onlywas the same key detected at the same location but it also was the closest match to the correct corresponding key in the second image any of these keys would be sufficient for recognition while matching keys are not found in some regions where highlights or shadows change for example on the shiny top of the camera in general the keys show good invariance to illumination change connections to biological vision the performance of human vision is obviously far superior to that of current computer vision systems so there is potentiallymuch to be gained by emulating biological processes fortunately there have been dramatic improvements within the past few years in understanding how object recognition is accomplished in animals and humans recent research in neuroscience has shown that object recognition in primates makes use of features of intermediate complexity that are largely invariant to changes in scale location and illumination tanaka perrett oram some examples of such intermediate features found in inferior temporal cortex it are neurons that respond to a dark five sided star shape a circle with a thin protruding element or a horizontal textured region within a triangular boundary these neuronsmaintain highly specific responses to shape features that appear anywhere within a large portion of the visual field and over a several octave range of scales ito et al the complexity of many of these features appears to be roughly the same as for the current sift features although there are also some neurons that respond to more complex shapes such as faces many of the neurons respond to color and texture properties in addition to shape the feature responses have been shown to depend on previous visual learning from exposure to specific objects containing the features logothetis pauls poggio these features appear to be derived in the brain by a highly computation intensive parallel process which is quite different from the staged filtering approach given in this paper however the results are much the same an image is transformed into a large set of local features that each match a small fraction of potential objects yet are largely invariant to common viewing transformations it is also known that object recognition in the brain depends on a serial process of attention to bind features to object interpretations determine pose and segment an object from a cluttered background this process is presumably playing the same role in verification as the parameter solving and outlier detection used in this paper since the accuracy of interpretations can often depend on enforcing a single viewpoint constraint conclusions and comments the sift features improve on previous approaches by being largely invariant to changes in scale illumination and local affine distortions the large number of features in a typical image allow for robust recognition under partial occlusion in cluttered images a final stage that solves for affine model parameters allows for more accurate verification and pose determination than in approaches that rely only on indexing an important area for further research is to buildmodels from multiple views that represent the structure of objects this would have the further advantage that keys from multiple viewing conditions could be combined into a single model thereby increasing the probabilityof findingmatches in new views the models could be true representations based on structure from motion solutions or could represent the space of appearance in terms of automated clustering and interpolation pope lowe an advantage of the latter approach is that it could also model non rigid deformations the recognition performance could be further improved by adding new sift feature types to incorporate color texture and edge groupings as well as varying feature sizes and offsets scale invariant edge groupings that make local figure ground discriminations would be particularly useful at object boundaries where background clutter can interfere with other features the indexing and verification framework allows for all types of scale and rotation invariant features to be incorporated into a single model representation maximumrobustness would be achieved by detecting many different feature types and relying on the indexing and clustering to select those that are most useful in a particular image adriana kovashka department of computer science january course info course website instructor adriana kovashka email please use at the beginning of the subject line office sennott square office hours by appointment grades on blackboard courseweb feedback is welcome plan for today introductions what is visual recognition how well does it work what are the challenges overview of topics course structure and requirements machine learning quiz for self evaluation introductions introductions what is your name what department are you at which year what are your research interests what one thing outside of research are you passionate about what do you hope to get out of this class visual recognition what is computer vision automatic understanding of images and video computing properties of the world from visual data measurement visual recognition algorithms to mine search and interact with visual data search and organization kristen grauman building truck street carriage horse balcony table person person car allows us to automate various processes checkout at a store surveillance annotating football videos etc enables content based image search and organization perception is an integral part of robotics how has it evolved how well does it work inputs in l g roberts ph d thesis mit department of electrical engineering kristen grauman and inputs today personal photo albums surveillance and security svetlana lazebnik movies news sports medical and scientific images imagenet categories images microsoft coco categories images pascal categories images sun categories images yong jae lee al mit kristen grauman belhumeur et al kooaba bay quack et al snavely et al kristen grauman devi parikh dollar et al bmvc google self driving car mars rover yong jae lee schuldt et al icpr what are the challenges illumination object pose viewpoint intra class appearance occlusions clutter kristen grauman antonio torralba kristen grauman hours of video added to youtube per minute new tagged photos added to flickr per minute thousands to millions of pixels in an image degrees of freedom in the pose of articulated objects humans human recognizable object categories half of the human brain is devoted to processing visual information kristen grauman less more kristen grauman overview of topics feature detection and matching detecting repeatable features describing images with local statistics matching features across images detection and classification detecting novel instances of objects classifying regions as one of several categories attributes describing the high level properties of objects measuring the degree of attribute presence segmentation detecting contours grouping pixels into semantic regions groups of objects scenes and context exploiting context to detect novel objects parsing out the elements of scenes pose and actions automatically annotating a human pose recognizing different human activities unsupervised visual discovery finding patterns in unannotated data vision and language describing images with sentences making better use of training data using a known category to learn another using non expert labelers to collect data ensuring the quality of this data use rationales to learn faster evaluate the quality of performed actions learning where in an image a human will look predicting which content is worth mentioning visualizing automatic predictions and outputs finding the weakest link in systems letting the data solve the problem life long learning and mining of patterns dataset bias recognizing actions in first person video summarizing long videos matching queries across domains modeling painterly styles course structure and requirements to learn about the state of the art approaches in visual recognition to think critically about vision approaches and to see connections between works and potential for improvement to practice critical reading clear writing and engaging presentation skills expected basic knowledge of probability and linear algebra experience or familiarity with machine learning is recommended paper reviews participation discussion paper presentations experiment presentation course project proposal mid semester progress status report final presentation final report page review for of the papers discussed each class usually the primary paper if any paper marked with read that first due the day before each class send email to instructor with subject paper review name your file first name last name month day pdf or doc docx skip paper reviews for papers you present answer the following questions summarize what this paper aims to do and what its main contribution is summarize the proposed approach summarize the experimental validation of the approach what are three advantages of the proposed approach what are three disadvantages or weaknesses of the approach or experimental validation suggest one possible extension of this approach i e one idea for future work any other thoughts comments or questions on this paper carefully read the assigned papers ask meaningful questions make meaningful comments about the paper strengths and weaknesses answer questions asked by others each student will give about presentations each presentation will cover papers presentations should be min long min if no experiment presentation and should be clear and well rehearsed presentations will be followed by a discussion moderated by the presenter see the course website for questions to address in presentation often you can find slides on the authors websites cite all sources and slide credits and use your own words slides should use text sparingly slides will be uploaded to course website important presenters should meet with the instructor on friday for tuesday presentations or monday for thursday presentations at the latest important presenters should email their draft slides and meeting time availability to the instructor days before desired meeting date with subject presentation slides exception day before meeting if presentation is on present experimental evaluation of paper students can volunteer to present an extra paper later better grade will be used presentations will be about min pick one aspect of the paper to evaluate cite any code you used explain what why you did and what you found out in your experiments ask for guidance if needed students will complete in depth study of one topic covered in class these projects can become conference publications for most types of projects students can work in pairs most project types require experimental evaluation projects can be one of the following an extension of one or more papers covered in class a novel approach with evaluation a definition of a new problem along with detailed argumentation of why this problem is important and challenging and an approach to solve this problem extensive analysis and experimental evaluation of one or more of the approaches covered in class extensive literature review and analysis on one of the topics covered in class this can only be done by students working individually timeline march project proposals due of course grade april project status reports due april project presentations april project final report due email these to the instructor with subject project see course website for details proposal pages in length include clear problem statement extensive literature review detailed outline of the approach and planned experimental setup students are encouraged to meet with the instructor to discuss proposal before proposals are due progress report describe progress identify problem areas use the cvpr latex template include sections introduction related work approach and results presentation min long final report by on tomorrow email the instructor kovashka cs pitt edu a list of the ten topics from the schedule on the course website that you are most interested in presenting as a paper presentation a list of ten papers that you might like to present as an experiment presentation sort these from to where denotes most interested use subject topic preferences questions quiz how would you learn to predict whether author a or author b wrote a given piece of text what learning problem corresponds to predicting the price of a car based on its features e g engine specs and the prices of other cars other than learning an individual model what else can you use training data for given a model that can predict whether an image has a cat in it or not what do you usually need to do to a new image in order to be able to apply this model to it what does overfitting mean why is it a problem how can you overcome this problem what two goals does a soft margin linear svm optimization objective capture how can you use maximum likelihood to select model parameters state bayes theorem what is the goal of performing k means how is this goal accomplished what are λ and v in relationship to a according to the following equation a v λ v if the dimensionality of v is what is the dimensionality of a this thursday describing images with features next tuesday recognition basics adriana research next thursday first presentation features every picture tells a story generating sentences from images slides credited yukun zhu goa ll t hi i a lot of t ec hno llogy som bodys scr ns ave r of k n a lac k lla ptop is connected to a llack el l tor thi i a d u a or set p o d school ut er monitor wit lh wa y to st i ckers on goal a u t o i l u st r at find p ict u g g es t e by g i v en text yellow train on the tra ck map from image space to meaning space map from sentence space to meaning space retrieve sentences for images via meaning space farhadi et al map from image space to meaning space map from sentence space to meaning space retrieve sentences for images via meaning space farhadi et al predict image content using trained classiﬁers farhadi et al map from image space to meaning space map from sentence space to meaning space retrieve sentences for images via meaning space farhadi et al extract subject verb and scene from sentences in the training data black cat over pink chair a black color cat si ng on chair in a room cat si ng on a chair looking in a mirror use taxonomy trees object subject cat verb si ng scene room animal human vehicle cat dog horse car bike train farhadi et al map from image space to meaning space map from sentence space to meaning space retrieve sentences for images via meaning space farhadi et al farhadi et al farhadi et al farhadi et al images images more data needed rashtchian et al farhadi et al descriptions per image object categories image clef challenge descriptions per image select image categories large amounts of paired data can help us study the relationship berg a ributes tutorial baby talk understanding and generating image descriptions presented by yingjie tang person car shoe berg a ributes tutorial credit tamara car berg a ributes tutorial credit tamara pink car a ributes of objects berg a ributes tutorial credit tamara car on road relationships between objects berg a ributes tutorial credit tamara little pink smart car parked on the side of a road in a london shopping district complex structured recognition outputs telling the story of an image berg a ributes tutorial credit tamara problem generate natural language descriptions for images this picture shows one person one grass one chair and one potted plant the person is near the green grass and in the chair the green grass is by the chair and near the potted plant problem generate natural language descriptions for images descriptive language more information about the visual world convey the style how people describe world learning from descriptive text it was an arresting face pointed of chin square of jaw her eyes were pale green without a touch of hazel starred with bristly black lashes and slightly tilted at the ends above them her thick black brows slanted upward cutting a startling oblique line in her magnolia white skin that skin so prized by southern women and so carefully guarded with bonnets veils and mittens against hot georgia suns scarlett o hara described in gone with the wind visually descriptive language provides information about the world especially the visual world how does the world work information about how people construct natural language for imagery guidance for visual recognition what should recognize how do people describe the world berg a ributes tutorial credit tamara problem generate natural language descriptions for images statistics gleaned from parsing large quantities of text data recognition algorithms from computer vision generating sentences for images mining for statistic models detectors scene objects n gram generating sentences generating sentences for images key words most previous work in nlp on automatically generating captions or descriptions for images is based on retrieval and summarization generating sentences for images scene based generated sentences are not as descriptive enough generating sentences for images small number of instances form large number of scenes avoid whole image features recognition and make tight connection between image content and sentence generation individual words with image regions use of spatial relationships between labeled parts of image use the attributes in computer vision to estimate modifiers for objects in images use human loop for hierarchical image parsing conditional random fields a framework for building probabilistic models to segment and label sequence data conditional random fields offer several advantages over hidden markov models and stochastic grammars for such tasks including the ability to relax strong independence assumptions made in those models lafferty j mccallum a pereira f nodes of the crf objects attributes prepositions nodes of the crf objects a large set of detectors collect a set of high score detections merge detections that are highly overlapping into groups create an object node for each group objects nodes set of object detectors that fired at that region in the image attribute nodes a set of appearance attributes that can modify the objects preposition nodes a set of prepositional relations that can occur between two objects label image text prior number of objects a number of objects the number of true obj labels minus the number of false obj labels normalized by the number of objects the number of true mod obj label pairs minus the number of false mod obj pairs the number of true obj prep obj triples minus the number of false obj prep obj triples normalized by the number of nodes and the number of pairs of objects n choose image based potentials the image potentials come from hand designed detection strategies optimized on external training sets text potential the text potentials are based on text statistics collected automatically from various corpora ψ obji objdet object and stuff potential for object detectors pascal object categories detectors trained additional non pascal object categories for flower laptop tiger and window for stuff detectors trained linear svms on the low level region features to recognize sky road building tree water and grass stuff categories svm outputs are mapped to probabilities ψ attri attrcl attribute potential train visual attribute classifiers that are relevant for our object and stuff categories mine large text corpus of flickr descriptions described in sec to find attribute terms the resulting list consists of visual attribute terms describing color e g blue gray texture e g striped furry material e g wooden feathered general appearance e g rusty dirty shiny and shape e g rectangular characteristics ψ prepij prepf uns preposition potential preposition terms two potential functions calculated from large corpora pairwise potential on attribute object label pairs ψ attri obji textp r a trinary potential on object preposition object triples ψ obji prepij objj textp r these potentials are the probability of various attributes for each object given the object and the probabilities of particular prepositional relationships between object pairs given the pair of objects parsing potential collect a large set of flickr image descriptions to count object potential ψp attri obji textp r collect statistics about the occurrence of each attribute and object pair to count amod attribute object collect million flickr image descriptions by querying for pairs of object terms for ψp obji prepij obji textp r reasons the counts for some objects can be too sparse collect additional google search based potentials ψg attri obji textpr and ψg obji prepij objj textpr smooth potential final potentials are computed as a smoothed combination of the parsing based potentials with the google potentials αψp α ψg output of crf triples our goal language models and templates n gram model the prediction of the next word depends only on the previous n words we want to determine whether to insert a function word x between a pair of words α and β in the meaning representation calculating p αxβ p α p x α p β x using bigram gram language models weakness it is difficult to enforce grammatically correct sentences using language models alone it is ignorant of discourse structure coherency among sentences as each sentence is generated independently templates with linguistic constraints constructing templates with linguistically motivated constraints this approach is based on the assumption that there are a handful of salient syntactic patterns in descriptive language that we can encode as templates templates with linguistic constraints templates with linguistic constraints this is a picture of one sky one road and one sheep the gray sky is over the gray road the gray sheep is by the gray road here we see one road one sky and one bicycle the road is near the blue sky and near the colorful bicycle the colorful bicycle is within the blue sky this is a picture of two dogs the first dog is near the second furry dog kulkarni et al credit tamara missed detections here we see one potted plant this is a picture of one dog false detections there are one road and one cat the furry road is in the furry cat this is a picture of one tree one road and one person the rusty tree is under the red road the colorful person is near the rusty tree and under the red road incorrect attributes this is a photograph of two sheeps and one grass the first black sheep is by the green grass and by the second black sheep the second black sheep is by the green grass this is a photograph of two horses and one grass the first feathered horse is within the green grass and by the second feathered horse the second feathered horse is within the green grass kulkarni et al credit tamara training sets test sets training set crawled wikipedia pages that describe objects our system can recognize to construct the training corpus for language models test set use the uiuc pas cal sentence which contains up to five human generated sentences that describe images automatic evaluation bleu a widely used metric for automatic evaluation of machine translation that measures the n gram precision of machine generated sentences with respect to human generated sentences weakness bleu will inevitably penalize many correctly generated sentences human evaluation perform human judgment on the entire test set to directly quantify these aspects overall the template generation method demonstrates a very high average human evaluation score of max for the quality of generated sentences an effective fully automatic system that generates natural language descriptions for images produce results much more specific to the image content than previous automated methods human evaluation validates the quality of the generated sentences keys to success automatically mining and parsing large text collections taking advantage of state of the art vision systems and combining all of these in a crf to produce input for language generation methods the descriptions of a sentence for the image is always biased this picture shows one person one grass one chair and one potted plant the person is near the green grass and in the chair the green grass is by the chair and near the potted plant the boy is happy to standing in front of the shop with his snacks how can we find an unbiased way to describe the image scene based description is in a high level description while the thing stuff based description is the low level description scene based description things stuff based description scene based description things stuff based description can we reverse back to the scene based description from the things stuff information a discriminatively trained multiscale deformable part model pedro felzenszwalb university of chicago david mcallester toyota technological institute at chicago deva ramanan uc irvine abstract this paper describes a discriminatively trained multi scale deformable part model for object detection our sys tem achieves a two fold improvement in average precision over the best performance in the pascal person de tection challenge it also outperforms the best results in the challenge in ten out of twenty categories the system relies heavily on deformable parts while deformable part models have become quite popular their value had not been demonstrated on difficult benchmarks such as the pascal challenge our system also relies heavily on new methods for discriminative training we combine a margin sensitive approach for data mining hard negative examples with a formalism we call latent svm a latent svm like a hid den crf leads to a non convex training problem how ever a latent svm is semi convex and the training prob lem becomes convex once latent information is specified for the positive examples we believe that our training meth ods will eventually make possible the effective use of more latent information such as hierarchical grammar models and models involving latent three dimensional pose introduction we consider the problem of detecting and localizing ob jects of a generic category such as people or cars in static images we have developed a new multiscale deformable part model for solving this problem the models are trained using a discriminative procedure that only requires bound ing box labels for the positive examples using these mod els we implemented a detection system that is both highly efficient and accurate processing an image in about sec onds and achieving recognition rates that are significantly better than previous systems our system achieves a two fold improvement in average precision over the winning system in the pascal person detection challenge the system also outperforms the best results in the challenge in ten out of twenty this material is based upon work supported by the national science foundation under grant no and figure example detection obtained with the person model the model is defined by a coarse template several higher resolution part templates and a spatial model for the location of each part object categories figure shows an example detection ob tained with our person model the notion that objects can be modeled by parts in a de formable configuration provides an elegant framework for representing object categories while these models are appealing from a conceptual point of view it has been difficult to establish their value in prac tice on difficult datasets deformable models are often out performed by conceptually weaker models such as rigid templates or bag of features one of our main goals is to address this performance gap our models include both a coarse global template cov ering an entire object and higher resolution part templates the templates represent histogram of gradient features as in we train models discriminatively how ever our system is semi supervised trained with a max margin framework and does not rely on feature detection we also describe a simple and effective strategy for learn ing parts from weakly labeled data in contrast to computa tionally demanding approaches such as we can learn a model in hours on a single cpu another contribution of our work is a new methodology for discriminative training we generalize svms for han dling latent variables such as part positions and introduce a new method for data mining hard negative examples dur ing training we believe that handling partially labeled data is a significant issue in machine learning for computer vi sion for example the pascal dataset only specifies a bounding box for each positive example of an object we treat the position of each object part as a latent variable we also treat the exact location of the object as a latent vari able requiring only that our classifier select a window that has large overlap with the labeled bounding box a latent svm like a hidden crf leads to a non convex training problem however unlike a hidden crf a latent svm is semi convex and the training problem be comes convex once latent information is specified for the positive training examples this leads to a general coordi nate descent algorithm for latent svms system overview our system uses a scanning window approach a model for an object consists of a global root filter and several part models each part model specifies a spatial model and a part filter the spatial model defines a set of allowed placements for a part relative to a detection window and a deformation cost for each placement the score of a detection window is the score of the root filter on the window plus the sum over parts of the maxi mum over placements of that part of the part filter score on the resulting subwindow minus the deformation cost this is similar to classical part based models both root and part filters are scored by computing the dot product be tween a set of weights and histogram of gradient hog features within a window the root filter is equivalent to a dalal triggs model the features for the part filters are computed at twice the spatial resolution of the root filter our model is defined at a fixed scale and we detect objects by searching over an image pyramid in training we are given a set of images annotated with bounding boxes around each instance of an object we re duce the detection problem to a binary classification prob lem each example x is scored by a function of the form fβ x maxz β φ x z here β is a vector of model pa rameters and z are latent values e g the part placements to learn a model we define a generalization of svms that we call latent variable svm lsvm an important prop erty of lsvms is that the training problem becomes convex if we fix the latent values for positive examples this can be used in a coordinate descent algorithm in practice we iteratively apply classical svm training to triples xn zn yn where zi is selected to be the best scoring latent label for xi under the model learned in the previous iteration an initial root filter is generated from the bounding boxes in the pascal dataset the parts are initialized from this root filter model the underlying building blocks for our models are the histogram of oriented gradient hog features from we represent hog features at two different scales coarse features are captured by a rigid template covering an entire image pyramid hog feature pyramid figure the hog feature pyramid and an object hypothesis de fined in terms of a placement of the root filter near the top of the pyramid and the part filters near the bottom of the pyramid detection window finer scale features are captured by part templates that can be moved with respect to the detection window the spatial model for the part locations is equiv alent to a star graph or fan where the coarse template serves as a reference position hog representation we follow the construction in to define a dense repre sentation of an image at a particular resolution the image is first divided into non overlapping pixel regions or cells for each cell we accumulate a histogram of gra dient orientations over pixels in that cell these histograms capture local shape properties but are also somewhat invari ant to small deformations the gradient at each pixel is discretized into one of nine orientation bins and each pixel votes for the orientation of its gradient with a strength that depends on the gradient magnitude for color images we compute the gradient of each color channel and pick the channel with highest gradi ent magnitude at each pixel finally the histogram of each cell is normalized with respect to the gradient energy in a neighborhood around it we look at the four blocks of cells that contain a particular cell and normalize the his togram of the given cell with respect to the total energy in each of these blocks this leads to a vector of length representing the local gradient information inside a cell we define a hog feature pyramid by computing hog features of each level of a standard image pyramid see fig ure features at the top of this pyramid capture coarse gradients histogrammed over fairly large areas of the input image while features at the bottom of the pyramid capture finer gradients histogrammed over small areas filters of each part relative to the root the spatial term filters are rectangular templates specifying weights for n n subwindows of a hog pyramid a w by h filter f is a vector with w h weights the score of a filter is i i i i defined by taking the dot product of the weight vector and the features in a w h subwindow of a hog pyramid the system in uses a single filter to define an object model that system detects objects from a particular class by scoring every w h subwindow of a hog pyramid and thresholding the scores let h be a hog pyramid and p x y l be a cell in the l th level of the pyramid let φ h p w h denote the vector obtained by concatenating the hog features in the w h subwindow of h with top left corner at p the score of f on this detection window is f φ h p w h below we use φ h p to denote φ h p w h when the dimensions are clear from context deformable parts here we consider models defined by a coarse root filter that covers the entire object and higher resolution part filters covering smaller parts of the object figure illustrates a placement of such a model in a hog pyramid the root fil where x i y i xi yi x y vi si gives the lo cation of the i th part relative to the root location both x i and y i should be between and there is a large exponential number of placements for a model in a hog pyramid we use dynamic programming and distance transforms techniques to compute the best location for the parts of a model as a function of the root location this takes o nk time where n is the number of parts in the model and k is the number of cells in the hog pyramid to detect objects in an image we score root locations according to the best possible placement of the parts and threshold this score the score of a placement z can be expressed in terms of the dot product β ψ h z between a vector of model parameters β and a vector ψ h z β fn an bn ψ h z φ h φ h φ h pn x y x y x n y n x y ter location defines the detection window the pixels inside the cells covered by the filter the part filters are placed several levels down in the pyramid so the hog cells at that level have half the size of cells in the root filter level we have found that using higher resolution features for defining part filters is essential for obtaining high recogni tion performance with this approach the part filters repre sent finer resolution edges that are localized to greater ac curacy when compared to the edges represented in the root filter for example consider building a model for a face the root filter could capture coarse resolution edges such as the face boundary while the part filters could capture details such as eyes nose and mouth the model for an object with n parts is formally defined by a root filter and a set of part models pn where pi fi vi si ai bi here fi is a filter for the i th part vi is a two dimensional vector specifying the center for a box of possible positions for part i relative to the root po sition si gives the size of this box while ai and bi are two dimensional vectors specifying coefficients of a quadratic function measuring a score for each possible placement of the i th part figure illustrates a person model a placement of a model in a hog pyramid is given by z pn where pi xi yi li is the location of the root filter when i and the location of the i th part when i we assume the level of each part is such that a hog cell at that level has half the size of a hog cell at the root level the score of a placement is given by the scores of each filter the data term plus a score of the placement we use this representation for learning the model parame ters as it makes a connection between our deformable mod els and linear classifiers on interesting aspect of the spatial models defined here is that we allow for the coefficients ai bi to be negative this is more general than the quadratic spring cost that has been used in previous work learning the pascal training data consists of a large set of im ages with bounding boxes around each instance of an ob ject we reduce the problem of learning a deformable part model with this data to a binary classification problem let d xn yn be a set of labeled exam ples where yi and xi specifies a hog pyramid h xi together with a range z xi of valid placements for the root and part filters we construct a positive exam ple from each bounding box in the training set for these ex amples we define z xi so the root filter must be placed to overlap the bounding box by at least negative exam ples come from images that do not contain the target object each placement of the root filter in such an image yields a negative training example note that for the positive examples we treat both the part locations and the exact location of the root filter as latent variables we have found that allowing uncertainty in the root location during training significantly improves the per formance of the system see section latent svms a latent svm is defined as follows we assume that each example x is scored by a function of the form negative examples at a time instead it is common to con struct training data consisting of the positive instances and hard negative instances where the hard negatives are data mined from the very large set of possible negative examples fβ x max z z x β φ x z here we describe a general method for data mining ex amples for svms and latent svms the method iteratively where β is a vector of model parameters and z is a set of latent values for our deformable models we define φ x z ψ h x z so that β φ x z is the score of placing the model according to z in analogy to classical svms we would like to train β from labeled examples d xn yn by optimizing the following objective function β d argmin λ β max yifβ xi solves subproblems using only hard instances the innova tion of our approach is a theoretical guarantee that it leads to the exact solution of the training problem defined using the complete training set our results require the use of a margin sensitive definition of hard examples the results described here apply both to classical svms and to the problem defined by step of the coordinate de scent algorithm for latent svms we omit the proofs of the theorems due to lack of space these results are related to working set methods by restricting the latent domains z xi to a single choice fβ becomes linear in β and we obtain linear svms as a special case of latent svms latent svms are instances of the general class of energy based models semi convexity note that fβ x as defined in is a maximum of func tions each of which is linear in β hence fβ x is convex in β this implies that the hinge loss max yifβ xi is convex in β when yi that is the loss function is convex in β for negative examples we call this property of the loss function semi convexity consider an lsvm where the latent domains z xi for the positive examples are restricted to a single choice the loss due to each positive example is now convex combined with the semi convexity property becomes convex in β if the labels for the positive examples are not fixed we can compute a local optimum of using a coordinate de scent algorithm holding β fixed optimize the latent values for the pos itive examples zi argmaxz z xi β φ x z holding zi fixed for positive examples optimize β by solving the convex problem defined above it can be shown that both steps always improve or maintain the value of the objective function in if both steps main tain the value we have a strong local optimum of in the sense that step searches over an exponentially large space of latent labels for positive examples while step simulta neously searches over weight vectors and an exponentially large space of latent labels for negative examples data mining hard negatives in object detection the vast majority of training exam ples are negative this makes it infeasible to consider all m β d x y d yfβ x that is m β d are training examples that are incorrectly classified or near the margin of the classifier defined by β we can show that β d only depends on hard instances theorem let c be a subset of the examples in d if m β d d c then β c β d this implies that in principle we could train a model us ing a small set of examples however this set is defined in terms of the optimal model β d given a fixed β we can use m β d to approximate m β d d this suggests an iterative algorithm where we repeatedly compute a model from the hard instances de fined by the model from the last iteration this is further justified by the following fixed point theorem theorem if β m β d β then β β d let c be an initial cache of examples in practice we can take the positive examples together with random nega tive examples consider the following iterative algorithm let β β c shrink c by letting c m β c grow c by adding examples from m β d up to a memory limit l theorem if c l after each iteration of step the algorithm will converge to β β d in finite time implementation details many of the ideas discussed here are only approximately implemented in our current system in practice when train ing a latent svm we iteratively apply classical svm train ing to triples xn zn yn where zi is se lected to be the best scoring latent label for xi under the model trained in the previous iteration each of these triples leads to an example φ xi zi yi for training a linear clas sifier this allows us to use a highly optimized svm pack age svmlight on a single cpu the entire training process takes to hours per object class in the pascal datasets including initialization of the parts root filter initialization for each category we auto matically select the dimensions of the root filter by looking at statistics of the bounding boxes in the training data we train an initial root filter using an svm with no latent variables the positive examples are constructed from the unoccluded training examples as labeled in the pascal data these examples are anisotropically scaled to the size and aspect ratio of the filter we use random subwindows from negative images to generate negative examples root filter update given the initial root filter trained as above for each bounding box in the training set we find the best scoring placement for the filter that significantly overlaps with the bounding box we do this using the orig inal un scaled images we retrain with the new positive set and the original random negative set iterating twice part initialization we employ a simple heuristic to ini tialize six parts from the root filter trained above first we select an area a such that equals of the area of the root filter we greedily select the rectangular region of area a from the root filter that has the most positive energy we zero out the weights in this region and repeat until six parts are selected the part filters are initialized from the root fil ter values in the subwindow selected for the part but filled in to handle the higher spatial resolution of the part the initial deformation costs measure the squared norm of a dis placement with ai and bi model update to update a model we construct new training data triples for each positive bounding box in the training data we apply the existing detector at all positions and scales with at least a overlap with the given bound ing box among these we select the highest scoring place ment as the positive example corresponding to this training bounding box figure negative examples are selected by finding high scoring detections in images not containing the target object we add negative examples to a cache un til we encounter file size limits a new model is trained by running svmlight on the positive and negative examples each labeled with part placements we update the model times using the cache scheme described above in each it eration we keep the hard instances from the previous cache and add as many new hard instances as possible within the memory limit toward the final iterations we are able to include all hard instances m β d in the cache picked a simple heuristic by cross validating over object classes we set the model aspect to be the most common mode aspect in the data we set the model size to be the largest size not larger than of the data figure the image on the left shows the optimization of the la tent variables for a positive example the dotted box is the bound ing box label provided in the pascal training set the large solid box shows the placement of the detection window while the smaller solid boxes show the placements of the parts the image on the right shows a hard negative example results we evaluated our system using the pascal voc and challenge datasets and protocol we refer to for details but emphasize that both challenges are widely acknowledged as difficult testbeds for object detec tion each dataset contains several thousand images of real world scenes the datasets specify ground truth bounding boxes for several object classes and a detection is consid ered correct when it overlaps more than with a ground truth bounding box one scores a system by the average precision ap of its precision recall curve across a testset recent work in pedestrian detection has tended to report detection rates versus false positives per window measured with cropped positive examples and negative images with out objects of interest these scores are tied to the reso lution of the scanning window search and ignore effects of non maximum suppression making it difficult to compare different systems we believe the pascal scoring method gives a more reliable measure of performance the challenge has object categories we entered a preliminary version of our system in the official competi tion and obtained the best score in categories our current system obtains the highest score in categories and the second highest score in categories table summarizes the results our system performs well on rigid objects such as cars and sofas as well as highly deformable objects such as per sons and horses we also note that our system is successful when given a large or small amount of training data there are roughly positive training examples in the person category but only in the sofa category figure shows some of the models we learned figure shows some ex ample detections we evaluated different components of our system on the longer established person dataset the top ap score table pascal voc results average precision scores of our system and other systems that entered the competition empty boxes indicate that a method was not tested in the corresponding class the best score in each class is shown in bold our current system ranks first in out of classes a preliminary version of our system ranked first in classes in the official competition bottle car sofa bicycle figure some models learned from the pascal voc dataset we show the total energy in each orientation of the hog cells in the root and part filters with the part filters placed at the center of the allowable displacements we also show the spatial model for each part where bright values represent cheap placements and dark values represent expensive placements in the pascal competition was obtained using a rigid template model of hog features the best previous re sult of adds a segmentation based verification step figure summarizes the performance of several models we trained our root only model is equivalent to the model from and it scores slightly higher at performance jumps to when the model is trained with a lsvm that selects a latent position and scale for each positive example this suggests lsvms are useful even for rigid templates because they allow for self adjustment of the detection win dow in the training examples adding deformable parts in creases performance to ap a factor of two above the best previous score finally we trained a model with parts but no root filter and obtained ap this illustrates the advantage of using a multiscale representation we also investigated the effect of the spatial model and allowable deformations on the person dataset recall that si is the allowable displacement of a part measured in hog cells we trained a rigid model with high resolution parts by setting si to this model outperforms the root only system by to if we increase the amount of allowable displacements without using a deformation cost we start to approach a bag of features performance peaks at si suggesting it is useful to constrain the part dis placements the optimal strategy allows for larger displace ments while using an explicit deformation cost the follow figure some results from the pascal dataset each row shows detections using a model for a specific class person bottle car sofa bicycle horse the first three columns show correct detections while the last column shows false positives our system is able to detect objects over a wide range of scales such as the cars and poses such as the horses the system can also detect partially occluded objects such as a person behind a bush note how the false detections are often quite reasonable for example detecting a bus with the car model a bicycle sign with the bicycle model or a dog with the horse model in general the part filters represent meaningful object parts that are well localized in each detection such as the head in the person model relative attributes devi parikh toyota technological institute chicago ttic kristen grauman university of texas at austin abstract human nameable visual attributes can benefit vari ous recognition tasks however existing techniques restrict these properties to categorical labels for example a per son is smiling or not a scene is dry or not and thus fail to capture more general semantic relationships we propose to model relative attributes given training data stating how object scene categories relate according to dif ferent attributes we learn a ranking function per attribute the learned ranking functions predict the relative strength of each property in novel images we then build a genera tive model over the joint space of attribute ranking outputs and propose a novel form of zero shot learning in which the supervisor relates the unseen object category to previously seen objects via attributes for example bears are furrier than giraffes we further show how the proposed relative attributes enable richer textual descriptions for new images which in practice are more precise for human interpreta tion we demonstrate the approach on datasets of faces and natural scenes and show its clear advantages over tradi tional binary attribute prediction for these new tasks introduction while traditional visual recognition approaches map low level image features directly to object category labels recent work proposes models using visual attributes attributes are properties observable in images that have human designated names e g striped four legged and they are valuable as a new semantic cue in various problems for example researchers have shown their im pact for strengthening facial verification object recog nition generating descriptions of unfamiliar ob jects and to facilitate zero shot transfer learning where one trains a classifier for an unseen object simply by specifying which attributes it has problem most existing work focuses wholly on at tributes as binary predicates indicating the presence or ab sence of a certain property in an image this may suffice for part based attributes e g has a head and some a smiling b c not smiling d natural e f manmade figure binary attributes are an artificially restrictive way to describe images while it is clear that a is smiling and c is not the more in formative and intuitive description for b is via relative attributes he is smiling more than a but less than c similarly scene e is less natural than d but more so than f our main idea is to model relative attributes via learned ranking functions and then demonstrate their impact on novel forms of zero shot learning and generating image descriptions binary properties e g spotted however for a large va riety of attributes not only is this binary setting restrictive but it is also unnatural for instance it is not clear if in fig ure b hugh laurie is smiling or not different people are likely to respond inconsistently in providing the presence or absence of the smiling attribute for this image or of the natural attribute for figure e indeed we observe that relative visual properties are a semantically rich way by which humans describe and com pare objects in the world they are necessary for instance to refine an identifying description the rounder pillow the same except bluer or to situate with respect to ref erence objects brighter than a candle dimmer than a flashlight furthermore they have potential to enhance active and interactive learning for instance offering a bet ter guide for a visual search find me similar shoes but shinier or refine the retrieved images of downtown chicago to those taken on sunnier days proposal in this work we propose to model relative at tributes as opposed to predicting the presence of an at tribute a relative attribute indicates the strength of an at tribute in an image with respect to other images for exam ple in figure while it is difficult to assign a meaningful value to the binary attribute smiling we could all agree on the relative attribute i e hugh laurie is smiling less than scarlett johansson but more than jared leto in addition to being more natural relative attributes would offer a richer mode of communication thus allowing access to more de tailed human supervision and so potentially higher recog nition accuracy as well as the ability to generate more in formative descriptions of novel images how can we learn relative properties whereas tradi tional supervised classification is appropriate to learn at tributes that are intrinsically binary it falls short when we want to represent visual properties that are nameable but not categorical our goal is instead to estimate the degree of that attribute presence which importantly differs from the probability of a binary classifier prediction to this end we devise an approach that learns a ranking function for each attribute given relative similarity constraints on pairs of examples or more generally a partial ordering on some examples the learned ranking function can esti mate a real valued for images indicating the relative strength of the attribute presence in them then we intro duce novel forms of zero shot learning and description that exploit the relative attribute predictions the proposed ranking approach accounts for a subtle but important difference between relative attributes and con ceivable alternatives based on regression or multi way clas sification while such alternatives could also allow for a richer vocabulary during training they could suffer from similar inconsistencies as binary attributes for example it is more difficult to define and perhaps more importantly agree on with what strength is he smiling than is he smiling more than she is thus we expect the relative mode of supervision our approach permits to be more natu ral and consistent for human labelers contributions our main contribution is the idea to learn relative visual attributes which to our knowledge has not been explored in any prior work our other contribution is to devise and demonstrate two new tasks well served by relative attributes zero shot learning from relative com parisons and image description in reference to example images or categories we demonstrate the approach for both tasks using the outdoor scenes dataset and a subset of the public figure face database we find that rela tive attributes yield significantly better zero shot learning accuracy when compared to their binary counterparts in addition we conduct human subject studies to evaluate the informativeness of the automatically generated image de scriptions and find that relative attributes are clearly more powerful than existing binary attributes in uniquely identi fying an image this paper we refer to rank as a real valued score related work we review related work on visual attributes other uses of relative cues and methods for learning comparisons binary attributes learning attribute categories allows prediction of color or texture types and can also pro vide a mid level cue for object or face recognition beyond object recognition the semantics intrinsic to at tributes enable zero shot transfer or descrip tion and part localization rather than manually define attribute vocabularies some work aims to discover attribute related concepts on the web extract them from existing knowledge sources or discover them interactively in contrast to our approach all such meth ods restrict the attributes to be categorical and in fact bi nary relative information relative information has been ex plored in vision in a variety of ways recent work on large scale recognition exploits wordnet based information to specify a semantic distance sensitive classifier or to make do with few labels by sharing training images among semantically similar classes stemming from a related motivation of limited labeled data wang et al make use of explicit similarity based supervision such as a ser val is like a leopard or a zebra is similar to the cross walk in texture to share training instances for categories with limited or no training instances unlike our approach that method learns a model for each object category and does not model attributes in contrast our attribute models are category independent and transferrable enabling rela tive descriptions between all classes moreover whereas that technique captures similarity among object categories ours models a general ordering of the images sorted by the strength of their attributes as well as a joint space over mul tiple such relative attributes kumar et al explore comparative facial attributes such as lips like barack obama for face verification these attributes although comparative are also modeled as binary classifiers and are similarity based as opposed to an ordering gupta et al and siddiquie et al use prepositions and adjectives to relate objects to each other for more effective contextual modeling and active learning re spectively in contrast our work involves relative modeling of attribute strengths for a richer vocabulary that enhances supervision and description of images learning to rank learning to rank has received exten sive attention in the machine learning literature for information retrieval in general and image retrieval in particular given a query image user preferences often captured via click data are incorporated to learn a ranking function with the goal of retrieving more relevant images in the top search results learned distance metrics e g can induce a ranking on images however this ranking is also specific to a query image and typically intended for nearest neighbor based classifiers our work learns a ranking function on images based on constraints specifying the relative strength of attributes and the result ing function is not relative to any other image in the dataset thus unlike query centric retrieval tasks we can charac terize individual images by the strength of the attributes present which we show is valuable for new recognition and description applications approach we first present our approach for learning relative at tributes section and then explain how we can use rela tive attributes for enhanced zero shot learning section and image description generation section learning relative attributes we are given a set of training images i i repre figure distinction between learning a wide margin ranking function right that enforces the desired ordering on training points and a wide margin binary classifier left that only separates the two classes and and does not necessarily preserve a desired ordering on the points rearranging the constraints reveals that the above formu lation without the similarity constraints in eqn is quite similar to the svm classification problem but on pairwise difference vectors minimize wt c tributes a am in addition for each attribute am we t wt xi xj ξij i j om are given a set of ordered pairs of images om i j wt x x γ i j s and a set of un ordered pairs sm i j such that m i j ij m i j om i j i e image i has a stronger pres ence of attribute am than j and i j sm i j i e i and j have similar relative strengths of am we note that om and sm can be deduced from any partial ordering of the images i in the training data with respect to strength of am either om or sm but not both can be empty our goal is to learn m ranking functions rm xi wt xi for m m such that the maximum number of the following constraints is satisfied i j om wt xi wt xj i j sm wt xi wt xj ξij γij where c is the trade off constant between maximiz ing the margin and satisfying the pairwise relative con straints we solve the above primal problem using newton method while we use a linear ranking function in our experiments the above formulation can be easily extended to kernels we note that this learning to rank formulation learns a function that explicitly enforces a desired ordering on the training images the margin is the distance between the clos est two projections within all desired training rankings in contrast if one were to train a binary classifier only the margin between the nearest binary labeled examples is en forced ordering among examples beyond those defining the m m margin is arbitrary see figure our experiments confirm while this is an np hard problem it is possible to approximate the solution with the introduction of non negative slack variables similar to svm classification we directly adapt the formulation proposed in which was originally applied to web page ranking except we use a quadratic loss function together with similarity constraints leading to the following optimization problem minimize wt c this distinction does indeed matter in practice as our learnt ranking function is more effective at capturing the relative strengths of the attributes than the score of a binary classi fier i e the magnitude of the svm decision function in addition training with comparisons image i is simi lar to j in terms of attribute am or i exhibits am less than is well suited to the task at hand attribute strengths are arguably more natural to express in relative terms as op posed to requiring absolute judgments in isolation i e i represents am with degree t wt xi wt xj ξij i j om tings zero shot learning with relative relationships and m m t t generating image descriptions we now introduce our wmxi wmxj γij i j sm ξij γij approach to incorporate relative attributes for each of these applications in turn zero shot learning from relationships ages from class c so we have c n µ σ for consider n categories of interest for example each i i s i i i category may be an object class or a type of scene dur ing training s of these categories are seen categories for which training images are provided while the remaining u n s categories are unseen for which no training images are provided the parameters of the generative model corresponding to each of the u unseen categories are selected under the guidance of the input relative descriptions in particular given an unseen category c u we employ the following if c u is described as c c u c where c respect to each other be it pairwise relationships or partial orders for example bears are furrier than giraffes but less and c are seen categories then we set the m th com ponent of the mean µ u to µ µ jm im km furry than rabbits lions are larger than dogs as large as if c u is described as c c we set µ to tigers but less large than elephants etc we note that all j i j jm pairs of categories need not be related in the supervision and different subsets of categories can be related for the dif ferent attributes the u unseen categories on the other hand are de scribed relative to one or two seen categories for a subset u µ dm where dm is the average distance between the sorted mean ranking scores µ of seen classes for attribute am it is reasonable to expect the unseen class to be as far from the specified seen class as other seen classes tend to be from each other similarly if c u is described as c u c we set u u µ u to µ dm as ci cj ck for attribute am or ci cj or jm im c u c where c and c are seen categories we if am is not used to describe c u we set µ u to be the note the simple and flexible supervision required for the cat egories especially the unseen ones for any attribute not mean across all training image ranks for am and the m th diagonal entry of σ u to be the variance of the necessarily all the user can select any seen category de picting a stronger and or weaker presence of the attribute same in the first three cases we simply set σ u s σ list based learning to rank techniques are available we choose the pairwise learning technique as described in sec tion to ensure this ease of supervision during testing a novel image is to be classified into any of the n categories our zero shot learning setting is more given a test image i we compute x i rm indicating the relative attribute ranking scores for the image it is then assigned to the seen or unseen category that assigns it the highest likelihood general than the model proposed by lampert et al in that the supervisor may not only associate attributes with categories but also express how the categories relate along c argmax j n p x i µj σj any number of the attributes we expect this richer repre sentation to allow better divisions between both the unseen and seen categories as we demonstrate in the experiments we propagate the category relationships provided during training to the corresponding images i e for seen classes from a bayesian perspective our approach to setting the parameters of the unseen categories generative models can be considered to be priors transferred from the knowledge of the models for the seen categories under reasonable pri ors the choice of mean and covariances correspond to the c and c c c i j i c j c minimum mean squared error and maximum likelihood es for attribute am we then learn all m relative attributes as described in section predicting the real valued rank of all images in the training dataset i allows us to transform xi rn x i rm such that each image i is now represented as an m dimensional vector x i indicating its rank score for all m attributes we now build a generative model for each of the s seen categories in rm we use a gaussian distribution and estimate the mean µ rm and m m covari ance matrix σ from the ranking scores of the training im generalizes naturally to allow stronger supervision per image in stance when available timates related formulations of transfer through parameter sharing have been studied by fei fei et al and stark et al for learning shape based object models with few training images though no prior models consider transfer ring knowledge based on relative comparisons as we do here we note that if one or more images from the unseen categories were subsequently to become available our esti mated parameters could easily be updated in light of the ad ditional evidence furthermore our general approach could potentially support more specific supervision about the rela tive relationships should it be available e g bears unseen are significantly more furry than cows seen describing images in relative terms the second application of relative attributes that we pro pose is that of describing novel images the goal is to be able to relate any new example to other images according to different properties whether its class happens to be fa miliar or not this basic functionality would allow for in stance the meaningful search example applications given in the introduction see recent work in for other forms of image description based on object action scene tags during training we are given a set of training images i i each represented by a feature vector xi rn a list a am of m attributes along with om i j t i j and sm i j t i j in relative strength of am we learn m ranking functions as described in sec tion and evaluate them on all training images in i given a novel image j to be described we evaluate all learnt ranking functions rm xj for each attribute am we identify two reference images i and k from i that will be used to describe j via relative attributes in principle with a good ranking function any reference images could be infor mative in our implementation we adhere to the following guidelines to avoid generating an overly precise descrip tion we wish to select i and k such that they are not very similar to j in terms of attribute strength however to avoid trivial descriptions they must not be too far from j either hence we pick i and k such that i j and j k in strength of a and th of the images in i lie between i and j as well as between j and k in the case of boundary conditions where no such i or k exist i is chosen to be the image in i with the least strength of am and k is set to the image in i with the highest strength of am the image j can then be described in terms of all or a subset of the m attributes relative to any identified pairs i k figure shows an example description generated by our approach as well as an illustration of selected pairs i k while more elaborate analysis of the dataset distribution and even psychophysics knowledge of the sensitivity of humans to change in different attributes could make the selection of reference images more effective we employ this straightforward technique as a proof of concept and leave such analysis for future work experiments we evaluate our approach on two datasets outdoor scene recognition osr dataset containing images from categories we use the dimensional gist descriptor as our image features a subset of the public figure face database pubfig con taining images from random identities images this application does not require category labels the relative supervision can be provided for categories which is propagated to images table binary and relative attribute assignments used in our experiments note that none of the relative orderings violate the binary memberships the osr dataset includes images from the following categories coast c forest f highway h inside city i mountain m open country o street s and tall building t the attributes shown above are listed in as the properties subjects used to organize the images the pubfig dataset includes images of alex rodriguez a clive owen c hugh laurie h jared leto j miley cyrus m scarlett johansson s viggo mortensen v and zac efron z the attributes shown above are a subset of the attributes provided with the dataset they were chosen for their simplicity sufficient variation among the categories and to avoid redundancy e g using young instead of old middle aged youth child each we use a concatenation of the gist descriptor and a dimensional lab color histogram as our image features table provides more details about the datasets and shows the binary memberships and relative orderings of categories by attributes these were collected using the judgements of a colleague unfamiliar with the details of this work we see the limitation of binary attributes in distin guishing between some categories while the same set of at tributes used relatively tease them apart although we have a full ordering in our experiments we sample random pairs of categories as supervision as noted below recall that different pairs of categories can be related for different at tributes note that we collect the binary supervision only to train baseline approaches our approach uses only the rela tive supervision as a sanity check we first demonstrate the superiority of our learnt ranks to capture relative orderings as compared to an approach that treats the score of binary classifiers as a rank section then we evaluate the use of relative attributes for the two new tasks sections and learned ranking vs classifier scores we train a binary linear svm hm by transferring the bi nary supervision listed in table to the training images for each attribute for an image pair i j in a held out test set images for osr for pubfig we evaluate the learnt classifier and if hm xi hm xj we predict i j else i j for am for comparison we learn a linear ranking function rm for each attribute using the relative constraints in table and compare rm xi to rm xj on the same test pairs both methods predictions are then compared to the ground truth relative ordering the learnt ranking function accuracy is and on the osr and pubfig datasets respectively as compared to and if using the binary classifier scores con firming the advantage of a ranking function to effectively osr unseen categories pubfig unseen categories capture relative information figure zero shot learning performance as the proportion of unseen cat egories increases total number of classes n remains constant at zero shot learning results we compare our zero shot approach to two baselines baselines our first baseline is the direct attribute pre diction dap model of lampert et al which uses bi nary attribute descriptions for all categories we train linear svms by transferring the binary supervision in table to training images from the seen categories a test image x is assigned to a category using osr labeled pairs pubfig labeled pairs m c argmax c x c n m figure zero shot learning performance as more pairs of seen categories are related i e labeled during training unseen categories demonstrate the benefit of the genera tive modeling of the categories in sra where p am bc x is computed by transforming the binary classifier score via a sigmoid function and bc is the ground truth binary bit taken by attribute am for class c as seen in table if am is not used to describe an unseen category p am bc x is uniform we call our second baseline score based relative at tributes sra it follows the same approach as in sec tion except that it replaces rank values with the binary classifier output score it is a stronger baseline than dap as it has the same benefits of the generative modeling of seen classes and relative descriptions of unseen classes as our approach it is not limited by the binary description of the categories which may be deprived as seen in table set up we compare all methods in several different sce narios unless specified we use unseen and seen cate gories to train the ranking functions we use category pairs among seen categories and unseen categories are de scribed relative to the two closest seen categories for each attribute one stronger one weaker we use training im ages per class and the rest for testing and report mean per class accuracy over random train test and seen unseen splits proportion of unseen categories we first study zero shot learning accuracy as the proportion of unseen cate gories increases figure shows the results first we see even when all categories are seen un seen our approach significantly outperforms both base lines this validates the power of relative attributes for the classical recognition task also sra gains over dap with further as we would expect accuracy for all three ap proaches decreases with more unseen categories however our method remains better than the baselines for most of the spectrum until only seen categories remain at which point it performs similarly to sra this is expected since beyond that with only seen categories the relative and bi nary supervision becomes equivalent both still compare favorably to dap due to the benefit of relative description in general we can expect that with even more total cate gories the description power of relative attributes will also increase as unseen categories would have more categories to be related to even with a fixed number of attributes a binary description on the other hand can only lose discrim inative power as more categories are added amount of supervision we next study the impact of varying the amount of supervision figure shows the results as we increase the number of pairs of seen categories used to generate relative con straints where for each attribute we randomly select the performance is quite robust to the number and choice of pairs as few as two pairs suffice when using only one pair our method receives significantly less supervision than the two baselines for which all six categories are labeled hence their flat curves in spite of this our approach per forms favorably on osr though suffers compared to sra on pubfig there are a total of possible pairs to be labeled as few as of them could determine a unique ordering on all categories more natural than less natural than osr more open than less open than image is more open than less open than shows more than shows less than dap sra att to describe unseen att to figure part of example description generated for left image by binary attribute baseline middle and our method right see text for details to the power of relative attributes to jointly carve out re gions in the space of attribute strengths corresponding to figure zero shot learning performance as fewer describe the unseen categories osr the unseen category this makes the distance of the refer ence categories less relevant as long as the relationships are correctly indicated looseness of constraints 40 20 looseness of constraints describing images results next we demonstrate our approach to generate relative descriptions of novel images to quantify their effective ness we perform a human subject study that pits the binary attribute baseline against our relative approach our method reports the properties predicted relative to reference images figure zero shot learning performance as the unseen categories are de scribed via looser relationships figure shows the results as we decrease the number of attributes used to describe the unseen category during training note that the number of attributes used to describe the seen categories during training remains the same see item in sec the accuracy of all methods degrades however the approaches using relative attributes sra and ours decay gracefully whereas dap suffers more dramat ically this illustrates how each attribute conveys stronger distinguishing power when used relatively this is a key re sult this scenario exemplifies the high level of flexibility in supervision of unseen categories that our approach enables which is crucial for practical applications quality of supervision what happens if the relation ships described for an unseen class are looser that is what if the annotator relates it to seen classes whose attribute strengths are more distant e g says miley is younger than vitto rather than miley is younger than scarlett a person closer in age ideally the supervisor would have freedom to specify any reference categories that is the most natural form of description and does not require the supervisor to know the exhaustive list of seen categories thus we next evaluate performance as we in crease the number of relative ranks away looseness from the seen categories used to describe the unseen category figure shows the results we see our approach is very robust to the looseness of the constraints we attribute this any level of looseness if there exists no seen category at a desired distance from the unseen category in either direction we simply use a one ended constraint hence when the constraints are at a looseness of since only out of categories are seen some of which often have similar attribute strengths a large percentage of the constraints are one sided see sec while the baseline reports the predicted pres ence absence of attributes only the human subject must guess which image led to the auto generated descriptions to our knowledge these are the first results to quantify how well algorithm generated attribute descriptions can commu nicate to humans we recruited subjects only some familiar with vi sion we randomly selected 20 pubfig and osr im ages for each of the test cases we present the subject a description using three randomly selected attributes plus a multiple choice set of three images one of which is cor rect the subject is asked to rank their guesses for which fits the description best see figure a to avoid bias we divided the subjects into two groups each group saw either the binary or the relative attributes but not both further we display reference images for either group task to help subjects understand the attribute meanings figure b shows the results subjects are significantly more likely to identify the correct image using our method description i e vs in the first choice this re inforces our claim that relative attributes can better capture the concept of the image and suggests their real promise for improved guided search or interactive learning we note that we augmented the baseline binary de scriptions with prototype images showing stark contrast of attribute presence even though unlike our approach they are not an intrinsic part of the generated description we suspect that subjects would perform even worse with purely textual binary descriptions thus the human study is if anything generous to the baseline our approach can be used to generate purely textual de scriptions as well where an image is described relative to other categories instead of images figure c shows ex amples here our method selects the categories to compare which image is more chubby than less chubby than more natural than tallbuilding less natural than forest more open than tallbuilding less open than coast more than tallbuilding more natural than insidecity less natural than highway more open than street less open than coast more than highway less than insidecity more natural than tallbuilding less natural than mountain more open than mountain less than opencountry more white than alexrodriguez more smiling than jaredleto less smiling than zacefron more visibleforehead than jaredleto less visibleforehead than mileycyrus more white than alexrodriguez less white than mileycyrus less smiling than hughlaurie more visibleforehead than zacefron less visibleforehead than mileycyrus more young than cliveowen less young than scar more bushyeyebrows than zacefron less bushyeyeb alexrodriguez more roundface than cliveowen than zacefron a b c d e f c images described relative to categories figure auto generated descriptions of images in c a bin not nat ural not open perspective rel more natural than tallbuilding less natu ral than forest more open than tallbuilding less open than coast more perspective than tallbuilding b bin not natural not open perspec tive rel more natural than insidecity less natural than highway more open than street less open than coast more perspective than highway less perspective than insidecity c bin natural open perspective rel more natural than tallbuilding less natural than mountain more open than mountain less perspective than opencountry d bin white not smil ing visibleforehead rel more white than alexrodriguez more smiling than jaredleto less smiling than zacefron more visibleforehead than jaredleto less visibleforehead than mileycyrus e bin white not smiling not visibleforehead rel more white than alexrodriguez less white than mileycyrus less smiling than hughlaurie more visiblefore head than zacefron less visibleforehead than mileycyrus f bin not young bushyeyebrows roundface rel more young than cliveowen less young than scarlettjohansson more bushyeyebrows than zacefron less bushyeyebrows than alexrodriguez more roundface than clive owen less roundface than zacefron to such that at least of the images in the category have an attribute strength larger than less than that computed for the image to be described echoing our quantitative results we can qualitatively see that the relative descrip tions are more precise and informative than the binary ones more results can be found on the authors websites conclusion we introduced relative attributes which allow for a richer language of supervision and description than the commonly used categorical binary attributes we pre sented two novel applications zero shot learning based on relationships and describing images relative to other images or categories through extensive experiments as well as a human subject study we clearly demonstrated the advan tages of our idea future work includes exploring more novel applications of relative attributes such as guided search or interactive learning and automatic discovery of relative attributes acknowledgements we thank the subjects of our hu man studies for their time this research is supported in part by nsf iis onr atl and the luce foundation to appear proceedings of the ieee conference on computer vision and pattern recognition cvpr finding the weakest link in person detectors devi parikh toyota technological institute chicago ttic c lawrence zitnick microsoft research redmond abstract detecting people remains a popular and challenging problem in computer vision in this paper we analyze parts based models for person detection to determine which components of their pipeline could benefit the most if im proved we accomplish this task by studying numerous de tectors formed from combinations of components performed by human subjects and machines the parts based model we study can be roughly broken into four components fea ture detection part detection spatial part scoring and con textual reasoning including non maximal suppression our experiments conclude that part detection is the weakest link for challenging person detection datasets non maximal suppression and context can also significantly boost per formance however the use of human or machine spatial models does not significantly or consistently affect detec tion accuracy introduction object detection remains an open and challenging prob lem in computer vision historically the subclass of detect ing people has attracted increased attention given its impor tance to many real world applications and its challenging level of difficulty the wide variety of poses and shapes people exhibit along with variations in clothing creates a very challenging task for modeling and learning algorithms recently person detectors have made significant progress using part based models the appearance of each part such as a person head foot or torso are represented by histograms of gradients hog color or harr wavelets the spatial relationships of object parts can be represented using trees k fans or constellation models each of these approaches propose a complex set of interdependent components to provide final detection results while the additional complexity of the approaches have led to increased performance understanding the role of each component in the final detection accuracy is diffi cult in this paper we propose a thorough analysis of parts based models to gain insight into which components of the input output figure in order to gain insight into which components of a parts based person detector could benefit the most if improved we replace each component i e part detection p feature extract f spatial modeling sm and non maxima suppresion nms in the pipeline with human sub jects green bars here we illustrate the various tasks performed by human subjects via example input output pairs pipeline could benefit the most if improved we accomplish this task by using human subjects to perform the individ ual components previously performed by the machine al gorithm for instance instead of using a machine classifier such as a latent svm trained on hog descriptors to de tect object parts we use human subjects to label whether a small image patch contains a human head foot torso etc a parts based detector can be roughly broken into four components feature detection part detection spatial part scoring and contextual reasoning including non maximal suppression we combine numerous human and machine performed components to form complete person detectors and recognizers the results indicate which components lead to the greatest increase in accuracy over the standard machine approach the experiments include the use of var ious feature types such as color edges and intensities for both detecting people and parts the use of human detected parts with a machine spatial model and machine detected parts with using a human spatial model other experi ments analyzing non maximal suppression techniques and contextual information are also performed related work we now discuss some existing techniques for person de tection as well works that conduct human studies to gain insights in computer vision we comprehensively discuss works on parts spatial models and contextual models in section person pedestrian detection given the importance of detecting people in images numerous detectors have been proposed a comparison of several approaches for pedes trian detection can be found in dollar et al wojek et al analyzes several features and classifier types dalal and triggs first proposed the locally normalized histogram of gradients detector which was improved upon by felzen szwalb et al using deformable parts models increased performance was found using numerous feature types and boosting by dollar et al and using multi level features and intersection kernel svms by maji et al human studies an early example of designing compu tational models with similar behavior to humans is shown in david marr book liu et al conducted human studies demonstrating that the high human performance in object discrimination can only be explained if humans are using information tarr et al and hinton et al studied whether humans use mental rotation for recog nition and determining if shapes have the same handiness a comparison of human and machine algorithms for select ing regions of interest in images was conducted by privit era et al fei fei et al demonstrated that human subjects can provide a large amount of detailed information about a scene even after viewing it for a very brief period of time bachmann et al show that humans can reliably recognize faces in images as small as pixels and oliva et al present similar results for scene recogni tion torralba et al and parikh et al show that humans can detect objects in images with signifi cantly higher performance than state of the art machine al gorithms using high resolution images the work of parikh et al uses human studies to determine if features clas sification algorithms or the amount of training data is most likely to account for the superiority of humans over ma chines in recognizing objects and scenes part based detector in this section we describe machine models for various components in a part based detector including feature ex traction parts modeling spatial models non maximal sup pression and contextual reasoning before we describe the corresponding set up for our human studies for each stage we follow the approach of felzenszwalb et al that has shown recent state of the art performance and briefly out line other approaches our studies are performed on sub sets of the commonly used inria dataset and the more challenging pascal dataset feature extraction and modeling parts numerous low level features and representations have been proposed for modeling objects and their parts rep resentations have progressed from modeling textures to histograms of gradients with global normalization and local normalization the work of felzenszwalb et al improved upon to reduce its dimensionality and increase accuracy methods using color and gradients without histograms have also been proposed wavelet approaches have shown benefits in computational efficiency several methods combine various features using decision trees or boosting techniques represen tations may also be learned using random decision forests feature mining deep belief nets mixture mod els or biologically inspired models in this paper we use the part detectors of felzenszwalb et al trained via a latent svm on histogram of oriented gradient features the models were pre trained and sup plied by felzenzswalb et al each component of the model contains a root filter and six part filters while provides two component models we only used one com ponent since slightly better results were achieved using a single component model on the datasets used in this paper the part detections were obtained by independently apply ing the part filters spatial model the spatial relationship of parts can be modeled using several previously proposed techniques constellation mod els use gaussian distributions to represent the rela tive positions of parts more restrictive but computationally efficient methods have been proposed using tree and k fan models tree based deformable models called pictorial structures provide both efficient detection and learning the appearance of objects may also be represented using multiple templates aspect graphs or by linking parts from different viewpoints we use a star graph spatial model similar to felzen szwalb et al the model assumes that the location of the parts are independent given the location of the person the locations of the parts relative to the person are modeled via a gaussian distribution with mean and co variance pa rameters µi σi for the ith part all co ordinates are nor malized with respect to the hypothesized size of the person each person candidate window is scored as max si x y overlapped with a higher scoring window overlap is com puted as the ratio of the intersection and union of the two windows we used an overlap threshold of experimental setups for human studies our experiments involve replacing various components where k is the number of parts in the model and si is the score associated with part i at location x y si x y sˆi x y ai x bi y ci x di y where sˆi x y is the score of the ithpart detector at loca tion x y and x and y are the positional offsets from the part mean position the coefficients ai bi ci di which model the covariance are learnt discriminatively via a linear svm to distinguish positive windows across the training dataset with overlap with a ground truth person bounding box from the negative windows with overlap with a ground truth person bounding box the mean parameters of the star graph µi are learnt through maximum likelihood estimation over the positive training windows using part detections that maximize si x y with the newly estimated mean parameters a new set of covari ance coefficients ai bi ci di are learnt resulting in an iterative learning procedure we initialize µi as the weighted mean of the part detec tions within the ground truth person bounding boxes in the training images the weights correspond to the part detec tion scores the coefficients ai bi ci di are initialized to context and non maximal suppression recently the use of context has received significant at tention for object recognition and detection context pro vides a useful aid for determining likely positions of objects using scene information or the location of other objects pairwise interactions of objects can be modeled using crfs or as a max margin learning problem the related problem of non maximal suppression this section we describe the techniques we employ the green bars in figure illustrate the various human studies we performed for human testing we broke the pipeline into four stages feature extraction part detection spatial modeling and nms context there are possible combi nations of contiguous stages that the human could perform of which we test we do not perform the feature extrac tion stage alone since we cannot get direct access to the features extracted by the human brain for further process ing with a machine in addition we do not perform the nms context stage alone all our human studies were per formed on amazon mechanical turk our experiments were conducted on inria and pascal images containing and people re spectively we hand labeled all the faces in the images and re scaled the images so that the faces were a canonical size fixing the scale reduces our search space making our human studies feasible the machine implementation with fixed scale gave an average precision ap of for our inria images and for our pascal images feature extraction f given a natural image human subjects may extract any low level features necessary for recognition however if we pre process images to retain only some of the informa tion we can constrain the low level features accessible to the human subjects in our experiments we show subjects grey scale images normalized gradient images and colored images at both high and low resolutions a normalized gra dient gˆ x y at pixel x y with gradient g x y is com puted as follows nms attempts to remove redundant detections of the same object this can be viewed as contextual information shared between objects of the same class i e two of the same ob g x y gˆ x y g x y ject cannot typically occupy overlapping areas of the im age in fact some approaches inherently solve nms in their multi object contextual models in this paper we only use nms and not more complex contextual models as the performance gains provided by the complex models were minimal on the pascal dataset we performed nms by removing windows that where g x y is a gaussian weighted average with a stan dard deviation of and is used to ensure g x y is above the level of noise for visibility the maximum nor malized gradient within a patch is scaled to see figures and for examples figure illustrates the settings where human subjects use their internal feature extractor a b e or are constrained by machine extracted features c d f human machine color high res grayscale high res norm grad high res color low res grayscale low res norm grad low res figure part detection visualizations created for human and machine detected parts hp ve windows b hp ve windows figure example patches classified by humans as left to right head torso legs and background in top to bottom regular grey scale low reso lution and normalized gradient images part detector p similar to the machine part detector our human studies use a sliding window approach overlapping small patches are extracted from the images figure e f human sub jects were randomly shown these patches across all images so no contextual information was available subjects were asked to classify each patch as containing a head torso arm hand leg foot any other part of a person or not a person at all each patch was classified by subjects ex ample patches shown to humans using color grey scale and normalized gradient images are shown in figure visu alizations of the detected parts aggregated across subjects are shown in figures and the different colors cor respond to different parts red head blue torso green arm yellow hand magenta leg and cyan feet the intensity of the color corresponds to the number of subjects that classi fied the local patch as the corresponding part analogous to the detector of felzenszwalb et al we also detect roots in a similar sliding window fashion which are low resolution templates of a person shown in white in fig ure we used root and part sizes similar to the machine im plementation specifically for inria the part patch sizes extracted from images were pixels and the root win dow sizes were 150 for pascal the part sizes were and the root sizes were in felzen szwalb et al the spatial resolution of the root is lower than that of the other parts similarly we downsample the root windows leading to an effective resolution of root detections are not shown in figure for simplicity c mp ve windows d mp ve windows figure training data with positive ve windows containing a person left and negative ve windows not containing a person right shown to human subjects for learning a spatial model windows are cropped from part visualizations for top human detected parts hp and bottom ma chine detected parts mp in gray scale pascal images for inria and for pascal for low resolution part detection in both inria and pascal the resolution of the parts was reduced to 20 and the roots were scaled to pixels in the largest dimension for easy viewing the parts and roots were displayed to subjects with the largest dimension scaled to pixels the part patches were sam pled with overlap between consecutive patches and the root windows were sampled at overlap spatial model sm to study the ability of the human subjects to reason about spatial relationships we train the subjects using the colored part visualizations as shown in figure sub jects are then asked to classify windows using the same part visualizations as containing a person or not see fig ure h the set of windows are overlapping and ran domly sampled subjects classified each window a con fidence score was computed as the average number of sub jects classifying a window as containing a person standard non maximal suppression can be performed by a machine using the confidence scores on all windows in an image fi nally a precision recall curve is computed to quantify the human subjects performance we note that similar part detection visualizations can be created for both human and machine detected parts as shown in figure this allows visualize the part detections of felzenszwalb et al which con tain highly overlapping detections we perform non maximal suppression among the parts each part is mapped to a color with intensity corre sponding to the estimated likelihood of a person given the part score fur ther evaluation indicates that our nms processing of the parts actually increases the machine ap for inria by and for pascal by a b c d e f g machine human human person detector inria pascal human part detector inria pascal h i j k figure summary of all experiments performed to test the use of hu mans and machines to perform various combinations of the components in a parts based person detector us to evaluate the human spatial model on machine or hu man detected parts inversely the human detected parts can be fed into a machine spatial model one variation of the above experiments used for further analysis is to show subjects windows extracted from natural images instead of part visualizations figure b this is equivalent to using human extracted features parts as well as the human spatial model we may also restrict the fea tures available to the human subjects by pre processing the images figure d while still using human detected parts and spatial models context and non maximal suppression nms in the human studies on spatial modeling we asked the subjects to classify cropped windows extracted from the im ages without context we can study nms and contextual reasoning by showing the subjects information over the en tire image and asking them to draw bounding boxes around detected persons figure a c g by performing this task subjects are implicitly performing nms and can use con textual information if provided as shown in figure the information shown to the subjects is of three types a orig inal color images c images after feature extraction and g part detections results in this section we provide the results of numerous ma chine and human studies we analyze the results with re spect to the four detector components feature extraction part detection spatial models and context nms we also attempt to quantitatively compare the relative performance gains that may be achieved by improving each component of the detector an illustration of the various combinations of human and machine experiments is shown in figure figure effect of features accuracy of human person detector left and human part detector with machine spatial model and nms right in high h and low l resolution color c grey scale g and normalized gradient n images effect of features we can analyze the effect of feature types using humans as person and part sliding window classifiers we compare results using the original image figure b c with results using different feature types figure e f the results are summarized in figure we see that the loss of color or resolution does not significantly affect detection accuracies using normalized gradients alone did degrade human per formance significantly especially on the pascal dataset effect of parts in order to quantify the effect of better part detectors we compare the performance of sliding window detectors on parts detected by humans and machines we report re sults from showing grey scale image patches to the sub jects since the machine models do not use color similar results were found with other features types there are sev eral pairs of results we may consider as shown in figure these include using the machine spatial model with nms f k the human spatial model and machine nms g h and the human spatial model with human nms i j the results are shown in figure the use of human part de tections significantly improves the performance of the de tectors in most cases for the challenging pascal dataset the improvement is as high as for human detected parts over machine detected parts effect of spatial models we evaluate the effect of spatial models using a series of human studies on both machine and human detected parts as shown in figure we compare machine to human spa tial models using machine detected parts h k and human detected parts b c and f g the results are shown in figure the results indicate that human spatial models do not significantly or consistently affect the ap scores across the various scenarios inria pascal nms context inria nms pascal nms machine parts human parts hsw msw hd hsw msw hd inria pascal hp mp hp mp figure effect of different part detectors accuracy of humans hsw and machines msw spatial models followed by machine nms as well as using humans hd for both spatial models and nms on part figure effect of nms context accuracy of human detectors using left high resolution grey scale images and using two right plots human part hp and machine part mp visualizations visualizations the human parts are parts detected by subjects on grey scale images inria ch gh nh cl gl nl pascal ch gh nh cl gl nl figure effect of spatial models accuracy of human and machine spatial models on human part visualizations ch gh nh cl gl nl and machine part visualizations mp effect of non maximal suppression and context finally we study the influence of context and non maximal suppression nms on detection performance two sets of experiments from figure can be compared first we can compare the human subjects as sliding win dow classifiers followed by machine nms e to the human subjects as detectors using the entire image d the results are shown in figure left for grey scale images the use of the subjects contextual models and nms significantly increases performance by up to on the pascal dataset over using the machine nms second we can compare results using the part visualiza tions with human spatial models and machine nms g h to human spatial models and human nms i j the results are shown in figure right since the subjects only have ac cess to part detections they are limited to performing intra category contextual reasoning or nms as demonstrated in parikh et al the use of inter category contextual infor mation may not be necessary given high resolution appear ance information a similar finding is found by compar ing the experiments in figure that all use high resolution information the amount of improvement in the left plot using all contextual information is similar to the right two plots that only use intra category contextual information figure compares the accuracy of human subjects as person detectors a d to humans as sliding window classi figure effect of context humans can reliably leverage contextual information and maintain robust detection accuracy even with impover ished appearance information fiers followed by machine nms b e across feature types the accuracy of human subjects at classifying image win dows in isolation decreases significantly as the appearance information becomes weak e g normalized gradient low resolution images however their performance at detect ing people in entire images remains quite robust as previ ously shown in this signifies the importance of contex tual information under impoverished scenarios summary we summarize the potential improvements in object de tection accuracies based on our human and machine studies in figure we show the average improvement in accura cies for part detection spatial modeling and context nms and their variances in reference to figure results are computed from f k g h and i j for part detections b c f g and h k for spatial models and g i and h j for context nms we find parts to have the biggest impact fol lowed by non maximal suppression spatial models do not have significant impact the people in the pascal images as compared to those in the inria dataset demonstrate a wider variation in poses and often exhibit challenging sce narios such as occlusion and truncation thus we observe a higher potential for improvement on the pascal dataset as seen in figure we note that the estimates of potential improvements can be viewed as lower bounds since our hu man studies were performed on amazon mechanical turk where subjects may be distracted and often provide noisy human detector b human sliding window c machine spatial model figure example failure cases for scenarios with different amounts of human involvement correct detections are shown in white false positives in red and false negatives in yellow a even when subjects are shown the entire image highly occluded people in bad lighting are missed b when subjects classify windows in isolation from the rest of the image as containing a person or not lack of context leads to false positives when the windows locally appear to have parts of a person c a machine spatial model applied to near perfect human part detections fails because of symmetric part detections subjects were asked to classify patches as containing arms legs etc and were not asked to distinguish between left right arms legs etc 05 parts spatial models nms inria pascal be learnt jointly with spatial models as in moreover the weaker the part models the bigger role spatial models could play in the final detection performance our analy sis does not account for such dependencies among various components in the pipeline in our human studies subjects were instructed to find parts with semantic meaning heads torso etc since the patches were presented in isolation we do not expect this figure summary of our results the component of a parts based person detector that can improve detection performance the most is the part detection followed by the nms component spatial models do not affect the resultant performance significantly a machine part detections b resultant detections c human part detections d resultant detections figure example detections where human detected parts allow for successful detection white while machine detected parts lead to false positive red and false negative yellow detections responses example failure cases for both human subjects and machines are shown in figures and discussion our analysis is restricted to sliding window parts based models it assumes a pipeline where the parts and spatial models are considered to be independent part models could semantic knowledge to provide contextual information to subjects however machine object detectors have the free dom to model parts without semantic meanings this flex ibility may allow for the use of better parts but could also make the underlying learning problem intractable the accuracies of human subjects as person detectors on color and grey scale images is higher than any experiment using a combination of machine and human components this implies that the pipeline proposed for the machine de tector may not be the same as the human subjects in conclusion we presented numerous studies combin ing both machine and human components for detecting peo ple by analyzing their relative performance we can deter mine which components could offer the greatest boost in overall performance if improved our results show that part detection is the weakest link on challenging datasets such as pascal followed by non maximal suppression and con text human spatial models appear to offer negligible per formance increase over machine spatial models grey scale information provided the same level of accuracy as color however accuracies suffered when using only normalized gradients future work involves similar analysis for de tecting generic object categories and other object detection models peekaboom a game for locating objects in images luis von ahn ruoran liu and manuel blum computer science department carnegie mellon university forbes avenue pittsburgh pa biglou royliu mblum cs cmu edu abstract we introduce peekaboom an entertaining web based game that can help computers locate objects in images people play the game because of its entertainment value and as a side effect of them playing we collect valuable image metadata such as which pixels belong to which object in the image the collected data could be applied towards constructing more accurate computer vision algorithms which require massive amounts of training and testing data not currently available peekaboom has been played by thousands of people some of whom have spent over hours a day playing and thus far has generated millions of data points in addition to its purely utilitarian aspect peekaboom is an example of a new emerging class of games which not only bring people together for leisure purposes but also exist to improve artificial intelligence such games appeal to a general audience while providing answers to problems that computers cannot yet solve author keywords distributed knowledge acquisition object segmentation object recognition computer vision web based games acm classification keywords learning knowledge acquisition h hci web based interaction introduction humans understand and analyze everyday images with little effort what objects are in the image where they are located what is the background what is the foreground etc computers on the other hand still have trouble with such basic visual tasks as reading distorted text or finding where in the image a simple object is located although researchers have proposed and tested many impressive algorithms for computer vision none have been made to work reliably and generally most of the best approaches for computer vision e g rely on machine learning train an algorithm to perform a visual task by showing it example images in which the task has already been performed for example permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee chi april montréal québec canada copyright acm 00 training an algorithm for testing whether an image contains a dog would involve presenting it with multiple images of dogs each annotated with the precise location of the dog in the image after processing enough images the algorithm learns to find dogs in arbitrary images a major problem with this approach however is the lack of training data which obviously must be prepared by hand databases for training computer vision algorithms currently have hundreds or at best a few thousand images orders of magnitude less than what is required in this paper we address the problem of constructing a massively large database for training computer vision algorithms the target database will contain millions of images all fully annotated with information about what objects are in the image where each object is located and how much of the image is necessary to recognize it our database will be similar to those previously shown to be useful for training computer vision algorithms e g to construct such a database we follow the approach taken by the esp game and introduce a new game called peekaboom peekaboom is an extremely enjoyable networked game in which simply by playing people help construct a database for training computer vision algorithms we guarantee the database correctness even if the people playing the game don t intend it as we will show in this paper our game is also very enjoyable with some people having played over hours a week we will further show that this game can be used to improve image search results and to calculate object bounding boxes similar to those in flickr see figure the esp game is an interactive system that allows people to label images while having fun the esp game collects random images from the web and outputs word labels describing the contents of the images the game has already collected millions of labels for arbitrary images given an image the esp game can be used to determine what objects are in the image but cannot be used to determine where in the image each object is located such location information is necessary for training and testing computer vision algorithms so the data collected by the esp game is not sufficient for our purposes the game introduced in this paper peekaboom improves on the data collected by the esp game and for each object in the image outputs precise location information as well as other information useful for training computer vision algorithms by playing a game people help us collect data figure peek and boom boom gets an image along with a word related to it and must reveal parts of the image for peek to guess the correct word peek can enter multiple guesses that boom can see not because they want to be helpful but because they have fun indeed peekaboom or the esp game or any game built on this premise can be treated as a human algorithm on input an image it outputs with arbitrarily high probability a correct annotation of the image instead of using a silicon processor this algorithm runs on a processor consisting of regular humans interacting throughout the web in addition to applications in computer vision and image search our system makes a significant contribution to hci because of the way it addresses the problem peekaboom presents an example of a new line of research devoted to solving large scale problems with human computing power where people interact with computers to extend the computational abilities of machines basic game play peekaboom as the name may suggest is a game with two main components peek and boom two random players from the web participate by taking different roles in the game when one player is peek the other is boom peek starts out with a blank screen while boom starts with an image and a word related to it see figure the goal of the game is for boom to reveal parts of the image to peek so that peek can guess the associated word boom reveals circular areas of the image by clicking a click reveals an area with a pixel radius peek on the other hand can enter guesses of what boom word is boom can see peek guesses and can indicate whether they are hot or cold when peek correctly guesses the word the players get points and switch roles play then proceeds on a new image word pair if the image word pair is too difficult the two players can pass or opt out of the current image passing creates the same effect as a correct guess from peek except that the players get no points to maximize points boom has an incentive to reveal only the areas of the image necessary for peek to guess the correct word for example if the image contains a car and a dog and the word associated to the image is dog then boom will reveal only those parts of the image that contain the dog thus given an image word pair data from the game yield the area of the image pertaining to the word pings another component of the game are pings ripples that appear on peek screen when boom right clicks on the image see figure if two players were playing with the image on figure then many correct words are possible from peek point of view elephant trunk tusk ear suppose the correct word is trunk to get peek to guess correctly boom can ping the trunk of the elephant by right clicking on it in doing so boom helps to disambiguate the trunk from the rest of the elephant figure pings to help peek boom can ping parts of the image by right clicking on them figure hints boom can further help peek by giving hints about how the word relates to the image is it a noun describing something in the image a noun related to the image text on the image or a verb hints another feature of the game are buttons that allow boom to give hints to peek about how the word relates to the image see figures and upon boom pressing of one of the hint buttons a corresponding flashing placard appears on peek screen the reason for having hints is that often the words can relate to the image in multiple ways as nouns verbs text or related nouns something not in the image but related to it the origin of images and labels all words presented to the players are related to their corresponding image on input an image word pair peekaboom outputs a region of the image that is related to the word we obtain millions of images with associated keyword labels from the esp game which we now describe in more detail as mentioned before the esp game is a two player online game that pairs random players from the web from the player perspective the goal of the esp game is to guess the word that their partner is typing for each image once both players have typed the same string they move on to a next image since the players can t communicate and don t know anything about each other the easiest way for both to type the same string is by typing something related to the common image the string upon which the two players agree is a very good label for the image we use the labels collected from the esp game as the words we present to the players in peekaboom game points and the bonus round although the exact number of points given to the players for different actions is not important we mention it to show the relative proportions furthermore we mention the different point strategies used by peekaboom to keep players engaged points are given to both peek and boom equally whenever peek guesses the correct word in the current implementation both obtain points points are not subtracted for passing points are also given to both peek and boom for using the hint buttons although this might appear counterintuitive since using hints deducts points in many other games we actually want the players to use the hint buttons as mentioned above hints give us additional information about the relationship between the word and the image and therefore we encourage players to use them twenty five extra points are given to both peek and boom whenever peek guesses the correct word and boom had used a hint points are not given for usage of the hot cold buttons every time the players correctly complete four images they are sent to a bonus round the bonus round is different in nature from the rest of the game and allows players to obtain up to points in the bonus round see figure players simply click on an object in the image the closer they are to each other clicks the more points they get for example both players could obtain an image of a car and be told click on the car figure the peekaboom bonus round players must click on the specified object within the image they obtain points proportional to how close their clicks are to each other clicks players obtain between and points for every click in the bonus round depending on how far the click is from their partner corresponding click the bonus round is timed players have to click on the same place as their partner as many times as they can in seconds if the object is not in the image players can pass because some images do not contain the object related to the word passing in the bonus round generates points for both players so we can learn whether the object is there players cannot pass after they have clicked on the image there are two reasons for the peekaboom bonus round first by giving players bite size milestones getting four images correctly we reinforce their incremental success in the game and thus encourage them to continue playing second the bonus round is an alternative approach to collecting training data for computer vision in it players click inside specific objects within an image such clicks give additional information for training computer vision algorithms in this paper we do not concern ourselves with such information but remark that it is also useful collecting image metadata our goal is to construct a database for training computer vision algorithms here we discuss exactly what information is collected by peekaboom and how it is collected on input an image word pair coming directly from the esp game peekaboom collects the following information how the word relates to the image is it an object person or animal in the image is it text in the image is it a verb describing an action in the image is it an object person or animal not in the image but related to it the esp game associates words to images but does not say how the word is related to the image figure for instance shows multiple ways in which a word can be related to an image hint buttons in peekaboom allow us to determine the relation of the word to the image this is useful in multiple ways but for the purposes of constructing training sets for computer vision it allows us to weed out related nouns and to treat text separately pixels necessary to guess the word when peek enters the correct word the area that boom has revealed is precisely enough to guess the word that is we can learn exactly what context is necessary to determine what the word refers to this context information is absolutely necessary when attempting to determine what type of object a set of pixels constitutes see figure the pixels inside the object animal or person if the word is a noun directly referring to something in the image pings give us pixels that are inside the object person or animal figure the image on the left contains a car driving through the street while the one on the right has a person crossing the same street both the car and the person are exactly the same set of pixels up to a rotation by degrees example taken from the most salient aspects of the objects in the image by inspecting the sequence of boom clicks we gain information about what parts of the image are salient with respect to the word boom typically reveals the most salient parts of the image first e g face of a dog instead of the legs etc elimination of poor image word pairs if many independent pairs of players agree to pass on an image without taking action on it then likely they found it impossibly hard because of poor picture quality or a dubious relation between the image and its label by implementing an eviction policy for images that we discover are bad we can improve the quality of the data collected as well as the fun level of the game when multiple players have gone through the same image these pieces of information can be combined intelligently to give extremely accurate and useful annotations for computer vision later in the paper for example we show how a simple algorithm can use the data produced by peekaboom to calculate accurate object bounding boxes see figure the single player game peekaboom is a two player game oftentimes however there will be an odd number of people attempting to play the game so the remaining person cannot be paired to prevent their frustration we also have a single player version of the game in which the player is matched with a server side bot our bot acts intelligently to simulate a human player by being based on pre recorded games in other words we take data collected from pairs of humans and use it as the basis for the computer player logic emulating a boom player is fairly simple the bot can regurgitate the sequence of recorded clicks to the human emulating peek is much more complicated the bot needs to have some concept of closeness of the human clicks to the set of recorded clicks for instance if the human does not reveal the dog in the picture the bot should not guess dog our bot only reveals a certain pre recorded guess if enough area has been revealed towards this end it employs a spatial data structure whose members are circles each of which corresponds to a click elements of the data structure are removed as they are clicked on by the human player when the data structure becomes empty the bot gives the correct answer moreover it has the ability to make incorrect guesses along the way based on the relative emptiness of the spatial data structure cheating peekaboom is a collaborative game partners work together to maximize their score when both partners do not communicate outside the game environment we obtain correct information however if the two partners collude to cheat on the game the data could be poisoned for instance if boom and peek know each other and have an outside means of communication then boom can simply tell peek what words to type peekaboom contains multiple anti cheating mechanisms through a combination of online in game enforcement and offline analysis we are able to detect and deal with cheating before detailing peekaboom anti cheating measures we mention that cheating attempts are uncommon although a minority of players might obtain satisfaction from gaming the system the majority of them just want to play the game honestly indeed as anecdotal evidence when peekaboom was tested in a room with children of ages they would cover the word with their hand to prevent others in the room from seeing the answers nevertheless peekaboom does have a full set of measures to prevent collusion the player queue when players log on to the game server they are not immediately paired off instead the server makes them wait n seconds where n is the number of seconds until the next matching interval currently matching intervals happen every seconds and when they do the server matches everyone in the queue with a partner any odd person out will be paired with a bot with a large number of players in the system we can ensure that a player partner is random and prevent colluders from getting matched just because they clicked start playing at the same time ip address checks we also check player ip addresses to ensure that they are not paired with themselves or with others that have a similar address similarity in ip addresses can imply geographical proximity seed images because our system is a web based game one point of concern is that bots i e automated players might play the game and pollute the pool of collected data to detect them we introduce seed images into the system in other words those for which we have hand verified metadata on being presented seed images if a player consistently fails to click on the relevant parts when playing boom or to guess the correct words when playing peek they will be added to a blacklist we discard all current and future game play data associated with anyone on the blacklist notice that almost by definition a computer program cannot successfully play peekaboom if it were able to do so then it would be able to recognize the objects in the images therefore this strategy prevents bots as well as otherwise malicious players from poisoning our data limited freedom to enter guesses since boom can see all of peek guesses the game allows a limited form of communication between the players indeed many of the peekaboom players use the guess field as a way to communicate with their partner it is not uncommon for the first guess in a game to be hi or for the first guess after passing on an image to be the correct word associated with the previous image it is also not uncommon for players to type sorry after taking too long on an image a possible cheating strategy is to exchange im screen names through the guess field and then using im communicate the correct words although we have never observed attempts to execute such a strategy we can mitigate it by not allowing peek to enter any non alphabetical characters such as numbers similarly we can prevent boom from seeing any guesses that are not words in the dictionary currently we do allow boom to see such guesses because we have not seen players attempt to cheat in this way however even if players are successful in such a strategy the other anti collusion mechanisms can deal with the corrupted data aggregating data from multiple players in addition to the above strategies we aggregate data from multiple players for a given image word pair by doing this we can eliminate outliers implementation we implemented the architecture of the game under the client server model the client application is delivered as a java applet while the server is written purely in java applets connect to a server which then matches the players with games of peekaboom upon two players completion of a match the server writes their game play data and scores to disk we then compile the collected data into desired formats our implementation of the game contains many features to improve game play spelling check incorrectly spelled words are displayed in a different color to notify players this is important because the peek player usually types multiple guesses in a short time often making spelling mistakes inappropriate word replacement since boom can see peek guesses we do not allow peek to enter inappropriate words whenever one of peek guesses is among a list of possible inappropriate words we substitute it with another word chosen from a list of innocent words such as love caring iluvpeekaboom etc top scores list and ranks the peekaboom website prominently displays the cumulative top scores of the day as well as the top scores of all time furthermore players are given a rank based on the total number of points they have accumulated throughout time see figure the different ranks are fresh meat points newbie points player points gangster points and godfather or more points we remark that ranks have proven an important component of peekaboom incentive strategy of the players that have obtained an account of them have scores that fall within points of the rank cutoffs given that these intervals cover less than of the space of possible cumulative scores this strongly suggests that many players simply play to reach a new rank figure top scores and player ranks players are shown their current rank and the number of points remaining for the next rank additional applications before going to the evaluation section we mention two additional applications for the data collected by peekaboom a benefit of these applications is that they are direct in that they do not require the training of machine learning algorithms improving image search results peekaboom gives an accurate estimate of the fraction of the image related to the word in question this estimate can be calculated from the area revealed by boom the fraction of the image related to a word can be used to order image search results images in which the word refers to a higher fraction of the total pixels should be ranked higher much like the goal of the esp game is to label all images on the web we can imagine peekaboom doing the same and thus further improving image search object bounding boxes in the same vein peekaboom can be used to directly calculate object bounding boxes similar to those used in flickr see figure flickr is a photo sharing service that allows users to tag images with keywords and to associate keywords with rectangular areas in the image the areas and tags however are not guaranteed to be correct since a user can enter anything they wish for their own images to exhibit the power of the data collected by peekaboom we show how to use it calculate such rectangles we emphasize however that the data collected by peekaboom is significantly richer and that to calculate the rectangles we discard vast amounts of the information collected by our game since peekaboom annotates arbitrary images on the web its data allows for an image search engine in which the results are highlighted similar to the highlighted words in google text search results using the data obtained in the first two weeks of game play we have implemented a prototype of such a search engine see figure the search engine can be accessed from the peekaboom website the bounding boxes were calculated as follows for a single play of an image word pair we create a matrix of and the dimensions of the matrix are the same as the dimensions of the image in pixels at first every entry in the matrix is a we add a in every pixel clicked by boom as well as in the circle of radius pixels around the click we thus obtain a matrix of and corresponding to the exact area that was revealed in a single game play next we combine different plays of the same image word pair by adding their corresponding matrices this gives a matrix whose entries are integers corresponding to the number of different players that revealed each pixel of the image on this combined matrix we apply a threshold of meaning that we substitute every value less than with and every value greater than with this gives a matrix corresponding to all the pixels that have been revealed by at least players next we cluster these pixels and calculate figure object bounding boxes obtained from peekaboom data the bounding boxes by taking for each cluster the leftmost rightmost topmost and bottommost points this algorithm may produce multiple bounding boxes for a single image word pair for instance in figure we can see that many of the results for eyes have two bounding boxes one corresponding to each eye as we will see the results produced by this simplistic algorithm are extremely accurate such results could be improved by making intelligent use of the additional data given by peekaboom such as pings the precise order of the areas revealed etc but for the purposes of this paper we use the simplistic algorithm alternative using ping data for pointing instead of showing bounding boxes calculated from revealed areas we could show arrows or lines pointing to the objects see figure such pointers can be easily calculated from the ping data the simplest algorithm for doing so is to select a ping at random and assume it is a good pointer for the object we will show that this simplistic algorithm gives very accurate results figure shows an image in which the different objects have been located using ping data more elaborate algorithms could give even better results we remark however that simply averaging the pings over multiple players to obtain a single pointer does not give accurate results for instance if the object was eyes averaging the pings gives a pointer to a region that is not an eye figure calculation of object pointers using pings evaluation user statistics the evaluation of our claims consists of two parts first we must show that the game is indeed enjoyable second we must show that the data produced by the game is accurate it is difficult to evaluate how enjoyable a game really is one approach is to ask participants a series of questions regarding how much they enjoyed playing the game our data for such an approach were extremely positive but we follow a different approach in this paper we present usage statistics from arbitrary people playing our game online this same approach was used by the esp game usage statistics peekaboom was released to a general audience on august of we present the usage statistics from the period starting august and ending september a total of different people played the game during this time generating pieces of data by different people we mean different user ids by a piece of data we mean a successful round of peekaboom in which peek correctly guessed the word given boom revealed region we mention that an image word pair can have multiple pieces of data associated to it if it occurs in multiple games if people gave us pieces of data then on average each person played on images since each session of the game lasts minutes and on average players go through images during a session in this one month period each person played on average minutes without counting time spent waiting for a partner etc over of the people played on more than one occasion that is more than of the people played on different dates furthermore every player in the top scores list played over games that over hours without including the time they spent waiting for a partner this undoubtedly attests to how enjoyable the game is user comments to give a further sense for how much the players enjoyed the game we include below some quotes taken from comments submitted by players using a link on the website the game itself is extremely addictive as there is an element of pressure involved in beating the clock a drive to score more points the feeling that you could always do better next time and a curiosity about what is going to come up next i would say that it gives the same gut feeling as combining gambling with charades while riding on a roller coaster the good points are that you increase and stimulate your intelligence you don t lose all your money and you don t fall off the ride the bad point is that you look at your watch and eight hours have just disappeared one unfortunate side effect of playing so much in such a short time was a mild case of carpal tunnel syndrome in my right hand and forearm but that dissipated quickly this game is like crack i ve been peekaboom free for hours unlike other games peekaboom is cooperative rather than competitive evaluation accuracy of collected data the usefulness of peekaboom as a data collection method rests in the quality of the data we collect although the design of the game inherently ensures correctness of the data we wanted to test whether it is as good as what would be collected directly from volunteers in a non game setting to do so we conducted two experiments to test first the accuracy of the bounding boxes we defined and second the utility of the pointing behavior in the game notice that these experiments are meant to analyze the correctness of the data and not whether such data can be used to train computer vision algorithms the usefulness of data about location of objects for training computer vision algorithms has been previously established experiment accuracy of bounding boxes in the first experiment we tested whether the bounding boxes for objects within an image that are calculated from peekaboom are as good as bounding boxes people would make around an object in a non game setting we selected at random image word pairs from the data pool that had been successfully played on by at least two independent pairs of people the images selected all had nouns as their word as opposed to text in the image or an adjective etc see figure all the images chosen had the word refer to a single object in the image for each image peekaboom data was used to calculate object bounding boxes using the method explained in previous sections we then had four volunteers make bounding boxes around the objects for each image providing us with bounding boxes drawn by volunteers the volunteers were asked for each image to draw a bounding box around the object that the word referred to we then selected at random one of the four volunteer bounding boxes for each image so as to end up with one volunteer generated bounding box for every one of the images finally we tested the amount of overlap between the bounding boxes generated by peekaboom and those generated by our volunteers the amount of overlap was determined using the formula overlap a b area a b area a b where a and b are the bounding boxes notice that if a b then overlap a b and if a is disjoint from b then overlap a b we calculated the average overlap across the images as well as the standard deviation results on average the overlap between the peekaboom bounding boxes and the volunteer generated ones was with standard deviation this means that the peekaboom bounding boxes were very close to those generated by the volunteers to illustrate we show in figure the bounding box that obtained the lowest overlap score figure experiment image with lowest overlap between a volunteer generated bounding box solid lines and one generated by peekaboom dashed lines given that peekaboom was not directly built to calculate bounding boxes this shows the wide applicability of the data collected experiment accuracy of pings in the second experiment we tested whether the object pointers that are calculated from peekaboom are indeed inside the objects as in the previous experiment we selected at random image label pairs from the data pool that have been successfully played on by at least two independent pairs of people the images selected all had the word as a noun as opposed to as text in the image or an adjective etc see figure all the images chosen had the word refer to a single object in the image for each image peekaboom data was used to calculate object pointers using the method explained in previous sections we then asked three volunteer raters to determine for each pointer whether it was inside the object or not the raters were shown examples of pointers inside and outside the object and were told that near an object does not count as inside the object results according to all the raters of the pointers were inside the object referred to by the word this gives positive evidence that ping data is accurate especially since it was calculated using such a simplistic algorithm generalizing our approach the approach presented in this paper solving a problem by having people play games online can be generalized to many other problems in artificial intelligence in follow up work for example we have created two other games verbosity and phetch in which players solve problems that computers cannot yet solve verbosity collects common sense facts to train reasoning algorithms for instance for the word milk the game outputs facts such as it is white people usually eat cereal with it etc verbosity is a two player game in which one player attempts to make the other say a target word e g milk without using the word they do so by saying many facts without using the word itself in their statements e g it is a white liquid the underlying game mechanism of verbosity is similar in nature to that of peekaboom much like designing an algorithm to solve a problem designing a game to harness valuable human cycles is to a large extent an art problems usually require a specifically tailored game in addition to an original idea creating such a game also depends on a broader set of criteria including looks the fluidity of the game graphics ease of use an intuitive user interface cognitive load the amount of user attention required to play the game and action the extent to which the game absorbs the user in the experience all of these aspects have been treated in this paper and we believe many of the techniques here presented generalize to creating other games with a purpose finally we believe that these design principles like the scientific method don t just provide ideas but a way of thinking games provide a valuable vehicle to solve problems that computers cannot yet solve ethical considerations as with all systems soliciting input from humans we must address the ethical issues behind the usage of the collected data towards this end we inform the players of the game purpose on the peekaboom website players participate willingly and knowingly indeed many people play because they like the fact that the game has a purpose furthermore we state on the record that the game purpose is to obtain accurate segmentations of objects from backgrounds and to train computer vision algorithms to recognize simple objects we have no intention of applying our data towards for example military surveillance related work we have presented a method for annotating arbitrary images and we have presented evidence that it produces high quality data we now survey the related work the esp game as mentioned before the esp game is two player game that collects word labels for arbitrary images peekaboom is similar to the esp game and in fact was inspired by it we consider peekaboom an extension of esp whereas esp gives data to determine which objects are in the image peekaboom can augment this data with information about where in the image objects are located in terms of game mechanics peekaboom is different from the esp game in several ways first peekaboom is asymmetric whereas both players in the esp game are performing the same role players of peekaboom alternate in performing different roles second peekaboom allows a significantly higher level of interaction among the players whereas in the esp game players cannot communicate at all in peekaboom one of the players can freely communicate with the other third the usage of hint buttons has proven very successful in peekaboom and such buttons could as well be incorporated into esp such differences in game mechanics reflect the difference in purpose of peekaboom and esp the open mind initiative perhaps less so peekaboom is also similar at least in spirit to the open mind initiative e g a worldwide effort to develop intelligent software open mind collects data from regular internet users referred to as netizens and feeds it to machine learning algorithms volunteers participate by answering questions and teaching concepts to computer programs peekaboom is similar to open mind in that we use regular people on the internet to annotate images however as with the esp game we put much greater emphasis on our method being fun we don t expect volunteers to annotate millions of images on the web we expect images to be annotated because people want to play our game whereas a typical open mind activity would ask participants to point to the object in question we transform the activity into a two player game in which players are not even asked to point to the object they do so only as a side effect of playing the game labelme labelme is a web based tool for image annotation anybody can annotate data using this tool and thus contribute to constructing a large database of annotated objects the incentive to annotate data is the data itself you can only have access to the database once you have annotated a certain number of images the main difference between peekaboom and labelme is the game aspect whereas labelme simply asks users to annotate an image peekaboom transforms the process into an enjoyable game labelme relies on people desire to help and thus assumes that the entered data is correct on the other hand peekaboom has multiple mechanisms to prevent players from polluting the data interactive machine learning another area of related work is that of interactively training machine learning algorithms e g in these systems a user is given immediate feedback about how well an algorithm is learning from the examples provided by them as with labelme peekaboom differs from these systems in the gaming aspect as well as in the assumption that our users are interested in training an algorithm conclusions and future work peekaboom is a novel complete game architecture for collecting image metadata segmenting objects in images is a unique challenge and we have tailored a game specifically to this end in the very near future we would like to make our pieces of data available to the world by formatting it as an image segmentation library like the esp game peekaboom encompasses much more than just a java applet delivered from a website rather the ideas behind the design and implementation of the game generalize to a way of harnessing and directing the power of the most intricate computing device in the world the human mind some day computers will be able to segment objects in images unassisted but that day is not today today we have engines like peekaboom that use the wisdom of humans to help naïve computers get to that point the actual process of making computers smarter given segmentation metadata is beyond the scope of this paper since it would require a far more sophisticated interpretation of the data than the simple bounding box derivation we have presented thus we see great potential in future work at the crossroads of human computer interaction and artificial intelligence where the output of our interactive system helps advance the state of the art in computer vision articulated pose estimation with flexible mixtures of parts yi yang deva ramanan dept of computer science university of california irvine dramanan ics uci edu abstract we describe a method for human pose estimation in static images based on a novel representation of part mod els notably we do not use articulated limb parts but rather capture orientation with a mixture of templates for each part we describe a general flexible mixture model for capturing contextual co occurrence relations between parts augmenting standard spring models that encode spa tial relations we show that such relations can capture no tions of local rigidity when co occurrence and spatial rela tions are tree structured our model can be efficiently opti mized with dynamic programming we present experimental results on standard benchmarks for pose estimation that in dicate our approach is the state of the art system for pose estimation outperforming past work by while being orders of magnitude faster introduction we examine the task of human pose estimation in static images a working technology would immediately impact many key vision tasks such as image understanding and activity recognition an influential approach is the picto rial structure framework which decomposes the ap pearance of objects into local part templates together with figure on the left we show the classic articulated limb model of marr and nishihara in the middle we show different orientation and foreshortening states of a limb each of which is evaluated separately in classic articulated body models on the right we approximate these trans formations with a mixture of non oriented pictorial struc tures in this case tuned to represent near vertical and near horizontal limbs geometric constraints on pairs of parts often visualized as springs when parts are parameterized by pixel location and orientation the resulting structure can model articulation this has been the dominant approach to human pose estima tion in contrast traditional models for object recognition use parts parameterized solely by location which simplifies both inference and learning such models have been shown to be very successful for object recognition in this work we introduce a novel unified representation for both models that produces state of the art results for human pose estimation representations for articulated pose full body pose estimation is difficult because of the many degrees of free doms to be estimated moreover limbs vary greatly in ap pearance due to changes in clothing and body shape as well as changes in viewpoint manifested in in plane orientations and foreshortening these difficulties complicate inference since one must typically search images with a large num ber of rotated and foreshortened templates we address these problems by introducing a novel but simple represen tation for modeling a family of affinely warped templates a mixture of non oriented pictorial structures fig we empirically demonstrate that such approximations can out perform explicitly articulated parts because mixture mod els can capture orientation specific statistics of background features fig representations for objects current object recogni tion systems are built on relatively simple structures encod ing mixtures of star models defined over tens of parts or implicitly defined shape models built on hundreds of parts in order to model the varied appearance of objects due to deformation viewpoint etc we argue that one will need vocabularies of hundreds or thousands of parts where only a subset are instanced at a time we augment clas sic spring models with co occurrence constraints that favor particular combinations of parts such constraints can cap ture notions of local rigidity for example two parts on the same limb should be constrained to have the same orienta tion state fig we show that one can embed such con straints in a tree relational graph that preserves tractability an open challenge is that of learning such complex repre horizontal more susceptible to overfitting to statistics of a particular dataset as warned by 05 diagonal vertical an alternate family of techniques has explored the trade off between generative and discriminative models trained explicitly for pose estimation approaches include condi tional random fields and margin based or boosted de tectors a final crucial issue is that of feature figure we plot the average hog feature as a polar his togram over gradient orientation channels as computed from the entire pascal dataset we see that on average images contain more horizontal gradients than ver tical gradients and much stronger horizontal gradients as compared to diagonal gradients this means that gradient statistics are not orientation invariant in practical terms we argue that it is easier to find diagonal limbs as opposed to horizontal ones because one is less likely to be confused by diagonal background clutter articulated limb models obtained by rotating a single template cannot exploit such orientation specific cues on the other hand our mixture models are tuned to detect parts at particular orientations and so can exploit such statistics sentations from data as in we conclude that super vision is a key ingredient for learning structured relational models we demonstrate results on the difficult task of pose esti mation we use two standard benchmark datasets we outperform all published past work on both datasets re ducing error by up to we do so with a novel but sim ple representation that is orders of magnitude faster than descriptors past work has explored the use of superpixels contours foreground background color models and gradient descriptors in terms of object detection our work is most similar to pictorial structure models that reason about mixtures of parts we show that our model generalizes such representations in sec our model when instanced as a tree can be written as a recursive grammar of parts model let us write i for an image pi x y for the pixel location of part i and ti for the mixture component of part i we write i k pi l and ti t we call ti the type of part i our moti vating example of types include orientations of a part e g a vertical versus horizontally oriented hand but types may span semantic classes an open versus closed hand for no tational convenience we define the lack of subscript to indi cate a set spanned by that subscript e g t tk co occurrence model to score of a configuration of parts we first define a compatibility function for part types that factors into a sum of local and pairwise scores previous approaches our model requires roughly sec ond to process a typical benchmark image allowing for the possibility of real time performance with further speedups ti ti tj i ij i v ij e such as cascaded or parallelized implementations the parameter bti favors particular type assignments for related work pose estimation has typically been addressed in the video domain dating back to classic model based approaches of o rourke and badler hogg rohr recent work has examined the problem for static images assum ing that such techniques will be needed to initialize video based articulated trackers probabilistic formulations are common one area of research is the encoding of spatial structure tree models are efficient and allow for efficient inference but are plagued by the well known phenom ena of double counting loopy models require approximate part i while the pairwise parameter bti tj favors particular co occurrences of part types for example if part types cor respond to orientations and part i and j are on the same rigid limb then bti tj would favor consistent orientation assign ments we write g v e for a k node relational graph whose edges specify which pairs of parts are constrained to have consistent relations we can now write the full score associated with a con figuration of part types and positions s i p t s t wti φ i pi wti tj ψ pi pj inference strategies such as importance sampling loopy belief propagation or iterative approximations i i v ij ij e recent work has suggested that branch and bound al gorithms with tree based lower bounds can globally solve such problems another approach to tackling the double counting phenomena is the use of stronger pose pri ors advocated by however such approaches maybe where φ i pi is a feature vector e g hog descriptor extracted from pixel location pi in image i we write ψ pi j dy yi yj the relative location of part i with respect to j notably this relative location is defined with respect to the pixel grid and not the orientation of part i as in classic articulated pictorial structures appearance model the first sum in is an appear ance model that computes the local score of placing a tem plate wti for part i tuned for type ti at location pi deformation model the second term can be inter a tree this can be done efficiently with dynamic program ming let kids i be the set of children of part i in g we compute the message part i passes to its parent j by the fol lowing scorei ti pi bti wi φ i pi mk ti pi preted as a switching spring model that controls the rela tive placement of part i and j by switching between a col lection of springs each spring is tailored for a particular i ti k kids i pair of types ti tj and is parameterized by its rest loca tion and rigidity which are encoded by wti tj mi tj pj max bti tj ti max score t p wti tj ψ p p special cases we now describe various special cases of our model which have appeared in the literature one obvious case is t in which case our model reduces to a standard pictorial structure more interesting cases are below semantic part models argue that part appearances should capture semantic classes and not visual classes this can be done with a type model consider a face model with eye and mouth parts one may want to model different types of eyes open and closed and mouths smiling and frowning the spatial relationship between the two does not likely depend on their type but open eyes may tend to co occur with smiling mouths this can be obtained as a special case of our model by using a single spring for all types of a particular pair of parts wti tj wij mixtures of deformable parts define a mixture of models where each model is a star based pictorial structure this can achieved by restricting the co occurrence model to allow for only globally consistent types i i ij i j i computes the local score of part i at all pixel locations pi and for all possible types ti by collecting messages from the children of i computes for every location and pos sible type of part j the best scoring location and type of its child part i once messages are passed to the root part i represents the best scoring config uration for each root position and type one can use these root scores to generate multiple detections in image i by thresholding them and applying non maximum suppression nms by keeping track of the argmax indices one can backtrack to find the location and type of each part in each maximal configuration computation the computationally taxing portion of dynamic programming is one has to loop over l t possible parent locations and types and compute a max over l t possible child locations and types making the computation o for each part when ψ pi pj is a quadratic function as is the case for us the inner max imization in can be efficiently computed for each com bination of ti and tj in o l with a max convolution or distance transform since one has to perform t dis ti tj ij if ti tj otherwise tance transforms message passing reduces to o lt per part articulation in our experiments we explore a simpli fied version of with a reduced set of springs special cases model maintains only a single spring per part so message passing reduces to o l models and maintain only t springs per part reducing mes wti tj wti sage passing to o lt it is worthwhile to note that our ij ij the above simplification states that the relative location of part with respect to its parent is dependant on part type but not parent type for example let i be a hand part j its par ent elbow part and assume part types capture orientation the above relational model states that a sideways oriented hand should tend to lie next to the elbow while a downward oriented hand should lie below the elbow regardless of the orientation of the upper arm inference inference corresponds to maximizing s x p t from over p and t when the relational graph g v e is articulated model is no more computationally complex than the deformable mixtures of parts in but is considerably more flexible as we show in our experiments in practice t is small in our experiments and the distance trans form is quite efficient so the computation time is dominated by computing the local scores of each type specific appear ance models wti φ i pi since this score is linear it can be efficiently computed for all positions pi by optimized convolution routines learning we assume a supervised learning paradigm given la beled positive examples in pn tn and negative examples in we will define a structured prediction objective func tion similar to those proposed in to do so let us write zn pn tn and note that the scoring function is linear in model parameters β w b and so can be writ ten as s i z β φ i z we would learn a model of the form deriving part type from position assume that our nth training image in has labeled joint positions pn let pn be the relative position of part i with respect to its parent in image in for each part i we cluster its relative position over the training set pn n to obtain t clusters we use k means with k t each cluster corresponds to a col arg min β β c ξ lection of part instances with consistent relative locations w ξi n t n pos β φ in zn ξn n neg z β φ in z ξn the above constraint states that positive examples should score better than the margin while negative examples for all configurations of part positions and types should score less than the objective function penalizes vio lations of these constraints using slack variables ξn detection vs pose estimation traditional structured prediction tasks do not require an explicit negative training set and instead generate negative constraints from positive examples with mis estimated labels z this corresponds to training a model that tends to score a ground truth pose highly and alternate poses poorly while this translates directly to a pose estimation task our above formulation also includes a detection component it trains a model that scores highly on ground truth poses but generates low scores on images without people we find the above to work well for both pose estimation and person detection optimization the above optimization is a quadratic program qp with an exponential number of constraints since the space of z is lt k fortunately only a small mi nority of the constraints will be active on typical problems e g the support vectors making them solvable in prac tice this form of learning problem is known as a structural svm and there exists many well tuned solvers such as the cutting plane solver of svmstruct and the stochastic gradient descent solver in we found good results by im plementing our own dual coordinate descent solver which we will describe in an upcoming tech report learning in practice most human pose datasets include images with labeled joint positions we define parts to be located at joints so these provide part position labels p but not part type labels t we now describe a procedure for generating type labels for our articulated model we first manually define the edge structure e by con necting joint positions based on average proximity because we wish to model articulation we can assume that part types should correspond to different relative locations of a part with respect to its parent in e for example sideways oriented hands occur next to elbows while downward facing hands occur below elbows this means we can use relative location as a supervisory cue to help derive type la bels that capture orientation and hence consistent orientations by our arguments above we define the type labels for parts tn based on cluster mem bership we show example results in fig partial supervision because part type is derived heuristically above one could treat tn as a latent variable that is also optimized during learning this latent svm problem can be solved by coordinate descent or the ccp algorithm we performed some initial experi ments with latent updating of part types using the coordi nate descent framework of but we found that type labels tend not to change over iterations we leave such partially supervised learning as interesting future work problem size on our training datasets the number of positive examples varies from and the number of negative images is roughly we treat each possible placement of the root on a negative image as a unique nega tive example xn meaning we have millions of negative con straints furthermore we consider models with hundreds of thousands of parameters we found that a careful optimized solver was necessary to manage learning at this scale experimental results datasets we evaluate results using the image parse dataset and the buffy dataset the parse set con tains pose annotated images of highly articulated full body images of human poses the buffy dataset contains pose annotated video frames over episodes of a tv show both datasets include a standard train test split and a standardized evaluation protocol based on the probability of a correct pose pcp which measures the percentage of correctly localized body parts notably buffy is also dis tributed with a set of validated detection windows returned by an upper body person detector run on the testset most previous work report results on this set as do we since our model also serves as a person detector we can also present pcp results on the full buffy testset to train our models we use the negative training images from the inriaperson database as our negative training set these images tend to be outdoor scenes that do not contain people models we define a full body skeleton for the parse set and a upper body skeleton for the buffy set to define a fully labeled dataset of part locations and types we group parts into orientations based on their relative location with respect to their parents as described in sec we show clustering results in fig we use the derived type labels to construct a fully supervised dataset from which we learn flexible mixtures of parts we show the full body model learned on the parse dataset in fig we set all parts to be hog cells in size to visualize the model we show trees generated by selecting one of the four types of each part and placing it at its maximum scoring position recall that each part type has its own appearance template and spring encoding its relative location with respect to its parent this is because we expect part types to correspond to orientation because of the supervised labeling shown in fig though we visualize trees we emphasize that there exists an exponential number of trees that our model can generate by composing different part types together structure we consider the effect of varying t the number of mixtures or types and k number of parts on the accuracy of pose estimation on the parse dataset in fig we experiment with a part model defined at joint positions shoulder elbow hand etc and a part model where midway points between limbs are added mid upper arm mid lower arm etc to increase coverage per formance increases with denser coverage and an increased number of part types presumably because additional orien tations are being captured for reference we also trained a star model but saw inferior performance compared to the tree models shown in fig we saw a slight improvement using a variable number of mixtures or per part tuned by cross validation these are the results presented below detection accuracy we use our model as an upper body detector on the buffy dataset in table we correctly detect of the people in the testset the dataset in clude two alternate detectors based on a rigid hog tem plate and a mixtures of star models which perform at and respectively the latter is widely regarded as a state of the art system for object recognition these results indicate the potential of our representation and su pervised learning framework for general object detection parse we give quantitative results for pcp in table and show example images in fig we refer the reader to the captions for a detailed analysis but our method outper forms all previously published results by a significant mar gin notably all previous work uses articulated parts we reduce error by we believe our high performance is to due to the fact that our models leverage orientation specific statistics fig and because parts and relations are simul taneously learned in a discriminative framework in con trast articulated models are often learned in stages using pre trained orientation invariant part detectors due to the computational burden of inference buffy we give quantitative results for pcp in table and show example images in fig we refer the reader to the captions for a detailed analysis but we outperform all past approaches when evaluated on a subset of standard ized windows or the entire testset notably all previous approaches use articulated parts our algorithm is several upper body detection on buffy table our model clearly outperforms past approaches for upper body detection notably use a star structured model of hog templates trained with weakly supervised data our results suggest more complex object structure when learned with supervision can yield improved results for detection performance vs number of types per part 64 figure we show the effect of model structure on pose estimation by evaluating pcp performance on the parse dataset overall increasing the number of parts by instanc ing parts at limb midpoints in addition to joints improves performance for both cases increasing the number of mix ture components improves performance likely due to the fact that more orientations can be modeled orders of magnitude faster than the next best approaches of when evaluated on the entire testset our approach reduces error by conclusion we have described a simple but flexible extension of tree based models of part mixtures when part mixture models correspond to part orientations our repre sentation can model articulation with greater speed and ac curacy than classic approaches our representation provides a general framework for modeling co occurrence relations between mixtures of parts as well as classic spatial relations between the location of parts we show that such relations capture notions of local rigidity we are applying this ap proach to the task of general object detection but have al ready demonstrated impressive results for the challenging task of human pose estimation acknowledgements funding for this research was pro vided by nsf grant onr muri grant and support from google and intel neck wrt head left knee wrt hip left foot wrt knee left elbow wrt shoulder left hand wrt elbow x x x x 0 x figure we take a data driven approach to orientation modeling by clustering the relative locations of parts with respect to their parents these clusters are used to generate mixture labels for parts during training for example heads tend to be upright and so the associated mixture models focus on upright orientations because hands articulate to a large degree mixture models for the hand are spread apart to capture a larger variety of relative orientations figure a visualization of our model for t trained on the parse dataset we show the local templates above and the tree structure below placing parts at their best scoring location relative to their parent though we visualize trees there exists an exponential number of realizable combinations obtained composing different part types together the score associated with each combination decomposes into a tree and so is efficient to search over image parse testset table we compare our model to all previous published results on the parse dataset using the standard criteria of pcp our total performance of compares favorably to the best previous result of we also outperform all previous results on a per part basis subset of buffy testset full buffy testset table the buffy testset is distributed with a subset of windows detected by a rigid hog upper body detector we compare our results to all published work on this set on the left we obtain the best overall pcp while being orders of magnitude faster than the next best approaches our total pipeline requires second to process an image while take minutes we outperform or nearly tie all previous results on a per part basis as pointed out by this subset contains little pose variation because it is biased to be responses of a rigid template the distributed evaluation protocol also allows one to compute performance on the full test videos by multiplying pcp values with the overall detection rate we do this for published results on the right table because our model also serves as a very accurate detector table we obtain significantly better results than past work when evaluated on the full testset figure results on the parse dataset we show part bounding boxes reported by our algorithm for each image the top rows show successful examples while the bottom row shows failure cases examining failure cases from left to right we find our model is not flexible enough to model horizontal people is confused by overlapping people suffers from double counting phenomena common to tree models both the left and right legs fire on the same image region and is confused when objects partially occlude people figure results on the buffy dataset we show part bounding boxes corresponding to upper body parts reported by our algorithm the left columns show successful examples while the right column show failure cases from top to bottom we see that our model still has difficultly with raised arms and is confused by vertical limb like clutter in the background world scale mining of objects and events from community photo collections till kooaba ag zurich switzerland bastian eth zurich biwi zurich switzerland luc van k u leuven ibbt leuven belgium abstract in this paper we describe an approach for mining images of objects such as touristic sights from community photo col lections in an unsupervised fashion our approach relies on retrieving geotagged photos from those web sites using a grid of geospatial tiles the downloaded photos are clustered into potentially interesting entities through a processing pipeline of several modalities including visual textual and spatial proximity the resulting clusters are analyzed and are au tomatically classified into objects and events using mining techniques we then find text labels for these clusters which are used to again assign each cluster to a corresponding wi kipedia article in a fully unsupervised manner a final ver ification step uses the contents including images from the selected wikipedia article to verify the cluster article assign ment we demonstrate this approach on several urban areas densely covering an area of over square kilometers and mining over photos making it probably the largest experiment of its kind to date categories and subject descriptors h information storage and retrieval content analysis and indexing general terms algorithms design experimentation theory introduction several recent developments have strongly influenced the state of the art in retrieval from visual databases first more powerful local visual features have led to significant progress in recognition capabilities both for specific objects and for object classes second while the state of the art in object class recognition scales to a few thousand images scaleable indexing methods for retrieval of specific objects have recently allowed scaling up to million images third with the ubiquitous availability of the internet and the widespread use of digital permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee civr july niagara falls ontario canada copyright acm cameras large databases of visual data have been created most notably community photo collections such as flickr http www flickr com these collections contain vast amounts of high quality images often labeled with keywords or tags an increasing number of those photos is also an notated with the geographic location the picture was taken at annotating photos with their geographic position of ten called geotagging is either done automatically with a gps device or by manually placing the photo on a map however these textual and geographic annotations are still of far lower quality than their counterparts in traditional databases such as stock photography or news archives in this work we deal with a crucial but often neglected building block towards internet scale image retrieval the automated collection of a high quality image database with correct annotations more precisely from the large amount of sparsely labeled content in community photo collections the task is to mine clusters of images containing objects in a fully unsupervised manner for each mined item we automatically derive a textual description the resulting cleaned image database for the mined objects and events is of far higher quality than the original data and facilitates a variety of applications for example the mined struc ture can be used for automated annotation of photos up loaded to community collections for retrieval and brows ing of landmark buildings automatic reconstruction of sights or for mobile phone tourist guide applica tions where users can point the integrated camera to a sight and retrieve information about it our approach is based on photographs which have been tagged with their geographic location flickr reports that over million such geotagged photos are currently uploaded each month this allows us to mine the world in a scalable manner without any prior knowledge on landmarks and their locations to that end we partition the world into a grid of square tiles and retrieve for each tile all the corresponding geotagged photos from flickr the geographic tiling thus allows us to handle the size of this vast problem and to parallelize computations in detail this paper makes the following contributions we demonstrate fully automatic world scale image min ing from community photo collections to our knowledge our approach is the first of its kind that can structure in terpret and annotate such amounts of visual data without user intervention we cluster the retrieved photos accord ing to several different modalities including visual content and text labels and clustering strategies we show how the intelligent combination of the resulting cluster assignments can capture and discriminate between distinct objects in side outside views of landmark buildings and panoramas and how it can represent the neighborhood relation between those sights for each cluster we additionally calculate a set of cues such as the number of days covered by its photos the number of users who took the photos etc we show how these additional features can be used to train a subsequent classifier which determines if an image cluster represents an object or an event we apply frequent itemset mining on the text associated with each cluster in order to assign cluster labels we propose an algorithm that employs the resulting frequent itemset labels to link clusters to wikipe dia pages providing additional information about the cluster content and that then in turn takes the wikipedia entries to verify clusters and filter out false assignments clos ing the loop we finally demonstrate how the verified clusters can be used to automatically label and geo locate additional photos for which no geotags were available the paper is structured as follows the next section dis cusses related work section then introduces our mining approach and describes how we cluster the mined photos section details how we classify clusters into objects and events and how we link them to wikipedia finally sec tion presents experimental results related work since the proposed method covers an entire multi modal processing pipeline it touches on a large variety of previ ous publications mining objects from visual data has been proposed for video for instance in those works also built on local features but focused on the spatial ar rangement of quantized features from video data working with data from community photo collections has received in creasing attention lately however most of those approaches are based either on text or only global vi sual features the local visual features which are used in this work however allow to find very good and extremely accurate matches between the depicted objects even under significant changes in viewpoint imaging conditions scale lighting clutter noise and partial occlusion a similar ap proach would not be possible using global measures such as color or texture histograms philbin and zisserman also worked with local features and multiple view geometry on a database of landmark buildings obtained from flickr the main goal of that work was to derive a scalable indexing method for local visual features the database was retrieved and annotated manually the work most similar to ours is probably here the authors also proposed cluster ing images from community photo collections using multi view geometry based matching between images the goal was to derive canonical views for certain landmarks and to use those as entry points for browsing initial image col lections were retrieved by querying photo collections with known keywords such as rome pantheon etc as we will demonstrate our fully unsupervised approach based on geographic tiling is not only more flexible but also more scalable the dataset used in contained photos while ours is one order of magnitude larger furthermore we add several layers of processing which extract semantic information such as classification into objects and events and which automatically include other content sources such as wikipedia for unsupervised labeling of objects to the best of our knowledge this work is the first to propose this figure tiles over paris the size of a tile is marked in red note the overlap of kind of pipeline taking as an input only a geographic tiling of the world and resulting in an output of automatically mined landmark objects together with their semantics in the form of automatically created links to wikipedia mining approach in summary our approach consists of the following steps gathering the geotagged data from the www clustering to group images of the same object event classification of clusters into objects or events frequent itemset mining to derive cluster labels unsupervised linking to wikipedia and verification of those links the following sections describe each of those steps in detail gathering the data to gather the raw data we query community photo col lections such as flickr first we divide the earth sur face into square tiles tk of about side length a tile center is set every in longitude and latitude direc tion such that the tiles have a high overlap for each tile we query the flickr api with the tile center coordinates and bounding box to obtain all geotagged photos for that area figure shows a section of a map with the tiles used for querying overlaid in total we processed about tiles for this work covering several european urban cen ters namely paris rome venice oxford zurich munich tallinn prague and st petersburg table lists the urban areas we covered and the number of tiles and photos re trieved for each area in total we covered an area of about square kilometers the majority of tiles about were empty the remaining tiles contained on average and a maximum of photos for each photo we download we also obtain the associ ated metadata namely the textual descriptions tags title description user id and timestamps photo clustering once the photos for each tile have been downloaded we process each cell to find clusters of photos with similar con tent as object candidates we first create dissimilarity ma trices for several modalities by calculating the pairwise dis tances between photos for each modality a hierarchical clustering step on the dissimilarity matrices then creates clusters of photos for the same object or event below we discuss the features and distances used for each modality table urban areas processed in this paper and the number of tiles and photos per area visual features and similarity to identify pairs of photos which contain the same object we employ matching based on local scale invariant features and projective geometry we first extract the visual features from each photo for this we employ surf features due to their fast extraction times and compact description shown in earlier works each image is thus represented as a bag of dimensional surf feature vectors for each pair of images in a tile tk we find matching features by calculating the nearest neighbor nn in euclidean distance between all feature pairs followed by a verification with the nearest neighbor criterion from note that this linear matching procedure is fast enough since the problem is separated into the geographic tiles using scaleable indexing methods such as could lower the processing times of the system even further to find object candidates from the matching features we next calculate homography mappings for each matched im in our implementation imax since we extract at most surf features per image sorted by their dis criminance i e the distance ranges in 01 text features and similarity three sources for text meta data were considered for each photo downloaded from flickr tags title and description we combine these three text fields into a single text per photo for further processing stages the first stage consists of a stoplist in addition to the common stopwords this list also contains collection specific stopwords such as years months and terms such as geotagged trip vacation honeymoon etc furthermore from each photo geotag we know its location and the corresponding place name for instance rome italy these location specific place names were added to the stoplist for each photo depending on its geotag filtering terms with these custom stoplists turned out to be crucial to obtain good cluster labels in later pro cessing stages as with the visual features we proceed by calculating the pairwise text similarities between the documents photos a vector space model with term weighting of the following form is applied wi j li j gi nj note that in the standard tf idf ranking li j tf i j gi log d and nj where tf i j is the frequency of term i in document j di is the number of documents containing term i and d is the total number of documents in our system the weighting elements are as follows log tf i j age pair i j i j li j pj log tf i j hxn xn n where h is the homography whose degrees of freedom g log d di i can be solved with four point correspondences n to uj be robust against the aforementioned outliers we estimate h using ransac the quality of several estimated mod nj uj els is measured by the number of inliers where an inlier i is defined by a threshold on the residual error the residual error for the model is determined by the distance of the true points from the points generated by the estimated h we accept hypotheses with at least inliers i as a match using this kind of homography mapping works well in our case since we have many photos taken from similar view points a fundamental matrix could handle larger viewpoint changes but it is also more costly to compute since it re quires more inliers to find the correct model furthermore mapping planar elements such as building facades works very well with homographies a similar approach has also been successfully applied in for a retrieval engine on a database of landmarks from oxford handling astonishing viewpoint and scale changes as mentioned above the accu racy achieved with these kinds of visual features is far better than with any kind of global features which are still often used for mining and retrieval in visual databases the distance matrix is built from the number of inlying feature matches iij for each image pair normalized by the maximum number of inliers found in the whole dataset iij where uj is the number of unique terms in document j the rationale behind the modifications of the weighting terms over the standard tf idf are as follows the logarithm in li j adjusts dampens weights of multiple occurring words per document gi is a probabilistic inverse document fre quency as proposed in which unlike idf assigns nega tive weights to terms that appear in more than half the doc uments finally the additional term nj is a pivoted unique normalization which is used to correct for discrepancies in document lengths we use the mysql www mysql com full text search which can be configured to use the modified tf idf ranking to compute the text disimilarity matrix for the photos belonging to each grid tile additional features besides the visual and text similarities between photos we also considered several additional cues we store the user data i e which flickr user took or uploaded a photo and the timestamps as we will show below these cues allow us to classify each cluster candidate into event or object types clustering dij imax if iij if iij for each tile tk we apply hierarchical agglomerative clus tering to the distance matrix of each modality this table cut off distances for clustering clustering approach was chosen since it builds on a dissimi larity matrix and is not restricted to metric spaces it is also rather flexible and very fast once the full distance matrix is available using different linking criteria for cluster merging allows us to create different kinds of clusters we employed the following well known linkage methods figure class examples object event none object is defined as any rigid physical item with a fixed position including landmark buildings statues etc as events we consider occasions that took place at a specific time and location for instance concerts parties etc thus single link dab min i a j b complete link dab max i a j b average link dab n n dij dij x dij we include as features the number of unique days the photos in a cluster were taken at obtained from the photos times tamps and the number of different users who contributed photos to the cluster divided by the cluster size d i j i a j b u where a and b are the clusters to merge and i and j index their ni and nj elements respectively the motivation behind these measures is to capture dif ferent kinds of visual properties that allow us to associate a semantic interpretation with the resulting clusters single link clustering adds images to a cluster as long as they yield a good match to at least one cluster member this results in elongated clusters that tend to span a certain area as a result if visual features are the basis for clustering it can group panoramas of images that have been taken from the same viewpoint or series of images around an object in contrast complete link clustering enforces that a new image matches to all cluster members this strategy will there fore result in very tight clusters that contain similar views of the same object or building average link clustering fi nally takes a compromise between those two extremes and provides clusters that still prefer views of the same object while allowing more flexibility in viewpoint shifts in our ap proach we do not want to restrict ourselves to any single of those alternatives instead we pursue them in parallel such an approach makes it possible to derive additional informa tion from a comparison of cluster outcomes for example we may first identify distinct objects or landmark buildings through complete or average link clusters and later find out which of them are located close to each other by their mem bership in the same single link cluster table summarizes the linkages and cutoff distances used for each modality labeling clusters in the preceding sections images with similar content or annotations were grouped into clusters which ideally should depict a single entity in this section the goal is to look into the contents of the clusters in more detail first we classify the clusters into objects landmarks etc and events in a next step we derive textual labels for the clusters from the associated metadata furthermore we introduce an ap proach to formulate text queries from the labels which are submitted to wikipedia to assign articles to the clusters a final verification step uses the images found in the wikipedia articles to verify this assignment classification into objects and events to discriminate between objects and events we rely on the collected metadata for the photos in each cluster an n where d is the number of days u the number of users and n the number of photos in the cluster typically objects such as landmarks are photographed by many people throughout the year an event on the other hand usually takes place only at one or two days and is covered by fewer users note that we only consider clusters with n here we manually labeled a ground truth of about clus ters with the class labels object event and none see figure for an example of each class we then trained an individual decision tree for the classes object and event on half of the labeled data and used the other half for validation the task in training and testing was to discrim inate the target class object or event against all other classes cross validated over random data partitions this simple classifier was able to achieve precision for objects and for events with a standard deviation of and respectively linking to wikipedia having the clusters classified into objects and events the next processing layer intends to add more descriptive labels the goal is to not only label the clusters with the most dom inant words but automatically link them to content on the internet such as corresponding wikipedia articles such a solution allows auto annotation of unlabeled images even down to outlining object parts using the information from other pictures of the same entity potential applications include mobile tourist guides where tourists use the inte grated camera of their mobile phones to take a picture of a landmark building a recognition service building upon our labeled database could then match the query to the corre sponding database entry and return the assigned wikipedia content to the user device such systems have been pro posed before e g but the automatic collection of the database from user generated content has not been ad dressed yet the proposed approach first finds relevant word combi nations from the text associated with each cluster using a frequent itemset mining algorithm the resulting frequent combinations are then used to query wikipedia in a second step an image based matching step finally verifies that the links are indeed correct frequent labels flickr and similar community photo collections provide us with text associated to photos however the text is often noisy and not all images are labeled furthermore if we want to use the text to find out more about the object by querying internet search engines we need to create queries from the raw tags any combination of words from the text could be the correct query however finding and trying all possible combinations would mean considering com binations of words where n can easily be in the hundreds we therefore resort to frequent itemset mining to find the most frequent combinations of words those can serve as labels for the objects and as query input for the next stage we quickly summarize the concepts of itemset mining originally frequent itemset mining algorithms were devel oped to solve problems in market basket analysis the task consists of detecting rules in large numbers millions of cus tomer transactions where the rules describe the probability that a customer buys item b given that he has already item a in his shopping basket more precisely as shown in the problem can be formulated as follows let i ip be a set of p items we call a subset a of i with m items an m itemset a transaction is an itemset t i with a transaction identifier tid t a transaction database d tn is a set of transactions with unique identifiers tid ti we say that a transaction t supports an itemset a if a t we can now define the support of an itemset a d in the transaction database d as follows supp a t d a t d an itemset a is called frequent in d if supp a smin where smin is a threshold for the minimal support fre quent itemsets are subject to the monotonicity property all m subsets of frequent m sets are also frequent the apriori algorithm was the first to take advantage of the monotonicity property to find frequent itemsets very quickly in our setting the text associated with each photo tags caption titles etc generates a transaction and the database consists of the set of photos in a cluster we use an imple mentation of the fpgrowth algorithm to mine the frequent itemsets for each cluster using a minimal support thresh old of 0 in order to ensure scalability only the top itemsets per cluster are kept the advantage of using itemset mining over other prob abilistic method is its speed and scalability tens of thou sands of word combinations can be processed in fractions of seconds furthermore mining variants such as maximal or closed frequent itemsets as well as additional statisti cal tests on the sets offer further opportunities for opti mization for instance maximal frequent itemsets itemsets with no frequent superset are especially useful for human readable labels on clusters since their subsets are not listed as additional labels querying wikipedia and link verification we use each frequent itemset mined in the previous sec tion to submit a query to an internet search engine more specifically we query google www google com limiting the search to wikipedia org by doing so the search covers wi kipedia in all available languages so terms in different lan guages can be handled automatically for each result list the top results are kept note that in the worst case this generates possible urls per cluster we keep a table dataset statistics score for each page which counts how often the same page was retrieved using different queries next we open each of the urls and parse the corresponding wikipedia page for images the idea is now to use the wikipedia content to verify the proposed linking between the cluster and the wikipedia page chances are high that our clusters contain some images taken from similar viewpoints as the ones used in wikipedia thus we extract features from the wikipedia images and try to match them to all images in the cluster using the same method as described in section if we find a matching image the proposed link is kept otherwise it is rejected results in the following we present results on the whole dataset collected to this date stemming from the geographic tiles that were inspected by our algorithm we first give an overview over the dataset followed by subsections discussing the results of the individual processing layers table sum marizes the dataset statistics in total over images were downloaded from flickr their visual features amount ing to gb and their metadata tags geotags exif data etc to over million pairwise similarities had to be computed less than million was greater than zero note that without the geographic tiling we would have had to calculate over billion pairwise similarities in the end a little over photos could be assigned to a cluster clusters here we present results for different types of clustering we start with a specific example to give an impression of the results we found figure shows examples from the area around the pantheon in rome the corresponding tile is among those with the largest number of elements contain ing 250 images several tiles overlap here we report the numbers for the dominant one it is well visible how the clustering splits the data into several semantically separate objects and contexts for example indoor a and frontal outdoor views b of the pantheon are found as separate entities both contain a large number of photos and respectively smaller clusters describe more specific el ements such as the view from the pantheon onto the piazza e the obelisk situated behind the pantheon c and even the tomb of victor emmanuel ii d inside the pantheon calculating the mean of the photo locations in each cluster allows us to place the cluster on a map clearly the loca tions of the different clusters are estimated very close to the true positions of the corresponding entities the clusters shown in this figure were obtained using single link cluster ing note how especially for clusters a b and c this allows us to merge a wide variety of views of the same object since only the closest matching pair has to be connected by a distance smaller than the threshold in total clusters were found in this area with a mean a b c e d figure clusters found around the pantheon and the number of photos contained in each note the automatic separation into indoor a outdoor b and panorama views e and the discovery of separate objects c d mean locations of the photos are shown on the map e is estimated at about the same position as b and is therefore not drawn on the map size of photos we evaluate clustering accuracy in terms of the cluster precision i e the number of correct images divided by the total number of images in the cluster as correct we count every image which contains the object the cluster refers to if there are special contexts such as an indoor view for an object only those e g indoor views are counted as correct given that definition the mean precision of the largest clusters is over note that since we deal with an unsupervised mining problem we cannot give reliable results for recall for comparison we also ran a clustering based purely on text using all text similarities between the photos in this area depending on the parameters we were only able to get clusters with a precision of about not only were we not able to discriminate between indoor and out door views based on text features the clusters also contained many outliers which did not contain the relevant object at all for instance only of the photos in the area carry tags such as inside or interior making a discrimination based on text very difficult in contrast cluster a in fig ure contains over photos of the inside of the pantheon the word pantheon appears with photos also in comparison to we are able to retrieve larger clusters while maintaining high precision to examine the results of the different types of visual clus tering further consider another example shown in figure it depicts the area around the louvre in paris figure a shows the estimated mean positions of single link clusters in total the area is covered by clusters the largest clus ter contains elements the mean size is elements one of the clusters marked in yellow is shown in figure b here each pin represents the location of one photo note how strongly the positions vary some examples of the clus ters contents are shown in the column next to the map again visualizing the mentioned variability in viewpoints in contrast figure c shows the complete link clusters for the same area the more restrictive clustering criterion results in smaller and more compact clusters the mean size is only elements and the maximum is complete link clus ters were found for this region again one cluster is selected and its elements are shown in figure d their locations are more compact and the contents of the cluster have less variability as the examples next to the map demonstrate also note again the grid overlaid on the maps in a and c which shows the tiles we used to retrieve photos by their geotags again cells make up a tile objects and events the classifier described in section allows us not only to detect objects but in sometimes even events applying the tree to the entire dataset resulted in the following distribution of objects and events of clusters single link were classified as objects as events vi sual inspection on randomly picked clusters showed that the classification precision is very accurate similar to the results obtained on the validation set in section figure shows some examples of event clusters the first cluster contains images from different events in a series taking place on dif ferent days oxford geek nights and was recognized due to the same location it took place in the second a movie premiere in italy and third event an exhibition in a gallery in paris were both covered by two photographers the last line represents the majority of events an event from a single day covered by only one photographer the smaller num ber of event clusters can be explained by two factors relying mostly on visual cues we can only detect events which take a b c d figure clusters around the louvre a shows single link clusters the photos of the cluster marked in yellow are located as shown in b c shows complete link clusters for the same area again with the photos of the yellow cluster in d only clusters with at least elements are shown figure typical events mined by our methods place in a environment where the background matches be tween photos second it seems that so far in general fewer people geotag photos of events linking to wikipedia figure visualizes the individual steps in linking clusters to wikipedia content the tags for the cluster a are mined to create frequent itemsets b note how the proximity to the louvre introduces noisy words such as museum and how the expression arc du triomphe could refer also to the other larger arc du triomphe in paris the frequent itemsets b are fed as queries to google and the candidate urls c are retrieved for each url the images contained in the page are extracted and matched back to the images in the cluster a figure d shows the best match from the cluster with the image from the wikipedia article e the final selected url is given in f figure shows some typical results of this process each result is represented by a pair of images the left image was extracted from wikipedia the one on the right is its closest match in the cluster there are typically many more match ing images in each cluster below each pair we provide the url of the mined wikipedia article followed by the cluster statistics for each cluster we report the number of pho tos the number of users who took them and the number of different days the photos were taken at we also report the precision obtained again by manual inspection as described above in general the precision is very high ranging between and especially very well known landmarks such as the sacre coeur the colosseum or the trevi fountain are covered by a large number of photos with very few false positives lesser known objects such as the radcliffe camera have fewer images and are thus also more vulnerable to a few false positives staying with the radcliffe camera note how multiple matching wikipe dia articles have been verified for the object the same effect can be observed in example or example where ar ticles in multiple languages were retrieved some matches are truly amazing for instance example where a paint ing matched to a photo of the colosseum or and with strong clutter and viewpoint change while most examples in figure refer to rather well known objects some rare gems were mined too a few examples are shown in figure example does not only link to the article sainte chapelle but also to an article about stained glass similarly mona lisa 2 is linked to a specific article and a more general one about leonardo da vinci in exam ple both the context forum romanum and the specific temple of vesta could be verified examples of smaller even lesser known entities are shown in note the may pole on viktualienmarkt in munich in one of the articles explains the location the other the tradition destinations with fewer tourists such as tallinn and zurich tend to have less photo coverage and also less content on wikipedia nevertheless some locations could be identified by our min ing pipeline finally example is a lucky shot where an event could be linked to a person and verified by coinci dence wikipedia contains an image of an event jules verne adventures film festival april which is also cov ered on flickr and labeled with the attending actors name clearly only larger events are covered in wikipedia so that the chance of detecting a correct link for any event is rather small furthermore homography based matching between images is well suited for rigid objects and scenes but less suited for events future work could thus extend the system by classifying event scenes wedding concert etc based on a bag of features approach and rather label it using the textual meta data than link it to wikipedia in total unique wikipedia articles were verified by matching their images to our clusters as described above the precision of this assignment was about i e of the articles referred to a cluster which contained images of the article correct subject these articles covered single link clusters querying wikipedia with the queries given by the frequent itemsets had resulted in over 20 urls for consideration and in more than twice as many images this demonstrates how effective our method is in mining relevant links out of a vast amount of irrelevant data auto annotation with the database we built in this paper auto annotation of unlabeled photos with their geo location and correspond ing wikipedia article becomes feasible a user can simply museum museum louvre carrousel carrousel triomphe carrousel triomphe arc carrousel triomphe arc du carrousel triomphe du carrousel arc carrousel arc du carrousel du triomphe triomphe arc triomphe arc du triomphe du arc arc du b c d e f figure matching clusters to wikipedia articles the text for the photos in a cluster a is mined for frequent word combinations b which are used to search wikipedia for candidate urls c each image of an article is in return matched to the images in the cluster if a good match e can be found the candidate link is selected f elements users days precision elements users days precision elements users days precision elements users days precision see matchted to the same cluster elements users days precision elements users days precision elements users days precision 40 elements users days precision elements users days precision elements users days precision elements users days precision elements users days precision elements users days precision elements users days precision figure a world tour with flickr and wikipedia the left image in each pair stems from wikipedia the right image is the best match in a mined cluster the wikipedia links which could be verified this way are reported below the images together with the cluster statistics note the high precision scores and the size of some clusters see text for a detailed discussion elements users days precision elements users days precision elements users days precision elements users days precision elements users days precsion elements users days precison elements 2 users days precision elements users days precision elements users days precision figure additional surprising mining results see text for a discussion select the rough geographic area e g by drawing a bound ing box around paris on the map and the photos will be automatically placed at their exact position and linked to relevant wikipedia articles to demonstrate this capability we downloaded sample query images of sights in paris from google see figure these are images which are neither present on flickr nor on wikipedia we load all clusters which we found in the paris area full area as given in ta ble and which could be assigned to a wikipedia article as described in the previous steps these conditions hold for clusters now we simply match the query images to the clusters and record the best matching image and cluster this process only takes minutes and the result is shown in figure the result location is selected as the mean loca tion of all images in the matching cluster note the precision of the placement in the magnified map elements all images are also linked to the correct wikipedia article in the spirit of figures and the links are not shown due to lack of space note how similar the arc de triomphe and arc de triomphe du carousel are first and second image in the left column also note how close the two objects arc de triom phe du carousel and the louvre pyramid are second and third map in the left column our method is able to han dle these uncertainties robustly and to discriminate between similar objects at different locations and different objects at the same location in contrast a direct matching of query images to wikipedia images would not be possible in most cases since the viewpoint changes might be too large the number of images in our clusters literallybridges the gap between the unannotated query image and the wikipedia image via the clusters created from flickr data combining this method with scalable indexing for local features will allow auto annotation of many holiday snaps within seconds conclusions we have presented a fully unsupervised mining pipeline for community photo collections the sole input is a grid of tiles on a world map the output is a database of mined objects and events many of them labeled with an automat ically created and verified link to wikipedia the pipeline chains processing steps of several modalities in a highly ef fective way the basis is a pairwise similarity calculation with local visual features and multi view geometry for each tile hierarchical clustering was demonstrated to be a very effective method to extract clusters of the same entities in different contexts indoor outdoor etc we observed that the clustering step on visual data is far more reliable than on text labels a simple tree based classifier on the metadata of photos was introduced to discriminate between object an event clusters itemset mining on the text of the clusters created with visual features was proposed to mine frequent word combinations per cluster those were used to search wikipedia for potentially relevant articles the relevance was verified by matching images from the wikipedia arti cles back to the mined clusters both the clustering and linking to wikipedia showed high precision finally in a last step we demonstrated how the database can be used to auto annotate unlabeled images without geotags besides the effective mining pipeline proposed in the pa per we also carried out one of the largest experiments with local visual features on data from community photo col lections by processing over photos the results of this large scale experiment are very encouraging and open a wealth of novel research opportunities figure auto annotation of novel images using the mined clusters assessing the quality of actions hamed pirsiavash carl vondrick antonio torralba massachusetts institute of technology hpirsiav vondrick torralba mit edu abstract while recent advances in computer vision have provided reli able methods to recognize actions in both images and videos the problem of assessing how well people perform actions has been largely unexplored in computer vision since methods for assessing action quality have many real world applications in healthcare sports and video retrieval we be lieve the computer vision community should begin to tackle this challeng ing problem to spur progress we introduce a learning based framework that takes steps towards assessing how well people perform actions in videos our approach works by training a regression model from spa tiotemporal pose features to scores obtained from expert judges more over our approach can provide interpretable feedback on how people can improve their action we evaluate our method on a new olympic sports dataset and our experiments suggest our framework is able to rank the athletes more accurately than a non expert human while promising our method is still a long way to rivaling the performance of expert judges indicating that there is significant opportunity in computer vision re search to improve on this difficult yet important task introduction recent advances in computer vision have provided reliable methods for rec ognizing actions in videos and images however the problem of automatically quantifying how well people perform actions has been largely unexplored we believe the computer vision community should begin to tackle the chal lenging problem of assessing the quality of people actions because there are many important real world applications for example in health care patients are often monitored and evaluated after hospitalization as they perform daily tasks which is expensive undertaking without an automatic assessment method in sports action quality assessments would allow an athlete to practice in front of fig we introduce a learning framework for assessing the quality of human actions from videos since we estimate a model for what con stitutes a high quality action our method can also provide feedback on how people can improve their ac tions visualized with the red arrows a camera and receive quality scores in real time providing the athlete with rapid feedback and an opportunity to improve their action in retrieval a video search engine may want to sort results based on the quality of the action performed instead of only the relevance however automatically assessing the quality of actions is not an easy com puter vision problem human experts for a particular domain such as coaches or doctors have typically been trained over many years to develop complex un derlying rules to assess action quality if machines are to assess action quality then they must discover similar rules as well in this paper we propose a data driven method to learn how to assess the quality of actions in videos to our knowledge we are the first to propose a general framework for learning to assess the quality of human based actions from videos our method works by extracting the spatio temporal pose features of people and with minimal annotation estimating a regression model that predicts the scores of actions fig shows an example output of our system in order to quantify the performance of our methods we introduce a new dataset for action quality assessment comprised of olympic sports footage al though the methods in this paper are general sports broadcast footage has the advantage that it is freely available and comes already rigorously annotated by the olympic judges we evaluate our quality assessments on both diving and figure skating competitions our results are promising and suggest that our method is significantly better at ranking people actions by their quality than non expert humans however our method is still a long way from rivaling the performance of expert judges indicating that there is significant opportunity in computer vision research to improve on this difficult yet important task moreover since our method leverages high level pose features to learn a model for action quality we can use this model to help machines understand people in videos as well firstly we can provide interpretable feedback to performers on how to improve the quality of their action the red vectors in fig are output from our system that instructs the olympic diver to stretch his hands and lower his feet our feedback system works by calculating the gradient for each body joint against the learned model that would have maximized people scores sec ondly we can create highlights of videos by finding which segments contributed the most to the action quality complementing work in video summarization we hypothesize that further progress in building better quality assessment models can improve both feedback systems and video highlights the three principal contributions of this paper revolve around automatically assessing the quality of people actions in videos firstly we introduce a general learning based framework for the quality assessment of human actions using spatiotemporal pose features secondly we then describe a system to generate feedback for performers in order to improve their score finally we release a new dataset for action quality assessment in the hopes of facilitating future research on this task the remainder of this paper describes these contributions in detail related work this paper builds upon several areas of computer vision we briefly review re lated work action assessment the problem of action quality assessment has been relatively unexplored in the computer vision community there have been a few promising efforts to judge how well people perform actions however these previous works have so far been hand crafted for specific actions the motivation for assessing peoples actions in healthcare applications has also been discussed before but the technical method is limited to recognizing actions in this paper we propose a generic learning based framework with state of the art features for action quality assessment that can be applied to most types of human actions to demonstrate this generality we evaluate on two distinct types of actions diving and figure skating furthermore our system is able to generate interpretable feedback on how performers can improve their action photograph assessment there are several works that assess photographs such as their quality interestingness and aesthetics in this work we instead focus on assessing the quality of human actions and not the quality of the video capture or its artistic aspects action recognition there is a large body of work studying how to rec ognize actions in both images and videos and we refer readers to excellent surveys for a full review while this paper also studies actions we are interested in assessing their quality rather than recognizing them features there are many features for action recognition using spatiotem poral bag of words interest points feature learning and human pose based however so far these features have primarily been shown to work for recognition we found that some of these features notably and with minor adjustments can be used for the quality assessment of actions too video summarization this paper complements work in video summa rization rather than relying on saliency features or priors we instead can summarize videos by discarding segments that did not impact the quality score of an action thereby creating a highlights reel for the video assessing action quality we now present our system for assessing the quality of an action from videos on a high level our model learns a regression model from spatio temporal features after presenting our model we then show how our model can be used to provide feedback to the people in videos to improve their actions we finally describe how our model can highlight segments of the video that contribute the most to the quality score features to learn a regression model to the action quality we extract spatio temporal fea tures from videos we consider two sets of features low level features that cap ture gradients and velocities directly from pixels and high level features based off the trajectory of human pose low level features since there has been significant progress in developing features for recognizing actions we tried using them for assessing actions too we use a hierarchical feature that obtains state of the art performance in action recognition by learning a filter bank with independent subspace analysis the learned filter bank consists of spatio temporal gabor like filters that capture edges and velocities in our experiments we use the implementation by with the network pre trained on the dataset high level pose features since most low level features capture statistics from pixels directly they are often difficult to interpret as we wish to provide feedback on how a performer can improve their actions we want the feedback to be interpretable inspired by actionlets we now present high level features based off human pose that are interpretable given a video we assume that we know the pose of the human performer in every frame obtained either through ground truth or automatic pose estimation let p j t be the x component of the jth joint in the tth frame of the video since we want our features to be translation invariant we normalize the joint positions relative to the head position q j t p j t p t where we have assumed that p t refers to the head note that q j is a function of time so we can represent it in the frequency domain by the discrete cosine transform dct q j aq j where a is the discrete cosine transformation matrix we then use the k lowest frequency components to create the feature j k only using the low frequencies helps remove high frequency noise due to pose estimation errors we use the absolute value of the frequency coefficients qi we compute φj for every joint for both the x and y components and con catenate them to create the final feature vector φ we note that if the video is long we break it up into segments and concatenate the features to produce one feature vector for the entire video this inreases the temporal resolution of our features for long videos actionlets uses a similar method with discrete fourier transform dft instead although there is a close relationship between dft and dct we see better results using dct we believe this is the case since dct provides a more compact representation additionally dct coefficients are real numbers instead of complex so less information is lost in the absolute value operation in order to estimate the joints of the performer throughout the video p j t we run a pose estimation algorithm to find the position of the joints in every frame we estimate the pose using a flexible parts model for each frame independently since finds the best pose for a single frame using dynamic fig pose estimation challenges some results for human pose estimation on our action quality dataset since the performers contort their body in unusual configurations pose estimation is very challenging on our dataset programming and we want the best pose across the entire video we find the n best pose solutions per frame using then we associate the poses using a dynamic programming algorithm to find the best track in the whole video the association looks for the single best smooth track covering the whole temporal span of the video fig shows some successes and failures of this pose estimation learning we then pose quality assessment as a supervised regression problem let φi rk n be the pose features for video i in matrix form where n is the number of joints and k is the number of low frequency components we write yi r to denote the ground truth quality score of the action in video i obtained by an expert human judge we then train a linear support vector regression l svr to predict yi given features φi over a training set in our experiments we use libsvm optimization is fast and takes less than a second on typical sized problems we perform cross validation to estimate hyperparameters domain knowledge we note that a comprehensive model for quality assess ment might use domain experts to annotate fine tuned knowledge on the action quality e g the leg must be straight however relying on domain experts is expensive and difficult to scale to a large number of actions by posing quality assessment as a machine learning problem with minimal interaction from an ex pert we can scale more efficiently in our system we only require a single real number per video corresponding to the score of the quality prototypical example moreover a fairly simple method to assess quality is to check the observed video against a ground truth video with perfect execution and then determine the difference however in practice many actions can have multiple ideal executions e g a perfect overhand serve might be just as good as a perfect underhand serve instead our model can handle multi modal score distributions feedback proposals as a performer executes an action in addition to assessing the quality we also wish to provide feedback on how the performer can improve his action since our regression model operates over pose based features we can determine how the performer should move to maximize the score we accomplish this by differentiating the scoring function with respect to joint location we calculate the gradient of the score with respect to the location of each joint s where s is the scoring function by calculating the maximum gradient we can find the joint and the direction that the performer must move to achieve the largest improvement in the score we are able to analytically calculate the gradient recall that l svr learns a weight vector w rk n such that w predicts the score of the action quality by the dot product k n s wfjφfj f j where φfj is the f th frequency componenet for the jth joint after basic algebra we can compute the gradient of the score s with respect to the location of each joint p j t s p j t f aftwfj sign t ti afti p j tl p tl by computing maxp j s we can find the joint and the direction the performer must move to most improve the score video highlights in addition to finding the joint that will result in the largest score improvement we also wish to measure the impact a segment of the video has on the quality score such a measure could be useful in summarizing the segments of actions that contribute to high or low scores we define a segment impact as how much the quality score would change if the segment were removed in order to remove a segment we compute the most likely feature vector had we not observed the missing segment the key observation is that since we only use the low frequency components in our fea ture vector there are more equations than unknowns when estimating the dct coefficients consequently removing a segment corresponds to simply removing some equations let b a be the inverse cosine transform where a is the psuedo inverse of a then the dct equation can be written as q j b q j if the data from we do not differentiate with respect to the head location because it is used for normalization removed segment displacement time fig interpolating segments this schematic shows how the displacement vector changes when a segment of the video is removed in order to compute impact the dashed curve is the original displacement and the solid curve is the most likely displacement given observations with a missing segment frames u through v is missing then the inferred dct coefficients are qˆ j bu v q j where bu v is the sub matrix of b that excludes rows u through v the frequency components qˆ j are the same dimensionality as q j but they have inferred the missing segment with the most likely joint trajectory fig visualizes how the features change with this transformation we use qˆ j to create the feature vector for the video with the missing seg ment finally we determine the impact of the missing segment by calculating the difference in scores between the original feature vector and the feature vector with the missing segment experiments in this section we evaluate both our quality assessment method and feedback system for quality improvement with quantitative experiments since quality as sessment has not yet been extensively studied in the computer vision community we first introduce a new video dataset for action quality assessment action quality dataset there are two primary hurdles in building a large dataset for action quality assessment firstly the score annotations are subjective and require an expert unfortunately hiring an expert to annotate hundreds of videos is expensive secondly in some applications such as health care there are privacy and legal issues involved in collecting videos from patients in order to establish a baseline dataset for further research we desire freely available videos we introduce an olympics video dataset for action quality assessment sports footage has the advantage that it can be obtained freely and the expert judge scores are frequently released publicly we collected videos from youtube for two categories of sports diving and figure skating from recent olympics and other worldwide championships the videos are long with multiple instances of actions performed by multiple people we annotated the videos with the start and end frame for each instance and we extracted the judge score the dataset will be publicly available fig diving dataset some of the best dives from our diving dataset each column corresponds to one video there is a large variation in the top scoring actions hence providing feedback is not as easy as pushing the action towards a canonical good performance fig figure skating dataset sample frames from our figure skating dataset notice the large variations of routines that the performers attempt this makes automatic pose estimation challenging diving fig shows a few examples of our diving dataset our diving dataset consists of videos the videos are slow motion from television broadcasting channels so the effective frame rate is frames per second each video is about frames and the entire dataset consists of frames the ground truth judge scores varies between 20 worst and best in our experiments we use instances for training and the rest for testing we repeated every experiment times with different random splits and averaged the results in addition to the olympic judge score we also consulted with the mit varsity diving coach who annotated which joints a diver should adjust to improve each dive we use this data to evaluate our feedback system for the quality improvement algorithm figure skating fig shows some frames from our figure skating dataset this dataset contains videos captured at frames per second each video is almost frames and the entire dataset is frames the judge score ranges between worst and best we use instances for training and the rest for testing as before we repeated every experiment 200 times with different random splits and averaged the results we note that our figure skating tends to be more challenging for pose estimation since it is at a lower frame rate and has more variation in the human pose and clothing e g wearing skirt table diving evaluation we show mean rank correlation on our diving dataset higher is better the pose based features provide the best performance table figure skating evaluation we calculate mean rank correlation on our figure skating dataset higher is better the hierarchical network features provide the best results although pose based features are not superior they still enable high level analysis by providing feedback for quality improvement we believe pose based features can benefit from using a better pose estimation quality assessment we evaluate our quality assessment on both the figure skating and diving dataset in order to compare our results against the ground truth we use the rank cor relation of the scores we predict against the scores the olympic judges awarded tab and tab show the mean performance over random train test splits of our datasets our results suggest that pose based features are competitive and even obtain the best performance on the diving dataset in addition our results indicate that features learned to recognize actions can be used to assess the qual ity of actions too we show some of the best and worst videos as predicted by our model in fig we compare our quality assessment against several baselines firstly we com pare to both space time interest points stip and pose based features with dis crete fourier transform dft instead of dct similar to both of these features performed worse secondly we also compare to ridge regression with all feature sets our results show that support vector regression often obtains significantly better performance we also asked non expert human annotators to predict the quality of each diver in the diving dataset interestingly after we instructed the subjects to read the wikipedia page on diving non expert annotators were only able to achieve a rank correlation of which is half the performance of support vector regression with pose features we believe this difference is evidence that our algorithm is starting to learn which human poses constitute good dives we note however that our method is far from matching olympic judges since they are able to predict the median judge score with a rank correlation of suggesting that there is still significant room for improvement olympic diving competitions have two scores the technical difficulty and the score the final quality of the action is then the product of these two quantities judges are fig examples of diving scores we show the two best and worst videos sorted by the predicted score each column is one video with ground truth and predicted score written below notice that in the last place video the diver lacked straight legs in the beginning and did not have a tight folding pose these two pitfalls are part of common diving advice given by coaches and our model has learned this independently limitations while our system is able to predict the quality of actions with some success it has many limitations one of the major bottlenecks is the pose estimation fig 2 shows a few examples of the successes and failures of the pose estimation pose estimation in our datasets is very challenging since the performers contort their body in many unusual configurations with significant variation in appearance the frequent occlusion by clothing for figure skating noticeably harms the pose estimation performance when the pose estimation is poor the quality score is strongly affected suggesting that advances in pose estimation or using depth sensors for pose can improve our system future work in action quality can be made robust against these types of failures as well by accounting for the uncertainty in the pose estimation our system is designed to work for one human performer only and does not model coordination between multiple people which is often important for many types of sports and activities we believe that future work in explicitly modeling team activities and interactions can significantly advance action quality assessment moreover we do not model objects used during actions such as sports balls or tools and we do not consider physical outcomes such as splashes told the technical difficulty apriori which gives them a slight competitive edge over our algorithms we did not model the technical difficulty in the interest of building a general system fig diving feedback proposals we show feedback for some of the divers the red vectors are instructing the divers to move their body in the direction of the arrow in general the feedback instructs divers to tuck their legs more and straighten their body before entering the pool in diving which may be important features for some activities finally while our representation captures the movements of human joint locations we do not explicitly model their synchronization e g keeping legs together or repetitions e g waving hands back and forth we suspect a stronger quality assessment model will factor in these visual elements feedback for improvement in addition to quality assessment we evaluate the feedback vectors that our method provides fig and fig show qualitatively a sample of the feedback that our algorithm suggests in general the feedback is reasonable often making modifications to the extremities of the performer in order to quantitatively evaluate our feedback method we needed to ac quire ground truth annotations we consulted with the mit diving team coach who watched a subset of the videos in our dataset in total and provided sug gestions on how to improve the dive the diving coach gave us specific feedback such as move left foot down as well as high level feedback e g legs should be straight here or tuck arms more we translated each feedback from the coach into one of three classes referring to whether the diver should adjust his upper body his lower body or maintain the same pose on each frame due to the subjective nature of the task the diving coach was not able to provide more detailed feedback annotations hence the feedback is coarsely mapped into these three classes we then evaluate our feedback as a detection problem we consider a feed back proposal from our algorithm as correct if it suggests to move a body part within a one second range of the coach making the same suggestion we use the magnitude of the feedback gradient as the importance of the feedback proposal fig figure skating feedback proposals we show feedback for some of the figure skaters where the red vectors are instructions for the figure skaters fig feedback limitations the feedback we generate is not perfect if the figure skater or diver were to rely completely on the feedback above they may fall over our model does not factor in physical laws motivating work in support inference we use a leave one out approach where we predict feedback on a video held out from training our feedback proposals obtain ap overall for diving compared to ap chance level we compute chance by randomly generating feedback that uniformly chooses between the upper body and lower body since our action quality assessment model is not aware of physical laws the feedback suggestions can be physically implausible fig shows a few cases where if the performer listened to our feedback they might fall over our method lack of physical models motivates work in support inference interestingly by averaging the feedback across all divers in our dataset we can find the most common feedback produced by our model fig shows the magnitude of feedback for each frame and each joint averaged over all divers for visualization proposes we warp all videos to have the same length most of the feedback suggests correcting the feet and hands and the most important frames turn out to be the initial jump off the diving board the zenith of the dive and the moment right before the diver enters the water frame marginals neck l shoulder r arm k arm ru torso r shoulder lu torso r elbow rd torso l elbow ld torso ru leg lu leg r forearm l forearm r hip l hip r knee l knee ld leg r hand l hand rd leg l foot r foot start zenith of dive frame preparing for water entry water entry end fig visualizing common feedback we visualize the average feedback magnitude across the entire diving dataset for each joint and frame red means high feedback and blue means low feedback the top and right edges show marginals over frames and joints respectively r and l stand for right and left respectively and u and d stand for upper and lower body respectively feet are the most common area for feedback on olympic divers and that the beginning and end of the dive are the most important time points highlighting impact we qualitatively analyze the video highlights produced by finding the segments that contributed the most to the final quality score we believe that this measure can be useful for video summarization since it reveals out of a long video which clips are the most important for the action quality we computed impact on a routine from the figure skating dataset in fig notice when the impact is near zero the figure skater is in a standard up right position or in between maneuvers the points of maximum impact correspond to jumps and twists of the figure skater which contributes positively to the score if the skater performs it correctly and negatively otherwise discussion if quality assessment is a subjective task is it reasonable for a machine to still obtain reasonable results remarkably the independent olympic judges agree with each other of the time which suggests that there is some underlying structure in the data one hypothesis to explain this correlation is that the judges are following a complex system of rules to gauge the score if so then the job of a machine quality assessment system is to extract these rules while the approach in this paper attempts to learn these rules we are still a long way from high performance on this task conclusions assessing the quality of actions is an important problem with many real world applications in health care sports and search to enable these applications we have introduced a general learning based framework to automatically assess an a b fig video highlights by calculating the impact each frame has on the score of the video we can summarize long videos with the segments that have the largest impact on the quality score notice how above when the impact is close to zero the skater is usually in an upright standard position and when the impact is large the skater is performing a maneuver action quality from videos as well as to provide feedback for how the performer can improve we evaluated our system on a dataset of olympic divers and figure skaters and we show that our approach is significantly better at assessing an action quality than a non expert human although the quality of an action is a subjective measure the independent olympic judges have a large correlation this implies that there is a well defined underlying rule that a computer vision system should be able to learn from data our hope is that this paper will motivate more work in this relatively unexplored area to appear proceedings of the international conference on computer vision iccv annotator rationales for visual recognition jeff donahue and kristen grauman dept of computer science university of texas at austin jdd grauman cs utexas edu abstract traditional supervised visual learning simply asks anno tators what label an image should have we propose an approach for image classification problems requiring sub jective judgment that also asks why and uses that infor mation to enrich the learned model we develop two forms of visual annotator rationales in the first the annotator highlights the spatial region of interest he found most influ ential to the label selected and in the second he comments on the visual attributes that were most important for either case we show how to map the response to synthetic contrast examples and then exploit an existing large margin learn ing technique to refine the decision boundary accordingly results on multiple scene categorization and human attrac tiveness tasks show the promise of our approach which can more accurately learn complex categories with the explana tions behind the label choices introduction image classification is an important challenge in com puter vision and has a variety of applications such as automating content based retrieval analyzing medical im agery or recognizing locations in photos much progress over the last decade shows that supervised learning algo rithms coupled with effective image descriptors can yield very good scene object and attribute predictions e g the standard training process entails gathering category labeled image exemplars essentially asking hu man annotators to say what is present and possibly where in the image it is in this respect current ap proaches give a rather restricted channel of input to the human viewer who undoubtedly has a much richer under standing than a simple label can convey thus our goal is to capture deeper cues from annotators we are particularly interested in complex visual recogni tion problems that require subjective judgment e g saying whether a face is attractive rating an athletic performance or else lack clear cut semantic boundaries e g describing a scene category categorizing by approximate age see is this anchor doing a is this scene from a is this woman serious or light story comedy or a drama hot or not are these tv characters is this scene a is this figure friends or nemeses lounge or a bedroom skater form good figure main premise subjective or complex image classifica tion tasks such as those depicted above may require deeper insight from human annotators than the usual category labels we pro pose to ask for spatial or attribute based rationales for the labels chosen and augment a large margin classifier objective to exploit both the labels and these explanations figure can we really expect to learn such subtle con cepts purely by training svms with hog descriptors and category names we instead propose to allow annotators to give a rationale for the label they choose and then di rectly use those explanations to strengthen a discriminative classifier their insight about why should not only enable more accurate models but potentially also do so with less total human effort since we could amortize the time spent analyzing the image to determine the label itself how can an annotator give an explanation we propose two modes in the first the annotators indicate which re gions of the image most influenced their label choice by drawing polygons that is they highlight what seemed most telling for the classification task at hand i can tell it class x mainly due to this region here in the second mode the annotators indicate which visual attributes were deemed most influential where an attribute refers to some nameable property or part for example assuming we have intermediate detectors for attributes like size color and spe cific textures they can state it too round to be an x or she attractive because she fit in either case the rationale should help focus the classi fier on the low or mid level image features that can best be used to discriminate between the desired image cate gories to that end we directly leverage an idea originally developed by zaidan and colleagues for document classifi cation it generates synthetic contrast examples that lack the features in the rationales and then adds constraints to a classifier objective that require the contrast examples to be considered less positive or less negative than the original examples in this way the contrast examples can refine the decision boundary in the target label space while recent work explores various issues in collecting useful labeled datasets we are the first to propose asking annotators for explanations of their labels to directly improve visual category learning with out injecting into a classifier knowledge of why a given la bel was chosen traditional discriminative feature selection techniques risk overfitting to inadequate or biased training examples in contrast our strategy stands to benefit more immediately from complex human insight and thus poten tially with less total training data we demonstrate our approach with both scene and hu man attractiveness categorization tasks and report results on the scenes and public figures face datasets as well as a new dataset of hot or not images we show that both proposed visual rationales can improve absolute recognition accuracy we also analyze their impact rela tive to several baselines including foreground segmented images and a standard mutual information feature selection approach overall we find that human intuition can be cap tured in a new way with the proposed technique related work much work in visual recognition results in unman ageable number of parameters in addition estimates of the parameters for the majority of edges would be noisy there are serious smoothing issues we adopt an approach similar to good turing smoothing methods to a control the num ber of parameters b do smoothing we have multiple estimates for the edges potentials which can provide more accurate estimates if used together we form the linear combinations of these potentials therefore in learning we are inter ested in finding weights of the linear combination of the initial estimates so that the final linearly combined potentials provide values on the mrf so that the ground truth triplet is the highest scored triplet for all examples this way we limit the number of parameters to the number of initial estimates we have four different estimates for edges our final score on the edges take the form of a linear combination of these estimates our four estimates for edges from node a to node b are the normalized frequency of the word a in our corpus f a the normalized frequency of the word b in our corpus f b the normalized frequency of a and b at the same time sentence potentials we need a representation of the sentences we represent a sentence by computing the similarity between the sentence and our triplets for that we need to have a notion of similarity for objects scenes and actions in text we used the curran clark parser to generate a dependency parse for each sentence we extracted the subject direct object and any nmod dependen cis involving a noun and a verb these dependencies were used to generate the object action pairs for the sentences in order to extract the scene information from the sentences we extracted the head nouns of the prepositional phrases except for the prepositions of and with and the head nouns of the phrase x in the background lin similarity measure for objects and scenes we use the lin similarity measure to determine the semantic distance between two words the lin similarity measure uses wordnet synsets as the possible meanings of each words the noun synsets are arranged in a heirarchy based on hypernym is a and hyponym instance of relations each synset is defined as having an information content based on how frequently the synset or a hyponym of the synset occurs in a corpus in the case semcor the similarity of two synsets is defined as twice the information content of the least common ancestor of the synsets divided by the sum of the information content of the two synsets similar synsets will have a lca that covers the two synsets and very little else when we compared two nouns we considered all pairs of a filtered list of synsets for each noun and used the most similar synsets we filtered the list of synsets for each noun by limiting it to the first four synsets that were at least as frequent as the most common synset of that noun we also required the synsets to be physical entities action co occurrence score we generated a second image caption data set consisting of roughly images pulled from six flickr groups for all pairs of verbs we used the likelihood ratio to determine if the two verbs co occurring in the different captions of the same image was significant we then used the likelihood ratio as the similarity score for the positively correlated verb pairs and the negative of the likelihood ratio as the similarity score for the negatively correlated verb pairs typically we found that this procedure discovered verbs that were either describing the same action or describing two actions that commonly co occurred node potentials we now can provide a similarity measure between sentences and objects actions and scenes using scores explained above below we explain our estimates of sentence node potentials first we compute the similarity of each object scene and action extracted from each sentence this gives us the the first estimates for the potentials over the nodes we call this the sentence node feature for each sentence we also compute the average of sentence node features for other four sentences describing the same images in the train set we compute the average of k nearest neighbors in the sentence node features space for a given sentence we consider this as our third estimate for nodes we also compute the average of the image node features for images corre sponding to the nearest neighbors in the item above the average of the sentence node features of reference sentences for the nearest neighbors in the item is considered as our fifth estimate for nodes we also include the sentence node feature for the reference sentence edge potentials the edge estimates for sentences are identical to to edge estimates for the images explained in previous section learning there are two mappings that need to be learned the map from the image space to the meaning space uses the image potentials and the map from the sentence space to the meaning space uses the sentence potentials learning the mapping from images to meaning involves finding the weights on the linear combinations of our image potentials on nodes and edges so that the ground truth triplets score highest among all other triplets for all examples this is a structure learning problem which takes the form of min λ ξ subject to wφ xi yi ξi y max wφ xi y l yi y i examples meaning space ξi i examples where λ is the tradeoff factor between the regularization and slack variables ξ φ is our feature functions xi corresponds to our ith image and yi is our structured label for the ith image we use the stochastic subgradient descent method to solve this minimization evaluation we emphasize quantitative evaluation in our work our vocabulary of meaning is significantly larger than the equivalent in evaluation requires innovation both in datasets and in measurement described below dataset we need a dataset with images and corresponding sentences and also labels for our representations of the meaning space no such dataset exists we build our own dataset of images and sentences around the pascal images this means we can use and compare to state of the art models and image annotations in pascal dataset pascal sentence data set to generate the sentences we started with the pascal development kit we randomly selected images belonging to each of the categories once we had a set of images we used amazon mechanical turk to generate five captions for each image we required the an notators to be based in the us and that they pass a qualification exam testing their ability to identify spelling errors grammatical errors and descriptive cap tions more details about the methods of collection can be found in our dataset has sentences for each image of the thousand images resulting in sentences we also manually add labels for triplets of objects actions scenes for each images these triplets label the main object in the image the main action and the main place there are different triplets in our train set and in test set there are triplets in the test set that appeared in the train set the dataset is available at inference our model is learned to maximize the sum of the scores along the path identi fied by a triplet in inference we search for the triplet which gives us the best additive score argmaxywt φ xi y these models prefer triplets with combina tion of strong and poor responses over all mediocre responses we conjecture that a multiplicative inference model would result in better predictions as the multiplicative model prefers all the responses to be reasonably good our mul tiplicative inference has the form of argmaxy wt φ xi y we select the best triplet given the potentials on the nodes and edges greedily by relaxing an edge and solving for the best path and re scoring the results using the relaxed edge matching once we predict triplets for images and sentences we can score a match between an image and a sentence if an image and a sentence predict very similar triplets they should be projections of nearby points in the meaning space and so they should have a high matching score a natural score of the similarity of sentence triplets and image triples is the sum of ranks of sentence meaning and image meaning the pair with smallest value of this sum is both strongly predicted by the image and strongly predicted by the sentence however this score is likely to be noisy and is difficult to compute because we must touch all pairs of meanings we use a good noise resistant approximation to obtain the score we obtain the top k ranking triplets derived from sentences and compute the rank of each as an image triplet obtain the top k ranking triplets derived from images and compute the rank of each as a sentence triplet sum the sum of ranks for each of these sets weighted by in the inverse rank of the triplet so as to emphasize triplets that score strongly out of vocabulary extension we generate sentences by searching a pool of sentences for one that has a good match score to the image we cannot learn a detector classifier for each ob ject action scene that exists this means we need to score the similarity between the image and sentences that contain unfamiliar words we propose using text information to attack this problem for each unknown object we can produce a score of the similarity of that object with all of the objects in our vocabu lary using distributional semantics methods explained in section we do the same thing for verbs and scenes as well these similarity measures work as a crude guide to our model for example in figure we don t have a detector for volkswagen herd woman and cattle but we can recognize them our similarity measures provides a similarity distributions over things we know this similarity distribution helps us to recognize objects actions and scenes for which we have no detector classifier using objects actions scenes we know experimental settings we divide our images to training images and testing images we use nearest neighbors in building potentials for images and sentences for matching we use closest triplets mapping to the meaning space table compares the results of mapping the images to the meaning space pre dicting triplets for images to do that we need a measure of comparisons between pairs of triplets the one that we predict and the ground truth triplets one way of doing this is by simple comparisons of triplets a prediction is correct if all three elements agree and wrong otherwise we could also measure if any of the el ements in the triplet match each score is insensitive to important aspects of loss for example predicting cat sit mat when ground truth is dog sit ground is not as bad as predicting bike ride street this implies that the penalty for confusing cats with dogs should be smaller than that for confusing cats with bikes the same argument holds for actions and scenes as well we also need our measure to take into account the amount of information a prediction conveys for example predicting object do scene is less favorable than cat sit mat tree measure tree measure we need a measure that reflects two important interacting components accuracy and specificity we believe the right way to score error is to use taxonomy trees we have taxonomy trees for objects actions and scenes and we can use them to measure the accuracy relevance and specificity of predictions we introduce a novel measure tree which reflects how accurate and specific the prediction is given a taxonomy tree for say objects objects we represent each prediction by the path from the root of the taxonomy tree to the predicted node for example if the prediction is cat we represent it as objects animal cat we can then report the standard measure using the precision and recall precision is defined as the total number of edges on the path that matches the edges on the ground truth path divided by the total number of edges on the ground truth path and recall as the total number of edges on the predicted path which is in the ground truth path divided by the total number of edges in the path for example the measure for predicting dog when the ground truth is cat is where the precision is and recall is the measure for predicting animal when the ground truth is cat is and it is for predicting bike when the ground truth is cat the same procedure is applied to actions and scenes the tree measure for a triple is the mean of the three measures for objects actions and scenes table shows tree measures for several different experimental settings blue measure similar to machine translation approaches where reports of accuracy involves scores for the correctness of the translation and the correctness of the generated translation in terms of language and logic we also consider another measure to check if the triplet we generate is logically valid or not analogous to the bleu score in machine translation literature we introduce the blue score which measures this for example bottle walk street is not valid for that we check if the triplet ever appeared in our corpus or not table shows these scores for the triplets predicted by several different experimental settings results to evaluate our method we provide qualitative and quantitative results there are two stages in our model first we show the ability of our method to map table evaluation of mapping from the image space to the meaning space obj means when we only consider the potentials on the object node and use uniform poten tials for other nodes and edges no edge means assuming a uniform potential over edges fw a stands for fixed weights with additive inference model this is the case where we use all the potentials but we don t learn any weights for them sl a means using structure learning with additive inference model fw m is similar to fw a with the exception that the inference model is multiplicative instead of additive sl m is the structure learning with multiplicative inference from the image space to the meaning space we then evaluate our results on predicting sentences for images annotation we also show qualitative results for finding images for sentences illustration mapping images to meanings table compares several different experimental settings in terms of two measures explained above tree and blue each column in table corresponds to an experimental setting we report average tree and average blue measures for five top triplets for all images we also breakdown the tree to objects actions and scenes in bottom three rows of the table annotation generating sentences from images figure shows top predicted triplets and top generated sentences for example images in our test set quantitative evaluation of generated sentence is very challenging we trained individuals to annotate generated sentences we ask them to annotate each generated sentence by either or means that the sentence is quite accurate with possible little mistakes about details in the sentence implies that the sentence have a rough idea about the image but it not very accurate and means that the sentence is not even remotely close to the image we generate sentences for each image the total average of the scores given by these individuals is the average number of sentences with score one per image is 48 the average number of sentences with score per image is of images have at least one sentence with score 354 sentences out of images have at least one sentence with score 2 illustration finding images best described by sentences not only our model can provide sentences that describe an image but it also can find images which are best described by a given sentence once the connections to the meaning space is established one could go in both directions from images to sentences or the other way around figure shows examples of finding images for sentences for more qualitative results please see the supplementary material fig generating sentences for images we show top five predicted triplets in the middle column and top five predicted sentences in the right column out of vocabulary extension figure depicts examples of the cases where we could successfully recognize ob jects actions for which we have no detector classifier this is very interesting as the intermediate meaning space allows us to benefit from distributional seman tics this means that we can learn to recognize unknown objects actions scenes by looking at the patterns of responses from other similar known detector classifiers discussion and future work sentences are rich compact and subtle representations of information even so we can predict good sentences for images that people like the intermediate meaning representation is one key component in our model as it allows benefiting from distributional semantics our sentence model is oversimplified we think an iterative procedure for going deeper in sentences and images would be the right direction once a sentence is generated for an image it is much easier to check for adjectives and adverbs aknowledgements this work was supported in part by the national science foundation under iis and in part by the office of naval research under 01 as part of the muri program in part by a gift from google any opinions findings and conclu sions or recommendations expressed in this material are those of the author and do a two girls in the store yellow train on the tracks a small herd of animals with a calf in the grass a horse being ridden within a fenced area fig finding images for sentences once the matching in the meaning space is es tablished we can generate sentences for images annotation and also find images that can be best describe by a sentence in this picture we show four sentences with four highest ranked images we provide a list of highest score images for each sentence for the test set in the supplementary material fig examples of failures in generating sentences for images not necessarily reflect those of the national science foundation or the office of naval research ali farhadi was supported by the google phd fellowship we also would like to thank majid ashtiani for his help on cluster computing and hadi kiapour attiye hosseini for their help on evaluation from images to sentences from sentences to images a red london united double decker bus drives down a city street a very colorful volkswagen beetle two young women with two little girl near them cattle feeding at a trough fig out of vocabulary extension we don t have detectors for drives women volkswagen and cattle despite this fact we could recognize these ob jects actions distributional semantics provide us with the ability to model unknown objects actions categories with their similarities to known categories here we show examples of sentences and images when we could recognize these unknowns for both generating sentences from images and finding images for sentences video google a text retrieval approach to object matching in videos josef sivic and andrew zisserman robotics research group department of engineering science university of oxford united kingdom abstract we describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video the object is represented by a set of viewpoint invariant region descriptors so that recog nition can proceed successfully despite changes in view point illumination and partial occlusion the temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors the analogy with text retrieval is in the implementation where matches on descriptors are pre computed using vec tor quantization and inverted file systems and document rankings are used the result is that retrieval is immediate returning a ranked list of key frames shots in the manner of google the method is illustrated for matching on two full length feature films introduction the aim of this work is to retrieve those key frames and shots of a video containing a particular object with the ease speed and accuracy with which google retrieves text docu ments web pages containing particular words this paper investigates whether a text retrieval approach can be suc cessfully employed for object recognition identifying an identical object in a database of images is now reaching some maturity it is still a challenging prob lem because an object visual appearance may be very dif ferent due to viewpoint and lighting and it may be partially occluded but successful methods now exist typically an object is represented by a set of overlapping regions each represented by a vector computed from the region appear ance the region segmentation and descriptors are built with a controlled degree of invariance to viewpoint and illu mination conditions similar descriptors are computed for all images in the database recognition of a particular ob ject proceeds by nearest neighbour matching of the descrip tor vectors followed by disambiguating using local spa tial coherence such as neighbourhoods ordering or spatial layout or global relationships such as epipolar geometry examples include 16 we explore whether this type of approach to recognition can be recast as text retrieval in essence this requires a visual analogy of a word and here we provide this by vector quantizing the descriptor vectors however it will be seen that pursuing the analogy with text retrieval is more than a simple optimization over different vector quantizations there are many lessons and rules of thumb that have been learnt and developed in the text retrieval literature and it is worth ascertaining if these also can be employed in visual retrieval the benefits of this approach is that matches are effec tively pre computed so that at run time frames and shots containing any particular object can be retrieved with no delay this means that any object occurring in the video and conjunctions of objects can be retrieved even though there was no explicit interest in these objects when de scriptors were built for the video however we must also determine whether this vector quantized retrieval misses any matches that would have been obtained if the former method of nearest neighbour matching had been used review of text retrieval text retrieval systems generally employ a number of standard steps the documents are first parsed into words second the words are repre sented by their stems for example walk walking and walks would be represented by the stem walk third a stop list is used to reject very common words such as the and an which occur in most documents and are therefore not discriminating for a particular document the remain ing words are then assigned a unique identifier and each document is represented by a vector with components given by the frequency of occurrence of the words the document contains in addition the components are weighted in vari ous ways described in more detail in section and in the case of google the weighting of a web page depends on the number of web pages linking to that particular page all of the above steps are carried out in advance of actual re trieval and the set of vectors representing all the documents in a corpus are organized as an inverted file to facilitate efficient retrieval an inverted file is structured like an ideal book index it has an entry for each word in the corpus fol lowed by a list of all the documents and position in that document in which the word occurs a text is retrieved by computing its vector of word frequencies and returning the documents with the closest measured by angles vectors in addition the match on the ordering and separation of the words may be used to rank the returned documents paper outline here we explore visual analogies of each of these steps section describes the visual descriptors used section then describes their vector quantization into visual words and section weighting and indexing for the vector model these ideas are then evaluated on a ground truth set of frames in section finally a stop list and ranking by a match on spatial layout are introduced in section and used to evaluate object retrieval throughout two feature films run lola run lola rennt tykwer and groundhog day ramis although previous work has borrowed ideas from the text retrieval literature for image retrieval from databases e g used the weighting and inverted file schemes to the best of our knowledge this is the first systematic appli cation of these ideas to object retrieval in videos viewpoint invariant description two types of viewpoint covariant regions are computed for each frame the first is constructed by elliptical shape adap tation about an interest point the method involves itera tively determining the ellipse centre scale and shape the scale is determined by the local extremum across scale of a laplacian and the shape by maximizing intensity gradient isotropy over the elliptical region the implementa tion details are given in this region type is referred to as shape adapted sa the second type of region is constructed by selecting ar eas from an intensity watershed image segmentation the regions are those for which the area is approximately sta tionary as the intensity threshold is varied the implemen tation details are given in this region type is referred to as maximally stable ms two types of regions are employed because they detect different image areas and thus provide complementary rep resentations of a frame the sa regions tend to be centered on corner like features and the ms regions correspond to blobs of high contrast with respect to their surroundings such as a dark window on a gray wall both types of re gions are represented by ellipses these are computed at twice the originally detected region size in order for the im age appearance to be more discriminating for a pixel video frame the number of regions computed is typi cally an example is shown in figure each elliptical affine invariant region is represented by a dimensional vector using the sift descriptor devel figure top row two frames showing the same scene from very different camera viewpoints from the film run lola run middle row frames with detected affine invariant regions super imposed maximally stable ms regions are in yellow shape adapted sa regions are in cyan bottom row final matched regions after indexing and spatial consensus note that the corre spondences define the scene overlap between the two frames oped by lowe in this descriptor was shown to be su perior to others used in the literature such as the response of a set of steerable filters or orthogonal filters and we have also found sift to be superior by comparing scene retrieval results against ground truth as in section the reason for this superior performance is that sift unlike the other descriptors is designed to be invariant to a shift of a few pixels in the region position and this localization er ror is one that often occurs combining the sift descriptor with affine covariant regions gives region description vec tors which are invariant to affine transformations of the im age note both region detection and the description is com puted on monochrome versions of the frames colour infor mation is not currently used in this work to reduce noise and reject unstable regions information is aggregated over a sequence of frames the regions de tected in each frame of the video are tracked using a simple constant velocity dynamical model and correlation any re gion which does not survive for more than three frames is rejected each region of the track can be regarded as an independent measurement of a common scene region the pre image of the detected region and the estimate of the descriptor for this scene region is computed by averaging the descriptors throughout the track this gives a measur able improvement in the signal to noise of the descriptors which again has been demonstrated using the ground truth tests of section building a visual vocabulary the objective here is to vector quantize the descriptors into clusters which will be the visual words for text retrieval then when a new frame of the movie is observed each de scriptor of the frame is assigned to the nearest cluster and this immediately generates matches for all frames through out the movie the vocabulary is constructed from a sub part of the movie and its matching accuracy and expressive power are evaluated on the remainder of the movie as de scribed in the following sections the vector quantization is carried out here by k means clustering though other methods k medoids histogram binning etc are certainly possible implementation regions are tracked through contiguous frames and a mean vector descriptor x i computed for each of the i regions to reject unstable regions the of tracks with the largest diagonal covariance matrix are rejected this generates an average of about regions per frame each descriptor is a vector and to simultaneously cluster all the descriptors of the movie would be a gargan tuan task instead a subset of shots is selected these shots are discussed in more detail in section cover ing about frames which represent about of all the frames in the movie even with this reduction there are still averaged track descriptors that must be clustered to determine the distance function for clustering the ma halanobis distance is computed as follows it is assumed that the covariance is the same for all tracks and this is computed by estimating from all the available data i e all descriptors for all tracks in the shots the maha lanobis distance enables the more noisy components of the vector to be weighted down and also decorrelates the components empirically there is a small degree of correla tion the distance function between two descriptors repre sented by their mean track descriptors x x is then given a b figure samples from the clusters corresponding to a single vi sual word a two examples of clusters of shape adapted regions b two examples of clusters of maximally stable regions of each type the number of clusters is chosen empirically to maximize retrieval results on the ground truth set of sec tion the k means algorithm is run several times with random initial assignments of points as cluster centres and the best result used figure shows examples of regions belonging to par ticular clusters i e which will be treated as the same vi sual word the clustered regions reflect the properties of the sift descriptors which penalize variations amongst re gions less than cross correlation this is because sift em phasizes orientation of gradients rather than the position of a particular intensity within the region the reason that sa and ms regions are clustered sepa rately is that they cover different and largely independent regions of the scene consequently they may be thought of as different vocabularies for describing the same scene and thus should have their own word sets in the same way as one vocabulary might describe architectural features and another the state of repair of a building visual indexing using text retrieval methods by d x x x x x x as is standard the descriptor space is affine transformed by the square root of so that euclidean distance may be used about clusters are used for shape adapted regions and about clusters for maximally stable regions the ratio of the number of clusters for each type is chosen to be approximately the same as the ratio of detected descriptors in text retrieval each document is represented by a vector of word frequencies however it is usual to apply a weighting to the components of this vector rather than use the fre quency vector directly for indexing here we describe the standard weighting that is employed and then the visual analogy of document retrieval to frame retrieval the standard weighting is known as term frequency inverse document frequency tf idf and is computed as follows suppose there is a vocabulary of k words then each document is represented by a k vector vd ti tk of weighted word frequencies with com rank of the ith relevant image in essence rank is zero if all nrel images are returned first the rank measure lies in the range to with corresponding to random retrieval ground truth image set results ponents ti nid log n figure shows the average normalized rank using each image of the data set as a query image with the tf idf weight nd ni where nid is the number of occurrences of word i in doc ument d nd is the total number of words in the document d ni is the number of occurrences of term i in the whole database and n is the number of documents in the whole database the weighting is a product of two terms the word frequency nid nd and the inverse document frequency log n ni the intuition is that word frequency weights words occurring often in a particular document and thus de scribe it well whilst the inverse document frequency down weights words that appear often in the database at the retrieval stage documents are ranked by their nor malized scalar product cosine of angle between the query vector vq and all document vectors vd in the database in our case the query vector is given by the visual words contained in a user specified sub part of a frame and the other frames are ranked according to the similarity of their weighted vectors to this query vector various weighting models are evaluated in the following section experimental evaluation of scene matching using visual words here the objective is to match scene locations within a closed world of shots the method is evaluated on frames from shots taken at different locations in the movie run lola run we have between frames from each location examples of three frames from each of four different locations are shown in figure there are signif icant viewpoint changes over the triplets of frames shown for the same location each frame of the triplet is from a different and distant in time shot in the movie in the retrieval tests the entire frame is used as a query region the retrieval performance is measured over all frames using each in turn as a query region the correct re trieval consists of all the other frames which show the same location and this ground truth is determined by hand for the complete frame set the retrieval performance is measured using the average normalized rank of relevant images given by ing described in section the benefit in having two feature types is evident the combination of both clearly gives bet ter performance than either one alone the performance of each feature type varies for different frames or locations for example in frames ms regions perform better and conversely for frames sa regions are superior the retrieval ranking is perfect for of the locations even those with significant viewpoint changes the ranking results are less impressive for images and though even in these cases the frame matches are not missed just low ranked this is due to a lack of regions in the over lapping part of the scene see figure this is not a problem of vector quantization the regions that are in common are correctly matched but due to few features being detected for this type of scene pavement texture we return to this point in section table shows the mean of the rank measure computed from all images for three standard text retrieval term weighting methods the tf idf weighting outperforms both the binary weights i e the vector components are one if the image contains the descriptor zero otherwise and term frequency weights the components are the frequency of word occurrence the differences are not very signifi cant for the ranks averaged over the whole ground truth set however for particular frames e g the difference can be as high as the average precision recall curve for all frames is shown in figure for each frame as a query we have computed precision as the number of relevant images i e of the same location relative to the total number of frames retrieved and recall as the number of correctly retrieved frames relative to the number of relevant frames again the benefit of combining the two feature types is clear these retrieval results demonstrate that there is no loss of performance in using vector quantization visual words compared to direct nearest neighbour or nearest neigh bour matching of invariants this ground truth set is also used to learn the system pa rameters including the number of cluster centres the mini mum tracking length for stable features and the proportion rank nnrel nrel ri i nrel nrel 2 of unstable descriptors to reject based on their covariance object retrieval where nrel is the number of relevant images for particular query image n is the size of the image set and ri is the in this section we evaluate searching for objects throughout the entire movie the object of interest is specified by the table the mean of the rank measure computed from all images of the ground truth set for different term weighting meth ods a average normalized rank of relevant frames shape adapted maximally stable shape adapted maximally stable 2 figure top frames and from the ground truth data set a poor ranking score is obtained for this pair bottom superimposed detected affine invariant regions the careful reader will note that due to the very different viewpoints only two of the left and right regions correspond between frames 05 70 94 image number b average precision recall curve user as a sub part of any frame a feature length film typically has frames to reduce complexity one keyframe is used per second of the video descriptors are computed for stable regions in each keyframe and the mean values are computed using two frames either side of the keyframe the descriptors are vec tor quantized using the centres clustered from the ground truth set here we are also evaluating the expressiveness of the vi sual vocabulary since frames outside the ground truth set contain new objects and scenes and their detected regions have not been included in forming the clusters 2 2 0 0 0 0 recall c figure ground truth data a each row shows a frame from three different shots of the same location in the ground truth data set b average normalized rank for location matching on the ground truth set c average precision recall curve for location matching on the ground truth set stop list using a stop list analogy the most frequent visual words that occur in almost all images are suppressed figure shows the frequency of visual words over all the keyframes of lola the top and bottom are stopped in our case the very common words are due to large clusters of over points the stop list boundaries were determined empirically to reduce the number of mismatches and size of the inverted file while keeping sufficient visual vocabulary figures show the benefit of imposing a stop list the very common visual words occur at many places in the im age and are responsible for mis matches most of these are removed once the stop list is applied the removal of the remaining mis matches is described next spatial consistency 500 frequency of visual words over all keyframes 200 frequency of visual words over all keyframes google increases the ranking for documents where the 0 word rank sorted by frequency 0 1000 6000 word rank sorted by frequency searched for words appear close together in the retrieved texts measured by word order this analogy is especially relevant for querying objects by a subpart of the image where matched covariant regions in the retrieved frames should have a similar spatial arrangement e g compactness to those of the outlined region in the query image the idea is implemented here by first retrieving frames using the weighted frequency vector alone and then re ranking them based on a measure of spatial consistency spatial consistency can be measured quite loosely sim ply by requiring that neighbouring matches in the query re gion lie in a surrounding area in the retrieved frame it can also be measured very strictly by requiring that neighbour ing matches have the same spatial layout in the query re gion and retrieved frame in our case the matched regions provide the affine transformation between the query and re trieved image so a point to point map is available for this strict measure we have found that the best performance is obtained in the middle of this possible range of measures a search area is defined by the nearest neighbours of each match and each region which also matches within this area casts a vote for that frame matches with no support are rejected the total number of votes determines the rank of the frame this works very well as is demonstrated in the last row of figure which shows the spatial consistency rejection of in correct matches the object retrieval examples of figures to employ this ranking measure and amply demonstrate its usefulness other measures which take account of the affine map ping between images may be required in some situations but this involves a greater computational expense object retrieval implementation use of inverted files in a classical file structure all words are stored in the document they appear in an inverted file structure has an entry hit list for each word where all occurrences of the word in all documents are stored in our case the inverted file has an entry for each visual word which stores all the matches i e occurrences of the same word in all frames the document vector is very sparse and use of an inverted file makes the retrieval very fast querying a database of frames takes about 0 second with a matlab implementation on a pentium b figure frequency of ms visual words among all keyframes of run lola run a before and b after application of a stoplist figure matching stages top row left query region and right its close up second row original word matches third row matches after using stop list last row final set of matches after filtering on spatial consistency example queries figures and show results of two object queries for the movie run lola run and figure shows the result of an object query on the film ground hog day both movies contain about keyframes both the actual frames returned and their ranking are excellent as far as it is possible to tell no frames containing the ob ject are missed no false negatives and the highly ranked frames all do contain the object good precision the object query results do demonstrate the expressive power of the visual vocabulary the visual words learnt for lola are used unchanged for the groundhog day retrieval summary and conclusions the analogy with text retrieval really has demonstrated its worth we have immediate run time object retrieval throughout a movie database despite significant viewpoint changes in many frames the object is specified as a sub part of an image and this has proved sufficient for quasi planar rigid objects there are of course improvements that can be made mainly to overcome problems in the visual processing low rankings are currently due to a lack of visual descriptors for some scene types however the framework allows other ex isting affine co variant regions to be added they will define an extended visual vocabulary for example those of another improvement would be to define the object of in terest over more than a single frame to allow for search on all its visual aspects the text retrieval analogy also raises interesting ques tions for future work in text retrieval systems the tex tual vocabulary is not static growing as new documents are added to the collection similarly we do not claim that our vector quantization is universal for all images so far we have learnt vector quantizations sufficient for two movies but ways of upgrading the visual vocabulary will need to be found one could think of learning visual vocabularies for different scene types e g city scape vs a forest finally we now have the intriguing possibility of follow ing other successes of the text retrieval community such as latent semantic indexing to find content and automatic clus tering to find the principal objects that occur throughout the movie acknowledgements we are grateful to david lowe jiri matas krystian mikolajczyk and frederik schaffalitzky for sup plying their region detector descriptor codes thanks to andrew blake mark everingham andrew fitzgibbon krystian mikola jczyk and frederik schaffalitzky for fruitful discussions this work was funded by ec project vibes figure object query example i first row left frame with user specified query region a poster in yellow and right close up of the query region the four remaining rows show left the and retrieved frames with the identified re gion of interest shown in yellow and right a close up of the im age with matched elliptical regions superimposed in this case keyframes were retrieved six from the same shot as the query image the rest from different shots at later points in the movie all retrieved frames contain the specified object note the poster appears on various billboards throughout the movie and berlin figure object query example ii run lola run first row left query region and right its close up next rows the and retrieved frames left and object close ups right with matched regions keyframes were retrieved contained the object the two incorrect frames were ranked and figure object query example iii groundhog day first row left query region and right its close up next rows the and retrieved frames left and object close ups with matched regions right keyframes were retrieved of which contained the object the first incorrect frame was ranked style aware mid level representation for discovering visual connections in space and time yong jae lee alexei a efros and martial hebert robotics institute carnegie mellon university efros hebert cs cmu edu abstract we present a weakly supervised visual data mining ap proach that discovers connections between recurring mid level visual elements in historic temporal and geographic spatial image collections and attempts to capture the un derlying visual style in contrast to existing discovery meth ods that mine for patterns that remain visually consistent throughout the dataset our goal is to discover visual ele ments whose appearance changes due to change in time or location i e exhibit consistent stylistic variations across the label space date or geo location to discover these elements we ﬁrst identify groups of patches that are style sensitive we then incrementally build correspondences to ﬁnd the same element across the entire dataset finally we train style aware regressors that model each element range of stylistic differences we apply our approach to date and geo location prediction and show substantial improve ment over several baselines that do not model visual style we also demonstrate the method effectiveness on the re lated task of ﬁne grained classiﬁcation introduction learn how to see realize that everything connects to everything else leonardo da vinci long before the age of data mining historians geog raphers anthropologists and paleontologists have been dis covering and analyzing patterns in data one of their main motivations is ﬁnding patterns that correlate with spatial geographical and or temporal historical information al lowing them to address two crucial questions where geo localization and when historical dating interestingly many such patterns be it the shape of the handle on an etruscan vase or the pattern of bark of a norwegian pine are predominantly visual the recent explosion in the sheer volume of visual information that humanity has been cap turing poses both a challenge it impossible to go through by hand and an opportunity discovering things that would never have been noticed before for these ﬁelds in this now with the eecs department at uc berkeley figure given historic car images our algorithm is not only able to automatically discover corresponding visual elements e g yel low green boxes despite the large visual variations but can model these variations to capture the changes in visual style across time work we take the ﬁrst steps in considering temporally as well as spatially varying visual data and developing a method for automatically discovering visual patterns that correlate with time and space of course ﬁnding recurring visual patterns in data un derlies much of modern computer vision itself it is what connects the disparate fragments of our visual world into a coherent narrative at the low level this is typically done via simple unsupervised clustering e g k means in visual words but clustering visual patterns that are more complex than simple blobs corners and oriented bars turns out to be rather difﬁcult because everything be comes more dissimilar in higher dimensions the emerg ing subﬁeld of visual category discovery visual data min ing proposes ways to address this issue most such approaches look for tight clumps in the data discovering visual patterns that stay globally con sistent throughout the dataset more recent discriminative methods such as take advantage of weak supervi sion to divide the dataset into discrete subsets e g kitchen vs bathroom paris vs not paris to discover speciﬁc visual patterns that repeatedly occur in one subset while not occurring in others but in addition to the globally consistent visual patterns e g the pepsi logo is exactly the same all over the world and the speciﬁc ones e g toilets are only found in bath rooms much in our visual world is neither global nor spe ciﬁc but rather undergoes a gradual visual change this is nowhere more evident than in the visual changes across large extents of space geography and time history con sider the three cars shown in figure one antique one classic and one from the although these cars are quite different visually they clearly share some common elements e g a headlight or a wheel but notice that even these common elements differ substantially in their ap pearance across the three car types making this a very chal lenging correspondence problem notice further that the way in which they differ is not merely random i e a statis tical noise term rather these subtle yet consistent dif ferences curvy vs boxy hood the length of the ledge under the door etc tend to reﬂect the particular visual style that is both speciﬁc to an era yet changing gradually over time figure if now we were given a photo of a different car and asked to estimate its model year we would not only need to detect the common visual elements on the new car but also understand what its stylistic differences e g the length of that ledge tell us about its age in this paper we propose a method for discovering connections between similar mid level visual elements in temporally and spatially varying datasets and modeling their visual style here we deﬁne visual style as appear ance variations of the same visual element due to change in time or location our central idea is to create reli able generic visual element detectors that ﬁre across the entire dataset independent of style and then model their style speciﬁc differences using weakly supervised image la bels date geo location etc the reason for doing the ﬁrst step is that each generic detector puts all of its detections into correspondence lower right in figure creating a closed world focused on one visual theme where it is much easier to subtract away the commonalities and fo cus on the stylistic differences furthermore without con ditioning on the generic detector it would be very difﬁcult to even detect the stylistically informative features for in stance the ledge in figure green box is so tiny that it is unlikely to be detectable in isolation but in combination with the wheel and part of the door the generic part it becomes highly discriminable we evaluate our method on the task of date and geo location prediction in three scenarios two historic car datasets with model year annotations and a street view im agery dataset annotated with gps coordinates we show that our method outperforms several baselines which do not explicitly model visual style moreover we also demon strate how our approach can be applied to the related task of ﬁne grained recognition of birds related work modeling space and time geo tagged datasets have been used for geo localization on the local re gional and planetary scales but we are not aware of any prior work on improving geo location by explicitly capturing stylistic differences between geo informative vi sual elements but see for anecdotal evidence of such possibility longitudinal i e long term temporal visual modeling has received relatively little attention most previ ous research has been on the special case of age estimation for faces see for a survey recent work includes mod eling the temporal evolution of web image collections and dating of historical color photographs we are not aware of any prior work on modeling historical visual style visual data mining existing visual data mining object discovery approaches have been used to discover object categories mid level patches attributes and low level foreground features typically an appropriate similarity measure is deﬁned be tween visual patterns i e images patches or contours and those that are most similar are grouped into discovered entities of these methods mid level discriminative patch mining shares the most algorithmic similarities with our work we also represent our visual elements with hog patches and reﬁne the clusters through discriminative cross validation training however unlike and all existing discovery methods we go beyond simply detect ing recurring visual elements and model the stylistic differ ences among the common discovered elements visual style analysis the seminal paper on style content separation uses bilinear models to factor out the style and content components in pre segmented pre aligned vi sual data e g images of letters in different fonts while we also use the term style to describe the differences between corresponding visual elements we are solving a rather different problem our aim is to automatically dis cover recurring visual elements despite their differences in visual style and then model those differences while our generic detectors could perhaps be thought of as captur ing content independent of style we do not explicitly factor out the style but model it conditioned on the content fine grained categorization can also be viewed as a form of style analysis as subtle differences within the same basic level category differentiate one subordinate category from another existing approaches use human labeled at tributes and keypoint annotations or template matching because these methods are focused on classiﬁcation they limit themselves to the simpler vi sual world of manually annotated object bounding boxes whereas our method operates on full images furthermore discovering one to one correspondences is given a pri mary role in our method whereas in most ﬁne grained ap peaky low entropy clusters b uniform high entropy clusters figure mining style sensitive visual elements clusters are considered style sensitive if they have peaky low entropy distribution across time a and style insensitive if their instances are distributed more uniformly b notice how the high entropy distributions b represent not only style insensitivity e g nondescript side of car but also visually noisy clusters both are disregarded by our method proaches the correspondences are already provided while template matching methods also try to discover cor respondences unlike our approach they do not explicitly model the style speciﬁc differences within each correspon dence set finally these approaches have not been applied to problems with continuous labels regression where cap turing the range of styles is particularly important lastly relative attributes model how objects scenes relate to one another via ordered pairs of labels a is fur rier than b we also share the idea of relating things however instead of using strong supervision to deﬁne these relationships we automatically mine for visual patterns that exhibit such behavior approach our goal is to discover and connect mid level visual el ements across temporally and spatially varying image col lections and model their style speciﬁc differences we as sume that the image collections are weakly supervised with date or location labels there are three main steps to our approach first as ini tialization we mine for style sensitive image patch clus ters that is groups of visually similar patches with similar labels date or location then for each initial cluster we try to generalize it by training a generic detector that com putes correspondences across the entire image collection to ﬁnd the same visual element independent of style finally for each set of correspondences we train a style aware re gression model that learns to differentiate the subtle stylistic differences between different instances of the same generic element in the following sections we describe each of the steps in turn we will use an image collection of historic cars as our running example but note that there is nothing speciﬁc to cars in our algorithm mining style sensitive visual elements most recurring visual patterns in our data will be ex tremely boring sky asphalt etc they will also not ex hibit any stylistic variation over time or space and not be of any use in historical dating or geo localization after all asphalt is always just asphalt even some parts of the car e g a window do not really change much over the decades on the other hand we would expect the shape of the hood between two cars to be more similar than between a and a car therefore our ﬁrst task is to mine for visual elements whose appearance somehow correlates with its labels i e date or location we call vi sual elements that exhibit this behavior style sensitive since we do not know a priori the correct scale loca tion and spatial extent of the style sensitive elements we randomly sample patches across various scales and loca tions from each image in the dataset following we represent each patch with a histogram of gradients hog descriptor and ﬁnd its top n nearest neighbor patches in the database using normalized correlation by matching it to each image in a sliding window fashion over multiple scales and locations to ensure that redundant overlapping patches are not chosen more than once for each matching image we only take its best matching patch each sampled patch and its n nearest neighbors ideally form a cluster of a recurring visual element although many clusters will be very noisy due to inadequacies of simple hog matching to identify the style sensitive clusters we can analyze the temporal distribution of labels for each cluster instances intuitively a cluster that has a tightly grouped peaky label distribution suggests a visual ele ment that prefers a particular time period and is thus style sensitive while a cluster that has a uniform label distribu tion suggests a pattern that doesn t change over time as extra bonus most noisy clusters will also have a uniform distribution since it is very unlikely to be style sensitive by random chance to measure the style sensitivity of clus ter c we histogram its labels and compute its entropy n histogram count for bin i and n denotes the number of quan tized label bins we normalize the histogram to sum to we then sort the clusters in ascending order of entropy fig ure a and b show examples of the highest and low est ranked clusters for the car dataset images notice how the highest ranked clusters correspond to style sensitive car elements while the lowest ranked clusters contain noisy or style insensitive ones we take the top m clusters as figure to account for a visual element variation in style over space or time we incrementally revise its detector by augmenting the positive training set with the top detections ﬁred only on images with nearby labels this produces an accurate generic detector that is invariant to the visual element changes in style singh et al 1950s 1990s our approach figure establishing correspondences across time a correspondences made using the discriminative patch mining approach using a positive set of frontal cars note how the correspondences break down a third of the way through b starting with the same initial set of frontal cars our algorithm gradually expands the positive set over the continuous label space until it is able to connect the same visual element across the entire temporal extent of the dataset our discovered style sensitive visual elements after reject ing near duplicate clusters measured by spatial overlap of more than between any of their cluster members establishing correspondences each of the top m clusters corresponds to a style sensitive visual element in a local region of the label space a few of these elements represent very speciﬁc visual fea tures that just do not occur in other parts of the data e g car tailﬁns from but most others have similar counter parts in other time periods and our goal is to connect them together which will allow us to model the change in style of the same visual element over the entire label space for instance one of the style sensitive elements could represent frontal cars from we want to ﬁnd corresponding frontal car patches across all time periods the same visual element however can look quite dif ferent across the label space especially over larger tempo ral extents figure to obtain accurate correspondences across all style variations we propose to train a discrimi native detector using an iterative procedure that exploits the continuous nature of the label space in general we expect the appearance of a visual element to change gradually as a function of its label our key idea is to initialize the detector using a style sensitive cluster as the initial positive training data but then incrementally revise it by augmenting the pos itive set with detections ﬁred only on images with nearby labels e g decades as shown in figure speciﬁcally we ﬁrst train a linear svm detector with the cluster patches as positives and patches sampled from thou sands of random flickr images as negatives these nega tives will make the detector discriminative against generic patterns occurring in the natural world which helps it to ﬁre accurately on unseen images we then incremen tally revise the detector at each step we run the current detector on a new subset of the data that covers a slightly broader range in label space and retrain it by augmenting the positive training set with the top detections we repeat this process until all labels have been accounted for mak ing these transitive connections produces a ﬁnal generic de tector that ﬁres accurately across the entire label space as shown in figure b note that automatic discovery of transitive visual correspondences across a dataset is very much in the spirit of the visual memex opening up several promising future directions for investigation there is an important issue that we must address to en sure that the detector is robust to noise the initial cluster can contain irrelevant outlier patches since some of the top n nearest neighbors of the query patch could be bad matches to prune out the noisy instances at each step of the incremental revision of our detector we apply cross validation training speciﬁcally we create multiple partitions of the training set and iteratively reﬁne the cur rent detector by training on one partition testing on another taking the resulting top detections as the new training instances and repeating steps until conver gence i e the top detections do not change effectively at each iteration the detector learns to boost the common patterns shared across the top detections and down weights their discrepancies without over ﬁtting which leads to more car database cardb internet movie car database imcdb east coast database edb figure each cardb and imcdb image is labeled with the car model year each edb image is labeled with its gps coordinate accurate detections in the next iteration note that a direct application of will not work for our case of continuous style varying data because the variability can be too great figure a shows detections made by a detector trained with using the same ini tial style sensitive cluster of cars as positives the detector produces accurate matches in nearby decades but the correspondence breaks down across larger temporal ex tents because it fails to model the variation in style finally we ﬁre each trained generic detector on all im ages and take the top detection per image and with svm score greater than to obtain the ﬁnal correspondences training style aware regression models the result of the previous step is a set of generic mid level detectors each tuned to a particular visual element and able to produce a set of corresponding instances under many different styles now we are ﬁnally ready to model that variation in style and because the correspondences are so good we can now forget about the larger dataset and focus entirely on each set of corresponding instances in iso lation making our modeling problem much simpler the ﬁnal step is to train a style aware regressor for each element that models its stylistic variation over the label space it is safe to assume that style will not change linearly over the label space e g with cars it is possible that stylis tic elements from one decade could be reintroduced as vin tage in a later decade to account for this we train a standard non linear support vector regressor svr with an e insensitive loss function using ground truth weakly supervised image labels e g date geo location as the target score we use gaussian kernels k xi xj exp γ xi xj where γ is the mean of the pair wise distances among all instances and xi is the hog fea ture for instance i under this kernel instances with similar appearance are most likely to have similar regression out puts furthermore to handle possible mis detections made by the generic detector which could add noise we weight each instance proportional to its detection score when train ing the svr we map a detection score to a weight in via a logistic function exp each re sulting model captures the stylistic differences of the same visual element found by the generic detector results in this section we evaluate our method ability to pre dict date location compared to several baselines provide in depth comparisons to the discriminative patch mining ap proach of show qualitative examples of discovered correspondences and learned styles and 4 apply our ap proach to ﬁne grained recognition of birds datasets we use three datasets car database cardb photos of cars made in crawled from www cardatabase net internet movie car database im cdb movie images of cars made in crawled from www imcdb org and east coast database edb 4 455 google street view images along the eastern coasts of georgia south carolina and north carolina ex ample images are shown in figure cardb and imcdb images are labeled with the model year of the main car in the image and edb images are labeled with their gps co ordinates these are the style labels and the only super visory information we use for edb since our svrs ex pect outputs although a multivariate regression method could also be used we project the images gps coor dinates to using pca this works because the area of interest is roughly linear i e long and narrow see figure c these datasets exhibit a number of challenges includ ing clutter occlusion scale location and viewpoint change and large appearance variations of the objects importantly unlike standard object recognition datasets ours have con tinuous labels we partition the cardb and edb datasets into train test sets with splits we evaluate on all datasets and focus additional analysis on cardb since it has the largest number of images image level date location prediction to evaluate on a label prediction task we need to combine all of our visual element predictors together we train an image level predic tion model using as features the outputs of each style aware regressor on an image speciﬁcally we represent an image i with feature φ i which is the concatenation of the max imum svm detection scores of the generic detectors over the image and the svr scores of their corresponding style aware regressors when testing on edb we aggregate the features in spatial bins via a spatial pyramid since we expect there to be spatial consistency of visual patterns across images we use these features to train an image level cardb date prediction error ours singh et al sp bow edb geo location prediction error ours sp singh et al bow table mean absolute error on cardb edb and imcdb for all methods the result on imcdb evaluates cross dataset general ization performance lower values are better figure box plots showing date and location prediction error on the cardb and edb datasets respectively lower values are bet ter our approach models the subtle stylistic differences for each discovered element in the data which leads to lower error rates gaussian svr this model essentially selects the most use ful style aware regressors for predicting style given the en tire image to ensure that the image level model does not overﬁt we train it on a separate validation set baselines for date location prediction we compare to three baselines bag of words bow spatial pyramid sp and singh et al for the ﬁrst two we de tect dense sift features compute a global visual word dic tionary on the full dataset and then train an intersection kernel svr using the date location labels for singh et al which mines discriminative patches but does not model their change in style we adapt the approach to train date location speciﬁc patch detectors using the initial style sensitive clusters discovered in sec speciﬁcally we take each speciﬁc cluster instances as positives and all patches from the remaining training images that do not share the same labels with a small don t care region in between as negatives now just like in the previous para graph we concatenate the max output of the detectors as features to train an image level gaussian svr we opti mize all baselines parameters by cross validation implementation details we sample pixel patches over an image pyramid at scales i e min max patch is pixels wide in original image and represent them with a hog descriptor for edb patches we augment hog with a tiny image in lab colorspace when training the style aware svrs we set n n and m for cardb and edb respectively for our generic svm detectors we ﬁx csvm and cover of the label space at each training step cardb years edb miles for our svrs we ﬁx e and set csvr and for cardb and edb respectively tuned using cross validation on the training set date and location prediction accuracy we ﬁrst evaluate our method ability to predict the cor rect date geo location of the images in cardb edb fig ure and table 1 top rows show the absolute error rates for all methods this metric is computed by taking the absolute difference between the ground truth and predicted labels our approach outperforms all baselines on both datasets the baselines have no mechanism to explicitly model stylistic differences as they are either mining discrimi nate patches over a subregion in label space singh et al or using quantized local features bow and sp that re sult in loss of ﬁne detail necessary to model subtle stylis tic changes without explicitly making connections over space time the baselines appear to have difﬁculty telling apart signal from noise in particular we show substan tial improvement on cardb because cars exhibit more pro nounced stylistic differences across eras that require accu rate modeling the stylistic differences in architecture and vegetation for edb are much more subtle this makes sense since the geographic region of interest only spans about miles along the u s east coast still our method is able to capture more of the stylistic differences to pro duce better results note that chance performance is around years and miles for test set all results are averaged over random trials the proposed multi level active selection yields the steepest learning curves random selection lags behind wasting annotation effort on less informative exam ples single level active is preferable to random selection yet we get best results when our active learner can choose between multiple types of annotations including segmen tations or image flags the total gains after are sig nificant given the complexity of the way classification problem with a test set containing image regions note that the random selection curve is probably an over estimate of its quality since we limit the unlabeled pool to only im ages from the msrc any example it requests is going to be fairly informative figure right shows results for the same setting when random images are added to the unla beled pool indicating that when uninformative images are present random selection lags even further behind active selection with a learned cost function finally we show the impact of using the predicted cost while mak ing active choices we train a binary multi instance classi fier for each category using image labels on th of the data per class in different runs the rest is used for testing we compare two mil active learners one using cost pre diction and one assigning a flat cost to annotations at test time both learners are charged the ground truth cost of getting the requested annotation figure shows representative good and bad learning curves with accuracy measured by the auroc value for tree and airplane using the predicted cost leads to much better accuracies at a lower cost whereas for sky there is little difference this may be because most sky regions look similar and take similar amounts of time to annotate figure right shows the cost required to improve the base classifier to different levels of accuracy the col umn shows the relative time savings our cost prediction en ables over a cost blind active learner that uses the same parameter rl should reflect the real cost of a classification mistake we set it to 50 since an error made by the automatic labeling would take around 50 to manually fix for the average image 2 1 active results for class tree active results for class aeroplane 1 05 active results for class sky with cost prediction with cost prediction 1 with cost prediction without cost without cost without cost 0 manual cost secs 0 200 manual cost secs 0 200 manual cost secs figure representative learning curves left and a summary over all classes right when using active selection with the learned cost predictor as compared to a baseline that makes active selections using a flat cost value rightmost savings in cost when using cost prediction within the active learner cp refers to using cost prediction and nc is without cost overall our active selection takes less effort to attain the same level of accuracy as a cost blind active learner selection strategy for larger improvements predicting the cost leads to noticeably greater savings in manual effort over savings to attain a accuracy improvement with our implementation of the incremental svm tech nique of 2 it takes on average 0 5 secs to evaluate a sin gle region and 20 secs to evaluate a bag image on a 1 6 ghz pc we are currently considering ways to alleviate the computational cost however even without real time per formance a distributed framework for image labeling that involves multiple annotators could be run efficiently conclusions we proposed an active learning framework that not only chooses examples based on their information content but also on the predicted cost of obtaining the information our framework operates in the challenging real world domain of multi label images with multiple possible types of anno tations our results demonstrate that 1 the active learner obtains accurate models with much less manual effort than typical passive learners 2 we can fairly reliably estimate how much a putative annotation will cost given the image content alone and our multi label multi level strategy outperforms conventional active methods that are restricted to requesting a single type of annotation acknowledgements many thanks to alex sorokin for helping us arrange the mechanical turk data collection this research was supported in part by nsf career microsoft research texas higher education co ordinating board award 01 40 darpa vi rat nsf eia and the henry luce foundation learning to predict where humans look tilke judd krista ehinger fre do durand antonio torralba mit computer science artificial intelligence laboratory and mit brain and cognitive sciences abstract for many applications in graphics design and human computer interaction it is essential to understand where humans look in a scene where eye tracking devices are not a viable option models of saliency can be used to pre dict fixation locations most saliency approaches are based on bottom up computation that does not consider top down image semantics and often does not match actual eye move ments to address this problem we collected eye tracking data of viewers on images and use this database as training and testing examples to learn a model of saliency based on low middle and high level image features this large database of eye tracking data is publicly available with this paper introduction for many applications in graphics design and human computer interaction it is essential to understand where hu mans look in a scene for example an understanding of vi sual attention is useful for automatic image cropping thumbnailing or image search it can be used to direct foveated image and video compression and levels of detail in non photorealistic rendering it can also be used in advertising design adaptive image display on small devices or seam carving some of these applications have been demonstrated by incorporating eye tracking into the process a user sits in front of a computer with an eye tracker that records the user fixations and feeds the data into the method how ever eye tracking is not always an option eye trackers are expensive and interactive techniques are a burden when pro cessing lots of data therefore it is necessary to have a way to predict where users will look without the eye track ing hardware as an alternative models of saliency have been used to measure the conspicuity of a location or the likelihood of a location to attract the attention of human ob servers most models of saliency are biologically figure eye tracking data we collected eye tracking data on images from viewers to use as ground truth data to train a model of saliency using machine learning gaze tracking paths and fixation locations are recorded for each viewer b a continuous saliency map c is found by convolving a gaussian over the fixation locations of all users this saliency map can be thresholded to show the most salient percent of the image d inspired and based on a bottom up computational model typically multiple low level visual features such as inten sity color orientation texture and motion are extracted from the image at multiple scales after a saliency map is computed for each of the features they are normalized and combined in a linear or non linear fashion into a mas ter saliency map that represents the saliency of each pixel sometimes specific locations are identified through a com bination of winner take all and inhibition of return opera tions though the models do well qualitatively the models have limited use because they frequently do not match ac tual human saccades from eye tracking data as in fig and finding a closer match depends on tuning many design parameters figure current saliency models do not accurately predict human fixations in row one the low level model selects bright spots of light as salient while viewers look at the human in row two the low level model selects the building strong edges and windows as salient while viewers fixate on the text we make two contributions in this paper the first is a large database of eye tracking experiments with la bels and analysis and the second is a supervised learning model of saliency which combines both bottom up image based saliency cues and top down image semantic depen dent cues our database consists of eye tracking data from different users across images to our knowledge it is the first time such an extensive collection of eye tracking data is available for quantitative analysis for a given im age the eye tracking data is used to create a ground truth saliency map which represents where viewers actually look fig we propose a set of low mid and high level im age features used to define salient locations and use a linear support vector machine to train a model of saliency we compare the performance of saliency models created with different features and show how combining all features pro duces the highest performing model as a demonstration that our model can be used for graphics applications we show the decarlo and santella abstracted nonphotoreal istic rendering technique adapted to use our saliency model instead of eye tracking input other researchers have also made some headway on im proving low level saliency models bruce and tsotsos present a model for visual saliency built on a first prin ciples information theoretic formulation dubbed attention based on information maximization aim which performs marginally better than the itti model avraham and lin denbaum work on esaliency uses a stochastic model to estimate the most probable targets mathematically the main difference between these works and ours is that their models are derived mathematically and not trained directly from a large database of eye tracking data cerf et al improve upon the itti model by adding face detection to the model in addition to adding face detection we add sev eral other higher level features which provide us with an increased performance over both the itti and cerf models our work is most closely related to the work of kien zle et al who also learn a model of saliency directly from human eye movement data their model consists of a nonlinear mapping from a normalized image patch to a real value trained to yield positive outputs on fixated patches and negative outputs on randomly selected image patches in contrast to our work they only used low level features furthermore their training set comprises only grayscale natural scene images in the specific situation of trying to predict where peo ple look in a pedestrian search task ehinger et al show that a model of search guidance combining three sources low level saliency target features and scene context out performs models based on any of these single sources our work focuses on predicting saliency in a free viewing con text and creates a model with a larger set of image features database of eye tracking data we collected a large database of eye tracking data to al low large scale quantitative analysis of fixation points and gaze paths and to provide ground truth data for saliency model research the images eye tracking data and accom panying code in matlab are all available on the web to fa cilitate research in perception and saliency across the vision and graphics community data gathering protocol we collected random images from flickr creative commons and labelme fig and recorded eye track ing data from fifteen users who free viewed these images the longest dimension of each image was pixels and the other dimension ranged from to with the ma jority at pixels there were landscape images and portrait images the users were males and females between the ages of and two of the viewers were researchers on the project and the others were naive view ers all viewers sat at a distance of approximately two feet from a inch computer screen of resolution in a dark room and used a chin rest to stabilize their head an eye tracker recorded their gaze path on a separate computer as they viewed each image at full resolution for seconds separated by second of viewing a gray screen to ensure high quality tracking results we checked camera calibra tion every images we divided the viewing into two ses sions of randomly ordered images each session was done on average at one week apart we provided a mem figure images a sample of the images that we collected from flickr and labelme though they were shown at original res olution and aspect ratio in the experiment they have been resized for viewing here ory test at the end of both viewings to motivate users to pay attention to the images we showed them images and asked them to indicate which ones they had seen before we discarded the first fixation from each scanpath to avoid adding trivial information from the initial center fixation in order to obtain a continuous saliency map of an im age from the eye tracking data of a user we convolve a gaussian filter across the user fixation locations similar to the landscape map of we also generate a saliency map of the average locations fixated by all viewers we can choose to threshold this continuous saliency map to get a bi nary map of the top n percent salient locations of the image fig analysis of dataset for some images all viewers fixate on the same loca tions while in other images viewers fixations are dispersed all over the image we analyze this consistency of human fixations over an image by measuring the entropy of the av erage continuous saliency map across viewers though the original images were of varying aspect rations we resized them to pixel images before calculating entropy figure shows a histogram of the entropies of the images in our database it also shows a sample of saliency maps with lowest and highest entropy and their corresponding im ages our data indicates a strong bias for human fixations to be near the center of the image as is consistent with previously analyzed eye tracking datasets figure shows the average human saliency map from all images of fixations lie within the center of the image of fixations lie within the center of the image this bias has often been attributed to the setup of the experiment where users are placed centrally in front of the screen and to the fact that human photographers tend to place objects of interest in the center of photographs we use an roc metric to evaluate the performance of human saliency maps to predict eye fixations using this method the saliency map from the fixation locations of one figure analysis of fixation locations the first two rows show examples of saliency maps made from human fixations with low and high entropy and their corresponding images images with high consistency low entropy tend to have one central object while images with low consistency high entropy are often images with several different textures bottom left is a histogram of the saliency map entropies bottom right is a plot of all the saliency maps from human eye fixations indicating a strong bias to the center of the image and of fixations lie within the indicated rectangles user is treated as a binary classifier on every pixel in the im age saliency maps are thresholded such that a given per cent of the image pixels are classified as fixated and the rest are classified as not fixated the human fixations from the other humans are used as ground truth by varying the threshold the roc curve is drawn and the area under the curve indicates how well the saliency map from one user can predict the ground truth fixations figure shows the average roc curve over all users and all images note that human performance is remarkably good of the ground truth human fixations are within the top salient areas of a novel viewer s saliency map and percent are within the top percent salient locations as stated before the fixations in the database have a strong bias towards the center because of this we find that simply using a gaussian blob centered in the middle of the image as the saliency map produces excellent results figure in this roc curve human performance is very high demonstrating that the locations where a human looks are very indicative of where other humans have looked the gaussian cen ter model performs much better than chance because of the strong bias of the fixations in the database towards the center as noted for other datasets as well by we plot the roc curve for the center gaussian on figure in order to analyze fixations on specific objects and im age features we hand labeled our image dataset for each image we labeled bounding boxes around any faces and text and indicated a line for the horizon if present us ing these labeled bounding boxes we calculated that of fixations are on faces fig though we did not label all people we noticed that many fixations landed on people including representations of people like drawings or sculp tures even if their faces were not visible in addition of fixations are on text this may be because signs are in nately designed to be salient for example a stop sign or a store sign are created specifically to draw attention we use these ground truth labels to study fixation prediction perfor mance on faces and as a ground truth for face and horizon detection we also qualitatively found that fixations from our database are often on animals cars and human body parts like eyes and hands these objects reflect both a no tion of what humans are attracted to and what objects are in our dataset by analyzing images with faces we noticed that viewers fixate on faces when they are within a certain size of the image but fixate of parts of the face eyes nose lips when presented with a close up of a face fig this suggests that there is a certain size for a region of interest roi that a person fixates on to get a quick sense of the size of rois we drew a rough bounding box around clustered fixations on images figure shows the histogram of the radii of the resulting rois investigating this concept is an interesting area of future work figure objects of interest in our database viewers frequently fixated on faces people and text other fixations were on body parts such as eyes and hands cars and animals we found the above image areas by selecting bounding boxes around connected areas of salient pixels on an image overlayed with its salient mask figure size of regions of interest in many images viewers fix ate on human faces however when viewing the close up of a face they look at specific parts of a face rather than the face as a whole suggesting a constrained area of the region of interest on the right is a histogram of the radii of the regions of interest in pixels learning a model of saliency in contrast to previous computational models that com bine a set of biologically plausible filters together to esti mate visual saliency we use a learning approach to train a classifier directly from human eye tracking data features used for machine learning the following are the low mid and high level features that we were motivated to work with after analyzing our dataset for each image we precomputed the features for every pixel of the image resized to and used these to train our model figure comparison of saliency maps each row of images compares the predictors of our svm saliency model the itti saliency map the center prior and the human ground truth all thresholded to show the top percent salient locations figure features a sample image bottom right and of the features that we use to train the model these include subband features itti and koch saliency channels distance to the center color features and automatic horizon face person and car detec tors the labels for our training on this image are based on a thresholded saliency map derived from human fixations to the left of bottom right low level features because they are physiologically plau sible and have been shown to correlate with visual at tention we use the local energy of the steerable pyra mid filters as features we currently find the pyra mid subbands in four orientations and three scales see fig first images we also include features used in a simple saliency model described by torralba and rosenholtz based on subband pyramids fig bottom left intensity orientation and color contrast have long been seen as important features for bottom up saliency we include the three channels corresponding to these im age features as calculated by itti and koch s saliency method we include the values of the red green and blue chan nels as well as the probabilities of each of these chan nels as features fig images to and the prob ability of each color as computed from color his tograms of the image filtered with a median filter at different scales fig images to mid level features because most objects rest on the sur face of the earth the horizon is a place humans natu rally look for salient objects we train a horizon line detector from mid level gist features high level features because we found that humans fixated so consistently on people and faces we run the viola jones face detector and the felzenszwalb person detector and include these as features to our model center prior when humans take pictures they naturally frame an object of interest near the center of the image for this reason we include a feature which indicates the distance to the center for each pixel training in order to train and test our model we divided our set of images into training images and testing images from each image we chose positively labeled pixels ran domly from the top salient locations of the human ground truth saliency map and negatively labeled pix els from the bottom salient locations to yield a training set of samples and testing set of samples we found that increasing the number of samples chosen per im age above did not increase performance it is probable that after a certain number of samples per image new sam ples only provide redundant information we chose samples from the top and bottom in order to have sam ples that were strongly positive and strongly negative we avoided samples on the boundary between the two we did not choose any samples within pixels of the boundary of the image our tests on models trained using ratios of negative to positive samples ranging from to showed no change in the resulting roc curve so we chose to use a ratio of we normalized the features of our training set to have zero mean and unit variance and used the same normaliza tion parameters to normalize our test data we used the liblinear support vector machine to train a model on the positive and negative training sam ples we used models with linear kernels because we found from experimentation that they performed as well as mod els with radial basis function kernels and models found with multiple kernel learning for our specific task linear models are also faster to compute and the resulting weights of features are easier to understand we set the misclassifi cation cost c at we found that performance was the same for c to c 000 and decreased when smaller than performance we measure performance of saliency models in two ways first we measure performance of each model by its roc curve second we examine the performance of differ ent models on specific subsets of samples samples inside and outside a central area of the image and on faces performance on testing images in figure we see a roc curve describing the performance of different saliency models averaged over all testing images for each image we predict the saliency per pixel using a specific trained model instead of using the predicted labels indicated by the sign of wt x b where w and b are learned parameters and x refers to the feature vector we use the value of wt x b as a continuous saliency map which indicates how salient each pixel is then we threshold this saliency map at n and percent of the image for binary saliency maps which are typically relevant for applications for each binary map we find the percentage of human fix ations within the salient areas of the map as the measure of performance notice that as the percentage of the image considered salient goes to the predictability or per centage of human fixations within the salient locations also goes to we make the following observations from the roc curves the model with all features combined outper forms models trained on single sets of features and models trained on competing saliency features from torralba and rozenholtz itti and koch and cerf et al note that we im plement the cerf et al method by training an svm on itti features and face detection alone we learn the best weights for the linear combination of features instead of using equal weights as they do the model with all features reaches of the way to human performance for example when images are thresholded at salient our model performs at while humans are at the model with all features except the distance to the center performs as well as the model based on the distance to the center this is quite good considering this model does not leverage any of the information about location and thus does not at all benefit from the huge bias of fixations toward the center the model trained on all features except the center per forms much better than any of the models trained on single sets of features for example at the salient location threshold the torralba based model performs at while the all in without center model performs at for a 20 jump in performance though object detectors may be figure the roc curve of performances for svms trained on each set of features individually and combined together we also plot human performance and chance for comparison very good at locating salient objects when those objects are present in an image it is not good at locating other salient locations when the objects are not present thus the over all performance for the object detector model is low and these features should be used only in conjunction with other features all models perform significantly better than chance indicating that each of the features individually do have some power to predict salient locations we measure which features add most to the model by calculating the delta improvement between the center model and the center model with a given set of features we ob serve that subband features and torralba s features which use subband features add the greatest improvement af ter that is color features horizon detection face and object detectors and itti channels performance on testing samples to understand the im pact of the bias towards the center of the dataset for some models we divided each image into a circular central and a peripheral region the central region was defined by the model based only on the feature which gave the distance of the example to the center in this model any sample farther than units away from the center where the distance from the center to the corner is was labeled negative and anything closer was labeled positive this is equivalent to the center of the image given this threshold we di figure here we show the average rate of true positives and true negatives for svms trained with different feature sets on different subsets of samples this value is equivalent to the performance of the model if there were an equal number of positive and negative samples in each subset vided the samples to those inside and outside the center in addition we chose to look at samples that landed on faces since viewers were particularly attracted by them in figure we plot performance of the model for dif ferent subsets of samples the performance here is defined as the average of the true positive and true negative rates this is equivalent to the performance of the model if there were an equal number of positive and negative samples in each subset we make the following observations about the trained models from this measure of performance 1 even though center model performs well over all the samples both sam ples inside and outside the center it performs only as well as chance for the other subsets of samples while over all samples the performance of the center model and the all features without center model perform the same the later model performs more robustly over all subsets of samples 3 understandably the model trained on features from ob ject detectors for faces people and cars performs better on the subsets with faces the svms using the center prior feature and the one using all features perform very well on positive and negative random testing points but are outperformed both in the inside and outside region this paradox stems from the fact that of the salient testing points are in the inside region whereas of the non salient testing points are in the outside one can show that this biased distribution provides a lift in performance figure stylization and abstraction of photographs decarlo and santella use eye tracking data to decide how to render a photograph with differing levels of detail we replicate this appli cation without the need for eye tracking hardware for methods that would either have a high true negative rate outside or a high true positive rate inside such as the center prior discussion this eye tracking database allows us to quan tify how consistent human fixations are across an image in general the fixation locations of several humans is strongly indicative of where a new viewer will look so far computer generated models have not matched humans ability to pre dict fixation locations though we feel we have moved a step closer in that direction by using a model that combines low mid and high level features qualitatively we learned that when free viewing images humans consistently look at some common objects they look at text other people and specifically faces if not peo ple they look at other living animals and specifically their faces in the absence of specific objects or text humans tend towards the center of the image or locations where low level features are salient as text face person and other object detectors get better models of saliency which include object detectors will also get better though all these trends are not surprising we are excited that this database will allow us to measure the trends quantitatively applications a good saliency model enables many applications that automatically take into account a notion of human percep tion where humans look and what they are interested in as an example we use our model in conjunction with the tech nique of decarlo and santella to automatically create a non photorealistic rendering of a photograph with different levels of detail fig they render more details at the locations users fixated on and less detail in the rest of the image while they require information from an eye track ing device in order to tailor the level of detail we use our saliency model to predict locations where people look conclusion in this work we make the following contributions we develop a collection of eye tracking data from people across images and have made it public for research use this is the largest eye tracking database of natural im ages that we are aware of and permits large scale quanti tative analysis of fixations p o ints a n d g aze p a ths w e use machine learning to train a bottom up top down model of saliency based on low mid and high level image features we demonstrate that our model outperforms several exist ing models and the center prior finally we show an exam ple of how our model can be used in practice for graphics applications for future work we are interested in understanding the impact of framing cropping and scaling images on fixa tions we believe that the same image cropped at different sizes will lead viewers to fixate o n d ifferent o bjects i n the image and should be more carefully examined preview interest in digital image processing methods stems from two principal applica tion areas improvement of pictorial information for human interpretation and processing of image data for storage transmission and representation for au tonomous machine perception this chapter has several objectives to define the scope of the field that we call image processing to give a historical per spective of the origins of this field to give you an idea of the state of the art in image processing by examining some of the principal areas in which it is ap plied to discuss briefly the principal approaches used in digital image pro cessing to give an overview of the components contained in a typical general purpose image processing system and to provide direction to the books and other literature where image processing work normally is reported what is digital image processing an image may be defined as a two dimensional function f x y where x and y are spatial plane coordinates and the amplitude of f at any pair of coordi nates x y is called the intensity or gray level of the image at that point when x y and the intensity values of f are all finite discrete quantities we call the image a digital image the field of digital image processing refers to processing digital images by means of a digital computer note that a digital image is com posed of a finite number of elements each of which has a particular location and value these elements are called picture elements image elements pels and pixels pixel is the term used most widely to denote the elements of a digital image we consider these definitions in more formal terms in chapter vision is the most advanced of our senses so it is not surprising that images play the single most important role in human perception however unlike hu mans who are limited to the visual band of the electromagnetic em spec trum imaging machines cover almost the entire em spectrum ranging from gamma to radio waves they can operate on images generated by sources that humans are not accustomed to associating with images these include ultra sound electron microscopy and computer generated images thus digital image processing encompasses a wide and varied field of applications there is no general agreement among authors regarding where image processing stops and other related areas such as image analysis and comput er vision start sometimes a distinction is made by defining image processing as a discipline in which both the input and output of a process are images we believe this to be a limiting and somewhat artificial boundary for example under this definition even the trivial task of computing the average intensity of an image which yields a single number would not be considered an image processing operation on the other hand there are fields such as com puter vision whose ultimate goal is to use computers to emulate human vi sion including learning and being able to make inferences and take actions based on visual inputs this area itself is a branch of artificial intelligence ai whose objective is to emulate human intelligence the field of ai is in its earliest stages of infancy in terms of development with progress having been much slower than originally anticipated the area of image analysis also called image understanding is in between image processing and com puter vision there are no clear cut boundaries in the continuum from image processing at one end to computer vision at the other however one useful paradigm is to consider three types of computerized processes in this continuum low mid and high level processes low level processes involve primitive opera tions such as image preprocessing to reduce noise contrast enhancement and image sharpening a low level process is characterized by the fact that both its inputs and outputs are images mid level processing on images involves tasks such as segmentation partitioning an image into regions or objects de scription of those objects to reduce them to a form suitable for computer pro cessing and classification recognition of individual objects a mid level process is characterized by the fact that its inputs generally are images but its outputs are attributes extracted from those images e g edges contours and the identity of individual objects finally higher level processing involves making sense of an ensemble of recognized objects as in image analysis and at the far end of the continuum performing the cognitive functions normally associated with vision based on the preceding comments we see that a logical place of overlap be tween image processing and image analysis is the area of recognition of indi vidual regions or objects in an image thus what we call in this book digital image processing encompasses processes whose inputs and outputs are images and in addition encompasses processes that extract attributes from images up to and including the recognition of individual objects as an illustration to clar ify these concepts consider the area of automated analysis of text the processes of acquiring an image of the area containing the text preprocessing that image extracting segmenting the individual characters describing the characters in a form suitable for computer processing and recognizing those individual characters are in the scope of what we call digital image processing in this book making sense of the content of the page may be viewed as being in the domain of image analysis and even computer vision depending on the level of complexity implied by the statement making sense as will become evident shortly digital image processing as we have defined it is used successfully in a broad range of areas of exceptional social and economic value the concepts developed in the following chapters are the foundation for the methods used in those application areas the origins of digital image processing one of the first applications of digital images was in the newspaper indus try when pictures were first sent by submarine cable between london and new york introduction of the bartlane cable picture transmission system in the early reduced the time required to transport a picture across the atlantic from more than a week to less than three hours specialized printing equipment coded pictures for cable transmission and then recon structed them at the receiving end figure was transmitted in this way and reproduced on a telegraph printer fitted with typefaces simulating a halftone pattern some of the initial problems in improving the visual quality of these early digital pictures were related to the selection of printing procedures and the distribution of intensity levels the printing method used to obtain fig was abandoned toward the end of in favor of a technique based on photo graphic reproduction made from tapes perforated at the telegraph receiving terminal figure shows an image obtained using this method the improve ments over fig are evident both in tonal quality and in resolution references in the bibliography at the end of the book are listed in alphabetical order by authors last names figure a digital picture produced in from a coded tape by a telegraph printer with special type faces mcfarlane figure a digital picture made in from a tape punched after the signals had crossed the atlantic twice mcfarlane figure unretouched cable picture of generals pershing and foch transmitted in from london to new york by tone equipment mcfarlane the early bartlane systems were capable of coding images in five distinct levels of gray this capability was increased to levels in figure is typical of the type of images that could be obtained using the tone equip ment during this period introduction of a system for developing a film plate via light beams that were modulated by the coded picture tape improved the reproduction process considerably although the examples just cited involve digital images they are not con sidered digital image processing results in the context of our definition be cause computers were not involved in their creation thus the history of digital image processing is intimately tied to the development of the digital computer in fact digital images require so much storage and computational power that progress in the field of digital image processing has been depen dent on the development of digital computers and of supporting technologies that include data storage display and transmission the idea of a computer goes back to the invention of the abacus in asia minor more than years ago more recently there were developments in the past two centuries that are the foundation of what we call a computer today however the basis for what we call a modern digital computer dates back to only the with the introduction by john von neumann of two key con cepts a memory to hold a stored program and data and conditional branching these two ideas are the foundation of a central processing unit cpu which is at the heart of computers today starting with von neumann there were a series of key advances that led to computers powerful enough to be used for digital image processing briefly these advances may be summa rized as follows the invention of the transistor at bell laboratories in the development in the and of the high level programming lan guages cobol common business oriented language and fortran formula translator the invention of the integrated circuit ic at texas instruments in the development of operating systems in the early the development of the microprocessor a single chip consisting of the central processing unit memory and input and output controls by intel in the early introduction by ibm of the personal computer in and progressive miniaturization of components starting with large scale integra tion li in the late then very large scale integration vlsi in the to the present use of ultra large scale integration ulsi concurrent with these advances were developments in the areas of mass storage and display sys tems both of which are fundamental requirements for digital image processing the first computers powerful enough to carry out meaningful image pro cessing tasks appeared in the early the birth of what we call digital image processing today can be traced to the availability of those machines and to the onset of the space program during that period it took the combination of those two developments to bring into focus the potential of digital image processing concepts work on using computer techniques for improving im ages from a space probe began at the jet propulsion laboratory pasadena california in when pictures of the moon transmitted by ranger were processed by a computer to correct various types of image distortion inherent in the on board television camera figure shows the first image of the moon taken by ranger on july at a m eastern daylight time edt about minutes before impacting the lunar surface the markers called reseau marks are used for geometric corrections as discussed in chapter this also is the first image of the moon taken by a u s spacecraft the imaging lessons learned with ranger served as the basis for improved methods used to enhance and restore images from the surveyor missions to the moon the mariner series of flyby missions to mars the apollo manned flights to the moon and others figure the first picture of the moon by a u s spacecraft ranger took this image on july at a m edt about minutes before impacting the lunar surface courtesy of nasa in parallel with space applications digital image processing techniques began in the late and early to be used in medical imaging remote earth resources observations and astronomy the invention in the early of computerized axial tomography cat also called computerized tomogra phy ct for short is one of the most important events in the application of image processing in medical diagnosis computerized axial tomography is a process in which a ring of detectors encircles an object or patient and an x ray source concentric with the detector ring rotates about the object the x rays pass through the object and are collected at the opposite end by the corresponding detectors in the ring as the source rotates this procedure is re peated tomography consists of algorithms that use the sensed data to con struct an image that represents a slice through the object motion of the object in a direction perpendicular to the ring of detectors produces a set of such slices which constitute a three dimensional d rendition of the inside of the object tomography was invented independently by sir godfrey n hounsfield and professor allan m cormack who shared the nobel prize in medicine for their invention it is interesting to note that x rays were discovered in by wilhelm conrad roentgen for which he received the nobel prize for physics these two inventions nearly years apart led to some of the most important applications of image processing today from the until the present the field of image processing has grown vigorously in addition to applications in medicine and the space program dig ital image processing techniques now are used in a broad range of applica tions computer procedures are used to enhance the contrast or code the intensity levels into color for easier interpretation of x rays and other images used in industry medicine and the biological sciences geographers use the same or similar techniques to study pollution patterns from aerial and satellite imagery image enhancement and restoration procedures are used to process degraded images of unrecoverable objects or experimental results too expen sive to duplicate in archeology image processing methods have successfully restored blurred pictures that were the only available records of rare artifacts lost or damaged after being photographed in physics and related fields com puter techniques routinely enhance images of experiments in areas such as high energy plasmas and electron microscopy similarly successful applica tions of image processing concepts can be found in astronomy biology nuclear medicine law enforcement defense and industry these examples illustrate processing results intended for human interpreta tion the second major area of application of digital image processing tech niques mentioned at the beginning of this chapter is in solving problems dealing with machine perception in this case interest is on procedures for extracting from an image information in a form suitable for computer processing often this information bears little resemblance to visual features that humans use in interpreting the content of an image examples of the type of information used in machine perception are statistical moments fourier transform coefficients and multidimensional distance measures typical problems in machine percep tion that routinely utilize image processing techniques are automatic character recognition industrial machine vision for product assembly and inspection military recognizance automatic processing of fingerprints screening of x rays and blood samples and machine processing of aerial and satellite imagery for weather prediction and environmental assessment the continuing decline in the ratio of computer price to performance and the expansion of networking and communication bandwidth via the world wide web and the internet have cre ated unprecedented opportunities for continued growth of digital image pro cessing some of these application areas are illustrated in the following section examples of fields that use digital image processing today there is almost no area of technical endeavor that is not impacted in some way by digital image processing we can cover only a few of these appli cations in the context and space of the current discussion however limited as it is the material presented in this section will leave no doubt in your mind re garding the breadth and importance of digital image processing we show in this section numerous areas of application each of which routinely utilizes the digital image processing techniques developed in the following chapters many of the images shown in this section are used later in one or more of the exam ples given in the book all images shown are digital the areas of application of digital image processing are so varied that some form of organization is desirable in attempting to capture the breadth of this field one of the simplest ways to develop a basic understanding of the extent of image processing applications is to categorize images according to their source e g visual x ray and so on the principal energy source for images in use today is the electromagnetic energy spectrum other important sources of energy in clude acoustic ultrasonic and electronic in the form of electron beams used in electron microscopy synthetic images used for modeling and visualization are generated by computer in this section we discuss briefly how images are gener ated in these various categories and the areas in which they are applied methods for converting images into digital form are discussed in the next chapter images based on radiation from the em spectrum are the most familiar especially images in the x ray and visual bands of the spectrum electromag netic waves can be conceptualized as propagating sinusoidal waves of varying wavelengths or they can be thought of as a stream of massless particles each traveling in a wavelike pattern and moving at the speed of light each mass less particle contains a certain amount or bundle of energy each bundle of energy is called a photon if spectral bands are grouped according to energy per photon we obtain the spectrum shown in fig ranging from gamma rays highest energy at one end to radio waves lowest energy at the other energy of one photon electron volts 103 gamma rays x rays ultraviolet visible infrared microwaves radio waves figure the electromagnetic spectrum arranged according to energy per photon figure examples of gamma ray imaging a bone scan b pet image c cygnus loop d gamma radiation bright spot from a reactor valve images courtesy of a g e medical systems b dr michael e casey cti pet systems c nasa d professors zhong he and david k wehe university of michigan the bands are shown shaded to convey the fact that bands of the em spec trum are not distinct but rather transition smoothly from one to the other gamma ray imaging major uses of imaging based on gamma rays include nuclear medicine and as tronomical observations in nuclear medicine the approach is to inject a pa tient with a radioactive isotope that emits gamma rays as it decays images are produced from the emissions collected by gamma ray detectors figure a shows an image of a complete bone scan obtained by using gamma ray imaging images of this sort are used to locate sites of bone pathology such as infections or tumors figure b shows another major modality of nuclear imaging called positron emission tomography pet the principle is the same as with x ray tomography mentioned briefly in section however instead of using an external source of x ray energy the patient is given a radioactive isotope that emits positrons as it decays when a positron meets an electron both are annihilated and two gamma rays are given off these are detected and a tomo graphic image is created using the basic principles of tomography the image shown in fig b is one sample of a sequence that constitutes a d rendition of the patient this image shows a tumor in the brain and one in the lung easily visible as small white masses a star in the constellation of cygnus exploded about years ago gener ating a superheated stationary gas cloud known as the cygnus loop that glows in a spectacular array of colors figure c shows an image of the cygnus loop in the gamma ray band unlike the two examples in figs a and b this image was obtained using the natural radiation of the object being imaged finally fig d shows an image of gamma radiation from a valve in a nuclear reactor an area of strong radiation is seen in the lower left side of the image x ray imaging x rays are among the oldest sources of em radiation used for imaging the best known use of x rays is medical diagnostics but they also are used exten sively in industry and other areas like astronomy x rays for medical and in dustrial imaging are generated using an x ray tube which is a vacuum tube with a cathode and anode the cathode is heated causing free electrons to be released these electrons flow at high speed to the positively charged anode when the electrons strike a nucleus energy is released in the form of x ray radiation the energy penetrating power of x rays is controlled by a voltage applied across the anode and by a current applied to the filament in the cathode figure a shows a familiar chest x ray generated simply by plac ing the patient between an x ray source and a film sensitive to x ray energy the intensity of the x rays is modified by absorption as they pass through the patient and the resulting energy falling on the film develops it much in the same way that light develops photographic film in digital radiography digital images are obtained by one of two methods by digitizing x ray films or by having the x rays that pass through the patient fall directly onto devices such as a phosphor screen that convert x rays to light the light signal in turn is captured by a light sensitive digitizing system we discuss digitization in more detail in chapters and angiography is another major application in an area called contrast enhancement radiography this procedure is used to obtain images called angiograms of blood vessels a catheter a small flexible hollow tube is in serted for example into an artery or vein in the groin the catheter is threaded into the blood vessel and guided to the area to be studied when the catheter reaches the site under investigation an x ray contrast medium is injected through the tube this enhances contrast of the blood vessels and enables the radiologist to see any irregularities or blockages figure b shows an exam ple of an aortic angiogram the catheter can be seen being inserted into the figure examples of x ray imaging a chest x ray b aortic angiogram c head ct d circuit boards e cygnus loop images courtesy of a and c dr david r pickens dept of radiology radiological sciences vanderbilt university medical center b dr thomas r gest division of anatomical sciences university of michigan medical school d mr joseph e pascente lixi inc and e nasa large blood vessel on the lower left of the picture note the high contrast of the large vessel as the contrast medium flows up in the direction of the kidneys which are also visible in the image as discussed in chapter angiography is a major area of digital image processing where image subtraction is used to en hance further the blood vessels being studied another important use of x rays in medical imaging is computerized axial to mography cat due to their resolution and d capabilities cat scans revo lutionized medicine from the moment they first became available in the early as noted in section each cat image is a slice taken perpendicularly through the patient numerous slices are generated as the patient is moved in a longitudinal direction the ensemble of such images constitutes a d rendition of the inside of the body with the longitudinal resolution being proportional to the number of slice images taken figure c shows a typical head cat slice image techniques similar to the ones just discussed but generally involving higher energy x rays are applicable in industrial processes figure d shows an x ray image of an electronic circuit board such images representative of literally hun dreds of industrial applications of x rays are used to examine circuit boards for flaws in manufacturing such as missing components or broken traces industrial cat scans are useful when the parts can be penetrated by x rays such as in plastic assemblies and even large bodies like solid propellant rocket motors figure e shows an example of x ray imaging in astronomy this image is the cygnus loop of fig c but imaged this time in the x ray band imaging in the ultraviolet band applications of ultraviolet light are varied they include lithography industrial inspection microscopy lasers biological imaging and astronomical observations we illustrate imaging in this band with examples from microscopy and astronomy ultraviolet light is used in fluorescence microscopy one of the fastest grow ing areas of microscopy fluorescence is a phenomenon discovered in the mid dle of the nineteenth century when it was first observed that the mineral fluorspar fluoresces when ultraviolet light is directed upon it the ultraviolet light itself is not visible but when a photon of ultraviolet radiation collides with an electron in an atom of a fluorescent material it elevates the electron to a higher energy level subsequently the excited electron relaxes to a lower level and emits light in the form of a lower energy photon in the visible red light region the basic task of the fluorescence microscope is to use an excitation light to irradiate a prepared specimen and then to separate the much weaker radiating fluores cent light from the brighter excitation light thus only the emission light reaches the eye or other detector the resulting fluorescing areas shine against a dark background with sufficient contrast to permit detection the darker the back ground of the nonfluorescing material the more efficient the instrument fluorescence microscopy is an excellent method for studying materials that can be made to fluoresce either in their natural form primary fluorescence or when treated with chemicals capable of fluorescing secondary fluorescence figures a and b show results typical of the capability of fluorescence microscopy figure a shows a fluorescence microscope image of normal corn and fig b shows corn infected by smut a disease of cereals corn a b c figure examples of ultraviolet imaging a normal corn b smut corn c cygnus loop images courtesy of a and b dr michael w davidson florida state university c nasa grasses onions and sorghum that can be caused by any of more than species of parasitic fungi corn smut is particularly harmful because corn is one of the principal food sources in the world as another illustration fig c shows the cygnus loop imaged in the high energy region of the ultraviolet band imaging in the visible and infrared bands considering that the visual band of the electromagnetic spectrum is the most familiar in all our activities it is not surprising that imaging in this band out weighs by far all the others in terms of breadth of application the infrared band often is used in conjunction with visual imaging so we have grouped the visible and infrared bands in this section for the purpose of illustration we consider in the following discussion applications in light microscopy astrono my remote sensing industry and law enforcement figure shows several examples of images obtained with a light microscope the examples range from pharmaceuticals and microinspection to materials characterization even in microscopy alone the application areas are too numer ous to detail here it is not difficult to conceptualize the types of processes one might apply to these images ranging from enhancement to measurements a b c d e f figure examples of light microscopy images a taxol anticancer agent magnified b cholesterol c microprocessor d nickel oxide thin film e surface of audio cd f organic superconductor images courtesy of dr michael w davidson florida state university table thematic bands in nasa landsat satellite band no name wavelength µm characteristics and uses visible blue maximum water penetration visible green 52 60 good for measuring plant vigor visible red 69 vegetation discrimination near infrared biomass and shoreline mapping middle infrared 75 moisture content of soil and vegetation thermal infrared soil moisture thermal mapping middle infrared 08 35 mineral mapping another major area of visual processing is remote sensing which usually in cludes several bands in the visual and infrared regions of the spectrum table shows the so called thematic bands in nasa landsat satellite the primary function of landsat is to obtain and transmit images of the earth from space for purposes of monitoring environmental conditions on the planet the bands are expressed in terms of wavelength with µm being equal to m we dis cuss the wavelength regions of the electromagnetic spectrum in more detail in chapter note the characteristics and uses of each band in table in order to develop a basic appreciation for the power of this type of multispectral imaging consider fig which shows one image for each of 7 figure 10 landsat satellite images of the washington d c area the numbers refer to the thematic bands in table images courtesy of nasa the spectral bands in table the area imaged is washington d c which in cludes features such as buildings roads vegetation and a major river the po tomac going though the city images of population centers are used routinely over time to assess population growth and shift patterns pollution and other factors harmful to the environment the differences between visual and in frared image features are quite noticeable in these images observe for exam ple how well defined the river is from its surroundings in bands and weather observation and prediction also are major applications of multi spectral imaging from satellites for example fig is an image of hurricane katrina one of the most devastating storms in recent memory in the western hemisphere this image was taken by a national oceanographic and atmos pheric administration noaa satellite using sensors in the visible and in frared bands the eye of the hurricane is clearly visible in this image figures and show an application of infrared imaging these images are part of the nighttime lights of the world data set which provides a global inventory of human settlements the images were generated by the infrared imaging system mounted on a noaa dmsp defense meteorological satel lite program satellite the infrared imaging system operates in the band 10 0 to 4 µm and has the unique capability to observe faint sources of visible near infrared emissions present on the earth surface including cities towns villages gas flares and fires even without formal training in image processing it is not difficult to imagine writing a computer program that would use these im ages to estimate the percent of total electrical energy used by various regions of the world a major area of imaging in the visual spectrum is in automated visual in spection of manufactured goods figure shows some examples figure a is a controller board for a cd rom drive a typical image processing task with products like this is to inspect them for missing parts the black square on the top right quadrant of the image is an example of a missing component figure satellite image of hurricane katrina taken on august courtesy of noaa figure infrared satellite images of the americas the small gray map is provided for reference courtesy of noaa figure b is an imaged pill container the objective here is to have a ma chine look for missing pills figure c shows an application in which image processing is used to look for bottles that are not filled up to an acceptable level figure d shows a clear plastic part with an unacceptable number of air pockets in it detecting anomalies like these is a major theme of industrial inspection that includes other products such as wood and cloth figure e shows a batch of cereal during inspection for color and the presence of anom alies such as burned flakes finally fig f shows an image of an intraocular implant replacement lens for the human eye a structured light illumina tion technique was used to highlight for easier detection flat lens deformations toward the center of the lens the markings at o clock and o clock are tweezer damage most of the other small speckle detail is debris the objective in this type of inspection is to find damaged or incorrectly manufactured im plants automatically prior to packaging as a final illustration of image processing in the visual spectrum consider fig figure a shows a thumb print images of fingerprints are rou tinely processed by computer either to enhance them or to find features that aid in the automated search of a database for potential matches figure b shows an image of paper currency applications of digital image processing in this area include automated counting and in law enforcement the reading of the serial number for the purpose of tracking and identifying bills the two ve hicle images shown in figs c and d are examples of automated license plate reading the light rectangles indicate the area in which the imaging system figure infrared satellite images of the remaining populated part of the world the small gray map is provided for reference courtesy of noaa a b c d e f figure some examples of manufactured goods often checked using digital image processing a a circuit board controller b packaged pills c bottles d air bubbles in a clear plastic product e cereal f image of intraocular implant fig f courtesy of mr pete sites perceptics corporation detected the plate the black rectangles show the results of automated reading of the plate content by the system license plate and other applications of char acter recognition are used extensively for traffic monitoring and surveillance imaging in the microwave band the dominant application of imaging in the microwave band is radar the unique feature of imaging radar is its ability to collect data over virtually any region at any time regardless of weather or ambient lighting conditions some a b c d figure some additional examples of imaging in the visual spectrum a thumb print b paper currency c and d automated license plate reading figure a courtesy of the national institute of standards and technology figures c and d courtesy of dr juan herrera perceptics corporation radar waves can penetrate clouds and under certain conditions can also see through vegetation ice and dry sand in many cases radar is the only way to explore inaccessible regions of the earth surface an imaging radar works like a flash camera in that it provides its own illumination microwave pulses to illuminate an area on the ground and take a snapshot image instead of a camera lens a radar uses an antenna and digital computer processing to record its images in a radar image one can see only the microwave energy that was reflected back toward the radar antenna figure shows a spaceborne radar image covering a rugged mountain ous area of southeast tibet about km east of the city of lhasa in the lower right corner is a wide valley of the lhasa river which is populated by tibetan farmers and yak herders and includes the village of menba mountains in this area reach about m ft above sea level while the valley floors lie about m 000 ft above sea level note the clarity and detail of the image unencumbered by clouds or other atmospheric conditions that normally interfere with images in the visual band figure spaceborne radar image of mountains in southeast tibet courtesy of nasa imaging in the radio band as in the case of imaging at the other end of the spectrum gamma rays the major applications of imaging in the radio band are in medicine and astronomy in medicine radio waves are used in magnetic resonance imaging mri this technique places a patient in a powerful magnet and passes radio waves through his or her body in short pulses each pulse causes a responding pulse of radio waves to be emitted by the patient tissues the location from which these sig nals originate and their strength are determined by a computer which produces a two dimensional picture of a section of the patient mri can produce pictures in any plane figure shows mri images of a human knee and spine the last image to the right in fig shows an image of the crab pulsar in the radio band also shown for an interesting comparison are images of the same region but taken in most of the bands discussed earlier note that each image gives a totally different view of the pulsar 7 examples in which other imaging modalities are used although imaging in the electromagnetic spectrum is dominant by far there are a number of other imaging modalities that also are important specifically we discuss in this section acoustic imaging electron microscopy and synthetic computer generated imaging imaging using sound finds application in geological exploration industry and medicine geological applications use sound in the low end of the sound spectrum hundreds of hz while imaging in other areas use ultrasound mil lions of hz the most important commercial applications of image processing in geology are in mineral and oil exploration for image acquisition over land one of the main approaches is to use a large truck and a large flat steel plate the plate is pressed on the ground by the truck and the truck is vibrated through a frequency spectrum up to hz the strength and speed of the a b figure mri images of a human a knee and b spine image a courtesy of dr thomas r gest division of anatomical sciences university of michigan medical school and b courtesy of dr david r pickens department of radiology and radiological sciences vanderbilt university medical center returning sound waves are determined by the composition of the earth below the surface these are analyzed by computer and images are generated from the resulting analysis for marine acquisition the energy source consists usually of two air guns towed behind a ship returning sound waves are detected by hydrophones placed in cables that are either towed behind the ship laid on the bottom of the ocean or hung from buoys vertical cables the two air guns are alter nately pressurized to psi and then set off the constant motion of the ship provides a transversal direction of motion that together with the return ing sound waves is used to generate a d map of the composition of the earth below the bottom of the ocean figure shows a cross sectional image of a well known d model against which the performance of seismic imaging algorithms is tested the arrow points to a hydrocarbon oil and or gas trap this target is brighter than the surrounding layers because the change in density in the target region is gamma x ray optical infrared radio figure images of the crab pulsar in the center of each image covering the electromagnetic spectrum courtesy of nasa figure cross sectional image of a seismic model the arrow points to a hydrocarbon oil and or gas trap courtesy of dr curtis ober sandia national laboratories larger seismic interpreters look for these bright spots to find oil and gas the layers above also are bright but their brightness does not vary as strongly across the layers many seismic reconstruction algorithms have difficulty imag ing this target because of the faults above it although ultrasound imaging is used routinely in manufacturing the best known applications of this technique are in medicine especially in obstetrics where unborn babies are imaged to determine the health of their develop ment a byproduct of this examination is determining the sex of the baby ul trasound images are generated using the following basic procedure the ultrasound system a computer ultrasound probe consisting of a source and receiver and a display transmits high frequency to mhz sound pulses into the body the sound waves travel into the body and hit a boundary between tissues e g between fluid and soft tissue soft tissue and bone some of the sound waves are reflected back to the probe while some travel on further until they reach another boundary and get reflected the reflected waves are picked up by the probe and relayed to the com puter 4 the machine calculates the distance from the probe to the tissue or organ boundaries using the speed of sound in tissue m and the time of each echo return the system displays the distances and intensities of the echoes on the screen forming a two dimensional image in a typical ultrasound image millions of pulses and echoes are sent and re ceived each second the probe can be moved along the surface of the body and angled to obtain various views figure shows several examples we continue the discussion on imaging modalities with some examples of electron microscopy electron microscopes function as their optical counter parts except that they use a focused beam of electrons instead of light to image a specimen the operation of electron microscopes involves the follow ing basic steps a stream of electrons is produced by an electron source and ac celerated toward the specimen using a positive electrical potential this stream figure examples of ultrasound imaging a baby b another view of baby c thyroids d muscle layers showing lesion courtesy of siemens medical systems inc ultrasound group is confined and focused using metal apertures and magnetic lenses into a thin monochromatic beam this beam is focused onto the sample using a magnetic lens interactions occur inside the irradiated sample affecting the electron beam these interactions and effects are detected and transformed into an image much in the same way that light is reflected from or absorbed by ob jects in a scene these basic steps are carried out in all electron microscopes a transmission electron microscope tem works much like a slide projec tor a projector shines transmits a beam of light through a slide as the light passes through the slide it is modulated by the contents of the slide this trans mitted beam is then projected onto the viewing screen forming an enlarged image of the slide tems work the same way except that they shine a beam of electrons through a specimen analogous to the slide the fraction of the beam transmitted through the specimen is projected onto a phosphor screen the interaction of the electrons with the phosphor produces light and there fore a viewable image a scanning electron microscope sem on the other hand actually scans the electron beam and records the interaction of beam and sample at each location this produces one dot on a phosphor screen a complete image is formed by a raster scan of the beam through the sample much like a tv camera the electrons interact with a phosphor screen and produce light sems are suitable for bulky samples while tems require very thin samples electron microscopes are capable of very high magnification while light microscopy is limited to magnifications on the order electron microscopes a b figure a sem image of a tungsten filament following thermal failure note the shattered pieces on the lower left b sem image of damaged integrated circuit the white fibers are oxides resulting from thermal destruction figure a courtesy of mr michael shaffer department of geological sciences university of oregon eugene b courtesy of dr j m hudak mcmaster university hamilton ontario canada can achieve magnification of 10 000 or more figure shows two sem im ages of specimen failures due to thermal overload we conclude the discussion of imaging modalities by looking briefly at im ages that are not obtained from physical objects instead they are generated by computer fractals are striking examples of computer generated images lu basically a fractal is nothing more than an iterative reproduction of a basic pattern according to some mathematical rules for instance tiling is one of the simplest ways to generate a fractal image a square can be subdi vided into four square subregions each of which can be further subdivided into four smaller square regions and so on depending on the complexity of the rules for filling each subsquare some beautiful tile images can be generated using this method of course the geometry can be arbitrary for instance the fractal image could be grown radially out of a center point figure a shows a fractal grown in this way figure 22 b shows another fractal a moonscape that provides an interesting analogy to the images of space used as illustrations in some of the preceding sections fractal images tend toward artistic mathematical formulations of growth of subimage elements according to a set of rules they are useful sometimes as random textures a more structured approach to image generation by computer lies in d modeling this is an area that provides an important intersection between image processing and computer graphics and is the basis for many d visualization systems e g flight simulators figures 22 c and d show examples of computer generated images since the original object is created in d images can be generated in any perspective from plane projections of the d volume images of this type can be used for medical training and for a host of other applications such as criminal forensics and special effects figure 22 a and b fractal images c and d images generated from d computer models of the objects shown figures a and b courtesy of ms melissa d binde swarthmore college c and d courtesy of nasa 4 fundamental steps in digital image processing it is helpful to divide the material covered in the following chapters into the two broad categories defined in section methods whose input and output are images and methods whose inputs may be images but whose outputs are attributes extracted from those images this organization is summarized in fig 23 the diagram does not imply that every process is applied to an image rather the intention is to convey an idea of all the methodologies that can be applied to images for different purposes and possibly with different objectives the discussion in this section may be viewed as a brief overview of the material in the remainder of the book image acquisition is the first process in fig 23 the discussion in section gave some hints regarding the origin of digital images this topic is considered in much more detail in chapter where we also introduce a number of basic digital image concepts that are used throughout the book note that acquisi tion could be as simple as being given an image that is already in digital form generally the image acquisition stage involves preprocessing such as scaling image enhancement is the process of manipulating an image so that the re sult is more suitable than the original for a specific application the word specific is important here because it establishes at the outset that enhancement techniques are problem oriented thus for example a method that is quite use ful for enhancing x ray images may not be the best approach for enhancing satellite images taken in the infrared band of the electromagnetic spectrum figure 23 fundamental steps in digital image processing the chapter indicated in the boxes is where the material described in the box is discussed outputs of these processes generally are images problem domain there is no general theory of image enhancement when an image is processed for visual interpretation the viewer is the ultimate judge of how well a particular method works enhancement techniques are so varied and use so many different image processing approaches that it is difficult to as semble a meaningful body of techniques suitable for enhancement in one chapter without extensive background development for this reason and also because beginners in the field of image processing generally find enhance ment applications visually appealing interesting and relatively simple to un derstand we use image enhancement as examples when introducing new concepts in parts of chapter and in chapters 3 and 4 the material in the latter two chapters span many of the methods used traditionally for image en hancement therefore using examples from image enhancement to introduce new image processing methods developed in these early chapters not only saves having an extra chapter in the book dealing with image enhancement but more importantly is an effective approach for introducing newcomers to the details of processing techniques early in the book however as you will see in progressing through the rest of the book the material developed in these chapters is applicable to a much broader class of problems than just image enhancement image restoration is an area that also deals with improving the appearance of an image however unlike enhancement which is subjective image restora tion is objective in the sense that restoration techniques tend to be based on mathematical or probabilistic models of image degradation enhancement on the other hand is based on human subjective preferences regarding what con stitutes a good enhancement result color image processing is an area that has been gaining in importance be cause of the significant increase in the use of digital images over the internet chapter 6 covers a number of fundamental concepts in color models and basic color processing in a digital domain color is used also in later chapters as the basis for extracting features of interest in an image wavelets are the foundation for representing images in various degrees of resolution in particular this material is used in this book for image data com pression and for pyramidal representation in which images are subdivided successively into smaller regions compression as the name implies deals with techniques for reducing the storage required to save an image or the bandwidth required to transmit it al though storage technology has improved significantly over the past decade the same cannot be said for transmission capacity this is true particularly in uses of the internet which are characterized by significant pictorial content image compression is familiar perhaps inadvertently to most users of computers in the form of image file extensions such as the jpg file extension used in the jpeg joint photographic experts group image compression standard morphological processing deals with tools for extracting image components that are useful in the representation and description of shape the material in this chapter begins a transition from processes that output images to processes that output image attributes as indicated in section 1 1 segmentation procedures partition an image into its constituent parts or objects in general autonomous segmentation is one of the most difficult tasks in digital image processing a rugged segmentation procedure brings the process a long way toward successful solution of imaging problems that require objects to be identified individually on the other hand weak or er ratic segmentation algorithms almost always guarantee eventual failure in general the more accurate the segmentation the more likely recognition is to succeed generative models for discrete data introduction in section we discussed how to classify a feature vector x by applying bayes rule to a generative classiﬁer of the form p y c x θ p x y c θ p y c θ the key to using such models is specifying a suitable form for the class conditional density p x y c θ which deﬁnes what kind of data we expect to see in each class in this chapter we focus on the case where the observed data are discrete symbols we also discuss how to infer the unknown parameters θ of such models bayesian concept learning consider how a child learns to understand the meaning of a word such as dog presumably the child parents point out positive examples of this concept saying such things as look at the cute dog or mind the doggy etc however it is very unlikely that they provide negative examples by saying look at that non dog certainly negative examples may be obtained during an active learning process the child says look at the dog and the parent says that a cat dear not a dog but psychological research has shown that people can learn concepts from positive examples alone xu and tenenbaum we can think of learning the meaning of a word as equivalent to concept learning which in turn is equivalent to binary classiﬁcation to see this deﬁne f x if x is an example of the concept c and f x otherwise then the goal is to learn the indicator function f which just deﬁnes which elements are in the set c by allowing for uncertainty about the deﬁnition of f or equivalently the elements of c we can emulate fuzzy set theory but using standard probability calculus note that standard binary classiﬁcation techniques require positive and negative examples by contrast we will devise a way to learn from positive examples alone for pedagogical purposes we will consider a very simple example of concept learning called the number game based on part of josh tenenbaum phd thesis tenenbaum the game proceeds as follows i choose some simple arithmetical concept c such as prime number or a number between and i then give you a series of randomly chosen positive examples xn drawn from c and ask you whether some new test case x belongs to c i e i ask you to classify x examples figure empirical predictive distribution averaged over humans in the number game first two rows after seeing and this illustrates diffuse similarity third row after seeing this illustrates rule like behavior powers of bottom row after seeing this illustrates focussed similarity numbers near source figure of tenenbaum used with kind permission of josh tenenbaum suppose for simplicity that all numbers are integers between and now suppose i tell you is a positive example of the concept what other numbers do you think are positive it hard to tell with only one example so your predictions will be quite vague presumably numbers that are similar in some sense to are more likely but similar in what way is similar because it is close by is similar because it has a digit in common is similar because it is also even and a power of but does not seem similar thus some numbers are more likely than others we can represent this as a probability distribution p x which is the probability that x c given the data for any x this is called the posterior predictive distribution figure top shows the predictive distribution of people derived from a lab experiment we see that people predict numbers that are similar to under a variety of kinds of similarity now suppose i tell you that and are also positive examples now you may guess that the hidden concept is powers of two this is an example of induction given this hypothesis the predictive distribution is quite speciﬁc and puts most of its mass on powers of as shown in figure third row if instead i tell you the data is you will get a different kind of generalization gradient as shown in figure bottom how can we explain this behavior and emulate it in a machine the classic approach to induction is to suppose we have a hypothesis space of concepts such as odd numbers even numbers all numbers between and powers of two all numbers ending in j for j etc the subset of that is consistent with the data d is called the version space as we see more examples the version space shrinks and we become increasingly certain about the concept mitchell however the version space is not the whole story after seeing there are many consistent rules how do you combine them to predict if x c also after seeing why did you choose the rule powers of two and not say all even numbers or powers of two except for both of which are equally consistent with the evidence we will now provide a bayesian explanation for this likelihood we must explain why we chose htwo powers of two and not say heven even numbers after seeing given that both hypotheses are consistent with the evidence the key intuition is that we want to avoid suspicious coincidences if the true concept was even numbers how come we only saw numbers that happened to be powers of two to formalize this let us assume that examples are sampled uniformly at random from the extension of a concept the extension of a concept is just the set of numbers that belong to it e g the extension of heven is the extension of numbers ending in is tenenbaum calls this the strong sampling assumption given this assumption the probability of independently sampling n items with replacement from h is given by ln ln p d h size h h this crucial equation embodies what tenenbaum calls the size principle which means the model favors the simplest smallest hypothesis consistent with the data this is more commonly known as occam razor to see how it works let then p htwo since there are only powers of two less than but p heven since there are even numbers so the likelihood that h htwo is higher than if h heven after examples the likelihood of htwo is whereas the likelihood of heven is this is a likelihood ratio of almost in favor of htwo this quantiﬁes our earlier intuition that d would be a very suspicious coincidence if generated by heven prior suppose d given this data the concept h powers of two except is more likely than h powers of two since h does not need to explain the coincidence that is missing from the set of examples however the hypothesis h powers of two except seems conceptually unnatural we can capture such intution by assigning low prior probability to unnatural concepts of course your prior might be different than mine this subjective aspect of bayesian reasoning is a source of much controversy since it means for example that a child and a math professor william of occam also spelt ockham was an english monk and philosopher will reach different answers in fact they presumably not only have different priors but also different hypothesis spaces however we can ﬁnesse that by deﬁning the hypothesis space of the child and the math professor to be the same and then setting the child prior weight to be zero on certain advanced concepts thus there is no sharp distinction between the prior and the hypothesis space although the subjectivity of the prior is controversial it is actually quite useful if you are told the numbers are from some arithmetic rule then given and you may think is likely but is unlikely but if you are told that the numbers are examples of healthy cholesterol levels you would probably think is unlikely and is likely thus we see that the prior is the mechanism by which background knowledge can be brought to bear on a problem without this rapid learning i e from small samples sizes is impossible so what prior should we use for illustration purposes let us use a simple prior which puts uniform probability on simple arithmetical concepts such as even numbers odd numbers prime numbers numbers ending in etc to make things more interesting we make the concepts even and odd more likely apriori we also include two unnatural concepts namely powers of plus and powers of except but give them low prior weight see figure a for a plot of this prior we will consider a slightly more sophisticated prior later on posterior the posterior is simply the likelihood times the prior normalized in this context we have p d h p h p h i d h h n p h d ht h p d h ht h p h i d h h n where i h is iff iff and only if all the data are in the extension of the hypothesis h figure plots the prior likelihood and posterior after seeing we see that the posterior is a combination of prior and likelihood in the case of most of the concepts the prior is uniform so the posterior is proportional to the likelihood however the unnatural concepts of powers of plus and powers of except have low posterior support despite having high likelihood due to the low prior conversely the concept of odd numbers has low posterior support despite having a high prior due to the low likelihood figure plots the prior likelihood and posterior after seeing now the likelihood is much more peaked on the powers of two concept so this dominates the posterior essentially the learner has an aha moment and ﬁgures out the true concept here we see the need for the low prior on the unnatural concepts otherwise we would have overﬁt the data and picked powers of except for in general when we have enough data the posterior p h becomes peaked on a single concept namely the map estimate i e p h d δhˆmap h where hˆmap argmaxh p h is the posterior mode and where δ is the dirac measure deﬁned by δ if x a if x a data even odd squares mult of mult of mult of mult of mult of mult of mult of mult of ends in ends in ends in ends in ends in ends in ends in ends in ends in powers of powers of powers of powers of powers of powers of powers of powers of powers of all powers of powers of prior lik post figure prior likelihood and posterior for d based on tenenbaum figure generated by numbersgame note that the map estimate can be written as hˆmap argmax p d h p h argmax log p d h log p h h h since the likelihood term depends exponentially on n and the prior stays constant as we get more and more data the map estimate converges towards the maximum likelihood estimate or mle hˆmle argmax p d h argmax log p d h h h in other words if we have enough data we see that the data overwhelms the prior in this data even odd squares mult of mult of mult of mult of mult of mult of mult of mult of ends in ends in ends in ends in ends in ends in ends in ends in ends in powers of powers of powers of powers of powers of powers of powers of powers of powers of all powers of powers of prior lik x post figure prior likelihood and posterior for d based on tenenbaum figure generated by numbersgame case the map estimate converges towards the mle if the true hypothesis is in the hypothesis space then the map ml estimate will converge upon this hypothesis thus we say that bayesian inference and ml estimation are consistent estimators see section for details we also say that the hypothesis space is identiﬁable in the limit meaning we can recover the truth in the limit of inﬁnite data if our hypothesis class is not rich enough to represent the truth which will usually be the case we will converge on the hypothesis that is as close as possible to the truth however formalizing this notion of closeness is beyond the scope of this chapter powers of powers of ends in squares even mult of mult of all powers of powers of p h figure posterior over hypotheses and the corresponding predictive distribution after seeing one example d a dot means this number is consistent with this hypothesis the graph p h d on the right is the weight given to hypothesis h by taking a weighed sum of dots we get p x c d top based on figure of tenenbaum figure generated by numbersgame posterior predictive distribution the posterior is our internal belief state about the world the way to test if our beliefs are justiﬁed is to use them to predict objectively observable quantities this is the basis of the scientiﬁc method speciﬁcally the posterior predictive distribution in this context is given by p x c d p y x h p h d h this is just a weighted average of the predictions of each individual hypothesis and is called bayes model averaging hoeting et al this is illustrated in figure the dots at the bottom show the predictions from each hypothesis the vertical curve on the right shows the weight associated with each hypothesis if we multiply each row by its weight and add up we get the distribution at the top when we have a small and or ambiguous dataset the posterior p h is vague which induces a broad predictive distribution however once we have ﬁgured things out the posterior becomes a delta function centered at the map estimate in this case the predictive distribution becomes p x c d p x h δhˆ h p x hˆ h this is called a plug in approximation to the predictive density and is very widely used due to its simplicity however in general this under represents our uncertainty and our predictions will not be as smooth as when using bma we will see more examples of this later in the book although map learning is simple it cannot explain the gradual shift from similarity based reasoning with uncertain posteriors to rule based reasoning with certain posteriors for example suppose we observe if we use the simple prior above the minimal consistent hypothesis is all powers of so only and get a non zero probability of being predicted this is of course an example of overﬁtting given the map hypothesis is all powers of two thus the plug in predictive distribution gets broader or stays the same as we see more data it starts narrow but is forced to broaden as it seems more data in contrast in the bayesian approach we start broad and then narrow down as we learn more which makes more intuitive sense in particular given there are many hypotheses with non negligible posterior support so the predictive distribution is broad however when we see the posterior concentrates its mass on one hypothesis so the predictive distribution becomes narrower so the predictions made by a plug in approach and a bayesian approach are quite different in the small sample regime although they converge to the same answer as we see more data a more complex prior to model human behavior tenenbaum used a slightly more sophisticated prior which was de rived by analysing some experimental data of how people measure similarity between numbers see tenenbaum for details the result is a set of arithmetical concepts similar to those mentioned above plus all intervals between n and m for n m note that these hypotheses are not mutually exclusive thus the prior is a mixture of two priors one over arithmetical rules and one over intervals p h h pinterval h the only free parameter in the model is the relative weight given to these two parts of the prior the results are not very sensitive to this value so long as reﬂecting the fact that people are more likely to think of concepts deﬁned by rules the predictive distribution of the model using this larger hypothesis space is shown in figure it is strikingly similar to the human predictive distribution shown in figure even though it was not ﬁt to human data modulo the choice of hypothesis space the beta binomial model the number game involved inferring a distribution over a discrete variable drawn from a ﬁnite hypothesis space h given a series of discrete observations this made the computations particularly simple we just needed to sum multiply and divide however in many applications the unknown parameters are continuous so the hypothesis space is some subset of rk where examples 88 figure predictive distributions for the model using the full hypothesis space compare to figure the predictions of the bayesian model are only plotted for those values of x for which human data is available this is why the top line looks sparser than figure source figure of tenenbaum used with kind permission of josh tenenbaum k is the number of parameters this complicates the mathematics since we have to replace sums with integrals however the basic ideas are the same we will illustrate this by considering the problem of inferring the probability that a coin shows up heads given a series of observed coin tosses although this might seem trivial it turns out that this model forms the basis of many of the methods we will consider later in this book including naive bayes classiﬁers markov models etc it is historically important since it was the example which was analyzed in bayes original paper of bayes analysis was subsequently generalized by pierre simon laplace creating what we now call bayes rule see stigler for further historical details we will follow our now familiar recipe of specifying the likelihood and prior and deriving the posterior and posterior predictive likelihood suppose xi ber θ where xi represents heads xi represents tails and θ is the rate parameter probability of heads if the data are iid the likelihood has the form p d θ θ where we have n i xi heads and n i xi tails these two counts are called the sufficient statistics of the data since this is all we need to know about to infer θ an alternative set of sufficient statistics are and n more formally we say is a sufficient statistic for data if p θ p θ data if we use a uniform prior this is equivalent to saying p θ p θ consequently if we have two datasets with the same sufficient statistics we will infer the same value for θ now suppose the data consists of the count of the number of heads observed in a ﬁxed number n of trials in this case we have bin n θ where bin represents the binomial distribution which has the following pmf bin k n θ n θk θ n k since n is a constant independent of θ the likelihood for the binomial sampling model is the k same as the likelihood for the bernoulli model so any inferences we make about θ will be the same whether we observe the counts d n or a sequence of trials d xn prior we need a prior which has support over the interval to make the math easier it would convenient if the prior had the same form as the likelihood i e if the prior looked like p θ θ for some prior parameters and if this were the case then we could easily evaluate the posterior by simply adding up the exponents p θ p d θ p θ θ θ θ when the prior and the posterior have the same form we say that the prior is a conjugate prior for the corresponding likelihood conjugate priors are widely used because they simplify computation and are easy to interpret as we see below in the case of the bernoulli the conjugate prior is the beta distribution which we encountered in section beta θ a b θa θ b the parameters of the prior are called hyper parameters we can set them in order to encode our prior beliefs for example to encode our beliefs that θ has mean and standard deviation we set a and b exercise or to encode our beliefs that θ has mean and that we think it lives in the interval with probability then we ﬁnd a and b exercise if we know nothing about θ except that it lies in the interval we can use a uni form prior which is a kind of uninformative prior see section for details the uniform distribution can be represented by a beta distribution with a b a b figure a updating a beta prior with a binomial likelihood with sufficient statistics to yield a beta posterior b updating a beta prior with a binomial likeli hood with sufficient statistics to yield a beta posterior figure generated by binomialbetaposteriordemo posterior if we multiply the likelihood by the beta prior we get the following posterior following equa tion p θ d bin θ beta θ a b beta θ a b in particular the posterior is obtained by adding the prior hyper parameters to the empirical counts for this reason the hyper parameters are known as pseudo counts the strength of the prior also known as the effective sample size of the prior is the sum of the pseudo counts a b this plays a role analogous to the data set size n figure a gives an example where we update a weak beta prior with a peaked likelihood function corresponding to a large sample size we see that the posterior is essentially identical to the likelihood since the data has overwhelmed the prior figure b gives an example where we update a strong beta prior with a peaked likelihood function now we see that the posterior is a compromise between the prior and likelihood note that updating the posterior sequentially is equivalent to updating in a single batch to see this suppose we have two data sets da and db with sufficient statistics n a n a and n b n b let n a n b and n a n b be the sufficient statistics of the combined datasets in batch mode we have p θ da db bin θ beta θ a b beta θ a b in sequential mode we have p θ da db p db θ p θ da b b b a a bin θ beta θ a b a b a b beta θ a b this makes bayesian inference particularly well suited to online learning as we will see later posterior mean and mode from equation the map estimate is given by θˆmap a a b n if we use a uniform prior then the map estimate reduces to the mle which is just the empirical fraction of heads θˆmle n this makes intuitive sense but it can also be derived by applying elementary calculus to maximize the likelihood function in equation exercise by contrast the posterior mean is given by a θ a b n this difference between the mode and the mean will prove important later we will now show that the posterior mean is convex combination of the prior mean and the mle which captures the notion that the posterior is a compromise between what we previously believed and what the data is telling us let a b be the equivalent sample size of the prior which controls its strength and let the prior mean be a then the posterior mean is given by e θ d m n λm λ θˆ n n n n mle where λ is the ratio of the prior to posterior equivalent sample size so the weaker the prior the smaller is λ and hence the closer the posterior mean is to the mle one can show similarly that the posterior mode is a convex combination of the prior mode and the mle and that it too converges to the mle posterior variance the mean and mode are point estimates but it is useful to know how much we can trust them the variance of the posterior is one way to measure this the variance of the beta posterior is given by a b var θ d a n b n a n b n we can simplify this formidable expression in the case that n a b to get var θ nnn θˆ θˆ n where θˆ is the mle hence the error bar in our estimate i e the posterior standard deviation is given by σ ivar θ d θˆ θˆ we see that the uncertainty goes down at a rate of n note however that the uncertainty variance is maximized when θˆ and is minimized when θˆ is close to or this means it is easier to be sure that a coin is biased than to be sure that it is fair posterior predictive distribution so far we have been focusing on inference of the unknown parameter let us now turn our attention to prediction of future observable data consider predicting the probability of heads in a single future trial under a beta a b poste rior we have p x d p x θ p θ d dθ a θ beta θ a b dθ e θ d a b thus we see that the mean of the posterior predictive distribution is equivalent in this case to plugging in the posterior mean parameters p x d ber x e θ d overﬁtting and the black swan paradox suppose instead that we plug in the mle i e we use p x ber x θˆmle unfortunately this approximation can perform quite poorly when the sample size is small for example suppose we have seen n tails in a row the mle is θˆ since this makes the observed data as probable as possible however using this estimate we predict that heads are impossible this is called the zero count problem or the sparse data problem and frequently occurs when estimating counts from small amounts of data one might think that in the era of big data such concerns are irrelevant but note that once we partition the data based on certain criteria such as the number of times a speciﬁc person has engaged in a speciﬁc activity the sample sizes can become much smaller this problem arises for example when trying to perform personalized recommendation of web pages thus bayesian methods are still useful even in the big data regime jordan the zero count problem is analogous to a problem in philosophy called the black swan paradox this is based on the ancient western conception that all swans were white in that context a black swan was a metaphor for something that could not exist black swans were discovered in australia by european explorers in the century the term black swan paradox was ﬁrst coined by the famous philosopher of science karl popper the term has also been used as the title of a recent popular book taleb this paradox was used to illustrate the problem of induction which is the problem of how to draw general conclusions about the future from speciﬁc observations from the past let us now derive a simple bayesian solution to the problem we will use a uniform prior so a b in this case plugging in the posterior mean gives laplace rule of succession p x d n n this justiﬁes the common practice of adding to the empirical counts normalizing and then plugging them in a technique known as add one smoothing note that plugging in the map parameters would not have this smoothing effect since the mode has the form θˆ a which becomes the mle if a b n a b predicting the outcome of multiple future trials suppose now we were interested in predicting the number of heads x in m future trials this is given by p x d m bin x θ m beta θ a b dθ m r θx θ m xθa θ b x b a b we recognize the integral as the normalization constant for a beta a x m x b distribution hence r thus we ﬁnd that the posterior predictive is given by the following known as the compound beta binomial distribution bb x a b m m b x a m x b this distribution has the following mean and variance e x m a a b mab var x a b a b m a b if m and hence x we see that the mean becomes e x d p x d a which is consistent with equation this process is illustrated in figure a we start with a beta prior and plot the posterior predictive density after seeing heads and tails figure b plots a plug in approximation using a map estimate we see that the bayesian prediction has longer tails spreading its probablity mass more widely and is therefore less prone to overﬁtting and blackswan type paradoxes the dirichlet multinomial model in the previous section we discussed how to infer the probability that a coin comes up heads in this section we generalize these results to infer the probability that a dice with k sides comes up as face k this might seem like another toy exercise but the methods we will study are widely used to analyse text data biosequence data etc as we will see later posterior predictive plugin predictive a b figure a posterior predictive distributions after seeing b plugin approximation figure generated by betabinompostpreddemo likelihood suppose we observe n dice rolls xn where xi k if we assume the data is iid the likelihood has the form k p d θ k k where nk n i yi k is the number of times event k occured these are the sufficient statistics for this model the likelihood for the multinomial model has the same form up to an irrelevant constant factor prior since the parameter vector lives in the k dimensional probability simplex we need a prior that has support over this simplex ideally it would also be conjugate fortunately the dirichlet distribution section satisﬁes both criteria so we will use the following prior dir θ α b α k αk k k k posterior multiplying the likelihood by the prior we ﬁnd that the posterior is also dirichlet p θ d p d θ p θ k k tt θnkθαk tt θαk nk dir θ αk nk we see that the posterior is obtained by adding the prior hyper parameters pseudo counts αk to the empirical counts nk we can derive the mode of this posterior i e the map estimate by using calculus however we must enforce the constraint that k θk we can do this by using a lagrange multiplier the constrained objective function or lagrangian is given by the log likelihood plus log prior plus the constraint θ λ nk log θk αk log θk λ θk to simplify notation we deﬁne nk nk αk taking derivatives with respect to λ yields the original constraint λ θk taking derivatives with respect to θk yields θk nk nk λ θk λθk we can solve for λ using the sum to one constraint nk k λ θk k n k λ where k αk is the equivalent sample size of the prior thus the map estimate is given by θˆk nk αk n k which is consistent with equation if we use a uniform prior αk we recover the mle θˆk nk n this is just the empirical fraction of times face k shows up we do not need to explicitly enforce the constraint that θk since the gradient of the objective has the form nk θk λ so negative values would reduce the objective rather than maximize it of course this does not preclude setting θk and indeed this is the optimal solution if nk and αk posterior predictive the posterior predictive distribution for a single multinoulli trial is given by the following expression p x j d r p x j θ p θ d dθ r p x j θj r p θ j θj d dθ jl dθj r θ p θ d dθ αj nj θ d αj nj where θ j are all the components of θ except θj see also exercise the above expression avoids the zero count problem just as we saw in section in fact this form of bayesian smoothing is even more important in the multinomial case than the binary case since the likelihood of data sparsity increases once we start partitioning the data into many categories worked example language models using bag of words one application of bayesian smoothing using the dirichlet multinomial model is to language modeling which means predicting which words might occur next in a sequence here we will take a very simple minded approach and assume that the i th word xi k is sampled independently from all the other words using a cat θ distribution this is called the bag of words model given a past sequence of words how can we predict which one is likely to come next for example suppose we observe the following sequence part of a children nursery rhyme mary had a little lamb little lamb little lamb mary had a little lamb its fleece as white as snow furthermore suppose our vocabulary consists of the following words mary lamb little big fleece white black snow rain unk here unk stands for unknown and represents all other words that do not appear elsewhere on the list to encode each line of the nursery rhyme we ﬁrst strip off punctuation and remove any stop words such as a as the etc we can also perform stemming which means reducing words to their base form such as stripping off the ﬁnal in plural words or the ing from verbs e g running becomes run in this example no words need stemming finally we replace each word by its index into the vocabulary to get we now ignore the word order and count how often each word occurred resulting in a histogram of word counts token word mary lamb little big ﬂeece white black snow rain unk count denote the above counts by nj if we use a dir α prior for θ the posterior predictive is just αj nj nj p x j d e θj d α t n t p x j d the modes of the predictive distribution are x lamb and x unk note that the words big black and rain are predicted to occur with non zero probability in the future even though they have never been seen before later on we will see more sophisticated language models naive bayes classiﬁers in this section we discuss how to classify vectors of discrete valued features x k d where k is the number of values for each feature and d is the number of features we will use a generative approach this requires us to specify the class conditional distribution p x y c the simplest approach is to assume the features are conditionally independent given the class label this allows us to write the class conditional density as a product of one dimensional densities d p x y c θ p xj y c θjc j the resulting model is called a naive bayes classiﬁer nbc the model is called naive since we do not expect the features to be independent even conditional on the class label however even if the naive bayes assumption is not true it often results in classiﬁers that work well domingos and pazzani one reason for this is that the model is quite simple it only has o cd parameters for c classes and d features and hence it is relatively immune to overﬁtting the form of the class conditional density depends on the type of each feature we give some possibilities below in the case of real valued features we can use the gaussian distribution p x y c θ d n xj μjc where μjc is the mean of feature j in objects of class c and is its in the case of binary features xj we can use the bernoulli distribution p x y c θ d ber xj μjc where μjc is the probability that feature j occurs in class c this is sometimes called the multivariate bernoulli naive bayes model we will see an application of this below in the case of categorical features xj k we can model use the multinoulli distribution p x y c θ d cat xj μ where μ is a histogram over the k jc jc possible values for xj in class c obviously we can handle other kinds of features or use different distributional assumptions also it is easy to mix and match features of different types model ﬁtting we now discuss how to train a naive bayes classiﬁer this usually means computing the mle or the map estimate for the parameters however we will also discuss how to compute the full posterior p θ d mle for nbc the probability for a single data case is given by p xi yi θ p yi π tt p xij θj tt πi yi c tt tt p xij θjc i yi c j c j c hence the log likelihood is given by c d c log p d θ nc log πc log p xij θjc we see that this expression decomposes into a series of terms one concerning π and dc terms containing the θjc hence we can optimize all these parameters separately from equation the mle for the class prior is given by πˆc nc n where nc i i yi c is the number of examples in class c the mle for the likelihood depends on the type of distribution we choose to use for each feature for simplicity let us suppose all features are binary so xj y c ber θjc in this case the mle becomes θˆjc njc nc it is extremely simple to implement this model ﬁtting procedure see algorithm for some pseudo code and naivebayesfit for some matlab code this algorithm obviously takes o nd time the method is easily generalized to handle features of mixed type this simplicity is one reason the method is so widely used figure gives an example where we have classes and binary features representing the presence or absence of words in a bag of words model the plot visualizes the θc vectors for the two classes the big spike at index corresponds to the word subject which occurs in both classes with probability in section we discuss how to ﬁlter out such uninformative features algorithm fitting a naive bayes classiﬁer to binary features nc njc for i n do c yi class label of i th example nc nc for j d do if xij then njc njc πˆc nc θˆjc njc n n p xj y a p xj y 600 b figure class conditional densities p xj y c for two document classes corresponding to x windows and ms windows figure generated by naivebayesbowdemo bayesian naive bayes the trouble with maximum likelihood is that it can overﬁt for example consider the example in figure the feature corresponding to the word subject call it feature j always occurs in both classes so we estimate θˆjc what will happen if we encounter a new email which does not have this word in it our algorithm will crash and burn since we will ﬁnd that p y c x θˆ for both classes this is another manifestation of the black swan paradox discussed in section a simple solution to overﬁtting is to be bayesian for simplicity we will use a factored prior d c p θ p π p θjc j c we will use a dir α prior for π and a beta prior for each θjc often we just take α and β corresponding to add one or laplace smoothing combining the factored likelihood in equation with the factored prior above gives the following factored posterior d c p θ d p π d p θjc d j c p π d dir nc αc p θjc d beta nc njc njc in other words to compute the posterior we just update the prior counts with the empirical counts from the likelihood it is straightforward to modify algorithm to handle this version of model ﬁtting using the model for prediction at test time the goal is to compute d p y c x d p y c d p xj y c d j the correct bayesian procedure is to integrate out the unknown parameters p y c x d r cat y c π p π d dπl jtt r ber xj y c θjc p θjc d l fortunately this is easy to do at least if the posterior is dirichlet in particular from equa tion we know the posterior predictive density can be obtained by simply plugging in the posterior mean parameters θ hence d p y c x d πc θjc i xj θjc i xj j θjk njc nc π nc αc c n where c αc if we have approximated the posterior by a single point p θ δθˆ θ where θˆ may be the ml or map estimate then the posterior predictive density is obtained by simply plugging in the parameters to yield a virtually identical rule p y c x d d πˆc θˆjc i xj θˆjc i xj j the only difference is we replaced the posterior mean θ with the posterior mode or mle θˆ however this small difference can be important in practice since the posterior mean will result in less overﬁtting see section the log sum exp trick we now discuss one important practical detail that arises when using generative classiﬁers of any kind we can compute the posterior over class labels using equation using the appropriate class conditional density and a plug in approximation unfortunately a naive implementation of equation can fail due to numerical underﬂow the problem is that p x y c is often a very small number especially if x is a high dimensional vector this is because we require that x p x y so the probability of observing any particular high dimensional vector is small the obvious solution is to take logs when applying bayes rule as follows log p y c x bc log c ct ebct bc log p x y c log p y c however this requires evaluating the following expression log ebct log p y c x log p x and we can t add up in the log domain fortunately we can factor out the largest term and just represent the remaining numbers relative to that for example log e e log e e log e in general we have log ebc log r ebc b rlog ebc b b where b maxc bc this is called the log sum exp trick and is widely used see the function logsumexp for an implementation this trick is used in algorithm which gives pseudo code for using an nbc to compute p yi xi θˆ see naivebayespredict for the matlab code note that we do not need the log sum exp trick if we only want to compute yˆi since we can just maximize the unnormalized quantity log p yi c log p xi y c feature selection using mutual information since an nbc is ﬁtting a joint distribution over potentially many features it can suffer from overﬁtting in addition the run time cost is o d which may be too high for some applications one common approach to tackling both of these problems is to perform feature selection to remove irrelevant features that do not help much with the classiﬁcation problem the simplest approach to feature selection is to evaluate the relevance of each feature separately and then algorithm predicting with a naive bayes classiﬁer for binary features for i n do for c c do lic log πˆc for j d do if xij then lic lic log θˆjc else lic lic log θˆjc pic exp lic logsumexp li yˆi argmaxc pic take the top k where k is chosen based on some tradeoff between accuracy and complexity this approach is known as variable ranking ﬁltering or screening one way to measure relevance is to use mutual information section between feature xj and the class label y i x y p x y log p xj y the mutual information can be thought of as the reduction in entropy on the label distribution once we observe the value of feature j if the features are binary it is easy to show exercise that the mi can be computed as follows ij θjcπc log θjc θ θj jc πc log θjc θj where πc p y c θjc p xj y c and θj p xj c πcθjc all of these quantities can be computed as a by product of ﬁtting a naive bayes classiﬁer figure illustrates what happens if we apply this to the binary bag of words dataset used in figure we see that the words with highest mutual information are much more discriminative than the words which are most probable for example the most probable word in both classes is subject which always occurs because this is newsgroup data which always has a subject line but obviously this is not very discriminative the words with highest mi with the class label are in decreasing order windows microsoft dos and motif which makes sense since the classes correspond to microsoft windows and x windows classifying documents using bag of words document classiﬁcation is the problem of classifying text documents into different categories one simple approach is to represent each document as a binary vector which records whether each word is present or not so xij iff word j occurs in document i otherwise xij we can then use the following class conditional density d d p xi yi c θ tt ber xij θjc tt θi xij θjc i xij j j class prob class prob highest mi mi subject subject windows this windows microsoft with this dos but with motif you but window table we list the most likely words for class x windows and class ms windows we also show the words with highest mutual information with class label produced by naivebayesbowdemo this is called the bernoulli product model or the binary independence model however ignoring the number of times each word occurs in a document loses some in formation mccallum and nigam a more accurate representation counts the number of occurrences of each word speciﬁcally let xi be a vector of counts for document i so xij ni where ni is the number of terms in document i so d xij ni for the class conditional densities we can use a multinomial distribution p x y c θ mu x n θ ni tt θxij where we have implicitly assumed that the document length ni is independent of the class here θjc is the probability of generating word j in documents of class c these parameters satisfy the constraint that d θjc for each class c although the multinomial classiﬁer is easy to train and easy to use at test time it does not work particularly well for document classiﬁcation one reason for this is that it does not take into account the burstiness of word usage this refers to the phenomenon that most words never appear in any given document but if they do appear once they are likely to appear more than once i e words occur in bursts the multinomial model cannot capture the burstiness phenomenon to see why note that equation has the form θnij and since θjc for rare words it becomes increasingly unlikely to generate many of them for more frequent words the decay rate is not as fast to see why intuitively note that the most frequent words are function words which are not speciﬁc to the class such as and the and but the chance of the word and occuring is pretty much the same no matter how many time it has previously occurred modulo document length so the independence assumption is more reasonable for common words however since rare words are the ones that matter most for classiﬁcation purposes these are the ones we want to model the most carefully various ad hoc heuristics have been proposed to improve the performance of the multinomial document classiﬁer rennie et al we now present an alternative class conditional density that performs as well as these ad hoc methods yet is probabilistically sound madsen et al since equation models each word independently this model is often called a naive bayes classiﬁer although technically the features xij are not independent because of the constraint j xij ni suppose we simply replace the multinomial class conditional density with the dirichlet compound multinomial or dcm density deﬁned as follows p x y c α r mu x n θ dir θ α dθ ni b xi αc this equation is derived in equation surprisingly this simple change is all that is needed to capture the burstiness phenomenon the intuitive reason for this is as follows after seeing one occurence of a word say word j the posterior counts on θj gets updated making another occurence of word j more likely by contrast if θj is ﬁxed then the occurences of each word are independent the multinomial model corresponds to drawing a ball from an urn with k colors of ball recording its color and then replacing it by contrast the dcm model corresponds to drawing a ball recording its color and then replacing it with one additional copy this is called the polya urn using the dcm as the class conditional density gives much better results than using the multinomial and has performance comparable to state of the art methods as described in madsen et al the only disadvantage is that ﬁtting the dcm model is more complex see minka elkan for the details exercises exercise mle for the bernoulli binomial model derive equation by optimizing the log of the likelihood in equation exercise marginal likelihood for the beta bernoulli model in equation we showed that the marginal likelihood is the ratio of the normalizing constants p d z γ γ γ z γ n γ γ we will now derive an alternative derivation of this fact by the chain rule of probability p n p p p in section we showed that the posterior predictive distribution is nk αk nk αk p x k n n αi n α where k and n is the data seen so far now suppose d h t t h h or d then p d α α α α α n show how this reduces to equation by using the fact that for integers α γ α exercise posterior predictive for beta binomial model recall from equation that the posterior predictive for the beta binomial is given by p x n d bb x n b x n x n prove that this reduces to p x d b α0i x 88 when n and hence x i e show that bb αi αi αi αi hint use the fact that γ γ exercise beta updating from censored likelihood source gelman suppose we toss a coin n times let x be the number of heads we observe that there are fewer than heads but we don t know exactly how many let the prior probability of heads be p θ beta θ compute the posterior p θ x up to normalization constants i e derive an expression proportional to p θ x hint the answer is a mixture distribution exercise uninformative prior for log odds ratio let φ logit θ log θ θ show that if p φ then p θ beta θ hint use the change of variables formula exercise mle for the poisson distribution the poisson pmf is deﬁned as poi x λ e λ λx for x where λ is the rate parameter derive the mle exercise bayesian analysis of the poisson distribution in exercise we deﬁned the poisson distribution with rate λ and derived its mle here we perform a conjugate bayesian analysis a derive the posterior p λ d assuming a conjugate prior p λ ga λ a b λa λb hint the posterior is also a gamma distribution b what does the posterior mean tend to as a and b recall that the mean of a ga a b distribution is a b exercise mle for the uniform distribution source kaelbling consider a uniform distribution centered on with width the density function is given by p x i x a a a given a data set xn what is the maximum likelihood estimate of a call it aˆ b what probability would the model assign to a new data point xn using aˆ c do you see any problem with the above approach brieﬂy suggest in words a better approach exercise bayesian analysis of the uniform distribution consider the uniform distribution unif θ the maximum likelihood estimate is θˆ max as we saw in exercise but this is unsuitable for predicting future data since it puts zero probability mass outside the training data in this exercise we will perform a bayesian analysis of the uniform distribution following minka the conjugate prior is the pareto distribution p θ pareto θ b k deﬁned in section given a pareto prior the joint distribution of θ and d xn is kbk p d θ θn k i θ max d let m max the evidence the probability that all n samples came from the same uniform distribution is p d kbk dθ m θn k k n k bn kbk n k mn k if m b if m b derive the posterior p θ d and show that if can be expressed as a pareto distribution exercise taxicab tramcar problem suppose you arrive in a new city and see a taxi numbered how many taxis are there in this city let us assume taxis are numbered sequentially as integers starting from up to some unknown upper bound θ we number taxis from for simplicity we can also count from without changing the analysis hence the likelihood function is p x u θ the uniform distribution the goal is to estimate θ we will use the bayesian analysis from exercise a suppose we see one taxi numbered so d m n using an improper non informative prior on θ of the form p θ p a θ θ what is the posterior p θ d b compute the posterior mean mode and median number of taxis in the city if such quantities exist c rather than trying to compute a point estimate of the number of taxis we can compute the predictive density over the next taxicab number using p di d α p di θ p θ d α dθ p di β where α b k are the hyper parameters β c n k are the updated hyper parameters now consider the case d m and di x using equation write down an expression for p x d α as above use a non informative prior b k d use the predictive density formula to compute the probability that the next taxi you will see say the next day has number or i e compute p x d α p x d α p x d α e brieﬂy describe sentences some ways we might make the model more accurate at prediction exercise bayesian analysis of the exponential distribution a lifetime x of a machine is modeled by an exponential distribution with unknown parameter θ the likelihood is p x θ θe θx for x θ a show that the mle is θˆ x where x n xi b suppose we observe the lifetimes in years of different iid machines what is the mle given this data c assume that an expert believes θ should have a prior distribution that is also exponential p θ expon θ λ choose the prior parameter call it λˆ such that e θ hint recall that the gamma distribution has the form ga θ a b θa θb and its mean is a b d what is the posterior p θ d λˆ e is the exponential prior conjugate to the exponential likelihood f what is the posterior mean e θ d λˆ g explain why the mle and posterior mean differ which is more reasonable in this example exercise map estimation for the bernoulli with non conjugate priors source jaakkola in the book we discussed bayesian inference of a bernoulli rate parameter with the prior p θ beta θ α β we know that with this prior the map estimate is given by θˆ α n α β where is the number of heads is the number of tails and n is the total number of trials a now consider the following prior that believes the coin is fair or is slightly biased towards tails p θ if θ otherwise derive the map estimate under this prior as a function of and n b suppose the true parameter is θ which prior leads to a better estimate when n is small which prior leads to a better estimate when n is large exercise posterior predictive distribution for a batch of data with the dirichlet multinomial model in equation we gave the the posterior predictive distribution for a single multinomial trial using a dirichlet prior now consider predicting a batch of new data xm consisting of m single multinomial trials think of predicting the next m words in a sentence assuming they are drawn iid derive an expression for p d d α your answer should be a function of α and the old and new counts sufficient statistics deﬁned as old k new k i xi k i d i xi k i d hint recall that for a vector of counts k the marginal likelihood evidence is given by p d α γ α γ nk αk where α k αk and n k nk exercise posterior predictive for dirichlet multinomial source koller a suppose we compute the empirical distribution over letters of the roman alphabet plus the space character a distribution over values from samples suppose we see the letter e times what is p e d if we assume θ dir where αk for all k b suppose in the samples we saw e times a times and p times what is p p a if we assume θ dir where αk for all k show your work exercise setting the beta hyper parameters suppose θ β and we believe that e θ m and var θ v using equation solve for and in terms of m and v what values do you get if m and v exercise setting the beta hyper parameters ii source draper suppose θ β and we believe that e θ m and p t θ u write a program that can solve for and in terms of m t and u hint write as a function of and m so the pdf only has one unknown then write down the probability mass contained in the interval as an integral and minimize its squared discrepancy from what values do you get if m t and u what is the equivalent sample size of this prior exercise marginal likelihood for beta binomial under uniform prior suppose we toss a coin n times and observe heads let bin n θ and θ beta show that the marginal likelihood is p n n hint γ x x if x is an integer exercise bayes factor for coin tossing suppose we toss a coin n times and observe heads let the null hypothesis be that the coin is fair and the alternative be that the coin can have any bias so p θ unif derive the bayes factor in favor of the biased coin hypothesis what if n and hint see exercise exercise irrelevant features with naive bayes source jaakkola let xiw if word w occurs in document i and xiw otherwise let θcw be the estimated probability that word w occurs in documents of class c then the log likelihood that document x belongs to class c is log p xi c θ log θxiw θcw xiw w w xiw log θcw xiw log θcw w w xiw log θcw log θ θ where w is the number of words in the vocabulary we can write this more succintly as log p xi c θ φ xi t β where xi xiw is a bit vector φ xi xi and β log log θcw log θ t we see that this is a linear classiﬁer since the class conditional density is a linear function an inner product of the parameters βc a assuming p c p c write down an expression for the log posterior odds ratio log p c xi in terms of the features φ xi and the parameters β p c xi and b intuitively words that occur in both classes are not very discriminative and therefore should not affect our beliefs about the class label consider a particular word w state the conditions on w and w or equivalently the conditions on w w under which the presence or absence of w in a test document will have no effect on the class posterior such a word will be ignored by the classiﬁer hint using your previous result ﬁgure out when the posterior odds ratio is c the posterior mean estimate of θ using a beta prior is given by θˆcw i c xiw n c where the sum is over the nc documents in class c consider a particular word w and suppose it always occurs in every document regardless of class let there be documents of class and be the number of documents in class where since e g we get much more non spam than spam this is an example of class imbalance if we use the above estimate for θcw will word w be ignored by our classiﬁer explain why or why not d what other ways can you think of which encourage irrelevant words to be ignored exercise class conditional densities for binary data consider a generative classiﬁer for c classes with class conditional density p x y and uniform class prior p y suppose all the d features are binary xj if we assume all the features are conditionally independent the naive bayes assumption we can write d p x y c ber xj θjc j this requires dc parameters a now consider a different model which we will call the full model in which all the features are fully dependent i e we make no factorization assumptions how might we represent p x y c in this case how many parameters are needed to represent p x y c b assume the number of features d is ﬁxed let there be n training cases if the sample size n is very small which model naive bayes or full is likely to give lower test set error and why c if the sample size n is very large which model naive bayes or full is likely to give lower test set error and why d what is the computational complexity of ﬁtting the full and naive bayes models as a function of n and d use big oh notation fitting the model here means computing the mle or map parameter estimates you may assume you can convert a d bit vector to an array index in o d time e what is the computational complexity of applying the full and naive bayes models at test time to a single test case f suppose the test case has missing data let xv be the visible features of size v and xh be the hidden missing features of size h where v h d what is the computational complexity of computing p y xv θˆ for the full and naive bayes models as a function of v and h exercise mutual information for naive bayes classiﬁers with binary features derive equation exercise fitting a naive bayes spam ﬁlter by hand source daphne koller consider a naive bayes model multivariate bernoulli version for spam classiﬁca tion with the vocabulary v secret offer low price valued customer today dollar million sports is for play healthy pizza we have the following example spam messages million dollar offer secret offer today secret is secret and normal messages low price for valued customer play secret sports today sports is healthy low price pizza give the mles for the following parameters θspam θsecret spam θsecret non spam θsports non spam θdollar spam gaussian models introduction in this chapter we discuss the multivariate gaussian or multivariate normal mvn which is the most widely used joint probability density function for continuous variables it will form the basis for many of the models we will encounter in later chapters unfortunately the level of mathematics in this chapter is higher than in many other chapters in particular we rely heavily on linear algebra and matrix calculus this is the price one must pay in order to deal with high dimensional data beginners may choose to skip sections marked with a in addition since there are so many equations in this chapter we have put a box around those that are particularly important notation let us brieﬂy say a few words about notation we denote vectors by boldface lower case letters such as x we denote matrices by boldface upper case letters such as x we denote entries in a matrix by non bold upper case letters such as xij all vectors are assumed to be column vectors unless noted otherwise we use xd to denote a column vector created by stacking d scalars similarly if we write x xd where the left hand side is a tall column vector we mean to stack the xi along the rows this is usually written as x xt xt t but that is rather ugly if we write x xd d where the left hand side is a matrix we mean to stack the xi along the columns creating a matrix basics recall from section that the pdf for an mvn in d dimensions is deﬁned by the following figure visualization of a dimensional gaussian density the major and minor axes of the ellipse are deﬁned by the ﬁrst two eigenvectors of the covariance matrix namely and based on figure of bishop the expression inside the exponent is the mahalanobis distance between a data vector x and the mean vector μ we can gain a better understanding of this quantity by performing an eigendecomposition of σ that is we write σ uλut where u is an orthonormal matrix of eigenvectors satsifying ut u i and λ is a diagonal matrix of eigenvalues using the eigendecomposition we have that σ u t λ uλ u ut where ui is the i th column of u containing the i th eigenvector hence we can rewrite the mahalanobis distance as follows x μ t σ x μ x μ t d i uiut λi x μ d d x μ t u ut x μ yi where yi ut x μ recall that the equation for an ellipse in is hence we see that the contours of equal probability density of a gaussian lie along ellipses this is illustrated in figure the eigenvectors determine the orientation of the ellipse and the eigenvalues determine how elogonated it is in general we see that the mahalanobis distance corresponds to euclidean distance in a transformed coordinate system where we shift by μ and rotate by u introduction mle for an mvn we now describe one way to estimate the parameters of an mvn using mle in later sections we will discuss bayesian inference for the parameters which can mitigate overﬁtting and can provide a measure of conﬁdence in our estimates theorem mle for a gaussian if we have n iid samples xi μ σ then the mle for the parameters is given by μˆmle n xi n i n x n σˆ x x x x t x xt x xt that is the mle is just the empirical mean and empirical covariance in the univariate case we get the following familiar results μˆ x n i i x x x x proof to prove this result we will need several results from matrix algebra which we summarize below in the equations a and b are vectors and a and b are matrices also the notation tr a refers to the trace of a matrix which is the sum of its diagonals tr a i aii the last equation is called the cyclic permutation property of the trace operator using this we can derive the widely used trace trick which reorders the scalar inner product xt ax as follows xt ax tr xt ax tr xxt a tr axxt proof we can now begin with the proof the log likelihood is μ σ log p d μ σ n log λ x μ λ x μ where λ σ is the precision matrix i i i using the substitution yi xi μ and the chain rule of calculus we have yi x μ t σ x μ yt σ μ i i yi i i μ hence σ σ t yi n n μ σ x μ i i n μ σ xi i μ μˆ x n i i x so the mle of μ is just the empirical mean now we can use the trace trick to rewrite the log likelihood for λ as follows n i i i n μ λ where log λ tr sμλ n sμ xi μ xi μ t i is the scatter matrix centered on μ taking derivatives of this expression with respect to λ yields λ n λ t st λ t λ σ s n μ so n σˆ xi n i μ xi μ which is just the empirical covariance matrix centered on μ if we plug in the mle μ x since both parameters must be simultaneously optimized we get the standard equation for the mle of a covariance matrix maximum entropy derivation of the gaussian in this section we show that the multivariate gaussian is the distribution with maximum entropy subject to having a speciﬁed mean and covariance see also section this is one reason the gaussian is so widely used the ﬁrst two moments are usually all that we can reliably estimate from data so we want a distribution that captures these properties but otherwise makes as few addtional assumptions as possible to simplify notation we will assume the mean is zero the pdf has the form p x exp xt σ if we deﬁne fij x xixj and λij σ ij for i j d we see that this is in the same form as equation the differential entropy of this distribution using log base e is given by h μ σ ln d σ we now show the mvn has maximum entropy amongst all distributions with a speciﬁed co variance σ theorem let q x be any density satisfying q x xixj σij let p n σ then h q h p proof from cover and thomas we have kl q p q x log q x dx p x h q r q x log p x dx h q r p x log p x dx h q h p where the key step in equation marked with a follows since q and p yield the same moments for the quadratic form encoded by log p x gaussian discriminant analysis one important application of mvns is to deﬁne the the class conditional densities in a generative classiﬁer i e p x y c θ n x μc σc the resulting technique is called gaussian discriminant analysis or gda even though it is a generative not discriminative classiﬁer see section for more on this distinction if σc is diagonal this is equivalent to naive bayes red female blue male red female blue male height a height b figure a height weight data b visualization of gaussians ﬁt to each class of the probability mass is inside the ellipse figure generated by gaussheightweight we can classify a feature vector using the following decision rule derived from equation yˆ x argmax log p y c π log p x θc when we compute the probability of x under each class conditional density we are measuring the distance from x to the center of each class μc using mahalanobis distance this can be thought of as a nearest centroids classiﬁer as an example figure shows two gaussian class conditional densities in representing the height and weight of men and women we can see that the features are correlated as is to be expected tall people tend to weigh more the ellipses for each class contain of the probability mass if we have a uniform prior over classes we can classify a new test vector as follows yˆ x argmin x μc t σ c x μc quadratic discriminant analysis qda the posterior over class labels is given by equation we can gain further insight into this model by plugging in the deﬁnition of the gaussian density as follows p y c x θ c c t thresholding this results in a quadratic function of x the result is known as quadratic discriminant analysis qda figure gives some examples of what the decision boundaries look like in parabolic boundary some linear some quadratic a b figure quadratic decision boundaries in for the and class case figure generated by discrimanalysisdboundariesdemo t t t t figure softmax distribution η t where η at different temperatures t when the temperature is high left the distribution is uniform whereas when the temperature is low right the distribution is spiky with all its mass on the largest element figure generated by linear discriminant analysis lda we now consider a special case in which the covariance matrices are tied or shared across classes σc σ in this case we can simplify equation as follows p y c x θ π exp μt σ xt σ μt σ l exp μt σ μt σ log π l exp xt σ since the quadratic term xt σ is independent of c it will cancel out in the numerator and denominator if we deﬁne γ μt σ log π βc σ then we can write p y c x θ eβt x γc eβt x γ t s η c where η βt x βt x γc and s is the softmax function deﬁned as follows c eηc s η c c ct eηct the softmax function is so called since it acts a bit like the max function to see this let us divide each ηc by a constant t called the temperature then as t we ﬁnd s η t c if c argmaxct ηct otherwise in other words at low temperatures the distribution spends essentially all of its time in the most probable state whereas at high temperatures it visits all states uniformly see figure for an illustration note that this terminology comes from the area of statistical physics where it is common to use the boltzmann distribution which has the same form as the softmax function an interesting property of equation is that if we take logs we end up with a linear function of x the reason it is linear is because the xt σ cancels from the numerator and denominator thus the decision boundary between any two classes say c and c will be a straight line hence this technique is called linear discriminant analysis or lda we can derive the form of this line as follows p y c x θ p y c x θ t t βc x γc βctx γct xt βct β γct γc see figure for some examples an alternative to ﬁtting an lda model and then deriving the class posterior is to directly ﬁt p y x w cat y wx for some c d weight matrix w this is called multi class logistic regression or multinomial logistic regression we will discuss this model in detail in section the difference between the two approaches is explained in section two class lda to gain further insight into the meaning of these equations let us consider the binary case in this case the posterior is given by eβt x p y x θ eβt x eβt x sigm β e t x x the abbreviation lda could either stand for linear discriminant analysis or latent dirichlet allocation sec tion we hope the meaning is clear from text in the language modeling community this model is called a maximum entropy model for reasons explained in section linear boundary all linear boundaries a b figure linear decision boundaries in for the and class case figure generated by discrimanalysisdboundariesdemo figure geometry of lda in the class case where i where sigm η refers to the sigmoid function equation now γ γ μt σ t log π π μ μ t σ μ μ log π π so if we deﬁne w β0 σ x μ μ μ log μ μ t σ then we have wt and hence p y x θ sigm wt x this is closely related to logistic regression which we will discuss in section so the ﬁnal decision rule is as follows shift x by project onto the line w and see if the result is positive or negative if σ then w is in the direction of so we classify the point based on whether its projection is closer to or this is illustrated in figure furthemore if then which is half way between the means if we make then gets closer to so more of the line belongs to class a priori conversely if the boundary shifts right thus we see that the class prior πc just changes the decision threshold and not the overall geometry as we claimed above a similar argument applies in the multi class case the magnitude of w determines the steepness of the logistic function and depends on how well separated the means are relative to the variance in psychology and signal detection theory it is common to deﬁne the discriminability of a signal from the background noise using a quantity called d prime d σ where is the mean of the signal and is the mean of the noise and σ is the standard deviation of the noise if d is large the signal will be easier to discriminate from the noise mle for discriminant analysis we now discuss how to ﬁt a discriminant analysis model the simplest way is to use maximum likelihood the log likelihood function is as follows log p d θ r n i yi c log log n x μc σc we see that this factorizes into a term for π and c terms for each μc and σc hence we can estimate these parameters separately for the class prior we have πˆc nc as with naive bayes for the class conditional densities we just partition the data based on its class label and compute the mle for each gaussian μˆc c i yi c xi σˆ c nc xi i yi c μˆ c xi μˆ c t see discrimanalysisfit for a matlab implementation once the model has been ﬁt you can make predictions using discrimanalysispredict which uses a plug in approximation strategies for preventing overﬁtting the speed and simplicity of the mle method is one of its greatest appeals however the mle can badly overﬁt in high dimensions in particular the mle for a full covariance matrix is singular if nc d and even when nc d the mle can be ill conditioned meaning it is close to singular there are several possible solutions to this problem use a diagonal covariance matrix for each class which assumes the features are conditionally independent this is equivalent to using a naive bayes classiﬁer section use a full covariance matrix but force it to be the same for all classes σc σ this is an example of parameter tying or parameter sharing and is equivalent to lda section use a diagonal covariance matrix and forced it to be shared this is called diagonal covariance lda and is discussed in section use a full covariance matrix but impose a prior and then integrate it out if we use a conjugate prior this can be done in closed form using the results from section this is analogous to the bayesian naive bayes method in section see minka for details fit a full or diagonal covariance matrix by map estimation we discuss two different kinds of prior below project the data into a low dimensional subspace and ﬁt the gaussians there see sec tion for a way to ﬁnd the best most discriminative linear projection we discuss some of these options below regularized lda suppose we tie the covariance matrices so σc σ as in lda and furthermore we perform map estimation of σ using an inverse wishart prior of the form iw diag σˆ mle see section then we have σˆ λdiag σˆ mle λ σˆ mle where λ controls the amount of regularization which is related to the strength of the prior see section for details this technique is known as regularized discriminant analysis or rda hastie et al when we evaluate the class conditional densities we need to compute σˆ and hence σˆ which is impossible to compute if d n however we can use the svd of x section to get around this as we show below note that this trick cannot be applied to qda which is a nonlinear function of x let x udvt be the svd of the design matrix where v is d n u is an n n orthogonal matrix and d is a diagonal matrix of size n furthermore deﬁne the n n matrix z ud this is like a design matrix in a lower dimensional space since we assume n d also deﬁne μz vt μ as the mean of the data in this reduced space we can recover the original mean using μ vμz since vt v vvt i with these deﬁnitions we can rewrite the mle as follows σˆ mle xt x μμt n zvt t zvt vμ vμ t vzt zvt vμ μt vt v zt z μ μt vt vςˆ zvt where σˆ z is the empirical covariance of z hence we can rewrite the map estimate as σˆ map vς zvt σ z λdiag σˆ z λ σˆ z note however that we never need to actually compute the d d matrix σˆ map this is because equation tells us that to classify using lda all we need to compute is p y c x θ exp δc where δ xt β γ β σˆ γ t μ β log π we can compute the crucial βc term for rda without inverting the d d matrix as follows ˆ c map c vς zvt vς μ vς z c where μz c vt μc is the mean of the z matrix for data belonging to class c see rdafit for the code diagonal lda a simple alternative to rda is to tie the covariance matrices so σc σ as in lda and then to use a diagonal covariance matrix for each class this is called the diagonal lda model and is equivalent to rda with λ the corresponding discriminant function is as follows compare to equation δ x log p x y c θ xj μcj log π typically we set μˆcj xcj and which is the pooled empirical variance of feature j j j pooled across classes deﬁned by c c i yi c xij xcj n c in high dimensional settings this model can work much better than lda and rda bickel and levina number of genes test train cv figure error versus amount of shrinkage for nearest shrunken centroid classiﬁer applied to the srbct gene expression data based on figure of hastie et al figure generated by shrunkencentroidssrbctdemo nearest shrunken centroids classiﬁer one drawback of diagonal lda is that it depends on all of the features in high dimensional problems we might prefer a method that only depends on a subset of the features for reasons of accuracy and interpretability one approach is to use a screening method perhaps based on mutual information as in section we now discuss another approach to this problem known as the nearest shrunken centroids classiﬁer hastie et al the basic idea is to perform map estimation for diagonal lda with a sparsity promoting laplace prior see section more precisely deﬁne the class speciﬁc feature mean μcj in terms of the class independent feature mean mj and a class speciﬁc offset δcj thus we have μcj mj δcj we will then put a prior on the δcj terms to encourage them to be strictly zero and compute a map estimate if for feature j we ﬁnd that δcj for all c then feature j will play no role in the classiﬁcation decision since μcj will be independent of c thus features that are not discriminative are automatically ignored the details can be found in hastie et al and greenshtein and park see shrunkencentroidsfit for some code let us give an example of the method in action based on hastie et al consider the problem of classifying a gene expression dataset which genes classes training samples and test samples using a diagonal lda classiﬁer produces errors on the test set using the nearest shrunken centroids classiﬁer produced errors on the test set for a range of λ values see figure more importantly the model is sparse and hence more interpretable figure plots an unpenalized estimate of the difference dcj in gray as well as the shrunken estimates δcj in blue these estimates are computed using the value of λ estimated by cv we see that only genes are used out of the original now consider an even harder problem with genes a training set of patients a test set of patients and different types of cancer ramaswamy et al hastie et al hastie et al report that nearest shrunken centroids produced errors on the test class class a b class class c d figure proﬁle of the shrunken centroids corresponding to λ cv optimal in fig ure this selects genes based on figure of hastie et al figure generated by shrunkencentroidssrbctdemo set using genes and that rda section produced errors on the test set using all genes the pmtk function cancerhighdimclassifdemo can be used to reproduce these numbers inference in jointly gaussian distributions given a joint distribution p it is useful to be able to compute marginals p and conditionals p we discuss how to do this below and then give some applications these operations take o time in the worst case see section for faster methods statement of the result theorem marginals and conditionals of an mvn suppose x is jointly gaussian with parameters μ σ λ σ then the marginals are given by p n p n and the posterior conditional is given by equation is of such crucial importance in this book that we have put a box around it so you can easily ﬁnd it for the proof see section we see that both the marginal and conditional distributions are themselves gaussian for the marginals we just extract the rows and columns corresponding to or for the conditional we have to do a bit more work however it is not that complicated the conditional mean is just a linear function of and the conditional covariance is just a constant matrix that is independent of we give three different but equivalent expressions for the posterior mean and two different but equivalent expressions for the posterior covariance each one is useful in different circumstances examples below we give some examples of these equations in action which will make them seem more intuitive marginals and conditionals of a gaussian let us consider a example the covariance matrix is σ the marginal p is a gaussian obtained by projecting the joint distribution onto the line p n p p p a b c figure a a joint gaussian distribution p with a correlation coefficient of we plot the contour and the principal axes b the unconditional marginal p c the conditional p n obtained by slicing a at height figure generated by suppose we observe the conditional p is obtained by slicing the joint distribution through the line see figure p n if σ we get p n ρ in figure we show an example where ρ μ and we see that e which makes sense since ρ means that we believe that if increases by beyond its mean then increases by we also see var this also makes sense our uncertainty about has gone down since we hav e learned omething about indirectly by observing if ρ we get p interpolating noise free data suppose we want to estimate a function deﬁned on the interval t such that yi f ti for n observed points ti we assume for now that the data is noise free so we want to interpolate it that is ﬁt a function that goes exactly through the data see section for the noisy data case the question is how does the function behave in between the observed data points it is often reasonable to assume that the unknown function is smooth in chapter we shall see how to encode priors over functions and how to update such a prior with observed values to get a posterior over functions but in this section we take a simpler approach which is adequate for map estimation of functions deﬁned on inputs we follow the presentation of calvetti and somersalo we start by discretizing the problem first we divide the support of the function into d equal subintervals we then deﬁne t xj f sj sj jh h d j d a b figure interpolating noise free data using a gaussian with prior precision λ a λ b λ see also figure based on figure of calvetti and somersalo figure generated by gaussinterpdemo we can encode our smoothness prior by assuming that xj is an average of its neighbors xj and xj plus some gaussian noise xj xj xj ej j d where e λ i the precision term λ controls how much we think the function will vary a large λ corresponds to a belief that the function is very smooth a small λ corresponds to a belief that the function is quite wiggly in vector form the above equation can be written as follows lx e where l is the d d second order ﬁnite difference matrix l the corresponding prior has the form t we will henceforth assume we have scaled l by λ so we can ignore the λ term and just write λ lt l for the precision matrix note that although x is d dimensional the precision matrix λ only has rank d thus this is an improper prior known as an intrinsic gaussian random ﬁeld see section for more information however providing we observe n data points the posterior will be proper now let be the n noise free observations of the function and be the d n unknown function values without loss of generality assume that the unknown variables are ordered ﬁrst then the known variables then we can partition the l matrix as follows l r d d n r d n we can also partition the precision matrix of the joint distribution t lt lt using equation we can write the conditional distribution as follows p n λ lt λ note that we can compute the mean by solving the following system of linear equations this is efficient since is tridiagonal figure gives an illustration of these equations we see that the posterior mean equals the observed data at the speciﬁed points and smoothly interpolates in between as desired iit is also interesting to plot the pointwise marginal credibility intervals μj data we also see that the variance goes up as we decrease the precision of the prior λ in terestingly λ has no effect on the posterior mean since it cancels out when multiplying and by contrast when we consider noisy data in section we will see that the prior precision affects the smoothness of posterior mean estimate the marginal credibility intervals do not capture the fact that neighboring locations are correlated we can represent that by drawing complete functions i e vectors x from the posterior and plotting them these are shown by the thin lines in figure these are not quite as smooth as the posterior mean itself this is because the prior only penalizes ﬁrst order differences see section for further discussion of this point data imputation suppose we are missing some entries in a design matrix if the columns are correlated we can use the observed entries to predict the missing entries figure shows a simple example we sampled some data from a dimensional gaussian and then deliberately hid of the data in each row we then inferred the missing entries given the observed entries using the true generating model more precisely for each row i we compute p xhi xvi θ where hi and vi are the indices of the hidden and visible entries in case i from this we compute the marginal distribution of each missing variable p xhij xvi θ we then plot the mean of this distribution xˆij e xj xvi θ this represents our best guess about the true value of that entry in the observed imputed truth figure illustration of data imputation left column visualization of three rows of the data matrix with missing entries middle column mean of the posterior predictive based on partially observed data in that row but the true model parameters right column true values figure generated by gaussimputationdemo sense that it minimizes our expected squared error see section for details figure shows that the estimates are quite close to the truth of course if j vi the expected value is equal to the observed value xˆij xij we can use var xhij xvi θ as a measure of conﬁdence in this guess although this is not shown alternatively we could draw multiple samples from p xhi xvi θ this is called multiple imputation in addition to imputing the missing entries we may be interested in computing the like lihood of each partially observed row in the table p xvi θ which can be computed using equation this is useful for detecting outliers atypical observations information form suppose x μ σ one can show that e x μ is the mean vector and cov x σ is the covariance matrix these are called the moment parameters of the distribution however it is sometimes useful to use the canonical parameters or natural parameters deﬁned as λ σ ξ σ we can convert back to the moment parameters using μ λ σ λ using the canonical parameters we can write the mvn in information form i e in exponential family form deﬁned in section nc x ξ λ d λ exp xt λx ξt λ ξ l where we use the notation c to distinguish from the moment parameterization it is also possible to derive the marginalization and conditioning formulas in information form we ﬁnd p nc 1ξ1 88 p nc λ11 thus we see that marginalization is easier in moment form and conditioning is easier in information form another operation that is signiﬁcantly easier in information form is multiplying two gaussians one can show that nc ξf λf nc ξg λg nc ξf ξg λf λg however in moment form things are much messier proof of the result we now prove theorem readers who are intimidated by heavy matrix algebra can safely skip this section we ﬁrst derive some results that we will need here and elsewhere in the book we will return to the proof at the end inverse of a partitioned matrix using schur complements the key tool we need is a way to invert a partitioned matrix this can be done using the following result theorem inverse of a partitioned matrix consider a general partitioned matrix m e f g h where we assume e and h are invertible we have m where m h m h h m h h h m h e e m e e m e m e m e m h e fh m e h ge we say that m h is the schur complement of m wrt h equation is called the partitioned inverse formula proof if we could block diagonalize m it would be easier to invert to zero out the top right block of m we can pre multiply as follows i fh e f e fh similarly to zero out the bottom left we can post multiply as follows e fh i e fh g h h i h putting it all together we get i fh e f i e fh x m z taking the inverse of both sides yields w z w and hence m zw substituting in the deﬁnitions we get e f i m h i fh m h h m h h i fh m h m h h m h h h m h alternatively we could have decomposed the matrix m in terms of e and m e h ge yielding e f g h e e m e e m e m e m e the matrix inversion lemma we now derive some useful corollaries of the above result corollary matrix inversion lemma consider a general partitioned matrix m e f g h where we assume e and h are invertible we have e fh e e h ge e fh e h ge e fh h ge h e the ﬁrst two equations are known as the matrix inversion lemma or the sherman morrison woodbury formula the third equation is known as the matrix determinant lemma a typical application in machine learning statistics is the following let e σ be a n n diagonal matrix let f gt x of size n d where n d and let h i then we have σ xxt σ σ i xt σ σ the lhs takes o n time to compute the rhs takes time o to compute another application concerns computing a rank one update of an inverse matrix let h a scalar f u a column vector and g vt a row vector then we have e uvt e e vt e e e e e vt e this is useful when we incrementally add a data vector to a design matrix and want to update our sufficient statistics one can derive an analogous formula for removing a data vector proof to prove equation we simply equate the top left block of equation and equa tion to prove equation we simple equate the top right blocks of equations and the proof of equation is left as an exercise proof of gaussian conditioning formulas we can now return to our original goal which is to derive equation let us factor the joint p as p p as follows e exp μ t σ x using equation the above exponent becomes x μ t i σ σ i i exp x μ σ σ x μ σ σ x μ σ σ x μ exp x μ t σ x μ this is of the form exp quadratic form in exp quadratic form in hence we have successfully factorized the joint as p p p n n where the parameters of the conditional distribution can be read off from the above equations using σ σ11 1σ21 we can also use the fact that m m h h to check the normalization constants are correct σ σ σ22 σ22 σ σ where dim and dim σ we leave the proof of the other forms of the result in equation as an exercise linear gaussian systems suppose we have two variables x and y let x rdx be a hidden variable and y rdy be a noisy observation of x let us assume we have the following prior and likelihood where a is a matrix of size dy dx this is an example of a linear gaussian system we can represent this schematically as x y meaning x generates y in this section we show how to invert the arrow that is how to infer x from y we state the result below then give several examples and ﬁnally we derive the result we will see many more applications of these results in later chapters statement of the result theorem bayes rule for linear gaussian systems given a linear gaussian system as in equation the posterior p x y is given by the following in addition the normalization constant p y is given by for the proof see section examples in this section we give some example applications of the above result inferring an unknown scalar from noisy measurements suppose we make n noisy measurements yi of some underlying quantity x let us assume the measurement noise has ﬁxed precision λy so the likelihood is p yi x n yi x λ y now let us use a gaussian prior for the value of the unknown source p x n x λ we want to compute p x yn we can convert this to a form that lets us apply bayes rule for gaussians by deﬁning y yn a an n row vector of and σ y diag λyi then we get p x y n x μn λ λn nλy nλyy n λy μ y μ n λn nλy nλy these equations are quite intuitive the posterior precision λn is the prior precision plus n units of measurement precision λy also the posterior mean μn is a convex combination of the mle y and the prior mean this makes it clear that the posterior mean is a compromise between the mle and the prior if the prior is weak relative to the signal strength is small relative to λy we put more weight on the mle if the prior is strong relative to the signal strength is large relative to λy we put more weight on the prior this is illustrated in figure which is very similar to the analogous results for the beta binomial model in figure note that the posterior mean is written in terms of n λyy so having n measurements each of precision λy is like having one measurement with value y and precision nλy we can rewrite the results in terms of the posterior variance rather than posterior precision prior variance prior variance figure inference about x given a noisy observation y a strong prior n the posterior mean is shrunk towards the prior mean which is a weak prior n the posterior mean is similar to the mle figure generated by as follows p x d n x μn τ n n τ nτ ny nτ where τ is the prior variance and τ λn is the posterior variance n we can also compute the posterior sequentially by updating after each observation if n we can rewrite the posterior after seeing a single observation as follows where we deﬁne σy τ and τ to be the variances of the likelihood prior and posterior p x y n x σy σy μ σ y we can rewrite the posterior mean in different ways σy σy y σy μ y μ σy σy y y μ σy the ﬁrst equation is a convex combination of the prior and the data the second equation is the prior mean adjusted towards the data the third equation is the data adjusted towards the prior mean this is called shrinkage these are all equivalent ways of expressing the tradeoff between likelihood and prior if is small relative to σy corresponding to a strong prior the amount of shrinkage is large see figure a whereas if is large relative to σy corresponding to a weak prior the amount of shrinkage is small see figure b another way to quantify the amount of shrinkage is in terms of the signal to noise ratio which is deﬁned as follows snr e e σy where x is the true signal y x e is the observed signal and e σy is the noise term inferring an unknown vector from noisy measurements now consider n vector valued observations yi n x σy and a gaussian prior x setting a i b and using y for the effective observation with precision n σy we have p x yn n x μn σn σ σ n σ y μn σn σ y n y σ 144 see figure for a example we can think of x as representing the true but unknown location of an object in space such as a missile or airplane and the yi as being noisy observations such as radar blips as we receive more blips we are better able to localize the source in section we will see how to extend this example to track moving objects using the famous kalman ﬁlter algorithm now suppose we have multiple measuring devices and we want to combine them together this is known as sensor fusion if we have multiple observations with different covariances cor responding to sensors with different reliabilities the posterior will be an appropriate weighted average of the data consider the example in figure we use an uninformative prior on x namely p x 1010i2 we get noisy observations x σy and x σy we then compute p x in figure a we set σy σy so both sensors are equally reliable in this case the posterior mean is half way between the two observations and in figure b we set σy and σy so sensor is more reliable than sensor in this case the posterior mean is closer to in figure c we set σy σy so sensor is more reliable in the component vertical direction and sensor is more reliable in the component horizontal direction in this case the posterior mean uses vertical component and horizontal component data prior post after obs figure illustration of bayesian inference for the mean of a gaussian a the data is generated from yi x σy where x t and σy we assume the sensor noise covariance σy is known but x is unknown the black cross represents x b the prior is p x n x c we show the posterior after data points have been observed figure generated by a b c figure we observe red cross and green cross and infer e μ θ black cross a equally reliable sensors so the posterior mean estimate is in between the two circles b sensor is more reliable so the estimate shifts more towards the green circle c sensor is more reliable in the vertical direction sensor is more reliable in the horizontal direction the estimate is an appropriate combination of the two measurements figure generated by note that this technique crucially relies on modeling our uncertainty of each sensor comput ing an unweighted average would give the wrong result however we have assumed the sensor precisions are known when they are not we should model out uncertainty about and as well see section for details interpolating noisy data we now revisit the example of section this time we no longer assume noise free observations instead let us assume that we obtain n noisy observations yi without loss of generality assume these correspond to xn we can model this setup as a linear gaussian system y ax e where e σy σy is the observation noise and a is a n d projection matrix that selects out the observed elements for example if n and d we have a using the same improper prior as before σx lt l we can easily compute the posterior mean and variance in figure we plot the posterior mean posterior variance and some posterior samples now we see that the prior precision λ effects the posterior mean as well as the posterior variance in particular for a strong prior large λ the estimate is very smooth and the uncertainty is low but for a weak prior small λ the estimate is wiggly and the uncertainty away from the data is high the posterior mean can also be computed by solving the following optimization problem min x n i xi yi d j i xj xj xj xj where we have deﬁned and xd xd for notational simplicity we recognize this as a discrete approximation to the following problem min r f t y t λ r f t where f t is the ﬁrst derivative of f the ﬁrst term measures ﬁt to the data and the second term penalizes functions that are too wiggly this is an example of tikhonov regularization which is a popular approach to functional data analysis see chapter for more sophisticated approaches which enforce higher order smoothness so the resulting samples look less jagged proof of the result we now derive equation the basic idea is to derive the joint distribution p x y p x p y x and then to use the results from section for computing p x y in more detail we proceed as follows the log of the joint distribution is as follows dropping irrelevant constants log p x y x μ t σ x μ y ax b t σ y ax b this is clearly a joint gaussian distribution since it is the exponential of a quadratic form expanding out the quadratic terms involving x and y and ignoring linear and constant terms we have q xt σ yt σ ax t σ ax yt σ x t σ at σ at σ x y σy a σy y x t σ x a b figure interpolating noisy data noise variance using a gaussian with prior precision λ a λ b λ see also figure based on figure of calvetti and somersalo figure generated by gaussinterpnoisydemo see also splinebasisdemo where the precision matrix of the joint is deﬁned as σ σ x at σ y at σ y σ σ λ λxx λxy λyx λyy from equation and using the fact that μy aμx b we have xx x y σx y λxxμx λxy y μy σx y σ x at σ y y b digression the wishart distribution the wishart distribution is the generalization of the gamma distribution to positive deﬁnite matrices press press has said the wishart distribution ranks next to the multi variate normal distribution in order of importance and usefuleness in multivariate statistics we will mostly use it to model our uncertainty in covariance matrices σ or their inverses λ σ the pdf of the wishart is deﬁned as follows wi λ s ν λ ν d exp tr λs here ν is called the degrees of freedom and s is the scale matrix we shall get more intuition for these parameters shortly the normalization constant for this distribution which requires integrating over all symmetric pd matrices is the following formidable expression zwi ν s ν where γd a is the multivariate gamma function d γd x πd d γ x i i hence a γ a and γ ν tt γ i the normalization constant only exists and hence the pdf is only well deﬁned if ν d there is a connection between the wishart distribution and the gaussian in particular let xi n σ then the scatter matrix s n xixt has a wishart distribution s wi σ hence e s n σ more generally one can show that the mean and mode of wi s ν are given by mean νs mode ν d s where the mode only exists if ν d if d the wishart reduces to the gamma distribution wi λ ν ga λ ν inverse wishart distribution recall that we showed exercise that if λ ga a b then that ig a b similarly if σ wi s ν then σ iw s ν d where iw is the inverse wishart the multidimensional generalization of the inverse gamma it is deﬁned as follows for ν d and s iw σ s ν σ ν d exp tr s ziw s ν ν one can show that the distribution has these properties mean s ν d mode s ν d if d this reduces to the inverse gamma iw s ν ig ν s wi dof s e a b figure visualization of the wishart distribution left some samples from the wishart distribution σ wi s ν where s 0262 6477 and ν right plots of the marginals which are gamma and the approximate sample based marginal on the correlation coefficient if ν there is a lot of uncertainty about the value of the correlation coefficient ρ see the almost uniform distribution on the sampled matrices are highly variable and some are nearly singular as ν increases the sampled matrices are more concentrated on the prior s figure generated by wiplotdemo visualizing the wishart distribution since the wishart is a distribution over matrices it is hard to plot as a density function however we can easily sample from it and in the case we can use the eigenvectors of the resulting matrix to deﬁne an ellipse as explained in section see figure for some examples for higher dimensional matrices we can plot marginals of the distribution the diagonals of a wishart distributed matrix have gamma distributions so are easy to plot it is hard in general to work out the distribution of the off diagonal elements but we can sample matrices from the distribution and then compute the distribution empirically in particular we can convert each sampled matrix to a correlation matrix and thus compute a monte carlo approximation section to the expected correlation coefficients e rij s r σ ij s where σ wi σ ν and r σ converts matrix σ into a correlation matrix σij rij ii σjj we can then use kernel density estimation section to produce a smooth approximation to the univariate density e rij for plotting purposes see figure for some examples inferring the parameters of an mvn so far we have discussed inference in a gaussian assuming the parameters θ μ σ are known we now discuss how to infer the parameters themselves we will assume the data has the form xi μ σ for i n and is fully observed so we have no missing data see section for how to estimate parameters of an mvn in the presence of missing values to simplify the presentation we derive the posterior in three parts ﬁrst we compute p μ d σ then we compute p σ d μ ﬁnally we compute the joint p μ σ d posterior distribution of μ we have discussed how to compute the mle for μ we now discuss how to compute its posterior which is useful for modeling our uncertainty about its value the likelihood has the form p d μ n x μ n σ for simplicity we will use a conjugate prior which in this case is a gaussian in particular if p μ μ then we can derive a gaussian posterior for μ based on the results in section we get p μ d σ n μ mn vn v v n σ mn vn σ n x v this is exactly the same process as inferring the location of an object based on noisy radar blips except now we are inferring the mean of a distribution based on noisy samples to a bayesian there is no difference between uncertainty about parameters and uncertainty about anything else we can model an uninformative prior by setting i in this case we have p μ d σ x n σ so the posterior mean is equal to the mle we also see that the posterior variance goes down as n which is a standard result from frequentist statistics posterior distribution of σ we now discuss how to compute p σ d μ the likelihood has the form p d μ σ σ n exp tr s σ the corresponding conjugate prior is known as the inverse wishart distribution section recall that this has the following pdf iw σ s ν σ d exp tr s σ here d is the degrees of freedom dof and is a symmetric pd matrix we see that s plays the role of the prior scatter matrix and d controls the strength of the prior and hence plays a role analogous to the sample size n n d n d n d figure estimating a covariance matrix in d dimensions using n samples we plot the eigenvalues in descending order for the true covariance matrix solid black the mle dotted blue and the map estimate dashed red using equation with λ we also list the condition number of each matrix in the legend based on figure of schaefer and strimmer figure generated by shrinkcovdemo multiplying the likelihood and prior we ﬁnd that the posterior is also inverse wishart p σ d μ σ n exp tr σ σ d exp tr σ σ n d exp tr σ s s iw σ sn νn νn n s sμ in words this says that the posterior strength νn is the prior strength plus the number of observations n and the posterior scatter matrix sn is the prior scatter matrix plus the data scatter matrix sμ map estimation we see from equation that σˆ mle is a rank min n d matrix if n d this is not full rank and hence will be uninvertible and even if n d it may be the case that σˆ is ill conditioned meaning it is nearly singular to solve these problems we can use the posterior mode or mean one can show using techniques analogous to the derivation of the mle that the map estimate is given by sn sμ σ map νn d n if we use an improper uniform prior corresponding to and we recover the mle let us now consider the use of a proper informative prior which is necessary whenever d n is large say bigger than let μ x so sμ sx then we can rewrite the map estimate as a convex combination of the prior mode and the mle to see this let be the prior mode then the posterior mode can be rewritten as sx n s σ λς λ σˆ map n n n n mle where λ controls the amount of shrinkage towards the prior this begs the question where do the parameters of the prior come from it is common to set λ by cross validation alternatively we can use the closed form formula provided in ledoit and wolf a schaefer and strimmer which is the optimal frequentist estimate if we use squared loss this is arguably not the most natural loss function for covariance matrices because it ignores the postive deﬁnite constraint but it results in a simple estimator which is implemented in the pmtk function shrinkcov we discuss bayesian ways of estimating λ later as for the prior covariance matrix it is common to use the following data dependent prior diag σˆ mle in this case the map estimate is given by σˆ map σˆ mle i j if i j λ σˆ mle i j otherwise thus we see that the diagonal entries are equal to their ml estimates and the off diago nal elements are shrunk somewhat towards this technique is therefore called shrinkage estimation or regularized estimation the beneﬁts of map estimation are illustrated in figure we consider ﬁtting a dimen sional gaussian to n n and n data points we see that the map estimate is always well conditioned unlike the mle in particular we see that the eigenvalue spectrum of the map estimate is much closer to that of the true matrix than the mle the eigenvectors however are unaffected the importance of regularizing the estimate of σ will become apparent in later chapters when we consider ﬁtting covariance matrices to high dimensional data univariate posterior in the case the likelihood has the form n p d n exp xi μ i the standard conjugate prior is the inverse gamma distribution which is just the scalar version of the inverse wishart ig a b exp figure sequential updating of the posterior for starting from an uninformative prior the data was generated from a gaussian with known mean μ and unknown variance figure generated by multiplying the likelihood and the prior we see that the posterior is also ig p d ig an bn an n n bn x i i μ see figure for an illustration the form of the posterior is not quite as pretty as the multivariate case because of the factors of this arises because iw ig another problem with using the ig distribution is that the strength of the prior is encoded in both and to avoid both of these problems it is common in the statistics literature to use an alternative parameterization of the ig distribution known as the scaled inverse chi squared distribution this is deﬁned as follows χ ig exp here controls the strength of the prior and encodes the value of the prior with this prior the posterior becomes p d μ χ νn νn n n xi μ we see that the posterior dof νn is the prior dof plus n and the posterior sum of squares is the prior sum of squares plus the data sum of squares we can emulate an uninformative prior p σ by setting which makes intuitive sense since it corresponds to a zero virtual sample size posterior distribution of μ and σ we now discuss how to compute p μ σ these results are a bit complex but will prove useful later on in this book feel free to skip this section on a ﬁrst reading likelihood the likelihood is given by p d μ σ nd σ exp now one can show that i xi μ t σ xi μ n xi μ t σ xi μ tr σ n x μ t σ x μ i hence we can rewrite the likelihood as follows p d μ σ nd σ n exp n μ x t σ μ x exp n tr σ we will use this form below prior the obvious prior to use is the following p μ σ n μ iw σ unfortunately this is not conjugate to the likelihood to see why note that μ and σ appear together in a non factorized way in the likelihood hence they will also be coupled together in the posterior the above prior is sometimes called semi conjugate or conditionally conjugate since both conditionals p μ σ and p σ μ are individually conjugate to create a full conjugate prior we need to use a prior where μ and σ are dependent on each other we will use a joint distribution of the form p μ σ p σ p μ σ looking at the form of the likelihood equation equation we see that a natural conjugate prior has the form of a normal inverse wishart or niw distribution deﬁned as follows niw μ σ n μ κ σ iw σ σ exp μ m t σ μ m d σ d zniw exp μ m t σ μ m tr σ zniw d where γd a is the multivariate gamma function the parameters of the niw can be interpreted as follows is our prior mean for μ and is how strongly we believe this prior and is proportional to our prior mean for σ and is how strongly we believe this prior one can show minka that the improper uninformative prior has the form lim μ σ k iw σ k σ k σ niw μ σ 208 in practice it is often better to use a weakly informative data dependent prior a common choice see e g chipman et al fraley and raftery is to use diag sx n and d to ensure e σ and to set x and to some small number such as although this prior has four parameters there are really only three free parameters since our uncertainty in the mean is proportional to the variance in particular if we believe that the variance is large then our uncertainty in μ must be large too this makes sense intuitively since if the data has large spread it may be hard to pin down its mean see also exercise where we will see the three free parameters more explicitly if we want separate control over our conﬁdence in μ and σ we must use a semi conjugate prior posterior the posterior can be shown exercise to be niw with updated parameters p μ σ d niw μ σ mn κn νn sn n x n m m x n n n n κn n νn n s s s x m x m t n x n t t s κnmnmn where we have deﬁned s n xixt as the uncentered sum of squares matrix this is easier to update incrementally than the centered version this result is actually quite intuitive the posterior mean is a convex combination of the prior mean and the mle with strength n and the posterior scatter matrix sn is the prior scatter matrix plus the empirical scatter matrix sx plus an extra term due to the uncertainty in the mean which creates its own virtual scatter matrix posterior mode the mode of the joint distribution has the following form argmax p μ σ d mn sn νn d if we set this reduces to sx argmax p μ σ x n d the corresponding estimate σˆ is almost the same as equation but differs by in the denominator because this is the mode of the joint not the mode of the marginal posterior marginals the posterior marginal for σ is simply p σ d r p μ σ d dμ iw σ sn νn the mode and mean of this marginal are given by sn sn σ e σ map νn d νn d one can show that the posterior marginal for μ has a multivariate student t distribution p μ d r p μ σ d dς t μ mn sn κn νn d νn d this follows from the fact that the student distribution can be represented as a scaled mixture of gaussians see equation figure the distribution is the prior mean and is how strongly we believe this is the prior variance and is how strongly we believe this a notice that the contour plot underneath the surface is shaped like a squashed egg b we increase the strength of our belief in the mean so it gets narrower c we increase the strength of our belief in the variance so it gets narrower figure generated by posterior predictive the posterior predictive is given by p x p x d p d so it can be easily evaluated in terms of a ratio of marginal likelihoods it turns out that this ratio has the form of a multivariate student t distribution 220 p x d r r n x μ σ niw μ σ mn κn νn sn dμdς t x mn κn sn κn νn d νn d the student t has wider tails than a gaussian which takes into account the fact that σ is unknown however this rapidly becomes gaussian like posterior for scalar data we now specialise the above results to the case where xi is these results are widely used in the statistics literature as in section it is conventional not to use the normal inverse wishart but to use the normal inverse chi squared or nix distribution deﬁned by μ n μ χ exp see figure for some plots along the μ axis the distribution is shaped like a gaussian and along the axis the distribution is shaped like a χ the contours of the joint density have a squashed egg appearance interestingly we see that the contours for μ are more peaked for small values of which makes sense since if the data is low variance we will be able to estimate its mean more reliably one can show that the posterior is given by p μ d μ mn κn νn m nx n κn κn n νn n n ν ν x x m x the posterior marginal for is just p d r p μ d dμ χ νn with the posterior mean given by e d νn has a student t distribution which follows from the scale mixture representation of the student p μ d r p μ d t μ mn κn νn with the posterior mean given by e μ mn let us see how these results look if we use the following uninformative prior p μ p μ p σ μ with this prior the posterior has the form p μ d μ mn x κn n νn n where x n x σˆ is the the sample standard deviation in section we show that this is an unbiased estimate of the variance hence the marginal posterior for the mean is given by p μ d t μ x n n and the posterior variance of μ is νn n var μ d ν σn n n n the square root of this is called the standard error of the mean i var μ d n thus an approximate posterior credible interval for the mean is i μ d x n bayesian credible intervals are discussed in more detail in section they are contrasted with frequentist conﬁdence intervals in section bayesian t test suppose we want to test the hypothesis that μ for some known value often given values xi μ this is called a two sided one sample t test a simple way to perform such a test is just to check if μ if it is not then we can be sure that μ a more common scenario is when we want to test if two paired samples have the same mean more precisely suppose yi and zi we want to determine if μ using xi yi zi as our data we can evaluate this quantity as follows p μ μ d r p μ d dμ this is called a one sided paired t test for a similar approach to unpaired tests comparing the difference in binomial proportions see section to calculate the posterior we must specify a prior suppose we use an uninformative prior as we showed above we ﬁnd that the posterior marginal on μ has the form p μ d t μ x n n now let us deﬁne the following t statistic t n where the denominator is the standard error of the mean we see that p μ d fn t where fν t is the cdf of the standard student t distribution t ν a more complex approach is to perform bayesian model comparison that is we compute the bayes factor described in section p d p d where is the point null hypothesis that μ and is the alternative hypothesis that μ see gonen et al rouder et al for details connection with frequentist statistics if we use an uninformative prior it turns out that the above bayesian analysis gives the same result as derived using frequentist methods we discuss frequentist statistics in chapter speciﬁcally from the above results we see that μ x n this has the same form as the sampling distribution of the mle μ x is n μ tn the reason is that the student distribution is symmetric in its ﬁrst two arguments so x μ ν μ x ν hence statements about the posterior for μ have the same form as statements about the sampling distribution of x consequently the one sided p value deﬁned in sec tion returned by a frequentist test is the same as p μ returned by the bayesian method see bayesttestdemo for an example despite the superﬁcial similarity these two results have a different interpretation in the bayesian approach μ is unknown and x is ﬁxed whereas in the frequentist approach x is unknown and μ is ﬁxed more equivalences between frequentist and bayesian inference in simple models using uninformative priors can be found in box and tiao see also section sensor fusion with unknown precisions in this section we apply the results in section to the problem of sensor fusion in the case where the precision of each measurement device is unknown this generalizes the results of section where the measurement model was assumed to be gaussian with known precision the unknown precision case turns out to give qualitatively different results yielding a potentially multi modal posterior as we will see our presentation is based on minka suppose we want to pool data from multiple sources to estimate some quantity μ r but the reliability of the sources is unknown speciﬁcally suppose we have two different measurement devices x and y with different precisions xi μ μ λ x and yi μ μ λ y we make two independent measurements with each device which turn out to be 245 we will use a non informative prior for μ p μ which we can emulate using an inﬁnitely broad gaussian p μ μ if the λx and λy terms were known then the posterior would be gaussian p μ d λx λy n μ mn λ λn nxλx nyλy m λxnxx λynyy n nxλx nyλy where nx is the number of x measurements ny is the number of y measurements x nx xi and y ny yi this result follows because the posterior precision is the sum of the measurement precisions and the posterior mean is a weighted sum of the prior mean which is and the data means however the measurement precisions are not known initially we will estimate them by maximum likelihood the log likelihood is given by μ λx λy log λx λx x i i μ log λy λy y i i μ the mle is obtained by solving the following simultaneous equations μ λxnx x μ λyny y μ nx λx λx nx xi i ny y μ μ λy this gives λy ny i i nxλˆxx nyλˆyy μˆ nxλˆx nyλˆy λˆx λˆ xi n i y μˆ μˆ we notice that the mle for μ has the same form as the posterior mean mn we can solve these equations by ﬁxed point iteration let us initialize by estimating λx and λy where nx xi x and ny yi y using this we get μˆ so p μ d λˆx λˆy n μ 0554 if we now iterate we converge to λˆx λˆy p μ λˆx λˆy μ 0798 the plug in approximation to the posterior is plotted in figure a this weights each sensor according to its estimated precision since sensor y was estimated to be much less reliable than sensor x we have e μ d λˆx λˆy x so we effectively ignore the y sensor now we will adopt a bayesian approach and integrate out the unknown precisions rather than trying to estimate them that is we compute p μ d p μ r p dx μ λx p λx μ dλxl r p dy μ λy p λy μ dλyl we will use uninformative jeffrey priors p μ p λx μ λx and p λy μ λy since the x and y terms are symmetric we will just focus on one of them the key integral is i r p dx μ λx p λx μ dλx r λ x nxλx nx exp nx λ x μ nx dλ x exploiting the fact that nx this simpliﬁes to x x x i r λ x exp λx x μ dλx we recognize this as proportional to the integral of an unnormalized gamma density ga λ a b λa λb where a and b x μ hence the integral is proportional to the normalizing constant of the gamma distribution γ a b a so we get i r p dx μ λx p λx μ dλx x μ and the posterior becomes p μ d x μ y μ 262 the exact posterior is plotted in figure b we see that it has two modes one near x and one near y these correspond to the beliefs that the x sensor is more reliable than the y one and vice versa the weight of the ﬁrst mode is larger since the data from the x sensor agree more with each other so it seems slightly more likely that the x sensor is the reliable one they obviously cannot both be reliable since they disagree on the values that they are reporting however the bayesian solution keeps open the possibility that the y sensor is the more reliable one from two measurements we cannot tell and choosing just the x sensor as the plug in approximation does results in over conﬁdence a posterior that is too narrow exercises exercise uncorrelated does not imply independent let x u and y clearly y is dependent on x in fact y is uniquely determined by x however show that ρ x y hint if x u a b then e x a b and var x b a exercise uncorrelated and gaussian does not imply independent unless jointly gaussian let x and y wx where p w p w it is clear that x and y are not independent since y is a function of x a show y n a b figure posterior for μ a plug in approximation b exact posterior figure generated by sensorfusionunknownprec b show cov x y thus x and y are uncorrelated but dependent even though they are gaussian hint use the deﬁnition of covariance cov x y e xy e x e y and the rule of iterated expectation e xy e e xy w exercise correlation coefficient is between and prove that ρ x y exercise correlation coefficient for linearly related variables is show that if y ax b for some parameters a and b then ρ x y similarly show that if a then ρ x y exercise normalization constant for a multidimensional gaussian prove that the normalization constant for a d dimensional gaussian is given by d σ exp x μ t σ x μ dx hint diagonalize σ and use the fact that σ i λi to write the joint pdf as a product of d one dimensional gaussians in a transformed coordinate system you will need the change of variables formula finally use the normalization constant for univariate gaussians exercise bivariate gaussian let x n μ σ where x and ρσ1σ2 σ ρσ1σ2 where ρ is the correlation coefficient show that the pdf is given by p σ 280 raw standarized 260 220 whitened figure a height weight data for the men b standardized c whitened exercise conditioning a bivariate gaussian consider a bivariate gaussian distribution p n x μ σ where σ ρ σ 269 where the correlation coefficient is given by ρ a what is p simplify your answer by expressing it in terms of ρ and b assume what is p now exercise whitening vs standardizing 270 a load the height weight data using rawdata dlmread heightweightdata txt the ﬁrst col umn is the class label male female the second column is height the third weight extract the height weight data corresponding to the males fit a gaussian to the male data using the empirical mean and covariance plot your gaussian as an ellipse use superimposing on your scatter plot it should look like figure a where have labeled each datapoint by its index turn in your ﬁgure and code b standardizing the data means ensuring the empirical variance along each dimension is this can be done by computing xij xj where σ j is the empirical std of dimension j standardize the data and replot it should look like figure b use axis equal turn in your ﬁgure and code c whitening or sphereing the data means ensuring its empirical covariance matrix is proportional to i so the data is uncorrelated and of equal variance along each dimension this can be done by computing λ ut x for each data vector x where u are the eigenvectors and λ the eigenvalues of x whiten the data and replot it should look like figure c note that whitening rotates the data so people move to counter intuitive locations in the new coordinate system see e g person who moves from the right hand side to the left exercise sensor fusion with known variances in suppose we have two sensors with known and different variances and but unknown and the same mean μ suppose we observe observations y n μ from the ﬁrst sensor and observations y μ from the second sensor for example suppose μ is the true temperature outside and sensor is a precise low variance digital thermosensing device and sensor is an imprecise high variance mercury thermometer let represent all the data from both sensors what is the posterior p μ assuming a non informative prior for μ which we can simulate using a gaussian with a precision of give an explicit expression for the posterior mean and variance exercise derivation of information form formulae for marginalizing and conditioning derive the information form results of section exercise derivation of the niw posterior derive equation 209 hint one can show that n x μ x μ t μ μ t 271 t t κ μ m μ m x m x m 272 this is a matrix generalization of an operation called completing the square derive the corresponding result for the normal wishart model exercise bic for gaussians source jaakkola the bayesian information criterion bic is a penalized log likelihood function that can be used for model selection see section it is deﬁned as bic log p d θˆ d log n 273 where d is the number of free parameters in the model and n is the number of samples in this question we will see how to use this to choose between a full covariance gaussian and a gaussian with a diagonal covariance obviously a full covariance gaussian has higher likelihood but it may not be worth the extra parameters if the improvement over a diagonal covariance matrix is too small so we use the bic score to choose the model following section we can write log p d σˆ μˆ n tr σˆ n log σˆ 274 n sˆ x n i i x xi x 275 where sˆ is the scatter matrix empirical covariance the trace of a matrix is the sum of its diagonals and we have used the trace trick a derive the bic score for a gaussian in d dimensions with full covariance matrix simplify your answer as much as possible exploiting the form of the mle be sure to specify the number of free parameters d b derive the bic score for a gaussian in d dimensions with a diagonal covariance matrix be sure to specify the number of free parameters d hint for the digaonal case the ml estimate of σ is the same as σˆ ml except the off diagonal terms are zero σˆ diag diag σˆ ml σˆ ml d d 276 in the scalar case completing the square means rewriting as a x b w where a b and w exercise gaussian posterior credible interval source degroot let x μ where μ is unknown but has prior μ μ0 the posterior after seeing n samples is μ μn this is called a credible interval and is the bayesian analog of a conﬁdence interval how big does n have to be to ensure p t μn u d 277 where t u is an interval centered on μn of width and d is the data hint recall that of the probability mass of a gaussian is within of the mean exercise map estimation for gaussians source jaakkola consider samples xn from a gaussian random variable with known variance and unknown mean μ we further assume a prior distribution also gaussian over the mean μ m with ﬁxed mean m and ﬁxed variance thus the only unknown is μ a calculate the map estimate μˆmap you can state the result without proof alternatively with a lot more work you can compute derivatives of the log posterior set to zero and solve b show that as the number of samples n increase the map estimate converges to the maximum likelihood estimate c suppose n is small and ﬁxed what does the map estimator converge to if we increase the prior variance d suppose n is small and ﬁxed what does the map estimator converge to if we decrease the prior variance exercise sequential recursive updating of σˆ source duda et al the unbiased estimates for the covariance of a d dimensional gaussian based on n samples is given by σˆ cn n x n i mn xi mn t 278 it is clear that it takes o time to compute cn if the data points arrive one at a time it is more efficient to incrementally update these estimates than to recompute from scratch a show that the covariance can be sequentially udpated as follows cn n c n x n n mn x n mn t 279 b how much time does it take per sequential update use big o notation c show that we can sequentially update the precision matrix using n r c n xn mn xn mn t c n l cn n cn x n mn t c n x n mn 280 hint notice that the update to cn consists of adding a rank one matrix namely uut where u xn mn use the matrix inversion lemma for rank one updates equation which we repeat here for convenience e uvt e e e vt e d what is the time complexity per update exercise likelihood ratio for gaussians source source alpaydin ex consider a binary classiﬁer where the k class conditional densities are mvn p x y j n x μj σj by bayes rule we have log p y x log p x y log p y p y x p x y p y in other words the log posterior ratio is the log likelihood ratio plus the log prior ratio for each of the cases in the table below derive an expression for the log likelihood ratio log p x y simplifying as much as possible p x y form of σj cov num parameters arbitrary σj kd d shared σj σ d d shared axis aligned σj σ with σij for i j d shared spherical σj exercise lda qda on height weight data the function discrimanalysisheightweightdemo ﬁts an lda and qda model to the height weight data compute the misclassiﬁcation rate of both of these models on the training set turn in your numbers and code exercise naive bayes with mixed features consider a class naive bayes classiﬁer with one binary feature and one gaussian feature y mu y π y c ber θc y c n μc let the parameter vectors be as follows π θ μ a compute p y the result should be a vector of numbers that sums to b compute p y c compute p y d explain any interesting patterns you see in your results hint look at the parameter vector θ exercise decision boundary for lda with semi tied covariances consider a generative classiﬁer with class conditional densities of the form x μc σc in lda we assume σc σ and in qda each σc is arbitrary here we consider the class case in which kς0 for k that is the gaussian ellipsoids have the same shape but the one for class is wider derive an expression for p y x θ simplifying as much as possible give a geometric interpretation of your result if possible exercise logistic regression vs lda qda source jaakkola suppose we train the following binary classiﬁers via maximum likelihood a gaussi a generative classiﬁer where the class conditional densities are gaussian with both covariance matrices set to i identity matrix i e p x y c n x μc i we assume p y is uniform b gaussx as for gaussi but the covariance matrices are unconstrained i e p x y c n x μc σc c linlog a logistic regression model with linear features d quadlog a logistic regression model using linear and quadratic features i e polynomial basis function expansion of degree after training we compute the performance of each model m on the training set as follows l m log p y x θˆ m 285 note that this is the conditional log likelihood p y x θˆ and not the joint log likelihood p y x θˆ we now want to compare the performance of each model we will write l m l m i if model m must have lower or equal log likelihood on the training set than m i for any training set in other words m is worse than m i at least as far as training set logprob is concerned for each of the following model pairs state whether l m l m i l m l m i or whether no such statement can be made i e m might sometimes be better than m i and sometimes worse also for each question brieﬂy sentences explain why a gaussi linlog b gaussx quadlog c linlog quadlog d gaussi quadlog e now suppose we measure performance in terms of the average misclassiﬁcation rate on the training set n r m i y n i i yˆ xi 286 is it true in general that l m l m i implies that r m r m i explain why or why not exercise gaussian decision boundaries source duda et al let p x y j n x μj σj where j and let the class priors be equal p y p y a find the decision region x p x p x 287 sketch the result hint draw the curves and ﬁnd where they intersect find both solutions of the equation p x p x 288 hint recall that to solve a quadratic equation bx c we use b x 289 b now suppose and all other parameters remain the same what is in this case exercise qda with classes consider a three category classiﬁcation problem let the prior probabilites p y p y p y the class conditional densities are multivariate normal densities with parameters t t t σ1 l l l 292 classify the following points a x b x exercise scalar qda note you can solve this exercise by hand or using a computer matlab r whatever in either case show your work consider the following training set of heights x in inches and gender y male female of some us college students x y m m m f f f a fit a bayes classiﬁer to this data using maximum likelihood estimation i e estimate the parameters of the class conditional likelihoods p x y c n x μc σc 293 and the class prior p y c πc what are your values of μc σc πc for c m f show your work so you can get partial credit if you make an arithmetic error b compute p y m x θˆ where x and θˆ are the mle parameters this is called a plug in prediction c what would be a simple way to extend this technique if you had multiple attributes per person such as height and weight write down your proposed model as an equation bayesian statistics introduction we have now seen a variety of different probability models and we have discussed how to ﬁt them to data i e we have discussed how to compute map parameter estimates θˆ argmax p θ using a variety of different priors we have also discussed how to compute the full posterior p θ as well as the posterior predictive density p x for certain special cases and in later chapters we will discuss algorithms for the general case using the posterior distribution to summarize everything we know about a set of unknown variables is at the core of bayesian statistics in this chapter we discuss this approach to statistics in more detail in chapter we discuss an alternative approach to statistics known as frequentist or classical statistics summarizing posterior distributions the posterior p θ summarizes everything we know about the unknown quantities θ in this section we discuss some simple quantities that can be derived from a probability distribution such as a posterior these summary statistics are often easier to understand and visualize than the full joint map estimation we can easily compute a point estimate of an unknown quantity by computing the posterior mean median or mode in section we discuss how to use decision theory to choose between these methods typically the posterior mean or median is the most appropriate choice for a real valued quantity and the vector of posterior marginals is the best choice for a discrete quantity however the posterior mode aka the map estimate is the most popular choice because it reduces to an optimization problem for which efficient algorithms often exist futhermore map estimation can be interpreted in non bayesian terms by thinking of the log prior as a regularizer see section for more details although this approach is computationally appealing it is important to point out that there are various drawbacks to map estimation which we brieﬂy discuss below this will provide motivation for the more thoroughly bayesian approach which we will study later in this chapter and elsewhere in this book a b figure a a bimodal distribution in which the mode is very untypical of the distribution the thin blue vertical line is the mean which is arguably a better summary of the distribution since it is near the majority of the probability mass figure generated by bimodaldemo b a skewed distribution in which the mode is quite different from the mean figure generated by gammaplotdemo no measure of uncertainty the most obvious drawback of map estimation and indeed of any other point estimate such as the posterior mean or median is that it does not provide any measure of uncertainty in many applications it is important to know how much one can trust a given estimate we can derive such conﬁdence measures from the posterior as we discuss in section plugging in the map estimate can result in overﬁtting in machine learning we often care more about predictive accuracy than in interpreting the parameters of our models however if we don t model the uncertainty in our parameters then our predictive distribution will be overconﬁdent we saw several examples of this in chapter and we will see more examples later overconﬁdence in predictions is particularly problematic in situations where we may be risk averse see section for details the mode is an untypical point choosing the mode as a summary of a posterior distribution is often a very poor choice since the mode is usually quite untypical of the distribution unlike the mean or median this is illustrated in figure a for a continuous space the basic problem is that the mode is a point of measure zero whereas the mean and median take the volume of the space into account another example is shown in figure b here the mode is but the mean is non zero such skewed distributions often arise when inferring variance parameters especially in hierarchical models in such cases the map estimate and hence the mle is obviously a very bad estimate how should we summarize a posterior if the mode is not a good choice the answer is to use decision theory which we discuss in section the basic idea is to specify a loss function where l θ θˆ is the loss you incur if the truth is θ and your estimate is θˆ if we use loss l θ θˆ i θ θˆ then the optimal estimate is the posterior mode loss means you only get points if you make no errors otherwise you get nothing there is no partial credit under figure example of the transformation of a density under a nonlinear transform note how the mode of the transformed distribution is not the transform of the original mode based on exercise of bishop figure generated by bayeschangeofvar this loss function for continuous valued quantities we often prefer to use squared error loss l θ θˆ θ θˆ the corresponding optimal estimator is then the posterior mean as we show in section or we can use a more robust loss function l θ θˆ θ θˆ which gives rise to the posterior median map estimation is not invariant to reparameterization a more subtle problem with map estimation is that the result we get depends on how we pa rameterize the probability distribution changing from one representation to another equivalent representation changes the result which is not very desirable since the units of measurement are arbitrary e g when measuring distance we can use centimetres or inches to understand the problem suppose we compute the posterior for x if we deﬁne y f x the distribution for y is given by equation which we repeat here for convenience py y px x dy the dx term is called the jacobian and it measures the change in size of a unit volume passed through f let xˆ argmaxx px x be the map estimate for x in general it is not the case that yˆ argmaxy py y is given by f xˆ for example let x n and y f x where f x exp x we can derive the distribution of y using monte carlo simulation see section the result is shown in figure we see that the original gaussian has become squashed by the sigmoid nonlinearity in particular we see that the mode of the transformed distribution is not equal to the transform of the original mode to see how this problem arises in the context of map estimation consider the following example due to michael jordan the bernoulli distribution is typically parameterized by its mean μ so p y μ μ where y suppose we have a uniform prior on the unit interval pμ μ i μ if there is no data the map estimate is just the mode of the prior which can be anywhere between and we will now show that different parameterizations can pick different points in this interval arbitrarily first let θ μ so μ the new prior is pθ θ pμ μ dθ for θ so the new mode is θˆmap arg max θ now let φ μ the new prior is pφ φ pμ μ dφ φ for φ so the new mode is φˆmap arg max φ thus the map estimate depends on the parameterization the mle does not suffer from this since the likelihood is a function not a probability density bayesian inference does not suffer from this problem either since the change of measure is taken into account when integrating over the parameter space one solution to the problem is to optimize the following objective function θ argmax p θ p θ i θ θ here i θ is the fisher information matrix associated with p x θ see section this estimate is parameterization independent for reasons explained in jermyn druilhet and marin unfortunately optimizing equation is often difficult which minimizes the appeal of the whole approach credible intervals in addition to point estimates we often want a measure of conﬁdence a standard measure of conﬁdence in some scalar quantity θ is the width of its posterior distribution this can be measured using a α credible interval which is a contiguous region c u standing for lower and upper which contains α of the posterior probability mass i e cα d u p θ u d α there may be many such intervals so we choose one such that there is α mass in each tail this is called a central interval a b figure a central interval and b hpd region for a beta posterior the ci is and the hpd is based on figure of hoff figure generated by betahpd if the posterior has a known functional form we can compute the posterior central interval using f α and u f α where f is the cdf of the posterior for example if the posterior is gaussian p θ and α then we have φ α and u φ α where φ denotes the cdf of the gaussian this is illustrated in figure c this justiﬁes the common practice of quoting a credible interval in the form of μ where μ represents the posterior mean σ represents the posterior standard deviation and is a good approximation to of course the posterior is not always gaussian for example in our coin example if we use a uniform prior and we observe heads out of n trials then the posterior is a beta distribution p θ beta we ﬁnd the posterior credible interval is 5673 see betacredibleint for the one line of matlab code we used to compute this if we don t know the functional form but we can draw samples from the posterior then we can use a monte carlo approximation to the posterior quantiles we simply sort the s samples and ﬁnd the one that occurs at location α s along the sorted list as s this converges to the true quantile see mcquantiledemo for a demo people often confuse bayesian credible intervals with frequentist conﬁdence intervals how ever they are not the same thing as we discuss in section in general credible intervals are usually what people want to compute but conﬁdence intervals are usually what they actually compute because most people are taught frequentist statistics but not bayesian statistics fortu nately the mechanics of computing a credible interval is just as easy as computing a conﬁdence interval see e g betacredibleint for how to do it in matlab highest posterior density regions a problem with central intervals is that there might be points outside the ci which have higher probability density this is illustrated in figure a where we see that points outside the left most ci boundary have higher density than those just inside the right most ci boundary this motivates an alternative quantity known as the highest posterior density or hpd region this is deﬁned as the set of most probable points that in total constitute α of the a pmin b figure a central interval and b hpd region for a hypothetical multimodal posterior based on figure of gelman et al figure generated by postdensityintervals probability mass more formally we ﬁnd the threshold p on the pdf such that α r θ p θ d p p θ d dθ and then deﬁne the hpd as cα d θ p θ d p in the hpd region is sometimes called a highest density interval or hdi for example figure b shows the hdi of a beta distribution which is we see that this is narrower than the ci even though it still contains of the mass furthermore every point inside of it has higher density than every point outside of it for a unimodal distribution the hdi will be the narrowest interval around the mode contain ing of the mass to see this imagine water ﬁlling in reverse where we lower the level until of the mass is revealed and only is submerged this gives a simple algorithm for computing hdis in the case simply search over points such that the interval contains of the mass and has minimal width this can be done by numerical optimization if we know the inverse cdf of the distribution or by search over the sorted data points if we have a bag of samples see betahpd for a demo if the posterior is multimodal the hdi may not even be a connected region see figure b for an example however summarizing multimodal posteriors is always difficult inference for a difference in proportions sometimes we have multiple parameters and we are interested in computing the posterior distribution of some function of these parameters for example suppose you are about to buy something from amazon com and there are two sellers offering it for the same price seller has positive reviews and negative reviews seller has positive reviews and negative reviews who should you buy from this example is from www johndcook com blog bayesian amazon see also lingpipe blog c om bayesian counterpart to fisher exact test on contingency tables a b figure a exact posteriors p θi i b monte carlo approximation to p δ we use kernel density estimation to get a smooth plot the vertical lines enclose the central interval figure generated by amazonsellerdemo on the face of it you should pick seller but we cannot be very conﬁdent that seller is better since it has had so few reviews in this section we sketch a bayesian analysis of this problem similar methodology can be used to compare rates or proportions across groups for a variety of other settings let and be the unknown reliabilities of the two sellers since we don t know much about them we ll endow them both with uniform priors θi beta the posteriors are p beta and p beta we want to compute p for convenience let us deﬁne δ as the difference in the rates alternatively we might want to work in terms of the log odds ratio we can compute the desired quantity using numerical integration r r beta we ﬁnd p δ which means you are better off buying from seller see amazonsellerdemo for the code it is also possible to solve the integral analytically cook a simpler way to solve the problem is to approximate the posterior p δ by monte carlo sampling this is easy since and are independent in the posterior and both have beta distributions which can be sampled from using standard methods the distributions p θi i are shown in figure a and a mc approximation to p δ together with a hpd is shown figure b an mc approximation to p δ is obtained by counting the fraction of samples where this turns out to be which is very close to the exact value see amazonsellerdemo for the code bayesian model selection in figure we saw that using too high a degree polynomial results in overﬁtting and using too low a degree results in underﬁtting similarly in figure a we saw that using too small a regularization parameter results in overﬁtting and too large a value results in underﬁtting in general when faced with a set of models i e families of parametric distributions of different complexity how should we choose the best one this is called the model selection problem one approach is to use cross validation to estimate the generalization error of all the candiate models and then to pick the model that seems the best however this requires ﬁtting each model k times where k is the number of cv folds a more efficient approach is to compute the posterior over models p d m p m p m d m m p m d from this we can easily compute the map model mˆ bayesian model selection argmax p m d this is called if we use a uniform prior over models p m this amounts to picking the model which maximizes p d m r p d θ p θ m dθ this quantity is called the marginal likelihood the integrated likelihood or the evidence for model m the details on how to perform this integral will be discussed in section but ﬁrst we give an intuitive interpretation of what this quantity means bayesian occam razor one might think that using p d m to select models would always favor the model with the most parameters this is true if we use p θˆm to select models where θˆm is the mle or map estimate of the parameters for model m because models with more parameters will ﬁt the data better and hence achieve higher likelihood however if we integrate out the parameters rather than maximizing them we are automatically protected from overﬁtting models with more parameters do not necessarily have higher marginal likelihood this is called the bayesian occam razor effect mackay murray and ghahramani named after the principle known as occam razor which says one should pick the simplest model that adequately explains the data one way to understand the bayesian occam razor is to notice that the marginal likelihood can be rewritten as follows based on the chain rule of probability equation p d p p p p yn n where we have dropped the conditioning on x for brevity this is similar to a leave one out cross validation estimate section of the likelihood since we predict each future point given all the previous ones of course the order of the data does not matter in the above expression if a model is too complex it will overﬁt the early examples and will then predict the remaining ones poorly another way to understand the bayesian occam razor effect is to note that probabilities must sum to one hence t p m where the sum is over all possible data sets complex models which can predict many things must spread their probability mass thinly and hence will not obtain as large a probability for any given data set as simpler models this is sometimes figure a schematic illustration of the bayesian occam razor the broad green curve corresponds to a complex model the narrow blue curve to a simple model and the middle red curve is just right based on figure of bishop see also murray and ghahramani figure for a similar plot produced on real data called the conservation of probability mass principle and is illustrated in figure on the horizontal axis we plot all possible data sets in order of increasing complexity measured in some abstract sense on the vertical axis we plot the predictions of possible models a simple one a medium one and a complex one we also indicate the actually observed data by a vertical line model is too simple and assigns low probability to model also assigns relatively low probability because it can predict many data sets and hence it spreads its probability quite widely and thinly model is just right it predicts the observed data with a reasonable degree of conﬁdence but does not predict too many other things hence model is the most probable model as a concrete example of the bayesian occam razor consider the data in figure we plot polynomials of degrees and ﬁt to n data points it also shows the posterior over models where we use a gaussian prior see section for details there is not enough data to justify a complex model so the map model is d figure shows what happens when n now it is clear that d is the right model the data was in fact generated from a quadratic as another example figure c plots log p λ vs log λ for the polynomial ridge regres sion model where λ ranges over the same set of values used in the cv experiment we see that the maximum evidence occurs at roughly the same point as the minimum of the test mse which also corresponds to the point chosen by cv when using the bayesian approach we are not restricted to evaluating the evidence at a ﬁnite grid of values instead we can use numerical optimization to ﬁnd λ argmaxλ p λ this technique is called empirical bayes or type ii maximum likelihood see section for details an example is shown in figure b we see that the curve has a similar shape to the cv estimate but it can be computed more efficiently d logev eb d logev eb a d logev eb b n method eb c m d figure a c we plot polynomials of degrees and ﬁt to n data points using empirical bayes the solid green curve is the true function the dashed red curve is the prediction dotted blue lines represent σ around the mean d we plot the posterior over models p d d assuming a uniform prior p d based on a ﬁgure by zoubin ghahramani figure generated by linregebmodelselvsn computing the marginal likelihood evidence when discussing parameter inference for a ﬁxed model we often wrote p θ d m p θ m p d θ m thus ignoring the normalization constant p m this is valid since p m is constant wrt θ however when comparing models we need to know how to compute the marginal likelihood p m in general this can be quite hard since we have to integrate over all possible parameter values but when we have a conjugate prior it is easy to compute as we now show let p θ q θ be our prior where q θ is an unnormalized distribution and is the normalization constant of the prior let p d θ q d θ ze be the likelihood where ze contains any constant factors in the likelihood finally let p θ d q θ d zn be our poste d logev eb d logev eb a d logev 410 eb b n method eb c m d figure same as figure except now n figure generated by linregebmodelselvsn rior where q θ q θ q θ is the unnormalized posterior and zn is the normalization constant of the posterior we have p θ p d θ p θ p d q θ d zn q d θ q θ d zn p d z z so assuming the relevant normalization constants are tractable we have an easy way to compute the marginal likelihood we give some examples below beta binomial model let us apply the above result to the beta binomial model since we know p θ beta θ a b where a a and b b we know the normalization constant of the posterior is b a b hence p θ p d θ p θ p d θa θ b n θ p d b a b p d b a b so n b a b p d b a b p d n b a b the marginal likelihood for the beta bernoulli model is the same as above except it is missing n dirichlet multinoulli model by the same reasoning as the beta bernoulli case one can show that the marginal likelihood for the dirichlet multinoulli model is given by b n α p d where b α b α k k γ γ αk α hence we can rewrite the above result in the following form which is what is usually presented in the literature γ k αk tt γ nk αk we will see many applications of this equation later gaussian gaussian wishart model consider the case of an mvn with a conjugate niw prior let be the normalizer for the prior zn be normalizer for the posterior and let zl nd be the normalizer for the likelihood then it is easy to see that zn p d z z d s νn n d νn πnd d s ν d ν d γd νn πnd κn sn νn γd this equation will prove useful later bic approximation to log marginal likelihood in general computing the integral in equation can be quite difficult one simple but popular approximation is known as the bayesian information criterion or bic which has the following form schwarz bic log p d θˆ dof θˆ log n log p d where dof θˆ is the number of degrees of freedom in the model and θˆ is the mle for the model we see that this has the form of a penalized log likelihood where the penalty term depends on the model complexity see section for the derivation of the bic score as an example consider linear regression as we show in section the mle is given by wˆ xt x y and rss n where rss n yi wˆ t xi the corresponding log likelihood is given by log p d θˆ n log n hence the bic score is as follows dropping constant terms bic n log d log n 32 where d is the number of variables in the model in the statistics literature it is common to use an alternative deﬁnition of bic which we call the bic cost since we want to minimize it bic cost log p d θˆ dof θˆ log n log p d in the context of linear regression this becomes bic cost n log d log n traditionally the bic score is deﬁned using the ml estimate θˆ so it is independent of the prior however for models such as mixtures of gaussians the ml estimate can be poorly behaved so it is better to evaluate the bic score using the map estimate as in fraley and raftery the bic method is very closely related to the minimum description length or mdl principle which characterizes the score for a model in terms of how well it ﬁts the data minus how complex the model is to deﬁne see hansen and yu for details there is a very similar expression to bic mdl called the akaike information criterion or aic deﬁned as aic m d log p d θˆmle dof m this is derived from a frequentist framework and cannot be interpreted as an approximation to the marginal likelihood nevertheless the form of this expression is very similar to bic we see that the penalty for aic is less than for bic this causes aic to pick more complex models however this can result in better predictive accuracy see e g clarke et al sec for further discussion on such information criteria effect of the prior sometimes it is not clear how to set the prior when we are performing posterior inference the details of the prior may not matter too much since the likelihood often overwhelms the prior anyway but when computing the marginal likelihood the prior plays a much more important role since we are averaging the likelihood over all possible parameter settings as weighted by the prior in figures and where we demonstrated model selection for linear regression we used a prior of the form p w α here α is a tuning parameter that controls how strong the prior is this parameter can have a large effect as we discuss in section intuitively if α is large the weights are forced to be small so we need to use a complex model with many small parameters e g a high degree polynomial to ﬁt the data conversely if α is small we will favor simpler models since each parameter is allowed to vary in magnitude by a lot if the prior is unknown the correct bayesian procedure is to put a prior on the prior that is we should put a prior on the hyper parameter α as well as the parametrs w to compute the marginal likelihood we should integrate out all unknowns i e we should compute p d m r r p d w p w α m p α m dwdα of course this requires specifying the hyper prior fortunately the higher up we go in the bayesian hierarchy the less sensitive are the results to the prior settings so we can usually make the hyper prior uninformative a computational shortcut is to optimize α rather than integrating it out that is we use p d m r p d w p w αˆ m dw where αˆ argmax p d α m argmax r p d w p w α m dw this approach is called empirical bayes eb and is discussed in more detail in section this is the method used in figures and bayes factor bf interpretation bf decisive evidence for strong evidence for bf moderate evidence for bf weak evidence for bf weak evidence for bf moderate evidence for bf strong evidence for bf decisive evidence for table jeffreys scale of evidence for interpreting bayes factors bayes factors suppose our prior on models is uniform p m then model selection is equivalent to picking the model with the highest marginal likelihood now suppose we just have two models we are considering call them the null hypothesis and the alternative hypothesis deﬁne the bayes factor as the ratio of marginal likelihoods bf p d p d p p d p d p this is like a likelihood ratio except we integrate out the parameters which allows us to compare models of different complexity if then we prefer model otherwise we prefer model of course it might be that is only slightly greater than in that case we are not very conﬁdent that model is better jeffreys proposed a scale of evidence for interpreting the magnitude of a bayes factor which is shown in table this is a bayesian alternative to the frequentist concept of a p value alternatively we can just convert the bayes factor to a posterior over models if p p we have p m d example testing if a coin is fair suppose we observe some coin tosses and want to decide if the data was generated by a fair coin θ or a potentially biased coin where θ could be any value in let us denote the ﬁrst model by and the second model by the marginal likelihood under is simply n a p value is deﬁned as the probability under the null hypothesis of observing some test statistic f d such as the chi squared statistic that is as large or larger than that actually observed i e pvalue d p f d f d d note that has almost nothing to do with what we really want to know which is p d log p d bic approximation to log p d a b figure a log marginal likelihood for the coins example b bic approximation figure generated by coinsmodelseldemo where n is the number of coin tosses the marginal likelihood under using a beta prior is p d m r p d θ p θ dθ b we plot log p vs the number of heads in figure a assuming n and the shape of the curve is not very sensitive to and α0 as long as α0 α1 if we observe or heads the unbiased coin hypothesis is more likely than since is a simpler model it has no free parameters it would be a suspicious coincidence if the coin were biased but happened to produce almost exactly heads tails however as the counts become more extreme we favor the biased coin hypothesis note that if we plot the log bayes factor log it will have exactly the same shape since log p is a constant see also exercise in figure b shows the bic approximation to log p for our biased coin example from section we see that the curve has approximately the same shape as the exact log marginal likelihood which is all that matters for model selection purposes since the absolute scale is irrelevant in particular it favors the simpler model unless the data is overwhelmingly in support of the more complex model jeffreys lindley paradox problems can arise when we use improper priors i e priors that do not integrate to for model selection hypothesis testing even though such priors may be acceptable for other purposes for example consider testing the hypotheses θ vs θ to deﬁne the marginal density on θ we use the following mixture model p θ p θ p p θ p this is only meaningful if p θ and p θ are proper normalized density functions in this case the posterior is given by p m d p p d p p d p p d p p d θ p θ dθ now suppose we use improper priors p θ and p θ then p d p p m c p θ dθ p d θ dθ p p d θ dθ p p p where i θi p d θ dθ is the integrated or marginal likelihood for model i now let p p hence p m d thus we can change the posterior arbitrarily by choosing c1 and c0 as we please note that using proper but very vague priors can cause similar problems in particular the bayes factor will always favor the simpler model since the probability of the observed data under a complex model with a very diffuse prior will be very small this is called the jeffreys lindley paradox thus it is important to use proper priors when performing model selection note however that if and share the same prior over a subset of the parameters this part of the prior can be improper since the corresponding normalization constant will cancel out priors the most controversial aspect of bayesian statistics is its reliance on priors bayesians argue this is unavoidable since nobody is a tabula rasa or blank slate all inference must be done conditional on certain assumptions about the world nevertheless one might be interested in minimizing the impact of one prior assumptions we brieﬂy discuss some ways to do this below uninformative priors if we don t have strong beliefs about what θ should be it is common to use an uninformative or non informative prior and to let the data speak for itself the issue of designing uninformative priors is actually somewhat tricky as an example of the difficulty consider a bernoulli parameter θ one might think that the most uninformative prior would be the uniform distribution beta but the posterior mean in this case is e θ d whereas the mle is hence one could argue that the prior wasn t completely uninformative after all clearly by decreasing the magnitude of the pseudo counts we can lessen the impact of the prior by the above argument the most non informative prior is lim beta c c beta c which is a mixture of two equal point masses at and see zhu and lu this is also called the haldane prior note that the haldane prior is an improper prior meaning it does not integrate to however as long as we see at least one head and at least one tail the posterior will be proper in section we will argue that the right uninformative prior is in fact beta clearly the difference in practice between these three priors is very likely negligible in general it is advisable to perform some kind of sensitivity analysis in which one checks how much one conclusions or predictions change in response to change in the modeling assumptions which includes the choice of prior but also the choice of likelihood and any kind of data pre processing if the conclusions are relatively insensitive to the modeling assumptions one can have more conﬁdence in the results jeffreys priors harold designed a general purpose technique for creating non informative priors the result is known as the jeffreys prior the key observation is that if p φ is non informative then any re parameterization of the prior such as θ h φ for some function h should also be non informative now by the change of variables formula pθ θ pφ φ dθ so the prior will in general change however let us pick pφ φ i φ where i φ is the fisher information i φ e d log p x φ this is a measure of curvature of the expected negative log likelihood and hence a measure of stability of the mle see section now d log p x θ d log p x φ dφ dθ dφ dθ squaring and taking expectations over x we have i θ e r d log p x θ 27 i φ dφ dθ i θ dφ i φ dθ harold jeffreys was an english mathematician statistician geophysicist and astronomer so we ﬁnd the transformed prior is p θ p dφ dφ φ i φ i θ so pθ θ and pφ φ are the same some examples will make this clearer example jeffreys prior for the bernoulli and multinoulli suppose x ber θ the log likelihood for a single sample is log p x θ x log θ x log θ the score function is just the gradient of the log likelihood θ log p x θ x x dθ θ θ the observed information is the second derivative of the log likelihood x x j θ log p x θ θ x θ the fisher information is the expected information θ i θ e j θ x x θ θ hence jeffreys prior is θ θ θ p θ θ θ beta θ θ now consider a multinoulli random variable with k states one can show that the jeffreys prior is given by p θ dir note that this is different from the more obvious choices of dir or dir k k example jeffreys prior for location and scale parameters one can show that the jeffreys prior for a location parameter such as the gaussian mean is p μ thus is an example of a translation invariant prior which satisﬁes the property that the probability mass assigned to any interval a b is the same as that assigned to any other shifted interval of the same width such as a c b c that is b c a c b p μ dμ a c b c a b p μ dμ this can be achieved using p μ which we can approximate by using a gaussian with inﬁnite variance p μ μ note that this is an improper prior since it does not integrate to using improper priors is ﬁne as long as the posterior is proper which will be the case provided we have seen n data points since we can nail down the location as soon as we have seen a single data point similarly one can show that the jeffreys prior for a scale parameter such as the gaussian variance is p this is an example of a scale invariant prior which satisﬁes the property that the probability mass assigned to any interval a b is the same as that assigned to any other interval a c b c which is scaled in size by some constant factor c for example if we change units from meters to feet we do not want that to affect our inferences this can be achieved by using p to see this note that b c a c p ds log b c log b c log a c r b we can approximate this using a degenerate gamma distribution section p ga the prior p is also improper but the posterior is proper as soon as we have seen n data points since we need at least two data points to estimate a variance robust priors in many cases we are not very conﬁdent in our prior so we want to make sure it does not have an undue inﬂuence on the result this can be done by using robust priors insua and ruggeri which typically have heavy tails which avoids forcing things to be too close to the prior mean let us consider an example from berger suppose x n θ we observe that x and we want to estimate θ the mle is of course θ which seems reasonable the posterior mean under a uniform prior is also θ but now suppose we know that the prior median is and the prior quantiles are at and so p θ p θ p θ p θ let us also assume the prior is smooth and unimodal it is easy to show that a gaussian prior of the form θ satisﬁes these prior constraints but in this case the posterior mean is given by which doesn t seem very satisfactory now suppose we use as a cauchy prior θ this also satisﬁes the prior constraints of our example but this time we ﬁnd using numerical method integration see robustpriordemo for the code that the posterior mean is about which seems much more reasonable mixtures of conjugate priors robust priors are useful but can be computationally expensive to use conjugate priors simplify the computation but are often not robust and not ﬂexible enough to encode our prior knowl edge however it turns out that a mixture of conjugate priors is also conjugate exercise and can approximate any kind of prior dallal and hall diaconis and ylvisaker thus such priors provide a good compromise between computational convenience and ﬂexibility for example suppose we are modeling coin tosses and we think the coin is either fair or is biased towards heads this cannot be represented by a beta distribution however we can model it using a mixture of two beta distributions for example we might use p θ beta θ beta θ if θ comes from the ﬁrst distribution the coin is fair but if it comes from the second it is biased towards heads we can represent a mixture by introducing a latent indicator variable z where z k means that θ comes from mixture component k the prior has the form p θ p z k p θ z k k where each p θ z k is conjugate and p z k are called the prior mixing weights one can show exercise that the posterior can also be written as a mixture of conjugate distributions as follows p θ d p z k d p θ d z k k where p z k d are the posterior mixing weights given by p z k p d z k p z k d kt p z k p d z k here the quantity p z k is the marginal likelihood for mixture component k see sec tion example suppose we use the mixture prior p θ θ θ where and and we observe heads and n0 tails the posterior becomes p θ d p z d beta θ b1 n0 p z d beta θ b2 n0 if heads and n0 tails then using equation the posterior becomes p θ d beta θ 654 beta θ see figure for an illustration mixture of beta distributions figure a mixture of two beta distributions figure generated by mixbetademo application finding conserved regions in dna and protein sequences we mentioned that dirichlet multinomial models are widely used in biosequence analysis let us give a simple example to illustrate some of the machinery that has developed speciﬁcally consider the sequence logo discussed in section now suppose we want to ﬁnd locations which represent coding regions of the genome such locations often have the same letter across all sequences because of evolutionary pressure so we need to ﬁnd columns which are pure or nearly so in the sense that they are mostly all as mostly all ts mostly all cs or mostly all gs one approach is to look for low entropy columns these will be ones whose distribution is nearly deterministic pure but suppose we want to associate a conﬁdence measure with our estimates of purity this can be useful if we believe adjacent locations are conserved together in this case we can let if location t is conserved and let zt otherwise we can then add a dependence between adjacent zt variables using a markov chain see chapter for details in any case we need to deﬁne a likelihood model p nt zt where nt is the vector of a c g t counts for column t it is natural to make this be a multinomial distribution with parameter θt since each column has a different distribution we will want to integrate out θt and thus compute the marginal likelihood p nt zt r p nt θt p θt zt dθt but what prior should we use for θt when zt we can use a uniform prior p θ zt dir but what should we use if zt after all if the column is conserved it could be a nearly pure column of as cs gs or ts a natural approach is to use a mixture of dirichlet priors each one of which is tilted towards the appropriate corner of the dimensional simplex e g p θ zt dir θ dir θ since this is conjugate we can easily compute p nt zt see brown et al for an application of these ideas to a real bio sequence problem hierarchical bayes a key requirement for computing the posterior p θ is the speciﬁcation of a prior p θ η where η are the hyper parameters what if we don t know how to set η in some cases we can use uninformative priors we we discussed above a more bayesian approach is to put a prior on our priors in terms of graphical models chapter we can represent the situation as follows η θ d 76 this is an example of a hierarchical bayesian model also called a multi level model since there are multiple levels of unknown quantities we give a simple example below and we will see many others later in the book example modeling related cancer rates consider the problem of predicting cancer rates in various cities this example is from johnson and albert in particular suppose we measure the number of people in various cities ni and the number of people who died of cancer in these cities xi we assume xi bin ni θi and we want to estimate the cancer rates θi one approach is to estimate them all separately but this will suffer from the sparse data problem underestimation of the rate of cancer due to small ni another approach is to assume all the θi are the same this is called parameter tying the resulting pooled mle is just θ i ni but the assumption that all the cities have the same rate is a rather strong one a compromise approach is to assume that the θi are similar but that there may be city speciﬁc variations this can be modeled by assuming the θi are drawn from some common distribution say θi beta a b the full joint distribution can be written as n p d θ η n p η bin xi ni θi beta θi η i where η a b note that it is crucial that we infer η a b from the data if we just clamp it to a constant the θi will be conditionally independent and there will be no information ﬂow between them by contrast by treating η as an unknown hidden variable we allow the data poor cities to borrow statistical strength from data rich ones suppose we compute the joint posterior p η θ from this we can get the posterior marginals p θi in figure a we plot the posterior means e θi as blue bars as well as the population level mean e a a b shown as a red line this represents the average of the θi we see that the posterior mean is shrunk towards the pooled estimate more strongly for cities with small sample sizes ni for example city and city both have a observed cancer incidence rate but city has a smaller population so its rate is shrunk more towards the population level estimate i e it is closer to the horizontal red line than city figure b shows the posterior credible intervals for θi we see that city which has a very large population 637 people has small posterior uncertainty consequently this city number of people with cancer truncated at credible interval on theta median pop of city truncated at mle red line pooled mle posterior mean 1000 red line pop mean 00 x a b figure a results of ﬁtting the model using the data from johnson and albert first row number of cancer incidents xi in cities in missouri second row population size ni the largest city number has a population of and incidents but we truncate the vertical axes of the ﬁrst two rows so that the differences between the other cities are visible third row mle θˆi the red line is the pooled mle fourth row posterior mean e θi the red line is e a a b the population level mean b posterior credible intervals on the cancer rates figure generated by cancerrateseb has the largest impact on the posterior estimate of η which in turn will impact the estimate of the cancer rates for other cities cities and which have the highest mle also have the highest posterior uncertainty reﬂecting the fact that such a high estimate is in conﬂict with the prior which is estimated from all the other cities in the above example we have one parameter per city modeling the probability the response is on by making the bernoulli rate parameter be a function of covariates θi sigm wt x we can model multiple correlated logistic regression tasks this is called multi task learning and will be discussed in more detail in section empirical bayes in hierarchical bayesian models we need to compute the posterior on multiple levels of latent variables for example in a two level model we need to compute p η θ d p d θ p θ η p η in some cases we can analytically marginalize out θ this leaves is with the simpler problem of just computing p η as a computational shortcut we can approximate the posterior on the hyper parameters with a point estimate p η d δηˆ η where ηˆ argmax p η d since η is typically much smaller than θ in dimensionality it is less prone to overﬁtting so we can safely use a uniform prior on η then the estimate becomes ηˆ argmax p d η argmax r p d θ p θ η dθl where the quantity inside the brackets is the marginal or integrated likelihood sometimes called the evidence this overall approach is called empirical bayes eb or type ii maximum likelihood in machine learning it is sometimes called the evidence procedure empirical bayes violates the principle that the prior should be chosen independently of the data however we can just view it as a computationally cheap approximation to inference in a hierarchical bayesian model just as we viewed map estimation as an approximation to inference in the one level model θ in fact we can construct a hierarchy in which the more integrals one performs the more bayesian one becomes method deﬁnition maximum likelihood map estimation ml ii empirical bayes θˆ argmaxθ p d θ θˆ argmaxθ p d θ p θ η ηˆ argmaxη p d θ p θ η dθ argmaxη p d η full bayes p θ η d p d θ p θ η p η note that eb can be shown to have good frequentist properties see e g carlin and louis efron so it is widely used by non bayesians for example the popular james stein estimator discussed in section can be derived using eb example beta binomial model let us return to the cancer rates model we can analytically integrate out the θi and write down the marginal likelihood directly as follows p d a b tt r bin xi ni θi beta θi a b dθi b a xi b ni xi b a b i various ways of maximizing this wrt a and b are discussed in minka having estimated a and b we can plug in the hyper parameters to compute the posterior p θi aˆ ˆb in the usual way using conjugate analysis the net result is that the posterior mean of each θi is a weighted average of its local mle and the prior means which depends on η a b but since η is estimated based on all the data each θi is inﬂuenced by all the data example gaussian gaussian model we now study another example that is analogous to the cancer rates example except the data is real valued we will use a gaussian likelihood and a gaussian prior this will allow us to write down the solution analytically in particular suppose we have data from multiple related groups for example xij could be the test score for student i in school j for j d and i nj we want to estimate the mean score for each school θj however since the sample size nj may be small for some schools we can regularize the problem by using a hierarchical bayesian model where we assume θj come from a common prior μ τ the joint distribution has the following form d nj p θ d η tt n θj μ τ tt n xij θj where we assume is known for simplicity we relax this assumption in exercise we explain how to estimate η below once we have estimated η μ τ we can compute the posteriors over the θj to do that it simpliﬁes matters to rewrite the joint distribution in the following form exploiting the fact that nj gaussian measurements with values xij and variance are equivalent to one measurement of value xj nj i xij with variance nj this yields p θ d ηˆ tt n θj μˆ n xj θj j from this it follows from the results of section that the posteriors are given by p θj d μˆ n θj bˆjμˆ bˆj xj bˆj bˆj where μˆ x and will be deﬁned below the quantity bˆj controls the degree of shrinkage towards the overall mean μ if the data is reliable for group j e g because the sample size nj is large then will be small relative to τ hence bˆj will be small and we will put more weight on xj when we estimate θj however groups with small sample sizes will get regularized shrunk towards the overall mean μ more heavily we will see an example of this below if σj σ for all groups j the posterior mean becomes θˆj bˆx bˆ xj x bˆ xj x this has exactly the same form as the james stein estimator discussed in section example predicting baseball scores we now give an example of shrinkage applied to baseball batting averages from efron and morris we observe the number of hits for d players during the ﬁrst t games call the number of hits bi we assume bj bin t θj where θj is the true batting average for player j the goal is to estimate the θj the mle is of course θˆj xj where xj bj t is the empirical batting average however we can use an eb approach to do better to apply the gaussian shrinkage approach described above we require that the likelihood be gaussian xj n θj for known we drop the i subscript since we assume nj mle top and shrinkage estimates bottom a mse mle mse shrunk player number a figure a mle parameters top and corresponding shrunken estimates bottom b we plot the true parameters blue the posterior mean estimate green and the mles red for of the players figure generated by shrinkagedemobaseball since xj already represents the average for player j however in this example we have a binomial likelihood while this has the right mean e xj θj the variance is not constant tθj θj var x var b j t j t so let us apply a variance stabilizing to xj to better match the gaussian assump tion yj f yj t arcsin 88 now we have approximately yj f θj μj we use gaussian shrinkage to estimate the μj using equation with and we then transform back to get θˆj sin μˆ t the results are shown in figure a b in a we plot the mle θˆj and the posterior mean θj we see that all the estimates have shrunk towards the global mean in b we plot the true value θj the mle θˆj and the posterior mean θj the true values of θj are estimated from a large number of independent games we see that on average the shrunken estimate is much closer to the true parameters than the mle is speciﬁcally the mean squared error deﬁned by mse d θj θj is over three times smaller using the shrinkage estimates estimating the hyper parameters in this section we give an algorithm for estimating η suppose initially that is the same for all groups in this case we can derive the eb estimate in closed form as we now show from equation 126 we have p xj μ τ r n xj θj n θj μ τ dθj n xj μ τ suppose e x μ and var x μ let y f x then a taylor series expansions gives y f μ x μ f i μ hence var y f i μ x μ f i μ μ a variance stabilizing transformation is a function f such that f i μ μ is independent of μ hence the marginal likelihood is d p d μ τ n xj μ τ j thus we can estimate the hyper parameters using the usual mles for a gaussian for μ we have d μˆ xj d j x 92 which is the overall mean for the variance we can use moment matching which is equivalent to the mle for a gaussian we simply equate the model variance to the empirical variance d xj d j x so since we know τ must be positive it is common to use the following revised estimate max hence the shrinkage factor is ˆ b in the case where the are different we can no longer derive a solution in closed form exercise discusses how to use the em algorithm to derive an eb estimate and exercise discusses how to perform full bayesian inference in this hierarchical model bayesian decision theory we have seen how probability theory can be used to represent and updates our beliefs about the state of the world however ultimately our goal is to convert our beliefs into actions in this section we discuss the optimal way to do this we can formalize any given statistical decision problem as a game against nature as opposed to a game against other strategic players which is the topic of game theory see e g shoham and leyton brown for details in this game nature picks a state or parameter or label y unknown to us and then generates an observation x which we get to see we then have to make a decision that is we have to choose an action a from some action space a finally we incur some loss l y a which measures how compatible our action a is with nature hidden state y for example we might use misclassiﬁcation loss l y a i y a or squared loss l y a y a we will see some other examples below our goal is to devise a decision procedure or policy δ which speciﬁes the optimal action for each possible input by optimal we mean the action that minimizes the expected loss δ x argmin e l y a a a in economics it is more common to talk of a utility function this is just negative loss u y a l y a thus the above rule becomes δ x argmax e u y a a a this is called the maximum expected utility principle and is the essence of what we mean by rational behavior note that there are two different interpretations of what we mean by expected in the bayesian version which we discuss below we mean the expected value of y given the data we have seen so far in the frequentist version which we discuss in section we mean the expected value of y and x that we expect to see in the future in the bayesian approach to decision theory the optimal action having observed x is deﬁned as the action a that minimizes the posterior expected loss ρ a x ep y x l y a l y a p y x y if y is continuous e g when we want to estimate a parameter vector we should replace the sum with an integral hence the bayes estimator also called the bayes decision rule is given by δ x arg min ρ a x a a bayes estimators for common loss functions in this section we show how to construct bayes estimators for the loss functions most commonly arising in machine learning map estimate minimizes loss the loss is deﬁned by l y a i y a if a y if a y this is commonly used in classiﬁcation problems where y is the true class label and a yˆ is the estimate for example in the two class case we can write the loss matrix as follows threshold p y i x p y i x x reject region figure for some regions of input space where the class posteriors are uncertain we may prefer not to choose class or instead we may prefer the reject option based on figure of bishop in section we generalize this loss function so it penalizes the two kinds of errors on the off diagonal differently the posterior expected loss is ρ a x p a y x p y x hence the action that minimizes the expected loss is the posterior mode or map estimate y x arg max p y x y y reject option in classiﬁcation problems where p y x is very uncertain we may prefer to choose a reject action in which we refuse to classify the example as any of the speciﬁed classes and instead say don t know such ambiguous cases can be handled by e g a human expert see figure for an illustration this is useful in risk averse domains such as medicine and ﬁnance we can formalize the reject option as follows let choosing a c correspond to picking the reject action and choosing a c correspond to picking one of the classes suppose we deﬁne the loss function as l y j a i if i j and i j c λr if i c λs otherwise where λr is the cost of the reject action and λs is the cost of a substitution error in exercise you will show that the optimal action is to pick the reject action if the most probable class has a probability below λr otherwise you should just pick the most probable class x x x a b b figure a c plots of the l y a y a q vs y a for q q and q figure generated by lossfunctionfig posterior mean minimizes quadratic loss for continuous parameters a more appropriate loss function is squared error loss or quadratic loss deﬁned as l y a y a the posterior expected loss is given by ρ a x e y a x e x y x hence the optimal estimate is the posterior mean aρ a x y x yˆ e y x r yp y x dy this is often called the minimum mean squared error estimate or mmse estimate in a linear regression problem we have p y x θ n y xt w in this case the optimal estimate given some training data d is given by e y x d xt e w d that is we just plug in the posterior mean parameter estimate note that this is the optimal thing to do no matter what prior we use for w posterior median minimizes absolute loss the loss penalizes deviations from the truth quadratically and thus is sensitive to outliers a more robust alternative is the absolute or loss l y a y a see figure the optimal estimate is the posterior median i e a value a such that p y a x p y a x see exercise for a proof supervised learning consider a prediction function δ and suppose we have some cost function y y which gives the cost of predicting y when the truth is y we can deﬁne the loss incurred by taking action δ i e using this predictor when the unknown state of nature is θ the parameters of the data generating mechanism as follows l θ δ e x y p x y θ y δ x l y δ x p x y θ this is known as the generalization error our goal is to minimize the posterior expected loss given by ρ δ d r p θ d l θ δ dθ this should be contrasted with the frequentist risk which is deﬁned in equation the false positive vs false negative tradeoff in this section we focus on binary decision problems such as hypothesis testing two class classiﬁcation object event detection etc there are two types of error we can make a false positive aka false alarm which arises when we estimate yˆ but the truth is y or a false negative aka missed detection which arises when we estimate yˆ but the truth is y the loss treats these two kinds of errors equivalently however we can consider the following more general loss matrix where lfn is the cost of a false negative and lfp is the cost of a false positive the posterior expected loss for the two possible actions is given by p y x p y x lfp lfn if lfn clfp it is easy to show exercise that we should pick yˆ iff p y x p y x τ where τ c c see also muller et al for example if a false negative costs twice as much as false positive so c then we use a decision threshold of before declaring a positive below we discuss roc curves which provide a way to study the fp fn tradeoff without having to choose a speciﬁc threshold roc curves and all that suppose we are solving a binary decision problem such as classiﬁcation hypothesis testing object detection etc also assume we have a labeled data set d xi yi let δ x truth σ estimate tp fp fn tn nˆ tp fp nˆ fn tn σ n tp fn n fp tn n tp fp fn tn table quantities derivable from a confusion matrix n is the true number of positives nˆ is the called number of positives n is the true number of negatives nˆ is the called number of negatives y y yˆ tp n tpr sensitivity recall fp n fpr type i yˆ fn n fnr miss rate type ii tn n tnr speciﬁty table estimating p yˆ y from a confusion matrix abbreviations fnr false negative rate fpr false positive rate tnr true negative rate tpr true positive rate i f x τ be our decision rule where f x is a measure of conﬁdence that y this should be monotonically related to p y x but does not need to be a probability and τ is some threshold parameter for each given value of τ we can apply our decision rule and count the number of true positives false positives true negatives and false negatives that occur as shown in table this table of errors is called a confusion matrix from this table we can compute the true positive rate tpr also known as the sensitivity recall or hit rate by using tpr tp n p yˆ y we can also compute the false positive rate fpr also called the false alarm rate or the type i error rate by using fpr fp n p yˆ y these and other deﬁnitions are summarized in tables and we can combine these errors in any way we choose to compute a loss function however rather than than computing the tpr and fpr for a ﬁxed threshold τ we can run our detector for a set of thresholds and then plot the tpr vs fpr as an implicit function of τ this is called a receiver operating characteristic or roc curve see figure a for an example any system can achieve the point on the bottom left fpr tpr by setting τ and thus classifying everything as negative similarly any system can achieve the point on the top right fpr t pr by setting τ and thus classifying everything as positive if a system is performing at chance level then we can achieve any point on the diagonal line tpr fpr by choosing an appropriate threshold a system that perfectly separates the positives from negatives has a threshold that can achieve the top left corner fpr t pr by varying the threshold such a system will hug the left axis and then the top axis as shown in figure a the quality of a roc curve is often summarized as a single number using the area under the curve or auc higher auc scores are better the maximum is obviously another summary statistic that is used is the equal error rate or eer also called the cross over rate deﬁned as the value which satisﬁes fpr fnr since fnr tpr we can compute the eer by drawing a line from the top left to the bottom right and seeing where it intersects the roc curve see points a and b in figure a lower eer scores are better the minimum is obviously 00 fpr a recall b figure a roc curves for two hypothetical classiﬁcation systems a is better than b we plot the true positive rate tpr vs the false positive rate fpr as we vary the threshold τ we also indicate the equal error rate eer with the red and blue dots and the area under the curve auc for classiﬁer b b a precision recall curve for two hypothetical classiﬁcation systems a is better than b figure generated by prhand y y yˆ tp nˆ precision ppv fp nˆ fdp yˆ fn nˆ tn nˆ npv table estimating p y yˆ from a confusion matrix abbreviations fdp false discovery probability npv negative predictive value ppv positive predictive value precision recall curves when trying to detect a rare event such as retrieving a relevant document or ﬁnding a face in an image the number of negatives is very large hence comparing tpr tp n to fpr fp n is not very informative since the fpr will be very small hence all the action in the roc curve will occur on the extreme left in such cases it is common to plot the tpr versus the number of false positives rather than vs the false positive rate however in some cases the very notion of negative is not well deﬁned for example when detecting objects in images see section if the detector works by classifying patches then the number of patches examined and hence the number of true negatives is a parameter of the algorithm not part of the problem deﬁnition so we would like to use a measure that only talks about positives the precision is deﬁned as tp nˆ p y yˆ and the recall is deﬁned as tp n p yˆ y precision measures what fraction of our detections are actually positive and recall measures what fraction of the positives we actually detected if yˆi is the predicted label and yi is the true label we can estimate precision and recall using p i yiyˆi r i yiyˆi a precision recall curve is a plot of precision vs recall as we vary the threshold τ see figure b hugging the top right is the best one can do this curve can be summarized as a single number using the mean precision averaging over class class pooled y y y y y y yˆ yˆ yˆ yˆ yˆ yˆ table illustration of the difference between macro and micro averaging y is the true label and yˆ is the called label in this example the macro averaged precision is the micro averaged precision is based on table of manning et al recall values which approximates the area under the curve alternatively one can quote the precision for a ﬁxed recall level such as the precision of the ﬁrst k entities recalled this is called the average precision at k score this measure is widely used when evaluating information retrieval systems f scores for a ﬁxed threshold one can compute a single precision and recall value these are often combined into a single statistic called the f score or score which is the harmonic mean of precision and recall p r r p 116 using equation we can write this as n f i yiyˆi 117 n i yi n i yˆi this is a widely used measure in information retrieval systems to understand why we use the harmonic mean instead of the arithmetic mean p r consider the following scenario suppose we recall all entries so r the precision will be given by the prevalence p y suppose the prevalence is low say p y the arithmetic mean of p and r is given by p r by contrast the harmonic mean of this strategy is only in the multi class case e g for document classiﬁcation problems there are two ways to generalize scores the ﬁrst is called macro averaged and is deﬁned as c c c where c is the score obtained on the task of distinguishing class c from all the others the other is called micro averaged and is deﬁned as the score where we pool all the counts from each class contingency table table gives a worked example that illustrates the difference we see that the precision of class is and of class is the macro averaged precision is therefore whereas the micro averaged precision is the latter is much closer to the precision of class than to the precision of class since class is ﬁve times larger than class to give equal weight to each class use macro averaging false discovery rates suppose we are trying to discover a rare phenomenon using some kind of high throughput measurement device such as a gene expression micro array or a radio telescope we will need to make many binary decisions of the form p yi d τ where d xi n and n may be large this is called multiple hypothesis testing note that the difference from standard binary classiﬁcation is that we are classifying yi based on all the data not just based on xi so this is a simultaneous classiﬁcation problem where we might hope to do better than a series of individual classiﬁcation problems how should we set the threshold τ a natural approach is to try to minimize the expected number of false positives in the bayesian approach this can be computed as follows fd τ d pi i pi τ 118 i p r e rro r disc o very where pi p yi is your belief that this object exhibits the phenomenon in question we then deﬁne the posterior expected false discovery rate as follows fdr τ d fd τ d n τ d 119 where n τ i i pi τ is the number of discovered items given a desired fdr tolerance say α 05 one can then adapt τ to achieve this this is called the direct posterior probability approach to controlling the fdr newton et al muller et al in order to control the fdr it is very helpful to estimate the pi jointly e g using a hierar chical bayesian model as in section rather than independently this allows the pooling of statistical strength and thus lower fdr see e g berry and hochberg for more information other topics in this section we brieﬂy mention a few other topics related to bayesian decision theory we do not have space to go into detail but we include pointers to the relevant literature contextual bandits a one armed bandit is a colloquial term for a slot machine found in casinos around the world the game is this you insert some money pull an arm and wait for the machine to stop if you re lucky you win some money now imagine there is a bank of k such machines to choose from which one should you use this is called a multi armed bandit and can be modeled using bayesian decision theory there are k possible actions and each action has an unknown reward payoff function rk by maintaining a belief state p k k p rk one can devise an optimal policy this can be compiled into a series of gittins indices gittins this optimally solves the exploration exploitation tradeoff which speciﬁes how many times one should try each action before deciding to go with the winner now consider an extension where each arm and the player has an associated feature vector call all these features x this is called a contextual bandit see e g sarkar scott li et al for example the arms could represent ads or news articles which we want to show to the user and the features could represent properties of these ads or articles such as a bag of words as well as properties of the user such as demographics if we assume a linear model for reward rk θt x we can maintain a distribution over the parameters of each arm p θk where is a series of tuples of the form a x r which speciﬁes which arm was pulled what its features were and what the resulting outcome was e g r if the user clicked on the ad and r otherwise we discuss ways to compute p θk from linear and logistic regression models in later chapters given the posterior we must decide what action to take one common heuristic known as ucb which stands for upper conﬁdence bound is to take the action which maximizes k argmax μk λσk k where μk e rk var rk and λ is a tuning parameter that trades off exploration and exploitation the intuition is that we should pick actions about which we believe are good μk is large and or actions about which we are uncertain σk is large an even simpler method known as thompson sampling is as follows at each step we pick action k with a probability that is equal to its probability of being the optimal action pk r i e r a x θ max e r a x θ p θ d dθ we can approximate this by drawing a single sample from the posterior θt p θ d and then utility theory suppose we are a doctor trying to decide whether to operate on a patient or not we imagine there are states of nature the patient has no cancer the patient has lung cancer or the patient has breast cancer since the action and state space is discrete we can represent the loss function l θ a as a loss matrix such as the following surgery no surgery no cancer lung cancer breast cancer these numbers reﬂects the fact that not performing surgery when the patient has cancer is very bad loss of or depending on the type of cancer since the patient might die not performing surgery when the patient does not have cancer incurs no loss performing surgery when the patient does not have cancer is wasteful loss of and performing surgery when the patient does have cancer is painful but necessary it is natural to ask where these numbers come from ultimately they represent the personal preferences or values of a ﬁctitious doctor and are somewhat arbitrary just as some people prefer chocolate ice cream and others prefer vanilla there is no such thing as the right loss utility function however it can be shown see e g degroot that any set of consistent preferences can be converted to a scalar loss utility function note that utility can be measured on an arbitrary scale such as dollars since it is only relative values that matter people are often squeamish about talking about human lives in monetary terms but all decision making requires sequential decision theory so far we have concentrated on one shot decision problems where we only have to make one decision and then the game ends in setion we will generalize this to multi stage or sequential decision problems such problems frequently arise in many business and engineering settings this is closely related to the problem of reinforcement learning however further discussion of this point is beyond the scope of this book exercises exercise proof that a mixture of conjugate priors is indeed conjugate derive equation exercise optimal threshold on classiﬁcation probability consider a case where we have learned a conditional probability distribution p y x suppose there are only two classes and let p y x and p y x consider the loss matrix below predicted label yˆ true label y a show that the decision yˆ that minimizes the expected loss is equivalent to setting a probability threshold θ and predicting yˆ if θ and yˆ if θ what is θ as a function of and show your work b show a loss matrix where the threshold is show your work exercise reject option in classiﬁers source duda et al in many classiﬁcation problems one has the option either of assigning x to class j or if you are too uncertain of choosing the reject option if the cost for rejects is less than the cost of falsely classifying the object it may be the optimal action let αi mean you choose action i for i c where c is the number of classes and c is the reject action let y j be the true but unknown state of nature deﬁne the loss function as follows λ αi y j if i j and i j c λr i c λs otherwise 122 in otherwords you incur loss if you correctly classify you incur λr loss cost if you choose the reject option and you incur λs loss cost if you make a substitution error misclassiﬁcation tradeoffs and one needs to use some kind of currency to compare different courses of action insurance companies do this all the time ross schachter a decision theorist at stanford university likes to tell a story of a school board who rejected a study on absestos removal from schools because it performed a cost beneﬁt analysis which was considered inhumane because they put a dollar value on children health the result of rejecting the report was that the absestos was not removed which is surely more inhumane in medical domains one often measures utility in terms of qaly or quality adjusted life years instead of dollars but it the same idea of course even if you do not explicitly specify how much you value different people lives your behavior will reveal your implicit values preferences and these preferences can then be converted to a real valued scale such as dollars or qaly inferring a utility function from behavior is called inverse reinforcement learning decision yˆ true label y predict predict reject a show that the minimum risk is obtained if we decide y j if p y j x p y k x for all k i e j is the most probable class and if p y j x λr otherwise we decide to reject b describe qualitatively what happens as λr λs is increased from to i e the relative cost of rejection increases exercise more reject options in many applications the classiﬁer is allowed to reject a test example rather than classifying it into one of the classes consider for example a case in which the cost of a misclassiﬁcation is but the cost of having a human manually make the decison is only we can formulate this as the following loss matrix a suppose p y x is predicted to be which decision minimizes the expected loss b now suppose p y x now which decision minimizes the expected loss c show that in general for this loss matrix but for any posterior distribution there will be two thresholds and such that the optimal decisionn is to predict if reject if and predict if where p y x what are these thresholds exercise newsvendor problem consider the following classic problem in decision theory economics suppose you are trying to decide how much quantity q of some product e g newspapers to buy to maximize your proﬁts the optimal amount will depend on how much demand d you think there is for your product as well as its cost to you c and its selling price p suppose d is unknown but has pdf f d and cdf f d we can evaluate the expected proﬁt by considering two cases if d q then we sell all q items and make proﬁt π p c q but if d q we only sell d items at proﬁt p c d but have wasted c q d on the unsold items so the expected proﬁt if we buy quantity q is eπ q p c qf d dd q q p c df d q c q d f d dd 123 simplify this expression and then take derivatives wrt q to show that the optimal quantity q which maximizes the expected proﬁt satisﬁes f q p c p exercise bayes factors and roc curves 124 let b p d p d be the bayes factor in favor of model suppose we plot two roc curves one computed by thresholding b and the other computed by thresholding p d will they be the same or different explain why exercise bayes model averaging helps predictive accuracy let δ be a quantity that we want to predict let be the observed data and be a ﬁnite set of models suppose our action is to provide a probabilistic prediction p and the loss function is l δ p log p δ we can either perform bayes model averaging and predict using pbma δ p δ m d p m d 125 m m or we could predict using any single model a plugin approximation pm δ p δ m d 126 show that for all models m m the posterior expected loss using bma is lower i e e l δ pbma l e l δ pm 127 where the expectation over δ is with respect to p δ d p δ m d p m d 128 m m hint use the non negativity of the kl divergence exercise mle and model selection for a discrete distribution source jaakkola let x denote the result of a coin toss x for tails x for heads the coin is potentially biased so that heads occurs with probability suppose that someone else observes the coin ﬂip and reports to you the outcome y but this person is unreliable and only reports the result correctly with probability i e p y x is given by assume that is independent of x and a write down the joint probability distribution p x y θ as a table in terms of θ b suppose have the following dataset x y what are the mles for and justify your answer hint note that the likelihood function factorizes p x y θ p y x p x what is p θˆ where denotes this parameter model you may leave your answer in fractional form if you wish c now consider a model with parameters θ representing p x y θ θx y only of these parameters are free to vary since they must sum to one what is the mle of θ what is p d θˆ where denotes this parameter model d suppose we are not sure which model is correct we compute the leave one out cross validated log likelihood of the parameter model and the parameter model as follows n l m log p xi yi m θˆ d i i and θˆ i denotes the mle computed on excluding row i which model will cv pick and why hint notice how the table of counts changes when you omit each training case one at a time e recall that an alternative to cv is to use the bic score deﬁned as bic m d log p d θˆ mle dof m log n where dof m is the number of free parameters in the model compute the bic scores for both models use log base e which model does bic prefer exercise posterior median is optimal estimate under loss prove that the posterior median is optimal estimate under loss exercise decision rule for trading off fps and fns if lfn clfp show that we should pick yˆ iff p y x p y x τ where τ c c frequentist statistics introduction the approach to statistical inference that we described in chapter is known as bayesian statistics perhaps surprisingly this is considered controversial by some people whereas the ap plication of bayes rule to non statistical problems such as medical diagnosis section spam ﬁltering section or airplane tracking section is not controversial the reason for the objection has to do with a misguided distinction between parameters of a statis tical model and other kinds of unknown quantities attempts have been made to devise approaches to statistical inference that avoid treating parameters like random variables and which thus avoid the use of priors and bayes rule such approaches are known as frequentist statistics classical statistics or orthodox statistics instead of being based on the posterior distribution they are based on the concept of a sampling distribution this is the distribution that an estimator has when applied to multiple data sets sampled from the true but unknown distribution see section for details it is this notion of variation across repeated trials that forms the basis for modeling uncertainty used by the frequentist approach by contrast in the bayesian approach we only ever condition on the actually observed data there is no notion of repeated trials this allows the bayesian to compute the probability of one off events as we discussed in section perhaps more importantly the bayesian approach avoids certain paradoxes that plague the frequentist approach see section nevertheless it is important to be familiar with frequentist statistics especially section since it is widely used in machine learning sampling distribution of an estimator in frequentist statistics a parameter estimate θˆ is computed by applying an estimator δ to some data so θˆ δ the parameter is viewed as ﬁxed and the data as random which is the exact opposite of the bayesian approach the uncertainty in the parameter estimate can be measured by computing the sampling distribution of the estimator to understand this parameters are sometimes considered to represent true but unknown physical quantities which are therefore not random however we have seen that it is perfectly reasonable to use a probability distribution to represent one uncertainty about an unknown constant boot true n mle 90 se boot true n mle se 1000 3000 1000 a b figure a bootstrap approximation to the sampling distribution of θˆ for a bernoulli distribution we use b 000 bootstrap samples the n datacases were generated from ber θ a mle with n b mle with n figure generated by bootstrapdemober concept imagine sampling many different data sets d from some true model p θ i e let d x n where xs p θ and θ is the true parameter here s indexes the sampled data set and n is the size of each such dataset now apply the estimator θˆ to each d to get a set of estimates θˆ d as we let s the distribution induced on θ is the sampling distribution of the estimator we will discuss various ways to use the sampling distribution in later sections but ﬁrst we sketch two approaches for computing the sampling distribution itself bootstrap the bootstrap is a simple monte carlo technique to approximate the sampling distribution this is particularly useful in cases where the estimator is a complex function of the true parameters the idea is simple if we knew the true parameters θ we could generate many say s fake datasets each of size n from the true distribution xs p θ for s i n we could then compute our estimator from each sample θˆs f xs and use the empirical distribution of the resulting samples as our estimate of the sampling distribution since θ is unknown the idea of the parametric bootstrap is to generate the samples using θˆ instead an alternative called the non parametric bootstrap is to sample the xs with replacement from the original data and then compute the induced distribution as before some methods for speeding up the bootstrap when applied to massive data sets are discussed in kleiner et al figure shows an example where we compute the sampling distribution of the mle for a bernoulli using the parametric bootstrap results using the non parametric bootstrap are essentially the same we see that the sampling distribution is asymmetric and therefore quite far from gaussian when n when n the distribution looks more gaussian as theory suggests see below a natural question is what is the connection between the parameter estimates θˆs θˆ xs computed by the bootstrap and parameter values sampled from the posterior θs p d conceptually they are quite different but in the common case that that the prior is not very strong they can be quite similar for example figure c d shows an example where we compute the posterior using a uniform beta prior and then sample from it we see that the posterior and the sampling distribution are quite similar so one can think of the bootstrap distribution as a poor man posterior see hastie et al for details however perhaps surprisingly bootstrap can be slower than posterior sampling the reason is that the bootstrap has to ﬁt the model s times whereas in posterior sampling we usually only ﬁt the model once to ﬁnd a local mode and then perform local exploration around the mode such local exploration is usually much faster than ﬁtting a model from scratch large sample theory for the mle in some cases the sampling distribution for some estimators can be computed analytically in particular it can be shown that under certain conditions as the sample size tends to inﬁnity the sampling distribution of the mle becomes gaussian informally the requirement for this result to hold is that each parameter in the model gets to see an inﬁnite amount of data and that the model be identiﬁable unfortunately this excludes many of the models of interest to machine learning nevertheless let us assume we are in a simple setting where the theorem holds the center of the gaussian will be the mle θˆ but what about the variance of this gaussian intuitively the variance of the estimator will be inversely related to the amount of curvature of the likelihood surface at its peak if the curvature is large the peak will be sharp and the variance low in this case the estimate is well determined by contrast if the curvature is small the peak will be nearly ﬂat so the variance is high let us now formalize this intuition deﬁne the score function as the gradient of the log likelihood evaluated at some point θˆ θˆ log p d θ θˆ deﬁne the observed information matrix as the gradient of the negative score function or equivalently the hessian of the nll j θˆ d θˆ log p d θ ˆ in this becomes j θˆ d θ θ log p d θ θˆ this is just a measure of curvature of the log likelihood function at θˆ since we are studying the sampling distribution xn is a set of random variables the fisher information matrix is deﬁned to be the expected value of the observed information matrix in θˆ θ eθ ij θˆ d this is not the usual deﬁnition but is equivalent to it under standard assumptions more precisely the standard deﬁnition is as follows we just give the scalar case to simplify notation i θˆ θ varθ d log p x θ ˆ that is the variance of the score function if θˆ is the mle it is easy to see that eθ d log p x θ ˆ since where eθ f d n f xi p xi θ is the expected value of the function f when applied to data sampled from θ often θ representing the true parameter that generated the data is assumed known so we just write in θˆ in θˆ θ for short furthermore it is easy to see that in θˆ n θˆ because the log likelihood for a sample of size n is just n times steeper than the log likelihood for a sample of size so we can drop the subscript and just write i θˆ θˆ this is the notation that is usually used now let θˆ θˆmle d be the mle where d θ it can be shown that θˆ n θ in θ as n see e g rice for a proof we say that the sampling distribution of the mle is asymptotically normal what about the variance of the mle which can be used as some measure of conﬁdence in the mle unfortunately θ is unknown so we can t evaluate the variance of the sampling distribution however we can approximate the sampling distribution by replacing θ with θˆ consequently the approximate standard errors of θˆk are given by for example from equation we know that the fisher information for a binomial sampling model is i θ θ θ so the approximate standard error of the mle is ˆ ˆ sˆe i ˆ i ˆ θ θ n where θˆ i xi compare this to equation 27 which is the posterior standard deviation under a uniform prior frequentist decision theory in frequentist or classical decision theory there is a loss function and a likelihood but there is no prior and hence no posterior or posterior expected loss thus there is no automatic way of deriving an optimal estimator unlike the bayesian case instead in the frequentist approach we are free to choose any estimator or decision procedure δ x a we want the gradient must be zero at a maximum so the variance reduces to the expected square of the score function i θˆ θ eθ d log p x θ it can be shown e g rice that eθ d log p x θ dθ dθ is a much more intuitive quantity than the variance of the score in practice the frequentist approach is usually only applied to one shot statistical decision problems such as classiﬁcation regression and parameter estimation since its non constructive nature makes it difficult to apply to sequential decision problems which adapt to data online having chosen an estimator we deﬁne its expected loss or risk as follows r θ δ ep d θ il θ δ d r l θ δ d p d θ dd where is data sampled from nature distribution which is represented by parameter θ in other words the expectation is wrt the sampling distribution of the estimator compare this to the bayesian posterior expected loss ρ a d π ep θ d π l θ a r l θ a p θ d π dθ we see that the bayesian approach averages over θ which is unknown and conditions on which is known whereas the frequentist approach averages over thus ignoring the observed data and conditions on θ which is unknown not only is the frequentist deﬁnition unnatural it cannot even be computed because θ is unknown consequently we cannot compare different estimators in terms of their frequentist risk we discuss various solutions to this below bayes risk how do we choose amongst estimators we need some way to convert r θ δ into a single measure of quality r δ which does not depend on knowing θ one approach is to put a prior on θ and then to deﬁne bayes risk or integrated risk of an estimator as follows rb δ ep θ r θ δ r r θ δ p θ dθ a bayes estimator or bayes decision rule is one which minimizes the expected risk δb argmin rb δ δ note that the integrated risk is also called the preposterior risk since it is before we have seen the data minimizing this can be useful for experiment design we will now prove a very important theorem that connects the bayesian and frequentist approaches to decision theory theorem a bayes estimator can be obtained by minimizing the posterior expected loss for each x proof by switching the order of integration we have rb δ r r l y δ x p x y θ p θ dθ x y r l y δ x p x y θ dθ r l y δ x p y x p x ρ δ x x p x x θ figure risk functions for two decision procedures and since has lower worst case risk it is the minimax estimator even though has lower risk for most values of θ thus minimax estimators are overly conservative to minimize the overall expectation we just minimize the term inside for each x so our decision rule is to pick δb x argmin ρ a x a a hence we see that the picking the optimal action on a case by case basis as in the bayesian approach is optimal on average as in the frequentist approach in other words the bayesian approach provides a good way of achieving frequentist goals in fact one can go further and prove the following theorem wald every admissable decision rule is a bayes decision rule with respect to some possibly improper prior distribution this theorem shows that the best way to minimize frequentist risk is to be bayesian see bernardo and smith for further discussion of this point minimax risk obviously some frequentists dislike using bayes risk since it requires the choice of a prior al though this is only in the evaluation of the estimator not necessarily as part of its construction an alternative approach is as follows deﬁne the maximum risk of an estimator as rmax δ max r θ δ θ a minimax rule is one which minimizes the maximum risk δmm argmin rmax δ δ for example in figure we see that has lower worst case risk than ranging over all possible values of θ so it is the minimax estimator see section for an explanation of how to compute a risk function for an actual model minimax estimators have a certain appeal however computing them can be hard and furthermore they are very pessimistic in fact one can show that all minimax estimators are equivalent to bayes estimators under a least favorable prior in most statistical situations excluding game theoretic ones assuming nature is an adversary is not a reasonable assumption admissible estimators the basic problem with frequentist decision theory is that it relies on knowing the true distri bution p θ in order to evaluate the risk however it might be the case that some estimators are worse than others regardless of the value of θ in particular if r θ r θ for all θ θ then we say that dominates the domination is said to be strict if the inequality is strict for some θ an estimator is said to be admissible if it is not strictly dominated by any other estimator example let us give an example based on bernardo and smith consider the problem of estimating the mean of a gaussian we assume the data is sampled from xi θ and use quadratic loss l θ θˆ θ θˆ the corresponding risk function is the mse some possible decision rules or estimators θˆ x δ x are as follows x x the sample mean x x the sample median x a ﬁxed value δκ x the posterior mean under a n θ κ prior n κ δκ x n κx n wx w for δκ we consider a weak prior κ and a stronger prior κ the prior mean is some ﬁxed value we assume is known thus x is the same as δκ x with an inﬁnitely strong prior κ let us now derive the risk functions analytically we can do this since in this toy example we know the true parameter θ in section we show that the mse can be decomposed into squared bias plus variance mse θˆ θ var iθˆ θˆ the sample mean is unbiased so its risk is mse θ var x n 05 risk functions for n 08 04 risk functions for n a b figure risk functions for estimating the mean of a gaussian using data sampled θ the solid dark blue horizontal line is the mle the solid light blue curved line is the posterior mean when κ left n samples right n samples based on figure b of bernardo and smith figure generated by riskfngauss the sample median is also unbiased one can show that the variance is approximately π so mse δ θ π for x the variance is zero so mse θ θ finally for the posterior mean we have mse δκ θ e i wx w θ e i w x θ w θ w n w θ 27 nσ2 θ n κ θ these functions are plotted in figure for n we see that in general the best estimator depends on the value of θ which is unknown if θ is very close to then which just predicts is best if θ is within some reasonable range around then the posterior mean which combines the prior guess of with the actual data is best if θ is far from the mle is best none of this should be suprising a small amount of shrinkage using the posterior mean with a weak prior is usually desirable assuming our prior mean is sensible what is more surprising is that the risk of decision rule sample median is always higher than that of sample mean for every value of θ consequently the sample median is an inadmissible estimator for this particular problem where the data is assumed to come from a gaussian in practice the sample median is often better than the sample mean because it is more robust to outliers one can show minka that the median is the bayes estimator under squared loss if we assume the data comes from a laplace distribution which has heavier tails than a gaussian more generally we can construct robust estimators by using ﬂexible models of our data such as mixture models or non parametric density estimators section and then computing the posterior mean or median stein paradox suppose we have n iid random variables xi n θi and we want to estimate the θi the obvious estimator is the mle which in this case sets θˆi xi it turns out that this is an inadmissible estimator under quadratic loss when n to show this it suffices to construct an estimator that is better the james stein estimator is one such estimator and is deﬁned as follows θˆi bˆx bˆ xi x bˆ xi x where x n xi and b is some tuning constant this estimate shrinks the θi towards the overall mean we derive this estimator using an empirical bayes approach in section it can be shown that this shrinkage estimator has lower frequentist risk mse than the mle sample mean for n this is known as stein paradox the reason it is called a paradox is illustrated by the following example suppose θi is the true iq of student i and xi is his test score why should my estimate of θi depend on the global mean x and hence on some other student scores one can create even more paradoxical examples by making the different dimensions be qualitatively different e g is my iq is the average rainfall in vancouver etc the solution to the paradox is the following if your goal is to estimate just θi you cannot do better than using xi but if the goal is to estimate the whole vector θ and you use squared error as your loss function then shrinkage helps to see this suppose we want to estimate θ from a single sample x n θ i a simple estimate is x but this will overestimate the result since e e i i i n θ consequently we can reduce our risk by pooling information even from unrelated sources and shrinking towards the overall mean in section we give a bayesian explanation for this see also efron and morris admissibility is not enough it seems clear that we can restrict our search for good estimators to the class of admissible estimators but in fact it is easy to construct admissible estimators as we show in the following example theorem let x θ and consider estimating θ under squared loss let x a constant independent of the data this is an admissible estimator proof suppose not then there is some other estimator with smaller risk so r θ r θ where the inequality must be strict for some θ suppose the true parameter is θ then r θ and r θ r x x dx since r θ r θ for all θ and r we have r and hence x x thus the only way can avoid having higher risk than at some speciﬁc point is by being equal to hence there is no other estimator with strictly lower risk so is admissible desirable properties of estimators since frequentist decision theory does not provide an automatic way to choose the best estimator we need to come up with other heuristics for choosing amongst them in this section we discuss some properties we would like estimators to have unfortunately we will see that we cannot achieve all of these properties at the same time consistent estimators an estimator is said to be consistent if it eventually recovers the true parameters that generated the data as the sample size goes to inﬁnity i e θˆ θ as where the arrow denotes convergence in probability of course this concept only makes sense if the data actually comes from the speciﬁed model with parameters θ which is not usually the case with real data nevertheless it can be a useful theoretical property it can be shown that the mle is a consistent estimator the intuitive reason is that maxi mizing likelihood is equivalent to minimizing kl p θ p θˆ where p θ is the true distribution and p θˆ is our estimate we can achieve kl divergence iff θˆ θ unbiased estimators the bias of an estimator is deﬁned as bias θˆ ep d θ iθˆ d θ 32 where θ is the true parameter value if the bias is zero the estimator is called unbiased this means the sampling distribution is centered on the true parameter for example the mle for a gaussian mean is unbiased bias μˆ e x μ e n n i μ nμ n μ if the model is unidentiﬁable the mle may select a set of parameters that is different from the true parameters but for which the induced distribution p θˆ is the same as the exact distribution such parameters are said to be likelihood equivalent however the mle for a gaussian variance is not an unbiased estimator of in fact one can show exercise that e n n however the following estimator n x x 35 is an unbiased estimator which we can easily prove as follows e e n n n in matlab var x returns whereas var x returns the mle for large enough n n the difference will be negligible although the mle may sometimes be a biased estimator one can show that asymptotically it is always unbiased this is necessary for the mle to be a consistent estimator although being unbiased sounds like a desirable property this is not always true see sec tion and lindley for discussion of this point minimum variance estimators it seems intuitively reasonable that we want our estimator to be unbiased although we shall give some arguments against this claim below however being unbiased is not enough for example suppose we want to estimate the mean of a gaussian from d xn the estimator that just looks at the ﬁrst data point θˆ d is an unbiased estimator but will generally be further from θ than the empirical mean x which is also unbiased so the variance of an estimator is also important a natural question is how long can the variance go a famous result called the cramer rao lower bound provides a lower bound on the variance of any unbiased estimator more precisely theorem cramer rao inequality let xn p x and θˆ θˆ xn be an unbiased estimator of then under various smoothness assumptions on p x we have var θˆ ni where i is the fisher information matrix see section a proof can be found e g in rice it can be shown that the mle achieves the cramer rao lower bound and hence has the smallest asymptotic variance of any unbiased estimator thus mle is said to be asymptotically optimal the bias variance tradeoff although using an unbiased estimator seems like a good idea this is not always the case to see why suppose we use quadratic loss as we showed above the corresponding risk is the mse we now derive a very useful decomposition of the mse all expectations and variances are wrt θˆ θˆ d denote the estimate and θ e θˆ vary d then we have denote the expected value of the estimate as we e θˆ θ e θˆ θ θ θ e θˆ θ θ θ e θˆ θ θ θ e θˆ θ θ θ var θˆ θˆ in words this is called the bias variance tradeoff see e g geman et al what it means is that it might be wise to use a biased estimator so long as it reduces our variance assuming our goal is to minimize squared error example estimating a gaussian mean let us give an example based on hoff suppose we want to estimate the mean of a gaussian from x xn we assume the data is sampled from xi θ an obvious estimate is the mle this has a bias of and a variance of var x θ n but we could also use a map estimate in section we show that the map estimate under a gaussian prior of the form n is given by n x x θ wx w θ n n where w controls how much we trust the mle compared to our prior this is also the posterior mean since the mean and mode of a gaussian are the same the bias and variance are given by e x θ w θ w θ var x w n sampling distribution truth prior n postmean0 mse of postmean mse of mle a sample size b figure left sampling distribution of the map estimate with different prior strengths the mle corresponds to right mse relative to that of the mle versus sample size based on figure of hoff figure generated by samplingdistgaussshrinkage so although the map estimate is biased assuming w it has lower variance let us assume that our prior is slightly misspeciﬁed so we use whereas the truth is θ in figure a we see that the sampling distribution of the map estimate for is biased away from the truth but has lower variance is narrower than that of the mle in figure b we plot mse x mse x vs n we see that the map estimate has lower mse than the mle especially for small sample size for the case corresponds to the mle and the case corresponds to a strong prior which hurts performance because the prior mean is wrong it is clearly important to tune the strength of the prior a topic we discuss later example ridge regression another important example of the bias variance tradeoff arises in ridge regression which we discuss in section in brief this corresponds to map estimation for linear regression under a gaussian prior p w w λ the zero mean prior encourages the weights to be small which reduces overﬁtting the precision term λ controls the strength of this prior setting λ results in the mle using λ results in a biased estimate to illustrate the effect on the variance consider a simple example figure on the left plots each individual ﬁtted curve and on the right plots the average ﬁtted curve we see that as we increase the strength of the regularizer the variance decreases but the bias increases bias variance tradeoff for classiﬁcation if we use loss instead of squared error the above analysis breaks down since the frequentist risk is no longer expressible as squared bias plus variance in fact one can show exercise of hastie et al that the bias and variance combine multiplicatively if the estimate is on ln ln ln ln figure illustration of bias variance tradeoff for ridge regression we generate data sets from the true function shown in solid green left we plot the regularized ﬁt for different data sets we use linear regression with a gaussian rbf expansion with centers evenly spread over the interval right we plot the average of the ﬁts averaged over all datasets top row strongly regularized we see that the individual ﬁts are similar to each other low variance but the average is far from the truth high bias bottom row lightly regularized we see that the individual ﬁts are quite different from each other high variance but the average is close to the truth low bias based on bishop figure figure generated by the correct side of the decision boundary then the bias is negative and decreasing the variance will decrease the misclassiﬁcation rate but if the estimate is on the wrong side of the decision boundary then the bias is positive so it pays to increase the variance friedman this little known fact illustrates that the bias variance tradeoff is not very useful for classiﬁcation it is better to focus on expected loss see below not directly on bias and variance we can approximate the expected loss using cross validatinon as we discuss in section empirical risk minimization frequentist decision theory suffers from the fundamental problem that one cannot actually compute the risk function since it relies on knowing the true data distribution by contrast the bayesian posterior expected loss can always be computed since it conditions on the the data rather than conditioning on θ however there is one setting which avoids this problem and that is where the task is to predict observable quantities as opposed to estimating hidden variables or parameters that is instead of looking at loss functions of the form l θ δ d where θ is the true but unknown parameter and δ d is our estimator let us look at loss functions of the form l y δ x where y is the true but unknown response and δ x is our prediction given the input x in this case the frequentist risk becomes r p δ e x y p l y δ x l y δ x p x y where p represents nature distribution of course this distribution is unknown but a simple approach is to use the empirical distribution derived from some training data to approximate p i e p x y p emp n x y δxi n i x δyi y we then deﬁne the empirical risk as follows r d d r p δ l y δ x in the case of loss l y δ x i y δ x this becomes the misclassiﬁcation rate in the case of squared error loss l y δ x y δ x this becomes the mean squared error we deﬁne the task of empirical risk minimization or erm as ﬁnding a decision procedure typically a classiﬁcation rule to minimize the empirical risk δerm d argmin remp d δ in the unsupervised case we eliminate all references to y and replace l y δ x with l x δ x where for example l x δ x x δ x which measures the reconstruc tion error we can deﬁne the decision rule using δ x decode encode x as in vector quantization section or pca section finally we deﬁne the empirical risk as r d δ l x δ x of course we can always trivially minimize this risk by setting δ x x so it is critical that the encoder decoder go via some kind of bottleneck regularized risk minimization note that the empirical risk is equal to the bayes risk if our prior about nature distribution is that it is exactly equal to the empirical distribution minka e r p δ p pemp remp d δ therefore minimizing the empirical risk will typically result in overﬁtting it is therefore often necessary to add a complexity penalty to the objective function rt d δ remp d δ λc δ where c δ measures the complexity of the prediction function δ x and λ controls the strength of the complexity penalty this approach is known as regularized risk minimization rrm note that if the loss function is negative log likelihood and the regularizer is a negative log prior this is equivalent to map estimation the two key issues in rrm are how do we measure complexity and how do we pick λ for a linear model we can deﬁne the complexity of in terms of its degrees of freedom discussed in section for more general models we can use the vc dimension discussed in section to pick λ we can use the methods discussed in section structural risk minimization the regularized risk minimization principle says that we should ﬁt the model for a given complexity penalty by using δˆλ argmin remp δ λc δ δ but how should we pick λ we cannot using the training set since this will underestimate the true risk a problem known as optimism of the training error as an alternative we can use the following rule known as the structural risk minimization principle vapnik λˆ argmin rˆ δˆλ λ where rˆ δ is an estimate of the risk there are two widely used estimates cross validation and theoretical upper bounds on the risk we discuss both of these below estimating the risk using cross validation we can estimate the risk of some estimator using a validation set if we don t have a separate validation set we can use cross validation cv as we brieﬂy discussed in section more precisely cv is deﬁned as follows let there be n d data cases in the training set denote the data in the k th test fold by k and all the other data by k in stratiﬁed cv these folds are chosen so the class proportions if discrete labels are present are roughly equal in each fold let be a learning algorithm or ﬁtting function that takes a dataset and a model index m this could a discrete index such as the degree of a polynomial or a continuous index such as the strength of a regularizer and returns a parameter vector θˆm f d m finally let be a prediction function that takes an input and a parameter vector and returns a prediction yˆ p x θˆ f x θˆ thus the combined ﬁt predict cycle is denoted as fm x d p x f d m the k fold cv estimate of the risk of fm is deﬁned by k r m d k l y p x f d m note that we can call the ﬁtting algorithm once per fold let f k x x k m be the function that was trained on all the data except for the test data in fold k then we can rewrite the cv estimate as k r m d k n l yi fk xi n l yi fk i xi n k i dk i where k i is the fold in which i is used as test data in other words we predict yi using a model that was trained on data that does not contain xi of k n the method is known as leave one out cross validation or loocv in this case n the estimated risk becomes r m d n l y f i x where f i x p x f d i m this requires ﬁtting the model n times where for f i we omit the i th training case fortunately for some model classes and loss functions namely linear models and quadratic loss we can ﬁt the model once and analytically remove the effect of the i th training case this is known as generalized cross validation or gcv example using cv to pick λ for ridge regression as a concrete example consider picking the strength of the regularizer in penalized linear regression we use the following rule λˆ arg min λ λmin λmax r λ dtrain k where λmin λmax is a ﬁnite range of λ values that we search over and r λ train k is the k fold cv estimate of the risk of using λ given by k k r λ d k l y f x where f k x xt wˆ λ k is the prediction function trained on data excluding fold k and wˆ λ arg minw nll w λ w is the map estimate figure b gives an example of a cv estimate of the risk vs log λ where the loss function is squared error when performing classiﬁcation we usually use loss in this case we optimize a convex upper bound on the empirical risk to estimate wλm but we optimize the cv estimate of the risk itself to estimate λ we can handle the non smooth loss function when estimating λ because we are using brute force search over the entire one dimensional space when we have more than one or two tuning parameters this approach becomes infeasible in such cases one can use empirical bayes which allows one to optimize large numbers of hyper parameters using gradient based optimizers instead of brute force search see section for details mean squared error fold cross validation ntrain log lambda a log lambda b figure a mean squared error for t penalized degree polynomial regression vs log regularizer same as in figures except now we have n training points instead of the stars correspond to the values used to plot the functions in figure b cv estimate the vertical scale is truncated for clarity the blue line corresponds to the value chosen by the one standard error rule figure generated by linregpolyvsregdemo the one standard error rule the above procedure estimates the risk but does not give any measure of uncertainty a standard frequentist measure of uncertainty of an estimate is the standard error of the mean deﬁned by σˆ se n 64 n where is an estimate of the variance of the loss l l l l y fk i x l l note that σ measures the intrinsic variability of li across samples whereas se measures our uncertainty about the mean l suppose we apply cv to a set of models and compute the mean and se of their estimated risks a common heuristic for picking a model from these noisy estimates is to pick the value which corresponds to the simplest model whose risk is no more than one standard error above the risk of the best model this is called the one standard error rule hastie et al for example in figure we see that this heuristic does not choose the lowest point on the curve but one that is slightly to its right since that corresponds to a more heavily regularized model with essentially the same empirical performance cv for model selection in non probabilistic unsupervised learning if we are performing unsupervised learning we must use a loss function such as l x δ x x δ x which measures reconstruction error here δ x is some encode decode scheme however as we discussed in section we cannot use cv to determine the complexity of δ since we will always get lower loss with a more complex model even if evaluated on the test set this is because more complex models will compress the data less and induce less distortion consequently we must either use probabilistic models or invent other heuristics upper bounding the risk using statistical learning theory the principle problem with cross validation is that it is slow since we have to ﬁt the model multiple times this motivates the desire to compute analytic approximations or bounds to the generalization error this is the studied in the ﬁeld of statistical learning theory slt more precisely slt tries to bound the risk r p h for any data distribution p and hypothesis h in terms of the empirical risk remp h the sample size n and the size of the hypothesis space let us initially consider the case where the hypothesis space is ﬁnite with size dim in other words we are selecting a model hypothesis from a ﬁnite list rather than optimizing real valued parameters then we can prove the following theorem for any data distribution p and any dataset of size n drawn from p the probability that our estimate of the error rate will be more than e wrong in the worst case is upper bounded as follows p max r h h emp d h r p h e dim h e proof to prove this we need two useful results first hoeffding inequality which states that if xn ber θ then for any e p x θ e 67 where x n xi second the union bound which says that if ad are a set of finally for notational brevity let r h r h p be the true risk and rˆn h remp h be the empirical risk using these results we have p max rˆn h r h e p f i rˆn h r h e h h h h p rˆn h r h e h h dim h e h h ths bound tells us that the optimism of the training error increases with dim but de creases with n as is to be expected if the hypothesis space is inﬁnite e g we have real valued parameters we cannot use dim instead we can use a quantity called the vapnik chervonenkis or vc dimen sion of the hypothesis class see vapnik for details stepping back from all the theory the key intuition behind statistical learning theory is quite simple suppose we ﬁnd a model with low empirical risk if the hypothesis space is very big relative to the data size then it is quite likely that we just got lucky and were given a data set that is well modeled by our chosen function by chance however this does not mean that such a function will have low generalization error but if the hypothesis class is sufficiently constrained in size and or the training set is sufficiently large then we are unlikely to get lucky in this way so a low empirical risk is evidence of a low true risk note that optimism of the training error does not necessarily increase with model complexity but it does increase with the number of different models that are being searched over the advantage of statistical learning theory compared to cv is that the bounds on the risk are quicker to compute than using cv the disadvantage is that it is hard to compute the vc dimension for many interesting models and the upper bounds are usually very loose although see kaariainen and langford one can extend statistical learning theory by taking computational complexity of the learner into account this ﬁeld is called computational learning theory or colt most of this work focuses on the case where h is a binary classiﬁer and the loss function is loss if we observe a low empirical risk and the hypothesis space is suitably small then we can say that our estimated function is probably approximately correct or pac a hypothesis space is said to be efficiently pac learnable if there is a polynomial time algorithm that can identify a function that is pac see kearns and vazirani for details surrogate loss functions minimizing the loss in the erm rrm framework is not always easy for example we might want to optimize the auc or scores or more simply we might just want to minimize the loss as is common in classiﬁcation unfortunately the risk is a very non smooth objective and hence is hard to optimize one alternative is to use maximum likelihood estimation instead since log likelihood is a smooth convex upper bound on the risk as we show below to see this consider binary logistic regression and let yi suppose our decision function computes the log odds ratio f x log p y xi w wt x η p y xi w then the corresponding probability distribution on the output label is p yi xi w sigm yiηi 72 let us deﬁne the log loss as as lnll y η log p y x w log e yη 73 figure illustration of various loss functions for binary classiﬁcation the horizontal axis is the margin yη the vertical axis is the loss the log loss uses log base figure generated by hingelossplot it is clear that minimizing the average log loss is equivalent to maximizing the likelihood now consider computing the most probable label which is equivalent to using yˆ if ηi and yˆ if ηi the loss of our function becomes y η i y yˆ i yη figure plots these two loss functions we see that the nll is indeed an upper bound on the loss log loss is an example of a surrogate loss function another example is the hinge loss lhinge y η max yη see figure for a plot we see that the function looks like a door hinge hence its name this loss function forms the basis of a popular classiﬁcation method known as support vector machines svm which we will discuss in section the surrogate is usually chosen to be a convex upper bound since convex functions are easy to minimize see e g bartlett et al for more information pathologies of frequentist statistics i believe that it would be very difficult to persuade an intelligent person that current frequentist statistical practice was sensible but that there would be much less difficulty with an approach via likelihood and bayes theorem george box frequentist statistics exhibits various forms of weird and undesirable behaviors known as pathologies we give a few examples below in order to caution the reader these and other examples are explained in more detail in lindley lindley and phillips lindley berger jaynes minka counter intuitive behavior of conﬁdence intervals a conﬁdence interval is an interval derived from the sampling distribution of an estimator whereas a bayesian credible interval is derived from the posterior of a parameter as we dis cussed in section more precisely a frequentist conﬁdence interval for some parameter θ is deﬁned by the following rather un natural expression cαt θ u p d θ u d d θ α 76 that is if we sample hypothetical future data from θ then u is a conﬁdence interval if the parameter θ lies inside this interval α percent of the time let us step back for a moment and think about what is going on in bayesian statistics we condition on what is known namely the observed data and average over what is not known namely the parameter θ in frequentist statistics we do exactly the opposite we condition on what is unknown namely the true parameter value θ and average over hypothetical future data sets this counter intuitive deﬁnition of conﬁdence intervals can lead to bizarre results consider the following example from berger suppose we draw two integers d from if x θ otherwise if θ we would expect the following outcomes each with probability let m min and deﬁne the following conﬁdence interval d u d m m for the above samples this yields hence equation 79 is clearly a ci since is contained in of these intervals however if then p θ so we know that θ must be yet we only have conﬁdence in this fact another less contrived example is as follows suppose we want to estimate the parameter θ of a bernoulli distribution let x n xi be the sample mean the mle is θˆ x an n i called a wald interval and is based on a gaussian approximation to the binomial distribution compare to equation 27 now consider a single trial where n and the mle is which overﬁts as we saw in section but our conﬁdence interval is also which seems even worse it can be argued that the above ﬂaw is because we approximated the true sampling distribution with a gaussian or because the sample size was to small or the parameter too extreme however the wald interval can behave badly even for large n and non extreme parameters brown et al p values considered harmful suppose we want to decide whether to accept or reject some baseline model which we will call the null hypothesis we need to deﬁne some decision rule in frequentist statistics it is standard to ﬁrst compute a quantity called the p value which is deﬁned as the probability under the null of observing some test statistic f such as the chi squared statistic that is as large or larger than that actually observed pvalue d p f d f d d this quantity relies on computing a tail area probability of the sampling distribution we give an example of how to do this below given the p value we deﬁne our decision rule as follows we reject the null hypothesis iff the p value is less than some threshold such as α 05 if we do reject it we say the difference between the observed test statistic and the expected test statistic is statistically signiﬁcant at level α this approach is known as null hypothesis signiﬁcance testing or nhst this procedure guarantees that our expected type i false positive error rate is at most α this is sometimes interpreted as saying that frequentist hypothesis testing is very conservative since it is unlikely to accidently reject the null hypothesis but in fact the opposite is the case because this method only worries about trying to reject the null it can never gather evidence in favor of the null no matter how large the sample size because of this p values tend to overstate the evidence against the null and are thus very trigger happy in general there can be huge differences between p values and the quantity that we really care about which is the posterior probability of the null hypothesis given the data p in particular sellke et al show that even if the p value is as slow as 05 the posterior probability of is at least and often much higher so frequentists often claim to have signiﬁcant evidence of an effect that cannot be explained by the null hypothesis whereas bayesians are usually more conservative in their claims for example p values have been used to prove that esp extra sensory perception is real wagenmakers et al even though esp is clearly very improbable for this reason p values have been banned from certain medical journals matthews another problem with p values is that their computation depends on decisions you make about when to stop collecting data even if these decisions don t change the data you actually observed for example suppose i toss a coin n times and observe successes heads and f failures tails so n f in this case n is ﬁxed and and hence f is random the relevant sampling model is the binomial bin n θ n θs θ n let the null hypothesis be that the coin is fair θ where θ is the probability of success heads the one sided p value using test statistic t is the reason we cannot just compute the probability of the observed value of the test statistic is that this will have probability zero under a pdf the p value is deﬁned in terms of the cdf so is always a number between and the two sided p value is bin bin in either case the p value is larger than the magical threshold so a frequentist would not reject the null hypothesis now suppose i told you that i actually kept tossing the coin until i observed f tails in this case f is ﬁxed and n and hence n f is random the probability model becomes the negative binomial distribution given by negbinom f θ f θs θ f where f n note that the term which depends on θ is the same in equations and 85 so the posterior over θ would be the same in both cases however these two interpretations of the same data give different p values in particular under the negative binomial model we get p p s h so the p value is and suddenly there seems to be signiﬁcant evidence of bias in the coin obviously this is ridiculous the data is the same so our inferences about the coin should be the same after all i could have chosen the experimental protocol at random it is the outcome of the experiment that matters not the details of how i decided which one to run although this might seem like just a mathematical curiosity this also has signiﬁcant practical implications in particular the fact that the stopping rule affects the computation of the p value means that frequentists often do not terminate experiments early even when it is obvious what the conclusions are lest it adversely affect their statistical analysis if the experiments are costly or harmful to people this is obviously a bad idea perhaps it is not surprising then that the us food and drug administration fda which regulates clinical trials of new drugs has recently become supportive of bayesian since bayesian methods are not affected by the stopping rule the likelihood principle the fundamental reason for many of these pathologies is that frequentist inference violates the likelihood principle which says that inference should be based on the likelihood of the observed data not based on hypothetical future data that you have not observed bayes obviously satisﬁes the likelihood principle and consequently does not suffer from these pathologies a compelling argument in favor of the likelihood principle was presented in birnbaum who showed that it followed automatically from two simpler principles the ﬁrst of these is the sufficiency principle which says that a sufficient statistic contains all the relevant information see http yamlb wordpress com the us fda is becoming progressively more bayes ian about an unknown parameter arguably this is true by deﬁnition the second principle is known as weak conditionality which says that inferences should be based on the events that happened not which might have happened to motivate this consider an example from berger suppose we need to analyse a substance and can send it either to a laboratory in new york or in california the two labs seem equally good so a fair coin is used to decide between them the coin comes up heads so the california lab is chosen when the results come back should it be taken into account that the coin could have come up tails and thus the new york lab could have been used most people would argue that the new york lab is irrelevant since the tails event didn t happen this is an example of weak conditionality given this principle one can show that all inferences should only be based on what was observed which is in contrast to standard frequentist procedures see berger and wolpert for further details on the likelihood principle why isn t everyone a bayesian given these fundamental ﬂaws of frequentist statistics and the fact that bayesian methods do not have such ﬂaws an obvious question to ask is why isn t everyone a bayesian the frequentist statistician bradley efron wrote a paper with exactly this title efron his short paper is well worth reading for anyone interested in this topic below we quote his opening section the title is a reasonable question to ask on at least two counts first of all everone used to be a bayesian laplace wholeheatedly endorsed bayes formulation of the inference problem and most century scientists followed suit this included gauss whose statistical work is usually presented in frequentist terms a second and more important point is the cogency of the bayesian argument modern statisticians following the lead of savage and de finetti have advanced powerful theoret ical arguments for preferring bayesian inference a byproduct of this work is a disturbing catalogue of inconsistencies in the frequentist point of view nevertheless everyone is not a bayesian the current era is the ﬁrst century in which statistics has been widely used for scientiﬁc reporting and in fact century statistics is mainly non bayesian however lindley predicts a change for the century time will tell whether lindley was right exercises exercise pessimism of loocv source suppose we have a completely random labeled dataset i e the features x tell us nothing about the class labels y with examples of class and examples of class where n1 n2 what is the best misclassiﬁcation rate any method can achieve what is the estimated misclassiﬁcation rate of the same method using loocv exercise james stein estimator for gaussian means consider the stage model yi θi n θi and θi μ n τ suppose is known and we observe the following data points i 1528 1470 a find the ml ii estimates of and τ b find the posterior estimates e θi yi and var θi yi for i the other terms i are computed similarly c give a credible interval for p θi yi for i do you trust this interval assuming the gaussian assumption is reasonable i e is it likely to be too large or too small or just right d what do you expect would happen to your estimates if were much smaller say you do not need to compute the numerical answer just brieﬂy explain what would happen qualitatively and why exercise is biased show that n xn μˆ is a biased estimator of i e show xn n μ σ xn hint note that xn are independent and use the fact that the expectation of a product of independent random variables is the product of the expectations exercise estimation of when μ is known suppose we sample xn μ where μ is a known constant derive an expression for the mle for in this case is it unbiased linear regression introduction linear regression is the work horse of statistics and supervised machine learning when augmented with kernels or other forms of basis function expansion it can model also non linear relationships and when the gaussian output is replaced with a bernoulli or multinoulli distribution it can be used for classiﬁcation as we will see below so it pays to study this model in detail model speciﬁcation as we discussed in section linear regression is a model of the form p y x θ n y wt x linear regression can be made to model non linear relationships by replacing x with some non linear function of the inputs φ x that is we use p y x θ n y wt φ x this is known as basis function expansion note that the model is still linear in the parameters w so it is still called linear regression the importance of this will become clear below a simple example are polynomial basis functions where the model has the form φ x x xd figure illustrates the effect of changing d increasing the degree d allows us to create increasingly complex functions we can also apply linear regression to more than input for example consider modeling temperature as a function of location figure a plots e y x and figure b plots e y x w3x2 maximum likelihood estimation least squares a common way to esitmate the parameters of a statistical model is to compute the mle which is deﬁned as θˆ arg max log p d θ a b figure linear regression applied to data vertical axis is temperature horizontal axes are location within a room data was collected by some remote sensing motes at intel lab in berkeley ca data courtesy of romain thibaux a the ﬁtted plane has the form fˆ x b temperature data is ﬁtted with a quadratic of the form fˆ x w3x2 produced by surfacefitdemo it is common to assume the training examples are independent and identically distributed commonly abbreviated to iid this means we can write the log likelihood as follows n θ log p d θ log p yi xi θ i instead of maximizing the log likelihood we can equivalently minimize the negative log likeli hood or nll n nll θ log p yi xi θ i the nll formulation is sometimes more convenient since many optimization software packages are designed to ﬁnd the minima of functions rather than maxima now let us apply the method of mle to the linear regression setting inserting the deﬁnition of the gaussian into the above we ﬁnd that the log likelihood is given by t rss w n log rss stands for residual sum of squares and is deﬁned by rss w n yi wt xi i the rss is also called the sum of squared errors or sse and sse n is called the mean squared error or mse it can also be written as the square of the norm of the vector of sum of squares error contours for linear regression a b figure a in linear least squares we try to minimize the sum of squared distances from each training point denoted by a red circle to its approximation denoted by a blue cross that is we minimize the sum of the lengths of the little vertical blue lines the red diagonal line represents yˆ x which is the least squares regression line note that these residual lines are not perpendicular to the least squares line in contrast to figure figure generated by residualsdemo b contours of the rss error surface for the same example the red cross represents the mle w figure generated by contoursssedemo residual errors rss w e e2 where ei yi wt xi we see that the mle for w is the one that minimizes the rss so this method is known as least squares this method is illustrated in figure a the training data xi yi are shown as red circles the estimated values xi yˆi are shown as blue crosses and the residuals ei yi yˆi are shown as vertical blue lines the goal is to ﬁnd the setting of the parameters the slope and intercept such that the resulting red line minimizes the sum of squared residuals the lengths of the vertical blue lines in figure b we plot the nll surface for our linear regression example we see that it is a quadratic bowl with a unique minimum which we now derive importantly this is true even if we use basis function expansion such as polynomials because the nll is still linear in the parameters w even if it is not linear in the inputs x derivation of the mle first we rewrite the objective in a form that is more amenable to differentiation nll w y xw t y xw wt xt x w w x y where n n i xi d xt x xixt i i xi dxi is the sum of squares matrix and n xt y xiyi i using results from equation we see that the gradient of this is given by n g w xt xw xt y xi wt xi yi i equating to zero we get xt xw xt y this is known as the normal equation the corresponding solution wˆ to this linear system of equations is called the ordinary least squares or ols solution which is given by geometric interpretation this equation has an elegant geometrical intrepretation as we now explain we assume n d so we have more examples than features the columns of x deﬁne a linear subspace of dimensionality d which is embedded in n dimensions let the j th column be x j which is a vector in rn this should not be confused with xi rd which represents the i th data case similarly y is a vector in rn for example suppose we have n examples in d dimensions 8957 x y 6130 these vectors are illustrated in figure we seek a vector yˆ rn that lies in this linear subspace and is as close as possible to y i e we want to ﬁnd argmin yˆ span x x d y yˆ since yˆ span x there exists some weight vector w such that yˆ wdx d xw yˆ y figure graphical interpretation of least squares for n examples and d features x and x are vectors in together they deﬁne a plane y is also a vector in but does not lie on this plane the orthogonal projection of y onto this plane is denoted yˆ the red line from y to yˆ is the residual whose norm we want to minimize for visual clarity all vectors have been converted to unit norm figure generated by leastsquaresprojection to minimize the norm of the residual y yˆ we want the residual vector to be orthogonal to every column of x so x t y yˆ for j d hence x t y yˆ xt y xw w xt x y hence our projected value of y is given by yˆ xwˆ x xt x y this corresponds to an orthogonal projection of y onto the column space of x the projection matrix p x xt x is called the hat matrix since it puts the hat on y convexity when discussing least squares we noted that the nll had a bowl shape with a unique minimum the technical term for functions like this is convex convex functions play a very important role in machine learning let us deﬁne this concept more precisely we say a set is convex if for any θ θt we have λθ λ θt s λ a b figure a illustration of a convex set b illustration of a nonconvex set x y a a b b figure a illustration of a convex function we see that the chord joining x f x to y f y lies above the function b a function that is neither convex nor concave a is a local minimum b is a global minimum figure generated by convexfnhand that is if we draw a line from θ to θt all points on the line lie inside the set see figure a for an illustration of a convex set and figure b for an illustration of a non convex set a function f θ is called convex if its epigraph the set of points above the function deﬁnes a convex set equivalently a function f θ is called convex if it is deﬁned on a convex set and if for any θ θt s and for any λ we have f λθ λ θt λf θ λ f θt see figure for a example a function is called strictly convex if the inequality is strict a function f θ is concave if f θ is convex examples of scalar convex functions in clude eθ and θ log θ for θ examples of scalar concave functions include log θ and θ intuitively a strictly convex function has a bowl shape and hence has a unique global minimum θ corresponding to the bottom of the bowl hence its second derivative must be positive everywhere d f θ a twice continuously differentiable multivariate function f is dθ convex iff its hessian is positive deﬁnite for all θ f often corresponds to the nll in the machine learning context the function recall that the hessian is the matrix of second partial derivatives deﬁned by hjk matrix h is positive deﬁnite iff vt hv for any non zero vector v θ also recall that a θj θk linear data with noise and outliers least squares laplace a b figure a illustration of robust linear regression figure generated by linregrobustdemocombined b illustration of t t and huber loss functions figure generated by huberlossdemo models where the nll is convex are desirable since this means we can always ﬁnd the globally optimal mle we will see many examples of this later in the book however many models of interest will not have concave likelihoods in such cases we will discuss ways to derive locally optimal parameter estimates robust linear regression it is very common to model the noise in regression models using a gaussian distribution with zero mean and constant variance ei where ei yi wt xi in this case maximizing likelihood is equivalent to minimizing the sum of squared residuals as we have seen however if we have outliers in our data this can result in a poor ﬁt as illustrated in figure a the outliers are the points on the bottom of the ﬁgure this is because squared error penalizes deviations quadratically so points far from the line have more affect on the ﬁt than points near to the line one way to achieve robustness to outliers is to replace the gaussian distribution for the response variable with a distribution that has heavy tails such a distribution will assign higher likelihood to outliers without having to perturb the straight line to explain them one possibility is to use the laplace distribution introduced in section if we use this as our observation model for regression we get the following likelihood p y x w b lap y wt x b exp b y w x the robustness arises from the use of y wt x instead of y wt x for simplicity we will assume b is ﬁxed let ri yi wt xi be the i th residual the nll has the form w ri w i likelihood prior name section gaussian uniform least squares gaussian gaussian ridge gaussian laplace lasso laplace uniform robust regression student uniform robust regression exercise table summary of various likelihoods and priors used for linear regression the likelihood refers to the distributional form of p y x w and the prior refers to the distributional form of p w map estimation with a uniform distribution corresponds to mle unfortunately this is a non linear objective function which is hard to optimize fortunately we can convert the nll to a linear objective subject to linear constraints using the following split variable trick first we deﬁne ri r r and then we impose the linear inequality constraints that r and ri now the constrained objective becomes min r r t r r wt xi r r yi 27 this is an example of a linear program with d unknowns and constraints since this is a convex optimization problem it has a unique solution to solve an lp we must ﬁrst write it in standard form which as follows min f t θ t aθ b aeqθ beq l θ u in our current example θ w r r f a b aeq x i i beq y l u this can be solved by any lp solver see e g boyd and vandenberghe see figure a for an example of the method in action an alternative to using nll under a laplace likelihood is to minimize the huber loss function huber deﬁned as follows lh r δ δ r if r δ this is equivalent to for errors that are smaller than δ and is equivalent to for larger errors see figure b the advantage of this loss function is that it is everywhere differentiable using the fact that d r sign r if r we can also check that the function is continuous since the gradients of the two parts of the function match at r δ namely d lh r δ r δ δ consequently optimizing the huber loss is much faster than using the laplace likelihood since we can use standard smooth optimization methods such as quasi newton instead of linear programming figure a gives an illustration of the huber loss function the results are qualitatively similiar to the probabilistic methods in fact it turns out that the huber method also has a probabilistic interpretation although it is rather unnatural pontil et al ln lambda 135 ln lambda 15 15 15 a 15 15 b figure degree polynomial ﬁt to n data points with increasing amounts of t regularization data was generated from noise with variance the error bars representing the noise variance get wider as the ﬁt gets smoother since we are ascribing more of the data variation to the noise figure generated by linregpolyvsregdemo ridge regression one problem with ml estimation is that it can result in overﬁtting in this section we discuss a way to ameliorate this problem by using map estimation with a gaussian prior for simplicity we assume a gaussian likelihood rather than a robust likelihood basic idea the reason that the mle can overﬁt is that it is picking the parameter values that are the best for modeling the training data but if the data is noisy such parameters often result in complex functions as a simple example suppose we ﬁt a degree polynomial to n data points using least squares the resulting curve is very wiggly as shown in figure a the corresponding least squares coefficients excluding are as follows 109 452 224 058 730 526 we see that there are many large positive and negative numbers these balance out exactly to make the curve wiggle in just the right way so that it almost perfectly interpolates the data but this situation is unstable if we changed the data a little the coefficients would change a lot we can encourage the parameters to be small thus resulting in a smoother curve by using a zero mean gaussian prior p w tt n wj τ where τ controls the strength of the prior the corresponding map estimation problem becomes n d argmax log n yi wt xi log n wj τ mean squared error 15 15 log lambda a 15 log lambda b figure a training error dotted blue and test error solid red for a degree polynomial ﬁt by ridge regression plotted vs log λ data was generated from noise with variance training set has size n note models are ordered from complex small regularizer on the left to simple large regularizer on the right the stars correspond to the values used to plot the functions in figure b estimate of performance using training set dotted blue fold cross validation estimate of future mse solid black negative log marginal likelihood log p d λ both curves have been vertically rescaled to to make them comparable figure generated by linregpolyvsregdemo it is a simple exercise to show that this is equivalent to minimizing the following j w y w wt x λ w 32 where λ τ and w j wt w is the squared two norm here the ﬁrst term is the mse nll as usual and the second term λ is a complexity penalty the corresponding solution is given by this technique is known as ridge regression or penalized least squares in general adding a gaussian prior to the parameters of a model to encourage them to be small is called regularization or weight decay note that the offset term is not regularized since this just affects the height of the function not its complexity by penalizing the sum of the magnitudes of the weights we ensure the function is simple since w corresponds to a straight line which is the simplest possible function corresponding to a constant we illustrate this idea in figure where we see that increasing λ results in smoother functions the resulting coefficients also become smaller for example using λ we have 128 807 457 704 948 472 625 360 711 063 349 in figure a we plot the mse on the training and test sets vs log λ we see that as we increase λ so the model becomes more constrained the error on the training set increases for the test set we see the characteristic u shaped curve where the model overﬁts and then underﬁts it is common to use cross validation to pick λ as shown in figure b in section we will discuss a more probabilistic approach we will consider a variety of different priors in this book each of these corresponds to a different form of regularization this technique is very widely used to prevent overﬁtting numerically stable computation interestingly ridge regression which works better statistically is also easier to ﬁt numerically since λid xt x is much better conditioned and hence more likely to be invertible than xt x at least for suitable largy λ nevertheless inverting matrices is still best avoided for reasons of numerical stability indeed if you write w inv x x x y in matlab it will give you a warning we now describe a useful trick for ﬁtting ridge regression models and hence by extension computing vanilla ols estimates that is more numerically robust we assume the prior has the form p w λ where λ is the precision matrix in the case of ridge regression λ τ i to avoid penalizing the term we should center the data ﬁrst as explained in exercise first let us augment the original data with some virtual data coming from the prior x x σ y y σ where λ λ λt is a cholesky decomposition of λ we see that x where the extra rows represent pseudo data from the prior is n d d we now show that the nll on this expanded data is equivalent to penalized nll on the original data f w y x w t y x w 35 y σ x σ t y σ x σ w σ σ λw λw y xw t y xw λw t λw y xw t y xw wt λw hence the map estimate is given by wˆ ridge x t x t y as we claimed now let x qr be the qr decomposition of x where q is orthonormal meaning qt q qqt i and r is upper triangular then x t x rt qt qr rt r r t hence wˆ ridge r t rt qt y r note that r is easy to invert since it is upper triangular this gives us a way to compute the ridge estimate while avoiding having to invert λ xt x we can use this technique to ﬁnd the mle by simply computing the qr decomposition of the unaugmented matrix x and using the original y this is the method of choice for solving least squares problems in fact it is so sommon that it can be implemented in one line of matlab using the backslash operator w x y note that computing the qr decomposition of an n d matrix takes o time and is numerically very stable if d n we should ﬁrst perform an svd decomposition in particular let x usvt be the svd of x where vt v in uut ut u in and s is a diagonal n n matrix now let z ud be an n n matrix then we can rewrite the ridge estimate thus wˆ ridge v zt z λin y in other words we can replace the d dimensional vectors xi with the n dimensional vectors zi and perform our penalized ﬁt as before we then transform the n dimensional solution to the d dimensional solution by multiplying by v geometrically we are rotating to a new coordinate system in which all but the ﬁrst n coordinates are zero this does not affect the solution since the spherical gaussian prior is rotationally invariant the overall time is now o dn operations connection with pca in this section we discuss an interesting connection between ridge regression and pca sec tion which gives further insight into why ridge regression works well our discussion is based on hastie et al let x usvt be the svd of x from equation we have wˆ ridge v λi y hence the ridge predictions on the training set are given by yˆ xwˆ ridge usvt v λi y d t t j j figure geometry of ridge regression the likelihood is shown as an ellipse and the prior is shown as a circle centered on the origin based on figure 15 of bishop figure generated by geomridge where s jj s λi jj j j and σj are the singular values of x hence 48 yˆ xwˆ ridge d u j j j λ ut y in contrast the least squares prediction is yˆ xwˆ ls usvt vs y uut y ujut y j if is small compared to λ then direction uj will not have much effect on the prediction in view of this we deﬁne the effective number of degrees of freedom of the model as follows d dof λ j j λ when λ dof λ d and as λ dof λ let us try to understand why this behavior is desirable in section we show that cov w xt x if we use a uniform prior for w thus the directions in which we are most uncertain about w are determined by the eigenvectors of this matrix with the smallest eigenvalues as shown in figure furthermore in section we show that the squared singular values are equal to the eigenvalues of xt x hence small singular values σj correspond to directions with high posterior variance it is these directions which ridge shrinks the most this process is illustrated in figure the horizontal parameter is not well determined by the data has high posterior variance but the vertical parameter is well determined hence wmap is close to wˆmle but wmap is shifted strongly towards the prior mean which is compare to figure c which illustrated sensor fusion with sensors of different reliabilities in this way ill determined parameters are reduced in size towards this is called shrinkage there is a related but different technique called principal components regression the idea is this ﬁrst use pca to reduce the dimensionality to k dimensions and then use these low dimensional features as input to regression however this technique does not work as well as ridge in terms of predictive accuracy hastie et al the reason is that in pc regression only the ﬁrst k derived dimensions are retained and the remaining d k dimensions are entirely ignored by contrast ridge regression uses a soft weighting of all the dimensions regularization effects of big data regularization is the most common way to avoid overﬁtting however another effective approach which is not always available is to use lots of data it should be intuitively obvious that the more training data we have the better we will be able to learn so we expect the test set error to decrease to some plateau as n increases this is illustrated in figure where we plot the mean squared error incurred on the test set achieved by polynomial regression models of different degrees vs n a plot of error vs training set size is known as a learning curve the level of the plateau for the test error consists of two terms an irreducible component that all models incur due to the intrinsic variability of the generating process this is called the noise ﬂoor and a component that depends on the discrepancy between the generating process the truth and the model this is called structural error in figure the truth is a degree polynomial and we try ﬁtting polynomials of degrees and to this data call the models and we see that the structural error for models and is zero since both are able to capture the true generating process however the structural error for is substantial which is evident from the fact that the plateau occurs high above the noise ﬂoor for any model that is expressive enough to capture the truth i e one with small structural error the test error will go to the noise ﬂoor as n however it will typically go to zero faster for simpler models since there are fewer parameters to estimate in particular for ﬁnite training sets there will be some discrepancy between the parameters that we estimate and the best parameters that we could estimate given the particular model class this is called approximation error and goes to zero as n but it goes to zero faster for simpler models this is illustrated in figure see also exercise in domains with lots of data simple methods can work surprisingly well halevy et al however there are still reasons to study more sophisticated learning methods because there will always be problems for which we have little data for example even in such a data rich domain as web search as soon as we want to start personalizing the results the amount of data available for any given user starts to look small again relative to the complexity of the problem this assumes the training data is randomly sampled and we don t just get repetitions of the same examples having informatively sampled data can help even more this is the motivation for an approach known as active learning where you get to choose your training data truth degree model degree 22 truth degree model degree 22 12 60 180 size of training set a 60 180 size of training set b truth degree model degree 22 truth degree model degree 22 16 16 12 12 60 120 180 200 size of training set c 60 100 120 140 160 180 200 size of training set d figure mse on training and test sets vs size of training set for data generated from a degree polynomial with gaussian noise of variance we ﬁt polynomial models of varying degree to this data a degree b degree c degree d degree note that for small training set sizes the test error of the degree polynomial is higher than that of the degree polynomial due to overﬁtting but this difference vanishes once we have enough data note also that the degree polynomial is too simple and has high test error even given large amounts of training data figure generated by linregpolyvsn in such cases we may want to learn multiple related models at the same time which is known as multi task learning this will allow us to borrow statistical strength from tasks with lots of data and to share it with tasks with little data we will discuss ways to do later in the book bayesian linear regression although ridge regression is a useful way to compute a point estimate sometimes we want to compute the full posterior over w and for simplicity we will initially assume the noise variance is known so we focus on computing p w d then in section we consider the general case where we compute p w we assume throughout a gaussian likelihood model performing bayesian inference with a robust likelihood is also possible but requires more advanced techniques see exercise computing the posterior in linear regression the likelihood is given by p y x w μ n y μ xw exp y xw y xw where μ is an offset term if the inputs are centered so i xij for each j the mean of the output is equally likely to be positive or negative so let us put an improper prior on μ of the form p μ and then integrate it out to get p y x w exp y xw 54 n where y n yi is the empirical mean of the output for notational simplicity we shall assume the output has been centered and write y for y the conjugate prior to the above gaussian likelihood is also a gaussian which we will denote by p w w using bayes rule for gaussians equation 125 the posterior is given by p w x y n w n y xw n w wn vn wn vn vn xt y v v xt x n vn xt x 58 if and τ then the posterior mean reduces to the ridge estimate if we deﬁne λ this is because the mean and mode of a gaussian are the same to gain insight into the posterior distribution and not just its mode let us consider a example y x w e where the true parameters are and in figure we plot the prior the likelihood the posterior and some samples from the posterior predictive in particular the right hand column plots the function y x w where x ranges over and w w wn vn is a sample from the parameter posterior initially when we sample from the prior ﬁrst row our predictions are all over the place since our prior is uniform after we see one data point second row our posterior becomes constrained by the corresponding likelihood and our predictions pass close to the observed data however we see that the posterior has a ridge like shape reﬂecting the fact that there are many possible solutions with different likelihood prior posterior w1 data space y x w1 w1 y x w1 w1 y x w1 w1 y w0 w0 x figure sequential bayesian updating of a linear regression model p y x y row represents the prior row represents the ﬁrst data point row represents the second data point y2 row represents the data point left column likelihood function for current data point middle column posterior given data so far p w n n so the ﬁrst line is the prior right column samples from the current prior posterior predictive distribution the white cross in columns and represents the true parameter value we see that the mode of the posterior rapidly after samples converges to this point the blue circles in column are the observed data points based on figure of bishop figure generated by slopes intercepts this makes sense since we cannot uniquely infer two parameters from one observation after we see two data points third row the posterior becomes much narrower and our predictions all have similar slopes and intercepts after we observe data points last row the posterior is essentially a delta function centered on the true value indicated by a white cross the estimate converges to the truth since the data was generated from this model and because bayes is a consistent estimator see section for discussion of this point computing the posterior predictive it tough to make predictions especially about the future yogi berra in machine learning we often care more about predictions than about interpreting the parame ters using equation 126 we can easily show that the posterior predictive distribution at a test point x is also gaussian p y x d n y xt w n w wn vn dw 60 n y wt x x x xt vnx the variance in this prediction x depends on two terms the variance of the observation noise and the variance in the parameters vn the latter translates into variance about observations in a way which depends on how close x is to the training data this is illustrated in figure 12 where we see that the error bars get larger as we move away from the training points representing increased uncertainty this is important for applications such as active learning where we want to model what we don t know as well as what we do by contrast the plugin approximation has constant sized error bars since p y x d n y xt w δwˆ w dw p y x wˆ see figure 12 a bayesian inference when is unknown in this section we apply the results in section to the problem of computing p w for a linear regression model this generalizes the results from section where we assumed was known in the case where we use an uninformative prior we will see some interesting connections to frequentist statistics conjugate prior as usual the likelihood has the form p y x w n y xw 64 by analogy to section one can show that the natural conjugate prior has the following form p w nig w 65 n w ig d 67 d v γ a exp w t w 68 plugin approximation mle 60 posterior predictive known variance 80 50 60 40 50 40 30 20 a b functions sampled from plugin approximation to posterior 50 100 functions sampled from posterior 80 40 35 60 30 40 20 20 15 c 20 d figure 12 a plug in approximation to predictive density we plug in the mle of the parameters b posterior predictive density obtained by integrating out the parameters black curve is posterior mean error bars are standard deviations of the posterior predictive density c samples from the plugin approximation to posterior predictive d samples from the posterior predictive figure generated by linregpostpreddemo with this prior and likelihood one can show that the posterior has the following form p w d nig w wn vn an bn 69 wn vn xt y 70 vn xt x an n 72 b b wt v yt y wt v 73 the expressions for wn and vn are similar to the case where is known the expression for an is also intuitive since it just updates the counts the expression for bn can be interpreted as follows it is the prior sum of squares plus the empirical sum of squares yt y plus a term due to the error in the prior on w the posterior marginals are as follows p d ig an bn p w d t wn bn v an n 75 we give a worked example of using these equations in section by analogy to section the posterior predictive distribution is a student t distribution in particular given m new test inputs x we have p y x d t y x w bn i x v x t 76 n an m n n the predictive variance has two components bn an im due to the measurement noise and bn an x vnx t due to the uncertainty in w this latter terms varies depending on how close the test inputs are to the training data it is common to set corresponding to an uninformative prior for and to set and g xt x for any positive value g this is called zellner g prior zellner here g plays a role analogous to λ in ridge regression however the prior covariance is proportional to xt x rather than i this ensures that the posterior is invariant to scaling of the inputs minka see also exercise we will see below that if we use an uninformative prior the posterior precision given n measurements is v xt x the unit information prior is deﬁned to contain as much information as one sample kass and wasserman to create a unit information prior for linear regression we need to use g n xt x which is equivalent to the g prior with uninformative prior an uninformative prior can be obtained by considering the uninformative limit of the conjugate g prior which corresponds to setting g this is equivalent to an improper nig prior with i and which gives p w σ d alternatively we can start with the semi conjugate prior p w p w p and take each term to its uninformative limit individually which gives p w σ this is equivalent to an improper nig prior with v i d and the corresponding posterior is given by p w d nig w wn vn an bn wn wˆ mle xt x y 78 vn xt x 79 a n d 80 bn 81 y xwˆ mle t y xwˆ mle 82 wj e wj d var wj d ci sig 998 17 001 w3 068 09947 138 274 294 124 w7 237 00601 324 849 181 w9 285 86485 079 508 table posterior mean standard deviation and credible intervals for a linear regression model with an uninformative prior ﬁt to the caterpillar data produced by linregbayescaterpillar the marginal distribution of the weights is given by s2 p w d t w wˆ n d c n d 83 where c xt x and wˆ is the mle we discuss the implications of these equations below an example where bayesian and frequentist inference coincide the use of a semi conjugate uninformative prior is interesting because the resulting posterior turns out to be equivalent to the results from frequentist statistics see also section in particular from equation 83 we have cjj s2 p wj d t wj wˆj n d n d 84 this is equivalent to the sampling distribution of the mle which is given by the following see e g rice casella and berger wj wˆj t 85 sj n d where sj n d is the standard error of the estimated parameter see section for a discussion of sampling distributions consequently the frequentist conﬁdence interval and the bayesian marginal credible interval for the parameters are the same in this case as a worked example of this consider the caterpillar dataset from marin and robert the details of what the data mean don t matter for our present purposes we can compute the posterior mean and standard deviation and the credible intervals ci for the regression coefficients using equation 84 the results are shown in table it is easy to check that these credible intervals are identical to the conﬁdence intervals computed using standard frequentist methods see linregbayescaterpillar for the code we can also use these marginal posteriors to compute if the coefficients are signiﬁcantly different from an informal way to do this without using decision theory is to check if its ci excludes from table we see that the cis for coefficients are all signiﬁcant by this measure so we put a little star by them it is easy to check that these results are the same as those produced by standard frequentist software packages which compute p values at the level although the correspondence between the bayesian and frequentist results might seem ap pealing to some readers recall from section that frequentist inference is riddled with patholo gies also note that the mle does not even exist when n d so standard frequentist inference theory breaks down in this setting bayesian inference theory still works although it requires the use of proper priors see maruyama and george for one extension of the g prior to the case where d n eb for linear regression evidence procedure so far we have assumed the prior is known in this section we describe an empirical bayes procedure for picking the hyper parameters more precisely we choose η α λ to maximize the marignal likelihood where λ be the precision of the observation noise and α is the precision of the prior p w w α this is known as the evidence procedure mackay see section for the algorithmic details the evidence procedure provides an alternative to using cross validation for example in figure b we plot the log marginal likelihood for different values of α as well as the maximum value found by the optimizer we see that in this example we get the same result as 5 cv shown in figure a we kept λ ﬁxed in both methods to make them comparable the principle practical advantage of the evidence procedure over cv will become apparent in section where we generalize the prior by allowing a different αj for every feature this can be used to perform feature selection using a technique known as automatic relevancy determination or ard by contrast it would not be possible to use cv to tune d different hyper parameters the evidence procedure is also useful when comparing different kinds of models since it provides a good approximation to the evidence p d m p d w m p w m η p η m dwdη 87 max p d w m p w m η p η m dw 88 it is important to at least approximately integrate over η rather than setting it arbitrarily for reasons discussed in section 5 5 indeed this is the method we used to evaluate the marginal alternatively we could integrate out λ analytically as shown in section 6 and just optimize α buntine and weigend however it turns out that this is less accurate than optimizing both α and λ mackay 5 fold cross validation ntrain 50 log evidence 105 60 70 80 90 100 110 120 100 20 15 5 5 log lambda a 140 150 20 15 5 5 log alpha b figure a estimate of test mse produced by 5 fold cross validation vs log λ the smallest value is indicated by the vertical line note the vertical scale is in log units c log marginal likelihood vs log α the largest value is indicated by the vertical line figure generated by linregpolyvsregdemo likelihood for the polynomial regression models in figures 5 and 5 for a more bayesian approach in which we model our uncertainty about η rather than computing point estimates see section 5 exercises exercise behavior of training set error with increasing sample size the error on the test will always decrease as we get more training data since the model will be better estimated however as shown in figure for sufficiently complex models the error on the training set can increase we we get more training data until we reach some plateau explain why exercise multi output linear regression source jaakkola when we have multiple independent outputs in linear regression the model becomes p y x w n yj wt xi since the likelihood factorizes across dimensions so does the mle thus wˆ wˆ wˆ m 90 where wˆ j xt x j in this exercise we apply this result to a model with dimensional response vector yi suppose we have some binary input data xi the training data is as follows let us embed each xi into using the following basis function φ t φ t 91 the model becomes yˆ wt φ x 92 where w is a matrix compute the mle for w from the above data exercise centering and ridge regression assume that x so the input data has been centered show that the optimizer of j w y xw t y xw λwt w 93 is y w xt x λi y exercise mle for for linear regression show that the mle for the error variance in linear regression is given by y xt wˆ 96 this is just the empirical variance of the residual errors when we plug in our estimate of wˆ exercise 5 mle for the offset term in linear regression linear regression has the form e y x wt x it is common to include a column of in the design matrix so we can solve for the offset term term and the other parameters w at the same time using the normal equations however it is also possible to solve for w and separately show that wˆ y xt w y xt w 97 so models the difference in the average output from the average predicted output also show that n wˆ xt xc yc xi x xi x t n yi y xi x c c i i where xc is the centered input matrix containing xc xi x along its rows and yc y y is the centered output vector thus we can ﬁrst compute wˆ on centered data and then estimate using y xt wˆ exercise 6 mle for simple linear regression simple linear regression refers to the case where the input is scalar so d show that the mle in this case is given by the following equations which may be familiar from basic statistics classes i xi x yi y i xiyi nx y cov x y x x var x y e y x 100 see for a demo exercise sufficient statistics for online linear regression source jaakkola consider ﬁtting the model yˆ using least squares unfortunately we did not keep the original data xi yi but we do have the following functions statistics of the data n n x n x y n y 101 n i i n n i i n n c n x x c n x x y y c n y y a what are the minimal set of statistics that we need to estimate hint see equation b what are the minimal set of statistics that we need to estimate hint see equation 97 c suppose a new data point xn yn arrives and we want to update our sufficient statistics without looking at the old data which we have not stored this is useful for online learning show that we can this for x as follows n x n n i xi n n n 103 x n x n n x this has the form new estimate is old estimate plus correction we see that the size of the correction diminishes over time i e as we get more samples derive a similar expression to update y d show that one can update c n recursively using n xy x n n yn nc n nx n y n n x n y n l 105 derive a similar expression to update cxx e implement the online learning algorithm i e write a function of the form w ss linregupdatess ss x y where x and y are scalars and ss is a structure containing the sufficient statistics f plot the coefficients over time using the dataset in speciﬁcally use x y polydatamake sampling thibaux check that they converge to the solution given by the batch offline learner i e ordinary least squares your result should look like figure turn in your derivation code and plot exercise bayesian linear regression in with known source bolstad consider ﬁtting a model of the form p y x θ n y to the data shown below online linear regression 5 6 5 15 20 time figure regression coefficients over time produced by exercise x 96 94 104 106 y 47 75 83 98 40 60 75 90 a compute an unbiased estimate of using y yˆ 107 the denominator is n since we have inputs namely the offset term and x here yˆi and wˆ is the mle b now assume the following prior on w p w p p 108 use an improper uniform prior on and a n prior on show that this can be written as a gaussian prior of the form p w n w what are and c compute the marginal posterior of the slope d where d is the dalta above and is the can use matlab if you like hint the posterior variance is a very small number d what is a 95 credible interval for exercise generative model for linear regression linear regression is the problem of estimating e y x using a linear function of the form wt x typically we assume that the conditional distribution of y given x is gaussian we can either estimate this conditional gaussian directly a discriminative approach or we can ﬁt a gaussian to the joint distribution of x y and then derive e y x x in exercise 5 we showed that the discriminative approach leads to these equations e y x wt x 109 y x w 110 w xt xc yc 111 c c where xc x x is the centered input matrix and x replicates x across the rows similarly yc y y is the centered output vector and y replicates y across the rows a by ﬁnding the maximum likelihood estimates of σxx σxy μx and μy derive the above equations by ﬁtting a joint gaussian to x y and using the formula for conditioning a gaussian see section show your work b what are the advantages and disadvantages of this approach compared to the standard discriminative approach exercise bayesian linear regression using the g prior show that when we use the g prior p w nig w g xt x the posterior has the following form p w d nig w wn vn an bn v g xt x n g w g wˆ 114 n g mle an n 115 s2 t t bn g wˆ mlex xwˆ mle 116 117 logistic regression introduction one way to build a probabilistic classiﬁer is to create a joint model of the form p y x and then to condition on x thereby deriving p y x this is called the generative approach an alternative approach is to ﬁt a model of the form p y x directly this is called the discrimi native approach and is the approach we adopt in this chapter in particular we will assume discriminative models which are linear in the parameters this will turn out to signiﬁcantly sim plify model ﬁtting as we will see in section 6 we compare the generative and discriminative approaches and in later chapters we will consider non linear and non parametric discriminative models model speciﬁcation as we discussed in section 6 logistic regression corresponds to the following binary classiﬁ cation model p y x w ber y sigm wt x a example is shown in figure b logistic regression can easily be extended to higher dimensional inputs for example figure shows plots of p y x w sigm wt x for input and different weight vectors w if we threshold these probabilities at 5 we induce a linear decision boundary whose normal perpendicular is given by w model ﬁtting in this section we discuss algorithms for estimating the parameters of a logistic regression model 5 w w 5 w 5 w 5 4 x x 5 x w w 5 5 w 5 x x w w 5 x w 5 5 10 10 10 10 10 10 x 10 10 x 5 10 10 10 10 x x x w w 5 10 10 10 10 x 4 5 6 figure plots of sigm here w deﬁnes the normal to the decision boundary points to the right of this have sigm wt x 5 and points to the left have sigm wt x 5 based on figure of mackay figure generated by mle the negative log likelihood for logistic regression is given by nll w log μi yi μi i yi i n yi log μi yi log μi i this is also called the cross entropy error function see section another way of writing this is as follows suppose y i instead of yi we have p y exp wt x n and p y t x hence nll w log exp y iwt xi 4 i unlike linear regression we can no longer write down the mle in closed form instead we need to use an optimization algorithm to compute it for this we need to derive the gradient and hessian in the case of logistic regression one can show exercise that the gradient and hessian 5 5 5 5 5 5 50 5 5 50 5 5 a b figure gradient descent on a simple function starting from for 20 steps using a ﬁxed learning rate step size η the global minimum is at a η b η 6 figure generated by steepestdescentdemo of this are given by the following d g f w μi dw i yi xi xt μ y 5 h d g w t μ xt μ μ x xt 6 xt sx where s diag μi μi one can also show exercise that h is positive deﬁnite hence the nll is convex and has a unique global minimum below we discuss some methods for ﬁnding this minimum steepest descent perhaps the simplest algorithm for unconstrained optimization is gradient descent also known as steepest descent this can be written as follows θk θk ηkgk where ηk is the step size or learning rate the main issue in gradient descent is how should we set the step size this turns out to be quite tricky if we use a constant learning rate but make it too small convergence will be very slow but if we make it too large the method can fail to converge at all this is illustrated in figure where we plot the following convex function f θ 5 5 θ1 we arbitrarily decide to start from in figure a we use a ﬁxed step size of η we see that it moves slowly along the valley in figure b we use a ﬁxed step size of η 6 we see that the algorithm starts oscillating up and down the sides of the valley and never converges to the optimum 5 5 5 exact line searching 50 5 5 a b figure a steepest descent on the same function as figure starting from using line search figure generated by steepestdescentdemo b illustration of the fact that at the end of a line search top of picture the local gradient of the function will be perpendicular to the search direction based on figure 10 6 of press et al let us develop a more stable method for picking the step size so that the method is guaran teed to converge to a local optimum no matter where we start this property is called global convergence which should not be confused with convergence to the global optimum by taylor theorem we have f θ ηd f θ ηgt d 10 where d is our descent direction so if η is chosen small enough then f θ ηd f θ since the gradient will be negative but we don t want to choose the step size η too small or we will move very slowly and may not reach the minimum so let us pick η to minimize φ η f θk ηdk this is called line minimization or line search there are various methods for solving this optimization problem see nocedal and wright for details figure a demonstrates that line search does indeed work for our simple problem however we see that the steepest descent path with exact line searches exhibits a characteristic zig zag behavior to see why note that an exact line search satisﬁes ηk arg minη φ η a necessary condition for the optimum is φt η by the chain rule φt η dt g where g f t θ ηd is the gradient at the end of the step so we either have g which means we have found a stationary point or g d which means that exact search stops at a point where the local gradient is perpendicular to the search direction hence consecutive directions will be orthogonal see figure b this explains the zig zag behavior one simple heuristic to reduce the effect of zig zagging is to add a momentum term θk θk as follows θk θk ηkgk μk θk θk 12 where μk controls the importance of the momentum term in the optimization community this is known as the heavy ball method see e g bertsekas an alternative way to minimize zig zagging is to use the method of conjugate gradients see e g nocedal and wright ch 5 or golub and van loan sec 10 this is the method of choice for quadratic objectives of the form f θ θt aθ which arise when solving linear systems however non linear cg is less popular 3 newton method algorithm newton method for minimizing a strictly convex function initialize for k until convergence do 3 evaluate gk f θk 4 evaluate hk θk 5 solve hkdk gk for dk 6 use line search to ﬁnd stepsize ηk along dk 7 θk θk ηkdk one can derive faster optimization methods by taking the curvature of the space i e the hessian into account these are called second order optimization metods the primary example is newton algorithm this is an iterative algorithm which consists of updates of the form θk θk ηkh the full pseudo code is given in algorithm this algorithm can be derived as follows consider making a second order taylor series approximation of f θ around θk fquad θ fk gt θ θk θ θk t hk θ θk let us rewrite this as fquad θ θt aθ bt θ c 15 where a h b g h θ t c f 16 the minimum of fquad is at θ a θ k h 17 thus the newton step dk h is what should be added to θk to minimize the second order approximation of f around θk see figure 4 a for an illustration xk xk dk a b xk xk dk figure 4 illustration of newton method for minimizing a function a the solid curve is the function f x the dotted line fquad x is its second order approximation at xk the newton step dk is what must be added to xk to get to the minimum of fquad x based on figure 13 4 of vandenberghe figure generated by newtonsmethodminquad b illustration of newton method applied to a nonconvex function we ﬁt a quadratic around the current point xk and move to its stationary point xk xk dk unfortunately this is a local maximum not minimum this means we need to be careful about the extent of our quadratic approximation based on figure 13 of vandenberghe figure generated by newtonsmethodnonconvex in its simplest form as listed newton method requires that hk be positive deﬁnite which will hold if the function is strictly convex if not the objective function is not convex then hk may not be positive deﬁnite so dk h may not be a descent direction see figure 4 b for an example in this case one simple strategy is to revert to steepest descent dk gk the levenberg marquardt algorithm is an adaptive way to blend between newton steps and steepest descent steps this method is widely used when solving nonlinear least squares problems an alternative approach is this rather than computing dk h directly we can solve the linear system of equations hkdk gk for dk using conjugate gradient cg if hk is not positive deﬁnite we can simply truncate the cg iterations as soon as negative curvature is detected this is called truncated newton 3 4 iteratively reweighted least squares irls let us now apply newton algorithm to ﬁnd the mle for binary logistic regression the newton update at iteration k for this model is as follows using ηk since the hessian is exact wk wk h wk xt skx y μk x skx x skx wk x y μk 20 xt skx skxwk y μk xt skx skzk 22 where we have deﬁned the working response as zk xwk s y μk 23 equation 22 is an example of a weighted least squares problem which is a minimizer of n ski zki wt xi i since sk is a diagonal matrix we can rewrite the targets in component form for each case i n as zki wt xi yi μki μki μki 25 this algorithm is known as iteratively reweighted least squares or irls for short since at each iteration we solve a weighted least squares problem where the weight matrix sk changes at each iteration see algorithm 10 for some pseudocode algorithm iteratively reweighted least squares irls w log y y 3 repeat 4 ηi wt xi 5 μi sigm ηi 6 si μi μi 7 zi ηi yi μi s diag n w xt sx sz 10 until converged 3 5 quasi newton variable metric methods the mother of all second order optimization algorithm is newton algorithm which we dis cussed in section 3 3 unfortunately it may be too expensive to compute h explicitly quasi newton methods iteratively build up an approximation to the hessian using information gleaned from the gradient vector at each step the most common method is called bfgs named after its inventors broyden fletcher goldfarb and shanno which updates the approximation to the hessian bk hk as follows y yt bk bk k yt sk bksk bksk t st bksk sk θk θk 27 yk gk gk this is a rank two update to the matrix and ensures that the matrix remains positive deﬁnite under certain restrictions on the step size we typically start with a diagonal approximation i thus bfgs can be thought of as a diagonal plus low rank approximation to the hessian alternatively bfgs can iteratively update an approximation to the inverse hessian ck h as follows ck i t k yt sk ck i t k yt sk skst yt sk since storing the hessian takes o space for very large problems one can use limited memory bfgs or l bfgs where hk or h is approximated by a diagonal plus low rank matrix in particular the product h can be obtained by performing a sequence of inner products with sk and yk using only the m most recent sk yk pairs and ignoring older information the storage requirements are therefore o md typically m 20 suffices for good performance see nocedal and wright for more information l bfgs is often the method of choice for most unconstrained smooth optimization problems that arise in machine learning although see section 5 3 6 regularization just as we prefer ridge regression to linear regression so we should prefer map estimation for logistic regression to computing the mle in fact regularization is important in the classiﬁcation setting even if we have lots of data to see why suppose the data is linearly separable in this case the mle is obtained when w corresponding to an inﬁnitely steep sigmoid function i wt x also known as a linear threshold unit this assigns the maximal amount of probability mass to the training data however such a solution is very brittle and will not generalize well to prevent this we can use regularization just as we did with ridge regression we note that the new objective gradient and hessian have the following forms f t w nll w λwt w 30 gt w g w λw ht w h w λi 32 it is a simple matter to pass these modiﬁed equations into any gradient based optimizer 3 7 multi class logistic regression now we consider multinomial logistic regression sometimes called a maximum entropy classiﬁer this is a model of the form exp wt x p y c x w c ct c exp wt x a slight variant known as a conditional logit model normalizes over a different set of classes for each data case this can be useful for modeling choices that users make between different sets of items that are offered to them let us now introduce some notation let μic p yi c xi w ηi c where ηi wt xi is a c vector also let yic i yi c be the one of c encoding of yi thus yi is a bit vector in which the c th bit turns on iff yi c following krishnapuram et al let us set wc to ensure identiﬁability and deﬁne w vec w c to be a d c column vector with this the log likelihood can be written as n c n c w log tt tt μyic yic log μic i c 1f c i c f c deﬁne the nll as f w w 36 we now proceed to compute the gradient and hessian of this expression since w is block structured the notation gets a bit heavy but the ideas are simple it helps to deﬁne a b be the kronecker product of matrices a and b if a is an m n matrix and b is a p q matrix then a b is the mp nq block matrix amnb returning to the task at hand one can show exercise 4 that the gradient is given by n g w f w μi yi xi i where yi i yi i yi c and μi w p yi xi w p yi c xi w are column vectors of length c for example if we have d 3 feature dimensions and c 3 classes this becomes g w i xi1 μi2 xi3 in other words for each class c the derivative for the weights in the c th column is wcf w μic yic xi 40 i this has the same form as in the binary logistic regression case namely an error term times xi this turns out to be a general property of distributions in the exponential family as we will see in section 3 one can also show exercise 4 that the hessian is the following block structured d c d c matrix h w w diag μi μiμt xixt for example if we have 3 features and 3 classes this becomes xi1xi3 xi where xi xixt in other words the block c ct submatrix is given by hc ct w μic δc ct μi ct xixt i this is also a positive deﬁnite matrix so there is a unique mle now consider minimizing f t w log p d w log p w where p w ttc n wc the new objective its gradient and hessian are given by f t w f w w v gt w g w v0 wc 47 c ht w h w ic v0 48 this can be passed to any gradient based optimizer to ﬁnd the map estimate note however that the hessian has size o cd cd which is c times more row and columns than in the binary case so limited memory bfgs is more appropriate than newton method see logregfit for some matlab code 4 bayesian logistic regression it is natural to want to compute the full posterior over the parameters p w for logistic regression models this can be useful for any situation where we want to associate conﬁdence intervals with our predictions e g this is necessary when solving contextual bandit problems discussed in section 5 7 3 unfortunately unlike the linear regression case this cannot be done exactly since there is no convenient conjugate prior for logistic regression we discuss one simple approximation below some other approaches include mcmc section 3 3 variational inference section expectation propagation kuss and rasmussen etc for notational simplicity we stick to binary logistic regression 4 laplace approximation in this section we discuss how to make a gaussian approximation to a posterior distribution the approximation works as follows suppose θ rd let p θ d z e e θ where e θ is called an energy function and is equal to the negative log of the unnormal ized log posterior e θ log p θ with z p being the normalization constant performing a taylor series expansion around the mode θ i e the lowest energy state we get e θ e θ θ θ t g θ θ t h θ θ 50 where g is the gradient and h is the hessian of the energy function evaluated at the mode g e θ h θ θ θt θ 51 since θ is the mode the gradient term is zero hence pˆ θ d e e θ exp θ θ t h θ θ l n θ θ h 53 z p d pˆ θ d dθ e e θ d h 54 the last line follows from normalization constant of the multivariate gaussian equation 54 is known as the laplace approximation to the marginal likelihood therefore equation is sometimes called the the laplace approximation to the posterior however in the statistics community the term laplace approximation refers to a more sophisticated method see e g rue et al for details it may therefore be better to use the term gaussian approximation to refer to equation a gaussian approximation is often a reasonable approximation since posteriors often become more gaussian like as the sample size increases for reasons analogous to the central limit theorem in physics there is an analogous technique known as a saddle point approximation 4 derivation of the bic we can use the gaussian approximation to write the log marginal likelihood as follows dropping irrelevant constants log p d log p d θ log p θ log h the penalization terms which are added to the log p θ are sometimes called the occam factor and are a measure of model complexity if we have a uniform prior p θ we can drop the second term and replace θ with the mle θˆ we now focus on approximating the third term we have h n hi where hi log p di θ let us approximate each hi by a ﬁxed matrix hˆ then we have log h log n hˆ log n d hˆ d log n log hˆ 56 where d dim θ and we have assumed h is full rank we can drop the log hˆ term since it is independent of n and thus will get overwhelmed by the likelihood putting all the pieces together we recover the bic score section 5 3 4 log p d log p d θˆ d log n 4 3 gaussian approximation for logistic regression now let us apply the gaussian approximation to logistic regression we will use a a gaussian prior of the form p w w v0 just as we did in map estimation the approximate posterior is given by p w d n w wˆ h 58 where wˆ arg minw e w e w log p w log p w and h w wˆ as an example consider the linearly separable data in figure 5 a there are many parameter settings that correspond to lines that perfectly separate the training data we show 4 examples the likelihood surface is shown in figure 5 b where we see that the likelihood is unbounded as we move up and to the right in parameter space along a ridge where 35 this is indicated by the diagonal line the reasons for this is that we can maximize the likelihood by driving w to inﬁnity subject to being on this line since large regression weights make the sigmoid function very steep turning it into a step function consequently the mle is not well deﬁned when the data is linearly separable to regularize the problem let us use a vague spherical prior centered at the origin w 100i multiplying this spherical prior by the likelihood surface results in a highly skewed posterior shown in figure 5 c the posterior is skewed because the likelihood function chops off regions of parameter space in a soft fashion which disagree with the data the map estimate is shown by the blue dot unlike the mle this is not at inﬁnity the gaussian approximation to this posterior is shown in figure 5 d we see that this is a symmetric distribution and therefore not a great approximation of course it gets the mode correct by construction and it at least represents the fact that there is more uncertainty along the southwest northeast direction which corresponds to uncertainty about the orientation of separating lines than perpendicular to this although a crude approximation this is surely better than approximating the posterior by a delta function which is what map estimation does 4 4 approximating the posterior predictive given the posterior we can compute credible intervals perform hypothesis tests etc just as we did in section 7 6 3 3 in the case of linear regression but in machine learning interest usually focusses on prediction the posterior predictive distribution has the form p y x d p y x w p w d dw data log likelihood 6 6 4 4 4 4 6 6 10 5 5 a 6 4 4 6 b log unnormalised posterior laplace approximation to posterior 6 6 4 4 4 4 6 6 6 4 4 6 c 6 4 4 6 d figure 5 a two class data in b log likelihood for a logistic regression model the line is drawn from the origin in the direction of the mle which is at inﬁnity the numbers correspond to 4 points in parameter space corresponding to the lines in a c unnormalized log posterior assuming vague spherical prior d laplace approximation to posterior based on a ﬁgure by mark girolami figure generated by logreglaplacegirolamidemo unfortunately this integral is intractable the simplest approximation is the plug in approximation which in the binary case takes the form p y x d p y x e w 60 where e w is the posterior mean in this context e w is called the bayes point of course such a plug in estimate underestimates the uncertainty we discuss some better approximations below p y x wmap decision boundary for sampled w 6 6 4 4 4 4 6 6 6 4 0 4 6 a 10 6 4 0 4 6 b mc approx of p y x numerical approx of p y x 6 6 4 4 0 0 4 4 6 6 6 4 0 4 6 c 6 4 0 4 6 d figure 6 posterior predictive distribution for a logistic regression model in top left contours of p y x wˆ map top right samples from the posterior predictive distribution bottom left averaging over these samples bottom right moderated output probit approximation based on a ﬁgure by mark girolami figure generated by logreglaplacegirolamidemo 4 4 monte carlo approximation a better approach is to use a monte carlo approximation as follows s p y x sigm ws t x 61 s where ws p w are samples from the posterior this technique can be trivially extended to the multi class case if we have approximated the posterior using monte carlo we can reuse these samples for prediction if we made a gaussian approximation to the posterior we can draw independent samples from the gaussian using standard methods figure 6 b shows samples from the posteiror predictive for our example figure 6 c 0 0 0 7 0 6 0 5 0 4 0 3 0 0 0 460 520 540 560 600 a 0 0 0 7 0 6 0 5 0 4 0 3 0 0 0 6 4 0 4 6 b figure 7 a posterior predictive density for sat data the red circle denotes the posterior mean the blue cross the posterior median and the blue lines denote the and percentiles of the predictive distribution figure generated by logregsatdemobayes b the logistic sigmoid function sigm x in solid red with the rescaled probit function φ λx in dotted blue superimposed here λ π which was chosen so that the derivatives of the two curves match at x 0 based on figure 4 of bishop figure generated by probitplot figure generated by probitregdemo shows the average of these samples by averaging over multiple predictions we see that the uncertainty in the decision boundary splays out as we move further from the training data so although the decision boundary is linear the posterior predictive density is not linear note also that the posterior mean decision boundary is roughly equally far from both classes this is the bayesian analog of the large margin principle discussed in section 14 5 figure 7 a shows an example in the red dots denote the mean of the posterior predictive evaluated at the training data the vertical blue lines denote 95 credible intervals for the posterior predictive the small blue star is the median we see that with the bayesian approach we are able to model our uncertainty about the probability a student will pass the exam based on his sat score rather than just getting a point estimate 4 4 probit approximation moderated output if we have a gaussian approximation to the posterior p w w mn vn we can also compute a deterministic approximation to the posterior predictive distribution at least in the binary case we proceed as follows p y x d sigm wt x p w d dw sigm a n a μa da 62 a wt x 63 μa e a mt x 64 var a p a d e da 65 p w d wt x mt x dw xt vnx thus we see that we need to evaluate the expectation of a sigmoid with respect to a gaussian this can be approximated by exploiting the fact that the sigmoid function is similar to the probit function which is given by the cdf of the standard normal a figure 7 b plots the sigmoid and probit functions we have rescaled the axes so that sigm a has the same slope as φ λa at the origin where π the advantage of using the probit is that one can convolve it with a gaussian analytically φ λa a μ da φ a 68 λ we now plug in the approximation sigm a φ λa to both sides of this equation to get sigm a n a μ da sigm κ μ 69 κ 70 applying this to the logistic regression model we get the following expression ﬁrst suggested in spiegelhalter and lauritzen p y x d sigm κ μa figure 6 d indicates that this gives very similar results to the monte carlo approximation using equation 71 is sometimes called a moderated output since it is less extreme than the plug in estimate to see this note that 0 κ and hence sigm κ μ sigm μ p y x wˆ 72 where the inequality is strict if μ 0 if μ 0 we have p y x wˆ 0 5 but the moderated prediction is always closer to 0 5 so it is less conﬁdent however the decision boundary occurs whenever p y x sigm κ μ 0 5 which implies μ wˆ t x 0 hence the decision boundary for the moderated approximation is the same as for the plug in approximation so the number of misclassiﬁcations will be the same for the two methods but the log likelihood will not note that in the multiclass case taking into account posterior covariance gives different answers than the plug in approach see exercise 3 10 3 of rasmussen and williams 4 5 residual analysis outlier detection it is sometimes useful to detect data cases which are outliers this is called residual analysis or case analysis in a regression setting this can be performed by computing ri yi yˆi where yˆi wˆ t xi these values should follow a 0 distribution if the modelling assumptions are correct this can be assessed by creating a qq plot where we plot the n theoretical quantiles of a gaussian distribution against the n empirical quantiles of the ri points that deviate from the straightline are potential outliers classical methods based on residuals do not work well for binary data because they rely on asymptotic normality of the test statistics however adopting a bayesian approach we can just deﬁne outliers to be points which which p yi yˆi is small where we typically use yˆi sigm wˆ t xi note that wˆ was estimated from all the data a better method is to exclude xi yi from the estimate of w when predicting yi that is we deﬁne outliers to be points which have low probability under the cross validated posterior predictive distribution deﬁned by p yi xi x i y i p yi xi w p yit xit w p w dw 73 it i this can be efficiently approximated by sampling methods gelfand for further discussion of residual analysis in logistic regression models see e g johnson and albert sec 3 4 5 online learning and stochastic optimization traditionally machine learning is performed offline which means we have a batch of data and we optimize an equation of the following form f θ f θ z 74 where zi xi yi in the supervised case or just xi in the unsupervised case and f θ zi is some kind of loss function for example we might use f θ zi log p yi xi θ 75 in which case we are trying to maximize the likelihood alternatively we might use f θ zi l yi h xi θ 76 where h xi θ is a prediction function and l y yˆ is some other loss function such as squared error or the huber loss in frequentist decision theory the average loss is called the risk see section 6 3 so this overall approach is called empirical risk minimization or erm see section 6 5 for details however if we have streaming data we need to perform online learning so we can update our estimates as each new data point arrives rather than waiting until the end which may never occur and even if we have a batch of data we might want to treat it like a stream if it is too large to hold in main memory below we discuss learning methods for this kind of scenario a simple implementation trick can be used to speed up batch learning algorithms when applied to data sets that are too large to hold in memory first note that the naive implementation makes a pass over the data ﬁle from the beginning to end accumulating the sufficient statistics and gradients as it goes then an update is performed and the process repeats unfortunately at the end of each pass the data from the beginning of the ﬁle will have been evicted from the cache since are are assuming it cannot all ﬁt into memory rather than going back to the beginning of the ﬁle and reloading it we can simply work backwards from the end of the ﬁle which is already in memory we then repeat this forwards backwards pattern over the data this simple trick is known as rocking 5 online learning and regret minimization suppose that at each step nature presents a sample zk and the learner must respond with a parameter estimate θk in the theoretical machine learning community the objective used in online learning is the regret which is the averaged loss incurred relative to the best we could have gotten in hindsight using a single ﬁxed parameter value k k regret f θ z min f θ z 77 for example imagine we are investing in the stock market let θj be the amount we invest in stock j and let zj be the return on this stock our loss function is f θ z θt z the regret is how much better or worse we did by trading at each step rather than adopting a buy and hold strategy using an oracle to choose which stocks to buy one simple algorithm for online learning is online gradient descent zinkevich which is as follows at each step k update the parameters using θk projθ θk ηkgk 78 where projv v argminw v w v is the projection of vector v onto space gk f θk zk is the gradient and ηk is the step size the projection step is only needed if the parameter must be constrained to live in a certain subset of rd see section 13 4 3 for details below we will see how this approach to regret minimization relates to more traditional objectives such as mle there are a variety of other approaches to regret minimization which are beyond the scope of this book see e g cesa bianchi and lugosi for details 5 stochastic optimization and risk minimization now suppose that instead of minimizing regret with respect to the past we want to minimize expected loss in the future as is more common in frequentist statistical learning theory that is we want to minimize f θ e f θ z 79 where the expectation is taken over future data optimizing functions where some of the variables in the objective are random is called stochastic optimization suppose we receive an inﬁnite stream of samples from the distribution one way to optimize stochastic objectives such as equation 79 is to perform the update in equation 78 at each step this is called stochastic gradient descent or sgd nemirovski and yudin since we typically want a single parameter estimate we can use a running average k θ θ k k t t 80 note that in stochastic optimization the objective is stochastic and therefore the algorithms will be too however it is also possible to apply stochastic optimization algorithms to deterministic objectives examples include simulated annealing section 6 and stochastic gradient descent applied to the empirical risk minimization problem there are some interesting theoretical connections between online learning and stochastic optimization cesa bianchi and lugosi but this is beyond the scope of this book this is called polyak ruppert averaging and can be implemented recursively as follows θk θk k θk θk 81 see e g spall kushner and yin for details 5 setting the step size we now discuss some sufficient conditions on the learning rate to guarantee convergence of sgd these are known as the robbins monro conditions ηk 82 k k the set of values of ηk over time is called the learning rate schedule various formulas are used such as ηk k or the following bottou bach and moulines ηk k κ 83 where 0 slows down early iterations of the algorithm and κ 0 5 controls the rate at which old values of are forgotten the need to adjust these tuning parameters is one of the main drawback of stochastic optimization one simple heuristic bottou is as follows store an initial subset of the data and try a range of η values on this subset then choose the one that results in the fastest decrease in the objective and apply it to all the rest of the data note that this may not result in convergence but the algorithm can be terminated when the performance improvement on a hold out set plateaus this is called early stopping 5 per parameter step sizes one drawback of sgd is that it uses the same step size for all parameters we now brieﬂy present a method known as adagrad short for adaptive gradient duchi et al which is similar in spirit to a diagonal hessian approximation see also schaul et al for a similar approach in particular if θi k is parameter i at time k and gi k is its gradient then we make an update as follows gi k 84 θi k θi k η τ si k where the diagonal step size vector is the gradient vector squared summed over all time steps this can be recursively updated as follows si k si k gi k 85 the result is a per parameter step size that adapts to the curvature of the loss function this method was original derived for the regret minimization case but it can be applied more generally 5 2 3 sgd compared to batch learning if we don t have an inﬁnite data stream we can simulate one by sampling data points at random from our training set essentially we are optimizing equation 74 by treating it as an expectation with respect to the empirical distribution algorithm 3 stochastic gradient descent initialize θ η 2 repeat 3 randomly permute data 4 for i n do 5 g f θ zi 6 θ projθ θ ηg 7 update η until converged in theory we should sample with replacement although in practice it is usually better to randomly permute the data and sample without replacement and then to repeat a single such pass over the entire data set is called an epoch see algorithm for some pseudocode in this offline case it is often better to compute the gradient of a mini batch of b data cases if b this is standard sgd and if b n this is standard steepest descent typically b 100 is used although a simple ﬁrst order method sgd performs surprisingly well on some problems especially ones with large data sets bottou the intuitive reason for this is that one can get a fairly good estimate of the gradient by looking at just a few examples carefully evaluating precise gradients using large datasets is often a waste of time since the algorithm will have to recompute the gradient again anyway at the next step it is often a better use of computer time to have a noisy estimate and to move rapidly through parameter space as an extreme example suppose we double the training set by duplicating every example batch methods will take twice as long but online methods will be unaffected since the direction of the gradient has not changed doubling the size of the data changes the magnitude of the gradient but that is irrelevant since the gradient is being scaled by the step size anyway in addition to enhanced speed sgd is often less prone to getting stuck in shallow local minima because it adds a certain amount of noise consequently it is quite popular in the machine learning community for ﬁtting models with non convex objectives such as neural networks section 16 5 and deep belief networks section 28 5 3 the lms algorithm as an example of sgd let us consider how to compute the mle for linear regression in an online fashion we derived the batch gradient in equation 7 14 the online gradient at iteration k is given by gk xi θt xi yi 86 3 2 5 2 5 0 5 0 black line lms trajectory towards ls soln red cross 10 7 6 5 rss vs iteration 0 5 0 2 3 a 4 30 5 10 15 20 25 30 b figure illustration of the lms algorithm left we start from θ 0 5 2 and slowly converging to the least squares solution of θˆ 0 92 red cross right plot of objective function over time note that it does not decrease monotonically figure generated by lmsdemo where i i k is the training example to use at iteration k if the data set is streaming we use i k k we shall assume this from now on for notational simplicity equation 86 is easy to interpret it is the feature vector xk weighted by the difference between what we predicted yˆk θt xk and the true response yk hence the gradient acts like an error signal after computing the gradient we take a step along it as follows θk θk ηk yˆk yk xk 87 there is no need for a projection step since this is an unconstrained optimization problem this algorithm is called the least mean squares or lms algorithm and is also known as the delta rule or the widrow hoff rule figure shows the results of applying this algorithm to the data shown in figure 7 2 we start at θ 0 5 2 and converge in the sense that θk θk 2 drops below a threshold of 10 2 in about iterations note that lms may require multiple passes through the data to ﬁnd the optimum by contrast the recursive least squares algorithm which is based on the kalman ﬁlter and which uses second order information ﬁnds the optimum in a single pass see section 18 2 3 see also exercise 7 7 5 4 the perceptron algorithm now let us consider how to ﬁt a binary logistic regression model in an online manner the batch gradient was given in equation 5 in the online case the weight update has the simple form θk θk ηkgi θk ηk μi yi xi 88 where μi p yi xi θk e yi xi θk we see that this has exactly the same form as the lms algorithm indeed this property holds for all generalized linear models section 3 we now consider an approximation to this algorithm speciﬁcally let yˆi arg max p y xi θ 89 0 represent the most probable class label we replace μi p y xi θ sigm θt xi in the gradient expression with yˆi thus the approximate gradient becomes gi yˆi yi xi 90 it will make the algebra simpler if we assume y rather than y 0 in this case our prediction becomes yˆi sign θt xi 91 then if yˆiyi we have made an error but if yˆiyi we guessed the right label at each step we update the weight vector by adding on the gradient the key observation is that if we predicted correctly then yˆi yi so the approximate gradient is zero and we do not change the weight vector but if xi is misclassiﬁed we update the weights as follows if yˆi but yi then the negative gradient is yˆi yi xi and if yˆi but yi then the negative gradient is yˆi yi xi we can absorb the factor of 2 into the learning rate η and just write the update in the case of a misclassiﬁcation as θk θk ηkyixi 92 since it is only the sign of the weights that matter not the magnitude we will set ηk see algorithm for the pseudocode one can show that this method known as the perceptron algorithm rosenblatt will converge provided the data is linearly separable i e that there exist parameters θ such that predicting with sign θt x achieves 0 error on the training set however if the data is not linearly separable the algorithm will not converge and even if it does converge it may take a long time there are much better ways to train logistic regression models such as using proper sgd without the gradient approximation or irls discussed in section 3 4 however the perceptron algorithm is historically important it was one of the ﬁrst machine learning algorithms ever derived by frank rosenblatt in and was even implemented in analog hardware in addition the algorithm can be used to ﬁt models where computing marginals p yi x θ is more expensive than computing the map output arg maxy p y x θ this arises in some structured output classiﬁcation problems see section 19 7 for details 5 5 a bayesian view another approach to online learning is to adopt a bayesian view this is conceptually quite simple we just apply bayes rule recursively p θ k p dk θ p θ k 93 this has the obvious advantage of returning a posterior instead of just a point estimate it also allows for the online adaptation of hyper parameters which is important since cross validation cannot be used in an online setting finally it has the less obvious advantage that it can be algorithm 4 perceptron algorithm input linearly separable data set xi rd yi for i n 2 initialize 3 k 0 4 repeat 5 k k 6 i k mod n 7 if yˆi yi then θk θk yixi else 10 no op until converged quicker than sgd to see why note that by modeling the posterior variance of each parameter in addition to its mean we effectively associate a different learning rate for each parameter de freitas et al which is a simple way to model the curvature of the space these variances can then be adapted using the usual rules of probability theory by contrast getting second order optimization methods to work online is more tricky see e g schraudolph et al sunehag et al bordes et al as a simple example in section 18 2 3 we show how to use the kalman ﬁlter to ﬁt a linear regression model online unlike the lms algorithm this converges to the optimal offline answer in a single pass over the data an extension which can learn a robust non linear regression model in an online fashion is described in ting et al for the glm case we can use an assumed density ﬁlter section 18 5 3 where we approximate the posterior by a gaussian with a diagonal covariance the variance terms serve as a per parameter step size see section 18 5 3 2 for details another approach is to use particle ﬁltering section 23 5 this was used in andrieu et al 2000 for sequentially learning a kernelized linear logistic regression model 6 generative vs discriminative classiﬁers in section 4 2 2 we showed that the posterior over class labels induced by gaussian discrim inant analysis gda has exactly the same form as logistic regression namely p y x sigm wt x the decision boundary is therefore a linear function of x in both cases note however that many generative models can give rise to a logistic regression posterior e g if each class conditional density is poisson p x y c poi x λc so the assumptions made by gda are much stronger than the assumptions made by logistic regression a further difference between these models is the way they are trained when ﬁtting a discrim inative model we usually maximize the conditional log likelihood n log p yi xi θ whereas when ﬁtting a generative model we usually maximize the joint log likelihood n log p yi xi θ it is clear that these can in general give different results see exercise 4 20 when the gaussian assumptions made by gda are correct the model will need less training data than logistic regression to achieve a certain level of performance but if the gaussian assumptions are incorrect logistic regression will do better ng and jordan this is because discriminative models do not need to model the distribution of the features this is illustrated in figure 10 we see that the class conditional densities are rather complex in particular p x y is a multimodal distribution which might be hard to estimate however the class posterior p y c x is a simple sigmoidal function centered on the threshold value of 0 this suggests that in general discriminative methods will be more accurate since their job is in some sense easier however accuracy is not the only important factor when choosing a method below we discuss some other advantages and disadvantages of each approach 6 pros and cons of each approach easy to ﬁt as we have seen it is usually very easy to ﬁt generative classiﬁers for example in sections 3 5 and 4 2 4 we show that we can ﬁt a naive bayes model and an lda model by simple counting and averaging by contrast logistic regression requires solving a convex optimization problem see section 3 4 for the details which is much slower fit classes separately in a generative classiﬁer we estimate the parameters of each class conditional density independently so we do not have to retrain the model when we add more classes in contrast in discriminative models all the parameters interact so the whole model must be retrained if we add a new class this is also the case if we train a generative model to maximize a discriminative objective salojarvi et al handle missing features easily sometimes some of the inputs components of x are not observed in a generative classiﬁer there is a simple method for dealing with this as we discuss in section 6 2 however in a discriminative classiﬁer there is no principled solution to this problem since the model assumes that x is always available to be conditioned on although see marlin for some heuristic approaches can handle unlabeled training data there is much interest in semi supervised learning which uses unlabeled data to help solve a supervised task this is fairly easy to do using generative models see e g lasserre et al liang et al but is much harder to do with discriminative models symmetric in inputs and outputs we can run a generative model backwards and infer probable inputs given the output by computing p x y this is not possible with a discriminative model the reason is that a generative model deﬁnes a joint distribution on x and y and hence treats both inputs and outputs symmetrically can handle feature preprocessing a big advantage of discriminative methods is that they allow us to preprocess the input in arbitrary ways e g we can replace x with φ x which could be some basis function expansion as illustrated in figure it is often hard to deﬁne a generative model on such pre processed data since the new features are correlated in complex ways well calibrated probabilities some generative models such as naive bayes make strong independence assumptions which are often not valid this can result in very extreme poste rior class probabilities very near 0 or discriminative models such as logistic regression are usually better calibrated in terms of their probability estimates we see that there are arguments for and against both kinds of models it is therefore useful to have both kinds in your toolbox see table for a summary of the classiﬁcation and 0 5 0 0 5 linear multinomial logistic regression 0 5 0 0 5 kernel rbf multinomial logistic regression 0 5 0 0 5 a 0 5 0 0 5 b figure a multinomial logistic regression for 5 classes in the original feature space b after basis function expansion using rbf kernels with a bandwidth of and using all the data points as centers figure generated by logregmultinomkerneldemo 5 2 4 0 3 0 6 2 0 4 0 2 0 0 0 2 0 4 0 6 0 x a 0 0 0 2 0 4 0 6 0 x b figure 10 the class conditional densities p x y c left may be more complex than the class posteriors p y c x right based on figure 27 of bishop figure generated by generativevsdiscrim regression techniques we cover in this book 6 2 dealing with missing data sometimes some of the inputs components of x are not observed this could be due to a sensor failure or a failure to complete an entry in a survey etc this is called the missing data problem little and rubin the ability to handle missing data in a principled way is one of the biggest advantages of generative models to formalize our assumptions we can associate a binary response variable ri 0 that speciﬁes whether each value xi is observed or not the joint model has the form p xi ri θ φ p ri xi φ p xi θ where φ are the parameters controlling whether the item model classif regr gen discr param non section discriminant analysis classif gen param sec 4 2 2 4 2 4 naive bayes classiﬁer classif gen param sec 3 5 3 5 2 tree augmented naive bayes classiﬁer classif gen param sec 10 2 linear regression regr discrim param sec 4 5 7 3 7 6 logistic regression classif discrim param sec 4 6 3 4 4 3 sparse linear logistic regression both discrim param ch 13 mixture of experts both discrim param sec 2 4 multilayer perceptron mlp neural network both discrim param ch 16 conditional random ﬁeld crf classif discrim param sec 19 6 k nearest neighbor classiﬁer classif gen non sec 4 2 14 7 3 inﬁnite mixture discriminant analysis classif gen non sec 14 7 3 classiﬁcation and regression trees cart both discrim non sec 16 2 boosted model both discrim non sec 16 4 sparse kernelized lin logreg sklr both discrim non sec 14 3 2 relevance vector machine rvm both discrim non sec 14 3 2 support vector machine svm both discrim non sec 14 5 gaussian processes gp both discrim non ch 15 smoothing splines regr discrim non section 15 4 6 table list of various models for classiﬁcation and regression which we discuss in this book columns are as follows model name is the model suitable for classiﬁcation regression or both is the model generative or discriminative is the model parametric or non parametric list of sections in book which discuss the model see also http googlecode com svn trunk docs tutorial html tu tsupervised html for the pmtk equivalents of these models any generative probabilistic model e g hmms boltzmann machines bayesian networks etc can be turned into a classiﬁer by using it as a class conditional density is observed or not if we assume p ri xi φ p ri φ we say the data is missing completely at random or mcar if we assume p ri xi φ p ri xo φ where xo is the observed part of xi we say the data is missing at random or mar if neither of these assumptions hold we say the data is not missing at random or nmar in this case we have to model the missing data mechanism since the pattern of missingness is informative about the values of the missing data and the corresponding parameters this is the case in most collaborative ﬁltering problems for example see e g marlin for further discussion we will henceforth assume the data is mar when dealing with missing data it is helpful to distinguish the cases when there is missing ness only at test time so the training data is complete data from the harder case when there is missingness also at training time we will discuss these two cases below note that the class label is always missing at test time by deﬁnition if the class label is also sometimes missing at training time the problem is called semi supervised learning 6 2 missing data at test time in a generative classiﬁer we can handle features that are mar by marginalizing them out for example if we are missing the value of we can compute p y c d θ p y c θ p d y c θ 94 p y c θ p x2 d y c θ 95 if we make the naive bayes assumption the marginalization can be performed as follows p x2 d y c θ p θ1c ttd p xj θjc tt p xj θjc 96 j 2 j 2 where we exploited the fact that p y c θ hence in a naive bayes classiﬁer we can simply ignore missing features at test time similarly in discriminant analysis no matter what regularization method was used to estimate the parameters we can always analytically marginalize out the missing variables see section 4 3 p x2 d y c θ n x2 d μc 2 d σc 2 d 2 d 97 6 2 2 missing data at training time missing data at training time is harder to deal with in particular computing the mle or map estimate is no longer a simple optimization problem for reasons discussed in section 3 2 however soon we will study are a variety of more sophisticated algorithms such as em algo rithm in section 4 for ﬁnding approximate ml or map estimates in such cases 6 3 fisher linear discriminant analysis flda discriminant analysis is a generative approach to classiﬁcation which requires ﬁtting an mvn to the features as we have discussed this can be problematic in high dimensions an alternative approach is to reduce the dimensionality of the features x rd and then ﬁt an mvn to the resulting low dimensional features z rl the simplest approach is to use a linear projection matrix z wx where w is a l d matrix one approach to ﬁnding w would be to use pca section 12 2 the result would be very similar to rda section 4 2 6 since svd and pca are essentially equivalent however pca is an unsupervised technique that does not take class labels into account thus the resulting low dimensional features are not necessarily optimal for classiﬁcation as illustrated in figure an alternative approach is to ﬁnd the matrix w such that the low dimensional data can be classiﬁed as well as possible using a gaussian class conditional density model the assumption of gaussianity is reasonable since we are computing linear combinations of potentially non gaussian features this approach is called fisher linear discriminant analysis or flda flda is an interesting hybrid of discriminative and generative techniques the drawback of this technique is that it is restricted to using l c dimensions regardless of d for reasons that we will explain below in the two class case this means we are seeking a single vector w onto which we can project the data below we derive the optimal w in the two class case we 4 means fisher pca 3 2 04 2 0 2 4 6 a fisher 20 15 10 5 40 35 30 25 20 15 10 5 0 b pca 25 20 15 10 5 08 6 4 2 0 2 4 c figure example of fisher linear discriminant a two class data in dashed green line ﬁrst principal basis vector dotted red line fisher linear discriminant vector solid black line joins the class conditional means b projection of points onto fisher vector shows good class separation c projection of points onto pca vector shows poor class separation figure generated by fisherldademo then generalize to the multi class case and ﬁnally we give a probabilistic interpretation of this technique 6 3 derivation of the optimal projection we now derive this optimal direction w for the two class case following the presentation of bishop sec 4 4 deﬁne the class conditional means as n1 i yi xi 2 xi i yi 2 98 let mk wt μk be the projection of each mean onto the line w also let zi wt xi be the projection of the data onto the line the variance of the projected points is proportional to s2 zi mk 2 99 i yi k the goal is to ﬁnd w such that we maximize the distance between the means while also ensuring the projected clusters are tight 2 j w s2 s2 100 2 we can rewrite the right hand side of the above in terms of w as follows wt sbw j w wt s 101 w where sb is the between class scatter matrix given by sb t 102 and sw is the within class scatter matrix given by sw xi xi t xi xi t 103 i yi to see this note that i yi 2 wt sbw wt t w 104 and wt sw w i yi wt xi xi t w i yi 2 wt xi xi t w 105 zi 2 zi 2 106 equation 101 is a ratio of two scalars we can take its derivative with respect to w and equate to zero one can show exercise 12 6 that that j w is maximized when sbw λsw w 107 3 0 5 0 2 0 5 5 0 2 2 5 3 2 3 5 3 4 3 2 0 2 3 4 a 4 5 4 3 5 3 2 5 2 5 0 5 0 b figure 12 a pca projection of vowel data to b flda projection of vowel data to we see there is better class separation in the flda case based on figure 4 of hastie et al figure generated by fisherdiscrimvoweldemo by hannes bretschneider where wt sbw λ wt s 108 w equation 107 is called a generalized eigenvalue problem if sw is invertible we can convert it to a regular eigenvalue problem s λw 109 however in the two class case there is a simpler solution in particular since sbw t w 110 then from equation 109 we have λ w s m2 m1 111 w s μ1 112 since we only care about the directionality and not the scale factor we can just set w s μ2 μ1 this is the optimal solution in the two class case if sw i meaning the pooled covariance matrix is isotropic then w is proportional to the vector that joins the class means this is an intuitively reasonable direction to project onto as shown in figure 11 6 3 2 extension to higher dimensions and multiple classes we can extend the above idea to multiple classes and to higher dimensional subspaces by ﬁnding a projection matrix w which maps from d to l so as to maximize wσbwt j w wσw 114 wt where σb σ nc μ n c c nc σ μ μc μ 115 116 c nc xi i yi c μc xi μc 117 the solution can be shown to be 2 w where u are the l leading eigenvectors of σ 2 σbς 2 assuming σw is non singular if it w w is singular we can ﬁrst perform pca on all the data figure 12 gives an example of this method applied to some d 10 dimensional speech data representing c 11 different vowel sounds we see that flda gives better class separation than pca note that flda is restricted to ﬁnding at most a l c dimensional linear subspace no matter how large d because the rank of the between class covariance matrix σb is c the term arises because of the μ term which is a linear function of the μc this is a rather severe restriction which limits the usefulness of flda 6 3 3 probabilistic interpretation of flda to ﬁnd a valid probabilistic interpretation of flda we follow the approach of kumar and andreo zhou et al they proposed a model known as heteroscedastic lda hlda which works as follows let w be a d d invertible matrix and let zi wxi be a transformed version of the data we now ﬁt full covariance gaussians to the transformed data one per class but with the constraint that only the ﬁrst l components will be class speciﬁc the remaining h d l components will be shared across classes and will thus not be discriminative that is we use p zi θ yi c n zi μc σc 119 μc mc 120 σc sc 0 121 where is the shared h dimensional mean and is the shared h h covariace the pdf of the original untransformed data is given by p xi yi c w θ w n wxi μc σc 122 w n wlxi mc sc n whxi s0 123 where w wl for ﬁxed w it is easy to derive the mle for θ one can then optimize wh w using gradient methods in the special case that the σc are diagonal there is a closed form solution for w gales and in the special case the σc are all equal we recover classical lda zhou et al in view of this this result it should be clear that hlda will outperform lda if the class covariances are not equal within the discriminative subspace i e if the assumption that σc is independent of c is a poor assumption this is easy to demonstrate on synthetic data and is also the case on more challenging tasks such as speech recognition kumar and andreo furthermore we can extend the model by allowing each class to use its own projection matrix this is known as multiple lda gales exercises exercise spam classiﬁcation using logistic regression consider the email spam data set discussed on of hastie et al this consists of email messages from which features have been extracted these are as follows 48 features in 0 100 giving the percentage of words in a given message which match a given word on the list the list contains words such as business free george etc the data was collected by george forman so his name occurs quite a lot 6 features in 0 100 giving the percentage of characters in the email that match a given character on the list the characters are feature 55 the average length of an uninterrupted sequence of capital letters max is 40 3 mean is 4 feature 56 the length of the longest uninterrupted sequence of capital letters max is 0 mean is 6 feature the sum of the lengts of uninterrupted sequence of capital letters max is 25 6 mean is 2 load the data from spamdata mat which contains a training set of size and a test set of size one can imagine performing several kinds of preprocessing to this data try each of the following separately a standardize the columns so they all have mean 0 and unit variance b transform the features using log xij 0 c binarize the features using i xij 0 for each version of the data ﬁt a logistic regression model use cross validation to choose the strength of the t 2 regularizer report the mean error rate on the training and test sets you should get numbers similar to this method train test stnd 0 082 0 079 log 0 052 0 059 binary 0 065 0 072 the precise values will depend on what regularization value you choose turn in your code and numerical results see also exercise 2 exercise 2 spam classiﬁcation using naive bayes we will re examine the dataset from exercise a use naivebayesfit and naivebayespredict on the binarized spam data what is the training and test error you can try different settings of the pseudocount α if you like this corresponds to the beta α α prior each θjc although the default of α is probably ﬁne turn in your error rates b modify the code so it can handle real valued features use a gaussian density for each feature ﬁt it with maximum likelihood what are the training and test error rates on the standardized data and the log transformed data turn in your 4 error rates and code exercise 3 gradient and hessian of log likelihood for logistic regression a let σ a be the sigmoid function show that dσ a σ a σ a 124 da b using the previous result and the chain rule of calculus derive an expression for the gradient of the log likelihood equation 5 c the hessian can be written as h xt sx where s diag μ1 μ1 μn μn show that h is positive deﬁnite you may assume that 0 μi so the elements of s will be strictly positive and that x is full rank exercise 4 gradient and hessian of log likelihood for multinomial logistic regression a let μik s ηi k prove that the jacobian of the softmax is μik μ ηij δkj μij 125 where δkj i k j b hence show that wct yic μic xi 126 i hint use the chain rule and the fact that c yic c show that the block submatrix of the hessian for classes c and ci is given by hc ct μic δc ct μi ct xixt 127 i exercise 5 symmetric version of t 2 regularized multinomial logistic regression source ex 18 3 of hastie et al multiclass logistic regression has the form exp wt x p y c x w c k c exp wt x 8 128 where w is a d c weight matrix we can arbitrarily deﬁne w 0 for one of the classes say c c since p y c x w c p y c x w in this case the model has the form exp wt x p y c x w c c exp wt x 8 129 k k if we don t clamp one of the vectors to some constant value the parameters will be unidentiﬁable however suppose we don t clamp wc 0 so we are using equation 8 128 but we add t 2 regularization by optimizing n c log p yi xi w λ wc 2 8 i c show that at the optimum we have c wˆcj 0 for j d for the unregularized terms we still need to enforce that 0 to ensure identiﬁability of the offset exercise 8 6 elementary properties of t 2 regularized logistic regression source jaaakkola consider minimizing j w t w d where train λ w 2 8 131 t w d log σ y xt w 8 132 is the average log likelihood on data set d for yi answer the following true false questions a j w has multiple locally optimal solutions t f b let wˆ arg minw j w be a global optimum wˆ is sparse has many zero entries t f c if the training data is linearly separable then some weights wj might become inﬁnite if λ 0 t f d t wˆ dtrain always increases as we increase λ t f e t wˆ dtest always increases as we increase λ t f exercise 8 7 regularizing separate terms in logistic regression source jaaakkola a consider the data in figure 8 13 where we ﬁt the model p y x w σ suppose we ﬁt the model by maximum likelihood i e we minimize j w t w dtrain 8 133 where t w train is the log likelihood on the training set sketch a possible decision boundary corresponding to wˆ copy the ﬁgure ﬁrst a rough sketch is enough and then superimpose your answer on your copy since you will need multiple versions of this ﬁgure is your answer decision boundary unique how many classiﬁcation errors does your method make on the training set b now suppose we regularize only the parameter i e we minimize w t w d train 8 suppose λ is a very large number so we regularize all the way to 0 but all other parameters are unregularized sketch a possible decision boundary how many classiﬁcation errors does your method make on the training set hint consider the behavior of simple linear regression w1x1 when x2 0 c now suppose we heavily regularize only the parameter i e we minimize w t w d train 8 135 sketch a possible decision boundary how many classiﬁcation errors does your method make on the training set figure 8 13 data for logistic regression question d now suppose we heavily regularize only the parameter sketch a possible decision boundary how many classiﬁcation errors does your method make on the training set generalized linear models and the exponential family introduction we have now encountered a wide variety of probability distributions the gaussian the bernoulli the student t the uniform the gamma etc it turns out that most of these are members of a broader class of distributions known as the exponential family in this chapter we discuss various properties of this family this allows us to derive theorems and algorithms with very broad applicability we will see how we can easily use any member of the exponential family as a class conditional density in order to make a generative classiﬁer in addition we will discuss how to build discriminative models where the response variable has an exponential family distribution whose mean is a linear function of the inputs this is known as a generalized linear model and generalizes the idea of logistic regression to other kinds of response variables 2 the exponential family before deﬁning the exponential family we mention several reasons why it is important it can be shown that under certain regularity conditions the exponential family is the only family of distributions with ﬁnite sized sufficient statistics meaning that we can compress the data into a ﬁxed sized summary without loss of information this is particularly useful for online learning as we will see later the exponential family is the only family of distributions for which conjugate priors exist which simpliﬁes the computation of the posterior see section 2 5 the exponential family can be shown to be the family of distributions that makes the least set of assumptions subject to some user chosen constraints see section 2 6 the exponential family is at the core of generalized linear models as discussed in section 3 the exponential family is at the core of variational inference as discussed in section 2 the exceptions are the student t which does not have the right form and the uniform distribution which does not have ﬁxed support independent of the parameter values 2 deﬁnition a pdf or pmf p x θ for x xm m and θ θ rd is said to be in the exponential family if it is of the form p x θ h x exp θt φ x z θ h x exp θt φ x a θ 2 where z θ x m h x exp θt φ x dx 3 a θ log z θ 4 here θ are called the natural parameters or canonical parameters φ x rd is called a vector of sufficient statistics z θ is called the partition function a θ is called the log partition function or cumulant function and h x is the a scaling constant often if φ x x we say it is a natural exponential family equation 2 can be generalized by writing p x θ h x exp η θ t φ x a η θ 5 where η is a function that maps the parameters θ to the canonical parameters η η θ if dim θ dim η θ it is called a curved exponential family which means we have more sufficient statistics than parameters if η θ θ the model is said to be in canonical form we will assume models are in canonical form unless we state otherwise 2 2 examples let us consider some examples to make things clearer 2 2 bernoulli the bernoulli for x 0 can be written in exponential family form as follows ber x μ μx μ x exp x log μ x log μ exp φ x t θ 6 where φ x i x 0 i x and θ log μ log μ however this representation is over complete since there is a linear dependendence between the features φ x i x 0 i x 7 consequently θ is not uniquely identiﬁable it is common to require that the representation be minimal which means there is a unique θ associated with the distribution in this case we can just deﬁne ber x μ μ exp x log μ l 8 now we have φ x x θ log μ which is the log odds ratio and z μ we μ can recover the mean parameter μ from the canonical parameter using μ sigm θ 2 2 2 multinoulli e θ we can represent the multinoulli as a minimal exponential family as follows where xk i x k cat x μ ktt μxk exp k k xk log μk 10 exp k k xk log μk k k xk log k k μk 11 exp x log f μk log k μk 12 exp xk log log μk μk 13 μk k where μk k μk we can write this in exponential family form as follows cat x θ exp θt φ x a θ 14 θ log μ1 log μk 15 μk μk φ x i x i x k 16 we can recover the mean parameters from the canonical parameters using eθk μk k eθj 17 from this we ﬁnd k eθj and hence a θ log k k eθk 19 if we deﬁne θk 0 we can write μ s θ and a θ log k eθk where s is the softmax function in equation 4 2 2 3 univariate gaussian the univariate gaussian can be written in exponential family form as follows 2 2 n x μ σ exp x μ 20 exp x2 μ x μ2 where z θ exp θt φ x 22 μ θ 2σ2 23 φ x x 2 μ2 z μ σ exp 25 2σ2 a θ 4θ2 2 log log 26 2 2 2 4 non examples not all distributions of interest belong to the exponential family for example the uniform distribution x unif a b does not since the support of the distribution depends on the parameters also the student t distribution section 11 4 5 does not belong since it does not have the required form 2 3 log partition function an important property of the exponential family is that derivatives of the log partition function can be used to generate cumulants of the sufficient statistics 2 for this reason a θ is sometimes called a cumulant function we will prove this for a parameter distribution this can be generalized to a k parameter distribution in a straightforward way for the ﬁrst 2 the ﬁrst and second cumulants of a distribution are its mean e x and variance var x whereas the ﬁrst and second moments are its mean e x and e derivative we have da d dθ dθ log exp θφ x h x dx 27 d exp θφ x h x dx r exp θφ x h x dx 28 φ x exp θφ x h x dx exp a θ 29 φ x exp θφ x a θ h x dx 30 φ x p x dx e φ x 31 for the second derivative we have φ x exp θφ x a θ h x φ x at θ dx 32 φ x p x φ x at θ dx x p x dx at θ φ x p x dx e x e φ x 2 var φ x 35 where we used the fact that at θ da e φ x in the multivariate case we have that θ θ e φi x φj x e φi x e φj x 36 i j and hence a θ cov φ x 37 since the covariance is positive deﬁnite we see that a θ is a convex function see section 7 3 3 2 3 example the bernoulli distribution for example consider the bernoulli distribution we have a θ log eθ so the mean is given by da eθ dθ eθ e θ sigm θ μ 38 the variance is given by d 1 e dθ θ 1 1 e θ 2 e θ 39 e θ 1 1 1 1 e θ 1 e θ eθ 1 1 e θ 1 μ μ 40 2 4 mle for the exponential family the likelihood of an exponential family model has the form n p d θ i 1 h xi g θ n exp fη θ t i 1 φ xi we see that the sufficient statistics are n and n n φ d xi φk xi for example for the ber noulli model we have φ i i xi 1 and for the univariate the pitman koopman darmois theorem states that under certain regularity conditions the exponential family is the only family of distributions with ﬁnite sufficient statistics here ﬁnite means of a size independent of the size of the data set one of the conditions required in this theorem is that the support of the distribution not be dependent on the parameter for a simple example of such a distribution consider the uniform distribution 1 p x θ u x θ θ i 0 x θ the likelihood is given by p d θ θ ni 0 max xi θ so the sufficient statistics are n and maxi xi this is ﬁnite in size but the uni form distribution is not in the exponential family because its support set depends on the parameters we now descibe how to compute the mle for a canonical exponential family model given n iid data points d xn the log likelihood is log p d θ θt φ d n a θ 45 since a θ is concave in θ and θt φ is linear in θ we see that the log likelihood is concave and hence has a unique global maximum to derive this maximum we use the fact that the derivative of the log partition function yields the expected value of the sufficient statistic vector section 2 3 θ log p d θ φ d n e φ x setting this gradient to zero we see that at the mle the empirical average of the sufficient statistics must equal the model theoretical expected sufficient statistics i e θˆ must satisfy e φ x 1 φ x 47 this is called moment matching for example in the bernoulli distribution we have φ x i x 1 so the mle satisﬁes n e φ x p x 1 μˆ i xi n i 1 1 48 2 5 bayes for the exponential family we have seen that exact bayesian analysis is considerably simpliﬁed if the prior is conjugate to the likelihood informally this means that the prior p θ τ has the same form as the likelihood p θ for this to make sense we require that the likelihood have ﬁnite sufficient statistics so that we can write p θ p θ this suggests that the only family of distributions for which conjugate priors exist is the exponential family we will derive the form of the prior and posterior below 2 5 1 likelihood the likelihood of the exponential family is given by p d θ g θ exp η θ sn 49 where sn n xi in terms of the canonical parameters this becomes p d η exp n ηt na η 50 where 1 sn 2 5 2 prior the natural conjugate prior has the form p θ τ 0 g θ exp η θ t τ 0 51 let us write τ 0 0 to separate out the size of the prior pseudo data from the mean of the sufficient statistics on this pseudo data τ 0 in canonical form the prior becomes p η τ 0 exp τ 0 ν0a η 52 2 5 3 posterior the posterior is given by p θ d p θ νn τ n p θ n τ 0 sn 53 so we see that we just update the hyper parameters by adding in canonical form this becomes p η d exp ηt 0 n n a η 54 p η n 0 n 55 n so we see that the posterior hyper parameters are a convex combination of the prior mean hyper parameters and the average of the sufficient statistics 2 5 4 posterior predictive density let us derive a generic expression for the predictive density for future observables dt x 1 x nt given past data d xn as follows for notational brevity we will combine the sufficient statistics with the size of the data as follows τ 0 τ 0 d n d and dt n t dt so the prior becomes p θ τ 1 g θ exp η θ t τ 0 z τ 0 0 56 the likelihood and posterior have a similar form hence p dt d p dt θ p θ d dθ 57 n t i 1 h x i z τ 0 d 1 g θ n n tdθ 58 exp ηk θ τk i 1 sk xi n t i 1 sk x i dθ 59 ttn t z τ 0 d s dt i 1 h x i z τ 0 60 s d if n 0 this becomes the marginal likelihood of t which reduces to the familiar form of normalizer of the posterior divided by the normalizer of the prior multiplied by a constant 2 5 5 example bernoulli distribution as a simple example let us revisit the beta bernoulli model in our new notation the likelihood is given by 1 θ i hence the conjugate prior is given by p θ ν τ 1 θ exp log θ τ 62 1 θ 1 θ 63 if we deﬁne α τ0 1 and β ν0 τ0 1 we see that this is a beta distribution p θ d s 1 θ ν0 τ0 n s 64 θτn 1 θ νn τn 65 we can derive the posterior predictive distribution as follows assume p θ beta θ α β and let s s d be the number of heads in the past data we can predict the probability of a given sequence of future heads dt x 1 x m with sufficient statistic st m i x i 1 as follows 1 p dt d p dt θ beta θ αn βn dθ 66 γ αn βn 1 t t where γ αn βn γ αn m γ βn m γ αn γ βn γ αn m βn m 68 αn m αn st α s st 69 βn m βn m st β n s m st 70 2 6 maximum entropy derivation of the exponential family although the exponential family is convenient is there any deeper justiﬁcation for its use it turns out that there is it is the distribution that makes the least number of assumptions about the data subject to a speciﬁc set of user speciﬁed constraints as we explain below in particular suppose all we know is the expected values of certain features or functions fk x p x fk 71 x where fk are known constants and fk x is an arbitrary function the principle of maximum entropy or maxent says we should pick the distribution with maximum entropy closest to uniform subject to the constraints that the moments of the distribution match the empirical moments of the speciﬁed functions to maximize entropy subject to the constraints in equation 71 and the constraints that p x 0 and x p x 1 we need to use lagrange multipliers the lagrangian is given by j p λ p x log p x 1 p x λk fk p x fk x 72 we can use the calculus of variations to take derivatives wrt the function p but we will adopt a simpler approach and treat p as a ﬁxed length vector since we are assuming x is discrete then we have j 1 log p x λ λ f x 73 p x setting j 0 yields 0 k k k p x 1 exp λ f x 74 w g 1 ηi g xi ψ μi θi ψ 1 figure 1 a visualization of the various features of a glm based on figure 8 3 of jordan where z using the sum to one constraint we have 1 p x 1 exp λ f x 75 hence the normalization constant is given by z exp λkfk x 76 thus the maxent distribution p x has the form of the exponential family section 2 also known as the gibbs distribution 3 generalized linear models glms linear and logistic regression are examples of generalized linear models or glms mccullagh and nelder these are models in which the output density is in the exponential family section 2 and in which the mean parameters are a linear combination of the inputs passed through a possibly nonlinear function such as the logistic function we describe glms in more detail below we focus on scalar outputs for notational simplicity this excludes multinomial logistic regression but this is just to simplify the presentation 3 1 basics to understand glms let us ﬁrst consider the case of an unconditional dstribution for a scalar response variable p y θ exp yiθ a θ c y l 77 where is the dispersion parameter often set to 1 θ is the natural parameter a is the partition function and c is a normalization constant for example in the case of logistic regression θ is the log odds ratio θ log μ where μ e y p y 1 is the mean parameter see section 2 2 1 to convert from the mean parameter to the natural parameter 3 generalized linear models glms distrib link g μ θ ψ μ μ ψ 1 θ e y n μ identity θ μ μ θ bin n μ logit θ log μ 1 μ μ sigm θ θ poi μ log θ log μ μ e table 1 canonical link functions ψ and their inverses for some common glms we can use a function ψ so θ ψ μ this function is uniquely determined by the form of the exponential family distribution in fact this is an invertible mapping so we have μ ψ 1 θ furthermore we know from section 2 3 that the mean is given by the derivative of the partition function so we have μ ψ 1 θ at θ now let us add inputs covariates we ﬁrst deﬁne a linear function of the inputs ηi wt xi 78 we now make the mean of the distribution be some invertible monotonic function of this linear combination by convention this function known as the mean function is denoted by g 1 so μi g 1 ηi g 1 wt xi 79 see figure 1 for a summary of the basic model the inverse of the mean function namely g is called the link function we are free to choose almost any function we like for g so long as it is invertible and so long as g 1 has the appropriate range for example in logistic regression we set μi g 1 ηi sigm ηi one particularly simple form of link function is to use g ψ this is called the canonical link function in this case θi ηi wt xi so the model becomes 2 yiwt xi a wt xi 2 l in table 1 we list some distributions and their canonical link functions we see that for the bernoulli binomial distribution the canonical link is the logit function g μ log η 1 η whose inverse is the logistic function μ sigm η based on the results in section 2 3 we can show that the mean and variance of the response variable are as follows e y xi w μi at θi 81 var y xi w att θi 82 to make the notation clearer let us consider some simple examples for linear regression we have log p yi xi w yiμi μ2 1 y2 log 83 where yi r and θi μi wt xi here a θ θ2 2 so e yi μi and var yi for binomial regression we have log p y x w y log πi n log 1 π log ni 84 i i i 1 πi i yi where yi 0 1 ni πi sigm wt xi θi log πi 1 πi wt xi and 1 here a θ ni log 1 eθ so e yi niπi μi var yi niπi 1 πi for poisson regression we have log p yi xi w yi log μi μi log yi 85 where yi 0 1 2 μi exp wt xi θi log μi wt xi and 1 here a θ eθ so e yi var yi μi poisson regression is widely used in bio statistical applications where yi might represent the number of diseases of a given person or place or the number of reads at a genomic location in a high throughput sequencing context see e g kuan et al 3 2 ml and map estimation one of the appealing properties of glms is that they can be ﬁt using exactly the same methods that we used to ﬁt logistic regression in particular the log likelihood has the following form 1 w log p d w n i i 1 86 i θiyi a θi 87 we can compute the gradient vector using the chain rule as follows d i dwj d i dθi dμi dηi dθi dμi dηi dwj 88 y at θ dθi dμi x 89 i i dμi dηi ij y dθi dμi μ x 90 i i dμi dηi ij if we use a canonical link θi ηi this simpliﬁes to 1 w w n i 1 yi μi xi 91 which is a sum of the input vectors weighted by the errors this can be used inside a stochastic gradient descent procedure discussed in section 8 5 2 however for improved efficiency we should use a second order method if we use a canonical link the hessian is given by 1 h dμi x xt 1 xt sx 92 name formula logistic g 1 η sigm η eη probit g 1 η φ η log log g 1 η exp exp η complementary log log g 1 η 1 exp exp η table 2 summary of some possible mean functions for binary regression where s diag dμn is a diagonal weighting matrix this can be used inside the dθn irls algorithm section 8 3 4 speciﬁcally we have the following newton update wt 1 xt stx stzt 93 zt θt s t 1 y μt 94 where θt xwt and μt g 1 ηt if we extend the derivation to handle non canonical links we ﬁnd that the hessian has another term however it turns out that the expected hessian is the same as in equation 92 using the expected hessian known as the fisher information matrix instead of the actual hessian is known as the fisher scoring method it is straightforward to modify the above procedure to perform map estimation with a gaus sian prior we just modify the objective gradient and hessian just as we added 2 regularization to logistic regression in section 8 3 6 3 3 bayesian inference bayesian inference for glms is usually conducted using mcmc chapter possible methods include metropolis hastings with an irls based proposal gamerman gibbs sampling using adaptive rejection sampling ars for each full conditional dellaportas and smith etc see e g dey et al 2000 for futher information it is also possible to use the gaussian approximation section 8 4 1 or variational inference section 21 8 1 1 4 probit regression in binary logistic regression we use a model of the form p y 1 xi w sigm wt xi in general we can write p y 1 xi w g 1 wt xi for any function g 1 that maps to 0 1 several possible mean functions are listed in table 2 in this section we focus on the case where g 1 η φ η where φ η is the cdf of the standard normal this is known as probit regression the probit function is very similar to the logistic function as shown in figure 8 7 b however this model has some advantages over logistic regression as we will see 4 1 ml map estimation using gradient based optimization we can ﬁnd the mle for probit regression using standard gradient methods let μi wt xi and let y i 1 1 then the gradient of the log likelihod for a speciﬁc case is given by g d log p y wt x dμi d log p y wt x x y iφ μi 95 i dw i i dw dμi i i i φ y iμi where φ is the standard normal pdf and φ is its cdf similarly the hessian for a single case is given by d t hi log p y i w xi xi φ μ 2 φ y μ 2 y iμiφ μi φ y μ xt 96 i i i i we can modify these expressions to compute the map estimate in a straightforward manner in particular if we use the prio r p w n 0 v0 the gradient and hessian of the penalized 4 2 latent variable interpretation we can interpret the probit and logistic model as follows first let us associate each item xi with two latent utilities and corresponding to the possible choices of yi 0 and yi 1 we then assume that the observed choice is whichever action has larger utility more precisely the model is as follows wt xi 97 wt xi 98 yi i 99 where δ s are error terms representing all the other factors that might be relevant in decision making that we have chosen not to or are unable to model this is called a random utility model or rum mcfadden train since it is only the difference in utilities that matters let us deﬁne zi u0i ei where ei if the δ s have a gaussian distribution then so does ei thus we can write zi wt xi ei 100 ei n 0 1 101 yi 1 i zi 0 102 following fruhwirth schnatter and fruhwirth we call this the difference rum or drum model when we marginalize out zi we recover the probit model p yi 1 xi w i zi 0 n zi wt xi 1 dzi 103 p wt xi e 0 p e wt xi 104 1 φ wt xi φ wt xi 105 where we used the symmetry of the gaussian 3 this latent variable interpretation provides an alternative way to ﬁt the model as discussed in section 11 4 6 interestingly if we use a gumbel distribution for the δ s we induce a logistic distibution for ei and the model reduces to logistic regression see section 24 5 1 for further details 4 3 ordinal probit regression one advantage of the latent variable interpretation of probit regression is that it is easy to extend to the case where the response variable is ordinal that is it can take on c discrete values which can be ordered in some way such as low medium and high this is called ordinal regression the basic idea is as follows we introduce c 1 thresholds γj and set yi j if γj 1 zi γj 106 where γc for identiﬁability reasons we set γ1 0 and γc for example if c 2 this reduces to the standard binary probit model whereby zi 0 produces yi 0 and zi 0 produces yi 1 if c 3 we partition the real line into 3 intervals 0 0 we can vary the parameter to ensure the right relative amount of probability mass falls in each interval so as to match the empirical frequencies of each class label finding the mles for this model is a bit trickier than for binary probit regression since we need to optimize for w and γ and the latter must obey an ordering constraint see e g kawakatsu and largey for an approach based on em it is also possible to derive a simple gibbs sampling algorithm for this model see e g hoff 4 4 multinomial probit models now consider the case where the response variable can take on c unordered categorical values yi 1 c the multinomial probit model is deﬁned as follows zic wt xic eic 107 e n 0 r 108 yi arg max zic 109 c see e g dow and endersby scott fruhwirth schnatter and fruhwirth for more details on the model and its connection to multinomial logistic regression by deﬁning w wc and xic 0 0 xi 0 0 we can recover the more familiar formulation zic xt wc since only relative utilities matter we constrain r to be a correlation matrix if instead of setting yi argmaxc zic we use yic i zic 0 we get a model known as multivariate probit which is one way to model c correlated binary outcomes see e g talhouk et al 3 note that the assumption that the gaussian noise term is zero mean and unit variance is made without loss of generality to see why suppose we used some other mean μ and variance then we could easily rescale w and add an offset term without changing the likelihood since p n 0 1 wt x p n μ wt x μ σ 5 multi task learning sometimes we want to ﬁt many related classiﬁcation or regression models it is often reasonable to assume the input output mapping is similar across these different models so we can get better performance by ﬁtting all the parameters at the same time in machine learning this setup is often called multi task learning caruana transfer learning e g raina et al or learning to learn thrun and pratt in statistics this is usually tackled using hierarchical bayesian models bakker and heskes as we discuss below although there are other possible methods see e g chai 5 1 hierarchical bayes for multi task learning let yij be the response of the i th item in group j for i 1 nj and j 1 j for example j might index schools i might index students within a school and yij might be the test score as in section 5 6 2 or j might index people and i might index purchaes and yij might be the identity of the item that was purchased this is known as discrete choice modeling train let xij be a feature vector associated with yij the goal is to ﬁt the models p yj xj for all j although some groups may have lots of data there is often a long tail where the majority of groups have little data thus we can t reliably ﬁt each model separately but we don t want to use the same model for all groups as a compromise we can ﬁt a separate model for each group but encourage the model parameters to be similar across groups more precisely suppose e yij xij g xt βj where g is the link function for the glm furthermore suppose βj n β and that β n μ in this model groups with small sample size borrow statistical strength from the groups with larger sample size because the βj s are correlated via the latent common parents β see section 5 5 for further discussion of this point the term controls how much group j depends on the common parents and the term j controls the strength of the overall prior suppose for simplicity that μ 0 and that σ2 and σ2 are all known e g they could be set j by cross validation the overall log probability has the form 1 βj β 2 β 2 we can perform map estimation of β j β using standard gradient methods alter natively we can perform an iterative optimization scheme alternating between optimizing the βj and the β since the likelihood and prior are convex this is guaranteed to converge to the global optimum note that once the models are trained we can discard β and use each model separately 5 2 application to personalized email spam ﬁltering an interesting application of multi task learning is personalized spam ﬁltering suppose we want to ﬁt one classiﬁer per user βj since most users do not label their email as spam or not it will be hard to estimate these models independently so we will let the βj have a common prior β representing the parameters of a generic user in this case we can emulate the behavior of the above model with a simple trick daume attenberg et al weinberger et al we make two copies of each feature xi one concatenated with the user id and one not the effect will be to learn a predictor of the form e yi xi u β wj t xi i u 1 xi i u j xi 111 where u is the user id in other words e yi xi u j βt wj t xi 112 thus β will be estimated from everyone s email whereas wj will just be estimated from user j s email to see the correspondence with the above hierarchical bayesian model deﬁne wj βj β then the log probability of the original model can be rewritten as p dj β wj wj 2 2σ2 β 2 2σ 113 if we assume σ2 σ2 the effect is the same as using the augmented feature trick with the j same regularizer strength for both wj and β however one typically gets better performance by not requiring that σ2 be equal to σ2 finkel and manning j 5 3 application to domain adaptation domain adaptation is the problem of training a set of classiﬁers on data drawn from different distributions such as email and newswire text this problem is obviously a special case of multi task learning where the tasks are the same finkel and manning used the above hierarchical bayesian model to perform domain adaptation for two nlp tasks namely named entity recognition and parsing they report reason ably large improvements over ﬁtting separate models to each dataset and small improvements over the approach of pooling all the data and ﬁtting a single model 5 4 other kinds of prior in multi task learning it is common to assume that the prior is gaussian however sometimes other priors are more suitable for example consider the task of conjoint analysis which requires ﬁguring out which features of a product customers like best this can be modelled using the same hierarchical bayesian setup as above but where we use a sparsity promoting prior on βj rather than a gaussian prior this is called multi task feature selection see e g lenk et al argyriou et al for some possible approaches it is not always reasonable to assume that all tasks are all equally similar if we pool the parameters across tasks that are qualitatively different the performance will be worse than not using pooling because the inductive bias of our prior is wrong indeed it has been found experimentally that sometimes multi task learning does worse than solving each task separately this is called negative transfer one way around this problem is to use a more ﬂexible prior such as a mixture of gaussians such ﬂexible priors can provide robustness against prior mis speciﬁcation see e g xue et al jacob et al for details one can of course combine mixtures with sparsity promoting priors ji et al many other variants are possible 6 generalized linear mixed models suppose we generalize the multi task learning scenario to allow the response to include infor mation at the group level xj as well as at the item level xij similarly we can allow the parameters to vary across groups βj or to be tied across groups α this gives rise to the following model e yij xij xj g xij t βj xj t βjt xij t α xj t αt 9 114 where the φk are basis functions this model can be represented pictorially as shown in figure 9 2 a such ﬁgures will be explained in chapter 10 note that the number of βj parameters grows with the number of groups whereas the size of α is ﬁxed frequentists call the terms βj random effects since they vary randomly across groups but they call α a ﬁxed effect since it is viewed as a ﬁxed but unknown constant a model with both ﬁxed and random effects is called a mixed model if p y x is a glm the overall model is called a generalized linear mixed effects model or glmm such models are widely used in statistics 9 6 1 example semi parametric glmms for medical data consider the following example from wand suppose yij is the amount of spinal bone mineral density sbmd for person j at measurement i let xij be the age of person and let xj be their ethnicity which can be one of white asian black or hispanic the primary goal is to determine if there are signiﬁcant differences in the mean sbmd among the four ethnic groups after accounting for age the data is shown in the light gray lines in figure 9 2 b we see that there is a nonlinear effect of sbmd vs age so we will use a semi parametric model which combines linear regression with non parametric regression ruppert et al we also see that there is variation across individuals within each group so we will use a mixed effects model speciﬁcally we will use xij 1 to account for the random effect of each person xij 0 since no other coefficients are person speciﬁc xij bk xij where bk is the k th spline basis functions see section 15 4 6 2 to account for the nonlinear effect of age and xj i xj w i xj a i xj b i xj h to account for the effect of the different ethnicities furthermore we use a linear link function the overall model is therefore e yij xij xj βj αt b xij eij 9 115 αwt i xj w αat i xj a αbt i xj b αht i xj h 9 116 where eij 0 σ2 α contains the non parametric part of the model related to age αt contains the parametric part of the model related to ethnicity and βj is a random offset for person j we endow all of these regression coefficients with separate gaussian priors we can then perform posterior inference to compute p α αt β σ2 d see section 9 6 2 for 10 3 inference we have seen that graphical models provide a compact way to deﬁne joint probability distribu tions given such a joint distribution what can we do with it the main use for such a joint distribution is to perform probabilistic inference this refers to the task of estimating unknown quantities from known quantities for example in section 10 2 2 we introduced hmms and said that one of the goals is to estimate the hidden states e g words from the observations e g speech signal and in section 10 2 4 we discussed genetic linkage analysis and said that one of the goals is to estimate the likelihood of the data under various dags corresponding to different hypotheses about the location of the disease causing gene in general we can pose the inference problem as follows suppose we have a set of correlated random variables with joint distribution p x1 v θ in this section we are assuming the parameters θ of the model are known we discuss how to learn the parameters in section 10 4 let us partition this vector into the visible variables xv which are observed and the hidden variables xh which are unobserved inference refers to computing the posterior distribution of the unknowns given the knowns visual recognition with humans in the loop abstract we present an interactive hybrid human computer method for object classification the method applies to classes of objects that are recognizable by people with appropriate expertise e g animal species or airplane model but not in general by people without such expertise it can be seen as a visual version of the questions game where questions based on simple visual attributes are posed interactively the goal is to identify the true class while minimizing the number of questions asked using the visual content of the image we introduce a general framework for incorporating almost any off the shelf multi class object recognition algorithm into the visual questions game and provide methodologies to account for imperfect user responses and unreliable computer vision algorithms we evaluate our methods on birds a difficult dataset of tightly related bird species and on the animals with attributes dataset our results demonstrate that incorporating user input drives up recognition accuracy to levels that are good enough for practical appli cations while at the same time computer vision reduces the amount of human interaction required introduction multi class object recognition has undergone rapid change and progress over the last decade these advances have largely focused on types of object categories that are easy for humans to recognize such as motorbikes chairs horses bot tles etc finer grained categories such as specific types of motorbikes chairs or horses are more difficult for humans and have received comparatively little attention one could argue that object recognition as a field is simply not ma ture enough to tackle these types of finer grained categories performance on basic level categories is still lower than what people would consider acceptable for practical applications state of the art accuracy on caltech is and in the voc detection challenge moreover the number of object categories in most object recognition datasets is still fairly low and in creasing the number of categories further is usually detrimental to performance a easy for humans b hard for humans c easy for humans chair airplane finch bunting yellow belly blue belly fig examples of classification problems that are easy or hard for humans while basic level category recognition left and recognition of low level visual at tributes right are easy for humans most people struggle with finer grained categories middle by defining categories in terms of low level visual properties hard classifica tion problems can be turned into a sequence of easy ones on the other hand recognition of finer grained subordinate categories is an important problem to study it can help people recognize types of objects they don t yet know how to identify we believe a hybrid human computer recognition method is a practical intermediate solution toward applying contemporary com puter vision algorithms to these types of problems rather than trying to solve object recognition entirely we take on the objective of minimizing the amount of human labor required as research in object recognition progresses tasks will become increasingly automated until eventually we will no longer need humans in the loop this approach differs from some of the prevailing ways in which peo ple approach research in computer vision where researchers begin with simpler and less realistic datasets and progressively make them more difficult and realis tic as computer vision improves e g caltech caltech caltech the advantage of the human computer paradigm is that we can provide usable services to people in the interim period where computer vision is still unsolved this may help increase demand for computer vision spur data collection and provide solutions for the types of problems people outside the field want solved in this work our goal is to provide a simple framework that makes it as effortless as possible for researchers to plug their existing algorithms into the human computer framework and use humans to drive up performance to lev els that are good enough for real life applications implicit to our model is the assumption that lay people generally cannot recognize finer grained categories e g myrtle warbler thruxton jackaroo etc due to imperfect memory or limited experiences however they do have the fundamental visual capabilities to recognize the parts and attributes that collectively make recognition possi ble see fig by contrast computers lack many of the fundamental visual capabilities that humans have but have perfect memory and are able to pool knowledge collected from large groups of people users interact with our system by answering simple yes no or multiple choice questions about an image or ob fig examples of the visual questions game on the class bird dataset human responses shown in red to questions posed by the computer shown in blue are used to drive up recognition accuracy in the left image computer vision algorithms can guess the bird species correctly without any user interaction in the middle image computer vision reduces the number of questions to in the right image computer vision provides little help ject as shown in fig similar to the questions we observe that the number of questions needed to classify an object from a database of c classes is usually o log c when user responses are accurate and can be faster when computer vision is in the loop our method of choosing the next question to ask uses an information gain criterion and can deal with noisy probabilistic user responses we show that it is easy to incorporate any computer vision algorithm that can be made to produce a probabilistic output over object classes our experiments in this paper focus on bird species categorization which we take to be a representative example of recognition of tightly related categories the bird dataset contains bird species and over images we believe that similar methodologies will apply to other object domains the structure of the paper is as follows in section we discuss related work in section we define the hybrid human computer problem and basic algorithm which includes methodologies for modeling noisy user responses and incorporating computer vision into the framework we describe our datasets and implementation details in section and present empirical results in section related work recognition of tightly related categories is still an open area in computer vi sion although there has been success in a few areas such as book covers and movie posters e g rigid mostly flat objects the problem is challenging because the number of object categories is larger with low interclass variance and variability in pose lighting and background causes high intraclass variance ability to exploit domain knowledge and cross category patterns and similarities becomes increasingly important there exist a variety of datasets related to recognition of tightly related cat egories including oxford flowers uiuc birds and while these works represent progress they still have shortcomings in scaling to large numbers of categories applying to other types of object domains or see for example http net fig visualization of the basic algorithm flow the system poses questions to the user which along with computer vision incrementally refine the probability distribution over classes achieving performance levels that are good enough for real world applications perhaps most similar in spirit to our work is the botanist field guide a system for plant species recognition with hundreds of categories and tens of thousands of images one key difference is that their system is intended pri marily for experts and requires plant leaves to be photographed in a controlled manner at training and test time making segmentation and pose normalization possible in contrast all of our training and testing images are obtained from flickr in unconstrained settings see fig and the system is intended to be used by lay people there exists a multitude of different areas in computer science that interleave vision learning or other processing with human input relevance feedback is a method for interactive image retrieval in which users mark the relevance of image search results which are in turn used to create a refined search query ac tive learning algorithms interleave training a classifier with asking users to label examples where the objective is to minimize the total number of la beling tasks our objectives are somewhat similar except that we are querying information at runtime rather than training time expert systems involve construction of a knowledge base and inference rules that can help non experts solve a problem our approach differs due to the added ability to observe image pixels as an additional source of information computationally our method also has similarities to algorithms based on information gain entropy calculation and decision trees finally a lot of progress has been made on trying to scale object recognition to large numbers of categories such approaches include using class taxonomies feature sharing error correcting output codes ecoc and attribute based classification methods all of these methods could be easily plugged into our framework to incorporate user interaction visual recognition with humans in the loop given an image x our goal is to determine the true object class c c by posing questions based on visual properties that are easy for the user to answer see fig at each step we aim to exploit the visual content of the image and algorithm visual questions game u for t to do j t maxk i c uk x u t ask user question qj t and u t u t uj t end for return class c maxc p c x u t the current history of question responses to intelligently select the next question the basic algorithm flow is summarized in fig let q qn be a set of possible questions e g isred hasstripes etc and ai be the set of possible answers to qi the user answer is some random variable ai ai we also allow users to qualify each response with a confidence value ri v e g v guessing probably definitely the user response is then a pair of random variables ui ai ri at each time step t we select a question qj t to pose to the user where j t n let j n t be an array of t indices to questions we will ask the user u t uj uj t is the set of responses obtained by time step t we use maximum information gain as the criterion to select qj t information gain is widely used in decision trees e g and can be computed from an estimate of p c x u t we define i c ui x u t the expected information gain of posing the addi tional question qi as follows ui ai v p ui x u t h c x ui u t h c x u t where h c x u t is the entropy of p c x u t c h c x u t p c x u t log p c x u t c the general algorithm for interactive object recognition is shown in algorithm in the next sections we describe in greater detail methods for modeling user responses and different methods for incorporating computer vision algorithms which correspond to different ways to estimate p c x u t incorporating computer vision when no computer vision is involved it is possible to pre compute a decision tree that defines which question to ask for every possible sequence of user re sponses with computer vision in the loop however the best questions depend dynamically on the contents of the image in this section we propose a simple framework for incorporating any multi class object recognition algorithm that produces a probabilistic output over classes we can compute p c x u where u is any arbitrary sequence of re sponses as follows p c x u p u c x p c x p u c p c x where z c p u c p c x here we make the assumption that p u c x p u c effectively this assumes that the types of noise or randomness that we see in user responses is class dependent and not image dependent we can still accommodate variation in responses due to user error subjectivity external factors and intraclass variance however we throw away some image related in formation for example we lose ability to model a change in the distribution of user responses as a result of a computer vision based estimate of object pose in terms of computation we estimate p c x using a classifier trained offline more details in section upon receiving an image we run the classifier once at the beginning of the process and incrementally update p c x u by gathering more answers to questions from the user one could imagine a system where a learning algorithm is invoked several times during the process as categories are weeded out by answers the system would use a more tuned classifier to update the estimate of p c x however our preliminary experiments with such methods did not show an note that when no computer vision is involved we simply replace p c x with a prior p c modeling user responses recall that for each question we may also ask a corresponding confidence value from the user which may be necessary when an attribute cannot be determined for example when the associated part are not visible we assume that the questions are answered independently given the category t p u c p ui c i the same assumption allows us to express p ui x u t in equation as c p ui x u t p ui c p c x u t c it may also be possible to use a more sophisticated model in which we estimate a full joint distribution for p u t c in our preliminary experiments this approach did not work well due to insufficient training data see supplementary material http www vision caltech edu visipedia html for more details fig examples of user responses for each of the attributes the distribu tion over guessing probably definitely is color coded with blue denoting and red denoting of the five answers per image attribute pair to compute p ui c p ai ri c p ai ri c p ri c we assume that p ri c p ri next we compute each p ai ri c as the posterior of a multinomial dis tribution with dirichlet prior dir αrp ai ri αcp ai c where αr and αc are constants p ai ri is a global attribute prior and p ai c is estimated by pooling together certainty labels in practice we use a larger prior term for guessing than definitely αguess αdef which effectively down weights the importance of any response with certainty level guessing datasets and implementation details in this section we provide a brief overview of the datasets we used methods used to construct visual questions computer vision algorithms we tested and parameter settings birds dataset birds is a dataset of images over bird species such as myrtle warblers pomarine jaegars and black footed albatrosses classes that cannot usually be identified by non experts in many cases different bird species are nearly visually identical see fig we assembled a set of visual questions list shown in fig which encom pass binary attributes e g the question hasbellycolor can take on dif ferent possible colors the list of attributes was extracted from whatbird a bird field guide website http www whatbird com we collected deterministic class attributes by parsing attributes from what bird com additionally we collected data of how non expert users respond to at tribute questions via a mechanical turk interface to minimize the effects of user subjectivity and error our interface provides prototypical images of each possi ble attribute response the reader is encouraged to look at the supplementary material for screenshots of the question answering user interface and example images of the dataset fig shows a visualization of the types of user response results we get on the birds dataset it should be noted that the uncertainty of the user responses strongly correlates with the parts that are visible in an image as well as overall difficulty of the corresponding bird species when evaluating performance test results are generated by randomly select ing a response returned by an mturk user for the appropriate test image animals with attributes we also tested performance on the animals with attributes awa a dataset of animal classes and binary attributes we consider this dataset less relevant than birds because classes are recognizable by non experts and therefore do not focus as much on this dataset implementation details and parameter settings for both datasets our computer vision algorithms are based on andrea vedaldi publicly available source code which combines vector quantized geometric blur and color gray sift features using spatial pyramids multiple kernel learn ing and per class vs all svms we added features based on full image color histograms and vector quantized color histograms for each classifier we used platt scaling to learn parameters for p c x on a validation set we used training examples for each birds class and training examples for each awa class bird training and testing images are roughly cropped additionally we compare performance to a second computer vision algorithm based on attribute classifiers which we train using the same features training code with positive and negative examples set using whatbird com attribute la bels we combined attribute classifiers into per class probabilities p c x using the method described in for estimating user response statistics on the birds dataset we used αguess αprob αdef and αc see section experiments in this section we provide experimental results and analysis of the hybrid human computer classification paradigm due to space limitations our discussion fo cuses on the birds dataset we include results see fig from which the user can verify that trends are similar on birds and awa and we include addi tional results on awa in the supplementary material number of binary questions asked fig different models of user responses left classification performance on birds method without computer vision performance rises quickly blue curve if users respond deterministically according to whatbird com attributes mturk users respond quite differently resulting in low performance green curve a learned model of mturk responses is much more robust red curve right a test image where users answer several questions incorrectly and our model still classifies the image correctly measuring performance we use two main methodologies for measuring performance which correspond to two different possible user interfaces method we ask the user exactly t questions predict the class with highest probability and measure the percent of the time that we are correct method after asking each question we present a small gallery of images of the highest probability class and allow the user to stop the system early we measure the average number of questions asked per test image for the second method we assume that people are perfect verifiers e g they will stop the system if and only if they have been presented with the correct class while this is not always possible in reality there is some trade off between classification accuracy and amount of human labor and we believe that these two metrics collectively capture the most important considerations results in this section we present our results and discuss some interesting trends toward understanding the visual questions classification paradigm user responses are stochastic in fig we show the effects of different models of user responses without using any computer vision when users are assumed to respond deterministically in accordance with the attributes from whatbird com performance rises quickly to within questions roughly however this assumption is not realistic when testing with responses 15 40 number of binary questions asked 05 12 number of binary questions asked fig performance on birds when using computer vision left plot comparison of classification accuracy method with and without computer vision when using mturk user responses two different computer vision algorithms are shown one based on per class vs all classifiers and another based on attribute classifiers right plot the number of questions needed to identify the true class method drops from to on average when incorporating computer vision from mechanical turk performance saturates at around low performance caused by subjective answers are unavoidable e g perception of the color brown vs the color buff and the probability of the correct class drops to zero after any inconsistent response although performance is times better than random chance it renders the system useless this demonstrates a challenge for exist ing field guide websites when our learned model of user responses see section is incorporated performance jumps to due to the ability to tolerate a reasonable degree of error in user responses see fig for an example never theless stochastic user responses increase the number of questions required to achieve a given accuracy level and some images can never be classified correctly even when asking all possible questions in section we discuss the reasons why performance saturates at lower than performance computer vision reduces manual labor the main benefit of computer vision occurs due to reduction in human labor in terms of the number of ques tions a user has to answer in fig we see that computer vision reduces the average number of yes no questions needed to identify the true bird species from 11 to using responses from mturk users without computer vision the distribution of question counts is bell shaped and centered around questions when computer vision is incorporated the distribution peaks at 0 questions but is more heavy tailed which suggests that computer vision algorithms are often good at recognizing the easy test examples examples that are sufficiently sim ilar to the training data but provide diminishing returns toward classifying the harder examples that are not sufficiently similar to training data as a result computer vision is more effective at reducing the average amount of time than reducing the time spent on the most difficult images user responses drive up performance an alternative way of interpret ing the results is that user responses drive up the accuracy of computer vision fig examples where computer vision and user responses work together left an image that is only classified correctly when computer vision is incorporated additionally the computer vision based method selects the question hasthroatcolor white a different and more relevant question than when vision is not used in the right image the user response to hascrowncolorblack helps correct computer vision when its initial prediction is wrong algorithms in fig we see that user responses improve overall performance from using 0 questions to computer vision improves overall performance even when users an swer all questions performance saturates at a higher level when using computer vision vs see fig the left image in fig shows an example of an image classified correctly using computer vision which is not classified cor rectly without computer vision even after asking questions in this example some visually salient features like the long neck are not captured in our list of visual attribute questions the features used by our vision algorithms also cap ture other cues such as global texture statistics that are not well represented in our list of attributes which capture mostly color and part localized patterns different questions are asked with and without computer vision in general the information gain criterion favors questions that can be an swered reliably and split the set of possible classes roughly in half questions like hasshapeperchinglike which divide the classes fairly evenly and hasunder partscoloryellow which tends to be answered reliably are commonly chosen when computer vision is incorporated the likelihood of classes change and different questions are selected in the left image of fig we see an example where a different question is asked with and without computer vision which allows the system to find the correct class using one question recognition is not always successful according the the cornell ornithol ogy the four keys to bird species recognition are size and shape http www allaboutbirds org netcommunity page aspx pid fig images that are misclassified by our system left the parakeet auklet image is misclassified due to a cropped image which causes an incorrect answer to the belly pattern question the parakeet auklet has a plain white belly see fig right the sayornis and gray kingbird are commonly confused due to visual similarity color and pattern behavior and habitat bird species classification is a difficult problem and is not always possible using a single image one po tential advantage of the visual questions paradigm is that other contextual sources of information such as behavior and habitat can easily be incorporated as additional questions fig illustrates some example failures the most common failure conditions occur due to classes that are nearly visually identical 2 images of poor viewpoint or low resolution such that some parts are not visible significant mistakes made by mturkers or inadequacies in the set of attributes we used vs all vs attribute based classification in general vs all classifiers slightly outperform attribute based classifiers however they converge to similar performance as the number of question increases as shown in fig and the features we use kernelized and based on bag of words may not be well suited to the types of attributes we are using which tend to be localized and associated with a particular part one potential advantage of attribute based methods is computational scalability when the number of classes increases whereas vs all methods always require c classifiers the number of attribute classifiers can be varied in order to trade off accuracy and computation time the table below displays the average number of questions needed method on the birds dataset using different number of attribute classifiers which were selected randomly vs all attr attr attr attr attr 43 72 7 8 52 conclusion object recognition remains a challenging problem for computer vision further more recognizing tightly related categories in one shot is difficult even for hu fig performance on animals with attributes left plot classification per formance method 1 simulating user responses using soft class attributes see right plot the required number of questions needed to identify the true class method 2 drops from 94 to 11 on average when incorporating computer vision mans without proper expertise our work attempts to leverage the power of both human recognition abilities and that of computer vision we presented a sim ple way of designing a hybrid human computer classification system which can be used in conjunction with a large variety of computer vision algorithms our results show that user input significantly drives up performance while it may take many years before object recognition algorithms achieve reasonable perfor mance on their own incorporating human input can produce usable recognition systems on the other hand having computer vision in the loop reduces the amount of required human labor to successfully classify an image finally we showed that incorporating models of stochastic user responses leads to much bet ter reliability in comparison to deterministic field guides generated by experts we believe our work opens the door to many interesting sub problems the most obvious next step is to explore other types of domains while we were able to extract a set of reasonable attributes questions for the bird dataset this may be more difficult for other domains one possible topic for future work is to find a more principled way of discovering a set of useful questions alternative types of user input such as asking the user to click on the location of certain parts could also be investigated lastly while we used off the shelf computer vision algorithms in this work it may be possible to improve them to better suit the challenges of tightly related category recognition such as algorithms that incorporate a part based model chap ter local features detection and description significant progress towards robustly recognizing objects has been made in the past decade through the development of local invariant features these features allow the algorithm to find local image structures in a repeatable fashion and to encode them in a representation that is invariant to a range of image transformations such as translation rotation scaling and affine deformation the resulting features then form the basis of approaches for recognizing both specific objects and object categories in this chapter we will explain the basic ideas and implementation steps behind state of the art local feature detectors and descriptors a more extensive treatment of local features in cluding detailed comparisons and usage guidelines can be found in tuytelaars and mikolajczyk systematic experimental comparisons are reported in mikolajczyk and schmid mikolajczyk et al introduction the purpose of local invariant features is to provide a representation that allows to efficiently match local structures between images that is we want to obtain a sparse set of local measurements that capture the essence of the underlying input images and that encode their interesting structure to meet this goal the feature extractors must fulfill two important criteria the feature extraction process should be repeatable and precise so that the same features are extracted from two images showing the same object at the same time the features should be distinctive so that different image structures can be told apart from each other in addition we typically require a sufficient number of feature regions to cover the target object so that it can still be recognized under partial occlusion this is achieved by the following feature extraction pipeline illustrated in figure find a set of distinctive keypoints define a region around each keypoint in a scale or affine invariant manner extract and normalize the region content n pixels d f a fb t figure an illustration of the recognition procedure with local features we first find distinctive keypoints in both images for each such keypoint we then define a surrounding region in a scale and rotation invariant manner we extract and normalize the region content and compute a local descriptor for each region feature matching is then performed by comparing the local descriptors using a suitable similarity measure courtesy of krystian mikolajczyk compute a descriptor from the normalized region match the local descriptors in the remainder of this chapter we will discuss the keypoint and descriptor steps in detail then in the following chapter we will describe methods for computing candidate matches among the descriptors once we have candidate matches we can then proceed to verify their geometric relationships as we will describe in chapter detection of interest points and regions keypoint localization the first step of the local feature extraction pipeline is to find a set of distinctive keypoints that can be reliably localized under varying imaging conditions viewpoint changes and in the presence of noise in particular the extraction procedure should yield the same feature locations if the input image is translated or rotated it is obvious that those criteria cannot be met for all image points for instance if we consider a point lying in a uniform region we cannot determine its exact motion since we cannot distinguish the point from its neighbors similarly if we consider a point on a straight line we can only measure its motion perpendicular to the line this motivates us to focus on a particular subset of points namely those exhibiting signal changes in two directions in the following we will present two keypoint detectors that employ different criteria for finding such regions the hessian detector and the harris detector the hessian detector the hessian detector beaudet searches for image locations that exhibit strong derivatives in two orthogonal directions it is based on the matrix of second derivatives the so called hessian as derivative operations are sensitive to noise we always use gaussian derivatives in the following i e we combine the derivative operation with a gaussian smoothing step with smoothing parameter σ h x ixx x σ ixy x σ ixy x σ iyy x σ l the detector computes the second derivatives ixx ixy and iyy for each image point and then searches for points where the determinant of the hessian becomes maximal det h ixx iyy i this search is usually performed by computing a result image containing the hessian determinant values and then applying non maximum suppression using a window in this procedure the search window is swept over the entire image keeping only pixels whose value is larger than the values of all immediate neighbors inside the window the detector then returns all remaining locations whose value is above a pre defined threshold θ as shown in figure top left the resulting detector responses are mainly located on corners and in strongly textured image areas the harris detector the popular harris förstner detector förstner and gülch harris and stephens was explicitly designed for geometric stability it defines keypoints to be points that have locally maximal self matching precision under translational least squares template matching triggs in practice these keypoints often correspond to corner like structures the detection procedure is visualized in figure the harris detector proceeds by searching for points x where the second moment matrix c around x has two large eigenvalues the matrix c can be computed from the first derivatives in a window around x weighted by a gaussian g x σ c x x i x σ ixiy x σ l figure the harris detector searches for image neighborhoods where the second moment matrix c has two large eigenvalues corresponding to two dominant orientations the resulting points often correspond to corner like structures courtesy of dennis simakov and darya frolova in this formulation the convolution with the gaussian g x σ takes the role of summing over all pixels in a circular local neighborhood where each pixel contribution is additionally weighted by its proximity to the center point instead of explicitly computing the eigenvalues of c the following equivalences are used det c trace c to check if their ratio r is below a certain threshold with c r det c rλ2 r this can be expressed by the following condition det c c t which avoids the need to compute the exact eigenvalues typical values for α are in the range of 06 the parameter σ is usually set to so that the considered image neighborhood is slightly larger than the support of the derivative operator used figure top right shows the results of the harris detector and compares them to those of the hessian as can be seen the returned locations are slightly different as a result of the changed selection criterion in general it can be stated that harris locations are more specific to corners f ii i x f ii i x m m figure the principle behind automatic scale selection given a keypoint location we evaluate a scale dependent signature function on the keypoint neighborhood and plot the resulting value as a function of the scale if the two keypoints correspond to the same structure then their signature functions will take similar shapes and corresponding neighborhood sizes can be determined by searching for scale space extrema of the signature function independently in both images courtesy of krystian mikolajczyk while the hessian detector also returns many responses on regions with strong texture variation in addition harris points are typically more precisely located as a result of using first derivatives rather than second derivatives and of taking into account a larger image neighborhood thus harris points are preferable when looking for exact corners or when precise localization is required whereas hessian points can provide additional locations of interest that result in a denser coverage of the object scale invariant region detection while shown to be remarkably robust to image plane rotations illumination changes and noise schmid et al the locations returned by the harris and hessian detectors are only repeatable up to relatively small scale changes the reason for this is that both detectors rely on gaussian derivatives computed at a certain fixed base scale σ if the image scale differs too much between the test images then the extracted structures will also be different for scale invariant feature extraction it is thus necessary to detect structures that can be reliably extracted under scale changes figure the scale normalized laplacian of gaussian log is a popular choice for a scale selection filter its filter mask takes the shape of a circular center region with positive weights surrounded by another circular region with negative weights the filter response is therefore strongest for circular image structures whose radius corresponds to the filter scale automatic scale selection the basic idea behind automatic scale selection is visualized in figure given a keypoint in each image of an image pair we want to determine whether the surrounding image neighborhoods contain the same structure up to an unknown scale factor in principle we could achieve this by sampling each image neighborhood at a range of scales and performing n n pairwise comparisons to find the best match this is however too expensive to be of practical use instead we evaluate a signature function on each sampled image neighborhood and plot the result value as a function of the neighborhood scale since the signature function measures properties of the local image neighborhood at a certain radius it should take a similar qualitative shape if the two keypoints are centered on corresponding image structures the only difference will be that one function shape will be squashed or expanded compared to the other as a result of the scaling factor between the two images thus corresponding neighborhood sizes can be detected by searching for extrema of the signature function independently in both images if corresponding extrema σ and σ t are found in both cases then the scaling factor between the two images can be obtained as σ t effectively this procedure builds up a scale space witkin of the responses produced by the application of a local kernel with varying scale parameter σ in order for this idea to work the signature function or kernel needs to have certain specific properties it can be shown that the only operator that fulfills all necessary conditions for this purpose is the scale normalized gaussian kernel g x σ and its derivatives lindeberg lxx lyy figure the laplacian of gaussian log detector searches for scale space extrema of the log function courtesy of krystian mikolajczyk the laplacian of gaussian log detector based on the above idea lindeberg proposed a detector for blob like features that searches for scale space extrema of a scale normalized laplacian of gaussian log lindeberg l x σ σ ixx x σ iyy x σ as shown in figure the log filter mask corresponds to a circular center surround structure with positive weights in the center region and negative weights in the surrounding ring structure thus it will yield maximal responses if applied to an image neighborhood that contains a similar roughly circular blob structure at a corresponding scale by searching for scale space extrema of the log we can therefore detect circular blob structures note that for such blobs a repeatable keypoint location can also be defined as the blob center the log can thus both be applied for finding the characteristic scale for a given image location and for directly detecting scale invariant regions by searching for location scale extrema of the log this latter procedure is visualized in figure and resulting interest regions are shown in figure bottom left the difference of gaussian dog detector as shown by lowe the scale space laplacian can be approximated by a difference of gaussian dog d x σ which can be more efficiently obtained from the difference of two adjacent sampling with step original image figure the difference of gaussian dog provides a good approximation for the laplacian of gaussian it can be efficiently computed by subtracting adjacent scale levels of a gaussian pyra mid the dog region detector then searches for scale space extrema of the dog function from tuytelaars and mikolajczyk scales that are separated by a factor of k d x σ g x kσ g x σ i x lowe shows that when this factor is constant the computation already includes the required scale normalization one can therefore divide each scale octave into an equal number k of intervals such that k k and σn for more efficient computation the resulting scale space can be implemented with a gaussian pyramid which resamples the image by a factor of after each scale octave see figure as in the case of the log detector dog interest regions are defined as locations that are simultaneously extrema in the image plane and along the scale coordinate of the d x σ function such points are found by comparing the d x σ value of each point with its neighborhood on the same scale level and with the closest neighbors on each of the two adjacent levels as depicted in the right side of figure since the scale coordinate is only sampled at discrete levels it is important in both the log and the dog detector to interpolate the responses at neighboring scales in order to increase the accuracy of detected keypoint locations in the simplest version this could be done by fitting a second order polynomial to each candidate point and its two closest neighbors a more exact approach was introduced by brown and lowe this approach simultaneously interpolates both the location and scale coordinates of detected peaks by fitting a quadric function figure example results of the top left hessian detector top right harris detector bottom left laplacian of gaussian detector bottom right difference of gaussian detector courtesy of krystian mikolajczyk from tuytelaars and mikolajczyk finally those regions are kept that pass a threshold t and whose estimated scale falls into a certain scale range smin smax the resulting interest point operator reacts to blob like structures that have their maximal extent in a radius of approximately of the detected points as can be derived from the zero crossings of the modeled laplacian in order to also capture some of the surrounding structure the extracted region is typically larger most current interest region detectors choose a radius of r around the detected points figure bottom right shows the result regions returned by the dog detector on an example image it can be seen that the obtained regions are very similar to those of the log detector in practice the dog detector is therefore often the preferred choice since it can be computed far more efficiently the harris laplacian detector the harris laplacian operator mikolajczyk and schmid was proposed for increased discriminative power compared to the laplacian or dog operators described so far it combines the harris operator specificity for corner like structures with the scale selection mechanism by lindeberg the method first builds up two separate scale spaces for the harris function and the laplacian it then uses the harris function to localize candidate points on each scale level and selects those points for which the laplacian simultaneously attains an extremum over scales the resulting points are robust to changes in scale image rotation illumination and camera noise in addition they are highly discriminative as several comparative studies show mikolajczyk and schmid as a drawback however the original harris laplacian detector typically returns a much smaller number of points than the laplacian or dog detectors this is not a result of changed threshold settings but of the additional constraint that each point has to fulfill two different maxima conditions simultaneously for many practical object recognition applications the lower number of interest regions may be a disadvantage as it reduces robustness to partial occlusion this is especially the case for object categorization where the potential number of corresponding features is further reduced by intra category variability for this reason an updated version of the harris laplacian detector has been proposed based on a less strict criterion mikolajczyk and schmid instead of searching for simultaneous maxima it selects scale maxima of the laplacian at locations for which the harris function also attains a maximum at any scale as a result this modified detector yields more interest points at a slightly lower precision which results in improved performance for applications where a larger absolute number of interest regions is required mikolajczyk et al the hessian laplace detector as in the case of the harris laplace the same idea can also be applied to the hessian leading to the hessian laplace detector as with the single scale versions the hessian laplace detector typically returns more interest regions than harris laplace at a slightly lower repeatability mikolajczyk et al affine covariant region detection the approaches discussed so far yield local features that can be extracted in a manner that is invariant to translation and scale changes for many practical problems however it also becomes important to find features that can be reliably extracted under large viewpoint changes if we assume that the scene structure we are interested in is locally planar then this would boil down to estimating and correcting for the perspective distortion a local image patch undergoes when seen from a dif ferent viewpoint unfortunately such a perspective correction is both computationally expensive and error prone since the local feature patches typically contain only a small number of pixels it has however been shown by a number of researchers matas et al mikolajczyk and schmid schaffalitzky and zisserman tuytelaars and van gool that a local affine approximation is sufficient in such cases we therefore aim to extend the region extraction procedure to affine covariant while a scale and rotation invariant region can be described by a circle an affine deformation transforms this circle to an ellipse we thus aim to find local regions for which such an ellipse can be reliably and repeatedly extracted purely from local image properties literature speaks of affine covariant extraction here in order to emphasize the property that extracted region shapes vary according to the underlying affine deformation this is required so that the region content will be invariant harris and hessian affine detectors both the harris laplace and hessian laplace detectors can be extended to yield affine covariant regions this is done by the following iterative estimation scheme the procedure is initialized with a circular region returned by the original scale invariant detector in each iteration we build up the region second moment matrix and compute the eigenvalues of this matrix this yields an elliptical shape as shown in figure corresponding to a local affine deformation we then transform the image neighborhood such that this ellipse is transformed to a circle and update the location and scale estimate in the transformed image this procedure is repeated until the eigenvalues of the second moment matrix are approximately equal as a result of this iterative estimation scheme we obtain a set of elliptical regions which are adapted to the local intensity patterns so that the same object structures are covered despite the deformations caused by viewpoint changes maximally stable extremal regions mser a different approach for finding affine covariant regions has been proposed by matas et al in contrast to the above methods which start from keypoints and progressively add invariance levels this approach starts from a segmentation perspective it applies a watershed segmentation algorithm to the image and extracts homogeneous intensity regions which are stable over a large range of thresholds thus ending up with maximally stable extremal regions mser by construction those regions are stable over a range of imaging conditions and can still be reliably extracted under viewpoint changes since they are generated by a segmentation process they are not restricted to elliptical shapes but can have complicated contours in fact the contour shape itself is often a good feature which has led to the construction of specialized contour feature descriptors matas et al for consistency with the other feature extraction steps discussed here an elliptical region can however also easily be fitted to the maximally stable regions by computing the eigenvectors of their second moment matrices other interest region detectors several other interest region detectors have been proposed that are not discussed here tuyte laars van gool introduced detectors for affine covariant intensity based regions ibr and edge based regions ebr tuytelaars and van gool kadir brady proposed a salient regions detector that was later on also extended to affine covariant extraction kadir and brady kadir et al an overview over those detectors and a discussion of their merits can be found in tuytelaars and mikolajczyk orientation normalization after a scale invariant region has been detected its content needs to be normalized for rotation invariance this is typically done by finding the region dominant orientation and then rotating the region content according to this angle in order to bring the region into a canonical orientation lowe suggests the following procedure for the orientation normalization step for each detected interest region the region scale is used to select the closest level of the gaussian pyramid so that all following computations are performed in a scale invariant manner we then build up a gradient orientation histogram with bins covering the range of orientations for each pixel in the region the corresponding gradient orientation is entered into the histogram weighted by the pixel gradient magnitude and by a gaussian window centered on the keypoint with a scale of the highest peak in the orientation histogram is taken as the dominant orientation and a parabola is fitted to the adjacent histogram values to interpolate the peak position for better accuracy in practice it may happen that multiple equally strong orientations are found for a single interest region in such cases selecting only one of them would endanger the recognition procedure since small changes in the image signal could cause one of the other orientations to be chosen instead which could lead to failed matches for this reason lowe suggests to create a separate interest region for each orientation peak that reaches at least of the dominant peak value lowe this strategy significantly improves the region detector repeatability at a relatively small additional cost according to lowe only about of the points are assigned multiple orientations summary of local detectors summarizing the above we have seen the following local feature detectors so far if precisely localized points are of interest we can use the harris and hessian detectors when looking for scale invariant regions we can choose between the log or dog detectors both of which react to blob shaped structures in addition we can combine the harris and hessian point detectors with the lapla cian scale selection idea to obtain the harris laplacian and hessian laplacian detectors finally we can further generalize those detectors to affine covariant region extraction resulting in the harris affine and hessian affine detectors the affine covariant region detectors are complemented by the mser detector which is based on maximally stable segmentation regions all of those de tectors have been used in practical applications detailed experimental comparisons can be found in mikolajczyk and schmid tuytelaars and mikolajczyk local descriptors once a set of interest regions has been extracted from an image their content needs to be encoded in a descriptor that is suitable for discriminative matching the most popular choice for this step is the sift descriptor lowe which we present in detail in the following the sift descriptor the scale invariant feature transform sift was originally introduced by lowe as combination of a dog interest region detector and a corresponding feature descriptor lowe however both components have since then also been used in isolation in particular a series of studies has local descriptors keypoint descriptor figure visualization of the sift descriptor computation for each orientation normalized scale invariant region image gradients are sampled in a regular grid and are then entered into a larger grid of local gradient orientation histograms for visibility reasons only a grid is shown here based on lowe and lowe confirmed that the sift descriptor is suitable for combination with all of the above mentioned region detectors and that it achieves generally good performance mikolajczyk and schmid the sift descriptor aims to achieve robustness to lighting variations and small positional shifts by encoding the image information in a localized set of gradient orientation histograms the descriptor computation starts from a scale and rotation normalized region extracted with one of the above mentioned detectors as a first step the image gradient magnitude and orientation is sampled around the keypoint location using the region scale to select the level of gaussian blur i e the level of the gaussian pyramid at which this computation is performed sampling is performed in a regular grid of locations covering the interest region for each sampled location the gradient orientation is entered into a coarser grid of gradient orientation histograms with orientation bins each weighted by the corresponding pixel gradient magnitude and by a circular gaussian weighting function with a σ of half the region size the purpose of this gaussian window is to give higher weights to pixels closer to the middle of the region which are less affected by small localization inaccuracies of the interest region detector this procedure is visualized for a smaller grid in figure the motivation for this choice of representation is that the coarse spatial binning allows for small shifts due to registration errors without overly affecting the descriptor at the same time the high dimensional representation provides enough discriminative power to reliably distinguish a large number of keypoints when computing the descriptor it is important to avoid all boundary effects both with respect to spatial shifts and to small orientation changes thus when entering a sampled pixel gradient information into the dimensional spatial orientation histogram its contribution should be smoothly distributed among the adjoining histogram bins using trilinear interpolation once all orientation histogram entries have been completed those entries are concatenated to form a single dimensional feature vector a final illumination normalization completes the extraction procedure for this the vector is first normalized to unit length thus adjusting for changing image contrast then all feature dimensions are thresholded to a maximum value of and the vector is again normalized to unit length this last step compensates for non linear illumination changes due to camera saturation or similar effects the surf detector descriptor as local feature detectors and descriptors have become more widespread efficient implementations are getting more and more important consequently several approaches have been proposed in order to speed up the interest region extraction and or description stages bay et al cornelis and van gool rosten and drummond among those we want to pick out the surf speeded up robust features approach which has been designed as an efficient alternative to sift bay et al figure the surf detector and descriptor were designed as an efficient alternative to sift instead of relying on ideal gaussian derivatives their computation is based on simple box filters which can be efficiently evaluated using integral images based on bay et al surf combines a hessian laplace region detector with its own gradient orientation based feature descriptor instead of relying on gaussian derivatives for its internal computations it is how ever based on simple box filters haar wavelets as shown in figure those box filters approximate the effects of the derivative filter kernels but they can be efficiently evaluated using integral images viola and jones in particular this evaluation requires the same constant number of lookups regardless of the image scale thus removing the need for a gaussian pyramid concluding remarks despite this simplification surf has been shown to achieve comparable repeatability as detec tors based on standard gaussian derivatives while yielding speedups of more than a factor of five compared to standard dog the surf descriptor is also motivated by sift and pursues a similar spatial binning strategy dividing the feature region into a grid however instead of building up a gradient orientation histogram for each bin surf only computes a set of summary statistics dx dx dy and dy resulting in a dimensional descriptor or a slightly extended set resulting in a dimensional descriptor version motivated by the success of surf a further optimized version has been proposed by cornelis and van gool that takes advantage of the computational power available in current cuda enabled graphics cards this gpusurf implementation has been reported to per form feature extraction for a image at frame rates up to hz i e taking only per frame thus making feature extraction a truly affordable processing step concluding remarks the first step in the specific object recognition pipeline is to extract local features from both the training and test images for this we can use any of the feature detectors and descriptors described in this chapter the appropriate choice of detector class single scale scale invariant or affine invariant mainly depends on the type of viewpoint variations foreseen for the target application for many practical recognition applications scale invariant features in particular sift lowe have proven a good compromise since they are fast to extract are robust to moderate viewpoint variations and retain enough discriminative power to allow for reliable matching when dealing with larger viewpoint changes as in wide baseline stereo matching applications affine invariance becomes important in order to still establish correspondences however the added invariance comes at the price of reduced discriminative power since several different elliptical regions can be mapped onto the same circular descriptor neighborhood and generally also a smaller number of features since not all regions have a characteristic affine covariant neighborhood whenever possible it is therefore advisable to use the simpler feature representation the development of local invariant features has had an enormous impact in many areas of computer vision including wide baseline stereo matching image retrieval object recognition and categorization they have provided the basis for many state of the art algorithms and have led to a number of new developments moreover efficient implementations for all detectors discussed in this chapter are freely available gpu oxf sur making them truly building blocks that other researchers can build on in the next two chapters we discuss how to compute candidate matching descriptors and then describe how the geometric consistency of those candidate matches are verified to perform specific object recognition in later chapters we will again draw on local descriptors to build models for generic object categorization chap ter matching local features in the previous chapter we saw several specific techniques to detect repeatable interest points in an image section and then robustly describe the local appearance at each such point section now given an image and its local features we need to be able to match them to similar looking local features in other images e g to model images of the specific objects we are trying to recognize see figure to identify candidate matches we essentially want to search among all previously seen local descriptors and retrieve those that are nearest according to euclidean distance in the feature space such as the dimensional sift space because the local descriptions are by design invariant to rotations translations scalings and some photometric effects this matching stage will be able to tolerate reasonable variations in view point pose and illumination across the views of the object further due to the features distinctive ness if we detect a good correspondence based on the local feature matches alone we will already have a reasonable measure of how likely it is that two images share the same object however to strengthen confidence and eliminate ambiguous matches it is common to follow the matching pro cess discussed in this chapter with a check for geometric consistency as we will discuss in chapter the naive solution to identifying local feature matches is straightforward simply scan through all previously seen descriptors compare them to the current input descriptor and take those within some threshold as candidates unfortunately however such a linear time scan is usually unrealistic in terms of computational complexity in many practical applications one has to search for matches in a database of millions of features thus efficient algorithms for nearest neighbor or similarity search are crucial the focus of this chapter is to describe the algorithms frequently used in the recognition pipeline to rapidly match local descriptors specifically in the first section of this chapter we overview both tree based algorithms for exact near neighbor search as well as approximate nearest neighbor algorithms largely hashing based that are more amenable for high dimensional descriptors then in section we describe a frequently used alternative based on visual vocabularies instead of performing similarity search in the raw vector space of the descriptors the vocabulary based method first quantizes the feature space into discrete visual words making it possible to index feature matches easily with an inverted file fact while our focus at this stage is on local feature matching for specific object recognition most of the algorithms discussed are quite general and also come into play for other recognition related search tasks such as near neighbor image retrieval model images or exemplars nput features in new image local feature descriptors from model images candidate matches base on descriptor similarity figure the goal when matching local features is to find those descriptors from any previously seen model exemplar that are near in the feature space to those local features in a novel image depicted on the left since each exemplar image may easily contain on the order of hundreds to thousands of interest points the database of descriptors quickly becomes very large to make searching for matches practical the database must be mapped to data structures for efficient similarity search efficient similarity search what methods are effective for retrieving descriptors relevant to a new image the choice first depends on the dimensionality of the descriptors for low dimensional points effective data struc tures for exact nearest neighbor search are known e g kd trees friedman et al for high dimensional points these methods become inefficient and so researchers often employ approximate similarity search methods this section overviews examples of both such techniques that are widely used in specific object matching tree based algorithms data structures using spatial partitions and recursive hyperplane decomposition provide an effi cient means to search low dimensional vector data exactly the kd tree friedman et al is one such approach that has often been employed to match local descriptors in several variants e g beis and lowe lowe muja and lowe silpa anan and hartley the kd tree is a binary tree storing a database of k dimensional points in its leaf nodes it recur sively partitions the points into axis aligned cells dividing the points approximately in half by a line perpendicular to one of the k coordinate axes the division strategies aim to maintain balanced trees and or uniformly shaped cells for example by choosing the next axis to split according to that which has the largest variance among the database points or by cycling through the axes in order to find the point nearest to some query one traverses the tree following the same divisions that were used to enter the database points upon reaching a leaf node the points found there are compared to the query the nearest one becomes the current best while the point is nearer than others to the query it need not be the absolute nearest for example consider a query occurring near the initial dividing split at the top of the tree which can easily be nearer to some points on the other side of the dividing hyperplane thus the search continues by backtracking along the unexplored branches checking whether the circle formed about the query by the radius given by the current best match intersects with a subtree cell area if it does that subtree is considered further and any nearer points found as the search recurses are used to update the current best if not the subtree can be pruned see figure for a sketch of an example tree and query the procedure guarantees that the nearest point will be found constructing the tree for n database points an offline cost for a single database requires o n log n time inserting points requires o log n time processing a query requires o n time and the algorithm is known to be quite effective for low dimensional data i e fewer than dimensions in high dimensional spaces however the algorithm ends up needing to visit many more branches during the backtracking stage and in general degrades to worst case linear scan performance in practice the particular behavior depends not only on the dimension of the points but also the distribution of the database examples that have been indexed combined with the choices in how divisions are computed other types of tree data structures can operate with arbitrary metrics ciaccia et al uhlmann removing the requirement of having data in a vector space by exploiting the triangle inequality however similar to kd trees the metric trees in practice rely on good heuristics for selecting useful partitioning strategies and in spite of logarithmic query times in the expectation also degenerate to a linear time scan of all items depending on the distribution of distances for the data set since high dimensional image descriptors are commonly used in object recognition several strategies to mitigate these factors have been explored one idea is to relax the search requirement to allow the return of approximate nearest neighbors using a variant of kd trees together with a priority queue arya et al beis and lowe another idea is to generate multiple randomized kd trees e g by sampling splits according to the coordinates variance and then process the query in all trees using a single priority queue across them silpa anan and hartley given the sensitivity of the algorithms to the data distribution some recent work also attempts to automatically select algorithm configurations and parameters for satisfactory performance by using a cross validation approach muja and lowe another interesting direction pursued for improving the efficiency and effectiveness of tree based search involves integrating learning or the matching task into the queries and k nn queries are also supported a build tree and populate with database points b perform initial traversal for a query point c record current best neighbor d backtrack to the unexplored sub trees e update nearest point and distance if a closer point is found f disregard subtrees that cannot be any closer figure sketch of kd tree processing a the database points are entered into a binary tree where each division is an axis aligned hyperplane b given a new query red point for which we wish to retrieve the nearest neighbor first the tree is traversed choosing the left or right subtree at each node according to the query value in the coordinate along which this division was keyed the green dotted box denotes the cell containing the points in the leaf node reached by this query c at this point we know the current best point is that in the leaf node that is closest to the query denoted with the outer red circle d then we backtrack and consider the other branch at each node that was visited checking if its cell intersects the current best circle around the query e if so its subtree is explored further and the current best radius is updated if a nearer point is found f continue and prune subtrees once a comparison at its root shows that it cannot improve on the current nearest point courtesy of the auton lab carnegie mellon university tree construction for example by using decision trees in which each internal node is associated with a weak classifier built with simple measurements from the feature patches lepetit et al obdrzalek and matas hashing based algorithms and binary codes hashing algorithms are an effective alternative to tree based data structures motivated by the inadequacy of existing exact nearest neighbor techniques to provide sub linear time search for high dimensional data including the kd tree and metric tree approaches discussed above randomized approximate hashing based similarity search algorithms have been explored the idea in approximate similarity search is to trade off some precision in the search for the sake of substantial query time reductions more specifically guarantees are of the general form if for a query point q there exists a database point x such that d q x r for some search radius r then with high probability a point xt is returned such that d q xt e r otherwise the absence of such a point is reported locality sensitive hashing locality sensitive hashing lsh charikar datar et al gionis et al indyk and motwani is one such algorithm that offers sub linear time search by hashing highly similar examples together in a hash table the idea is that if one can guarantee that a ran domized hash function will map two inputs to the same bucket with high probability only if they are similar then given a new query one needs only to search the colliding database examples to find those that are most probable to lie in the input near neighborhood the search is approximate however and one sacrifices a predictable degree of error in the search in exchange for a significant improvement in query time more formally a family of lsh functions is a distribution of functions where for any two objects xi and xj pr h f rh xi h xj sim xi xj where sim xi xj is some similarity function and h x is a hash function drawn from f that returns a single bit charikar concatenating a series of b hash functions drawn from f yields b dimensional hash keys when h xi h xj xi and xj collide in the hash table because the probability that two inputs collide is equal to the similarity between them highly similar objects are indexed together in the hash table with high probability on the other hand if two objects are very dissimilar they are unlikely to share a hash key see figure at query time one maps the query to its hash bucket pulls up any database instances also in that bucket and then exhaustively searches only those few examples in practice multiple hash tables are often used each with independently of this guarantee for nearest neighbors rather than r radius neighbors also exist sensitive hashing has been formulated in two related contexts one in which the likelihood of collision is guaranteed rel ative to a threshold on the radius surrounding a query point indyk and motwani and another where collision probabilities are equated with a similarity function score charikar we use the latter definition here h rk n hr r figure overview of locality sensitive hashing if hash functions guarantee a high probability of collision for features that are similar under a metric of interest one can search a large database in sub linear time via locality sensitive hashing techniques charikar indyk and motwani a list of k hash functions hrk are applied to map n database images to a hash table where similar items are likely to share a bucket after hashing a query q one must only evaluate the similarity between q and the database examples with which it collides to obtain the approximate near neighbors from kulis et al copyright ieee drawn hash functions and the query is compared against the union of the database points to which it hashes in all tables given valid lsh functions the query time for retrieving e near neighbors is bounded by o n e for the hamming distance and a database of size n gionis et al one can therefore trade off the accuracy of the search with the query time required early lsh functions were developed to accommodate the hamming distance indyk and motwani inner prod ucts charikar and fp norms datar et al these methods were quickly adopted by vision researchers for a variety of image search applications shakhnarovich et al since meaningful image comparisons for recognition often demand richer comparison mea sures work in the vision community has developed novel locality sensitive hash functions for addi tional classes of metrics for example an embedding of the normalized partial matching between two sets of local features is given in grauman and darrell that allows sub linear time hashing for the pyramid match kernel see section below a related form of hashing com putes sketches of feature sets and allows search according to the sets overlap broder this min hash framework has been demonstrated and extended for near duplicate detection and im age search in chum et al most recently a kernelized form of lsh klsh is proposed in kulis and grauman which makes it possible to perform locality sensitive hashing for arbitrary kernel functions results are shown for various kernels relevant to object recognition in cluding the kernel that is often employed for comparing bag of words descriptors to be defined below aside from widening the class of metrics and kernels supportable with lsh researchers have also considered how to integrate machine learning elements so that the hash functions are better suited for a particular task for object recognition this means that one wants hash functions that are more likely to map instances of the same object to the same hash buckets or similarly patch descriptors from the same real world object point to the same bucket for example parameter sen sitive hashing psh shakhnarovich et al is an lsh based algorithm that uses boosting to select hash functions that best reflect similarity in a parameter space of interest semi supervised hash functions make it possible to efficiently index data according to learned distances jain et al kulis et al strecha et al wang kumar and chang typically supervision is given in the form of similar and dissimilar pairs of instances and then while the metric learning algorithm updates its parameters to best capture those constraints the hash functions parameters are simultaneously adjusted while most methods assume that all supervision is available in batch at the onset online metric learners that accumulate constraints over time together with hash tables that can be updated incrementally have also been developed jain et al binary embedding functions embedding functions are a related mechanism that are used to map expensive distance functions into something more manageable computationally either constructed or learned these embeddings aim to approximately preserve the desired distance function when mapping to a low dimensional space that is more easily searchable with known techniques informally given an original feature space x and associated distance function dx the basic idea is to designate a function f x e that maps the inputs into a new space e with associated distance de in such a way that de f x f y dx x y for any x y often the target space for the embedding is the hamming space such binary codes have the advantage of requiring minimal memory they also permit fast bit counting routines for the hamming distance and can be indexed directly using the computer memory addresses work in the vision and learning community has developed useful embedding functions that aim to preserve a variety of similarity metrics with simple low dimensional binary codes for example the boostmap athitsos et al and boosted similarity sensitive coding boost ssc shakhnarovich algorithms learn an embedding using different forms of boosting combining multiple weighted embeddings so as to preserve the proximity structure given by the original distance function building on this notion more recent work develops semantic hashing algorithms that train embedding functions using boosting or multiple layers of restricted boltzmann machines salakhutdinov and hinton torralba et al results show the im pact for searching gist image descriptors torralba et al embeddings based on random projections have also been explored for shift invariant kernels which includes a gaussian ker nel raginsky and lazebnik such methods are related to lsh in the sense that both seek small keys that can be used to encode similar inputs and often these keys exist in hamming space however note that while hash functions also typically map the data to binary strings the hash keys in that case the codes serve to insert instances into buckets whereas technically the embedding function outputs are treated as a new feature space in which to perform the similarity search a rule of thumb for reducing ambiguous matches when matching local feature sets extracted from real world images many features will stem from background clutter and will therefore have no meaningful neighbor in the other set other features lie on repetitive structures and may therefore have ambiguous matches for example imagine an image containing a building with many identical windows hence one needs to find a way to distinguish reliable matches from unreliable ones this cannot be done based on the descriptor distance alone since some descriptors are more discriminative than others an often used strategy initially proposed by lowe is to consider the ratio of the distance to the closest neighbor to that of the second closest one as a decision criterion specifically we identify the nearest neighbor local feature originating from an exemplar in the database of training images and then consider the second nearest neighbor that originates from a different object than the nearest neighbor feature if the ratio of the distance to the first neighbor over the distance to the second neighbor is relatively large this is a sign that the match may be ambiguous similarly if the ratio is low it suggests that it is a reliable match this strategy effectively penalizes features that come from a densely populated region of feature space and that are therefore more ambiguous by comparing the probability density functions of correct and incorrect matches in quantitative experiments lowe arrives at the recommendation to reject all matches in which the distance ratio is greater than which in his experiments eliminated of the false matches while discarding less than correct matches lowe indexing features with visual vocabularies in this section we overview the concept of a visual vocabulary a strategy that draws inspiration from the text retrieval community and enables efficient indexing for local image features rather than preparing a tree or hashing data structure to aid in direct similarity search the idea is to quantize the local feature space by mapping the local descriptors to discrete tokens we can then match them by simply looking up features assigned to the identical token in the following we first describe the formation of visual words sections through and then describe their utility for indexing section note that we will return to this repre sentation later in section in the context of object categorization as it is the basis for the simple but effective bag of words image descriptor a b c d figure a schematic to illustrate visual vocabulary construction and word assignment a a large corpus of representative images are used to populate the feature space with descriptor instances the white ellipses denote local feature regions and the black dots denote points in some feature space e g sift b next the sampled features are clustered in order to quantize the space into a discrete number of visual words the visual words are the cluster centers denoted with the large green circles the dotted green lines signify the implied voronoi cells based on the selected word centers c now given a new image the nearest visual word is identified for each of its features this maps the image from a set of high dimensional descriptors to a list of word numbers d a bag of visual words histogram can be used to summarize the entire image see section it counts how many times each of the visual words occurs in the image creating a visual vocabulary methods for indexing and efficient retrieval with text documents are mature and effective enough to operate with millions or billions of documents at once baeza yates and ribeiro neto documents of text contain some distribution of words and thus can be compactly summarized by their word counts known as a bag of words since the occurrence of a given word tends to be sparse across different documents an index that maps words to the files in which they occur can map a keyword query directly to potentially relevant content for example if we query a document database with the word car we should immediately eliminate the many documents that never mention the word car what cues then can one take from text processing to aid visual search an image is a sort of document and using the representations introduced in chapter it contains a set of local feature descriptors however at first glance the analogy would stop there text words are discrete tokens whereas local image descriptors are high dimensional real valued feature points how could one obtain discrete visual words to do so we must impose a quantization on the feature space of local image descriptors that way any novel descriptor vector can be coded in terms of the discretized region of feature space to which it belongs the standard pipeline to form a so called visual vocabulary consists of collecting a large sample of features from a representative corpus of images and quantizing the feature space according to their statistics often simple k means clustering is used to perform the quantization one initializes the k cluster centers with randomly selected features in the corpus and then iterates between updating each point cluster membership based on which cluster center it is nearest to and updating the k means based on the mean of the points previously assigned to each cluster in that case the visual words are the k cluster centers and the size of the vocabulary k is a user supplied parameter once the vocabulary is established the corpus of sampled features can be discarded then a novel image features can be translated into words by determining which visual word they are nearest to in the feature space i e based on the euclidean distance between the cluster centers and the input descriptor see figure for a diagram of the procedure drawing inspiration from text retrieval methods sivic and zisserman proposed quan tizing local image descriptors for the sake of rapidly indexing video frames with an inverted file sivic and zisserman they showed that local descriptors extracted at interest points could be mapped to visual words by computing prototypical descriptors with k means clustering and that having these tokens enabled faster retrieval of frames containing the same words further more they showed the potential of exploiting a term frequency inverse document frequency weighting on the words which de emphasizes those words that are common to many images and thus possibly less informative and a stop list which ignores extremely frequent words that appear in nearly every image analogous to a or the in text what will a visual word capture the answer depends on several factors including what corpus of features is used to build the vocabulary the number of words selected the quantization algorithm used and the interest point or sampling mechanism chosen for feature extraction intuitively the figure four examples of visual words each group shows instances of patches that are assigned to the same visual word from sivic and zisserman copyright ieee larger the vocabulary the more fine grained the visual words in general patches assigned to the same visual word should have similar low level appearance see figure particularly when the vocabulary is formed in an unsupervised manner there are no constraints that the common types of local patterns be correlated with object level parts however in later chapters we will see some methods that use visual vocabularies or codebooks to provide candidate parts to a part based category model vocabulary trees the discussion above assumes a flat quantization of the feature space but many current tech niques exploit hierarchical partitions bosch et al grauman and darrell moosmann et al nister and stewenius yeh et al in particular the vocabu lary tree approach nister and stewenius uses hierarchical k means to recursively subdivide the feature space given a choice of the branching factor and number of levels vocabulary trees offer a significant advantage in terms of the computational cost of assigning novel image features to words from linear to logarithmic in the size of the vocabulary this in turn makes it practical to use much larger vocabularies e g on the order of one million words experimental results suggest that these more specific words smaller quantized bins are particularly useful for matching features for specific instances of objects nister and stewenius philbin et al since quantization entails a hard partitioning of the feature space it can also be useful in practice to use multiple randomized hierarchical partitions and or to perform a soft assignment in which a feature results in multiple weighted entries in nearby bins choices in vocabulary formation an important concern in creating the visual vocabulary is the choice of data used to construct it generally researchers report that the most accurate results are obtained when using the same data source to create the vocabulary as is going to be used for the classification or retrieval task this can be especially noticeable when the application is for specific level recognition rather than generic categorization for example to index the frames from a particular movie the vocabulary made from a sample of those frames would be most accurate using a second movie to form the vocabulary should still produce meaningful results though likely weaker accuracy when training a recognition system for a particular set of categories one would typically sample descriptors from training examples covering all categories to try and ensure good coverage that said with a large enough pool of features taken from diverse images admittedly a vague criterion it does appear workable to treat the vocabulary as universal for any future word assignments furthermore researchers have developed methods to inject supervision into the vocabu lary moosmann et al perronnin et al winn et al and even to integrate the classifier construction and vocabulary formation processes yang et al in this way one can essentially learn an application specific vocabulary the choice of feature detector or interest operator will also have notable impact on the types of words generated factors to consider are the invariance properties required the type of images to be described and the computational cost allowable using an interest operator e g a dog detector yields a sparse set of points that is both compact and repeatable due to the detector automatic scale selection for specific level recognition e g identifying a particular object or landmark building these points can also provide an adequately distinct description a common rule of thumb is to use multiple complementary detectors that is to combine the outputs from a corner favoring interest operator with those from a blob favoring interest operator see section of chapter for a discussion of visual word representations and choices for category level recognition inverted file indexing visual vocabularies offer a simple but effective way to index images efficiently with an inverted file an inverted file index is just like an index in a book where the keywords are mapped to the page numbers where those words are used in the visual word case we have a table that points from the word number to the indices of the database images in which that word occurs for example in the cartoon illustration in figure the database is processed and the table is populated with image indices in part a in part b the words from the new image are used to index into that table thereby directly retrieving the database images that share its distinctive words retrieval via the inverted file is faster than searching every image assuming that not all images contain every word in practice an image distribution of words is indeed sparse since the index maintains no information about the relative spatial layout of the words per image typically a spatial verification step is performed on the images retrieved for a given query as we discuss in detail in the following chapter a all database images are loaded into the index mapping words to image numbers b a new query image is mapped to indices of database images that share a word figure main idea of an inverted file index for images represented by visual words concluding remarks in short the above methods offer ways to reduce the computational cost of finding similar image descriptors within a large database while certainly crucial to practical applications of specific object recognition based on local features the focus of this segment of the lecture they are also commonly used for other search problems ranging from image retrieval to example based category recognition making this section also relevant to generic category algorithms that we will discuss starting in chapter which matching algorithm should be used when the tree or hashing algorithms directly per form similarity search offering the algorithm designer the most control on how candidate matches are gathered in contrast a visual vocabulary corresponds to a fixed quantization of a vector space and lacks such control on the other hand a visual vocabulary approach has the ability to compactly summarize all local descriptors in an image or window allowing a fast check for overall agreement between two images in general the appropriate choice for an application will depend on the similar ity metric that is required for the search the dimensionality of the data the available online memory and the offline resources for data structure setup or other overhead costs at this point we have shown how to detect describe and match local features good local feature matches between images can alone suggest a specific object has been found however to discount spurious matches or to recognize an object from very sparse local features it is important to also perform a geometric verification stage see figure thus the following chapter closes our discussion of specific object recognition with techniques to verify spatial consistency of the matches a matched features alone do not ensure a confident object match b candidate matches must next be verified for geometric consistency figure the candidate feature matches established using the methods described in this chapter may strongly suggest whether a specific object is present but are typically verified for geometric consistency in this example the good appearance matches found in the top right example can be discarded once we find they do not fit a geometric transformation well whereas those found in the top left example will check out in terms of both appearance and geometric consistency courtesy of ondrej chum coordinates each point vector is extended by an additional coordinate w e g x x y w if w figure the different levels of geometric transformations courtesy of krystian mikolajczyk then the point lies on the plane at infinity else the point location in ordinary cartesian coordinates can be obtained by dividing each coordinate by w i e x w y w estimating similarity transformations a similarity transformation can already be hypothesized from a single scale and rotation invariant interest region observed in both images let fa xa ya θa sa and fb xb yb θb sb be the two corresponding regions with center coordinates x y rotation θ and scale then we can obtain the transformation from a to b in homogenous coordinates as ds cos dθ sin dθ dx dx xb xa tsim sin dθ ds cos dθ dy where dy yb ya dθ θb θa ds sb sa if only feature locations are available we require at least two point correspondences then we can compute the two vectors between point pairs in the same image and we obtain tsim as the transformation that projects one such vector onto its corresponding vector in the other image estimating affine transformations similar to the above an affine transformation can already be obtained from a single affine covariant region correspondence recall from chapter that for estimating the elliptical region shape we had to compute the region second moment matrix m the transformation that projects the elliptical region onto a circle is given by the square root of this matrix we can thus obtain the transfor mation from region fa xa ya θa ma onto region fb xb yb θb mb by first projecting fa onto a circle rotating it according to dθ θb θa and then projecting the rotated circle back onto fb this leads to the following transformation cos dθ sin dθ taff mb rma with r sin dθ cos dθ alternatively we can estimate the affine transformation from three or more non collinear point correspondences if more than three such correspondences are available we can use all of them in order to counteract the influence of noise and obtain a more accurate transformation estimate this is done as follows we start by writing down the affine transformation we want to estimate in non homogeneous coordinates this transformation is given by a 2 matrix m and a translation vector t such that xb mxa t xb l xa l l yb ya we can now collect the unknown parameters into one vector b m4 t and write the equation in matrix form for a number of point correspondences xai and xbi a b xb xai yai xai yai m4 xbi ybi if we have exactly three point correspondences a will be square and we can obtain the solution from its inverse as b a if more than three correspondences are available we can solve the equation by building the pseudo inverse of a b ata a xb it can be shown that this solution minimizes the estimation error in the least squares sense the results of an affine estimation procedure on a real world recognition example are shown in figure 2 homography estimation a homography i e a projection of a plane onto another plane can be estimated from at least four point correspondences when using more than four correspondences this again has the advantage that we can smooth out noise by searching for a least squares estimate compared to the affine estimation above the estimation becomes a bit more complicated since we now need to work with figure 2 example results of affine transformation estimation for recognition top left model images bottom left test image right estimated affine models and supporting features from lowe copyright ieee projective geometry we can do that by using homogeneous coordinates the homography transfor mation from a point xa to its counterpart xb can then be written as follows xb xbt with xbt hxa xb xbt xbt xa yb zbt ybt ybt ya the simplest way to estimate a homography from feature correspondences is the direct linear trans formation dlt method hartley and zisserman using several algebraic manipulations this method sets up a similar estimation procedure as above resulting in the following matrix equation for the homography parameters h ah h12 xb1 xb1 yb1 h21 h23 the solution to this equation is the null space vector of a this can be obtained by computing the singular value decomposition svd of a where the solution is given by the singular vector corresponding to the smallest singular value the svd of a results in the following decomposition 0 and the solution for h is given by the last column of vt as above this solution minimizes the least squares estimation error since the homography has only degrees of freedom we are free to bring the result vector into a canonical form by an appropriate normalization this could be done by normalizing the result vector by its last entry h 9 although this procedure is often used it is problematic since may also be zero hartley and zisserman therefore recommend to normalize the vector length instead which avoids this problem h v19 v99 10 it should be noted that there are also several more elaborate estimation procedures based on nonlinear optimizations for those we however refer the reader to the detailed treatment in hartley and zisserman more general transformations the transformations discussed in this section can also be interpreted in terms of the camera models they afford affine transformations can only describe the effects of affine cameras a simplified camera model that only allows for orthographic or parallel projection they are a suitable representation if the effects of perspective distortion are small such as when the object of interest is far away from the camera and its extent in depth is comparatively small affine transformations can also be used to approximate the effects of perspective projection for small regions such as the local neighborhood of an interest region in order to describe the effects of general perspective cameras a projective transformation is needed section presented an approach for estimating homographies which capture the per spective projection of a planar surface in the case of a more general scene homographies are no longer sufficient and we need to check if the point correspondences are consistent with an epipolar geometry if the internal camera calibration is known the corresponding constraints can be expressed by the so called essential matrix which captures the rigid transformation translation rotation of the camera with respect to a static scene the essential matrix can be estimated from correspondence pairs if the internal camera calibration is unknown we need to estimate the fundamental matrix which can be estimated from correspondence pairs using a non linear approach or from correspondence pairs using a linear approach although those constraints are routinely used in reconstruction they are however only rarely used for object recognition since their estimation is generally less robust we therefore do not cover them here and refer to hartley and zisserman for details 2 dealing with outliers the assumption that all feature correspondences are correct rarely holds in practice in real world problems we often have to deal with a large fraction of outlier correspondences i e correspondences that are not explained by the chosen transformation model these outliers can stem from two sources they can either be caused by wrong or ambiguous feature matches or they can be due to correct matches that are just not explained by an overly simplistic transformation model e g if an affine transformation model is used to approximate a projective transformation in both cases the net effect is the same namely that the transformed location of a point from one image projected into the other image differs from its correspondence location in that image by more than a certain tolerance threshold the problem with outliers is that they can lead to arbitrarily wrong estimation results in connection with least squares estimation imagine a simple estimation problem of finding the best fitting line given a sample of data points if we use a least squares error criterion then moving a single data point sufficiently far away from the correct line will bias the estimated solution towards this point and may move the estimation result arbitrarily far from the desired solution the same thing will happen with all transformation methods discussed in section since they are also based on least squares estimation in order to obtain robust estimation results it is therefore necessary to limit the effect of outliers on the obtained solution for this we can use the property that the correct solution will result in consistent transformations for all inlier data points while any incorrect solution will generally only be supported by a smaller random subset of data points the recognition task thus boils down to finding a consistent transformation together with a maximal set of inliers supporting this transformation in the following we will present two popular approaches for this task ransac and the generalized hough transform both approaches have been successfully used for real world estimation problems in the past we will then briefly compare the two estimation schemes and discuss their relative advantages and disadvantages 2 ransac ransac or random sample consensus fischler and bolles has become a popular tool for solving geometric estimation problems in datasets containing outliers ransac is a non deterministic algorithm that operates in a hypothesize and test framework thus it only returns a good result with a certain probability but this probability increases with the number of iterations figure visualization of the ransac procedure for a simple problem of fitting lines to a dataset of points in in each iteration a minimal set of two points is sampled to define a line and the number of inlier points within a certain distance to this line is taken as its score in the shown example the hypothesis on the left has 7 inliers while the one on the right has making it a better explanation for the observed data courtesy of jinxiang chai given a set of tentative correspondences ransac randomly samples a minimal subset of m correspondences from this set in order to hypothesize a geometric model e g using any of the techniques described in section this model is then verified against the remaining correspondences and the number of inliers is determined as its score this process is iterated until a termination criterion is met thus the ransac procedure can be summarized as follows sample a minimal subset of m correspondences 2 estimate a geometric model t from these m correspondences verify the model t against all remaining correspondences and calculate the number of inliers i if i i store the new model t t together with its number of inliers i i repeat until the termination criterion is met see below the ransac procedure is visualized in figure for an example of fitting lines to a set of points in the plane for this kind of problem the size of the minimal sample set is m 2 i e two points are sufficient to define a line in in each iteration we thus sample two points to define a line and we determine the number of inliers to this model by searching for all points within a certain distance to the line in the example in figure the hypothesis on the left has 7 inliers while the one on the right has inliers thus the second hypothesis is a better explanation for the observed data and will replace the first one if chosen in the random sampling procedure in the above example ransac is applied to the task of finding lines in note however that ransac is not limited to this task but it can be applied to arbitrary transformation models including those derived in section in such a case we define inliers to be those points whose algorithm ransac k 0 ε m n i 0 while η εm k do sample m random correspondences compute a model t from these samples compute the number i of inliers for t if i i then i i ε i n store t end if k k end while transformation error i e the distance of the transformed point to its corresponding point in the other image is below a certain threshold it can be seen that the more inliers a certain model has the more likely it is also to be sampled since any subset of m of its inliers will give rise to a very similar model hypothesis more generally an important role in this estimation is played by the true inlier ratio ε i n of the dataset i e by the ratio of the inliers of the correct solution to all available correspondences if this ratio is known then it becomes possible to estimate the number of samples that must be drawn until an uncontaminated sample is found with probability we can thus derive a run time bound as follows let ε be the fraction of inliers as defined above and let m be the size of the sampling set then the probability that a single sample of m points is correct is εm and the probability that no correct sample is found in k ransac iterations is given by η εm k we therefore need to choose k high enough such that η is kept below the desired failure rate as the true inlier ratio is typically unknown a common strategy is to use the inlier ratio of the best solution found thus far in order to formulate the termination criterion the resulting procedure is summarized in algorithm ransac has proven its worth in a large number of practical applications in many cases yielding good solutions already in a moderate number of iterations as a result of the rather coarse quality criterion the number of inliers in a certain tolerance band the initial solution returned by ransac will however only provide a rough alignment of the model a common strategy is therefore to refine this solution further e g through a standard least squares minimization that operates only on the inlier set however as such a step may change the status of some inlier or outlier points an iterative procedure with alternating fitting and inlier outlier classification steps is advisable since ransac introduction by fischler bolles in var ious improvements and extensions have been proposed in the literature y model image test image ght voting space figure visualization of the generalized hough transform ght for object recognition each local feature matched between model and test image shown in yellow defines a transformation of the entire object reference frame shown in blue the ght lets each such feature pair vote for the parameters of the corresponding transformation and accumulates those votes in a binned voting space in this example a dimensional voting space is shown for translation x y and rotation θ in practice scale could be added as a dimension of the voting space courtesy of svetlana lazebnik david lowe and lowe left and from leibe schindler and van gool right copyright springer verlag see proc ieee int l workshop years of ransac in conjunction with cvpr for some examples among those are extensions to speed up the different ransac stages capel chum and matas matas and chum sattler et al to deliver run time guarantees for real time performance nistér raguram et al and to improve the quality of the estimated solution chum et al frahm and pollefeys torr and zisserman we refer to the rich literature for details 2 2 generalized hough transform another robust fitting technique is the hough transform the hough transform named after its inventor p v c hough was originally introduced in as an efficient method for finding straight lines in images hough its basic idea is to take the parametric form of a model e g the equation for a line in and swap the role of the variables and parameters in order to obtain an equivalent representation in the parameter space such that data points lying on the same parametric model are projected onto the same point in parameter space ballard later on showed how this idea could be generalized to detect arbitrary shapes leading to the generalized hough transform ght the basic idea of this extension is that we can let observed single feature correspondences vote for the parameters of the transformation that would project the object in the model image to the correct view in the test image for this we use the single feature estimation approaches described in sections and 2 for scale invariant and affine invariant transformation models similar to the above we subdivide the parameter space into a discrete grid of accumulator cells and enter the vote from each feature correspondence by incrementing the corresponding accumulator cell value local maxima in the hough voting space then correspond to consistent feature configurations and thus to object detection hypotheses figure visualizes the corresponding ght procedure for an example of a rotation invariant recognition problem as pointed out by lowe it is important to avoid all quantization artifacts when per forming the ght this can be done e g by interpolating the vote contribution into all adjacent cells alternatively or in addition depending on the level of noise and the granularity of the parameter space discretization we can apply gaussian smoothing on the filled voting space this becomes all the more important the higher the dimensionality of the voting space gets as the influence of noise will then spread the votes over a larger number of cells 2 3 discussion comparing ransac with the ght there is clearly a duality between both approaches both try to find a consistent model configuration under a significant fraction of outlier correspondences the ght achieves this by starting from a single feature correspondence and casting votes for all model parameters with which this correspondence is consistent in contrast ransac starts from a minimal subset of correspondences to estimate a model and then counts the number of correspondences that are consistent with this model thus the ght represents the uncertainty of the estimation in the model parameter space through the voting space bin size and optional gaussian smoothing while ransac represents the uncertainty in the image space by setting a bound on the projection error the complexity of the ght is linear in the number of feature correspondences assuming a single vote is cast for each feature and in the number of voting space cells this means that the ght can be efficiently executed if the size of the voting space is small but that it can quickly become prohibitive for higher dimensional data in practice a voting space is often considered the upper limit for efficient execution as a positive point however the ght can handle a larger percentage of outliers with higher dimensionality in some cases since inconsistent votes are then spread out over a higher dimensional volume and are thus less likely to create spurious peaks in addition the algorithm runtime is independent of the inlier ratio in contrast ransac requires a search through all data points in each iteration in or der to find the inliers to the current model hypothesis thus it becomes more expensive for larger datasets and for lower inlier ratios on the other hand advantages of ransac are that it is a general method suited to a large range of estimation problems that it is easy to imple ment and that it scales better to higher dimensional models than the ght in addition nu merous extensions have been proposed to alleviate ransac shortcomings for a range of prob lems proc ieee int l workshop years of ransac in conjunction with cvpr we have now seen the three key steps that state of the art methods use to perform specific object recognition local feature description matching and geometric verification in the next chapter we will give examples of specific systems using this general approach chap ter example systems specific object recognition in the following we will present some applications where the specific object recognition techniques presented above are used in practice the purpose of this overview is to give the reader a feeling for the range of possibilities but it should by no means be thought of as an exclusive list image matching a central motivation for the development of affine invariant local features was their use for wide baseline stereo matching although this is not directly a recognition problem it bears many parallels thinking of one camera image as the model view we are interested in finding a consistent set of correspondences in the other view under an epipolar geometry transformation model not covered in section 1 figure 1 shows an example for such an application in which feature correspondences are first established using affine covariant regions and ransac is then used to find consistent matches tuytelaars and van gool figure 2 shows another application where local feature based matching is used for cre ating panoramas this approach is again based on sift features which are used to both find overlapping image pairs and estimate a homography between them in order to stitch the images together brown and lowe 2 object recognition the introduction of local scale and rotation invariant features such as sift lowe has made it possible to develop robust and efficient approaches for specific object recognition a popular example is the approach proposed by lowe based on the generalized hough transform described in section 2 2 this approach has been widely used in mobile robotic applications and now forms part of the standard repertoire of vision libraries for robotics figure 3 shows recognition results obtained with the ght in lowe the approach described in that paper first extracts scale and rotation invariant sift features in each image and then uses matching feature pairs in order to cast votes in a coarsely binned dimensional x y θ voting space the resulting similarity transformation is in general not sufficient to represent a object pose in space however the hough voting step provides an efficient way of clustering consistent features by their contribution to the same voting bin the resulting pose hypotheses figure 1 example application wide baseline stereo matching from tuytelaars and van gool copyright springer verlag figure 2 example application image stitching courtesy of matthew brown and from brown and lowe copyright springer verlag 2 object recognition background subtraction for model boundaries objects recognized recognition in spite of occlusion figure 3 object recognition results with the approach by lowe based on the generalized hough transform based on lowe figure 4 example application large scale image retrieval the first column shows a user specified query region the other columns contain automatically retrieved matches from a database of about images from philbin et al copyright ieee figure example application image auto annotation the green bounding boxes show automatically created annotations of interesting buildings in novel test images each such bounding box is automatically linked to the corresponding article in wikipedia from gammeter et al copyright ieee are then refined by fitting an affine transformation to the feature clusters in the dominant voting bins and counting the number of inlier points as hypothesis score as a result the approach can correctly recognize complex objects and estimate their rough pose despite viewpoint changes and considerable partial occlusion 3 large scale image retrieval the techniques from chapter 3 through make it possible to scale the recognition procedure to very large data sets a large scale recognition application making use of this capability was presented by philbin et al see figure 4 here a number of different affine covariant region detectors are pooled in order to create a feature representation for each database image the extracted features are stored in an efficient indexing structure see chapter 4 in order to allow efficient retrieval from large image databases containing to 1 000 images given a user specified query region in one image the system first retrieves a shortlist of database images containing matching features and then performs a geometric verification step using ransac with an affine transformation model in order to verify and rank matching regions in other images 4 mobile visual search 4 mobile visual search a particular field where large scale content based image retrieval is actively used today is visual search from mobile phones here the idea is that a user takes a photo of an interesting object from his her mobile phone and sends it as a query to a recognition server the server recognizes the depicted object and sends back object specific content to be displayed on the mobile device one of the first approaches to demonstrate practical large scale mobile visual search was pro posed by nister and stewenius their approach based on local features and the vocabulary tree indexing scheme described in section 4 1 1 could recognize examples from a database of 000 cd covers in less than a second while running on a single laptop in the meantime a number of commercial services have sprung up that offer mobile visual search capabilities covering databases of several million images among them google goggles www google com mobile goggles kooaba visual search http www kooaba com and amazon remembers almost all such services are based on the local feature based recognition matching and geometric verification pipeline described in the previous chapters as large databases have to be searched scalable matching and indexing techniques are key despite the large database sizes the employed techniques have however been optimized so far that response times of 1 2s are feasible including feature extraction matching and geometric verification but without considering communication delays for image transmission as a result of the local feature based recognition pipeline the approaches work particularly well for textured locally planar objects such as book cd dvd covers movie posters wine bottle labels or building facades 5 image auto annotation as a final application example we present an approach for large scale image auto tagging gammeter et al quack et al i e for detection of interesting objects in consumer photos and the automatic assignment of meaningful labels or tags to those objects this is visualized in figure 5 the approach by gammeter et al quack et al starts by automatically mining geotagged photos from internet photo collections and roughly bins them into geospatial grid cells by their geotags the images in each cell are then matched in order to find clus ters of images showing the same buildings using surf features and ransac with a homography model which are then also automatically linked to wikipedia pages through their tags given a novel test image the previously extracted image clusters are used as beacons against which the test image is matched again using surf features and ransac with a homography model if a match can be established the matching image region is automatically annotated with the building name location and with a link to the associated web content 